<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Mixtures of Submodular Functions for Image Collection Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Tschiatschek</surname></persName>
							<email>tschiatschek@tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
							<email>rkiyer@u.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haochen</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">LinkedIn &amp; Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
							<email>bilmes@u.washington.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Mixtures of Submodular Functions for Image Collection Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3EB8852F2C2EB4EF159E0D928D1960F4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of image collection summarization by learning mixtures of submodular functions. Submodularity is useful for this problem since it naturally represents characteristics such as fidelity and diversity, desirable for any summary. Several previously proposed image summarization scoring methodologies, in fact, instinctively arrived at submodularity. We provide classes of submodular component functions (including some which are instantiated via a deep neural network) over which mixtures may be learnt. We formulate the learning of such mixtures as a supervised problem via large-margin structured prediction. As a loss function, and for automatic summary scoring, we introduce a novel summary evaluation method called V-ROUGE, and test both submodular and non-submodular optimization (using the submodular-supermodular procedure) to learn a mixture of submodular functions. Interestingly, using non-submodular optimization to learn submodular functions provides the best results. We also provide a new data set consisting of 14 real-world image collections along with many human-generated ground truth summaries collected using Amazon Mechanical Turk. We compare our method with previous work on this problem and show that our learning approach outperforms all competitors on this new data set. This paper provides, to our knowledge, the first systematic approach for quantifying the problem of image collection summarization, along with a new data set of image collections and human summaries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The number of photographs being uploaded online is growing at an unprecedented rate. A recent estimate is that 500 million images are uploaded to the internet every day (just considering Flickr, Facebook, Instagram and Snapchat), a figure which is expected to double every year <ref type="bibr" target="#b21">[22]</ref>. Organizing this vast amount of data is becoming an increasingly important problem. Moreover, the majority of this data is in the form of personal image collections, and a natural problem is to summarize such vast collections. For example, one may have a collection of images taken on a holiday trip, and want to summarize and arrange this collection to send to a friend or family member or to post on Facebook. Here the problem is to identify a subset of the images which concisely represents all the diversity from the holiday trip. Another example is scene summarization <ref type="bibr" target="#b27">[28]</ref>, where one wants to concisely represent a scene, like the Vatican or the Colosseum. This is relevant for creating a visual summary of a particular interest point, where we want to identify a representative set of views. Another application that is gaining importance is summarizing video collections <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13]</ref> in order to enable efficient navigation of videos. This is particularly important in security applications, where one wishes to quickly identify representative and salient images in massive amounts of video.</p><p>These problems are closely related and can be unified via the problem of finding the most representative subset of images from an entire image collection. We argue that many formulations of this problem are naturally instances of submodular maximization, a statement supported by the fact that a number of scoring functions previously investigated for image summarization are (apparently unintentionally) submodular <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>A set function f (•) is said to be submodular if for any element v and sets A ⊆ B ⊆ V \{v}, where V represents the ground set of elements, f (A ∪ {v}) -f (A) ≥ f (B ∪ {v}) -f (B). This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular functions naturally model notions of coverage and diversity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref>. In this paper, we investigate structured prediction methods for learning weighted mixtures of submodular functions for image collection summarization.</p><p>Related Work: Previous work on image summarization can broadly be categorized into (a) clustering-based approaches, and (b) approaches which directly optimize certain scoring functions. The clustering papers include <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>. For example, <ref type="bibr" target="#b11">[12]</ref> proposes a hierarchical clustering-based summarization approach, while <ref type="bibr" target="#b7">[8]</ref> uses k-medoids-based clustering to generate summaries. Similarly <ref type="bibr" target="#b15">[16]</ref> proposes top-down based clustering. A number of other methods attempt to directly optimize certain scoring functions. For example, <ref type="bibr" target="#b27">[28]</ref> focuses on scene summarization and poses an objective capturing important summarization metrics such as likelihood, coverage, and orthogonality. While they do not explicitly mention this, their objective function is in fact a submodular function. Furthermore, they propose a greedy algorithm to optimize their objective. A similar approach was proposed by <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29]</ref>, where a set cover function (which incidentally also is submodular) is used to model coverage, and a minimum disparity formulation is used to model diversity. Interestingly, they optimize their objective using the same greedy algorithm. Similarly, <ref type="bibr" target="#b14">[15]</ref> models the problem of diverse image retrieval via determinantal point processes (DPPs). DPPs are closely related to submodularity, and in fact, the MAP inference problem is an instance of submodular maximization. Another approach for image summarization was posed by <ref type="bibr" target="#b4">[5]</ref>, where they define an objective function using a graph-cut function, and attempt to solve it using a semidefinite relaxation. They unintentionally use an objective that is submodular (and approximately monotone <ref type="bibr" target="#b17">[18]</ref>) that can be optimized using the greedy algorithm.</p><p>Our Contributions: We introduce a family of submodular function components for image collection summarization over which a convex mixture can be placed, and we propose a large margin formulation for learning the mixture. We introduce a novel data set of fourteen personal image collections, along with ground truth human summaries collected via Amazon mechanical Turk, and then subsequently cleaned via methods described below. Moreover, in order to automatically evaluate the quality of novel summaries, we introduce a recall-based evaluation metric, which we call V-ROUGE, to compare automatically generated summaries to the human ones. We are inspired by ROUGE <ref type="bibr" target="#b16">[17]</ref>, a wellknown evaluation criterion for evaluating summaries in the document summarization community, but we are unaware of any similar efforts in the computer vision community for image summarization. We show evidence that V-ROUGE correlates well with human evaluation. Finally, we extensively validate our approach on these data sets, and show that it outperforms previously explored methods developed for similar problems. The resulting learnt objective, moreover, matches human summarization performance on test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image Collection Summarization</head><p>Summarization is a task that most humans perform intuitively. Broadly speaking, summarization is the task of extracting information from a source that is both minimal and most important. The precise meaning of most important (relevance) is typically subjective and thus will differ from individual to individual and hence is difficult to precisely quantify. Nevertheless, we can identify two general properties that characterize good image collection summarizes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>: Fidelity: A summary should have good coverage, meaning that all of the distinct "concepts" in the collection have at least one representative in the summary. For example, a summary of a photo collection containing both mountains and beaches should contain images of both scene types.</p><p>Diversity: Summaries should be as diverse as possible, i.e., summaries should not contain images that are similar or identical to each other. Other words for this concept include diversity or dispersion. In computer vision, this property has been referred to as orthogonality <ref type="bibr" target="#b27">[28]</ref>.</p><p>Note that <ref type="bibr" target="#b27">[28]</ref> also includes the notion of "likelihood," where summary images should have high similarity to many other images in the collection. We believe, however, that such likelihood is covered by fidelity. I.e., a summary that only has images similar to many in the collection might miss certain outlier, or minority, concepts in the collection, while a summary that has high fidelity should include a representative image for every both majority and minority concept in the collection.Also, the above properties could be made very high without imposing further size or budget constraints. I.e., the goal of a summary is to find a small or within-budget subset having the above properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>We cast the problem of image collection summarization as a subset selection problem: given a collection of images I = (I 1 , I 2 , • • • , I |V | ) represented by an index set V and given a budget c, we aim to find a subset S ⊆ V, |S| ≤ c, which best summarizes the collection. Though alternative approaches are possible, we aim to solve this problem by learning a scoring function F : 2 V → R + , such that high quality summaries are mapped to high scores and low quality summaries to low scores. Then, image collection summarization can be performed by computing:</p><formula xml:id="formula_0">S * ∈ argmax S⊆V,|S|≤c F (S).<label>(1)</label></formula><p>For arbitrary set functions, computing S * is intractable, but for monotone submodular functions we rely on the classic result <ref type="bibr" target="#b24">[25]</ref> that the greedy algorithm offers a constant-factor mathematical quality guarantee. Computational tractability notwithstanding, submodular functions are natural for measuring fidelity and diversity <ref type="bibr" target="#b18">[19]</ref> as we argue in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Criteria: V-ROUGE</head><p>Before describing practical submodular functions for mixture components, we discuss a crucial element for both summarization evaluation and for the automated learning of mixtures: an objective evaluation criterion for judging the quality of summaries. Our criterion is constructed similar to the popular ROUGE score used in multi-document summarization <ref type="bibr" target="#b16">[17]</ref> and that correlates well with human perception. For document summarization, ROUGE (which in fact, is submodular <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>) is defined as:</p><formula xml:id="formula_1">r S (A) = w∈W S∈S min (c w (A), c w (S)) w∈W S∈S c w (S) ( r(A) when S is clear from the context), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where S is a set of human-generated reference summaries, W is a set of features (n-grams), and where c w (A) is the occurrence-count of w in summary A. We may extend r(•) to handle images by letting W be a set of visual words, S a set of reference summaries, and c w (A) be the occurrence-counts of visual word w in summary A. Visual words can for example be computed from SIFT-descriptors <ref type="bibr" target="#b20">[21]</ref> as common in the popular bag-of-words framework in computer vision <ref type="bibr" target="#b30">[31]</ref>. We call this V-ROUGE (visual ROUGE). In our experiments, we use visual words extracted from color histograms, from super-pixels, and also from OverFeat <ref type="bibr" target="#b26">[27]</ref>, a deep convolutional network -details are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Framework</head><p>We construct our submodular scoring functions F w (•) as convex combinations of non-negative submodular functions</p><formula xml:id="formula_3">f 1 , f 2 , . . . , f m , i.e. F w (S) = m i=1 w i f i (S)</formula><p>, where w = (w 1 , . . . , w m ), w i ≥ 0, i w i = 1. The functions f i are submodular components and assumed to be normalized: i.e., f i (∅) = 0, and f i (V ) = 1 for polymatroid functions and max A⊆V f i (A) ≤ 1 for non-monotone functions. This ensures that the components are compatible with each other. Obviously, the merit of the scoring function F w (•) depends on the selection of the components. In Section 4, we provide a large number of natural component choices, mixtures over which span a large diversity of submodular functions. Many of these component functions have appeared individually in past work and are unified into a single framework in our approach.</p><p>Large-margin Structured Prediction: We optimize the weights w of the scoring function F w (•) in a large-margin structured prediction framework, i.e. the weights are optimized such that human summaries S are separated from competitor summaries by a loss-dependent margin:</p><formula xml:id="formula_4">F w (S) ≥ F w (S ) + (S ), ∀S ∈ S, S ∈ Y \ S,<label>(3)</label></formula><p>where (•) is the considered loss function, and where Y is a structured output space (for example Y is the set of summaries that satisfy a certain budget c, i.e. Y = {S ⊆ V : |S | ≤ c}). We assume the loss to be normalized, 0 ≤ (S ) ≤ 1, ∀S ⊆ V , to ensure mixture and loss are calibrated. Equation ( <ref type="formula" target="#formula_4">3</ref>) can be stated as F w (S) ≥ max S ∈Y [F w (S ) + (S )] , ∀S ∈ S which is called lossaugmented inference. We introduce slack variables and minimize the regularized sum of slacks <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_5">min w≥0, w 1=1 S∈S max S ∈Y [F w (S ) + (S )] -F w (S) + λ 2 w 2 2 ,<label>(4)</label></formula><p>where the non-negative orthant constraint, w ≥ 0, ensures that the final mixture is submodular. Note a 2-norm regularizer is used on top of a 1-norm constraint w 1 = 1 which we interpret as a prior to encourage higher entropy, and thus more diverse mixture, distributions. Tractability depends on the choice of the loss function. An obvious choice is (S) = 1 -r(S), which yields a non-submodular optimization problem suitable for optimization methods such as <ref type="bibr" target="#b9">[10]</ref> (and which we try in Section 7). We also consider other loss functions that retain submodularity in loss augmented inference. For now, assume that S = max S ∈Y [F w (S ) + (S )] can be estimated efficiently. The objective in (4) can then be minimized using standard stochastic gradient descent methods, where the gradient for sample S with respect to weight w i is given as</p><formula xml:id="formula_6">∂ ∂w i F w ( S) + ( S) -F w (S) + λ 2 w 2 2 = f i ( S) -f i (S) + λw i .<label>(5)</label></formula><p>Loss Functions: A natural loss function is 1-R (S) = 1 -r(S) where r(S) = V-ROUGE(S). Because r(S) is submodular, 1 -r(S) is supermodular and hence maximizing F w (S ) + (S ) requires difference-of-submodular set function maximization <ref type="bibr" target="#b23">[24]</ref> which is NP-hard <ref type="bibr" target="#b9">[10]</ref>. We also consider two alternative loss functions <ref type="bibr" target="#b19">[20]</ref>, complement V-ROUGE and surrogate V-ROUGE.</p><p>Complement V-ROUGE sets c (S) = r(V \ S) and is still submodular but it is non-monotone. c (•) does have the necessary characteristics of a proper loss: summaries S + with large V-ROUGE score are mapped to small values and summaries S -with small V-ROUGE score are mapped to large values. In particular, submodularity means r(S)</p><formula xml:id="formula_7">+ r(V \ S) ≥ r(V ) + r(∅) = r(V ) or r(V \ S) ≥ r(V ) -r(S) = 1 -r(S)</formula><p>, so complement rouge is a submodular upper bound of the ideal supermodular loss. We define surrogate V-ROUGE as surr (A) = 1</p><formula xml:id="formula_8">Z S∈S w∈W c S c w (A), where W c</formula><p>S is the set of visual words that do not appear in reference summary S and Z is a normalization constant. Here, a summary has a high loss if it contains many visual words that do not occur in reference summaries and a low loss if it mainly contains visual words that occur in the reference summaries. Surrogate V-ROUGE is not only monotone submodular, it is modular.</p><p>Loss augmented Inference: Depending on the loss function, different algorithms for performing loss augmented inference, i.e. computation of the maximum in (4), must be used. When using the surrogate loss l surr (•), the mixture function together with the loss, i.e. f L (S) = F w (S) + (S), is submodular and monotone. Hence, the greedy algorithm <ref type="bibr" target="#b24">[25]</ref> can be used for maximization. This algorithm is extremely simple to implement, and starting at S 0 = ∅, iteratively chooses an element j / ∈ S t that maximizes f L (S t ∪ j), until the budget constraint is violated. While the complexity of this simple procedure is O(n 2 ) function evaluations, it can be significantly accelerated, thanks again to submodularity <ref type="bibr" target="#b22">[23]</ref>, which in practice we find is almost linear time. When using complement rouge c (•) as the loss, f L (S) is still submodular but no longer monotone, so we utilize the randomized greedy algorithm <ref type="bibr" target="#b1">[2]</ref> (which is essentially a randomized variant of the greedy algorithm above, and has approximation guarantees). Finally, when using loss 1-V-ROUGE, F w (S) + (S) is neither submodular nor monotone and approximate maximization is intractable. However, we resort to well motivated and scalable heuristics, such as the submodular-supermodular procedures that have shown good performance in various applications <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Runtime Inference: Having learnt the weights for the mixture components, the resulting function F w (S) = m i=1 w i f i (S) is monotone submodular, which can be optimized by the accelerated greedy algorithm <ref type="bibr" target="#b22">[23]</ref>. Thanks to submodularity, we can obtain near optimal solutions very efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submodular Component Functions</head><p>In this section, we consider candidate submodular component functions to use in F w (•). We consider first functions capturing more of the notion of fidelity, and then next diversity, although the distinction is not entirely crystal clear in these functions as some have aspects of both. Many of the components are graph-based. We define a weighted graph G(V, E, s), with V representing a the full set of images and E is every pair of elements in V . Each edge (i, j) ∈ E has weight s i,j computed from the visual features as described in Section 7. The weight s i,j is a similarity score between images i and j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fidelity-like Functions</head><p>A function representing the fidelity of a subset to the whole is one that gets a large value when the subset faithfully represents that whole. An intuitively reasonable property for such a function is one that scores a summary highly if it is the case that the summary, as a whole, is similar to a large majority of items in the set V . In this case, if a given summary A has a fidelity of f (A), then any superset B ⊃ A should, if anything, have higher fidelity, and thus it seems natural to use only monotone non-decreasing functions as fidelity functions. Submodularity is also a natural property since as more and more properties of an image collection are covered by a summary, the less chance any given image not part of the summary would have in offering additional coverage -in other words, submodularity is a natural model for measuring the inherent redundancy in any summary. Given this, we briefly describe some possible choices for coverage functions: Facility Location. Given a summary S ⊆ V , we can quantify coverage of the whole image collection V by the similarity between i ∈ V and its closest image j ∈ S. Summing these similarities yields the facility location function f fac.loc. (S) = i∈V max j∈S s i,j . The facility location function has been used for scene summarization in <ref type="bibr" target="#b27">[28]</ref> and as one of the components in <ref type="bibr" target="#b19">[20]</ref>.</p><p>Sum Coverage. Here, we compute the average similarity in S rather than the similarity of the best element in S only. From the graph perspective (G) we sum over the weights of edges with at least one vertex in S. Thus, f sum cov. (S) = i∈V j∈S s i,j .</p><p>Thresholded sum/truncated graph cut This function has been used in document summarization <ref type="bibr" target="#b19">[20]</ref> and is similar to the sum coverage function except that instead of summing over all elements in S, we threshold the inner sum. Define σ i (S) = j∈S s i,j , i.e. informally, σ i (S) conveys how much of image i is covered by S. In order to keep an element i from being overly covered by S as the cause of the objective getting large, we define f thresh.sum (S) = i∈V min(σ i (S), α σ i (V )), which is both monotone and submodular <ref type="bibr" target="#b19">[20]</ref>. Under budget constraints, this function avoids summaries that over-cover any images.</p><p>Feature functions. Consider a bag-of-words image model where for i ∈ V , b i = (b i,w ) w∈W is a bag-of-words representation of image i indexed by the set of visual words W (cf. Section 5). We can then define a feature coverage function <ref type="bibr" target="#b13">[14]</ref>, defined using the visual words, as follows: f feat.cov. (S) = w∈W g i∈I b i,w , where g(•) is a monotone non-decreasing concave function. This class is both monotone and submodular, and an added benefit of scalability, since it does not require computation of a O(n 2 ) similarity matrix like the graph-based functions proposed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Diversity</head><p>Diversity is another trait of a good summary, and there are a number of ways to quantify it. In this case, while submodularity is still quite natural, monotonicity sometimes is not.</p><p>Penalty based diversity/dispersion Given a set S, we penalize similarity within S by summing over all pairs as follows: f dissim. (S) = -i∈S j∈S,j&gt;i s i,j <ref type="bibr" target="#b27">[28]</ref> (a variant, also submodular, takes the formi,j∈S s i,j <ref type="bibr" target="#b18">[19]</ref>). These functions are submodular, and monotone decreasing, so when added to other functions can yield non-monotone submodular functions. Such functions have occurred before in document summarization <ref type="bibr" target="#b18">[19]</ref>, as a dispersion function <ref type="bibr" target="#b0">[1]</ref>, and even for scene summarization <ref type="bibr" target="#b27">[28]</ref> (in this last case, the submodularity property was not explicitly mentioned).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diversity reward based on clusters.</head><p>As in <ref type="bibr" target="#b19">[20]</ref>, we define a cluster based function rewarding diversity. Given clusters P 1 , P 2 , • • • , P k obtained by some clustering algorithm. We define diversity reward functions f div.reward (S) = k j=1 g(S ∩ P j ), where g(•) is a monotone submodular function so that f div.reward (•) is also monotone and submodular. Given a budget, f div.reward (S) is maximized by selecting S as diverse, over different clusters, as possible because of diminishing credit when repeatedly choosing an item in a cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Visual Words for Evaluation</head><p>V-ROUGE (see Section 2.2) depends on a visual "bag-of-words" vocabulary, and to construct a visual vocabulary, multitude choices exists. Common choices include SIFT descriptors <ref type="bibr" target="#b20">[21]</ref>, color descriptors <ref type="bibr" target="#b33">[34]</ref>, raw image patches <ref type="bibr" target="#b6">[7]</ref>, etc. For encoding, vector quantization (histogram encoding) <ref type="bibr" target="#b3">[4]</ref>, sparse coding <ref type="bibr" target="#b34">[35]</ref>, kernel codebook encoding <ref type="bibr" target="#b3">[4]</ref>, etc. can all be used. For the construction of our V-ROUGE metric, we computed three lexical types and used their union as our visual vocabulary. The different types are intended to capture information about the images at different scales of abstraction.</p><p>Color histogram. The goal here is to capture overall image information via color information. We follow the method proposed in <ref type="bibr" target="#b33">[34]</ref>: Firstly, we extract the most frequent colors in RGB color space from the images in an image collection using 10 × 10 pixel patches. Secondly, these frequent colors are clustered by k-means into 128 clusters, resulting in 128 cluster centers. Finally, we quantize the most frequent colors in every 10 × 10 pixel image patch using nearest neighbor vector quantization. For every image, the resulting bag-of-colors is normalized to unit 1 -norm.</p><p>Super pixels. Here, we wish to capture information about small objects or image regions that are identified by segmentation. Images are first segmented using the quick shift algorithm implemented in VLFeat <ref type="bibr" target="#b32">[33]</ref>. For every segment, dense SIFT descriptors are computed and clustered into 200 clusters. Then, a patch-wise intermediate bag of words b patch is computed by vector quantization and the RGB color histogram of the corresponding patch c patch is appended to that set of words. This results in intermediate features  <ref type="bibr" target="#b3">[4]</ref>. For every image, the resulting bag-of-words representation is normalized to the unit 1 -norm.</p><formula xml:id="formula_9">φ patch = [b patch , c patch ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Data Collection</head><p>Dataset. One major contribution of our paper is our new data set which we plan soon to publicly release. Our data set consists of 14 image collections, each comprising 100 images. The image collections are typical real world personal image collections as they, for the most part, were taken during holiday trips. For each collection, human-generated summaries were collected using Amazon mechanical Turk. Workers were asked to select a subset of 10 images from an image collection such that it summarizes the collection in the best possible way. <ref type="foot" target="#foot_0">1</ref> In contrast to previous work on movie summarization <ref type="bibr" target="#b12">[13]</ref>, Turkers were not tested for their ability to produce high quality summaries. Every Turker was rewarded 10 US cents for every summary.</p><p>Pruning of poor human-generated summaries. The summaries collected using Amazon mechanical Turk differ drastically in quality. For example, some of the collected summaries have low quality because they do not represent an image collection properly, e.g. they consist only of pictures of the same people but no pictures showing, say, architecture. Even though we went through several distinct iterations of summary collection via Amazon Turk, improving the quality of our instructions each time, it was impossible to ensure that all individuals produced meaningful summaries. Such low quality summaries can drastically degrade performance of the learning algorithm. We thus developed a strategy to automatically prune away bad summaries, where "bad" is defined as the worst V-ROUGE score relative to a current set of human summaries. The strategy is depicted in Algorithm 1. Each pruning step removes the worst human summary, and then creates a new instance of V-ROUGE using the updated pruned summaries. Pruning proceeds as long as a significant fraction (greater than a desired "p-value") of null-hypothesis summarizes (generated uniformly at random) scores better than the worst human summary. We chose a significant value of p = 0.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>To validate our approach, we learned mixtures of submodular functions with 594 component functions using the data set described in Section 6. In this data set, all human generated reference summaries are size 10, and we evaluated performance of our learnt mixtures also by producing size 10 summaries. The component functions were the monotone submodular functions described in Section 4 using features described in Section 5. For weight optimization, we used AdaGrad <ref type="bibr" target="#b5">[6]</ref>, an adaptive subgradient method allowing for informative gradient-based learning. We do 20 passes through the samples in the collection.</p><p>We considered two types of experiments: 1) cheating experiments to verify that our proposed mixture components can effectively learn good scoring functions; and 2) a 14-fold cross-validation experiment to test our approach in real-world scenarios. In the cheating experiments, training and testing is performed on the same image collection, and this is repeated 14 times. By contrast, for our 14-fold cross-validation experiments, training is performed on 13 out of 14 image collections and testing is performed on the held out summary, again repeating this 14 times. In both experiment types, since our learnt functions are always monotone submodular, we compute summaries S * of size 10 that approximately maximize the scoring functions using the greedy algorithm. For these summaries, we compute the V-ROUGE score r(S * ). For easy score interpretation, we normalize it according to sc(S * ) = (r(S * ) -R)/(H -R), where R is the average V-ROUGE score of random summaries (computed from 1000 summaries) and where H is the average V-ROUGE score of the collected final pruned human summaries. The result sc(S * ) is smaller than zero if S * scores worse than the average random summary and larger than one if it scores better than the average human summary.</p><p>The best cheating results are shown as Cheat in Table <ref type="table">1</ref>, learnt using 1-V-ROUGE as a loss. The results in column Min are computed by constrainedly minimizing V-ROUGE via the methods of <ref type="bibr" target="#b10">[11]</ref>, and the results in column Max are computed by maximizing V-ROUGE using the greedy algorithm. Therefore, the Max column is an approximate upper bound on our achievable performance. Clearly, we are able to learn good scoring functions, as on average we significantly exceed average human performance, i.e., we achieve an average score of 1.42 while the average human score is 1.00.</p><p>Results for cross-validation experiments are presented in Table <ref type="table">1</ref>. In the columns Our Methods we present the performance of our mixtures learnt using the proposed loss functions described in Section 3. We also present a set of baseline comparisons, using similarity scores computed via a histogram intersection <ref type="bibr" target="#b31">[32]</ref> method over the visual words used in the construction of V-ROUGE. We present baseline results for the following schemes: FL the facility location objective f fac.loc. (S) alone; FL pen the facility location objective mixed with a λ-weighted penalty, i.e. f fac.loc. (S) + λf dissim. (S); MMR Maximal marginal relevance <ref type="bibr" target="#b2">[3]</ref>, using λ to tradeoff between relevance and diversity; GC pen Graphcut mixed with a λ-weighted penalty, similar to FL pen but where graphcut is used in place of facility location; kM K-Medoids clustering <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">Algorithm 14.2]</ref>. Initial cluster centers were selected uniformly at random. As a dissimilarity score between images i and j, we used 1 -s i,j . Clustering was run 20 times, and we used the cluster centers of the best clustering as the summary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 1 |R|R∈R 1 Figure 1 :</head><label>1111</label><figDesc>Figure 1: Three example 10×10 image collections from our new data set.</figDesc><graphic coords="7,169.38,209.07,91.08,90.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>These intermediate features are again clustered into 200 clusters. Finally, the intermediate features are vector-quantized according to their 1 -distance. This final bag-of-words representation is normalized to unit 1 -norm. Deep convolutional neural network. Our deep neural network based words are meant to capture high-level information from the images. We use OverFeat [27], i.e. an image recognizer and feature extractor based on a convolutional neural network for extracting medium to high level image features. A sliding window is moved across an input picture such that every image is divided into 10 × 10 blocks (using a 50% overlap) and the pixels within the window are presented to OverFeat as input. The activations on layer 17 are taken as intermediate features φ k and clustered by k-means into 300 clusters. Then, each φ k is encoded by kernel codebook encoding</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We did not provide explicit instructions on precisely how to summarize an image collection and instead only asked that they choose a representative subset. We relied on their high-level intuitive understanding that the gestalt of the image collection should be preserved in the summary.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This material is based upon work supported by the National Science Foundation under Grant No. (IIS-1162606), the Austrian Science Fund under Grant No. (P25244-N15), a Google and a Microsoft award, and by the Intel Science and Technology Center for Pervasive Computing. Rishabh Iyer is also supported by a Microsoft Research Fellowship award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In each of the above cases where a λ weight is used, we take for each image collection the λ ∈ {0, 0.1, 0.2, . . . , 0.9, 1.0} that produced a submodular function that when maximized produced the best average V-ROUGE score on the 13 training image sets. This approach, therefore, selects the best baseline possible when performing a grid-search on the training sets. Note that both λ-dependent functions, i.e. FL pen and GC pen , are non-monotone submodular. Therefore, we used the randomized greedy algorithm <ref type="bibr" target="#b1">[2]</ref> for maximization which has a mathematical guarantee (we ran the algorithm 10 times and used the best result).</p><p>Table <ref type="table">1</ref> shows that using 1-V-ROUGE as a loss significantly outperforms the other methods. Furthermore, the performance is on average better than human performance, i.e. we achieve an average score of 1.13 while the average human score is 1.00. This indicates that we can efficiently learn scoring functions suitable for image collection summarization. For the other two losses, i.e. surrogate and complement V-ROUGE, performance is significantly worse. Thus, in this case it seems advantageous to use the proper (supermodular) loss and heuristic optimization (the submodular-supermodular procedure <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10]</ref>) for loss-augmented inference during training, compared to using an approximate (submodular or modular) loss in combination with an optimization algorithm for loss-augmented inference with strong guarantees. This could, however, perhaps be circumvented by constructing a more accurate strictly submodular surrogate loss but we leave this to future work. We have considered the task of automated summarization of image collections. A new data set together with many human generated ground truth summaries was presented and a novel automated evaluation metric called V-ROUGE was introduced. Based on large-margin structured prediction, and either submodular or non-submodular optimization, we proposed a method for learning scoring functions for image collection summarization and demonstrated its empirical effectiveness. In future work, we would like to scale our methods to much larger image collections. A key step in this direction is to consider low complexity and highly scalable classes of submodular functions. Another challenge for larger image collections is how to collect ground truth, as it would be difficult for a human to summarize a collection of, say, 10,000 images.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Max-sum diversification, monotone submodular functions and dynamic updates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st symposium on Principles of Database Systems</title>
		<meeting>of the 31st symposium on Principles of Database Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="155" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Submodular maximization with cardinality constraints</title>
		<author>
			<persName><forename type="first">N</forename><surname>Buchbinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lemtexpitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selecting canonical views for view-based 3-d object recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demirci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Abrahamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004-08">Aug 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="273" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video summarization by k-medoid clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Essannouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O H</forename><surname>Thami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Applied Computing (SAC)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1400" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithms for approximate minimization of the difference between submodular functions, with applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast semidifferential-based submodular function optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating summaries and visualization for large collections of geo-referenced photographs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video summarization using web-image priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Submodularity for data selection in machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014-10">October 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">k-DPPs: Fixed-size determinantal point processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image annotation by large-scale content-based image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th ann. ACM Int. Conf. on Multimedia</title>
		<meeting>14th ann. ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="607" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/HLT-2011</title>
		<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning mixtures of submodular shells with application to document summarization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="479" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Internet trends</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Kleiner Perkins Caufield &amp; Byers</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accelerated greedy algorithms for maximizing submodular set functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization Techniques</title>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A submodular-supermodular procedure with applications to discriminative structure learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting><address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functions-i</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="294" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic video summarization by graph modeling</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003-10">Oct 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scene Summarization for Online Image Collections</title>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extractive summarization of personal photos from life events</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Summarization of personal photologs using multidimensional content and context</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Retrieval (ICMR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discovering objects and their location in images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005-10">Oct 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Color indexing. Int. journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bag-of-colors for improved image search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wengert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia (MM)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1437" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
