<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@cs.stonybrook.edu</email>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
							<email>skiena@cs.stonybrook.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rami Al-Rfou Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2623330.2623732</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Management]: Database Applications -Data Mining</term>
					<term>I.2.6 [Artificial Intelligence]: Learning</term>
					<term>I.5.1 [Pattern Recognition]: Model -Statistical social networks</term>
					<term>deep learning</term>
					<term>latent representations</term>
					<term>learning with partial labels</term>
					<term>network classification</term>
					<term>online learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. Deep-Walk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.</p><p>DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate Deep-Walk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.</p><p>DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our proposed method learns a latent space representation of social interactions in R d . The learned representation encodes community structure so it can be easily exploited by standard classification methods. Here, our method is used on Zachary's Karate network <ref type="bibr" target="#b45">[44]</ref> to generate a latent representation in R 2 . Note the correspondence between community structure in the input graph and the embedding. Vertex colors represent a modularity-based clustering of the input graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The sparsity of a network representation is both a strength and a weakness. Sparsity enables the design of efficient discrete algorithms, but can make it harder to generalize in statistical learning. Machine learning applications in networks (such as network classification <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b38">37]</ref>, content recommendation <ref type="bibr" target="#b12">[12]</ref>, anomaly detection <ref type="bibr" target="#b6">[6]</ref>, and missing link prediction <ref type="bibr" target="#b24">[23]</ref>) must be able to deal with this sparsity in order to survive.</p><p>In this paper we introduce deep learning (unsupervised feature learning) <ref type="bibr" target="#b3">[3]</ref> techniques, which have proven successful in natural language processing, into network analysis for the first time. We develop an algorithm (DeepWalk) that learns social representations of a graph's vertices, by modeling a stream of short random walks. Social representations are latent features of the vertices that capture neighborhood similarity and community membership. These latent representations encode social relations in a continuous vector space with a relatively small number of dimensions. Deep-Walk generalizes neural language models to process a special language composed of a set of randomly-generated walks. These neural language models have been used to capture the semantic and syntactic structure of human language <ref type="bibr">[7]</ref>, and even logical analogies <ref type="bibr" target="#b30">[29]</ref>.</p><p>DeepWalk takes a graph as input and produces a latent representation as an output. The result of applying our method to the well-studied Karate network is shown in Figure <ref type="figure" target="#fig_1">1</ref>. The graph, as typically presented by force-directed layouts, is shown in Figure <ref type="figure" target="#fig_1">1a</ref>. Figure <ref type="figure" target="#fig_1">1b</ref> shows the output of our method with 2 latent dimensions. Beyond the striking similarity, we note that linearly separable portions of (1b) correspond to clusters found through modularity maximization in the input graph (1a) (shown as vertex colors).</p><p>To demonstrate DeepWalk's potential in real world scenarios, we evaluate its performance on challenging multi-label network classification problems in large heterogeneous graphs. In the relational classification problem, the links between feature vectors violate the traditional i.i.d. assumption. Techniques to address this problem typically use approximate inference techniques <ref type="bibr" target="#b33">[32]</ref> to leverage the dependency information to improve classification results. We distance ourselves from these approaches by learning label-independent representations of the graph. Our representation quality is not influenced by the choice of labeled vertices, so they can be shared among tasks.</p><p>DeepWalk outperforms other latent representation methods for creating social dimensions <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b42">41]</ref>, especially when labeled nodes are scarce. Strong performance with our representations is possible with very simple linear classifiers (e.g. logistic regression). Our representations are general, and can be combined with any classification method (including iterative inference methods). DeepWalk achieves all of that while being an online algorithm that is trivially parallelizable.</p><p>Our contributions are as follows:</p><p>• We introduce deep learning as a tool to analyze graphs, to build robust representations that are suitable for statistical modeling. DeepWalk learns structural regularities present within short random walks.</p><p>• We extensively evaluate our representations on multilabel classification tasks on several social networks. We show significantly increased classification performance in the presence of label sparsity, getting improvements 5%-10% of Micro F1, on the sparsest problems we consider. In some cases, DeepWalk's representations can outperform its competitors even when given 60% less training data.</p><p>• We demonstrate the scalability of our algorithm by building representations of web-scale graphs, (such as YouTube) using a parallel implementation. Moreover, we describe the minimal changes necessary to build a streaming version of our approach.</p><p>The rest of the paper is arranged as follows. In Sections 2 and 3, we discuss the problem formulation of classification in data networks, and how it relates to our work. In Section 4 we present DeepWalk, our approach for Social Representation Learning. We outline ours experiments in Section 5, and present their results in Section 6. We close with a discussion of related work in Section 7, and our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM DEFINITION</head><p>We consider the problem of classifying members of a social network into one or more categories. Let G = (V, E), where V represent the members of the network, E are their connections, E ⊆ (V × V ), and GL = (V, E, X, Y ) is a partially labeled social network, with attributes X ∈ R |V |×S where S is the size of the feature space for each attribute vector, and</p><formula xml:id="formula_0">Y ∈ R |V |×|Y| , Y is the set of labels.</formula><p>In a traditional machine learning classification setting, we aim to learn a hypothesis H that maps elements of X to the labels set Y. In our case, we can utilize the significant information about the dependence of the examples embedded in the structure of G to achieve superior performance.</p><p>In the literature, this is known as the relational classification (or the collective classification problem <ref type="bibr" target="#b38">[37]</ref>). Traditional approaches to relational classification pose the problem as an inference in an undirected Markov network, and then use iterative approximate inference algorithms (such as the iterative classification algorithm <ref type="bibr" target="#b33">[32]</ref>, Gibbs Sampling <ref type="bibr" target="#b16">[15]</ref>, or label relaxation <ref type="bibr" target="#b20">[19]</ref>) to compute the posterior distribution of labels given the network structure.</p><p>We propose a different approach to capture the network topology information. Instead of mixing the label space as part of the feature space, we propose an unsupervised method which learns features that capture the graph structure independent of the labels' distribution.</p><p>This separation between the structural representation and the labeling task avoids cascading errors, which can occur in iterative methods <ref type="bibr" target="#b35">[34]</ref>. Moreover, the same representation can be used for multiple classification problems concerning that network.</p><p>Our goal is to learn XE ∈ R |V |×d , where d is small number of latent dimensions. These low-dimensional representations are distributed; meaning each social phenomena is expressed by a subset of the dimensions and each dimension contributes to a subset of the social concepts expressed by the space.</p><p>Using these structural features, we will augment the attributes space to help the classification decision. These features are general, and can be used with any classification algorithm (including iterative methods). However, we believe that the greatest utility of these features is their easy integration with simple machine learning algorithms. They scale appropriately in real-world networks, as we will show in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LEARNING SOCIAL REPRESENTATIONS</head><p>We seek to learn social representations with the following characteristics:</p><p>• Adaptability -Real social networks are constantly evolving; new social relations should not require repeating the learning process all over again.</p><p>• Community aware -The distance between latent dimensions should represent a metric for evaluating social similarity between the corresponding members of the network. This allows generalization in networks with homophily.</p><p>• Low dimensional -When labeled data is scarce lowdimensional models generalize better, and speed up convergence and inference.</p><p>• Continuous -We require latent representations to model partial community membership in continuous space. In addition to providing a nuanced view of community membership, a continuous representation has smooth decision boundaries between communities which allows more robust classification. Our method satisfies these requirements by learning representation for vertices from a stream of short random walks, using optimization techniques originally designed for language modeling. Here, we review the basics of both random walks and language modeling, and describe how their combination satisfies our requirements.  The distribution of vertices appearing in short random walks (2a) follows a power-law, much like the distribution of words in natural language (2b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Random Walks</head><p>We denote a random walk rooted at vertex vi as Wv i . It is a stochastic process with random variables</p><formula xml:id="formula_1">W 1 v i , W 2 v i , . . . , W k v i such that W k+1 v i</formula><p>is a vertex chosen at random from the neighbors of vertex v k . Random walks have been used as a similarity measure for a variety of problems in content recommendation <ref type="bibr" target="#b12">[12]</ref> and community detection <ref type="bibr" target="#b2">[2]</ref>. They are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph <ref type="bibr" target="#b39">[38]</ref>.</p><p>It is this connection to local structure that motivates us to use a stream of short random walks as our basic tool for extracting information from a network. In addition to capturing community information, using random walks as the basis for our algorithm gives us two other desirable properties. First, local exploration is easy to parallelize. Several random walkers (in different threads, processes, or machines) can simultaneously explore different parts of the same graph. Secondly, relying on information obtained from short random walks make it possible to accommodate small changes in the graph structure without the need for global recomputation. We can iteratively update the learned model with new random walks from the changed region in time sub-linear to the entire graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Connection: Power laws</head><p>Having chosen online random walks as our primitive for capturing graph structure, we now need a suitable method to capture this information. If the degree distribution of a connected graph follows a power law (i.e. scale-free), we observe that the frequency which vertices appear in the short random walks will also follow a power-law distribution.</p><p>Word frequency in natural language follows a similar distribution, and techniques from language modeling account for this distributional behavior. To emphasize this similarity we show two different power-law distributions in Figure <ref type="figure" target="#fig_3">2</ref>. The first comes from a series of short random walks on a scale-free graph, and the second comes from the text of 100,000 articles from the English Wikipedia.</p><p>A core contribution of our work is the idea that techniques which have been used to model natural language (where the symbol frequency follows a power law distribution (or Zipf 's law )) can be re-purposed to model community structure in networks. We spend the rest of this section reviewing the growing work in language modeling, and transforming it to learn representations of vertices which satisfy our criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Language Modeling</head><p>The goal of language modeling is to estimate the likelihood of a specific sequence of words appearing in a corpus. More formally, given a sequence of words W n 1 = (w0, w1, • • • , wn), where wi ∈ V (V is the vocabulary), we would like to maximize the Pr(wn|w0, w1, • • • , wn−1) over all the training corpus. Recent work in representation learning has focused on using probabilistic neural networks to build general representations of words which extend the scope of language modeling beyond its original goals.</p><p>In this work, we present a generalization of language modeling to explore the graph through a stream of short random walks. These walks can be thought of as short sentences and phrases in a special language; the direct analog is to estimate the likelihood of observing vertex vi given all the previous vertices visited so far in the random walk, i.e.</p><formula xml:id="formula_2">Pr vi | (v1, v2, • • • , vi−1)<label>(1)</label></formula><p>Our goal is to learn a latent representation, not only a probability distribution of node co-occurrences, and so we introduce a mapping function Φ : v ∈ V → R |V |×d . This mapping Φ represents the latent social representation associated with each vertex v in the graph. (In practice, we represent Φ by a |V | × d matrix of free parameters, which will serve later on as our XE). The problem then, is to estimate the likelihood:</p><formula xml:id="formula_3">Pr vi | Φ(v1), Φ(v2), • • • , Φ(vi−1)<label>(2)</label></formula><p>However, as the walk length grows, computing this conditional probability becomes unfeasible. A recent relaxation in language modeling <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref> turns the prediction problem on its head. First, instead of using the context to predict a missing word, it uses one word to predict the context. Secondly, the context is composed of the words appearing to both the right and left of the given word. Finally, it removes the ordering constraint on the problem, instead, requiring the model to maximize the probability of any word appearing in the context without the knowledge of its offset from the given word. In terms of vertex representation modeling, this yields the optimization problem:</p><formula xml:id="formula_4">minimize Φ − log Pr {vi−w, • • • , vi+w} \ vi | Φ(vi)<label>(3)</label></formula><p>We find these relaxations are particularly desirable for social representation learning. First, the order independence assumption better captures a sense of 'nearness' that is provided by random walks. Moreover, this relaxation is quite useful for speeding up the training time by building small models as one vertex is given at a time.</p><p>Solving the optimization problem from Eq. 3 builds representations that capture the shared similarities in local graph structure between vertices. Vertices which have similar neighborhoods will acquire similar representations (encoding co-citation similarity), allowing generalization on machine learning tasks.</p><p>By combining both truncated random walks and language models we formulate a method which satisfies all of our desired properties. This method generates representations of social networks that are low-dimensional, and exist in a continuous vector space. Its representations encode latent forms of community membership, and because the method </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">METHOD</head><p>In this section we discuss the main components of our algorithm. We also present several variants of our approach and discuss their merits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>As in any language modeling algorithm, the only required input is a corpus and a vocabulary V. DeepWalk considers a set of short truncated random walks its own corpus, and the graph vertices as its own vocabulary (V = V ). While it is beneficial to know V and the frequency distribution of vertices in the random walks ahead of the training, it is not necessary for the algorithm to work as we will show in 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithm: DeepWalk</head><p>The algorithm consists of two main components; first a random walk generator, and second, an update procedure. The random walk generator takes a graph G and samples uniformly a random vertex vi as the root of the random walk Wv i . A walk samples uniformly from the neighbors of the last vertex visited until the maximum length (t) is reached. While we set the length of our random walks in the experiments to be fixed, there is no restriction for the random walks to be of the same length. These walks could have restarts (i.e. a teleport probability of returning back to their root), but our preliminary results did not show any advantage of using restarts. In practice, our implementation specifies a number of random walks γ of length t to start at each vertex.</p><p>Lines 3-9 in Algorithm 1 shows the core of our approach. The outer loop specifies the number of times, γ, which we should start random walks at each vertex. We think of each iteration as making a 'pass' over the data and sample one walk per node during this pass. At the start of each pass we generate a random ordering to traverse the vertices. This is not strictly required, but is well-known to speed up the convergence of stochastic gradient descent.</p><p>In the inner loop, we iterate over all the vertices of the graph. For each vertex vi we generate a random walk |Wv i | = t, and then use it to update our representations (Line 7). We use the SkipGram algorithm <ref type="bibr" target="#b28">[27]</ref> to update Algorithm 2 SkipGram(Φ, Wv i , w)</p><p>1: for each vj ∈ Wv i do 2:</p><formula xml:id="formula_5">for each u k ∈ Wv i [j − w : j + w] do 3: J(Φ) = − log Pr(u k | Φ(vj)) 4: Φ = Φ − α * ∂J ∂Φ 5:</formula><p>end for 6: end for these representations in accordance with our objective function in Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">SkipGram</head><p>SkipGram is a language model that maximizes the cooccurrence probability among the words that appear within a window, w, in a sentence. It approximates the conditional probability in Equation 3 using an independence assumption as the following</p><formula xml:id="formula_6">Pr {vi−w, • • • , vi+w} \ vi | Φ(vi) = i+w j=i−w j =i</formula><p>Pr(vj|Φ(vi))</p><p>(4) Algorithm 2 iterates over all possible collocations in random walk that appear within the window w (lines 1-2). For each, we map each vertex vj to its current representation vector Φ(vj) ∈ R d (See Figure <ref type="figure" target="#fig_5">3b</ref>). Given the representation of vj, we would like to maximize the probability of its neighbors in the walk (line 3). We can learn such a posterior distribution using several choices of classifiers. For example, modeling the previous problem using logistic regression would result in a huge number of labels (that is equal to |V |) which could be in millions or billions. Such models require vast computational resources which could span a whole cluster of computers <ref type="bibr" target="#b4">[4]</ref>. To avoid this necessity and speed up the training time, we instead use the Hierarchical Softmax <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b32">31]</ref> to approximate the probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Hierarchical Softmax</head><p>Given that u k ∈ V , calculating Pr(u k | Φ(vj)) in line 3 is not feasible. Computing the partition function (normalization factor) is expensive, so instead we will factorize the conditional probability using Hierarchical softmax. We assign the vertices to the leaves of a binary tree, turning the prediction problem into maximizing the probability of a specific path in the hierarchy (See Figure <ref type="figure" target="#fig_5">3c</ref>). If the path to vertex u k is identified by a sequence of tree nodes (b0, b1, . .</p><formula xml:id="formula_7">. , b log |V | ), (b0 = root, b log |V | = u k ) then Pr(u k | Φ(vj)) = log |V | l=1 Pr(b l | Φ(vj))<label>(5)</label></formula><p>Now, Pr(b l | Φ(vj)) could be modeled by a binary classifier that is assigned to the parent of the node b l as Equation <ref type="formula" target="#formula_8">6</ref>shows,</p><formula xml:id="formula_8">Pr(b l | Φ(vj) = 1/(1 + e −Φ(v j )•Ψ(b l ) )<label>(6)</label></formula><p>where Ψ(b l ) ∈ R d is the representation assigned to tree node b l 's parent. This reduces the computational complexity of calculating Pr(</p><formula xml:id="formula_9">u k | Φ(vj)) from O(|V |) to O(log |V |).</formula><p>We can speed up the training process further, by assigning shorter paths to the frequent vertices in the random walks. Huffman coding is used to reduce the access time of frequent elements in the tree.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Optimization</head><p>The model parameter set is θ = {Φ, Ψ} where the size of each is O(d|V |). Stochastic gradient descent (SGD) <ref type="bibr" target="#b5">[5]</ref> is used to optimize these parameters (Line 4, Algorithm 2). The derivatives are estimated using the back-propagation algorithm. The learning rate α for SGD is initially set to 2.5% at the beginning of the training and then decreased linearly with the number of vertices that are seen so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parallelizability</head><p>As shown in Figure <ref type="figure" target="#fig_3">2</ref> the frequency distribution of vertices in random walks of social network and words in a language both follow a power law. This results in a long tail of infrequent vertices, therefore, the updates that affect Φ will be sparse in nature. This allows us to use asynchronous version of stochastic gradient descent (ASGD), in the multi-worker case. Given that our updates are sparse and we do not acquire a lock to access the model shared parameters, ASGD will achieve an optimal rate of convergence <ref type="bibr" target="#b37">[36]</ref>. While we run experiments on one machine using multiple threads, it has been demonstrated that this technique is highly scalable, and can be used in very large scale machine learning <ref type="bibr" target="#b9">[9]</ref>. Figure <ref type="figure" target="#fig_7">4</ref> presents the effects of parallelizing DeepWalk. It shows the speed up in processing BlogCatalog and Flickr networks is consistent as we increase the number of workers to 8 (Figure <ref type="figure" target="#fig_7">4a</ref>). It also shows that there is no loss of predictive performance relative to the running DeepWalk serially (Figure <ref type="figure" target="#fig_7">4b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Algorithm Variants</head><p>Here we discuss some variants of our proposed method, which we believe may be of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Streaming</head><p>One interesting variant of this method is a streaming approach, which could be implemented without knowledge of the entire graph. In this variant small walks from the graph are passed directly to the representation learning code, and the model is updated directly. Some modifications to the learning process will also be necessary. First, using a decaying learning rate may no longer be desirable as it assumes the knowledge of the total corpus size. Instead, we can initialize</p><formula xml:id="formula_10">2 0 2 1 2 2 2 3</formula><p># of Workers  the learning rate α to a small constant value. This will take longer to learn, but may be worth it in some applications. Second, we cannot necessarily build a tree of parameters any more. If the cardinality of V is known (or can be bounded), we can build the Hierarchical Softmax tree for that maximum value. Vertices can be assigned to one of the remaining leaves when they are first seen. If we have the ability to estimate the vertex frequency a priori, we can also still use Huffman coding to decrease frequent element access times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Non-random walks</head><p>Some graphs are created as a by-product of agents interacting with a sequence of elements (e.g. users' navigation of pages on a website). When a graph is created by such a stream of non-random walks, we can use this process to feed the modeling phase directly. Graphs sampled in this way will not only capture information related to network structure, but also to the frequency at which paths are traversed.</p><p>In our view, this variant also encompasses language modeling. Sentences can be viewed as purposed walks through an appropriately designed language network, and language models like SkipGram are designed to capture this behavior.</p><p>This approach can be combined with the streaming variant (Section 4.4.1) to train features on a continually evolving network without ever explicitly constructing the entire graph. Maintaining representations with this technique could enable web-scale classification without the hassles of dealing with a web-scale graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL DESIGN</head><p>In this section we provide an overview of the datasets and methods which we will use in our experiments. Code and data to reproduce our results will be available at the first author's website. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>An overview of the graphs we consider in our experiments is given in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>• BlogCatalog <ref type="bibr" target="#b40">[39]</ref> is a network of social relationships provided by blogger authors. The labels represent the topic categories provided by the authors.</p><p>• Flickr <ref type="bibr" target="#b40">[39]</ref> is a network of the contacts between users of the photo sharing website. The labels represent the interest groups of the users such as 'black and white photos'.</p><p>• YouTube <ref type="bibr" target="#b41">[40]</ref> is a social network between users of the popular video sharing website. The labels here represent groups of viewers that enjoy common video genres (e.g. anime and wrestling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Methods</head><p>To validate the performance of our approach we compare it against a number of baselines:</p><p>• SpectralClustering <ref type="bibr" target="#b42">[41]</ref>: This method generates a representation in R d from the d-smallest eigenvectors of L, the normalized graph Laplacian of G. Utilizing the eigenvectors of L implicitly assumes that graph cuts will be useful for classification.</p><p>• Modularity <ref type="bibr" target="#b40">[39]</ref>: This method generates a representation in R d from the top-d eigenvectors of B, the Modularity matrix of G. The eigenvectors of B encode information about modular graph partitions of G <ref type="bibr" target="#b36">[35]</ref>.</p><p>Using them as features assumes that modular graph partitions will be useful for classification.</p><p>• EdgeCluster <ref type="bibr" target="#b41">[40]</ref>: This method uses k-means clustering to cluster the adjacency matrix of G. Its has been shown to perform comparably to the Modularity method, with the added advantage of scaling to graphs which are too large for spectral decomposition.</p><p>• wvRN <ref type="bibr" target="#b26">[25]</ref>: The weighted-vote Relational Neighbor is a relational classifier. Given the neighborhood Ni of vertex vi, wvRN estimates Pr(yi|Ni) with the (appropriately normalized) weighted mean of its neighbors (i.e Pr(yi|Ni) = 1 Z v j ∈N i wij Pr(yj | Nj)). It has shown surprisingly good performance in real networks, and has been advocated as a sensible relational classification baseline <ref type="bibr" target="#b27">[26]</ref>.</p><p>• Majority: This naïve method simply chooses the most frequent labels in the training set.</p><p>1 http://bit.ly/deepwalk</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>In this section we present an experimental analysis of our method. We thoroughly evaluate it on a number of multilabel classification tasks, and analyze its sensitivity across several parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multi-Label Classification</head><p>To facilitate the comparison between our method and the relevant baselines, we use the exact same datasets and experimental procedure as in <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b41">40]</ref>. Specifically, we randomly sample a portion (TR) of the labeled nodes, and use them as training data. The rest of the nodes are used as test. We repeat this process 10 times, and report the average performance in terms of both Macro-F1 and Micro-F1. When possible we report the original results <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b41">40]</ref> here directly.</p><p>For all models we use a one-vs-rest logistic regression implemented by LibLinear <ref type="bibr" target="#b11">[11]</ref> extended to return the most probable labels as in <ref type="bibr" target="#b40">[39]</ref>. We present results for Deep-Walk with (γ = 80, w = 10, d = 128). The results for (SpectralClustering, Modularity, EdgeCluster) use Tang and Liu's preferred dimensionality, d = 500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">BlogCatalog</head><p>In this experiment we increase the training ratio (TR) on the BlogCatalog network from 10% to 90%. Our results are presented in Table <ref type="table" target="#tab_3">2</ref>. Numbers in bold represent the highest performance in each column.</p><p>DeepWalk performs consistently better than EdgeCluster, Modularity, and wvRN. In fact, when trained with only 20% of the nodes labeled, DeepWalk performs better than these approaches when they are given 90% of the data. The performance of SpectralClustering proves much more competitive, but DeepWalk still outperforms when labeled data is sparse on both Macro-F1 (TR ≤ 20%) and Micro-F1 (TR ≤ 60%).</p><p>This strong performance when only small fractions of the graph are labeled is a core strength of our approach. In the following experiments, we investigate the performance of our representations on even more sparsely labeled graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Flickr</head><p>In this experiment we vary the training ratio (TR) on the Flickr network from 1% to 10%. This corresponds to having approximately 800 to 8,000 nodes labeled for classification in the entire network. Table <ref type="table" target="#tab_4">3</ref> presents our results, which are consistent with the previous experiment. DeepWalk outperforms all baselines by at least 3% with respect to Micro-F1. Additionally, its Micro-F1 performance when only 3% of the graph is labeled beats all other methods even when they have been given 10% of the data. In other words, DeepWalk can outperform the baselines with 60% less training data. It also performs quite well in Macro-F1, initially performing close to SpectralClustering, but distancing itself to a 1% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">YouTube</head><p>The YouTube network is considerably larger than the previous ones we have experimented on, and its size prevents two of our baseline methods (SpectralClustering and Modularity) from running on it. It is much closer to a real world graph than those we have previously considered.</p><p>The results of varying the training ratio (TR) from 1% to 10% are presented in  This experiment showcases the performance benefits that can occur from using social representation learning for multilabel classification. DeepWalk, can scale to large graphs, and performs exceedingly well in such a sparsely labeled environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Parameter Sensitivity</head><p>In order to evaluate how changes to the parameterization of DeepWalk effect its performance on classification tasks, we conducted experiments on two multi-label classifications tasks (Flickr, and BlogCatalog). In the interest of brevity, we have fixed the window size and the walk length to emphasize local structure (w = 10, t = 40). We then vary the number of latent dimensions (d), the number of walks started per vertex (γ), and the amount of training data available (TR) to determine their impact on the network classification performance. Figures 5a2 and 5a4 examine the effects of varying the dimensionality and number of walks per vertex. The relative performance between dimensions is relatively stable across different values of γ. These charts have two interesting observations. The first is that there is most of the benefit is accomplished by starting γ = 30 walks per node in both graphs. The second is that the relative difference between different values of γ is quite consistent between the two graphs. Flickr has an order of magnitude more edges than BlogCatalog, and we find this behavior interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Effect of Dimensionality</head><p>These experiments show that our method can make useful models of various sizes. They also show that the performance of the model depends on the number of random walks it has seen, and the appropriate dimensionality of the model depends on the training examples available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Effect of sampling frequency</head><p>Figure <ref type="figure" target="#fig_8">5b</ref> shows the effects of increasing γ, the number of random walks that we start from each vertex.</p><p>The results are very consistent for different dimensions (Fig. <ref type="figure" target="#fig_8">5b1</ref>, Fig. <ref type="figure" target="#fig_8">5b3</ref>) and the amount of training data (Fig. <ref type="figure" target="#fig_8">5b2</ref>, Fig. <ref type="figure" target="#fig_8">5b4</ref>). Initially, increasing γ has a big effect in the results, but this effect quickly slows (γ &gt; 10). These results demonstrate that we are able to learn meaningful latent representations for vertices after only a small number of random walks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>The main differences between our proposed method and previous work can be summarized as follows:</p><p>1. We learn our latent social representations, instead of computing statistics related to centrality <ref type="bibr" target="#b13">[13]</ref> or partitioning <ref type="bibr" target="#b42">[41]</ref>.</p><p>classification process. Exact inference in the collective classification problem is NP-hard, and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge <ref type="bibr" target="#b38">[37]</ref>.</p><p>The most relevant relational classification algorithms to our work incorporate community information by learning clusters <ref type="bibr" target="#b34">[33]</ref>, by adding edges between nearby nodes <ref type="bibr" target="#b15">[14]</ref>, by using PageRank <ref type="bibr" target="#b25">[24]</ref>, or by extending relational classification to take additional features into account <ref type="bibr" target="#b44">[43]</ref>. Our work takes a substantially different approach. Instead of a new approximation inference algorithm, we propose a procedure which learns representations of network structure which can then be used by existing inference procedure (including iterative ones).</p><p>A number of techniques for generating features from graphs have also been proposed <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b40">[39]</ref><ref type="bibr" target="#b41">[40]</ref><ref type="bibr" target="#b42">[41]</ref>. In contrast to these methods, we frame the feature creation procedure as a representation learning problem.</p><p>Graph Kernels <ref type="bibr" target="#b43">[42]</ref> have been proposed as a way to use relational data as part of the classification process, but are quite slow unless approximated <ref type="bibr" target="#b21">[20]</ref>. Our approach is complementary; instead of encoding the structure as part of a kernel function, we learn a representation which allows them to be used directly as features for any classification method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Unsupervised Feature Learning</head><p>Distributed representations have been proposed to model structural relationship between concepts <ref type="bibr" target="#b19">[18]</ref>. These representations are trained by the back-propagation and gradient descent. Computational costs and numerical instability led to these techniques to be abandoned for almost a decade. Recently, distributed computing allowed for larger models to be trained <ref type="bibr" target="#b4">[4]</ref>, and the growth of data for unsupervised learning algorithms to emerge <ref type="bibr" target="#b10">[10]</ref>. Distributed representations usually are trained through neural networks, these networks have made advancements in diverse fields such as computer vision <ref type="bibr" target="#b23">[22]</ref>, speech recognition <ref type="bibr" target="#b8">[8]</ref>, and natural language processing <ref type="bibr" target="#b1">[1,</ref><ref type="bibr">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>We propose DeepWalk, a novel approach for learning latent social representations of vertices. Using local information from truncated random walks as input, our method learns a representation which encodes structural regularities. Experiments on a variety of different graphs illustrate the effectiveness of our approach on challenging multi-label classification tasks.</p><p>As an online algorithm, DeepWalk is also scalable. Our results show that we can create meaningful representations for graphs which are too large for standard spectral methods. On such large graphs, our method significantly outperforms other methods designed to operate for sparsity. We also show that our approach is parallelizable, allowing workers to update different parts of the model concurrently.</p><p>In addition to being effective and scalable, our approach is also an appealing generalization of language modeling. This connection is mutually beneficial. Advances in language modeling may continue to generate improved latent representations for networks. In our view, language modeling is actually sampling from an unobservable language graph. We believe that insights obtained from modeling observable graphs may in turn yield improvements to modeling unobservable ones.</p><p>Our future work in the area will focus on investigating this duality further, using our results to improve language modeling, and strengthening the theoretical justifications of the method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Our proposed method learns a latent space representation of social interactions in R d . The learned representation encodes community structure so it can be easily exploited by standard classification methods. Here, our method is used on Zachary's Karate network<ref type="bibr" target="#b45">[44]</ref> to generate a latent representation in R 2 . Note the correspondence between community structure in the input graph and the embedding. Vertex colors represent a modularity-based clustering of the input graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The distribution of vertices appearing in short random walks (2a) follows a power-law, much like the distribution of words in natural language (2b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of DeepWalk. We slide a window of length 2w + 1 over the random walk Wv 4 , mapping the central vertex v1 to its representation Φ(v1). Hierarchical Softmax factors out Pr(v3 | Φ(v1)) and Pr(v5 | Φ(v1)) over sequences of probability distributions corresponding to the paths starting at the root and ending at v3 and v5. The representation Φ is updated to maximize the probability of v1 co-occurring with its context {v3, v5}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effects of parallelizing DeepWalk</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Parameter Sensitivity Study graph representations, EdgeCluster. When 1% of the labeled nodes are used for test, the Micro-F1 improves by 14%. The Macro-F1 shows a corresponding 10% increase. This lead narrows as the training data increases, but DeepWalk ends with a 3% lead in Micro-F1, and an impressive 5% improvement in Macro-F1.This experiment showcases the performance benefits that can occur from using social representation learning for multilabel classification. DeepWalk, can scale to large graphs, and performs exceedingly well in such a sparsely labeled environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure</head><label></label><figDesc>Figure 5a shows the effects of increasing the number of latent dimensions available to our model. Figures 5a1 and 5a3 examine the effects of varying the dimensionality and training ratio. The performance is quite consistent between both Flickr and BlogCatalog and show that the optimal dimensionality for a model is dependent on the number of training examples. (Note that 1% of Flickr has approximately as many labeled examples as 10% of BlogCatalog).Figures 5a2 and 5a4 examine the effects of varying the dimensionality and number of walks per vertex. The relative performance between dimensions is relatively stable across different values of γ. These charts have two interesting observations. The first is that there is most of the benefit is accomplished by starting γ = 30 walks per node in both graphs. The second is that the relative difference between</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Graphs used in our experiments.</figDesc><table><row><cell cols="2">Name BlogCatalog</cell><cell>Flickr</cell><cell>YouTube</cell></row><row><cell>|V |</cell><cell>10,312</cell><cell>80,513</cell><cell>1,138,499</cell></row><row><cell>|E|</cell><cell>333,983</cell><cell cols="2">5,899,882 2,990,443</cell></row><row><cell>|Y|</cell><cell>39</cell><cell>195</cell><cell>47</cell></row><row><cell>Labels</cell><cell>Interests</cell><cell>Groups</cell><cell>Groups</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>They show that DeepWalk significantly outperforms the scalable baseline for creating</figDesc><table><row><cell>% Labeled Nodes</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell>DeepWalk</cell><cell cols="6">36.00 38.20 39.60 40.30 41.00 41.30</cell><cell>41.50</cell><cell>41.50</cell><cell>42.00</cell></row><row><cell>SpectralClustering</cell><cell>31.06</cell><cell>34.95</cell><cell>37.27</cell><cell>38.93</cell><cell>39.97</cell><cell cols="4">40.99 41.66 42.42 42.62</cell></row><row><cell>EdgeCluster</cell><cell>27.94</cell><cell>30.76</cell><cell>31.85</cell><cell>32.99</cell><cell>34.12</cell><cell>35.00</cell><cell>34.63</cell><cell>35.99</cell><cell>36.29</cell></row><row><cell>Micro-F1(%) Modularity</cell><cell>27.35</cell><cell>30.74</cell><cell>31.77</cell><cell>32.97</cell><cell>34.09</cell><cell>36.13</cell><cell>36.08</cell><cell>37.23</cell><cell>38.18</cell></row><row><cell>wvRN</cell><cell>19.51</cell><cell>24.34</cell><cell>25.62</cell><cell>28.82</cell><cell>30.37</cell><cell>31.81</cell><cell>32.19</cell><cell>33.33</cell><cell>34.28</cell></row><row><cell>Majority</cell><cell>16.51</cell><cell>16.66</cell><cell>16.61</cell><cell>16.70</cell><cell>16.91</cell><cell>16.99</cell><cell>16.92</cell><cell>16.49</cell><cell>17.26</cell></row><row><cell>DeepWalk</cell><cell cols="2">21.30 23.80</cell><cell>25.30</cell><cell>26.30</cell><cell>27.30</cell><cell>27.60</cell><cell>27.90</cell><cell>28.20</cell><cell>28.90</cell></row><row><cell>SpectralClustering</cell><cell>19.14</cell><cell cols="8">23.57 25.97 27.46 28.31 29.46 30.13 31.38 31.78</cell></row><row><cell>EdgeCluster</cell><cell>16.16</cell><cell>19.16</cell><cell>20.48</cell><cell>22.00</cell><cell>23.00</cell><cell>23.64</cell><cell>23.82</cell><cell>24.61</cell><cell>24.92</cell></row><row><cell>Macro-F1(%) Modularity</cell><cell>17.36</cell><cell>20.00</cell><cell>20.80</cell><cell>21.85</cell><cell>22.65</cell><cell>23.41</cell><cell>23.89</cell><cell>24.20</cell><cell>24.97</cell></row><row><cell>wvRN</cell><cell>6.25</cell><cell>10.13</cell><cell>11.64</cell><cell>14.24</cell><cell>15.86</cell><cell>17.18</cell><cell>17.98</cell><cell>18.86</cell><cell>19.57</cell></row><row><cell>Majority</cell><cell>2.52</cell><cell>2.55</cell><cell>2.52</cell><cell>2.58</cell><cell>2.58</cell><cell>2.63</cell><cell>2.61</cell><cell>2.48</cell><cell>2.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Multi-label classification results in BlogCatalog</figDesc><table><row><cell>% Labeled Nodes</cell><cell>1%</cell><cell>2%</cell><cell>3%</cell><cell>4%</cell><cell>5%</cell><cell>6%</cell><cell>7%</cell><cell>8%</cell><cell>9%</cell><cell>10%</cell></row><row><cell>DeepWalk</cell><cell>32.4</cell><cell cols="9">34.6 35.9 36.7 37.2 37.7 38.1 38.3 38.5 38.7</cell></row><row><cell cols="2">SpectralClustering 27.43</cell><cell cols="9">30.11 31.63 32.69 33.31 33.95 34.46 34.81 35.14 35.41</cell></row><row><cell>Micro-F1(%) EdgeCluster</cell><cell>25.75</cell><cell cols="9">28.53 29.14 30.31 30.85 31.53 31.75 31.76 32.19 32.84</cell></row><row><cell>Modularity</cell><cell>22.75</cell><cell>25.29</cell><cell>27.3</cell><cell cols="6">27.6 28.05 29.33 29.43 28.89 29.17</cell><cell>29.2</cell></row><row><cell>wvRN</cell><cell>17.7</cell><cell cols="9">14.43 15.72 20.97 19.83 19.42 19.22 21.25 22.51 22.73</cell></row><row><cell>Majority</cell><cell>16.34</cell><cell cols="9">16.31 16.34 16.46 16.65 16.44 16.38 16.62 16.67 16.71</cell></row><row><cell>DeepWalk</cell><cell>14.0</cell><cell cols="9">17.3 19.6 21.1 22.1 22.9 23.6 24.1 24.6 25.0</cell></row><row><cell cols="11">SpectralClustering 13.84 17.49 19.44 20.75 21.60 22.36 23.01 23.36 23.82 24.05</cell></row><row><cell>Macro-F1(%) EdgeCluster</cell><cell>10.52</cell><cell cols="9">14.10 15.91 16.72 18.01 18.54 19.54 20.18 20.78 20.85</cell></row><row><cell>Modularity</cell><cell>10.21</cell><cell cols="6">13.37 15.24 15.11 16.14 16.64 17.02</cell><cell cols="3">17.1 17.14 17.12</cell></row><row><cell>wvRN</cell><cell>1.53</cell><cell>2.46</cell><cell>2.91</cell><cell>3.47</cell><cell>4.95</cell><cell>5.56</cell><cell>5.82</cell><cell>6.59</cell><cell>8.00</cell><cell>7.26</cell></row><row><cell>Majority</cell><cell>0.45</cell><cell>0.44</cell><cell>0.45</cell><cell>0.46</cell><cell>0.47</cell><cell>0.44</cell><cell>0.45</cell><cell>0.47</cell><cell>0.47</cell><cell>0.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Multi-label classification results in Flickr</figDesc><table><row><cell>% Labeled Nodes</cell><cell>1%</cell><cell>2%</cell><cell>3%</cell><cell>4%</cell><cell>5%</cell><cell>6%</cell><cell>7%</cell><cell>8%</cell><cell>9%</cell><cell>10%</cell></row><row><cell>DeepWalk</cell><cell cols="10">37.95 39.28 40.08 40.78 41.32 41.72 42.12 42.48 42.78 43.05</cell></row><row><cell>SpectralClustering</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Micro-F1(%) EdgeCluster</cell><cell>23.90</cell><cell>31.68</cell><cell>35.53</cell><cell>36.76</cell><cell>37.81</cell><cell>38.63</cell><cell>38.94</cell><cell>39.46</cell><cell>39.92</cell><cell>40.07</cell></row><row><cell>Modularity</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>wvRN</cell><cell>26.79</cell><cell>29.18</cell><cell>33.1</cell><cell>32.88</cell><cell>35.76</cell><cell>37.38</cell><cell>38.21</cell><cell>37.75</cell><cell>38.68</cell><cell>39.42</cell></row><row><cell>Majority</cell><cell>24.90</cell><cell>24.84</cell><cell>25.25</cell><cell>25.23</cell><cell>25.22</cell><cell>25.33</cell><cell>25.31</cell><cell>25.34</cell><cell>25.38</cell><cell>25.38</cell></row><row><cell>DeepWalk</cell><cell cols="10">29.22 31.83 33.06 33.90 34.35 34.66 34.96 35.22 35.42 35.67</cell></row><row><cell>SpectralClustering</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Macro-F1(%) EdgeCluster</cell><cell>19.48</cell><cell>25.01</cell><cell>28.15</cell><cell>29.17</cell><cell>29.82</cell><cell>30.65</cell><cell>30.75</cell><cell>31.23</cell><cell>31.45</cell><cell>31.54</cell></row><row><cell>Modularity</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>wvRN</cell><cell>13.15</cell><cell>15.78</cell><cell>19.66</cell><cell>20.9</cell><cell>23.31</cell><cell>25.43</cell><cell>27.08</cell><cell>26.48</cell><cell>28.33</cell><cell>28.89</cell></row><row><cell>Majority</cell><cell>6.12</cell><cell>5.86</cell><cell>6.21</cell><cell>6.1</cell><cell>6.07</cell><cell>6.19</cell><cell>6.17</cell><cell>6.16</cell><cell>6.18</cell><cell>6.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Multi-label classification results in YouTube</figDesc><table><row><cell></cell><cell>0.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell></row><row><cell>M i cr o F 1</cell><cell>0.32 0.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M i cr o F 1</cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell>γ</cell><cell></cell><cell>M i cr o F 1</cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d</cell><cell></cell><cell>M i cr o F 1</cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell>Training 0.01</cell><cell>0.05</cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell>1 3</cell><cell></cell><cell>30 50</cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16 32</cell><cell></cell><cell>128 256</cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell>Training 0.01</cell><cell>0.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.02</cell><cell>0.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.02</cell><cell>0.09</cell></row><row><cell></cell><cell>2 4</cell><cell>2 5</cell><cell>2 6 d</cell><cell>2 7</cell><cell>2 8</cell><cell></cell><cell>2 4</cell><cell>2 5</cell><cell>2 6 d</cell><cell>2 7</cell><cell>2 8</cell><cell></cell><cell></cell><cell>2 0</cell><cell>2 1</cell><cell>2 2</cell><cell>2 3</cell><cell>γ</cell><cell>2 4</cell><cell>2 5</cell><cell>2 6</cell><cell>2 7</cell><cell></cell><cell>2 0</cell><cell>2 1</cell><cell>2 2</cell><cell>2 3</cell><cell>γ</cell><cell>2 4</cell><cell>2 5</cell><cell>2 6</cell><cell>2 7</cell></row><row><cell></cell><cell cols="4">(a1) Flickr, γ = 30</cell><cell></cell><cell></cell><cell cols="5">(a2) Flickr, T R = 0.05</cell><cell></cell><cell></cell><cell cols="8">(b1) Flickr, T R = 0.05</cell><cell></cell><cell></cell><cell cols="5">(b2) Flickr, d = 128</cell></row><row><cell></cell><cell>0.42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell></cell></row><row><cell>M i cr o F 1</cell><cell>0.36 0.38 0.34</cell><cell></cell><cell></cell><cell>Training 0.1</cell><cell>0.5</cell><cell>1 M i cr o F</cell><cell>0.25 0.30 0.35 0.20</cell><cell></cell><cell></cell><cell>1 3</cell><cell>γ</cell><cell>30 50</cell><cell>1 M i cr o F</cell><cell>0.35 0.25 0.30 0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16 32</cell><cell>d</cell><cell>128 256</cell><cell>1 M i cr o F</cell><cell>0.35 0.30 0.25 0.20</cell><cell></cell><cell></cell><cell></cell><cell>Training 0.1</cell><cell>0.5</cell></row><row><cell></cell><cell>0.32</cell><cell></cell><cell></cell><cell>0.2</cell><cell>0.9</cell><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell>90</cell><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell>0.9</cell></row><row><cell></cell><cell>2 4</cell><cell>2 5</cell><cell>2 6 d</cell><cell>2 7</cell><cell>2 8</cell><cell></cell><cell>2 4</cell><cell>2 5</cell><cell>2 6 d</cell><cell>2 7</cell><cell>2 8</cell><cell></cell><cell></cell><cell>2 0</cell><cell>2 1</cell><cell>2 2</cell><cell>2 3</cell><cell>γ</cell><cell>2 4</cell><cell>2 5</cell><cell>2 6</cell><cell>2 7</cell><cell></cell><cell>2 0</cell><cell>2 1</cell><cell>2 2</cell><cell>2 3</cell><cell>γ</cell><cell>2 4</cell><cell>2 5</cell><cell>2 6</cell><cell>2 7</cell></row><row><cell></cell><cell cols="5">(a3) BlogCatalog, γ = 30</cell><cell cols="7">(a4) BlogCatalog, T R = 0.5</cell><cell cols="10">(b3) BlogCatalog, T R = 0.5</cell><cell cols="6">(b4) BlogCatalog, d = 128</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">(a) Stability over dimensions, d</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">(b) Stability over number of walks, γ</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">. We do not attempt to extend the classification procedure itself (through collective inference<ref type="bibr" target="#b38">[37]</ref> or graph kernels<ref type="bibr" target="#b22">[21]</ref>).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">3. We propose a scalable online method which uses only local information. Most methods require global information and are offline<ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b40">[39]</ref><ref type="bibr" target="#b41">[40]</ref><ref type="bibr" target="#b42">[41]</ref></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">].<ref type="bibr" target="#b4">4</ref>. We apply unsupervised representation learning to graphs. In this section we discuss related work in network classification and unsupervised feature learning.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank the reviewers for their helpful comments. This research was partially supported by NSF Grants DBI-1060572 and IIS-1017181, and a Google Faculty Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">1 Relational Learning Relational classification (or collective classification) methods</title>
		<imprint/>
	</monogr>
	<note>15, 25, 32] use links between data items as part of the 9</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
				<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-08">August 2013</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science, 2006. FOCS&apos;06. 47th Annual IEEE Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic gradient learning in neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neuro-Nîmes 91</title>
				<meeting>Neuro-Nîmes 91<address><addrLine>Nimes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991. EC2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ICML, ICML &apos;08</title>
				<meeting>the 25th ICML, ICML &apos;08</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1232" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. Knowledge and Data Engineering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pirotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="369" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Leveraging label-independent features for classification in sparsely labeled networks: An empirical study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Social Network Mining and Analysis</title>
				<imprint>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using ghost edges for classification in sparsely labeled networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD, KDD &apos;08</title>
				<meeting>the 14th ACM SIGKDD, KDD &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="256" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">It&apos;s who you know: Graph mining using recursive structural features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD, KDD &apos;11</title>
				<meeting>the 17th ACM SIGKDD, KDD &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="663" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth annual conference of the cognitive science society</title>
				<meeting>the eighth annual conference of the cognitive science society<address><addrLine>Amherst, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the foundations of relaxation labeling processes. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="287" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast random walk graph kernel</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete input spaces</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification of network data using very few labels</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on</title>
				<imprint>
			<date type="published" when="2010-08">Aug 2010</date>
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple relational classifier</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Macskassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Multi-Relational Data Mining</title>
				<meeting>the Second Workshop on Multi-Relational Data Mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="64" to="76" />
		</imprint>
	</monogr>
	<note>KDD-2003</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classification in networked data: A toolkit and a univariate case study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Macskassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="935" to="983" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international workshop on artificial intelligence and statistics</title>
				<meeting>the international workshop on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Iterative classification in relational data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data</title>
				<meeting>AAAI-2000 Workshop on Learning Statistical Models from Relational Data</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Leveraging relational autocorrelation with latent group models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Multi-relational Mining, MRDM &apos;05</title>
				<meeting>the 4th International Workshop on Multi-relational Mining, MRDM &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A bias/variance decomposition for models using collective inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="87" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modularity and community structure in networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="8577" to="8582" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-sixth annual ACM symposium on Theory of computing</title>
				<meeting>the thirty-sixth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD, KDD &apos;09</title>
				<meeting>the 15th ACM SIGKDD, KDD &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scalable learning of collective behavior based on sparse social dimensions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
				<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Leveraging social media networks for classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="478" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-label relational neighbor classification using social context features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD</title>
				<meeting>the 19th ACM SIGKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An information flow model for conflict and fission in small groups1</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of anthropological research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="452" to="473" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
