<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Biophysical and Electronic Engineering</orgName>
								<orgName type="institution">University of Genova</orgName>
								<address>
									<postCode>16145</postCode>
									<settlement>Genoa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Information and Communication Tech-nologies</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1EF93A803B4686672842C48EA2A40D2</idno>
					<idno type="DOI">10.1109/TNN.2003.816033</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Digital Architecture for Support Vector Machines:</p><p>Theory, Algorithm, and FPGA Implementation Davide Anguita, Member, IEEE, Andrea Boni, and Sandro Ridella, Member, IEEE Abstract-In this paper, we propose a digital architecture for support vector machine (SVM) learning and discuss its implementation on a field programmable gate array (FPGA). We analyze briefly the quantization effects on the performance of the SVM in classification problems to show its robustness, in the feedforward phase, respect to fixed-point math implementations; then, we address the problem of SVM learning. The architecture described here makes use of a new algorithm for SVM learning which is less sensitive to quantization errors respect to the solution appeared so far in the literature. The algorithm is composed of two parts: the first one exploits a recurrent network for finding the parameters of the SVM; the second one uses a bisection process for computing the threshold. The architecture implementing the algorithm is described in detail and mapped on a real current-generation FPGA (Xilinx Virtex II). Its effectiveness is then tested on a channel equalization problem, where real-time performances are of paramount importance.</p><p>Index Terms-Digital neuroprocessors, field programmable gate arrays (FPGAs), quantization effects, support vector machine (SVM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE hardware implementation of neural networks has recently attracted new interest from the neurocomputing community despite the skepticism generated by the devices of the first generation, which appeared during the 1980s (see, for example, <ref type="bibr" target="#b0">[1]</ref> for a survey of these solutions). Nowadays, two research areas can be easily identified: the first one focuses on biological inspired devices and builds on Mead's seminal work <ref type="bibr" target="#b1">[2]</ref>. In this case, the physical behavior of microelectronic devices is exploited to realize complex information processing with very low requirements in terms of area and power consumption <ref type="bibr" target="#b2">[3]</ref>.</p><p>The second area addresses the hardware implementation of algorithms, which are inspired by the neurocomputing framework but not necessarily justified form a biological point of view. Its main target is the design of dedicated analog or digital hardware with improved characteristics (e.g., performance, silicon area, power consumption, etc.) respect to a software implementation on a general-purpose microprocessor. This approach has been the most criticized in the past <ref type="bibr" target="#b3">[4]</ref>, mainly because one of the main targets of the research has been the raw computational power. There are, however, many other advantages in designing special-purpose devices, as recalled before, and in some cases (e.g., embedded systems) a dedicated solution can be preferable <ref type="bibr" target="#b4">[5]</ref>.</p><p>Our work fits in this last framework: we propose a new algorithm for support vector machine (SVM) learning and a digital architecture that implements it. The SVM is a new learning-by-example paradigm recently proposed by Vapnik and based on its statistical learning theory <ref type="bibr" target="#b5">[6]</ref>. After the first preliminary studies, SVMs have shown a remarkable efficiency, especially when compared with traditional artificial neural networks (ANNs), like the multilayer perceptron. The main advantage of SVM, with respect to ANNs, consists in the structure of the learning algorithm, characterized by the resolution of a constrained quadratic programming problem (CQP), where the drawback of local minima is completely avoided. Our algorithm improves on the proposals appeared so far (e.g., <ref type="bibr" target="#b16">[17]</ref> and consists of two parts: the first one, previously reported in the literature, solves the CQP respect all the parameters of the network, except the threshold, while the second one allows the computation of such threshold by using an iterative procedure.</p><p>The proposed algorithm can be easily mapped to a digital architecture: to assess the effectiveness of our approach, and measure the actual performance of both the algorithm and the architecture, we implement our solution on a field programmable gate array (FPGA) and test it on a telecommunication application. Our choice is motivated by recent advances in the FPGA-based technology, which allows easy reprogrammability, fast development times and reduced efforts with respect to full-custom very large-scale integration (VLSI) design. At the same time, the advances in the microelectronics process technology allow the design of FPGA-based digital systems having performances very close to the ones obtained by a manual full-custom layout (see, for example, [25], which details an efficient implementation of a neural processor).</p><p>In Section II, we revise briefly the SVM. Section III addresses the hardware implementation of SVMs when targeting digital solutions. New results on the robustness of this learning machine, respect to quantization errors, are presented, and, after surveying the state of the art of SVM learning algorithms, targeted to VLSI implementations, our new proposal is detailed. The digital architecture is described in Section IV and its FPGA implementation, along with the experiments on a telecommunication application, is described in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SVM</head><p>The objective of SVM learning is finding a classification function that approximates the unknown , on the basis of the set of measures , where is an input pattern, and the corresponding target.</p><p>To allow for nonlinear classification functions, the training points are mapped from the input space to a feature space , with , through a nonlinear mapping ; then, a simple linear hyperplane is used for separating the points in the feature space. By using clever mathematical properties of nonlinear mappings, SVMs avoid explicitly working in the feature space, so that the advantages of the linear approach are retained even though a nonlinear separating function is found (see <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> for more details)</p><p>The function is given by <ref type="bibr" target="#b0">(1)</ref> where is the normal vector of the separating hyperplane. Among all possible hyperplanes, SVMs find the one that corresponds to a function having a maximal margin or, in other words, the maximum distance from the points of each class. Since it is not common that all the points can be correctly classified, even if mapped in the high-dimensional feature space , the SVM allows for some errors but penalizes their cardinality.</p><p>Formally, this description leads to a CQP, by requiring us to find <ref type="bibr" target="#b1">(2)</ref> subject to <ref type="bibr" target="#b2">(3)</ref> This is usually referred as the Primal CQP. The function to be minimized is composed by two parts: the first one forces the hyperplane to have a maximal margin, while the second term penalizes the presence of misclassified points. The constant simply sets the tradeoff between the two terms.</p><p>One of the reason of the SVM success is the fact that this approach can be used to control the complexity of the learning machine, namely through the structural risk minimization principle <ref type="bibr" target="#b5">[6]</ref>. This principle can provide a method for controlling the complexity of the learning machine and define upper bounds of its generalization ability, albeit in a statistical framework.</p><p>The above -CQP is usually rewritten in dual form , by using the Lagrange multiplier theory <ref type="bibr" target="#b5">[6]</ref> (4) subject to <ref type="bibr" target="#b4">(5)</ref> where ( <ref type="formula">6</ref>) which allows us to write using the dual variables <ref type="bibr" target="#b6">(7)</ref> There are several advantages in using the dual formulation: the main one is that there is no need to know explicitly the function , but only the inner product between two points in the feature space. This is the well-known kernel trick that allows to deal implicitly with nonlinear mappings through the use of kernel functions <ref type="bibr" target="#b7">(8)</ref> Using the above notation, the -CQP can be rewritten in a compact form, as follows: <ref type="bibr" target="#b8">(9)</ref> where and is a vector of all ones. Since the seminal works on kernel functions, many kernels of the form given by (8) have been found; among them are the linear, the Gaussian, and the polynomial kernels <ref type="bibr" target="#b9">(10)</ref> Another advantage of using the kernel functions lies in the fact that they are positive semidefinite functionals. Therefore, using this property and the fact that the constraints of the above optimization problem are affine, any local minima is also a global one and algorithms exist which, given a fixed tolerance, find the solution in a finite number of steps <ref type="bibr" target="#b8">[9]</ref>. Furthermore, if the kernel is strictly positive definite, which is always the case except for pathological situations, the solution is also unique. These properties overcome many typical drawbacks of traditional neural-network approaches, such as the determination of a suitable minimum, the choice of the starting point, the optimal stopping criteria, etc.</p><p>As a final remark, note that the threshold does not appear in the dual formulation, but it can be found by using the Karush-Khun-Tucker (KKT) conditions at optimality. Let be the set of true support vectors, then the following equality holds: <ref type="bibr" target="#b10">(11)</ref> thus, can be found by <ref type="bibr" target="#b11">(12)</ref> Actually, this approach is correct as long as ; otherwise one can use a more robust method, suggested in <ref type="bibr" target="#b9">[10]</ref> (see also <ref type="bibr" target="#b8">[9]</ref>), where the threshold is computed using the KKT conditions, but without resorting to support vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HARDWARE IMPLEMENTATION OF SVMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. New VLSI-Friendly Algorithm for SVM Learning</head><p>Optimization problems like -CQP are well known to the scientific community, as usually faced when solving several real-world tasks. As a consequence, they have been deeply studied by researchers, and several methods have been proposed for their resolution. Among others, methods that can be easily implemented in hardware are particularly appealing: we refer to them as VLSI-friendly algorithms. The leading idea of these methods is to map the problem on a dynamical system described by a differential equation <ref type="bibr" target="#b12">(13)</ref> with and whose stable point, for , coincides with the solution of the optimization problem.</p><p>Equation ( <ref type="formula">13</ref>) can be seen as a recurrent neural network and, from an electronic point of view, can be implemented, on analog hardware, with simple electronic devices <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>A digital architecture can be targeted in a similar way by defining a recurrent relation of the form <ref type="bibr" target="#b13">(14)</ref> A simple way to obtain ( <ref type="formula">14</ref>) from ( <ref type="formula">13</ref>) is to use the Euler's method for integration, and to obtain <ref type="bibr" target="#b14">(15)</ref> where is the integration step. Unfortunately, finding a suitable , which guarantees the convergence of toward the solution when , is not a trivial task. Recently, a useful convergence result has been presented for a network that solves a CQP with inequality constraints <ref type="bibr" target="#b15">[16]</ref>. As shown in <ref type="bibr" target="#b16">[17]</ref>, this algorithm (digital SVM or DSVM) can be applied effectively to SVM learning when a Gaussian kernel is chosen. The underlying idea is to exploit the fact that a Gaussian kernel maps the data to an infinite feature space, so the effect of removing one of the parameters of the SVM from the learning process can be negligible. In particular, if we let , we force the separating hyperplane to pass through the origin in the feature space and the equality constraint disappears from the dual formulation of the CQP. Then, the DSVM can be used for solving the resulting problem <ref type="bibr" target="#b15">(16)</ref> The core of DSVM is very simple</p><formula xml:id="formula_0">(17)<label>(18)</label></formula><p>where . The convergence of the above recurrent relation is guaranteed, provided that . Even if several experiments on real-world data sets have demonstrated the effectiveness of this method, it is greatly penalized by two facts: the generalization of a SVM with a constant threshold does not fit the usual theoretical framework, and it is not easily applicable to other kernels (e.g., the linear one).</p><p>Here, we suggest a new approach, which allows the use of the DSVM algorithm and the computation of the threshold as well. This result is based on a recent work on parametric optimization <ref type="bibr" target="#b17">[18]</ref>: the main idea is to design a learning algorithm composed by two parts, working iteratively. The first part addresses the resolution of a CQP with fixed , whereas the second one im- plements a procedure for updating the value of itself, in order to reach iteratively the optimal threshold value .</p><p>If we consider the problem of SVM learning, it is easy to deduce that, whenever the threshold is considered as an a priori known parameter, then the dual formulation becomes <ref type="bibr" target="#b18">(19)</ref> where . Such a problem can be solved by a slightly modified version of DSVM, listed in Table <ref type="table">I</ref>. Given the input kernel matrix , a threshold , and a starting value , the algorithm solves the CQP of ( <ref type="formula">19</ref>) by providing an intermediate solution . As our aim is to solve the CQP with the equality constraint, it is possible to deduce, from the term , the range of variation of . In fact, being the optimal value of the threshold to be find and a tentative value at a given step, if then , otherwise if then : in other words, is a nonlinear function of that crosses zero only once, when , therefore, lies in a range for which and <ref type="bibr" target="#b17">[18]</ref>. Consequently, a simple bisection procedure can be derived for finding , that is the only point where . Our proposal, called FIxed b Svm (Fibs), is listed in Table <ref type="table">II</ref>: it has been designed with the goal of an implementation on a digital architecture, but, as will be clear subsequently, it can be implemented on a general-purpose floating-point platform as well.</p><p>Its functionality is based on the search of a range of values to which the threshold belongs. Then, at each step, it proceeds according to a simple bisection process by finding a tentative value , and by updating and on the basis of the value of . It terminates when the range becomes smaller than a given tolerance . Note that, when the algorithm starts, both and are not known, therefore, a first search of the feasible range must be performed. We found experimentally that and are good starting choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantization Effects in SVMs</head><p>To the best of our knowledge, no theoretical analysis of the quantization effects of the SVM parameters has appeared so far in the literature. For this reason, we quantify here some of these effects in order to prove that the feedforward phase of the SVM can be safely implemented in digital hardware.</p><p>We perform a worst-case analysis based on the properties of interval arithmetic <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Other techniques can be used as well (e.g., statistical methods <ref type="bibr" target="#b11">[12]</ref>), but we believe that worst-case results, when applicable, can be of more interest, when targeting digital architectures, because provide guaranteed bounds on the performance of the implementation.    where is the number of patterns with positive (negative) target and . In the above derivation, we have assumed, for simplicity, that , as in Gaussian kernels, but a similar result can be derived for other kernels as well.</p><p>Then, the bound on the quantization effect is given by (28)</p><p>The above equation can be used to analyze the contribute to the quantization error from each parameter of the SVM. As an example of this analysis, let us assume that each parameter is quantized with a similar step . According to SVM theory, any pattern lying outside the margin gives an output ; therefore, a pattern can be misclassified if the total quantization error is greater than one and of opposite sign respect to the correct target value. In other words, using (28), a necessary condition for a misclassification error is given by (29)</p><p>If the two classes are balanced the above equation simplifies to an intuitive relation between the maximum admissible quantization step and some parameters of the SVM (30) Equation (30) shows that the quantization step cannot be greater than a quantity that depends on: 1) the number of the patterns and 2) the size of the alphas. This is an intuitive result: in fact, as the number of patterns grows, the summation on them can suffer from the accumulation of quantization errors </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DIGITAL ARCHITECTURE</head><p>In Section III, we have detailed the Fibs algorithm and we have shown that can it be applied to solve an SVM-based classification problem with any kind of kernel function. In this section, we describe in detail an architecture that implements it, following a top-down approach. At first, we will show the main signals and blocks that compose the design, then we will detail each one of them. The architecture described in this paper is just a first study on this topic, useful to understand its main properties, such as number of clock cycles needed to reach a feasible solution, clock frequency, device utilization, etc. With this aim, we focus our attention on the design of a prototype of a RBF-SVM, but the same design can be easily extended to different kind of SVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Main Blocks and Signals</head><p>The input-output interface of our design is represented in Fig. <ref type="figure" target="#fig_0">1</ref>. It is characterized by two data lines, namely , used for input data, and , used for the output, of 16 bits each. The lines and act as handshake signals for the I-O. The signal begins a learning session, while the signal reports both its termination and the fact that the entity can start to submit the parameters and to the output. Finally, , , are the enable, the asynchronous reset, and the clock signals, respectively.</p><p>The functionality of the can be subdivided in three basic phases.</p><p>1) Loading Phase: When the SVMblock receives a , transition , it begins the loading of the target vector , and the kernel matrix . Actually, without loss of generality, we suppose that the SVMblock receives and stores in its memory directly the negated . The values must be delivered by scanning the matrix row by row (or column by column, thank to the simmetry properties of ), and following a typical handshake protocol. The activation of the asyncronous reset permits one to clean up all registers and to start a new learning with the initial value , otherwise the final value of the previous learning is assumed as new starting point.</p><p>2) Learning Phase: As soon as the loading is completed, the starts the learning phase, according to the Fibs algorithm detailed in Section III; once it terminates, the ready signal is activated in the output, and the block can begin the output phase; 3) Output phase: The submits the results of learning, that is the values , , to the output using the same communication protocol of the loading phase. These logical phases are implemented by the general architecture depicted in the block-scheme of Fig. <ref type="figure" target="#fig_0">1</ref>. It is mainly composed by four computing blocks, namely the , , , and -blocks, and three controllers for the loading, learning, and output phase, respectively. Whereas all the signals to/from the controllers are connected on the via a tristate-based connection, data are connected on the , while the information on the indexes each element of the kernel matrix, as detailed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Structure of Each Block</head><p>In this section, we will describe the function of each basic block of Fig. <ref type="figure" target="#fig_0">1</ref>, each of which implements a specific function expressed in algorithm Fibs: the -block contains both the memory to store matrix and all the digital components needed to implement the algorithm of Table <ref type="table">I</ref>; the -block provides all the counters and indexes needed to select either the entries of the kernel matrix and each alpha value during the flow of the algorithm; the -block contains registers and combinatorial logic for the control the values , , and . Finally, the -block contains a register where the target vector is stored, and the logic components for the computation of the equality constraint, namely the value of in Table <ref type="table">II</ref>. As a final remark, let us note that all the connections to the and the , are realized via a tristate connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>-Block: The structure of the -block is represented in Fig. <ref type="figure" target="#fig_1">2</ref>. It mainly consists of two counters, that we call (column counter, used to index a column of the kernel matrix) and</p><p>(row counter, used to index a row of the kernel matrix), respectively. The is also used to select a RAM, during loading, inside the -block of , and to select a particular value of alpha, both during the computation of and during the output phase. The output of is directly connected to the . The is an up-counter, while the s counts both up and down, as detailed IV-B2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>-Block: We implemented the DSVM algorithm in a one-dimensional systolic array composed by basic processing elements ( s), each of which has the task of updating an element of the vector . As previously indicated, we suppose that matrix is precomputed and stored in a set of RAMs of size bit; each RAM contains a row of , as shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>is the length of the word used to code each element , while with we indicate the length of the words used to code each . Fig. <ref type="figure" target="#fig_3">4</ref> shows the structure of a generic .</p><p>At a given time-step k, each holds the component in the register</p><p>, and its role is to compute ; the value is known a priori at each , when the -block is enabled to operate, and is deduced from the value , obtained from the -block and from the value of the target , provided by .</p><p>As a first step, the value is computed and stored in ; is then delivered to , whereas receives from . As a second step, computes and stores  and so on. Finally, after steps, corresponding to clock cycles, the final value is ready at the ouput of each . The storing of inside the -block is the following: each contains the th row of , that is the value (stored at location 0), (stored at location 1), and so on; finally is stored at location . In this way the corresponding has the value ready for the computation, each time a new arrives. During learning, is indexed by the address given by the counter , which counts in mode ; instead, during loading, the same counter, used to store the th row, counts in mode , and begins its counting from the value . Such an initial value is provided by the external counter (see Fig. <ref type="figure" target="#fig_1">2</ref>). Once each component of is ready, as we suppose here to implement an RBF-SVM, and being for such machines , a shift of positions must be executed. To perform  the shift, we directly connect the most significant wires of each to the less significant wires of the corresponding adder, thus avoiding, in practice, an actual shift. The set of multiplexers subsequent to the adder act as limiters, in order to constraint each alpha to lie inside the box and to compute , which is stored in the corresponding register . As soon as the value is computed and stored in , its previous value is stored in : this further set of registers is useful in order to verify the termination of the algorithm, represented by the activation of the signal . In particular, each component is a simple binary comparator providing "1" when the inputs are all equal and "0" otherwise. Finally, the -block, implements an AND operation of its inputs.</p><p>If a further iteration must be executed , another cycle begins after the storing of each in the corresponding of the .</p><p>3) Bias-Block: The -block (see Fig. <ref type="figure" target="#fig_4">5</ref>), is simply composed by three left-shift registers, containing , , and , an adder, two 2-to-1 multiplexers, a 3-to-1 multiplexer, and a binary comparator, which detects the termination of the learning phase whenever or . Note that, as is a feasible condition stopping the algorithm, the -block receives also the signal from the -block, which is equal to "1" when and "0" otherwise. Finally, the bisection process is simply implemented without connecting the LSB of the adder to the input of the multiplexer (such a connection is represented with a dashed line in Fig. <ref type="figure" target="#fig_4">5</ref>), thus avoiding, as in the case of the multiplication for discussed in Sections IV-B1 and IV-B2, an actual right-shift.</p><p>4) -Block: The role of the -block (see Fig. <ref type="figure" target="#fig_5">6</ref>), is the computation of , namely the sign of the quantity . It is composed by a serial-input serial-output circular shift register ( ), containing the vector , a "2's complement" block, a 2-to-1 multiplexer, an adder and a register acting as an accumulator. The value is simply the MSB of . The -block works during the loading phase, when the labels are stored in the , and during the learning phase after the value is generated by the -block. The connection with the is as follows: the serial-input of , read only when the input shldSISO is active high (load), is connected to the LSB of , while the vector is fully connected to . In practice, contains, at a given time during learning, a value , from the set of registers . 5) Controllers: As previously indicated, the is composed by three finite-state machines (FSMs), each one having the role of controlling a given phase of the Fibs-algorithm; we indicate them as , and , respectively. Actually, is subdivided in three different modules that we call , and .</p><p>is the actual supervisor of the architecture and manages all the transitions for the correct flow of algorithm Fibs. In particular, it enables to control the functionality of the -block, during a part of learning, and, after the value is obtained, enables the submodule , which supervises the -block. receives all the control signals , and , and decides the correct action according to their value, following the algorithm given in Table <ref type="table">II</ref>. Table <ref type="table">III</ref>, reports the main characteristics of the FSMs, such as number of states, transitions, total inputs/outputs. The choice of subdividing the controller of the system in five different sub-modules, is justified by the fact that, for synthesis purposes, it is better to have small modules working separately, as confirmed by the results discussed in Section V-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In order to test the proposed algorithm and the corresponding digital architecture, we chose a well-known benchmarking dataset (Sonar), and several datasets from a telecommunication problem, recently used for the application of SVMs to channel equalization purposes. In Section V-A, we also show how the theoretical results on the quantization effects (Section III-B) relate to the actual misclassification of the SVM on the Sonar dataset. The second problem, described in Section V-B, is particularly appealing because it is a typical case where a dedicated hardware can be of great usefulness. The floating and the fixed-point experiments are discussed in Section V-C; furthermore, in such section we report the comparison of our approach with the well-known SMO algorithm <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> for SVM learning. Finally, in Section V-D, we discuss the implementation on a FPGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sonar Dataset</head><p>The Sonar dataset is a well-known linearly separable problem, extensively used for benchmarking purposes of learning algorithms <ref type="bibr" target="#b20">[21]</ref>: its popularity is mainly due to the difficulty of the classification task. As the computation of the threshold is fundamental for the determination of the separating surface, it is particularly suitable to test the algorithm proposed in this paper. The sonar data set is composed by 208 samples of 60 features each, usually subdivided in 104 training and 104 test patterns. It is known that a linear classifier misclassifies 23 test patterns, while a RBF-SVM misclassifies six test patterns if the threshold is used, and eight test patterns otherwise <ref type="bibr" target="#b16">[17]</ref>.</p><p>The Sonar dataset has been used for testing the quality of the quantization error bounds found in Section III-B. In particular, we solved the problem using a Gaussian kernel with and : the solution consists of support vectors . Then, a random perturbation of size has been applied to each parameter of the network and the average, minimum, and maximum misclassification errors have been registered on different trials.  The results are shown in Fig. <ref type="figure" target="#fig_7">7</ref>, as a function of : the worst-case bound given by (29), as expected, is quite conservative and suggests that approximately ten bits are necessary for avoiding any misclassification error, while the experimental value is eight. However, it is worthwhile noting that are an infinitesimal amount respect to an exhaustive search for misclassification errors, but require many CPU hours on a conventional PC. The actual number of possibilities of adding to each parameter of the SVM is , therefore, an exhaustive search is impossible to perform in practice and this approach could never provide enough confidence on the result.</p><p>The worst-case bound, instead, provides a safe value, which can be easily computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Channel Equalization Problem</head><p>The channel equalization problem is a typical application where a special-purpose device can be effectively used, on the receiver side, in order to estimate one between two symbols , of an independent sequence emitted from a given source. All the unknown nonlinear effects of the involved components (transmitter, and receiver) are modeled as finite-impulse response (FIR) filters, plus a Gaussian distributed noise with zero mean and variance (31)</p><p>The classical theory, tackles this problem by finding an optimal classifier (the Bayesian maximum -ikelihood detector), which provides an estimate of through the observation of an -dimensional vector . Whereas these methods require the knowledge of the symbols probability, and the analytic structure of the model, neural network-based approaches have been successfully applied <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> to systems where such information are not known.</p><p>Table <ref type="table" target="#tab_8">IX</ref> shows the synthesis report, that is, the number of instantiated components for each block and for the whole design.</p><p>The functional simulations are summarized in Fig. <ref type="figure" target="#fig_10">10</ref>. In particular, it shows the convergence of the threshold toward the optimum for different number of clock cycles, in both (A) and (B) cases. Each learning phase terminates after 14 000 (A) and 140 000 (B) cycles, while each loading phase terminates after 290 (A) and 4226 (B) cycles. Fig. <ref type="figure" target="#fig_11">10(a</ref>) and (b) indicates that a feasible can be reached quite soon during learning: this suggests an acceptable rate of classification can be obtained well before the end of the learning process. In order to validate this assertion, we measured the percentage of test error during learning. Fig. <ref type="figure" target="#fig_11">10(c</ref>) shows the value of test errors for different clock cycles for . As one can easily verify, after only 90 000 clock cycles (50 000 before the termination of the learning), the obtained performances are quite stable around the value obtained at the end of the algorithm.</p><p>The Xilinx synthesis tool reports an indicative clock frequency, whose final estimated value is known only after the place and route procedure, which physically assigns the resources of the device, such as Slices and connections among them. Table X lists the characteristics of the architecture after the Place and Route phases and shows the device utilization, expressed in number of slices and number of lookup tables, and the clock frequency, for different architectures and optimization criteria. As expected, our approach is particularly efficient from the device utilization point of view. This is a remarkable property because several other modules, such as CPU cores, can be designed on the same device, thus allowing the building of stand-alone intelligent systems on chip.</p><p>From the observation of Table <ref type="table">X</ref>, an interesting and unusual behavior emerges. In fact, in the case , using the speed as optimization criteria, the synthesis tool provides better performances than the area case both in clock frequency and device utilization. An explanation of this could derive from the fact that, when changing the optimization criteria, different synthesis and Place and Route algorithms are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we have discussed the implementation of an SVM-based learning approach on a digital architecture and a case-study of its realization on an FPGA device. To the best of our knowledge, this is the first work on this subject, and several issues need to be studied in more detail. First of all, the obtained performances could be greatly improved. In particular, the whole architecture appears to be quite independent from the target device, therefore, it is likely that slightly modified versions exists, which originate mapped-circuits working with higher clock frequencies.</p><p>The main point, however, is that an efficient digital architecture can be adopted to solve real-world problems with SVMs, where a specialized hardware for on-line learning is of paramount importance, and it can be easily embedded as a part of a larger system on chip (SoC).</p><p>Future work will address this issue, in the spirit of previous neural processors <ref type="bibr">[25]</ref>, with the integration on the same chip of a general-purpose processing unit and a learning coprocessor module.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. SVMblock.</figDesc><graphic coords="4,104.22,62.28,384.84,347.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. counters-block.</figDesc><graphic coords="4,302.64,438.06,253.68,177.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. dsvm-block.</figDesc><graphic coords="5,47.88,62.28,494.52,645.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Generic PE.</figDesc><graphic coords="6,66.96,62.28,459.36,206.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. bias-Block.</figDesc><graphic coords="6,303.42,307.80,249.48,336.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. s-block.</figDesc><graphic coords="7,38.10,62.28,251.28,505.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>than half of the quantization step. Note that, for simplifying the notation . Using the properties of interval arithmetic<ref type="bibr" target="#b12">[13]</ref> and the fact that , it is possible to derive upper bounds of the quantization effects (23)<ref type="bibr" target="#b23">(24)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Misclassification error due to quantization effects.</figDesc><graphic coords="8,120.48,62.28,352.32,251.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Distribution of data for Models 1a (a) and 1b (b) [D = 2; = 0:2; m = 500(A); m = 32(B)].</figDesc><graphic coords="9,119.10,336.00,352.08,252.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Distribution of data for Models 2a (a) and 2b (b) [D = 0; = 0:2; m = 500(A); m = 32(B)].</figDesc><graphic coords="10,120.72,337.56,351.84,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Convergence of b for (a) m = 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. (Continued) Convergence of b for percentage of test errors during learning for (b) m = 32 and (c) m = 32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,304.98,89.22,246.36,299.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,39.36,89.22,248.52,520.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ALGORITHM 1 :</head><label>I1</label><figDesc>DSVM WITH FIXED BIAS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II ALGORITHM 2 :</head><label>II2</label><figDesc>FIBS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV FLOATING</head><label>IV</label><figDesc>AND FIXED-POINT EXPERIMENTS FOR SONAR (LINEAR KERNEL)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V FLOATING</head><label>V</label><figDesc>AND FIXED-POINT EXPERIMENTS FOR MODEL 1A</figDesc><table /><note><p>( = 0:5; C = 0:05)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI FLOATING</head><label>VI</label><figDesc>AND FIXED-POINT EXPERIMENTS FOR MODEL 1B ( = 0:5; C = 0:09)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII FLOATING</head><label>VII</label><figDesc>AND FIXED-POINT EXPERIMENTS FOR MODEL 2A</figDesc><table /><note><p>( = 0:5; C = 0:02)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII FLOATING</head><label>VIII</label><figDesc>AND FIXED-POINT EXPERIMENTS FOR MODEL 2B ( = 0:5; C = 0:08)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX HDL</head><label>IX</label><figDesc>SYNTHESIS REPORT FOR m = 8 AND m = 32</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors wish to thank two anonymous reviewers for helping to improve both the structure and the content of the paper and pointing out some imprecision in the first draft of this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In practice, a classifier is selected on the basis of previous samples, having the following structure:</p><p>(32)</p><p>Here, we consider the following two different nonlinear models of the channel, that, substantially, differ for the delay .</p><p>• Model 1: . • Model 2:</p><p>. Furthermore, as our aim is to test the behavior of the algorithm and, above all, to study its actual hardware implementation, we consider different number of training patterns. In particular, we choose , as in <ref type="bibr" target="#b22">[23]</ref>, and : this last choice guarantees a good tradeoff between the final generalization ability of the learned model and its device utilization. We call Model 1a, 1b, 2a, 2b, the corresponding distributions. Finally, in order to estimate the generalization error of the obtained SVM, we use a separate test set composed by and samples, respectively. With reference to (31) we consider the following channel:</p><p>(33) that assumes an ISI equal to 2. We choose and . Figs. <ref type="figure">8</ref> and<ref type="figure">9</ref> show the distribution of data obtained by (33) with the given parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Floating-and Fixed-Point Experiments</head><p>When designing a digital architecture one of the most important aspects that must be considered is the length of the word that represents the information inside the design. This parameter has a crucial role because it influences both the length of the registers and, as a consequence, the device utilization and the performance of the digital learning system.</p><p>When we faced the design of Fibs and the design of the corresponding architecture in particular, we needed to understand: 1) its behavior when using a floating-point math with respect to standard SVM learning algorithms, like the SMO; 2) the required number of bits. To obtain these answers, we designed several experiments, both on the Sonar dataset, using a linear kernel, and on models 1a, 1b, 2a, and 2b.</p><p>The results of our first experiment on the Sonar dataset are reported in Table <ref type="table">IV</ref>. In this table we report, for every kind of architecture, the number of support vectors (nsv), the value of the threshold , the value of the quantity (eq.), the number of training errors (TR), and, finally, the number of test errrors (TS). As expected, the floating-point (FP) version of our algorithm obtains good results with respect to the SMO algorithm. Note that we set and . Table <ref type="table">IV</ref> shows both floating-and fixed-point results for different register lengths. The results of the fixed-point experiments are reported in the second part of the table, where, with the notation , the following information is indicated: the number of bits used to code each , the number of bits used to code the integer , and the fractional part of each and . From the observation of the results three main important properties emerge: 1) Fibs requires a relatively low number of bits, especially to code the kernel matrix; 2) the accuracy obtained on the equality constraint is not very critical, but, above all; 3) the quantization effect of the kernel matrix is a benefit for the generalization capability. These results are also confirmed by the experiments on the models generated from the telecommunication problem, as reported in Tables V-VIII. In particular, the values reported in the tables clearly indicate that very few bits can be used to code each , and that a coding configuration, which outperforms the FP solution, can often be found. Note that we used a RBF-SVM with for both models and</p><p>(for Model 1a), (for Model 1b), (for Model 2a) and (for Model 2b). To complete our analysis, we compared our results with the ones reported in <ref type="bibr" target="#b22">[23]</ref>. The results of our algorithm outperform the ones reported there, obtained with polynomial kernels, even by using only 32 samples. In fact, whereas <ref type="bibr" target="#b22">[23]</ref> reaches an accuracy of 4.2% on the test of Model 1a, we reached 3.6% with Model 1a and 4.2% with Model 1b. Similar results are obtained with Model 2: case 2a improves on <ref type="bibr" target="#b22">[23]</ref> with an accuracy of 16%; case 2b, instead, is worse, as we measured an accuracy of 23.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. FPGA Implementation, Functional Simulations, and Synthesis Analysis</head><p>The digital architectures described here have been implemented and tested on one of the most powerful Xilinx FPGAs, the Virtex-II, particularly suited to embed high-performance digital signal processors <ref type="bibr" target="#b23">[24]</ref>. We chose, as target device, the , characterized by 8M system gates, an array of 112 104 configuration logic blocks (CLBs) providing 46 592 Slices (1 CLB = 4 Slices) and a maximum of 1456 Kbits distibuted RAM. Furthermore, it provides 168 18 18 multiplier blocks and 168 18-Kbit selected-RAM blocks for an amount of 3024 Kbits RAM. Note that the device is very large for current standards and, as detailed in the rest of this paper, only a small amount of hardware is actually used by our architecture.</p><p>In order to study the main properties of our design, such as the device utilization, we performed several experiments, at first by choosing a small number of patterns , and then by choosing a more realistic size, that is , discussed also, from the generalization point of view, in Section V-C. By using a VHDL description, we could parameterize our design and change the size from to without any particular effort, thus allowing an efficient study of the implementation properties for different training set sizes.</p><p>[25] S. McBader, L. Clementel, A. Sartori, A. Boni, and P. Lee, "Softtotem:</p><p>An FPGA implementation of the totem parallel processor," presented at the 12th Int. Conf. Field Programmable Logic Application, France, 2002.</p><p>Davide Anguita (S'93-M'93) received the Laurea degree in electronic engineering in 1989 and the Ph.D. degree in computer science and electronic engineering from the University of Genova, Genoa, Italy, in 1993.</p><p>After working as a Research Associate at the International Computer Science Institute, Berkeley, CA, on special-purpose processors for neurocomputing, he joined the Department of Biophysical and Electronic Engineering, University of Genova, where he teaches digital electronics, programmable logic devices, and smart electronic systems. His current research focuses on industrial applications of artificial neural networks and kernel methods and their implementation on electronic devices.</p><p>Andrea Boni received the Laurea degree in electronic engineering in 1996 and the Ph.D. degree in computer science and electronic engineering from the University of Genova, Genoa, Italy, in 2000.</p><p>After working as a Research Consultant with the Department of Biophysical and Electronic Engineering, University of Genova, he joined the Department of Information and Communication Technologies, University of Trento, Trento, Italy, where he teaches digital electronics and adaptive electronic systems. His main scientific interests focus on the study and development of digital circuits for advanced information processing.</p><p>Sandro Ridella (M'93) received the Laurea degree in electronic engineering from the University of Genova, Genoa, Italy, in 1966.</p><p>He is a Full Professor in the Department of Biophysical and Electronic Engineering, University of Genova, Italy, where he teaches inductive learning and statistics and optimization methods. In the last ten years, his scientific activity has been mainly focused in the field of neural networks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Networks in Hardware: Architectures, Products and Applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Lindsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Roy. Inst. Technol. On-Line Lectures</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Stockolm, Sweden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analog VLSI and Neural Systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analog vs. digital: Extrapolations from electronics to neurobiology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sarpeshkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1601" to="1638" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neurocomputers: A dead end?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Omondi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="475" to="482" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ULSI architectures for artificial neural networks</title>
		<author>
			<persName><forename type="first">U</forename><surname>Rückert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro Mag</title>
		<imprint>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="2002-06">May-June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning With Kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Asymptotic convergence of an SMO algorithm without any assumption</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="248" to="250" />
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feasible direction decomposition algorithms for training support vector machines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="315" to="350" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Worst case analysis of weight inaccuracy effects in multilayer perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ridella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rovetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="415" to="418" />
			<date type="published" when="1999-03">Mar. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analysis of the effects of quantization in multilayer neural networks using a statistical model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jabri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="334" to="338" />
			<date type="published" when="1992-03">Mar. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Introduction to Interval Computation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herzberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Circuital implementation of support vector machines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ridella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rovetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1596" to="1597" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved neural network for SVM learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1243" to="1244" />
			<date type="published" when="2002-09">Sept. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convergence analysis of a discrete-time recurrent neural network to perform quadratic real optimization with bound constraints</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Perez-Ilzarbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1344" to="1351" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning algorithm for nonlinear support vector machines suited for digital VLSI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ridella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental learning and selective sampling via parametric optimization framework for SVM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convergence of a generalized SMO algorithm for SVM classifier design</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="351" to="360" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Characterization of the sonar signals benchmark</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive equalization of finite nonlinear channels using multilayer perceptrons, signal processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F N</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="119" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Support vector machine techniques for nonlinear equalization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sebald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bucklew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="3217" to="3226" />
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m">Virtex II Platform FPGA Handbook (ver. 1.3)</title>
		<imprint>
			<publisher>Xilinx</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
