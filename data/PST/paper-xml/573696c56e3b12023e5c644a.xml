<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-and Content-aware Embeddings for Query Rewriting in Sponsored Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
							<email>mihajlo@yahoo-inc.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>701 First Ave</addrLine>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
							<email>nemanja@yahoo-inc.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>701 First Ave</addrLine>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
							<email>vladan@yahoo-inc.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>701 First Ave</addrLine>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
							<email>silvestr@yahoo-inc.com</email>
							<affiliation key="aff1">
								<address>
									<addrLine>125 Shaftesbury Ave</addrLine>
									<settlement>London</settlement>
									<country key="GB">England</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
							<email>narayanb@yahoo-inc.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>701 First Ave</addrLine>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yahoo</forename><surname>Labs</surname></persName>
						</author>
						<title level="a" type="main">Context-and Content-aware Embeddings for Query Rewriting in Sponsored Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D4006FF8155E9F2343B4A39BAF5657F</idno>
					<idno type="DOI">10.1145/2766462.2767709</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval Information Retrieval</term>
					<term>Query Rewriting</term>
					<term>Algorithms Query rewriting</term>
					<term>word embeddings</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Search engines represent one of the most popular web services, visited by more than 85% of internet users on a daily basis. Advertisers are interested in making use of this vast business potential, as very clear intent signal communicated through an issued query allows effective targeting of users. This idea is embodied in a sponsored search model, where each advertiser maintains a list of keywords they deem indicative of increased user response rate with regards to their business. According to this targeting model, when a query is issued all advertisers with a matching keyword are entered into an auction according to the amount they bid for the query and the winner gets to show their ad. One of the main challenges is the fact that a query may not match many keywords, resulting in lower auction value, lower ad quality, and lost revenue for advertisers and publishers. Possible solution is to expand a query into a set of related queries and use them to increase the number of matched ads, called query rewriting. To this end, we propose rewriting method based on a novel query embedding algorithm, which jointly models query content as well as its context within a search session. As a result, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via simple K-nearest neighbor search. The method was trained on more than 12 billion sessions, one of the largest corpus reported thus far, and evaluated on both public TREC data set and an in-house sponsored search data set. The results show that the proposed approach significantly outperformed existing state-of-the-art, strongly indicating its benefits and monetization potential.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years targeted advertising has become one of the largest and most lucrative advertising channels. Despite the fact that traditional offline advertising still accounts for the majority of advertising expenditures <ref type="bibr" target="#b34">[34]</ref>, future potential of this burgeoning field is clearly exemplified by reported ad revenue of over 20 billion dollars in the first half of 2013 in the US alone, combined with a remarkable growth of around 20% on a yearly basis <ref type="bibr" target="#b21">[21]</ref>. The size and importance of the online advertising, as well as the interesting open questions that the scale and variety of the targeting tasks bring, has drawn attention of many researchers from both industry and academia, resulting in a number of novel methods and improvements to the flourishing field <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b32">32]</ref>.</p><p>Due to a large diversity of the internet medium, targeted advertising has evolved to encompass many different outlets for the advertisers interested in reaching their target audience. These include behavioral targeting <ref type="bibr" target="#b12">[12]</ref> (where users are targeted based on their general browsing behavior), email retargeting <ref type="bibr" target="#b18">[18]</ref> (targeting users based on their e-mail interaction patterns), site re-targeting (targeting users based on their historical search queries), to name a few. In this work we consider a task of sponsored search advertising <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b22">22]</ref>, a very popular advertising model that targets users with the most relevant ads by considering an immediate query issued by a user. It has become one of the prevalent means of advertising due to a fact that the current query carries a strong signal about an immediate intent of the user, resulting in a highly effective targeting model <ref type="bibr" target="#b15">[15]</ref>.</p><p>In the sponsored search model each advertiser maintains a list of keywords that they deem relevant to their product (e.g., Nike may maintain a keyword list containing terms such as "running shoes" and "athletics"). In addition to a list of keywords, each advertiser also specifies a monetary bid amount they are willing to pay if their ad is shown and clicked by the user. Then, when a user issues a query in a search engine, the query is compared against each advertisers' keyword list, and all advertisers with the matching keyword are entered into an auction. Finally, according to advertiser's bid amount and the estimated quality and rel-evance of an ad that they wish to show, one advertiser and their corresponding ad are chosen for user targeting. Value of the auction increases when number of matched advertisers and their bids are high, which directly results in a better ad quality and higher revenue for both advertisers and publishers (i.e., websites that host the ads).</p><p>Due to a large number of queries that the users could possibly issue, a common occurrence is that an exact query match cannot be found, as the advertisers usually cannot cover all relevant queries related to their product or service. However, even when the exact match does not exist, most often there does exist a non-matching keyword that is still highly related to the query. For example, query "purina one" and bidded keyword "dog food" are strongly related yet a string match would fail to make the connection, which directly translates into lost revenue. To mitigate this problem, query rewriting is used to expand the original query by providing K related ones for which ads are available, and which can be used instead to qualify advertisers for an auction <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b40">40</ref>]. In the above example, a query rewriting method can be used to rewrite "purina one" into related queries such as "dog food", "cat food", "purina pro plan", and others, thus increasing likelihood of retrieving more high-quality, relevant ads, likely to be clicked by a user that issued the query.</p><p>In this paper we address this critical step in sponsored search, and propose a novel query rewriting algorithm motivated by recent advances in distributed neural language models <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref>. We explore and expand upon these approaches for the task of query rewriting, resulting in significant performance improvements over the current state-ofthe-art methods. Key contributions are summarized below:</p><p>• We describe an application of distributed language models to query rewriting, where we propose to use three novel methods for learning distributed, lowdimensional query representations that compactly capture their semantic meaning;</p><p>• We propose and discuss how to add ad clicks and search result clicks to query context, which allows specialization of representations for various tasks of critical interest to search engine companies (e.g., query suggestion, query-to-ad matching, query categorization), indicating even wider applicability of the method in the advertising domain;</p><p>• We trained and evaluated the models using more than 12 billion search sessions, resulting in rewriting results of high quality. Empirical results show that the proposed approach significantly outperform the existing state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In this section we describe related work in the domains of query rewriting and neural language modeling that motivated the approach proposed in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query rewriting for sponsored search</head><p>Owing to the importance of query rewriting for the success of query-ad auctions, various algorithms have been proposed in the literature to address this critical step. These include graph-based methods such as Query Flow Graph (QFG) <ref type="bibr" target="#b6">[6,</ref><ref type="bibr">7]</ref> that learn from users' browsing behavior, as well as methods that exploit syntactical relationships between queries <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b23">23]</ref>. However, disadvantage of existing solutions is that they mostly do not take into account complex semantic relationships between queries, which may lead to suboptimal rewrites. For example, the rewrites typically result in the original queries with added or removed terms, such as "purina one" being rewritten as "purina" or "purina one pro". Thus, they are often obvious and have already been considered by the advertisers that targeted the original query, and do not add a significant value to the auction.</p><p>We note that query rewriting task is related to the problem of query suggestion, albeit the goals of the two techniques are quite different. In the case of query suggestion the objective is to provide users with queries that are important for meeting users' information needs <ref type="bibr" target="#b2">[2,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b39">39]</ref>, while in query rewriting the task is to expand query to increase both the number and quality of retrieved ads. We refer an interested reader to <ref type="bibr" target="#b36">[36]</ref> for an overview of query log mining approaches as applied to query suggestion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural language models</head><p>In a number of natural language processing (NLP) applications, including information retrieval, part-of-speech tagging, chunking, and many others, the specific objective can be generalized to the task of assigning a probability value to a sequence of words. To this end, language models have been developed, defining a mathematical model to capture statistical properties of words and the dependancies among them <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b27">27]</ref>. Traditionally, language models represent each word as a feature vector using one-hot representation, where a word vector has the same length as the size of a vocabulary, and a vector element that corresponds to the observed word is equal to 1, and 0 otherwise. However, this approach often exhibits significant limitations in practical tasks, suffering from high dimensionality of the problem and severe data sparsity, resulting in suboptimal performance.</p><p>Neural language models have been proposed to address these issues, inducing low-dimensional, distributed embeddings of words by means of neural networks <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b38">38]</ref>. Such approaches take advantage of the word order in text documents, explicitly modeling the assumption that closer words in the word sequence are statistically more dependent. Historically, inefficient training of the neural network-based models has been an obstacle to their wider applicability, being proportional to the size of the vocabulary which may grow to several millions in practical tasks. However, this issue has been successfully addressed with recent advances in the field, particularly with the development of highly scalable continuous bag-of-words (CBOW) and skip-gram (SG) language models <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref> for learning word representations. These powerful, efficient models have shown very promising results in capturing both syntactic and semantic relationships between words in large-scale text corpora, obtaining state-of-the-art results on a plethora of NLP tasks. More recently, the concept of distributed representations has been extended beyond word representations to sentences and paragraphs <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b28">28]</ref>, relational entities <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b37">37]</ref>, general textbased attributes <ref type="bibr" target="#b26">[26]</ref>, descriptive text of images <ref type="bibr" target="#b25">[25]</ref>, nodes in graph structure <ref type="bibr" target="#b33">[33]</ref>, and other applications going beyond NLP domain for which they were originally proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED APPROACH</head><p>To address the shortcomings of the existing state-of-theart methods for query rewriting, we propose to take a rad-ically new approach to this task, motivated by the recent success of distributed language models in NLP applications <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b38">38]</ref>. In the context of NLP, distributed models are able to learn word representations in a low-dimensional continuous vector space using a surrounding context of the word in a sentence, where in the resulting embedding space semantically similar words are close to each other <ref type="bibr" target="#b31">[31]</ref>. Our objective is to take advantage of this property for the task of query rewriting, and to learn query representations in a lowdimensional space where semantically similar queries would be close. As a result, and in contrast to rewriting methods commonly used in practice, related queries could have a high similarity score even if they do not have any shared terms. Clearly, such approach would allow us to reduce complex task of query rewriting to a trivial K-nearest-neighbor search in the new embedding space.</p><p>However, application of distributed language models to the task of query rewriting is not an easy endeavour. Finding distributed query representation, as opposed to finding word representations, brings very unique challenges quite different from those found in everyday NLP problems. First, there are significant differences between language used in everyday language and web searches <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b35">35]</ref>. For example, web search users often use summarization when searching for content (e.g., typing "vacations Spain" instead of "vacations in Spain"), thus omitting frequent words. Further, some ngrams that rarely appear in everyday language often appear together in web search <ref type="bibr" target="#b24">[24]</ref>, and queries are characterized by more spelling mistakes. Moreover, contrary to everyday language where linguistic rules and notions of words and sentences are clearly defined, search queries are composed of terms where there is no existing notion of "sentence of queries" or the surrounding context equivalent to natural language domain.</p><p>In this paper we address these issues, and propose three query rewriting methods that bring the state-of-the-art distributed language models closer to the setting of sponsored search: 1) context2vec, where we exploit the fact that user query is recorded with a timestamp, from which we create "query sentences" and apply state-of-the-art language model <ref type="bibr" target="#b31">[31]</ref>; 2) content2vec, where we propose to learn distributed query vector representations from its content without considering session information, which is equivalent to para-graph2vec method from <ref type="bibr" target="#b28">[28]</ref>; and 3) context-content2vec, where we propose to use a novel two-level architecture <ref type="bibr" target="#b14">[14]</ref> that jointly models content of queries (i.e., word tokens that form the query) along with the query context (defined as other temporally close queries within a search session). Empirical results suggest that the resulting rewrites are highly related to the original queries, while being more diverse than rewrites produced by the current state-of-the-art methods. In addition, using a large data base of query-bids collected for thousands of advertisers, we show that the rewrites produced by the proposed method match the highest percentage of bidded queries, providing a strong positive impact on the overall revenue of both publishers and advertisers.</p><p>In addition to search query logs, search engines systematically log a number of other valuable signals that can help better explain and model the user intent. For example, ad clicks and clicks on search results are also recorded, and may provide a supplementary intent signal that can be used to improve query representations. To help exploit these two valuable sources of information, we propose to incorporate both ad clicks and search result clicks into user query sessions, and show how the additional signals are seamlessly handled by the proposed models. The benefits of including the ad and search clicks for query rewriting task are clearly confirmed by the empirical results, showing increased relevance of query rewrites and higher coverage of advertisers' bid terms. Moreover, in addition to improving query representations, in this way we also learn low-dimensional representations of ad clicks and search clicks in the same embedding space, which opens doors for using the proposed methods on a number of important online tasks, such as query-to-ad matching, query suggestion, and query categorization, to name a few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">METHODOLOGY</head><p>In this section we describe the proposed methodology for the task of query rewriting in sponsored search. As discussed earlier, the goal is to find queries related to the issued one, which would allow us to retrieve relevant ads that were not matched by the original query. To solve this problem, we propose to learn representation of queries in lowdimensional space from historical search logs using neural language models. Query rewriting can then be performed by finding K nearest neighbors of the issued query in the learned embedding space.</p><p>More specifically, given a set S of S search sessions obtained from online users, where each session s = (q1, . . . , qM s ) ∈ S is defined as an uninterupted sequence of Ms queries, and each query qm = (wm1, wm2, . . . wmT m ) consists of Tm words, our objective is to find D-dimensional real-valued representation vq m ∈ R D of each query qm such that semantically similar queries lie nearby in the new space.</p><p>We propose three approaches for learning query representations that address specifics of the web search environment. We first propose context2vec and content2vec methods that consider query context and query content, respectively, motivated by previous work on learning word representations from news articles <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>. We then detail context-content2vec, a two-level architecture for joint modeling of both query context and query content, which results in better, more useful query representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model 1: context2vec</head><p>The context2vec model involves learning low-dimensional representations of queries from search logs by using a notion of a search session as a "sentence" and queries within the session as "words", borrowing the terminology from NLP domain (see Figure <ref type="figure" target="#fig_0">1a</ref>). More specifically, context2vec learns query representations using the skip-gram model <ref type="bibr" target="#b31">[31]</ref> by maximizing the objective function over the entire set S of search sessions, defined as L = s∈S qm∈s -b≤i≤b,i =0 log P(qm+i|qm).</p><p>(4.1)</p><p>Probability P(qm+i|qm) of observing a neighboring query qm+i given the current query qm is defined using soft-max,</p><formula xml:id="formula_0">P(qm+i|qm) = exp(v qm v q m+i ) V q=1 exp(v qm v q ) ,<label>(4.2)</label></formula><p>where vq and v q are the input and output vector representations of query q, b is defined as length of the context for query 2) we can see that context2vec models temporal context of query sequences, where queries with similar contexts (i.e., with similar neighboring queries) will have similar vector representations in the projected semantic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model 2: content2vec</head><p>We propose content2vec method to simultaneously learn vector representations of queries and the words contained within them, motivated by the paragraph2vec algorithm <ref type="bibr" target="#b28">[28]</ref>. The content2vec architecture is illustrated in Figure <ref type="figure" target="#fig_0">1b</ref>. Training data set was derived from user search logs by disregarding the query timestamps, and consisted of queries qm and their containing words qm = (wm1, wm2, . . . wmT m ). During training, query vectors are learned so they predict the words in their content, while word vectors are learned so they predict their context words within the query. More specifically, objective of content2vec is to maximize the loglikelihood over the set S of all query sessions, L = </p><p>where c is length of the context for words within the query. The probability P(wmt|wm,t-c : wm,t+c, qm) is defined using a soft-max function,</p><formula xml:id="formula_2">P(wmt|wm,t-c : wm,t+c, qm) = exp(v v w mt ) V w=1 exp(v v w ) ,<label>(4.4)</label></formula><p>where v w mt is the output vector representation of wmt, and v is averaged vector representation of the context including corresponding qm, defined as</p><formula xml:id="formula_3">v = 1 2c + 1 (vq m + -c≤j≤c,j =0 vw m,t+j ),<label>(4.5)</label></formula><p>where vw is the input vector representation of w. Similarly, the probability P(qm|wm1 : wmT m ) is defined as</p><formula xml:id="formula_4">P(qm|wm1 : wmT m ) = exp(v m v qm ) V w=1 exp(v m v w ) ,<label>(4.6)</label></formula><p>where v qm is the output vector representation of qm, and vm is averaged input vector representation of all the words </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model 3: context-content2vec</head><p>Both context2vec and content2vec models have limited modeling power in the light of the available data. This is due to the fact that they are not capable of exploiting all the available query log information as they model only one aspect of the data at hand (i.e., either make use of the context of queries in search sessions or their content, respectively). To overcome this limitation, we propose a two-layer context-content2vec <ref type="bibr" target="#b14">[14]</ref> specifically tailored for the purpose of modeling query logs. The two-layer architecture of the proposed model is illustrated in Figure <ref type="figure" target="#fig_0">1c</ref>. The upper layer models the temporal context of query sequences in search sessions, based on the assumption that temporally closer queries are statistically more dependent. On the other hand, the bottom layer models the content information of word sequences found within queries.</p><p>More formally, given S sessions of queries together with their content, objective of the two-layer context-content2vec model is to maximize the log-likelihood of the training data, L = s∈S qm∈s -b≤i≤b,i =0 log P(qm+i|qm) + αm log P(qm|wm1 : wmT m ) + w mt ∈qm log P(wmt|wm,t-c : wm,t+c, qm) , <ref type="bibr">(4.8)</ref> where α weights are hyperparameters that trade off between minimization of the log-likelihood of query sequences (i.e., query context) and the log-likelihood of word sequences (i.e., query content). Denoting frequency of the m th query as Km, we set the hyperparameters as αm = 1 log(1+Km) , such that rarely seen queries rely more on content and frequently seen queries more on context.</p><p>As can be observed, context-content2vec model combines the two earlier models, using the context2vec model in the upper layer and the content2vec model in the lower layer. Note that the probabilities from equation (4.8) are defined in equations (4.2), (4.4), and (4.6). In order to learn from both contextual and content information, the training data set is a union of context and content training data sets defined previously in sections 4.1 and 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training and inference</head><p>The models are optimized using stochastic gradient ascent, suitable for large-scale problems. However, computation of gradients ∇L in (4.1), (4.3), and (4.8) is proportional to the vocabulary size V , which is computationally expensive in practical tasks as V could easily reach hundreds of millions. As an alternative, we used negative sampling approach proposed in <ref type="bibr" target="#b31">[31]</ref>, which significantly reduces the computational complexity.</p><p>When the vector representations of all queries are found, we can perform query rewriting in a straightforward manner. For a given user query q, we generate K query rewrites using K nearest neighbors (K-NN) of query q in the lowdimensional space with respect to a cosine distance <ref type="bibr" target="#b31">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section we describe the considered data set and give empirical evaluation of the proposed rewriting methods. We learned embeddings for more than 45 million queries, using one of the largest search data sets reported so far, comprising over 12 billion sessions collected on the US website of Yahoo Search. The vectors were used to produce query rewrites, evaluated in terms of relevance and ad coverage and compared to the state-of-the-art methods from the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training data sets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Query content data</head><p>For purposes of learning query embeddings from query content, we took 45 million most frequent queries and formed dataset Dcontent = {(qm, wm1, wm2, . . . , wmT m )}, where qm is the m-th query and (wm1, wm2, . . . wmT m ) are the words contained within qm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Search session data</head><p>Search sessions are defined as uninterrupted sequences of web search activity. Following <ref type="bibr" target="#b16">[16]</ref>, the session ends when a user is inactive for more than 30 minutes. A new session is initiated with the following search query.</p><p>To be able to learn query embeddings from interactions of queries within a search session we created dataset Dcontext = {si, i = 1, ..., S}, derived by sessionizing user search logs data into sessions si, that were in turn represented as a set of queries ordered in time, si = (qi1, qi2, ..., qiM i ). In a case of repetitive queries, such as si = (qi1, qi2, qi3 = qi2, qi4 = qi2, qi5), repetitions were de-duplicated to obtain si = (qi1, qi2, qi5). Finally, search sessions that contained only a single search query were discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Ad clicks and search link clicks</head><p>In a search session, queries are often accompanied by ad clicks and search link clicks. These events can be used as additional context to improve query representations and specialize them for a specific task. For example, while one of the main goals of query rewriting in sponsored search is to produce relevant alternatives to the original query, another important goal is for rewrites to match as many bid terms as possible to increase the auction value. Thus, to produce more commercial query rewrites, we use ad click events within query contexts when learning vector representations.</p><p>For this purpose we extend the session data set by adding ad click events to user sessions, where each ad click is uniquely identified by ad identification number. The data set D ad = {si, i = 1, ..., S} consists of sessions si comprising both search queries and ad clicks, si = (qi1, qi2, ai3, qi4, ai5..., qiM i ), where qim refers to search queries, and aim refer to ad click events. Moreover, to further improve learned query representations we also considered adding search link clicks to the user sessions (i.e., clicks that a user made on links given as search results, also referred to as organic results), motivated by the idea behind QFG approach. To this end, we expanded data set D ad to obtain data set D ad+link = {si, i = 1, ..., S}, formed by further adding link click events lim to search sessions si.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Handling "search stopwords"</head><p>Unlike in everyday language, where the most frequent tokens are articles and prepositions (e.g., a, the, in), in search data the most frequent tokens are navigational queries, such as google, yahoo, facebook, etc. Due to the fact that over a longer period they occur in a direct neighborhood of majority of other queries, there is a risk that the embedding space will shrink. We have empirically observed this phenomenon, and during training the navigational queries tend to pull all other queries towards them, preventing the vectors from spreading further away in the hyperspace.</p><p>The authors of <ref type="bibr" target="#b31">[31]</ref> suggested to deal with the frequent tokens in news articles by downsampling. This heuristic involved discarding words with probability P(wi) = 1τ f (w i ) , where f (wi) is the frequency of word wi, and τ is a user-set parameter. However, this approach is not applicable in a context of web search which poses quite different requirements. In particular, we aim to learn good representations of navigational queries while at the same time prevent them from adversely influencing representation learning for non-navigational queries. To achieve this objective, we propose a one-direction learning rule for navigational queries. In particular, their vectors are being updated by vectors of queries appearing in the context, but are not used to update the vectors of other queries in their context. We identified navigational queries editorially by evaluating the most frequent 3,000 queries for navigational intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Training parameters</head><p>Models were trained using a cluster of 9 machines with 256GB of RAM memory and 24 cores. Dimensionality of the  embedding space was set to D = 300, context neighborhood size was set to 5 and content neighborhood size was set to 7. Finally, we used negative sampling to speed up the training, and used 10 random samples in each vector update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Query rewriting models</head><p>We considered the following approaches in our empirical analysis, where each resulted in vector representation for 45 million most frequent queries.</p><p>1) word2vecnews model was used as a simple baseline. Query vectors were constructed by summing publicly available word vectors for word tokens in a query (whitespace was used as a token separator), trained on Google News data set with English stopwords removed 1 .</p><p>2) word2vec search query vectors constructed by summing word vectors for word tokens in a query, trained using Dcontent data set.</p><p>3) content2vec model (cn2vec) was trained using Dcontent where queries were used as a global context to the containing words, as illustrated in Figure <ref type="figure" target="#fig_0">1b</ref>.</p><p>4) context2vec model (cx2vec) was trained using Dcontext, as illustrated in Figure <ref type="figure" target="#fig_0">1a</ref>.</p><p>5) context-content2vec model (cx-cn2vec) used both Dcontent and Dcontext data to train query vectors, leveraging the two-layer architecture from Section 4.3. Since the model learns from both content and context, one of the motivations behind cx-cn2vec is to improve embeddings for queries that were not seen in many sessions. In Table <ref type="table" target="#tab_0">1</ref> we give several illustrative examples of such tail queries. Unlike cx2vec, cx-cn2vec relies more on content in case of rare queries, and thus provides better rewrites. For example, at the time of creation of our data set the query "iphone 6 repair service" was a tail query, resulting in poor cx2vec rewrites such as "mp3attic music" or "social security disability bronx ny". On the other hand, cx-cn2vec provided more relevant rewrites. As illustrated in Table <ref type="table" target="#tab_1">2</ref>, we can see that addition of D ad resulted in quite different query rewrites than using a data set without ad clicks. For example, rewrites for query "makeup" mainly contain terms that are related to tips, tutorials, and pictures when no ads were used in training. Conversely, rewrites for the same query when clicks on ads are considered contain more commercial search terms.</p><p>In addition, given ad and query vectors in the same embedding space, we can easily retrieve similar queries for any given ad. This by-product is extremely useful for suggesting new bid keywords for specific categories of ads (note that in our system ads are categorized into one or more interest categories, such as "sports" or "travel"). As an example, in Figure <ref type="figure" target="#fig_3">2</ref> we show K = 5,000 most similar queries to ads in "automotive" and "air travel" categories. Keywords with higher cosine similarity are shown with larger font sizes. We can observe that the key concepts of the category are well captured with the most similar queries. To produce rewrites for out-of-dictionary queries, embedding models generated their vectors by summing the existing vectors of word tokens within queries (excluding stopwords). Unlike the embedding methods, QFG could not produce rewrites for queries that were not seen in the graph.</p><p>Our evaluation did not include topic models such as LDA <ref type="bibr" target="#b5">[5]</ref> or PLSA <ref type="bibr" target="#b19">[19]</ref>, as earlier research <ref type="bibr" target="#b20">[20]</ref> found that these methods perform poorly on short text documents.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation</head><p>In the following two sections we show our main experimental results, where we report performance of the competing approaches in terms of relevance and ad coverage of query rewrites. We also provide examples of rewrites by cx-cn2vec ad+link method in an online video 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Relevance</head><p>We used editorial judgments of query rewrites to compare query rewriting methods in terms of relevance.</p><p>In-house data. The first data set we used is an inhouse data set consisting of the query and several rewrites produced by current production system that were graded editorially. The editors were instructed to assign the following grades: bad, fair, good, and excellent. In total, the data set includes more than 40,000 (query, rewrite, grade) tuples, such as ("nfl news", "latest nfl news", "excellent"), ("nfl news", "nfl shooting", "fair"), ("nfl news", "nfl rumor", "good"), ("metro transit", "metro pcs", "bad").</p><p>Given a query we ranked the rewrite candidates based on a model's output score and retrieved the top K candidates. Next, we computed the NDCG metric using editorial grades of rewrites as labels (0 for bad, 1 for fair, 2 for good, and 3 for excellent) at values of K ranging from 1 to 20. In Figure <ref type="figure" target="#fig_6">3</ref> we report the results for different models.</p><p>2 http://youtu.be/pvfFQSCYhqI By considering the reported results, several conclusions can be drawn. Regarding models that did not utilize search session data (i.e., word2vecnews, word2vec search , and cn2vec), we can see that the word embeddings specifically tailored for search queries perform better than using word embeddings learned using news data. In addition, learning query vectors as global context of words using cn2vec leads to slightly better results than directly summing the words. However, all three models performed worse than the QFG ad+link baseline method, which made use of the co-occurrence of queries, ads, and links in search sessions. Query embeddings trained directly on search sessions, i.e. cx2vec, already outperform QFG ad+link . Further improvements were observed when embeddings were learned from both context and content. Finally, incorporating ad and link click events showed incremental boost in relevance. The largest gain was observed when links were added as an additional context.</p><p>In addition, in Table <ref type="table" target="#tab_2">3</ref> we report average similarities of (query, rewrite) pairs in each editorial grade group. It is important to note that scores are comparable only within the same method and not across methods. For conclusive comparison we calculated the level of separation between the groups by p-value of t-test, which tests the hypothesis that the means of two neighboring grades are equal. By comparing p-values reported we can quantify which method does the best job of separating the four grade groups. Findings are similar to the ones from Figure <ref type="figure" target="#fig_6">3</ref>. Additionally, we find that QFG ad+link method has issues with "bad" grade group, and that without incorporating search context, the embedding models have similar average scores in "good" and "fair" grade groups. As expected, separation improves when embedding models incorporate search context, with the standard deviation between the groups reducing even further when ads and links were considered as an additional context.</p><p>In the case when one or both queries from the editorial grade list were not found in a model, we generated the vectors by summing vectors of query tokens. Bottom part of Table <ref type="table" target="#tab_2">3</ref> shows results for such cases. We can observe that similar conclusion hold even for such out-of-dictionary queries.</p><p>TREC data. The second data we used was a publicly available TREC Web Track data set<ref type="foot" target="#foot_0">3</ref> from 2009 to 2013, containing a total of 250 queries. Using the competing methods we produced 5 rewrites for each query, and evaluated the result editorially. The editors were given instructions to rate the rewrites in the following way: grade 0 if the rewrite is irrelevant, grade 1 if relevant, and grade 2 if it is excellent. Editorial grades for each method as well as the Levenshtein distance <ref type="bibr" target="#b29">[29]</ref> were averaged and reported in Table <ref type="table" target="#tab_4">5</ref>. We can see that the cx-cn2vec ad+link query embedding method, learned from query content and search session context including ad and link clicks, returned the most relevant rewrites and outperformed the baseline QFG method. Once more, learning query embeddings from content of queries on its own was not enough to outperform QFG. Interestingly, there was just a small difference between cx2vec and cx-cn2vec models. It can be explained by the fact that cx-cn2vec generally improves rewrites for tail queries, while TREC data set mostly consists of frequent queries.</p><p>In addition to providing highly relevant rewrites, cx-cn2vec ad+link showed the highest diversity, a favorable property for rewrite algorithms. Considering the performance in terms of Levenshtein distance, we can observe that rewrites produced by models which learned from content are less diverse than the ones produced by approaches that modeled query context during training.</p><p>Examples of query rewrites obtained by the competing methods are given in Table <ref type="table" target="#tab_3">4</ref>. As can be seen, there exist significant differences between the rewrites of the competing approaches. As discussed earlier, the content-based model cn2vec is sensitive to cases when the query words appeared in different contexts across the query dictionary. For example, words "budget" and "calculator" from the query "wedding budget calculator" mostly appeared in more general financial contexts. For this reason in the first 5 rewrites we have queries that are not related to wedding. The model that utilized both content and context data cx-cn2vec outputs highly relevant, interesting rewrites that capture a number of meanings and contexts of the original query. The model that considered ads during training cx-cn2vec ad produced more commercial rewrites, referencing stores and sales. We also bolded queries that matched the actual bidterms in the system, discussed in more detail in the following experiment. Examples of query rewrites produced by cx-cn2vec ad+link are shown in an online video 4 . The video mostly covers queries from TREC 2010 data set. 4 http://youtu.be/n5kHKyKQAa8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Ad coverage</head><p>In the previous section we showed the advantage of proposed methods in terms of relevance. In this section we evaluate how beneficial those rewrites are from the ad matching perspective. It is of fundamental interest to sponsored search to ensure that the rewrites being provided as alternatives to the users' queries match as many additional relevant bidterms as possible. The percentage of rewrites that match bid terms is defined as coverage. We conducted an off-line experiment to compare query rewriting methods: 1) on an in-house dataset of 2,000 editorially selected queries which the editors deemed representative of the query set; and 2) on the TREC data set. For each query in the data sets we generated K = 5 rewrites and look-up bidterms in our current sponsored search demand. We report average coverage (relative improvement over QFG) over the entire set of queries for each of the two data sets. Table <ref type="table" target="#tab_5">6</ref> summarizes the results of our analysis.</p><p>To estimate monetary value of each query rewriting method we used two proxy measures. First, for the query rewrites produced by each method we look-up the bid amounts for the queries that were matched in the bidterm database. We report the total sum of these amounts, and refer to this metric as "revenue potential". In addition, for each rewrite method we calculated effective cost per mille (eCPM). Given a query rewrite q, and an ad a for which q was a bidterm, we calculated eCPM value eqa by multi- plying cost per click (CPC) dollar amount and the clickthrough rate (CTR) of that (query, ad) pair, computed as number of ad clicks divided by number of ad impressions. Finally, we report the weighted sum of eCPM's, q a wqeqa, where the weight wq is proportional to the number of times the query appeared in the search logs. Considering the results presented in Table <ref type="table" target="#tab_5">6</ref>, we can see that query embedding models trained using news documents and query content achieved lower average coverage than QFG method. However, QFG was in turn outperformed by the coverage of rewrites produced using query embedding approaches. Moreover, by taking the results from both experiments into account, it can be concluded that cx-cn2vec ads+links is the best choice as it achieves the highest relevance while maintaining large ad coverage. When link clicks were added on top of ad clicks we observed a large improvement in relevance, with ad coverage remaining almost the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper we described novel query rewriting methods based on recently proposed neural language models. The methods learn low-dimensional, distributed representations of search queries based on context (context2vec), content (content2vec), or combined context and content (context-content2vec) of the queries within search sessions. To specialize the query rewrites for sponsored search application, we further incorporated ad clicks and search link clicks into the training data. We evaluated the proposed methods using both in-house and publicly available TREC data sets. When compared to the current state-of-the-art approaches, we showed that context-content2vec generates the most relevant query rewrites, while at the same time maintains high level of ad coverage. The results clearly indicate significant advantages of context-content2vec over the state-of-the-art query rewrite algorithms, and suggest high monetization potential of the query embedding approach to the task of sponsored search advertising. In our ongoing work, we plan to experiment with more involved search sessionization algorithms and navigational query detection algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Query embedding models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>s∈S qm∈s log P(qm|wm1 : wmT m ) + w mt ∈qm log P(wmt|wm,t-c : wm,t+c, qm) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of most similar queries to select ads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1</head><label></label><figDesc>https://code.google.com/p/word2vec 6) cx-cn2vec ad model was trained using Dcontext and D ad (i.e., context data with ad clicks added). Training resulted in additional 8.5 million ad vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 )</head><label>7</label><figDesc>cx-cn2vec ad+link model was trained using Dcontent and D ad+link (i.e., context data with ad clicks and link clicks added). Training resulted in additional 19 million search link vectors and 8.5 million ad vectors. 8) QFG ad+link model was trained using a click-flow graph constructed from D ad+link dataset, with additional 19 million search link vectors and 8.5 million ad vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NDCG@K for different competing methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Differences in query rewrites of context2vec and context-content2vec for tail queries</figDesc><table><row><cell>Query</cell><cell>cx2vec</cell><cell>cx-cn2vec</cell></row><row><cell></cell><cell>satellite tv otego</cell><cell>minnesota insurance</cell></row><row><cell></cell><cell>satellite tv menominee</cell><cell>minnesota insurance license practice exams</cell></row><row><cell cols="2">minnesota insurance exam crossword puzzles satellite tv west end</cell><cell>online insurance exam crossword puzzles</cell></row><row><cell></cell><cell>satellite tv townsend</cell><cell>colorado insurance exam crossword puzzles</cell></row><row><cell></cell><cell>satellite tv lake sara</cell><cell>online minnesota insurance exam crossword puzzles</cell></row><row><cell></cell><cell>staphylococcal enteritis definition</cell><cell>microwave oven food safety</cell></row><row><cell></cell><cell>salmonella enteritis definition</cell><cell>microwave baby food safety</cell></row><row><cell>microwave food safety</cell><cell>listeria monocytogenes prevention</cell><cell>microwave food safety studies</cell></row><row><cell></cell><cell>e coli cdc</cell><cell>microwave food safety issues</cell></row><row><cell></cell><cell>preventing cross contamination</cell><cell>foodsafety.com</cell></row><row><cell></cell><cell>steak sauce substitute</cell><cell>cast iron skillet recipes</cell></row><row><cell></cell><cell>montreal seasoning ingredients</cell><cell>how to cook with cast iron skillet</cell></row><row><cell>what to cook in cast iron skillet</cell><cell>ground turkey breakfast sausage</cell><cell>how to cook in a cast iron skillet</cell></row><row><cell></cell><cell cols="2">how to cook steak on cast iron skillet how to cook with a cast iron skillet</cell></row><row><cell></cell><cell>reseason cast iron</cell><cell>chicken in cast iron skillet</cell></row><row><cell></cell><cell>mp3attic music</cell><cell>at&amp;t iphone repair service</cell></row><row><cell></cell><cell>donar ovulos en elche</cell><cell>iphone 5c repair service</cell></row><row><cell>iphone 6 repair services</cell><cell>credit a la consommation rapide</cell><cell>iphone repair</cell></row><row><cell></cell><cell>smart phone repair service</cell><cell>iphone service repair</cell></row><row><cell></cell><cell>social security disability bronx ny</cell><cell>iphone repair services</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Examples of query rewrites with and without ad clicks in training data (bolded rewrites matched bid-terms)</figDesc><table><row><cell cols="6">makeup (no ads) makeup (with ads) snowboarding (no ads) snowboarding (with ads) seafood (no ads) seafood (with ads)</cell></row><row><cell>makeup tips</cell><cell>lipstick</cell><cell>snowbaording</cell><cell>snowboards</cell><cell>sea food</cell><cell>seafood restaurant</cell></row><row><cell>fashion makeup</cell><cell>mac makeup</cell><cell>snow boarding</cell><cell>snowboarding gear</cell><cell>crab legs</cell><cell>seafood restaurants</cell></row><row><cell>make up</cell><cell>makeup sets</cell><cell cols="2">snowboarding information burton snowboarding</cell><cell>best seafood</cell><cell>crab shack</cell></row><row><cell>makeup pictures</cell><cell>eye shadow</cell><cell>snowboarding jumps</cell><cell>snowboard deals</cell><cell>oysters</cell><cell>seafood market</cell></row><row><cell>makeup images</cell><cell>makeup covergirl</cell><cell>snowboard pics</cell><cell>snowboards on sale</cell><cell>lobster recipes</cell><cell>sea food</cell></row><row><cell>makeup tutorial</cell><cell>makeup items</cell><cell cols="2">shaun white snowboarding snowboarding mountains</cell><cell cols="2">seafood market sea food menu</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean and standard deviation (in brackets) of query similarity (cosine distance) between pairs of editorially judged query rewrites (top: in-dictionary queries, bottom: out-of-dictionary queries)</figDesc><table><row><cell cols="2">grade</cell><cell>pairs</cell><cell>cn2vec</cell><cell>cx2vec</cell><cell cols="6">cx-cn2vec cx-cn2vec ad cx-cn2vec ad+link word2vecnews word2vec search QFG ad+link</cell></row><row><cell cols="2">Excellent</cell><cell cols="6">1,518 0.630 (0.136) 0.658 (0.107) 0.669 (0.107) 0.668 (0.114)</cell><cell cols="2">0.733 (0.094) 0.818 (0.146)</cell><cell>0.648 (0.151) 0.329 (0.667)</cell></row><row><cell cols="2">Good</cell><cell cols="6">5,531 0.599 (0.136) 0.621 (0.125) 0.637 (0.100) 0.632 (0.104)</cell><cell cols="2">0.683 (0.097) 0.770 (0.152)</cell><cell>0.614 (0.155) 0.205 (0.574)</cell></row><row><cell cols="2">Fair</cell><cell cols="6">4,021 0.550 (0.167) 0.565 (0.129) 0.577 (0.124) 0.566 (0.130)</cell><cell cols="2">0.605 (0.132) 0.749 (0.190)</cell><cell>0.567 (0.173) 0.114 (0.366)</cell></row><row><cell cols="2">Bad</cell><cell cols="6">4,229 0.398 (0.196) 0.363 (0.170) 0.349 (0.184) 0.336 (0.187)</cell><cell cols="2">0.425 (0.179) 0.517 (0.280)</cell><cell>0.395 (0.201) 0.166 (0.584)</cell></row><row><cell cols="2">avg. p-value</cell><cell>-</cell><cell>1.39e-15</cell><cell>5.44e-26</cell><cell cols="2">8.27e-28</cell><cell>2.99e-30</cell><cell>1e-100</cell><cell>4.121e-07</cell><cell>1.014e-14</cell><cell>0.013</cell></row><row><cell cols="2">Excellent</cell><cell cols="6">2,119 0.791 (0.166) 0.623 (0.141) 0.628 (0.134) 0.623 (0.143)</cell><cell cols="2">0.668 (0.147) 0.824 (0.145)</cell><cell>0.790 (0.157)</cell><cell>-</cell></row><row><cell cols="2">Good</cell><cell cols="6">11,305 0.752 (0.155) 0.587 (0.135) 0.592 (0.130) 0.584 (0.137)</cell><cell cols="2">0.612 (0.141) 0.796 (0.145)</cell><cell>0.756 (0.156)</cell><cell>-</cell></row><row><cell cols="2">Fair</cell><cell cols="6">11,146 0.715 (0.136) 0.561 (0.145) 0.565 (0.139) 0.558 (0.146)</cell><cell cols="2">0.584 (0.147) 0.769 (0.175)</cell><cell>0.707 (0.141)</cell><cell>-</cell></row><row><cell cols="2">Bad</cell><cell cols="6">7,849 0.635 (0.199) 0.383 (0.211) 0.387 (0.209) 0.382 (0.212)</cell><cell cols="2">0.410 (0.208) 0.509 (0.311)</cell><cell>0.602 (0.208)</cell><cell>-</cell></row><row><cell cols="2">avg. p-value</cell><cell>-</cell><cell>4.99e-26</cell><cell>1.137e-27</cell><cell cols="2">4.0423e-32</cell><cell>1.196e-30</cell><cell>2.926e-40</cell><cell>2.038e-16</cell><cell>9.388e-22</cell><cell>-</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NDCG @ K</cell><cell>0.7 0.75 0.65</cell><cell></cell><cell></cell><cell cols="3">cx-cn2vec ad + link cx-cn2vec ad</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell>cx-cn2vec cx2vec</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>cn2vec</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell cols="2">QFG ad + link word2vec search</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1 0.5</cell><cell>5</cell><cell>10 K</cell><cell cols="2">15 word2vec news</cell><cell>20</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Examples of rewrites obtained using the competing methods (bolded rewrites matched bid-terms)</figDesc><table><row><cell>Original</cell><cell>QFG ad+link</cell><cell>cn2vec</cell><cell>cx-cn2vec</cell><cell>cx-cn2vec ad</cell></row><row><cell></cell><cell>wedding budget</cell><cell>monthly budget calculator</cell><cell>wedding planning checklist</cell><cell>wedding budget worksheet</cell></row><row><cell>wedding</cell><cell>wedding cost calculator</cell><cell>online budget calculator</cell><cell>wedding budget template</cell><cell>wedding vendors</cell></row><row><cell>budget</cell><cell>wedding calculator</cell><cell>budget wedding</cell><cell>wedding budget worksheet</cell><cell>the knot</cell></row><row><cell>calculator</cell><cell>wedding cost breakdown</cell><cell>budget calculator free</cell><cell>wedding checklist printable</cell><cell>wedding planning checklist</cell></row><row><cell></cell><cell cols="2">wedding budget worksheet average wedding budget</cell><cell>wedding costs average</cell><cell>wedding wire</cell></row><row><cell></cell><cell>gmat prep</cell><cell>gmat prep online</cell><cell>gmat preparation courses</cell><cell>gmat study books</cell></row><row><cell></cell><cell>gmat classes</cell><cell>gmat prep courses</cell><cell>sample gmat tests</cell><cell>gmat test prep classes</cell></row><row><cell>gmat prep</cell><cell>gmat</cell><cell>gmat online prep</cell><cell>gmat prep class</cell><cell>gmat prep class</cell></row><row><cell>classes</cell><cell>kaplan gmat course</cell><cell>online gmat prep</cell><cell>which is easier gre or gmat</cell><cell>kaplan gre courses</cell></row><row><cell></cell><cell>kaplan gmat</cell><cell>best gmat prep courses</cell><cell>how much is the gmat</cell><cell>free gmat sample tests</cell></row><row><cell></cell><cell>building a fence</cell><cell>how to build fence</cell><cell cols="2">how to build a fence on a hill how to build a fence minecraft</cell></row><row><cell></cell><cell>build your own fence</cell><cell>build fence</cell><cell>how to fence a yard</cell><cell>fancy fences and gates</cell></row><row><cell cols="2">how to build how to build a wood fence</cell><cell>build a fence</cell><cell>how to build a fence gate</cell><cell>how to build a metal fence</cell></row><row><cell>a fence</cell><cell>do it yourself fence</cell><cell>how to build a cheap fence</cell><cell>how to build a brick wall fence</cell><cell>home depot com fencing</cell></row><row><cell></cell><cell>how to build a privacy fence</cell><cell>build your own fence</cell><cell>how to build a fence video</cell><cell>back yard fences</cell></row><row><cell></cell><cell>solar panels for homes</cell><cell>solar panels for your home</cell><cell>solar power</cell><cell>solar power</cell></row><row><cell></cell><cell>solar electric panels</cell><cell cols="2">solar panels for residential homes solar energy</cell><cell>solar panels for homes</cell></row><row><cell cols="2">solar panels solar power</cell><cell>solar panels for</cell><cell>solar panels for homes</cell><cell>solar panels on sale</cell></row><row><cell></cell><cell>ebay solar panels</cell><cell>solar panels on ebay</cell><cell>solar power systems</cell><cell>solar panel kits</cell></row><row><cell></cell><cell>how to make solar panels</cell><cell>davis solar</cell><cell>solar panel</cell><cell>solar panels for sale</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of query rewrite methods (TREC data)</figDesc><table><row><cell>Method</cell><cell cols="2">Editorial grade Levenshtein dist.</cell></row><row><cell>QFG ad+link word2vecnews</cell><cell>1.0441 0.9189</cell><cell>11.70 10.91</cell></row><row><cell>word2vec search</cell><cell>0.9492</cell><cell>11.32</cell></row><row><cell>cn2vec</cell><cell>0.9571</cell><cell>11.37</cell></row><row><cell>cx2vec</cell><cell>1.1273</cell><cell>13.79</cell></row><row><cell>cx-cn2vec</cell><cell>1.1343</cell><cell>13.13</cell></row><row><cell>cx-cn2vec ad</cell><cell>1.2281</cell><cell>13.62</cell></row><row><cell>cx-cn2vec ad+link</cell><cell>1.2457</cell><cell>13.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Relative improvement over the QFG method of different query rewrite methods on query-bid data</figDesc><table><row><cell></cell><cell></cell><cell>In-house data</cell><cell></cell><cell></cell><cell>TREC data</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Coverage Revenue potential eCPM Coverage Revenue potential eCPM</cell></row><row><cell>QFG ad+link word2vecnews</cell><cell>1.00 0.76</cell><cell>1.00 0.39</cell><cell>1.00 0.46</cell><cell>1.00 0.32</cell><cell>1.00 0.66</cell><cell>1.00 0.73</cell></row><row><cell>word2vec search</cell><cell>0.87</cell><cell>0.58</cell><cell>0.77</cell><cell>0.57</cell><cell>0.84</cell><cell>0.75</cell></row><row><cell>cn2vec</cell><cell>0.89</cell><cell>0.62</cell><cell>0.84</cell><cell>0.59</cell><cell>0.84</cell><cell>0.74</cell></row><row><cell>cx2vec</cell><cell>1.16</cell><cell>1.80</cell><cell>1.41</cell><cell>1.41</cell><cell>1.16</cell><cell>1.20</cell></row><row><cell>cx-cn2vec</cell><cell>1.18</cell><cell>1.86</cell><cell>1.38</cell><cell>1.44</cell><cell>1.21</cell><cell>1.19</cell></row><row><cell>cx-cn2vec ad</cell><cell>1.20</cell><cell>1.89</cell><cell>1.60</cell><cell>1.52</cell><cell>1.35</cell><cell>1.31</cell></row><row><cell>cx-cn2vec ad+link</cell><cell>1.18</cell><cell>1.88</cell><cell>1.45</cell><cell>1.50</cell><cell>1.28</cell><cell>1.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>http://trec.nist.gov/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Web-scale user modeling for targeting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Query recommendation using query logs in search engines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mendoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 International Conference on Current Trends in Database Technology, EDBT&apos;04</title>
		<meeting>the 2004 International Conference on Current Trends in Database Technology, EDBT&apos;04<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="588" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Modern information retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>ACM press</publisher>
			<biblScope unit="volume">463</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The query-flow graph: Model and applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bonchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="609" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient query recommendations in the long tail via center-piece subgraphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bonchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venturini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;12</title>
		<meeting>the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Refining search queries by the suggestion of correlated terms from prior searches</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Spiegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">225</biblScope>
			<date type="published" when="1999">Dec. 21 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A taxonomy of web search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigir forum</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Search advertising using web relevance feedback</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ciccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1013" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale behavioral targeting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical neural language models for joint representation of streaming documents and their content</title>
		<author>
			<persName><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhamidipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sponsored search: A brief history</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Fain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="12" to="13" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey on session detection methods in query logs and a proposal for future evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gayo-Avello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1822" to="1843" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Search retargeting using directed query embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhamidipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generating ad targeting rules using sparse principal component analysis with constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vucetic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Empirical study of topic modeling in twitter</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Social Media Analytics</title>
		<meeting>the First Workshop on Social Media Analytics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Iab internet advertising revenue report: 2013 first six months&apos; results</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Interactive Advertising Bureau</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sponsored search: An overview of the concept, history, and technology</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mullen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Electronic Business</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="114" to="131" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating query substitutions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on World Wide Web</title>
		<meeting>the 15th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using the web to obtain frequencies for unseen bigrams</title>
		<author>
			<persName><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="459" to="484" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A multiplicative model for learning distributed text-based attribute representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2710</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relevance based language models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><forename type="first">V</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Physics-Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to target: what works for behavioral targeting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagherjeiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ciccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ratnaparkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><surname>Deepwalk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.6652</idno>
		<title level="m">Online learning of social representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Global entertainment and media outlook: 2014-2018</title>
		<author>
			<persName><surname>Pwc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Analysis of a very large web search engine query log</title>
		<author>
			<persName><forename type="first">C</forename><surname>Silverstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moricz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACm SIGIR Forum</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mining query logs: Turning search usage data into knowledge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="174" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Orthogonal query recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Loker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez-Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM Conference on Recommender Systems, RecSys &apos;13</title>
		<meeting>the 7th ACM Conference on Recommender Systems, RecSys &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Comparing click logs and editorial labels for training query rewriting</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW 2007 Workshop on Query Log Analysis: Social And Technological Challenges</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
