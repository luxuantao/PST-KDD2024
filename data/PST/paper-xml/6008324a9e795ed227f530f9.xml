<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HOW TO FIND YOUR FRIENDLY NEIGHBORHOOD: GRAPH ATTENTION DESIGN WITH SELF-SUPERVISION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
							<email>dongkwan.kim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
							<email>alice.oh@kaist.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HOW TO FIND YOUR FRIENDLY NEIGHBORHOOD: GRAPH ATTENTION DESIGN WITH SELF-SUPERVISION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are widely used in various domains, such as social networks, biology, and chemistry. Since their patterns are complex and irregular, learning to represent graphs is challenging <ref type="bibr" target="#b5">(Bruna et al., 2014;</ref><ref type="bibr" target="#b19">Henaff et al., 2015;</ref><ref type="bibr" target="#b9">Defferrard et al., 2016;</ref><ref type="bibr" target="#b10">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b2">Atwood &amp; Towsley, 2016)</ref>. Recently, graph neural networks (GNNs) have shown a significant performance improvement by generating features of the center node by aggregating those of its neighbors <ref type="bibr">(Zhou et al., 2018;</ref><ref type="bibr" target="#b56">Wu et al., 2020)</ref>. However, real-world graphs are often noisy with connections between unrelated nodes, and this causes GNNs to learn suboptimal representations. Graph attention networks (GATs) <ref type="bibr" target="#b52">(Veličković et al., 2018)</ref> adopt self-attention to alleviate this issue. Similar to attention in sequential data <ref type="bibr" target="#b31">(Luong et al., 2015;</ref><ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b51">Vaswani et al., 2017)</ref>, graph attention captures the relational importance of a graph, in other words, the degree of importance of each of the neighbors to represent the center node. GATs have shown performance improvements in node classification, but they are inconsistent in the degree of improvement across datasets, and there is little understanding of what graph attention actually learns.</p><p>Hence, there is still room for graph attention to improve, and we start by assessing and learning the relational importance for each graph via self-supervised attention. We leverage edges that explicitly encode information about the importance of relations provided by a graph. If node i and j are linked, they are more relevant to each other than others, and if node i and j are not linked, they are not important to each other. Although conventional attention is trained without direct supervision, if we have prior knowledge about what to attend, we can supervise attention using them <ref type="bibr" target="#b29">(Knyazev et al., 2019;</ref><ref type="bibr" target="#b59">Yu et al., 2017)</ref>. Specifically, we exploit a self-supervised task, using the attention value as input to predict the likelihood that an edge exists between nodes.</p><p>To encode edges in graph attention, we first analyze what graph attention learns and how it relates to the presence of edges. In this analysis, we focus on two commonly used attention mechanisms, GAT's original single-layer neural network (GO) and dot-product (DP), as building blocks of our proposed model, self-supervised graph attention network (SuperGAT). We observe that DP attention shows better performance than GO attention in the task to predict link with attention value. On the other hand, GO attention outperforms DP attention in capturing label-agreement between a target node and its neighbors. Based on our analysis, we propose two variants of SuperGAT, scaled dotproduct (SD) and mixed <ref type="bibr">GO and DP (MX)</ref>, to emphasize the strength of GO and DP.</p><p>Then, which graph attention models the relational importance best and produces the best node representations? We find that it depends on the average degree and homophily of the graph. We generate synthetic graph datasets with various degrees and homophily, and analyze how the choice of attention affects node classification performance. Based on this result, we propose a recipe to design graph attention with edge self-supervision that works most effectively for given graph characteristics. We conduct experiments on a total of 17 real-world datasets and demonstrate that our recipe can be generalized across them. In addition, we show that models developed by our method improve performance over baselines.</p><p>We present the following contributions. First, we present models with self-supervised attention using edge information. Second, we analyze the classic attention forms GO and DP using label-agreement and link prediction tasks, and this analysis reveals that GO is better at label agreement and DP at link prediction. Third, we propose recipes to design graph attention concerning homophily and average degree and confirm its validity through experiments on real-world datasets. We make our code available for future research (https://github.com/dongkwan-kim/SuperGAT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deep neural networks are actively studied in modeling graphs, for example the graph convolutional networks <ref type="bibr" target="#b27">(Kipf &amp; Welling, 2017)</ref> which approximate spectral graph convolution <ref type="bibr" target="#b5">(Bruna et al., 2014;</ref><ref type="bibr" target="#b9">Defferrard et al., 2016)</ref>. A representative work in a non-spectral way is the graph attention networks (GATs) <ref type="bibr" target="#b52">(Veličković et al., 2018)</ref> which model relations in graphs using self-attention mechanism <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref>. Similar to attention in sequence data <ref type="bibr" target="#b3">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b31">Luong et al., 2015;</ref><ref type="bibr" target="#b51">Vaswani et al., 2017)</ref>, variants of attention in graph neural networks <ref type="bibr" target="#b50">(Thekumparampil et al., 2018;</ref><ref type="bibr">Zhang et al., 2018;</ref><ref type="bibr" target="#b53">Wang et al., 2019a;</ref><ref type="bibr" target="#b14">Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b62">Zhang et al., 2020;</ref><ref type="bibr" target="#b20">Hou et al., 2020)</ref> are trained without direct supervision. Our work is motivated by studies that improve attention's expressive power by giving direct supervision <ref type="bibr" target="#b29">(Knyazev et al., 2019;</ref><ref type="bibr" target="#b59">Yu et al., 2017)</ref>. Specifically, we employ a self-supervised task to predict edge presence from attention value. This is in line with two branches of recent GNN research: self-supervision and graph structure learning.</p><p>Recent studies about self-supervised learning for GNNs propose tasks leveraging the inherent information in the graph structure: clustering, partitioning, context prediction after node masking, and completion after attribute masking <ref type="bibr" target="#b22">(Hu et al., 2020b;</ref><ref type="bibr" target="#b23">Hui et al., 2020;</ref><ref type="bibr" target="#b48">Sun et al., 2020;</ref><ref type="bibr" target="#b58">You et al., 2020)</ref>. To the best of our knowledge, ours is the first study to analyze self-supervised learning of graph attention with edge information. Our self-supervised task is similar to link prediction <ref type="bibr" target="#b30">(Liben-Nowell &amp; Kleinberg, 2007)</ref>, which is a well-studied problem and recently tackled by neural networks <ref type="bibr" target="#b63">(Zhang &amp; Chen, 2017;</ref><ref type="bibr">2018)</ref>. Our DP attention to predict links is motivated by graph autoencoder (GAE) <ref type="bibr" target="#b26">(Kipf &amp; Welling, 2016)</ref> and its extensions <ref type="bibr" target="#b37">(Pan et al., 2018;</ref><ref type="bibr" target="#b38">Park et al., 2019)</ref> reconstructing edges by applying a dot-product decoder to node representations.</p><p>Graph structure learning is an approach to learn the underlying graph structure while jointly learning downstream tasks <ref type="bibr" target="#b24">(Jiang et al., 2019;</ref><ref type="bibr" target="#b13">Franceschi et al., 2019;</ref><ref type="bibr" target="#b28">Klicpera et al., 2019;</ref><ref type="bibr" target="#b46">Stretcu et al., 2019;</ref><ref type="bibr">Zheng et al., 2020)</ref>. Since real-world graphs often have noisy edges, encoding structure information contributes to learn better representation. However, recent models with graph structure learning suffer from high memory and computational complexity. Some studies target all spaces where edges can exist, so they require O(|V | 2 ) space and computational complexity <ref type="bibr" target="#b24">(Jiang et al., 2019;</ref><ref type="bibr" target="#b13">Franceschi et al., 2019)</ref>. Others using iterative training (or co-training) between the GNNs and the structure learning model are time-intensive in training <ref type="bibr" target="#b13">(Franceschi et al., 2019;</ref><ref type="bibr" target="#b46">Stretcu et al., 2019)</ref>. We moderate this problem using graph attention, which consists of parallelizable operations, and our model is built on it without additional parameters. Our model learns attention values that are predictive of edges, and this can be seen as a new paradigm of learning the graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head><p>In this section, we review the original GAT <ref type="bibr" target="#b52">(Veličković et al., 2018)</ref> and then describe our selfsupervised GAT (SuperGAT) models.</p><formula xml:id="formula_0">W ⃗ h i W ⃗ h j ⃗ a e ij,GO e ij,DP e ij,SD Dot-product 1/ F ϕ ij,MX,DP ϕ ij,SD e ij,MX × ϕ ij,GO σ σ σ Original GAT α ij softmax j = P(( j, i) ∈ E) e ij ϕ ij</formula><p>Figure <ref type="figure">1</ref>: Overview of attention mechanism of SuperGATs: GO, DP, MX, and SD. Blue circles (e ij ) represent the unnormalized attention before softmax and red diamonds (φ ij ) indicate the probability of edge between node i and j. The attention mechanism of the original GAT <ref type="bibr" target="#b52">(Veličković et al., 2018)</ref> is in the dashed rectangle.</p><p>Notation For a graph G = (V, E), N is the number of nodes and F l is the number of features at layer l. Graph attention layer takes a set of features</p><formula xml:id="formula_1">H l = {h l 1 , . . . , h l N }, h l i ∈ R F l</formula><p>as input and produces output features H l+1 = {h l+1 1 , . . . , h l+1 N }. To compute h l+1 i , the model multiplies the weight matrix W l+1 ∈ R F l+1 ×F l to H l , linearly combines the features of its first-order neighbors (including itself) j ∈ N i ∪ {i} by attention coefficients α l+1 ij , and finally applies a non-linear activation ρ.</p><formula xml:id="formula_2">That is h l+1 i = ρ j∈Ni∪{i} α l+1 ij W l+1 h l j . We can compute α l+1 ij = softmax j (LReLU(e l+1 ij )) by normalizing e l+1 ij = a e (W l+1 h l i , W l+1 h l j )</formula><p>with softmax on N i ∪ {i} under leaky ReLU activation <ref type="bibr" target="#b32">(Maas et al., 2013)</ref>, where a e is a function of the form</p><formula xml:id="formula_3">R F l+1 × R F l+1 → R.</formula><p>Graph Attention Forms Among two widely used attention mechanisms, the original GAT (GO) computes the coefficients by single-layer feed-forward network parameterized by a l+1 ∈ R 2F l+1 . The other is the dot-product (DP) attention, <ref type="bibr" target="#b31">(Luong et al., 2015;</ref><ref type="bibr" target="#b51">Vaswani et al., 2017)</ref> motivated by prior work on node representation learning, and it adopts the same mathematical expression for link prediction score <ref type="bibr" target="#b49">(Tang et al., 2015;</ref><ref type="bibr" target="#b26">Kipf &amp; Welling, 2016)</ref>,</p><formula xml:id="formula_4">e l+1 ij,GO = (a l+1 ) W l+1 h l i W l+1 h l j and e l+1 ij,DP = (W l+1 h l i ) • W l+1 h l j .<label>(1)</label></formula><p>From now on, we call GAT that uses GO and DP as GAT GO and GAT DP , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-supervised Graph Attention Network</head><p>We propose SuperGAT with the idea of guiding attention with the presence or absence of an edge between a node pair. We exploit the link prediction task to self-supervise attention with labels from edges: for a pair i and j, 1 if an edge exists and 0 otherwise. We introduce a φ with sigmoid σ to infer the probability φ ij of an edge between i and j.</p><formula xml:id="formula_5">a φ : R F × R F → R and φ ij = P ((j, i) ∈ E) = σ(a φ (W h i , W h j ))<label>(2)</label></formula><p>We employ four types (GO, DP, SD, and MX) of SuperGAT based on GO and DP attention. For a φ , the form of which is the same as a e in GAT GO and GAT DP , we name them SuperGAT GO and SuperGAT DP respectively. For more advanced versions, we describe SuperGAT SD (Scaled Dotproduct) and SuperGAT MX (Mixed GO and DP) by unnormalized attention e ij and probability φ ij that an edge exist between i and j.</p><p>SuperGAT SD : e ij,SD = e ij,DP / √ F , φ ij,SD = σ(e ij,SD ).</p><p>(3) SuperGAT MX : e ij,MX = e ij,GO • σ(e ij,DP ), φ ij,MX = σ(e ij,DP ).</p><p>(4)</p><p>SuperGAT SD divides the dot-product of nodes by a square root of dimension as Transformer <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref>. This prevents some large values to dominate the entire attention after softmax. SuperGAT MX multiplies GO and DP attention with sigmoid. The motivation of this form comes from the gating mechanism of Gated Recurrent Units <ref type="bibr" target="#b6">(Cho et al., 2014)</ref>. Since DP attention with the sigmoid represents the probability of an edge, it can softly drop neighbors that are not likely linked while implicitly assigning importance to the remaining nodes.</p><p>Training samples are a set of edges E and the complementary set E c = (V × V ) \ E. However, if the number of nodes is large, it is not efficient to use all possible negative cases in E c . So, we use negative sampling as in training word or graph embeddings <ref type="bibr" target="#b36">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b49">Tang et al., 2015;</ref><ref type="bibr" target="#b16">Grover &amp; Leskovec, 2016)</ref>, arbitrarily choosing a total of p n • |E| negative samples E − from E c where the negative sampling ratio p n ∈ R + is a hyperparameter. SuperGAT is capable of modeling graphs that are sparse with a sufficiently large number of negative samples (i.e., |V × V | |E|), but this is generally not a problem because most real-world graphs are sparse <ref type="bibr" target="#b7">(Chung, 2010)</ref>.</p><p>We define the optimization objective of layer l as a binary cross-entropy loss L l E ,</p><formula xml:id="formula_6">L l E = 1 |E∪E − | (j,i)∈E∪E − 1 (j,i)=1 • log φ l ij + 1 (j,i)=0 • log 1 − φ l ij ,<label>(5)</label></formula><p>where 1 • is an indicator function. We use a subset of E ∪ E − sampled by probability p e ∈ (0, 1] (also a hyperparameter) at each training iteration for a regularization effect from randomness.</p><p>Finally, we combine cross-entropy loss on node labels (L V ), self-supervised graph attention losses for all L layers (L l E ), and L2 regularization loss, with mixing coefficients λ E and λ 2 .</p><formula xml:id="formula_7">L = L V + λ E • L l=1 L l E + λ 2 • W 2 . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>We use the same form of multi-head attention in GAT and take the mean of each head's attention value before the sigmoid to compute φ ij . Note that SuperGAT has equivalent time and space complexity as GAT. To compute L l E for one head, we need additional operations in terms of O(F l • |E ∪ E − |), and we do not need extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Our primary research objective is to design graph attentions that are effective with edge selfsupervision. To do this, we pose four specific research questions. We first analyze what basic graph attentions (GO and DP) learn (RQ1 and 2) and how that can be improved with edge self-supervision (RQ3 and 4). We describe each research question and the corresponding experiment design below.</p><p>RQ1. Does graph attention learn label-agreement? First, we evaluate what the graph attentions of GAT GO and GAT DP learn without edge supervision. For this, we present ground-truth of relational importance and a metric to assess graph attention with ground-truth. <ref type="bibr" target="#b53">Wang et al. (2019a)</ref> showed that node representations in the connected component converge to the same value in deep GATs. If there is an edge between nodes with different labels, then it will be hard to distinguish the two corresponding labels with GAT of sufficiently many layers; that is, ideal attention should give all weights to label-agreed neighbors. In that sense, we choose label-agreement between nodes as ground-truth of importance.</p><p>We compare label-agreement and graph attention based on Kullback-Leibler divergence of the normalized attention α k = [α kk , α k1 , . . . , α kJ ] with label agreement distribution for the center node k and its neighbors 1 to J. The label agreement distribution, k = [ kk , k1 , . . . , kJ ] is defined by, kj = ˆ kj / s ˆ ks , ˆ kj = 1 (if k and j have the same label) or 0 (otherwise).</p><p>We employ KL divergence in Eq. 8, whose value becomes small when attention captures well the label-agreement between a node and its neighbors.</p><formula xml:id="formula_10">KLD(α k , k ) = j∈N k ∪{k} α kj log(α kj / kj )<label>(8)</label></formula><p>RQ2. Is graph attention predictive of edge presence? To evaluate how well edge information is encoded in SuperGAT, we conduct link prediction experiments with SuperGAT GO and SuperGAT DP using φ ij of the last layer as a predictor. We measure the performance by AUC over multiple runs.</p><p>Since link prediction performance depends on the mixing coefficient λ E in Eq. 6, we adopt multiple λ E ∈ {10 −3 , 10 −2 , . . . , 10 3 }. We train with an incomplete set of edges, and test with the missing edges and the same number of negative samples. At the same time, node classification performance is measured with the same settings to see how learning edge presence affects node classification.</p><p>RQ3. Which graph attention should we use for given graphs? The above two research questions explore what different graph attention learns with or without supervision of edge presence. Then, which graph attention is effective among them for given graphs? We hypothesize that different graph attention will have different abilities to model graphs under various homophily and average degree. We choose these two properties among various graph statistics because they determine the quality and quantity of labels in our self-supervised task. From the perspective of supervised learning of graph attention with edge labels, the learning result depends on how noisy labels are (i.e., how low the homophily is) and how many labels exist (i.e., how high the average degree is). So, we generate 144 synthetic graphs (Section 4.1) controlling 9 homophily (0.1 -0.9) and 16 average degree (1 -100) and perform the node classification task in the transductive setting with GCN, GAT GO , SuperGAT SD , and SuperGAT MX .</p><p>In RQ3, there are also practical reasons to use the average degree and homophily, out of many graph properties (e.g., diameter, degree sequence, degree distribution, average clustering coefficient). First, the graph property can be computed efficiently even for large graphs. Second, there should be an algorithm that can generate graphs by controlling the property of interest only. Third, the property should be a scalar value because if the synthetic graph space is too wide, it would be impossible to conduct an experiment with sufficient coverage. Average degree and homophily satisfy the above conditions and are suitable for our experiment, unlike some of the other graph properties.</p><p>RQ4. Does design choice based on RQ3 generalize to real-world datasets? Experiments on synthetic datasets provide an understanding of graph attention models' performance, but they are oversimplified versions of real-world graphs. Can design choice from synthetic datasets be generalized to real-world datasets, considering more complex structures and rich features in real-world graphs? To answer this question, we conduct experiments on 17 real-world datasets with the various average degree (1.8 -35.8) and homophily (0.16 -0.91), and compare them with synthetic graph experiments in RQ3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>Real-world datasets We use a total of 17 real-world datasets (Cora, CiteSeer, PubMed, Cora-ML, Cora-Full, DBLP, ogbn-arxiv, CS, Physics, Photo, Computers, Wiki-CS, Four-Univ, Chameleon, Crocodile, Flickr, and PPI) in diverse domains (citation, co-authorship, co-purchase, web page, and biology) and scales (2k -169k nodes). We try to use their original settings as much as possible. To verify research questions 1 and 2, we choose four classic benchmarks: Cora, CiteSeer, PubMed in the transductive setting, and PPI in the inductive setting. See appendix A.1 for detailed description, splits, statistics (including degree and homophily), and references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic datasets</head><p>We generate random partition graphs of n nodes per class and c classes (Fortunato, 2010), using NetworkX library <ref type="bibr" target="#b17">(Hagberg et al., 2008)</ref>. A random partition graph is a graph of communities controlled by two probabilities p in and p out . If the nodes have the same class labels, they are connected with p in , and otherwise, they are connected with p out . To generate a graph with an average degree of d avg = n • δ, we choose p in and p out by p in + (c − 1) • p out = δ. The input features of nodes are sampled from overlapping multi-Gaussian distributions <ref type="bibr" target="#b1">(Abu-El-Haija et al., 2019)</ref>. We set n to 500, c to 10, and choose d avg between 1 and 100, p in from {0.1δ, 0.2δ, . . . , 0.9δ}. We use 20 samples per class for training, 500 for validation and 1000 for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTAL SET-UP</head><p>We follow the experimental set-up of GAT with minor adjustments. All parameters are initialized by Glorot initialization <ref type="bibr" target="#b15">(Glorot &amp; Bengio, 2010)</ref> and optimized by Adam <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014)</ref>. We apply L2 regularization, dropout <ref type="bibr" target="#b45">(Srivastava et al., 2014)</ref> to features and attention coefficients, and early stopping on validation loss and accuracy. We use ELU <ref type="bibr">(Clevert et al., 2016)</ref> as a nonlinear activation ρ. Unless specified, we employ a two-layer SuperGAT with F = 8 features and K = 8 attention heads (total 64 features). All models are implemented in PyTorch <ref type="bibr" target="#b39">(Paszke et al., 2019)</ref> and PyTorch Geometric <ref type="bibr" target="#b11">(Fey &amp; Lenssen, 2019)</ref>. See appendix A.5 for detailed model and hyperparameter configurations. Baselines For all datasets, we compare our model against representative graph neural models: graph convolutional network (GCN) <ref type="bibr" target="#b27">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b18">(Hamilton et al., 2017)</ref>, and graph attention network (GAT) <ref type="bibr" target="#b52">(Veličković et al., 2018)</ref>. Furthermore, for Cora, CiteSeer, and PubMed, we choose recent graph neural architectures that learn aggregation coefficients (or discrete structures) over edges: constrained graph attention network (CGAT<ref type="foot" target="#foot_0">1</ref> ) <ref type="bibr" target="#b53">(Wang et al., 2019a)</ref>, graph learning-convolutional network (GLCN) <ref type="bibr" target="#b24">(Jiang et al., 2019)</ref>, learning discrete structure (LDS) <ref type="bibr" target="#b13">(Franceschi et al., 2019)</ref>, graph agreement model (GAM) <ref type="bibr" target="#b46">(Stretcu et al., 2019)</ref>, and Neu-ralSparse (NS in short) <ref type="bibr">(Zheng et al., 2020)</ref>. For the PPI, we use CGAT as an additional baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>This section describes the experimental results which answer the research questions in Section 4. We include a qualitative analysis of attention, quantitative comparisons on node classification and link prediction, and recipes of graph attention design. The results for sensitivity analysis of important hyper-parameters are in the appendix B.5.</p><p>Does graph attention learn label-agreement? GO learns label-agreement better than DP.</p><p>We draw box plots of KL divergence between attention and label agreement distributions of twolayer and four-layer GAT with GO and DP attention for Cora dataset in Figure <ref type="figure" target="#fig_0">2</ref>. We see similar patterns in other datasets and place their plots in the appendix B.3. At the rightmost of each subfigure, we draw the KLD distribution when uniform attention is given to all neighborhoods. Note that the maximum value of KLD of each node is different since the degree of nodes is different. Also, the KLD distribution shows a long-tail shape like a degree distribution of real-world graphs.</p><p>There are three observations regarding distributions of KLD. First, we observe that the KLD distribution of GO attention shows a pattern similar to the uniform attention for all citation datasets. This implies that trained GO attention is similar to the uniform distribution, which is in line with previously reported results in the case of entropy<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b55">(Wang et al., 2019b)</ref>. Second, KLD values of DP attention tend to be larger than those of GO attention for the last layer, resulting in bigger long-tails. This mismatch between the learned distribution of DP attention and the label agreement distribution suggests that DP attention does not learn label-agreement in the neighborhood. Third, the deeper the model (more than two), the larger the KLD value of DP attention in the last layer. This is because the variance of DP attention increases as the layer gets deeper, as explained below in Proposition 1. Proposition 1. For l + 1th GAT layer, if W and a are independent and identically drawn from zero-mean uniform distribution with variance σ 2 w and σ 2 a respectively, assuming that parameters are independent to input features h l and elements of h l are independent to each other,</p><formula xml:id="formula_11">Var[e l+1 ij,GO ] = 2F l+1 σ 2 w σ 2 a E( h l 2 2 ) and Var[e l+1 ij,DP ] ≥ F l+1 σ 4 w 4 5 E ((h l i ) h l j ) 2 + Var((h l i ) h l j ) (9)</formula><p>The proof is given in the appendix B.1. While the variance of GO depends on the norm of features only, the variance of DP depends on the expectation of the square of input's dot-product and variance of input's dot-product. Stacking GAT layers, the more features of i and j correlate with each other, the larger the input's dot-product will be. After DP attention is normalized by softmax, which intensifies the larger values among them, normalized DP attention attends to only a small portion of the neighbors and learns a biased representation. Is graph attention predictive for edge presence? DP predicts edge presence better than GO.</p><p>In Figure <ref type="figure">3</ref>, we report the mean AUC over multiple runs (5 for PPI and 10 for others) for link prediction (red lines) and node classification (gray lines). As the mixing coefficient λ E increases, the link prediction score increases in all datasets and attentions. This is a natural result considering that λ E is the weight factor of self-supervised graph attention loss (L E in Equation <ref type="formula" target="#formula_7">6</ref>). For three out of four datasets, DP attention outperforms GO for link prediction for all range of λ E in our experiment. Surprisingly, even for small λ E , DP attention shows around 80 AUC, much higher than the performance of GO attention. PPI is an exception where GO attention shows higher performance for small λ E than DP, but the difference is slight. The results of this experiment demonstrate that DP attention is more suitable than GO attention in encoding edges.</p><p>This figure also includes node classification performance. For all datasets except PubMed, we observe a trade-off between node classification and link prediction; that is, node classification performance decreases in SuperGAT GO and SuperGAT DP as λ E increases and thus link prediction performance increases. PubMed also shows a decrease in performance at the largest λ E we have tested. This implies that it is hard to learn the relational importance from edges by simply optimizing graph attention for link prediction.</p><p>Which graph attention should we use for given graphs? It depends on homophily and average degree of the graph.</p><p>In Figure <ref type="figure">4</ref>, we draw the mean test accuracy gains (over 5 runs) against GAT GO as the average degree increases from 1 to 100, for different values of homophily, on 64 synthetic graphs with GCN, SuperGAT SD , SuperGAT MX . See full results from appendix B.2. We define homophily h as the average ratio of neighbors with the same label as the center node <ref type="bibr" target="#b40">(Pei et al., 2020)</ref>.</p><formula xml:id="formula_12">That is h = 1 |V | i∈V j∈Ni 1 l(i)=l(j) |N i |</formula><p>, where l(i) is the label of node i. The expectation of homophily for random partition graphs is analytically p in /δ, and we just adopt this value to label the homophily of graphs in Figure <ref type="figure">4</ref>.</p><p>We make the following observations from this figure. First, if the homophily is low (≤ 0.2), SuperGAT SD performs best among models because DP attention tends to focus on a small number of neighbors. This result empirically confirms what we analytically found in Proposition 1. Second, even when homophily is low, the performance gain of SuperGAT against GAT increases as the average degree increases to a certain level (around 10), meaning relation modeling can benefit from self-supervision if there are sufficiently many edges providing supervision. This is in agreement with prior study of label noise for deep neural networks where they find that the absolute amount of data with correct labels affects the learning quality more than the ratio between data with noisy and correct labels <ref type="bibr" target="#b41">(Rolnick et al., 2017)</ref>. Third, if the average degree and homophily are high enough, Best graph attention (See Fig. <ref type="figure" target="#fig_2">5</ref>)</p><p>there is no difference between all models, including GCNs. If there are more correct edges beyond a certain amount, we can learn fine representation without self-supervision. Most importantly, if the average degree is not too low or high and homophily is above 0.2, SuperGAT MX performs better than or similar to SuperGAT SD . This implies that we can take advantage of both GO attention to learn label-agreement and DP attention to learn edge presence by mixing GO and DP. Note that many of the real-world graphs belong to this range of graph characteristics.</p><p>The results on synthetic graphs imply that understanding of graph domains should be preceded to design graph attention. That is, by knowing the average degree and homophily of the graphs, we can choose the optimal graph attention in our design space.</p><p>Does design choice based on RQ3 generalize to real-world datasets? It does for 15 of 17 realworld datasets.</p><p>In Figure <ref type="figure" target="#fig_2">5</ref>, we plot the best-performed graph attention for synthetic graphs with square points in the plane of average degree and homophily. The size is the performance gain of SuperGAT against GAT, and the color indicates the best model. If the difference is not statistically significant (p-value ≥ .05) between GAT and SuperGAT, and between SuperGAT MX and SuperGAT SD , we mark as GAT-Any and SuperGAT-Any, respectively. We call this plot a recipe since it introduces the optimal attention to a specific region's graph.</p><p>Now we map the results of 17 real-world datasets in Figure <ref type="figure" target="#fig_2">5</ref> according to their average degree and homophily. Average degree and homophily can be found in the appendix A.1, and experimental results of graph attention models are summarized in Tables 1, 2 and 3. We report the mean and standard deviation of performance over multiple seeds (30 for graphs with more than 50k nodes and 100 for others). We put unpaired t-test results of SuperGAT with GAT GO with asterisks.</p><p>We find that the graph attention recipe based on synthetic experiments can generalize across realworld graphs. PPI and Four-Univ ( ) are surrounded by squares of SuperGAT SD ( ) at the bottom of the plane. Wiki-CS ( ), located in the SuperGAT-Any region ( ), also show no difference in performance between SuperGATs. Nine datasets ( ), which SuperGAT MX shows the highest performance, are located in the MX regions ( ) or within the margin of two squares. Note that there are two MX regions: lower-middle average degree (2.5 -7.5) and high homophily (0.8 -0.9), and uppermiddle average degree (7.5 -50) and lower-middle homophily (0.3 -0.5). There are five datasets with no significant performance change across graph attention ( ). CiteSeer, Photo, and Computers are within a margin of one square from the GAT-Any region ( ); however, Flickr and Crocodile are in the SuperGAT-Any region. To find out the cause of this irregularity, we examine the distribution of degree and per-node homophily (appendix A.4). We observe a more complex mixture distribution of homophily and average degree in Flickr and Crocodile, and this seems to be equivalent to mixing graphs of different characteristics, resulting in inconsistent results with our attention design recipe.</p><p>Comparison with baselines For a total of 17 datasets, SuperGAT outperforms GCN for 13 datasets, GAT for 12 datasets, GraphSAGE for 16 datasets. Interestingly, for CS, Physics, Cora-ML, and Flickr, in which our model performs worse than GCN, GAT also cannot surpass GCN. It is not yet known when the degree-normalized aggregation of GCN outperforms the attentionbased aggregation, and more research is needed to figure out how to embed the degree information into graph attention. Tables <ref type="table" target="#tab_1">2 and 3</ref> show performance comparisons between SuperGAT and recent GNNs for Cora, CiteSeer, PubMed, and PPI. Our model performs better for CiteSeer (0.6%p) and PubMed (3.4%p) than GLCN, which gives regularization to all relations in a graph. GCN + NS (NeuralSparse) performs better than our model for CiteSeer (1.5%p) but not for Cora (0.6%p). CGAT modified for semi-supervised learning shows lower performance than GAT. Although LDS and GAM which use iterative training show better performance except for LDS on Cora, these models require significantly more computation for the iterative training. For example, GCN + GAM compared to our model needs more ×34 more training time for Cora, ×72 for CiteSeer, and ×82 for PubMed. See appendix A.7 and B.4 for the experimental set-up and the result of wall-clock time analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed novel graph neural architecture designs to self-supervise graph attention following the input graph's characteristics. We first assessed what graph attention is learning and analyzed the effect of edge self-supervision to link prediction and node classification performance. This analysis showed two widely used attention mechanisms (original GAT and dot-product) have difficulty encoding label-agreement and edge presence simultaneously. To address this problem, we suggested several graph attention forms that balance these two factors and argued that graph attention should be designed depending on the input graph's average degree and homophily. Our experiments demonstrated that our graph attention recipe generalizes across various real-world datasets such that the models designed according to the recipe outperform other baseline models.</p><p>Cheng Four-Univ The Four-Univ dataset is a web page networks from computer science departments of diverse universities <ref type="bibr" target="#b8">(Craven et al., 1998)</ref>. Nodes are web pages, edges are hyperlinks between them, and node features are TF-IDF vectors of web page's contents. There are five graphs consists of four universities (Cornell, Texas, Washington, and Wisconsin) and a miscellaneous graph from other universities. As the original authors suggested<ref type="foot" target="#foot_2">3</ref> , we use three graphs of universities and a miscellaneous graph for training, another one graph for validation (Cornell), and the other one graph for the test (Texas). Classification labels are types of web pages (student, faculty, staff, department, course, and project).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.5 FLICKR</head><p>The Flickr dataset is a graph of images from Flickr <ref type="bibr" target="#b33">(McAuley &amp; Leskovec, 2012;</ref><ref type="bibr" target="#b60">Zeng et al., 2020)</ref>. Nodes are images, and edges indicate whether two images share common properties such as geographic location, gallery, and users commented. Node features are a bag-of-words representation of images. We use labels and split in in <ref type="bibr" target="#b60">Zeng et al. (2020)</ref>. For labels, they construct seven classes by manually merging 81 image tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.6 PROTEIN-PROTEIN INTERACTION</head><p>The protein-protein interaction (PPI) dataset <ref type="bibr">(Zitnik &amp; Leskovec, 2017;</ref><ref type="bibr" target="#b18">Hamilton et al., 2017;</ref><ref type="bibr" target="#b47">Subramanian et al., 2005)</ref> is a well-known benchmark in the inductive setting. A graph is given for human tissue, the nodes are proteins, the node's features are biological signatures like genes, and the edges illustrate proteins' interactions. The dataset consists of 20 training graphs, two validation graphs, and two test graphs. This dataset has multi-labels of gene ontology sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 LINK PREDICTION</head><p>For link prediction, we split 5% and 10% of edges for validation and test set, respectively. We fix the negative edges for the test set and sample negative edges for the training set at each iteration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SYNTHETIC DATASET</head><p>To the best of our knowledge, our synthetic datasets are not used in recent literature. Therefore, we give some small examples of synthetic datasets to see qualitatively how the average degree and homophily vary according to δ and p in . Specifically, we draw 2D t-SNE plot of node features and edges in Figure <ref type="figure" target="#fig_4">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 DISTRIBUTION OF DEGREE AND HOMOPHILY OF DATASETS</head><p>In Figure <ref type="figure" target="#fig_5">7</ref>, we draw kernel density estimation plots of per-node homophily and degree of nodes in real-world datasets. We define per-node homophily as the ratio of neighbors with the same label as the center node, that is, We focus more on the part outside of where the degree is 1 (0 with log scale), and the per-node homophily is 0. These are leaf nodes incorrectly connected and does not significantly affect the learning overall graph representation. In most datasets, only the largest mode exists, or there are some small modes around it. However, in Flickr and Crocodile, we can observe that the interval between modes is wide. More specifically, Crocodile's modes are in the area of (high degree, low per-node homophily) and (low degree, high per-node homophily), and Flickr's modes cover most homophily at a specific degree. Note that we can regard a mixture of distribution as a mixture of different sub-graphs. We argue that this is the reason our recipe does not fit for these two datasets.</p><formula xml:id="formula_13">h i = j∈Ni 1 l(i)=l(j) |N i |.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 MODEL &amp; HYPERPARAMETER CONFIGURATIONS</head><p>Model Since we experiment with numbers of datasets, we maintain almost the same configurations across datasets. We do not use other methods such as residual connections, deeper layers, batch normalization, edge augmentation, and more hidden features, although we have confirmed from previous studies that these techniques contribute to performance improvement. For example, prior 88.2 ± 0.3 78.9 ± 0.2 87.4 ± 0.3 CGAT (Our Impl.: 2-layer w/ 64-features) 88.9 ± 0.3 78.9 ± 0.2 86.9 ± 0.2 SuperGAT MX (2-layer w/ 64-features) 88.7 ± 0.2 79.1 ± 0.2 87.0 ± 0.1 work has shown f1-score close to 100 for PPI. To clearly see the difference between the various graph attention designs, we intentionally keep a simple model configuration.</p><p>Hyperparameter For real-world datasets, we tune two hyperparameters (mixing coefficients λ 2 and λ E ) by Bayesian optimization for the mean performance of 3 random seeds. We choose negative sampling ratio p n from {0.3, 0.5, 0.7, 0.9}, and edge sampling ratio p e from {0.6, 0.8, 1.0}. We fix dropout probability to 0.0 for PPI, 0.2 for ogbn-arxiv, 0.6 for others. We set learning rate to 0.05 (ogbn-arxiv), 0.01 (PubMed, PPI, Wiki-CS, Photo, Computers, CS, Physics, Crocodile, Cora-Full, DBLP), 0.005 (Cora, CiteSeer, Cora-ML, Chameleon), 0.001 (Four-Univ). For ogbn-arxiv, we set the number of features per head to 16 and the number of heads in the last layer to one; otherwise, we use eight features per head and eight heads in the last layer.</p><p>For synthetic datasets, we choose λ E from {10 −5 , 10 −4 , 10 −3 , 10 −2 , 10 −1 , 1, 10, 10 2 } and λ 2 from {10 −7 , 10 −5 , 10 −3 }. We fix learning rate to 0.01, dropout probability to 0.2, p n to 0.5, and, p e to 0.8 for all synthetic graphs.</p><p>Table <ref type="table" target="#tab_4">6</ref> describes hyperparameters for SuperGAT on four real-world datasets. For other datasets and experiments, please see the code (./SuperGAT/args.yaml).</p><p>A.6 CGAT IMPLEMENTATION CGAT <ref type="bibr" target="#b53">(Wang et al., 2019a)</ref> has two auxiliary losses: graph structure based constraint L g and class boundary constraint L b . We borrow their notation for this section: V as a set of nodes, N i as a set of one-hop neighbors, N + i as a set of neighbors that share labels, N − i as a set of neighbors that do not share labels, ζ • as a margin between attention values, and φ (v i , v k ) as unnormalized attention value.</p><formula xml:id="formula_14">L g = i∈V j∈Ni\N − i k∈(V\Ni) max (0, φ (v i , v k ) + ζ g − φ (v i , v j ))<label>(10)</label></formula><formula xml:id="formula_15">L b = i∈V j∈N + i k∈N − i max (0, φ (v i , v k ) + ζ b − φ (v i , v j ))<label>(11)</label></formula><p>Since label information is included in these two losses, they are difficult to use in semi-supervised settings that provide few labeled samples. In fact, in the CGAT paper, they conduct experiments in full-supervised settings; that is, they use all nodes in training except validation and test nodes.</p><p>So, we only use L g modified for semi-supervised learning.</p><formula xml:id="formula_16">L SSL g = i∈V j∈Ni k∈(V\Ni) (0, φ (v i , v k ) + ζ g − φ (v i , v j ))<label>(12)</label></formula><p>With L c , the multi-class cross-entropy on node labels, CGAT's optimization objective is</p><formula xml:id="formula_17">L = L c + λ g L g + λ b L b ,<label>(13)</label></formula><p>and our modified CGAT's loss is</p><formula xml:id="formula_18">L = L c + λ g L SSL g .<label>(14)</label></formula><p>In addition to losses, CGAT proposes top-k softmax and node importance based negative sampling (NINS). Top-k softmax picks up nodes with top-k attention values among neighbors. NINS adopts importance sampling when choosing negative sample nodes.</p><p>Since the code for CGAT has not been released, we implement our own version. In all experiments in our paper, we use only the modified losses (Equation <ref type="formula" target="#formula_18">14</ref>) and top-k softmax due to the training and implementation complexity of NINS. For PPI, even if we do not assume a semi-supervised setting, we use the same loss because we could not accurately implement multi-label cases for Equation <ref type="formula" target="#formula_14">10</ref>and 11 with only CGAT's description.</p><p>To verify the functionality of our implementation, we report the results of a full-supervised setting with the original loss (Equation <ref type="formula" target="#formula_17">13</ref>) like CGAT paper, in Table <ref type="table" target="#tab_5">7</ref>. Our implementation of CGAT shows almost the same performance reported in the original paper. In addition, SuperGAT and CGAT showed almost similar performance in a full-supervised setting. Note that the original paper employs two hidden layers with hidden dimensions as 32 for Cora, 64 for CiteSeer, and three hidden layers with hidden dimensions 32 for PubMed, where models in our experiments are all two-layer with 64 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 WALL-CLOCK TIME EXPERIMENTAL SET-UP</head><p>To demonstrate our model's efficiency, we measure the mean wall-clock time of the entire training process of three runs using a single GPU (GeForce GTX 1080Ti). We compare our model with GAT <ref type="bibr" target="#b52">(Veličković et al., 2018)</ref> and GAM <ref type="bibr" target="#b46">(Stretcu et al., 2019)</ref>. GAT is the basic model using a simpler attention mechanism than ours, and GAM is the state-of-the-art model using co-training with the auxiliary model.</p><p>For GAT and SuperGAT, we use our implementation (including hyperparameter settings) in Py-Torch <ref type="bibr" target="#b39">(Paszke et al., 2019)</ref>. For GAM, we adopt the code in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> from the authors<ref type="foot" target="#foot_4">5</ref> and choose GCN + GAM model, which showed the best performance. We retain the default settings in the code but use the hyperparameters reported in the paper, if possible. With this setting, GCN + GAM on PubMed is not finished after 24 hours; therefore, we manually early-stop the training at the best accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 PROOF OF PROPOSITION</head><p>Proposition 2. For l + 1th GAT layer, if W and are independent and identically drawn from zero-mean uniform distribution with variance σ 2 w and σ 2 a respectively, assuming that parameters are independent to input features h l and elements of h l are independent to each other,</p><formula xml:id="formula_19">Var[e l+1 ij,GO ] = 2F l+1 σ 2 w σ 2 a E( h l 2 2 ) and Var[e l+1 ij,DP ] ≥ F l+1 σ 4 w 4 5 E ((h l i ) h l j ) 2 + Var((h l i ) h l j ) (15) Proof. Let h = W h l , then h i,k = F l+1 r=1 W kr h l i,r . Note that, e l+1 ij,GO = a [h i h j ] and e l+1 ij,DP = h i h j<label>(16)</label></formula><p>First, we compute E(a 2 ) and E(h 2 ).</p><formula xml:id="formula_20">E(a 2 ) = Var(a) + E(a) 2 = σ 2 a (17) E(h 2 k ) = E F l+1 r=1 W kr h l •,r 2 (18) = E F l+1 r=1 W 2 kr (h l •,r ) 2 (19) = E W 2 k,• E F l+1 r=1 (h l •,r ) 2 (20) = σ 2 w E h l 2 2 (21)</formula><p>For the variance of e l+1 ij,GO ,</p><p>Var(e l+1 ij,GO ) = Var(a</p><formula xml:id="formula_21">[h i h j ]) (22) = Var F l+1 r=1 (a r h i,r + a r+F l+1 h j,r ) (23) = 2F l+1 Var(ah ) (24) = 2F l+1 E a 2 E h 2 − E (a) 2 E (h ) 2 (25) = 2F l+1 E a 2 E h 2 (26) = 2F l+1 σ 2 a σ 2 w E h l 2 2 (27)</formula><p>Now we compute E(h i h j ) and E(h 2 i h 2 j ),</p><formula xml:id="formula_22">E(h i,k h j,k ) = E F l+1 r=1 W kr h l i,r F l+1 r=1 W kr h l j,r<label>(28)</label></formula><formula xml:id="formula_23">= E F l+1 r=1 W 2 kr h l i,r h l j,r<label>(29)</label></formula><formula xml:id="formula_24">= E(W 2 k,• )E F l+1 r=1 h l i,r h l j,r<label>(30)</label></formula><formula xml:id="formula_25">= σ 2 w E((h l i ) h l j ) (31) E(h 2 i,k h 2 j,k ) (32) = E F l+1 r=1 W kr h l i,r 2 F l+1 W kr h l j,r 2 (33) = E(W 4 k,• )E   F l+1 r=1 (h l i,r h l j,r ) 2   + E(W 2 k,• ) 2 E   F l+1 s =t (h l i,s h l j,t ) 2 + 2(h l i,s h l j,s )(h l i,t h l j,t )   (34) = E(W 4 k,• )E   F l+1 r=1 (h l i,r h l j,r ) 2   + E(W 2 k,• ) 2 E r (h l i,r ) 2 r (h l j,r ) 2 − r (h l i,r h l j,r ) 2 (35) + 2E(W 2 k,• ) 2 E   r h l i,r h l j,r 2 − r h l i,r h l j,r 2   (36) = 9 5 σ 4 w E   F l+1 r=1 (h l i,r h l j,r ) 2   + σ 4 w E   h l i 2 2 h l j 2 2 − 3 F l+1 r=1 (h l i,r h l j,r ) 2   + 2σ 4 w E ((h l i ) h l j ) 2 (37) = σ 4 w E   h l i 2 2 h l j 2 2 − 6 5 F l+1 r=1 (h l i,r h l j,r ) 2 + 2((h l i ) h l j ) 2   (38) = σ 4 w E   4 5 ((h l i ) h l j ) 2 + 1 10 s =t (h l i,s h l j,t + h l i,t h l j,s ) 2 + 8(h l i,s h l j,t ) 2   + σ 4 w E ((h l i ) h l j ) 2<label>(39)</label></formula><p>Note that for the zero-mean uniform distribution U(−u, u) with variance  25.4 ± 0.6 29.2 ± 0.5 37.1 ± 0.5 47.6 ± 1.0 58.1 ± 0.9 73.1 ± 2.8 85.3 ± 2.6 92.7 ± 1.1 97.9 ± 0.5 12.5 24.0 ± 0.4 27.6 ± 0.5 39.6 ± 0.8 48.9 ± 0.4 65.8 ± 3.1 77.8 ± 2.4 88.5 ± 1.5 95.0 ± 0.9 99.2 ± 0.0 15 22.1 ± 0.5 28.2 ± 0.6 44.0 ± 0.7 53.0 ± 0.5 69.9 ± 4.7 79.1 ± 2.0 92.2 ± 1.8 98.0 ± 0.4 99.6 ± 0.3 20 24.3 ± 0.6 30.9 ± 0.7 41.9 ± 0.9 58.1 ± 1.2 75.0 ± 2.9 86.7 ± 1.1 96.1 ± 0.3 98.9 ± 0.1 99.8 ± 0.1 25 26.6 ± 1.0 31.1 ± 0.3 44.9 ± 0.2 62.4 ± 0.7 80.1 ± 2.6 91.0 ± 1.5 97.5 ± 0.5 99.5 ± 0.2 100.0 ± 0.0 32.5 26.3 ± 0.7 33.8 ± 0.7 51.8 ± 0.8 67.5 ± 2.9 83.5 ± 1.9 95.7 ± 0.3 98.4 ± 0.2 100.0 ± 0.0 100.0 ± 0.0 40 23.7 ± 0.5 34.2 ± 0.5 53.4 ± 0.4 72.8 ± 1.5 87.6 ± 0.9 96.1 ± 0.6 99.7 ± 0.1 99.9 ± 0.0 100.0 ± 0.0 50 25.0 ± 1.1 36.4 ± 1.0 55.1 ± 0.5 82.7 ± 3.0 91.2 ± 0.7 98.6 ± 0.4 99.8 ± 0.0 99.9 ± 0.0 100.0 ± 0.0 75 25.9 ± 0.6 40.6 ± 0.6 68.0 ± 1.0 87.9 ± 2.6 97.1 ± 0.6 99.6 ± 0.1 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100 25.1 ± 0.5 42.0 ± 0.9 72.4 ± 1.4 92.6 ± 0.4 98.6 ± 0.2 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0. 31.9 ± 1.6 34.6 ± 1.6 49.5 ± 0.9 62.0 ± 0.7 77.2 ± 1.5 85.9 ± 0.3 94.4 ± 0.9 98.5 ± 0.5 99.4 ± 0.1 20 34.4 ± 1.8 38.3 ± 1.6 54.1 ± 1.9 70.0 ± 1.1 83.9 ± 1.0 93.7 ± 0.1 97.5 ± 0.3 99.0 ± 0.3 99.7 ± 0.0 25 35.8 ± 2.0 42.0 ± 2.4 57.7 ± 0.6 77.7 ± 0.9 87.9 ± 1.2 95.0 ± 0.7 98.7 ± 0.6 99.6 ± 0.3 99.9 ± 0.1 32.5 37.4 ± 1.2 44.7 ± 1.1 66.5 ± 1.9 79.9 ± 1.2 91.4 ± 1.4 98.0 ± 0.5 99.0 ± 0.3 99.8 ± 0.1 100.0 ± 0.0 40 37.5 ± 2.0 45.1 ± 0.9 66.5 ± 1.1 85.7 ± 1.5 93.5 ± 1.0 97.6 ± 0.6 99.5 ± 0.1 99.9 ± 0.1 99.9 ± 0.1 50 38.7 ± 1.8 49.5 ± 2.3 68.9 ± 2.5 89.5 ± 0.8 96.0 ± 0.8 99.0 ± 0.4 99.7 ± 0.2 99.8 ± 0.1 100.0 ± 0.0 75 39.6 ± 3.0 53.5 ± 1.7 77.6 ± 2.6 92.8 ± 2.3 98.0 ± 1.2 99.5 ± 0.3 99.8 ± 0.2 100.0 ± 0.0 100.0 ± 0.0 100 41.3 ± 1.2 56.6 ± 1.4 81.1 ± 3.5 95.9 ± 1.2 99.0 ± 0.4 99.8 ± 0.1 99.9 ± 0.1 100.0 ± 0.1 100.0 ± 0.0 SuperGAT SD 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 38.5 ± 1.0 40.5 ± 1.5 39.5 ± 1.6 42.3 ± 0.9 44.0 ± 0.5 47.1 ± 0.5 50.1 ± 1.0 53.0 ± 0.3 55.1 ± 1.0 1.5 36.7 ± 1.2 37.8 ± 1.8 42.0 ± 0.4 42.9 ± 0.8 45.0 ± 0.7 47.8 ± 0.5 52.9 ± 0.7 54. 37.8 ± 0.8 42.1 ± 0.9 56.9 ± 0.9 70.0 ± 0.5 86.1 ± 0.8 95.6 ± 0.3 99.1 ± 0.1 99.6 ± 0.2 99.7 ± 0.1 25 40.0 ± 1.0 48.8 ± 1.2 59.5 ± 0.5 78.7 ± 0.2 90.7 ± 0.2 97.5 ± 0.1 99.4 ± 0.1 100.0 ± 0.0 99.9 ± 0.1 32.5 39.7 ± 1.1 48.5 ± 0.7 69.4 ± 0.4 82.7 ± 0.5 94.3 ± 0.2 99.3 ± 0.1 99.7 ± 0.1 99.9 ± 0.1 99.9 ± 0.0 40 44.2 ± 1.1 48.7 ± 1.3 69.2 ± 0.7 88.3 ± 0.4 97.1 ± 0.1 99.7 ± 0.1 99.9 ± 0.0 99.8 ± 0.2 100.0 ± 0.0 50 44.3 ± 0.7 53.2 ± 0.6 73.4 ± 1.2 91.2 ± 0.4 97.7 ± 0.2 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 75 44.8 ± 0.7 56.1 ± 0.9 82.7 ± 0.6 95.7 ± 0.2 99.8 ± 0.2 99.9 ± 0.1 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100 43.1 ± 1.3 60.7 ± 0.7 87.4 ± 0.3 98.3 ± 0.1 99.9 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 SuperGAT MX 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 33.4 ± 0.7 36.3 ± 0.8 35.4 ± 0.7 40.5 ± 0.3 43.4 ± 0.6 47.0 ± 0.8 50.0 ± 0.5 54.2 ± 1.1 56.4 ± 0.4 1.5</p><formula xml:id="formula_26">σ 2 w , E(W 2 •,• ) = Var(W •,• ) + E(W •,• ) 2 = σ 2 w (<label>40</label></formula><formula xml:id="formula_27">)</formula><formula xml:id="formula_28">σ 2 w = 1 12 (u − (−u)) 2 = 1 3 u 2 (41) E(W 4 •,• ) =</formula><formula xml:id="formula_29">= Var h i h j (44) = Var F l+1 r=1 h i,r h j,r<label>(43)</label></formula><formula xml:id="formula_30">= F l+1 Var h i h j (46) = F l+1 E((h i h j ) 2 ) − E(h i h j ) 2 (47) = F l+1 σ 4 w E   4 5 ((h l i ) h l j ) 2 + 1 10 s =t (h l i,s h l j,t + h l i,t h l j,s ) 2 + 8(h l i,s h l j,t ) 2   (48) + F l+1 σ 4 w E ((h l i ) h l j ) 2 − E((h l i ) h l j ) 2 (49) ≥ F l+1 σ 4 w 4 5 E ((h l i ) h l j ) 2 + Var((h l i ) h l j )<label>(45)</label></formula><p>28.8 ± 0.7 31.5 ± 1.0 36.0 ± 0.8 41.1 ± 1.1 41.3 ± 0.6 47.5 ± 0.8 52.3 ± 0.7 54.8 ± 0.5 63.4 ± 0.3 2.5 26.6 ± 0.9 30.5 ± 1.3 33.6 ± 0.8 42.7 ± 1.5 44.7 ± 0.5 52.2 ± 0.5 56.5 ± 0.8 66.7 ± 0.8 74.5 ± 0.7 3.5 27.7 ± 0.9 33.9 ± 1.7 37.9 ± 0.7 42.4 ± 0.6 49.7 ± 0.8 54. 36.4 ± 1.3 39.9 ± 2.1 53.8 ± 1.5 68.5 ± 0.8 81.0 ± 0.4 90.2 ± 1.0 96.1 ± 0.9 99.2 ± 0.2 99.6 ± 0.1 20 35.2 ± 3.1 41.0 ± 2.1 62.3 ± 1.2 73.9 ± 0.9 86.3 ± 0.6 95.2 ± 1.3 98.6 ± 0.7 99.6 ± 0.1 99.9 ± 0.2 25 36.8 ± 1.9 46.6 ± 1.7 62.7 ± 1.1 81.9 ± 1.7 90.7 ± 0.8 96.8 ± 1.5 99.3 ± 0.3 99.8 ± 0.2 100.0 ± 0.0 32.5 36.7 ± 0.8 46.8 ± 0.5 68.9 ± 0.6 83.8 ± 0.7 93.7 ± 1.0 98.4 ± 0.7 99.8 ± 0.1 100.0 ± 0.0 100.0 ± 0.0 40 39.8 ± 2.4 48.8 ± 1.7 72.1 ± 0.8 87.5 ± 1.7 96.6 ± 0.8 98.9 ± 0.3 99.9 ± 0.1 100.0 ± 0.0 100.0 ± 0.0 50 42.1 ± 1.3 53.6 ± 2.3 75.1 ± 1.9 92.7 ± 1.0 97.1 ± 0.9 99.6 ± 0.2 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 75 41.9 ± 1.1 55.9 ± 2.4 80.8 ± 1.2 95.0 ± 1.1 99.5 ± 0.2 99.9 ± 0.1 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100 39.6 ± 2.0 59.2 ± 1.8 84.2 ± 3.1 96.9 ± 0.7 99.7 ± 0.1 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 FULL RESULT OF SYNTHETIC GRAPH EXPERIMENTS</head><p>In Table <ref type="table" target="#tab_6">8</ref>, we report all results of synthetic graph experiments. We experiment on a total of 144 synthetic graphs controlling 9 homophily (0.1, 0.2, ..., 0.9) and 16 average degree <ref type="bibr">(1, 1.5, 2.5, 3.5, 5, 7.5, 10, 12.5, 15, 20, 25, 32.5, 40, 50, 75, 100)</ref>.  In Figure <ref type="figure" target="#fig_7">8</ref>, we draw box plots of KL divergence between attention distribution and label agreement distribution for all nodes and layers of two-layer GATs and four-layer GATs. As shown in the paper, we can see that DP attention does not capture label-agreement rather than GO attention. Also, the degree of this phenomenon becomes stronger as the layer goes down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 WALL-CLOCK TIME RESULT</head><p>In Table <ref type="table" target="#tab_10">9</ref>, we report the mean wall-clock time (over three runs) of the training of GAT, GAM, and SuperGAT MX . In SuperGAT, we find that negative sampling of edges is the bottleneck of training. So, we additionally implement SuperGAT MX + MPNS, which employs multi-processing when sampling negative edges. There are three observations in this experiment. GCN + GAM is highly time-intensive in the training stage (×53.9 -×328.1 versus GAT) for all datasets. Compared to GAT, our model needs ×2.7 more training time for Cora and ×7.2 for PubMed, and we reduce the time by applying multi-processing to negative sampling (×1.8 for Cora and ×4.0 for PubMed). For CiteSeer, we can see that SuperGAT MX ends faster than GAT because of faster convergence and fewer epochs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of KL divergence between normalized attention and label-agreement on all nodes and layers for Cora dataset (Left: two-layer GAT, Right: four-layer GAT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure3: Test performance on node classification and link prediction for GO and DP attentions against the mixing coefficient λ E . We report accuracy (Cora, CiteSeer, PubMed) and micro f1-score (PPI) for node classification, and AUC for link prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Photo</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. In this figure, we can observe that the average degree (d avg = n • δ) increases as δ increases, and homophily (h = p in /δ) increases as p in increases. Note that these are raw input features sampled from the 2D Gaussian distribution, not learned node representation. We use the code from the prior work 4 (Abu-El-Haija et al., 2019) and apply normalization by standard score. We choose δ from {0.025, 0.2}, p in from {0.1δ, 0.5δ, 0.9δ}, and fix n = 100 and c = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: t-SNE plots of node features and edges for synthetic graph examples. Hyperparameters are δ ∈ {0.025 (Top), 0.2 (Bottom)} and p in ∈ {0.1δ (Left), 0.5δ (Center), 0.9δ (Right)}.</figDesc><graphic url="image-18.png" coords="17,119.37,491.18,122.75,92.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Kernel density estimate plot of distribution of degree and per-node homophily in realworld graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Distribution of KL divergence between normalized attention and label-agreement on all nodes and layers for Cora, CiteSeer, PubMed, and PPI (Left: two-layer GAT, Right: four-layer GAT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of classification accuracies of GCN, GraphSAGE, GAT, SuperGAT SD , and SuperGAT MX for real-world datasets (30 runs for ogbn-arxiv and Flickr, and 100 runs for others).</figDesc><table><row><cell>Model</cell><cell cols="2">ogbn-arxiv CS</cell><cell cols="2">Physics Cora-ML Cora-Full DBLP</cell><cell>Cham.</cell><cell cols="2">Four-Univ Wiki-CS Photo</cell><cell>Comp.</cell><cell>Flickr</cell><cell>Croco.</cell></row><row><cell>GCN</cell><cell>33.3±1.2</cell><cell cols="7">91.5±0.2 92.5±0.2 85.0±0.4 59.5±0.2 77.8±0.5 33.1±0.9 74.8±0.6 74.0±1.0 91.6±0.6 84.5±1.4 51.2±0.4 32.6±0.4</cell></row><row><cell cols="2">GraphSAGE 54.6±0.3</cell><cell cols="7">90.0±0.1 92.2±0.1 83.7±0.4 59.2±0.2 78.7±0.6 41.0±0.9 74.4±0.6 77.5±0.5 90.4±1.1 83.0±1.4 50.7±0.2 53.0±1.0</cell></row><row><cell>GAT</cell><cell>54.1±0.5</cell><cell cols="7">89.5±0.2 91.2±0.6 83.2±0.6 58.7±0.3 78.2±1.5 40.8±0.7 74.2±0.7 77.6±0.6 91.8±0.6 85.7±0.9 50.9±0.2 53.3±1.0</cell></row><row><cell cols="2">SuperGATSD 54.5  *  *  ±0.3</cell><cell cols="2">88.8 ↓ ±0.4 91.6  *  *  ±0.5 84.5  *  *  ±0.4 55.8 ↓ ±0.6</cell><cell cols="3">79.4  *  *  ±0.8 41.6  *  *  ±0.7 76.2  *  *  ±0.8</cell><cell cols="2">77.9±0.7 86.8 ↓ ±2.5 82.2 ↓ ±0.9 45.1 ↓ ±1.2 53.3±0.9</cell></row><row><cell cols="2">SuperGATMX 55.1  *  *  ±0.2</cell><cell cols="5">90.2  *  *  ±0.2 91.9  *  *  ±0.5 84.7  *  *  ±0.4 59.6  *  *  ±0.2 80.7  *  *  ±0.7 42.0  *  *  ±0.8 75.3  *  *  ±0.6</cell><cell cols="2">77.9  *  ±0.5 91.8±0.9 85.7±1.1 50.8 ↓ ±0.2 53.3±0.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of classification accuracies with 100 random seeds for Cora, CiteSeer, and PubMed. We mark with daggers ( †) the reprinted results from the respective papers.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Table 3: Summary of micro f1-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">scores with 30 random seeds for</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PPI.</cell><cell></cell></row><row><cell>Model</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell cols="2">Model</cell><cell>PPI</cell></row><row><cell>GCN  †</cell><cell>81.5</cell><cell>70.3</cell><cell>79.0</cell><cell>GCN</cell><cell></cell><cell>61.5 ± 0.4</cell></row><row><cell>GraphSAGE</cell><cell cols="3">82.1 ± 0.6 71.9 ± 0.9 78.0 ± 0.7</cell><cell cols="2">GraphSAGE</cell><cell>59.0 ± 1.2</cell></row><row><cell>CGAT</cell><cell cols="3">81.4 ± 1.1 70.1 ± 0.9 78.1 ± 1.0</cell><cell cols="2">CGAT</cell><cell>68.3 ± 1.7</cell></row><row><cell>GLCN  †</cell><cell>85.5</cell><cell>72.0</cell><cell>78.3</cell><cell>GAT</cell><cell></cell><cell>72.2 ± 0.6</cell></row><row><cell>LDS  †</cell><cell>84.1</cell><cell>75</cell><cell>-</cell><cell cols="2">SuperGATSD</cell><cell>74.4  *  *  ±0.4</cell></row><row><cell cols="4">GCN + GAM  † 86.2 83.7 ± 1.4 74.1 ± 1.4 -73.5 86.0 GCN + NS  †</cell><cell cols="3">SuperGATMX 67.2 ↓ ±1.2</cell></row><row><cell>GAT  † SuperGATSD SuperGATMX</cell><cell cols="3">83.0 ± 0.7 72.5 ± 0.7 79.0 ± 0.4 82.7 ↓ ±0.6 72.5±0.8 81.3  *  *  ±0.5 84.3  *  *  ±0.6 72.6±0.8 81.7  *  *  ±0.5</cell><cell>** * ↓ Color</cell><cell cols="2">p-value &lt; .0001 p-value &lt; .0005 Worse than GATGO</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen,  and Wei Wang.  Robust graph representation learning via neural sparsification. In International Conference on Machine Learning, 2020. Average degree and homophily of real-world graphs.</figDesc><table><row><cell>Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,</cell></row><row><cell>and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint</cell></row><row><cell>arXiv:1812.08434, 2018.</cell></row><row><cell>Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue</cell></row><row><cell>networks. Bioinformatics, 33(14):i190-i198, 2017.</cell></row></table><note>links between them. Node features are a bag-of-words representation with informative nouns in the article. The number of features is 13183, but we use a reduced dimension of 500 by PCA. The split is 20-per-class/30-per-class/rest from<ref type="bibr" target="#b44">Shchur et al. (2018)</ref>. The original dataset is for the regression task to predict monthly traffic, but we group values into six bins and make it a classification problem.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the real-world datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2"># Nodes # Edges</cell><cell cols="3"># Features # Classes Split</cell><cell cols="3"># Training Nodes # Val. Nodes # Test Nodes</cell></row><row><cell>Four-Univ</cell><cell>4518</cell><cell>3426</cell><cell>2000</cell><cell>6</cell><cell>fixed</cell><cell>4014 (3 Gs)</cell><cell>248 (1 G)</cell><cell>256 (1 G)</cell></row><row><cell>PPI</cell><cell>56944</cell><cell>818716</cell><cell>50</cell><cell>121</cell><cell>fixed</cell><cell>44906 (20 Gs)</cell><cell>6514 (2 Gs)</cell><cell>5524 (2 Gs)</cell></row><row><cell cols="2">Chameleon 2277</cell><cell>36101</cell><cell>500</cell><cell>6</cell><cell cols="2">random 120</cell><cell>180</cell><cell>1977</cell></row><row><cell>Crocodile</cell><cell>11631</cell><cell>180020</cell><cell>500</cell><cell>6</cell><cell cols="2">random 120</cell><cell>180</cell><cell>11331</cell></row><row><cell>Flickr</cell><cell>89250</cell><cell>449878</cell><cell>500</cell><cell>7</cell><cell>fixed</cell><cell>44625</cell><cell>22312</cell><cell>22313</cell></row><row><cell>Cora-Full</cell><cell>19793</cell><cell>63421</cell><cell>500</cell><cell>70</cell><cell cols="2">random 1395</cell><cell>2049</cell><cell>16349</cell></row><row><cell cols="2">ogbn-arxiv 169343</cell><cell cols="2">1166243 128</cell><cell>40</cell><cell>fixed</cell><cell>90941</cell><cell>29799</cell><cell>48603</cell></row><row><cell>Wiki-CS</cell><cell>11701</cell><cell>297110</cell><cell>300</cell><cell>10</cell><cell>fixed</cell><cell>580</cell><cell>1769</cell><cell>5847</cell></row><row><cell>CiteSeer</cell><cell>3327</cell><cell>4732</cell><cell>3703</cell><cell>6</cell><cell>fixed</cell><cell>120</cell><cell>500</cell><cell>1000</cell></row><row><cell>PubMed</cell><cell>19717</cell><cell>44338</cell><cell>500</cell><cell>3</cell><cell>fixed</cell><cell>60</cell><cell>500</cell><cell>1000</cell></row><row><cell>Cora-ML</cell><cell>2995</cell><cell>8158</cell><cell>2879</cell><cell>7</cell><cell cols="2">random 140</cell><cell>210</cell><cell>2645</cell></row><row><cell>DBLP</cell><cell>17716</cell><cell>52867</cell><cell>1639</cell><cell>4</cell><cell cols="2">random 80</cell><cell>120</cell><cell>17516</cell></row><row><cell cols="2">Computers 13752</cell><cell>245861</cell><cell>767</cell><cell>10</cell><cell cols="2">random 200</cell><cell>300</cell><cell>13252</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>1433</cell><cell>7</cell><cell>fixed</cell><cell>140</cell><cell>500</cell><cell>1000</cell></row><row><cell>CS</cell><cell>18333</cell><cell>81894</cell><cell>500</cell><cell>15</cell><cell cols="2">random 300</cell><cell>450</cell><cell>17583</cell></row><row><cell>Photo</cell><cell>7650</cell><cell>119081</cell><cell>745</cell><cell>8</cell><cell cols="2">random 160</cell><cell>240</cell><cell>7250</cell></row><row><cell>Physics</cell><cell>34493</cell><cell>247962</cell><cell>500</cell><cell>5</cell><cell cols="2">random 100</cell><cell>150</cell><cell>34243</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for experiments on real-world datasets.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>λ 2</cell><cell>λ E</cell><cell>p e</cell><cell>p n</cell></row><row><cell>Cora</cell><cell cols="2">SuperGAT 0.007829935945</cell><cell>10.88266937</cell><cell cols="2">0.8 0.5</cell></row><row><cell></cell><cell cols="2">SuperGAT MX 0.008228864973</cell><cell>11.34657453</cell><cell cols="2">0.8 0.5</cell></row><row><cell cols="3">CiteSeer SuperGAT SD 0.04823808657</cell><cell>0.09073992828</cell><cell cols="2">0.8 0.3</cell></row><row><cell></cell><cell cols="2">SuperGAT MX 0.04161321832</cell><cell>0.01308169273</cell><cell cols="2">0.8 0.5</cell></row><row><cell cols="4">PubMed SuperGAT SD 0.0002030927563 18.82560333</cell><cell cols="2">0.6 0.7</cell></row><row><cell></cell><cell cols="2">SuperGAT MX 2.19E-04</cell><cell>10.4520518</cell><cell cols="2">0.6 0.5</cell></row><row><cell>PPI</cell><cell cols="2">SuperGAT SD 3.39E-07</cell><cell cols="2">0.001034351842 1</cell><cell>0.5</cell></row><row><cell></cell><cell cols="2">SuperGAT MX 1.00E-07</cell><cell>1.79E-06</cell><cell>1</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Summary of classification accuracies with 10 random seeds for Cora, CiteSeer, and PubMed in the full-supervised setting. We mark with asterisks the reprinted results from the respective papers.</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>GAT*</cell><cell cols="3">87.2 ± 0.3 77.3 ± 0.3 87.0 ± 0.3</cell></row><row><cell>CGAT*</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Summary of classification accuracies (of 5 runs) for synthetic datasets.</figDesc><table><row><cell>Avg. degree</cell><cell></cell><cell></cell><cell></cell><cell>Homophily</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row><row><cell>1</cell><cell cols="5">32.7 ± 1.1 35.1 ± 0.4 34.7 ± 0.4 40.8 ± 0.4 43.4 ± 0.3 46.2 ± 0.5</cell><cell>49.7 ± 0.5</cell><cell>54.8 ± 0.9</cell><cell>56.2 ± 0.3</cell></row><row><cell>1.5</cell><cell cols="5">28.7 ± 0.5 30.0 ± 0.6 34.6 ± 0.4 40.8 ± 0.4 41.7 ± 0.4 46.9 ± 0.5</cell><cell>52.1 ± 0.6</cell><cell>54.5 ± 0.5</cell><cell>63.6 ± 0.5</cell></row><row><cell>2.5</cell><cell cols="5">24.6 ± 0.2 27.9 ± 0.9 31.0 ± 0.3 39.7 ± 0.6 44.0 ± 0.6 51.4 ± 0.8</cell><cell>56.6 ± 0.5</cell><cell>65.2 ± 0.6</cell><cell>74.5 ± 0.4</cell></row><row><cell>3.5</cell><cell cols="5">25.8 ± 0.8 30.5 ± 0.8 31.9 ± 0.7 38.4 ± 0.4 45.0 ± 1.0 49.8 ± 0.5</cell><cell>60.8 ± 0.7</cell><cell>70.3 ± 0.3</cell><cell>79.5 ± 0.3</cell></row><row><cell>5</cell><cell cols="5">25.0 ± 0.3 26.3 ± 0.5 33.0 ± 1.1 39.8 ± 0.7 50.2 ± 0.5 58.6 ± 1.1</cell><cell>68.8 ± 0.3</cell><cell>78.0 ± 1.9</cell><cell>88.5 ± 0.2</cell></row><row><cell>7.5</cell><cell cols="5">26.1 ± 0.5 29.8 ± 0.6 34.0 ± 0.6 43.8 ± 0.9 52.0 ± 0.4 70.7 ± 3.2</cell><cell>74.6 ± 2.4</cell><cell>88.6 ± 0.8</cell><cell>95.0 ± 0.2</cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Mean wall-clock time (seconds) of three runs of the training process on real-world datasets.</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>GAT</cell><cell>11.3 ± 2.7</cell><cell>20.4 ± 6.7</cell><cell>21.1 ± 2.0</cell></row><row><cell>GCN + GAM</cell><cell cols="3">709.3 ± 235.9 1099.3 ± 812.5 6923.3 ± 7042.0</cell></row><row><cell>SuperGAT MX</cell><cell>30.8 ± 0.5</cell><cell>19.3 ± 1.1</cell><cell>151.4 ± 11.4</cell></row><row><cell cols="2">SuperGAT MX + MPNS 20.8 ± 0.2</cell><cell>15.2 ± 0.1</cell><cell>84.6 ± 0.9</cell></row><row><cell cols="4">B.3 LABEL-AGREEMENT STUDY FOR OTHER DATASETS AND DEEPER MODELS</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Since CGAT uses node labels in the loss function, it is difficult to use it in semi-supervised learning. So, we modify its auxiliary loss for SSL. See appendix A.6 for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://docs.dgl.ai/en/latest/tutorials/models/1_gnn/9_gat.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/samihaija/mixhop/blob/master/data/synthetic/make_x. py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/tensorflow/neural-structured-learning/tree/master/ research/gam</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by the Engineering Research Center Program through the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF-2018R1A5A1059921)</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL SET-UP</head><p>A.1 REAL-WORLD DATASET In this section, we describe details (including nodes, edges, features, labels, and splits) of real-world datasets. We report statistics of real-world datasets in Tables <ref type="table">4 and 5</ref>. For multi-label graphs (PPI), we extend the homophily to the average of the ratio of shared labels on neighbors over nodes, i.e., h = 1</p><p>, where C i is a set of labels for node i and C is a set of all labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 CITATION NETWORK</head><p>We use a total of 7 citation network datasets. Nodes are documents, and edges are citations. The task for all citation network datasets is to classify each paper's topic.</p><p>Cora, CiteSeer, PubMed We use three benchmark datasets for semi-supervised node classification tasks in the transductive setting <ref type="bibr" target="#b43">(Sen et al., 2008;</ref><ref type="bibr" target="#b57">Yang et al., 2016)</ref>. The features of the nodes are bag-of-words representations of documents. We follow the train/validation/test split of previous work <ref type="bibr" target="#b27">(Kipf &amp; Welling, 2017)</ref>. We use 20 samples per class for training, 500 samples for the validation, and 1000 samples for the test.</p><p>Cora-ML, Cora-Full, DBLP These are other citation network datasets from <ref type="bibr">Bojchevski &amp; Günnemann (2018)</ref>. Node features are bag-of-words representations of documents. For CoraFull with features more than 5000, we reduce the dimension to 500 by performing PCA. We use the split setting in <ref type="bibr" target="#b44">Shchur et al. (2018)</ref>: 20 samples per class for training, 30 samples per class for validation, the rest for the test.</p><p>ogbn-arxiv The ogbn-arxiv is a recently proposed large-scale dataset of citation networks <ref type="bibr" target="#b21">(Hu et al., 2020a;</ref><ref type="bibr" target="#b54">Wang et al., 2020)</ref>. Nodes represent arXiv papers, and edges indicate citations between papers, and node features are mean vectors of skip-gram word embeddings of their titles and abstracts. We use the public split by publication dates provided by the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 CO-AUTHOR NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS, Physics</head><p>The CS and Physics are co-author networks in each domain <ref type="bibr" target="#b44">(Shchur et al., 2018)</ref>. Nodes are authors, and edges mean whether two authors co-authored a paper. Node features are paper keywords from the author's papers, and we reduce the original dimension (6805 and 8415) to 500 using PCA. The split is the 20-per-class/30-per-class/rest from <ref type="bibr" target="#b44">Shchur et al. (2018)</ref>. The goal of this task is to classify each author's respective field of study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 AMAZON CO-PURCHASE</head><p>Photo, Computers The Photo and Computers are parts of the Amazon co-purchase graph <ref type="bibr" target="#b34">(McAuley et al., 2015;</ref><ref type="bibr" target="#b44">Shchur et al., 2018)</ref>. Nodes are goods, and edges indicate whether two goods are frequently purchased together, and node features are a bag-of-words representation of product reviews. The split is the 20-per-class/30-per-class/rest from <ref type="bibr" target="#b44">Shchur et al. (2018)</ref>. The task is to classify the categories of goods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 WEB PAGE NETWORK</head><p>Wiki-CS The Wiki-CS dataset is computer science related page networks in Wikipedia <ref type="bibr" target="#b35">(Mernyei &amp; Cangea, 2020)</ref>. Nodes represent articles about computer science, and edges represent hyperlinks between articles. The features of nodes are mean vectors of GloVe word embeddings of articles. There are 20 standard splits, and we experiment with five random seeds for each split (total 100 runs). The task is to classify the main category of articles.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chameleon, Crocodile</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 SENSITIVITY ANALYSIS OF HYPER-PARAMETERS</head><p>We analyze sensitivity of mixing coefficient of losses λ E , negative sampling ratio p n , and edge sampling ratio p e . We plot mean node classification performance (over 5 runs) against each hyperparameter in Figure <ref type="figure">9</ref>, 10, and 11 respectively. We use the best model for each dataset: SuperGAT MX for citation networks and SuperGAT SD for PPI.</p><p>For λ E , there is a specific range that maximizes test performance in all datasets. Performance on PPI is the largest when λ E is 10 −3 , but the difference is relatively small comparing to others. We observe that there is an optimal level of the edge supervision for each dataset, and using too large λ E degrades node classification performance.</p><p>For p n , using too many negative samples has been shown to decrease performance. The optimal number of negative samples is different for each dataset, and all are less than the number of positive samples (p n &lt; 1.0). Note that as p n increases, the required GPU memory also increases. When p n = 5.0, the model and data for PPI could not be accommodated by one single GPU (GeForce GTX 1080Ti).</p><p>When p e changes, the performance also changes, but the pattern is different by datasets. For Cora and PubMed, the performance against p e shows the convex curve. Performance for CiteSeer generally decreases as p e increases, but there are intervals the performance change of which is nearly zero. In the case of PPI, there are no noticeable changes against p e .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,</pubPlace>
		</imprint>
	</monogr>
	<note>TensorFlow: Large-scale machine learning on heterogeneous systems</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2010">2010. 2016</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="726" to="732" />
		</imprint>
	</monogr>
	<note>Notices of the AMS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to extract symbolic knowledge from the world wide web</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Dipasquo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayne</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seán</forename><surname>Slattery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence, AAAI &apos;98/IAAI &apos;98</title>
				<meeting>the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence, AAAI &apos;98/IAAI &apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Community detection in graphs</title>
		<author>
			<persName><forename type="first">Santo</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reports</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="issue">3-5</biblScope>
			<biblScope unit="page" from="75" to="174" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph representation learning via hard and channel-wise attention networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="741" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring network structure, dynamics, and function using networkx</title>
		<author>
			<persName><forename type="first">Aric</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Swart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Chult</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Los Alamos, NM (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Los Alamos National Lab.(LANL)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring and improving the use of graph information in graph neural networks</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Tb</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Collaborative graph convolutional networks: Unsupervised learning meets semi-supervised learning</title>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4215" to="4222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with graph learning-convolutional networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doudou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11313" to="11320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4204" to="4214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine learning Workshop on Deep Learning for Audio, Speech and Language Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image labeling on a network: using social-network metadata for image classification</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="828" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval</title>
				<meeting>the 38th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Wiki-cs: A wikipedia-based benchmark for graph neural networks</title>
		<author>
			<persName><forename type="first">Péter</forename><surname>Mernyei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Cangea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02901</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><surname>S Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Long</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Symmetric graph convolutional autoencoder for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">Jiwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuewang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6519" to="6528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10694</idno>
		<title level="m">Deep learning is robust to massive label noise</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13021</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph agreement models for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Otilia</forename><surname>Stretcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnamurthy</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8710" to="8720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Vamsi K Mootha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Gillette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">L</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">R</forename><surname>Pomeroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><surname>Lander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="15545" to="15550" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-stage self-supervised learning for graph convolutional networks on graphs with few labeled nodes</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5892" to="5899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Line: Largescale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Attention-based graph neural network for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Kiran K Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03735</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Improving graph attention networks with large margin-based constraints</title>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11945</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.01315" />
		<title level="m">Deep graph library: Towards efficient and scalable deep learning on graphs. International Conference on Learning Representations Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Supervising neural attention models for video captioning by human gaze data</title>
		<author>
			<persName><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeonhwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Hun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="490" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adaptive structural fingerprints for graph attention networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaokang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
