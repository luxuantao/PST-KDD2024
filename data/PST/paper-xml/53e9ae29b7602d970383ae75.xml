<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Causes of Bloat, The Limits of Health</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nick</forename><surname>Mitchell</surname></persName>
							<email>nickm@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">IBM T.J. Watson Research Center 19 Skyline Drive Hawthorne</orgName>
								<address>
									<postCode>10532</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gary</forename><surname>Sevitsky</surname></persName>
							<email>sevitsky@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">IBM T.J. Watson Research Center 19 Skyline Drive Hawthorne</orgName>
								<address>
									<postCode>10532</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Causes of Bloat, The Limits of Health</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D4E84105A72413AFA681B6330071243A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.2.8 Metrics [performance measures] General Terms Memory Footprint</term>
					<term>Data Structure Design</term>
					<term>Characterization bloat</term>
					<term>memory footprint</term>
					<term>metrics</term>
					<term>limit studies</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Applications often have large runtime memory requirements. In some cases, large memory footprint helps accomplish an important functional, performance, or engineering requirement. A large cache, for example, may ameliorate a pernicious performance problem. In general, however, finding a good balance between memory consumption and other requirements is quite challenging. To do so, the development team must distinguish effective from excessive use of memory.</p><p>We introduce health signatures to enable these distinctions. Using data from dozens of applications and benchmarks, we show that they provide concise and applicationneutral summaries of footprint. We show how to use them to form value judgments about whether a design or implementation choice is good or bad. We show how being independent of any application eases comparison across disparate implementations. We demonstrate the asymptotic nature of memory health: certain designs are limited in the health they can achieve, no matter how much the data size scales up. Finally, we show how to use health signatures to automatically generate formulas that predict this asymptotic behavior, and show how they enable powerful limit studies on memory health.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is fairly easy these days to design and implement a data model in a way that consumes a large amount of memory. For example, we have seen Java server applications that require a gigabyte of memory to support a few thousand users. That a data structure is big is of course a sign for concern. We propose that the health of a data structure's use of memory depends not so much on its size, but rather on the relationship between actual data and structural overhead.</p><p>The makeup of memory depends on the design of data types, and the way instances of these types are glued together into collections <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Consider an example data model with entities E 1 , E 2 , and E 3 ; each instance of E 3 has a single byte field of actual data, each E 2 contains one or more E 3 instances, and each E 1 contains one or more E 2 instances. Developers must choose how to map this model to a physical implementation. In an object oriented language, one typically maps each entity to a class, and uses a standard collection, such as the Java LinkedList, to store the multivalued relationships. Unfortunately, this implementation results in an extremely unhealthy heap. With fan-out of 1000 at each level, this implementation's fraction of actual data is only 10%. Furthermore, as Section 4 shows, the fraction of actual data can never be better than 10% -no matter how many instances of E 2 or E 3 there are to amortize bookkeeping overheads.</p><p>In this paper, we first introduce a scheme for exposing the underlying causes of such poor health. Second, we leverage this scheme to introduce a way to determine whether a data design will ever be good.</p><p>The Causes of Bloat Depending on the data type design, each instance will use its bytes for a variety of purposes. Section 2.1 introduces an instance health categorization system that divides the bytes of each instance into one of four application-neutral <ref type="bibr" target="#b17">[18]</ref> categories: primitive data, header bytes, pointers, and null pointers. For example, each instance of E 3 above has only a single byte of primitive data, but 12 bytes of JVM-imposed object header.</p><p>Depending on the collection design, each instance will serve one of a number of purposes. For example, each in-stance of LinkedList serves as a wrappers to the collection as a whole. A consequence of this design is that, when LinkedLists are nested <ref type="bibr" target="#b1">[2]</ref>, the wrapper cost is magnified. Section 2.2 introduces a collection health categorization system that divides instances into one of four applicationneutral categories based on the role they play in collections.</p><p>Combining these two classification schemes yields deeper insight into whether bytes are serving a useful purpose. For example, when we contrast the two primitive fields in each LinkedList instance, used for bookkeeping purposes, with the primitive byte field in E 3 , we see that not all primitive data should be judged equally. The role of an object can give valuable clues into the purpose of its bytes. Section 2.3 introduces the health signature, the composition of the collection and instance health categorizations; it is formed by intersecting the two categorizations. We present health signatures from a variety of applications and benchmarks.</p><p>To tie questions about features of a design to their memory consequences, Section 3 introduces health judgment schemes. A judgment scheme maps a set of features to the design's health signature. Features of interest will vary depending on the analysis. We introduce two schemes. The overhead judgment scheme highlights, as features of the design, the various ways overhead is introduced. For example, we can analyze the amount of pointer overhead that a design incurs. Section 3.1 shows that a HashMap of 2-characters Strings will devote 29% of its space to pointer overhead.</p><p>The scaling judgment scheme is focused on collections, and the fixed and per-element costs they incur. The memory impact of collection choices can be difficult to predict, particularly when using standard collections whose implementations are hidden. The scaling judgment scheme helps to surface these costs. For example, Section 3.3 shows a real application that devotes a surprising 74% of its memory to collection fixed and per-element costs.</p><p>Another benefit of judgment schemes is as a concise health summary. We present these summaries for dozens of applications, to illustrate the great variety of health characteristics. The application neutrality of the health signature enables comparison of these disparate applications. In addition, Section 3.3 provides a detailed example of how each scheme offers distinct insights into a real-world memory problem.</p><p>The Limits of Health From a health judgment, we can derive a health metric: the ratio of actual data to the total. We have observed that this ratio has asymptotic behavior. For example, our LinkedList of LinkedLists of E 3 objects will never contain more than 10% actual data; the application discussed in Section 3.3 can never contain more than 17%. Section 4 demonstrates this behavior, and presents a way to derive scaling formulas that predict it. We show how the scaling properties of a nested data structure depend upon the structure of that nesting. We introduce the content schematic, a concise summary of this nesting structure. The content schematic of a data structure identifies the distinct contexts in a data structure that may vary in size as the application runs. We show how to combine a scaling judgment scheme with the content schematic in order to automatically derive formulas. We present several example studies to demonstrate the power of scaling formulas in pinpointing the limits of health for a given design.</p><p>Summary This paper presents the following contributions:</p><p>• two application-neutral systems that classify the role of bytes in objects, and the role of objects in collections</p><p>• health signatures that distinguish the role of bytes based on the role of objects in collections</p><p>• a judgment scheme to distinguish the sources of overhead in a design, and a second scheme to expose the design's scaling properties</p><p>• a technique for finding and summarizing the data structures in a heap snapshot into content schematics</p><p>• an algorithm for automatically constructing scaling formulas to gauge the asymptotic health of a data structure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Health Signature</head><p>We summarize a heap snapshot, or a subset of a snapshot, to identify potential inefficiencies in the way memory is structured. A health signature is a two-dimensional summary using two distinct categorizations of memory:</p><p>• instance health: for each object, categorize its bytes according to the function those bytes serve.</p><p>• collection health: for each data collection, categorize its constituent objects according to the function each serves in implementing the collection's functionality.</p><p>In both cases, the categories are application-neutral, in the sense that they are not in terms of data types, or any other such application-specific artifacts. We begin by introducing the categories, and then show how to derive health signatures from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instance Health</head><p>Every instance of a data type consumes a certain number of bytes in the runtime heap. Some of these bytes store the type's instance variables, while some store information needed by the underlying runtime, such as for garbage collection or lightweight synchronization. We categorize the bytes of an object in this way: by what purpose those bytes implement or facilitate.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the four categories of instance health. The first category covers all primitive data, whether from primitive fields of objects, or from arrays of primitive data. In Java, each primitive element consumes one, two, four, or eight bytes. The second category accounts for the bookkeeping information that the runtime sets aside to help manage each object, commonly termed "object header". The runtime  uses this header to facilitate tasks such as garbage collection, lightweight synchronization, and reflection. The size of each object header is usually independent of the instances type; many Java virtual machines impose a 12-or 20-byte object header, depending on whether it uses 32-or 64-bit addressing. The header of array instances differs from that of object instances. For example, in Java, it is common to have a twelve-byte object header, with an additional 1-4 bytes to keep track of an array's length. <ref type="foot" target="#foot_0">1</ref>The final two categories cover bytes set aside for pointers between objects. Each pointer slot, whether from a field of an object or from an array of pointers, consumes one word (either 32 or 64 bits on most contemporary virtual machines). This is the same even if the pointer is null. Null array slots occur in the common case of an application that allocates an array with some default capacity, but only makes use of a smaller number of elements during the course of its run. The third category includes bytes from pointer slots that refer to extant objects, and the fourth category includes the null case.  <ref type="table">2</ref>. The categories of collection health.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates an instance health categorization. The figure shows an array of Java String objects, the one String in that array, and the character array object which underlies that String. On a 32-bit platform with 12-byte object headers and 8-byte alignment, this example will consume a total of 72 bytes: 40 from object headers, 8 from pointers, 8 from null pointers, and only 16 from primitive data. Furthermore, of those 16 bytes, only 4 come from the true data of this structure: the two characters in the String's character array. To come to this conclusion requires a categorization of objects that distinguishes the String from its character array.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Collection Health</head><p>We now address sources of bloat that stem from the way individual objects are glued together into larger collections. Inside every collection will of course be the actual data intended to be collected together. Unfortunately, the actual data may only occupy a small fraction of the collection's total size. Everything else represents the infrastructure costs. This dichotomy, between actual data and collection infrastructure, is the basis for our second categorization scheme. Whereas the instance health breaks down the bytes of each instance, the collection health analysis categorizes the objects of a collection.</p><p>Each instance inside a collection serves one of four primary roles <ref type="bibr" target="#b14">[15]</ref>. Table <ref type="table">2</ref> shows these four categories of collection health. First, every collection consists of a number of "backbones" that allow the collection to contain a variable number of objects. Some backbones are arrays, which are commonly used when append-only random access is needed. Other backbones have a chained or recursive structure, to allow for efficient random insertion and deletion. We label these as entry instances. For example, in the Java standard libraries, instances of the data type LinkedList$Entry fall into this category.</p><p>Next, many data structure implementations include a data type that represents the collection as a whole; in the Java standard libraries, such data types include HashSet and LinkedList. In fact, every implementation in the Java collections library was designed so as to devote one instance to serve this role. The head category includes these heads of collections. In general, we include in this category any instance that points to to an array or entry instance. The final category, contained, includes what remains; it includes the non-infrastructure instances inside collections.  Figure <ref type="figure" target="#fig_1">2</ref> shows an example collection health categorization. This heap contains a HashMap whose keys and values are Java Strings, and the Strings in turn contain character arrays. We consider the case where the key and value point to the same object, <ref type="foot" target="#foot_2">3</ref>  The Strings also fall into the head category, since they refer to the arrays of primitive characters. Using instance sizing information from IBM Java 1.5, this heap consumes a total of 364 bytes. Of that, 144 bytes, 40% of the total, belongs to the head category: three strings, each 32 bytes, and one hash map; 76 bytes belongs to the array category, from the four-element array; 96 bytes, 26% of the total, belong to the entry category: three hash map entry objects; and 48 bytes, only 13%, belong to the contained category: three 2-element primitive arrays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Composing the Two Classifications</head><p>The composition of instance health and collection health lets us study the role of an instance's bytes in the context of a collection. For example, bytes that seem reasonable (e.g. primitive data) may come mostly from objects that serve an infrastructure role, and that one did not realize would incur such a great memory overhead. Conversely, bytes that seem to be excessive (e.g. pointer bytes) may come from infrastructure objects that enable a useful function (e.g. offering random insertion and deletion).</p><p>With four categories in each categorization system, a health signature is a 4 × 4 matrix. Table <ref type="table">3</ref>. The health signature for the example in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>of bytes in the intersection of a collection health category and an instance health category.</p><p>In Table <ref type="table">3</ref> we show the health signature for the data structure in Figure <ref type="figure" target="#fig_1">2</ref>. The leftmost column shows the distribution of primitive bytes, which at the surface we might consider to be the "actual data" of the data structure. We see, however, that most of the primitive bytes are found in head objects. Each String in Java contains three primitive fields for bookkeeping purposes: a memo of the string's hash code, the string's length, and an offset, in case the underlying characters are shared. Each HashMap, also a head instance, devotes five primitive fields to bookkeeping. The only actual data in this data structure is found in the String's character array, the 12 bytes shown in the contained-primitive cell. Similarly, each HashMap has four pointer fields, three of which are null by default. These are used for caching in cases which may not occur. We contrast them with the 56 bytes in the array-null category, a sign that an array may have been sized too large. By taking into account the context in which bytes are used, the health signature allows us to make these finer distinctions about the ways a design spends its bytes.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> presents health signatures from four snapshots, and illustrates the very different signatures that show up in practice. Figure <ref type="figure" target="#fig_3">3</ref>(a) shows the health signature of a Da-Capo benchmark called antlr. The signature indicates that antlr devotes 70% of its bytes to primitive bytes within contained objects; the number of bytes spent in pointers, arrays and list-style entries are all small fractions of the total heap size. Figure <ref type="figure" target="#fig_3">3(b)</ref> shows the very different health signature of javac, a benchmark from the SPECJVM98 suite. In contrast to antlr, javac spends only 20% of its bytes in the primitivecontained intersection. In addition, object headers and pointers show up in much greater concentration. The health signature of Figure <ref type="figure" target="#fig_3">3</ref>(c) comes from application S. Its health signature seems to indicate some severe problems with memory health. In particular, contained objects do not consume a plurality of the heap, and primitive data appears mostly in heads of collections. The fourth snapshot, from a J2EE <ref type="bibr" target="#b21">[22]</ref> server application, has characteristics in common with each of the other three applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implementation Details</head><p>To automatically categorize by instance or collection health, our implementation analyzes heap snapshots. To collect snapshots, we used the built-in facilities of Java virtual machines (JVM) to trigger writing a snapshot to disk. In some cases, the JVM produced a snapshot upon heap exhaustion. In other cases, we explicitly requested them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Instance Health Implementation</head><p>Membership of bytes in a category is a local property of each object. Therefore, categorizing bytes is mostly a straightforward task: for each instance, count up the header and pointer bytes, and consider primitive bytes to be the instance's total size minus that sum. Depending on the richness of information provided by the heap snapshot format, however, there may be several wrinkles.</p><p>First, it may not be possible to distinguish null field slots from primitive data bytes with certainty. For example, most heap snapshot formats do not describe the field layout; rather, they specify the size of each type's instances, and, for each instance, its number of outgoing non-null references. In this situation, we conservatively estimate the number of null references by computing the maximum number of non-null references, over all instances of each type.</p><p>Second, it is usually infeasible to detect uninitialized primitive data. Therefore, the numbers reported in this paper are based on the assumption that the bytes of a primitive array always contain meaningful data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Collection Health Implementation</head><p>The heap snapshot is a graph, where nodes correspond to objects and edges correspond to references between objects. We first compute a spanning forest over the graph. The forest we use is based on the dominator relation <ref type="bibr" target="#b14">[15]</ref> because it eliminates back edges in a reliable way -one that does not depend on an arbitrary ordering of graph roots. We categorize data types based on a purely structural analysis of the spanning forest (so that "refers to" below means in the spanning forest). From the categorization of types, it is trivial to categorize objects, based on their type.</p><p>Every array of reference type belongs to the array category. Now consider a type having an instance that immediately refers to a different instance of that type. All instances of such types belong to the entry category. This strategy under-approximates the set of all list-style and recursive data types. Computing this property in general from only a structural analysis of the graph is a challenge for future research. Then, we categorize as head any type with an instance that immediately points to to an object whose type is either categorized array, entry, or is a primitive array. Any remaining data types are considered to be contained.</p><p>This implementation maps every object to a single category, except in the case of nested collections. Whenever a collection contains other collections, each collection head is also a contained object. For this paper, we prioritize the categories, to ensure that each object falls into one category: we give priority to the array and entry categories, followed by head, and then contained objects. In this way, all of the LinkedList objects in a list of lists will be considered to be heads of collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Judgment Schemes</head><p>For a developer experienced with a code base and with memory health analysis, a health signature can provide a necessary level of detail for problem resolution. For developers new to the code or to memory analysis, a summary that incorporates some amount of a priori judgment may be necessary. A health signature imposes no value judgments on whether an application's use of memory is good or bad. Moreover, a health signature is a two-dimensional summary. A one-dimensional summary can make deviations from expected norms quickly apparent. It can also enable bulk comparisons across versions, data structures, diverse applications, and varying load scenarios. Table <ref type="table">5</ref>. The overhead judgment scheme applied to the example in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>In this section we introduce health judgment schemes, one-dimensional summaries of memory health signatures. Health judgment schemes can be designed for specific analysis tasks, combining elements in health signatures to bring out certain features. We demonstrate two health judgment schemes. An overhead judgment scheme, described in Section 3.1, highlights common ways in which overhead is introduced into a design. A scaling judgment scheme, described in Section 3.2, can help evaluate collection choices and can expose how a data structure will scale. In Section 3.3 we apply these two judgment schemes to a real-world memory problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Overhead Judgment Scheme</head><p>We introduce the overhead judgment scheme to help distinguish the data in a data structure from the structure's overhead. We consider the ways in which space for primitive fields, object headers, and pointers may be introduced into a design. Table <ref type="table" target="#tab_4">4</ref> shows how this scheme judges the categories from a health signature.</p><p>A design may use primitive fields to store actual data or bookkeeping information. As we saw earlier with String, we can use the role an object plays in a collection to help distinguish the purpose of its primitive fields. The overhead judgment scheme classifies contained-primitive bytes, such as the characters in a String's character array, as data; it classifies head-primitive bytes, like those in the String proper, as primitive overhead. We also consider entry-primitive bytes as data, rather than primitive overhead. We do this so as not to penalize applications that combine entry and contained functionality into a single type. <ref type="foot" target="#foot_3">4</ref>Object headers and some pointers are introduced during class design when deciding which fields to include in a class, where to use subclassing, and where to delegate fields to separate classes. We classify bytes spent on object header as an indicator of small objects -instances with few fields, or arrays with few elements. We classify pointer and null bytes as pointer overhead, an infrastructure cost of a highly delegated or highly interconnected design. Pointers may also be employed for a different purpose: to maintain one-tomany relationships. We make this distinction by classifying non-null pointer bytes in array or entry objects as collection glue. We classify null pointers from array and entry objects, however, as pointer overhead; they are the overhead of oversized arrays or very short linked lists.</p><p>Table <ref type="table">5</ref> shows the overhead judgment scheme applied to the data structure in Figure <ref type="figure" target="#fig_1">2</ref>. The large values for small objects and pointer overhead highlight the degree of delegation in this design.</p><p>Figure <ref type="figure">4</ref> shows the application of this scheme to a diverse assortment of snapshots. Table <ref type="table" target="#tab_5">6</ref> lists the applications in our corpus. We study snapshots from a large number of real applications: under-test and deployed servers, and standalone and Eclipse-based <ref type="bibr" target="#b11">[12]</ref> clients. These heap snapshots range in size from 50 megabytes to 2 gigabytes; the largest snapshots contain as many as 20-40 million live objects.</p><p>Our corpus also includes snapshots from two benchmark suites, SPECJVM98 <ref type="bibr" target="#b23">[24]</ref> and DaCapo <ref type="bibr" target="#b2">[3]</ref> (using dacapo-beta051009.jar). For each benchmark, we configure it to run its largest configuration, and acquire several dozen heap snapshots during the run. We generally pick the largest snapshot from among those collected. <ref type="foot" target="#foot_4">5</ref> It is worth noting that none of the benchmarks have heaps bigger than 10 megabytes. Most of the benchmark snapshots were considerably smaller than that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Scaling Judgment Scheme</head><p>The scaling judgment scheme distinguishes contained objects from collection implementations, exposing how each contributes to overhead. Table <ref type="table" target="#tab_6">7</ref> shows how it is derived from the health signature. The primary distinction is between contained objects (the first row in the table) and head, array, and entry objects (the remaining rows). Within each we define two finer categories. For contained data, we label as data any primitive field data, just as we did in the overhead judgment scheme. We label the rest of the bytes -object headers and null and non-null pointers -as data overhead. For objects implementing collection infrastructure, we divide their bytes into fixed and variable collection overhead. Fixed collection overhead includes all of the bytes in headof-collection objects, and the object header portion of arrays of references. <ref type="foot" target="#foot_5">6</ref> Variable collection overhead includes bytes allocated for the elements of arrays of references, whether null or non-null, and all of the bytes of entry objects. Table <ref type="table" target="#tab_7">8</ref> shows the scaling judgment scheme applied to the example in Figure <ref type="figure" target="#fig_1">2</ref>. This breakdown highlights the degree to which memory is devoted to collection overhead, both fixed and variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">An Example: Applying Health Judgment Schemes</head><p>In this section we walk through an example from a realworld application with large footprint problems, to demonstrate the power of the two health judgment schemes. The example also illustrates the variety of memory problems seen in industrial applications. The application is a planning system, application S in Table <ref type="table" target="#tab_5">6</ref>. It was at the prototype stage of development, and as such, the primary concern was producing a working algorithm quickly. Like many industrial applications, even those much closer to production, memory usage was not taken into account until a problem appeared.</p><p>The primary data structure is a level graph where the same vertex may appear with different edges at multiple levels of the graph. We explore one particularly expensive data structure: the representation of the edges. Figure <ref type="figure" target="#fig_6">7</ref> shows a schematic view of that portion of the data, which cost 42MB in this run. Each node in the figure represents instances having the same ownership context; some intermediate classes have been elided (explained formally in Section 4.2). The node labels reflect the cost of all instances at that node, including those of elided classes. Edges are labeled with their average fan out. In the application's level graph, each edge is stored as an Edge object pointing to its source and target vertices. There are two indexes for looking up the edges for a (vertex, level) pair: one for in-and one for out-edges. The first column of Table <ref type="table" target="#tab_8">9</ref> shows the scaling judgment scheme applied to this data. We can see that a very small portion of the bytes, 6%, are devoted to actual data, while 74% are devoted to collections. While we would expect an index, especially one mapping each key to many values, to involve a significant amount of overhead due to collections, we would not expect it to so completely swamp the data. What is more telling is how much memory (7.4MB or 17%) is devoted to collection fixed overhead costs. According to the developer, this was a production-scale run, not a small test. We would expect the collection fixed costs to be amortized across a run with 148,000 edges. The fact that they are so large suggests that some of these costs are being magnified rather than amortized.</p><p>We next compute scaling judgments over the two subordinate data structures, first the keys and then the values, since each may have different characteristics. According to the developer, the Arrays$ArrayList class was chosen not because a one-to-many relationship was needed, but rather for expediency of coding, since it enabled a coding idiom where test cases with constant vertex/values pairs could be coded in a single line. The second column in Table <ref type="table" target="#tab_8">9</ref> shows the cost of using a collection class for this purpose: high fixed and variable collection overhead costs. The high fixed component (73% of the keys' cost) is due to requiring two instances, an ArrayList and array, to store just two elements. The use of an ArrayList also required the programmer to box the level number into an Integer, adding to the data overhead and variable collection costs.</p><p>The overhead judgment scheme can provide another view into this design, shedding light on overall overhead costs across contained and collection data. The first column in Table 10 illustrates this analysis for the keys. The high value for small objects is a consequence of the two levels of delegation, from Arrays$ArrayList to Object array to Integer. The developer can easily replace this design with a single class: Pair {int level; Object vertex;}. The second column in Table <ref type="table" target="#tab_10">10</ref> shows the reduction that would be achieved.</p><p>The third column of  hold. Notice that the majority of the cost is due to collection overhead here as well, with significant fixed and variable components. The standard Java HashSet, which stores a set of unique values, is implemented by delegating to the more general HashMap, which in turn points to an array of HashMap$Entry objects that point to keys and values, and to each other. <ref type="foot" target="#foot_6">7</ref> The delegation of HashSet to HashMap raises the fixed collection overhead for each of these nested HashMaps; the overgenerality of HashMap raises both the fixed and variable overhead costs: the HashMap has bookkeeping fields that are not relevant to this use, and all of the inner HashMap$Entry objects point to the same (unused) singleton value. Note that overall, a HashMap$Entry, containing four fields, is larger than the Edge object that it is indexing. This is one of the reasons behind the high variable collection overhead cost in our data structure. Note also that the fixed collection overhead cost is high because each HashMap holds only 4-5 elements on average. The choice of ArrayList seemed to the developer like an inappropriate choice once the costs were known. The choice of HashSet seemed more reasonable for the intended use. The developer wanted the code to be safe by guaranteeing uniqueness, and assumed that the widely-used standard Java HashSet would be well optimized. One can easily imagine refactored HashSet and HashMap classes that shared code while optimizing for these two different uses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Asymptotic Analysis</head><p>Ideally, health should improve as input size increases, because fixed infrastructure costs are amortized over increasingly large amounts of data. Unfortunately, with multiple data structures and nested collections, this may not be the case. In many situations, the fraction of actual data in an application reaches an asymptotic value. In this section, we first present a simple example that illustrates the asymptotic behavior of memory health. To model this behavior, we introduce a definition of a data structure and a summary of its internal structure. We then provide an algorithm for automatically constructing formulas, on a per-data structure basis. Each formula predicts how a data structure's memory health changes as the sizes of its components vary. Finally, we give examples of scaling studies that are enabled by having these formulas, and demonstrate them on an application from our test suite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Observations of Asymptotic Behavior</head><p>Consider a linked list that stores data structures each of which has a collection health categorization of 4 bytes of contained, 24 bytes of head, and 8 bytes of entry. The linked list itself imposes 4 bytes of entry per contained structure and 12 bytes of head. We focus on these categories to keep this motivating example simple. With one contained structure, the relevant portion of the collection health categorization, (contained, head, entry), will be (4, 36, 12); with two contained structures <ref type="bibr" target="#b7">(8,</ref><ref type="bibr">60,</ref><ref type="bibr" target="#b19">20)</ref>; with n entries, (4n, 24n + 12, 8n + 4). In absolute terms, all three categories approach infinity. However, any one category, relative to the whole, approaches an asymptotic value. When we normalize, we get (8%, 69%, 23%) with one entry, and (9%, 68%, 22%) with two entries. In the limit of large n, this will approach (11%, 67%, 22%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OBSERVATION 1. The ratio of any health signature category to the sum of all health signature categories approaches an asymptotic value. This value is governed by the health of the collections and contained structures.</head><p>To illustrate asymptotic behavior in more depth, we experiment with three variations of a collection of collections of primitive bytes (recall this example from the introduction). The first uses the standard Java LinkedList collection for both the outer and the inner collections, and a distinct data type T to store the primitive byte field: a LinkedList of LinkedLists of instances of T . The second has the same shape, but replaces the Java list implementation with one from GNU Trove <ref type="bibr" target="#b8">[9]</ref>. Trove avoids the entry costs, by requiring that T implement the next and previous pointers itself. The third uses an array of primitive byte arrays. 8  Figure <ref type="figure" target="#fig_5">5</ref> shows the asymptotic behavior of these three implementations, as the size of the inner and outer collections vary. We use the overhead judgment scheme to categorize  the bytes of each scenario. The x-axis of each chart plots an increasing size of the outer collection; the four columns of charts show the behavior of an inner collection size of, in order from left to right, 10, 10 2 , 10 3 , and 10 4 . We have annotated the fraction of data in each chart. For example, the standard Java LinkedList approaches 8-10% data and the Trove implementation approaches 8-17% data. Because the design of these structures is so poor, Trove improves health by only a small amount. In contrast, the array of primitive byte arrays scales in a much more healthy way. The asymptotic health ranges from 34% to 98%. In addition, with large enough inner collections, the health improves as data size increases.</p><p>The health of the whole heap varies depending on how each data structure within the heap changes. The results shown in Figure <ref type="figure" target="#fig_5">5</ref> and in previous sections were the result of analyzing the heap as a whole. An unhealthy data structure may become more healthy as it increases in size. But, if a second, healthier data structure stays fixed in size, while an unhealthy one increases in size, the combined effect of the two will show the downwards trend, as in most of the experiments of Figure <ref type="figure" target="#fig_5">5</ref>. In those examples, the JVM itself includes some unchanging data structures that, compared to the ones experimented with, are relatively healthy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Content Schematic</head><p>To study the asymptotic behavior of health, we must analyze at a finer granularity than the whole heap. We show how to decompose the heap into data structures, and how to arrange each data structure into a tree of regions that may possibly change in size. We refer to this data structure summary as a content schematic.</p><p>First, we define data structures by unique ownership, drawing on earlier work <ref type="bibr" target="#b14">[15]</ref>. DEFINITION 1. Given a heap snapshot considered as an object reference graph G, a data structure of G rooted at type t is the equivalence class of trees in the dominator spanning forest of G whose root node has type t.<ref type="foot" target="#foot_7">9</ref>   Memory health varies widely across the data structures of an application. For example, Figure <ref type="figure" target="#fig_4">6</ref> shows the overhead judgment scheme applied to the four largest data structures of application O. Each has a distinct health signature.</p><p>To model the scaling properties of a data structure, we must identify its expansion points. To do so, we leverage the backbone-folding and type-indistinguishable equivalence relations introduced in <ref type="bibr" target="#b14">[15]</ref> to group objects into "regions". The backbone-folding equivalence relation folds any array or entry object into the nearest head, and any contained object into the nearest parent contained object that is a child of an array or entry object. In this way, containerrelated infrastructure objects are grouped with the owning container, and the constituents of contained data structures are similarly unified with the head of contained structures. The type-indistinguishable relation creates a class for head objects that share the same path-to-root of head and contained data types; similarly, it creates a class for contained objects with the same paths-to-root. <ref type="foot" target="#foot_8">10</ref>DEFINITION 2. Given an object i that dominates a set of objects O, a region of i is the subset of objects in O equivalent according to the backbone-folding and typeindistinguishable equivalence relations. The heads of a region are those objects at the head of a chain of backbone folding. When we refer to the region type of a region, we mean the type of the head objects. Finally, the number of elements in a region is the number of head instances of that region.  classified head. Furthermore, since we define regions over the dominator forest, the regions under any given object form a tree. DEFINITION 3. For a region r, the parent p(r) of r is the region headed by objects that most closely dominate objects that point to the heads of r. The average number of elements of r is the ratio of the number of elements in r to the number of elements in p(r). Given a data structure i, the content schematic of i is the tree of regions comprising i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Under this definition, observe that a region is capable of changing in size if the region type of its parent region is</head><p>For example, Figure <ref type="figure" target="#fig_6">7</ref> shows the content schematic for a subset of the largest data structure in application S (the same subset studied in Section 3.3). The figure labels each node with the region type of that region, and with the sum of the instance sizes of objects that fall into the equivalence class of that region. The figure labels an edge between regions r 1 and r 2 with the average number of elements in r 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scaling Formulas</head><p>The health of a data structure depends upon the contributions of its constituent regions. To construct a formula that predicts its health, we first define the incremental contribution of every region to the cumulative health. DEFINITION 4. Given a region r, the base health signature of r is the health signature H computed over just those objects that belong to r. The per-element base health signature of r is its base health signature normalized by the average number elements of r.</p><p>For every region in a data structure, we generate a formula that predicts its cumulative health -i.e. the health of that region and all its descendants. In this paper, we derive one formula for the primitive-contained category of the health signature, and a second formula for the other, infrastructural, categories. Observation 1 hypothesized that the ratio of either of these to the total has asymptotic behavior. DEFINITION 5. Given a region r, let the cumulative amount of actual data in r be D r . Let the cumulative amount of overhead be J r . We define the scaling formula of r to be:</p><formula xml:id="formula_0">S r = J r + D r D r = 1 + J r D r</formula><p>Note that we have defined S to be the total number of bytes divided by the data bytes, rather than the other way around. We do this only because it leads to simpler formulas.</p><p>To explore the limits of memory health, and to make predictions of health under various conditions that might not have been captured by any dynamic run, we express the scaling formula symbolically. A symbolic scaling formula is a function of three main data structure properties: the data structure's nesting of regions, the health of collections, and the health of contained objects. Table <ref type="table" target="#tab_0">11</ref> summarizes the independent variables that we consider. We now define these factors and show how to derive the set of symbols for a given region-i.e. the domain of the scaling formula S. After deriving the set of unknowns, we then express D and J in terms of those unknowns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Expansion Factors of a Content Schematic</head><p>The content schematic's nesting of regions in part governs how its health changes. For a given content schematic, we identify three ways in which the nesting structure governs changes in a data structure's health. We term these governors of size expansion factors.</p><p>First, as the application runs, the number of elements in each region may change. For example, this kind of expansion factor captures the number of distinct keys or values in a hash map. Observe that this quantity is the same as the average number of elements in a region. We term these primary expansion factors.</p><p>Second, certain collection implementations may overlap the variable collection overheads among the child regions. Consider the outer Java HashMap in Figure <ref type="figure" target="#fig_6">7</ref>: it maps keys of type Arrays$ArrayList to values of type HashSet. The content schematic includes three regions, one for this outer HashMap, one for the keys, and one for the values. The HashMap$Entry objects of the outer HashMap region are variable collection overhead that is shared between the key and value regions. In some instances, the magnitude of the overlap for a region is a fixed constant times the sum of the primary expansion factors of its child regions. For example, in a Java HashMap with distinct and never-null key and values, the overlap factor will always be one half the sum of the child expansion factors: in this situation, a HashMap of size 100 will have 100 keys and 100 values. However, it is often important to consider this overlap has an expansion factor distinct from the primary expansion factors -e.g. when key and values overlap, or are sometimes null. We term these overlap expansion factors.</p><p>Finally, it is often helpful to study the effect of varying the size of primitive arrays. For example, in an application that uses Java BigDecimals or Strings, studying the limiting ef-symbol meaning n r average number of elements in region r m r overlap of e for the children of region r d r per-contained structure actual data in r j r per-contained structure overhead in r c t fixed collection overhead of container type t e t amortized variable collection overhead of t Table <ref type="table" target="#tab_0">11</ref>. The independent variables in a scaling formula In Section 4.4, we show how treating these factors as either unknowns or constants allows exploration of the asymptotic behavior of the scaling formula S.</p><p>fect of the underlying primitive array sizes can highlight design problems. We term these primitive expansion factors. 11 DEFINITION 6. Consider a content schematic T with k regions. We denote the primary expansion factors of T by the symbols n 1 , . . . , n k , and the overlap expansion factors of T by the symbols m 1 , . . . , m k . An expansion factor may be left as an unknown, or it may be hard-wired to have a positive constant expansion factor.</p><p>For example, the data structure shown in Figure <ref type="figure" target="#fig_6">7</ref> has six primary expansion factors. Three of these correspond to regions whose region heads are classified head (i.e. heads of collections): n 1 for the outer HashMap, n 2 for the HashSet, and n 3 for the Arrays$ArrayList. The scaling properties of the entire structure's health, and of each enclosed structures, depends in part upon these expansion factors. Consider the case of n 2 : as it increases, the variable overhead of HashSet collections and the fixed overhead of the Edge data structures are magnified, but the fixed costs of the outer HashMap are amortized.</p><p>It is often helpful to use the properties of a particular snapshot to hard-wire the values of certain expansion factors. For example, given a heap snapshot, it is possible to hardwire a primary expansion factor n r to the average number of elements that belong to r in that heap snapshot. One can calculate the average or maximum size of primitive arrays in a certain region, and hard-wire that value for the primitive expansion factors within that region. Section 4.4 explores various scenarios of selectively hard-wiring expansion factors.</p><p>It is also possible to estimate a constant value for the overlap expansion factor of a region from a heap snapshot: find all entry objects in that region, and compute the number of graph edges v in the snapshot that point from one entry object to objects in more than one child region. In many cases, the maximum value of v is a reasonable choice for hard-wiring that region's overlap expansion factor. 11 Our current implementation does not make primitive expansion factors explicit, but instead models primitive array growth by treating the d cost (described below) as an unknown. (b) contained costs for regions head by the given type Table <ref type="table" target="#tab_0">12</ref>. For data types inferred to be heads of collections or heads of contained structures, we can optionally hard-wire c, e, j, and d to the numeric values given by the per-element base scaling judgment scheme for those collection types. This table gives some example values, in bytes, from various Java standard collections and for the contained structures in the application from Figure <ref type="figure" target="#fig_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effects of Collection Choices</head><p>The scaling properties of a data structure also depend on the kinds of collections used, and how those collections scale. Each collection implementation incurs a fixed infrastructure cost that can be amortized across all elements in the collection, and an infrastructure cost for every contained data structure.</p><p>DEFINITION 7. Given a region r with a head of container type t, the fixed infrastructure cost of r is denoted by c t ; when we write c r , we mean this as shorthand for the numeric value 0, if the heads of r are not heads of containers, or c t otherwise. Likewise, we define e t and the shorthand e r to be the variable infrastructure cost. We say that c t or e t is hardwired if it is assigned a non-negative constant value.</p><p>This definition assigns a single pair (c t , e t ) to each collection type t, rather than one pair for each instance of a collection type. This accurately reflects the scaling behavior of most collections. However, in some cases there will be e costs that are amortized over a number of contained elements. Consider the case of a Java HashMap, which uses explicit chaining to handle hash collisions. For this collection implementation, the pointer bytes for the base array are amortized over all contained elements with the same hash code. We have chosen to simplify the presentation for this paper, and so do not account for these amortized e costs.</p><p>To compute c t and e t , we can leverage the scaling judgment scheme introduced in Section 3.2. First, pick an instance of collection type t, and compute its per-element base scaling judgment. Then set c t to be the size of the fixed collection overhead category of the judgment; set e t to be the size of the variable collection overhead category.</p><p>For example, we can account for the differences in the way a HashMap and a LinkedList scale. Table <ref type="table" target="#tab_0">12</ref>(a) provides values of c and e for four collection implementations from the standard Java library. These values were automatically generated using our implementation of the above algorithm. In generating a scaling formula, one may choose to hardwire c and e to numeric values in this way, or may treat them as unknowns. The former analysis enables one to study the</p><formula xml:id="formula_1">D r = d r + r ∈kids(r) n r D r J r = c r + j r -m r e r + r ∈kids(r)</formula><p>n r (e r + J r ) Figure <ref type="figure">8</ref>. For a region r, the formulas for the cumulative actual data D r and infrastructure overhead J r . scaling effects of a given set of collection choices, while the latter allows one to study the effect of swapping one collection implementation for another. In this paper, we focus on the former.</p><p>The Effects of Contained Structure Design Finally, the scaling formula of a region depends upon the health of the contained data structures. DEFINITION 8. Given a region r, we denote the per-element actual data to be d r , and the per-element infrastructure cost to be j r .</p><p>Observe that the values of d r and j r can be computed just as we did for c t and e t . First, compute a per-element base scaling judgment for r. Then d r and j r are the values of the judgment from the data category and the data overhead categories, respectively. For example, Table <ref type="table" target="#tab_0">12</ref>(b) shows the values of d and j derived automatically for the application S. Each of the two rows corresponds to a region from Figure <ref type="figure" target="#fig_6">7</ref> whose region heads are classified as contained. The Integer object has the expected four bytes of data per element, plus twelve bytes of data overhead (since this analysis was performed on a snapshot from a JVM with twelve-byte object headers). The Edge object has proportionally higher costs of each: 8 and 24 bytes, respectively.</p><p>Scaling Formulas For a region r, the scaling formula S r depends on the expansion factors, the c and e overheads, and on the d and j costs. We model the data and overhead D r and J r with the recursive formulas shown in Figure <ref type="figure">8</ref>. The cumulative amount of actual data D r in region r is its per-element actual data plus the sum of its children's actual data, weighted by their primary expansion factors. The cumulative amount of infrastructure in r, J r , is given by its e overhead and j costs, plus the sum of its children's infrastructure costs, weighted by their primary expansion factors. The infrastructure of r also includes at most one e cost for every element in the child regions. This may over count, in the case of overlap. Therefore, the formula for J r must subtract off m r e, i.e. one e cost for every degree of variable-cost overlap among the child regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Using Scaling Formulas for Limit Studies</head><p>Simple data structures have simple scaling formulas. In general, however, the formulas grow quite complicated. Even with simple cases such as shown in Table <ref type="table" target="#tab_16">13</ref>, the formulas alone are not the final story. Rather, they enable a user to perform a variety of asymptotic studies that illuminate the limits of health for a given design, or a given use case.</p><p>For example, a scaling formula of S 1 = 1 + 28 d , corresponding to the String data structure in Table <ref type="table" target="#tab_16">13</ref>, has the property 1 ≤ S 1 ≤ 29. This structure, on its own, is asymptotically perfectly healthy: longer and longer Strings become increasingly healthy until the ratio of total to actual data is 1. Compare this with an ArrayList of 2-element HashSets of 10-character Strings (note that in Java, each character consumes two bytes). In this case, S 2 = 5.2 + 2 5n , whose bounds are 5.2 ≤ S 2 ≤ 5.6. No matter how many entries in the ArrayList, the actual data will consume no more than 1/5.2 ≈ 19% of the heap. The health of this second case is bounded because of poor design choices made for enclosed structures: short Strings, and small HashSets. Hard-wiring the expansion factors of the inner regions to be small lets us explore the limits to which infrastructure costs can be amortized.</p><p>In any limit study, one must fix certain unknowns and let others vary. We experiment with the effect of scaling the big regions, and also of scaling the average d (the actual data in contained objects). In the former, we fix what's inside large collections, to explore how well health scales given those lower-level decisions. In the latter, we see how healthy the contained structures need to be to achieve good health overall, given a fixed nesting of collections.</p><p>To perform a limit study, first choose a region to analyze. Then, choose which factors to hard-wire, and which to leave as unknowns. In the case of a big-regions limit study, hardwire the expansion factors of not-big regions contained under the chosen region to the average values observed in the heap snapshot used for analysis. Furthermore, hard-wire the d r and j r to the average values for region r, as described above. Use the recursive formulas shown in Figure <ref type="figure">8</ref> to generate S. In our implementation, we then use a symbolic algebra system, Maxima <ref type="bibr" target="#b6">[7]</ref>, to simplify the formulas. We then use Maxima to compute limits. In some cases, the limit of S approaches 1 as the unknowns increase; this is usually the case when treating d as an unknown (with increasing actual data in one's data structures, the health increases monotonically). In this situation, we can use Maxima to solve for the value of d necessary to achieve good health, e.g. S &lt; 1.2 (which corresponds to a fraction of at least 80% actual data).</p><p>We first present results for some simple structures. The third and fourth columns of Table <ref type="table" target="#tab_16">13</ref>  We now perform the two limit studies on the subset of application S analyzed in Section 3.3 (and visualized in Figure <ref type="figure" target="#fig_6">7</ref>). We have omitted the complete formula; it has a large number of unknowns, and offers an unnecessary level of detail. Each limit study hard-wires many of the unknowns, and results in formulas that can be more easily studied. Table <ref type="table" target="#tab_4">14</ref> shows the scaling formulas for the two limit studies. The first row shows the limit study which varies the number of keys and values. The formula has two unknowns: m and n, denoting the expansion factors of the two regions that were large in the given snapshot. These correspond to the HashSet and ArrayList nodes in Figure <ref type="figure" target="#fig_6">7</ref>. This study shows that this subset of the largest data structure of application S is doomed to have a fraction of actual data between 13% and 17%. Observe from Figure <ref type="figure">4</ref> that application S has a fraction of actual data lower than either figure. The largest data structure in the snapshot we analyzed was 170MB, of which 40MB came from the subset studied here. The remaining space was largely preallocated, but as yet unfilled, collections! The superposition of these two structures resulted in a overall fraction of actual data of around 8%. The second row poses the hypothetical study of adding more primitive data to the leafmost regions. It shows that, in order to achieve S &lt; 1.2 (at least 80% actual data), the Edge and Integer structures must each have at least 242 bytes of data. This study highlights the severe inefficiencies of the application's data model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>This work has been inspired by a variety of research. 12 Work on dynamic metrics <ref type="bibr" target="#b7">[8]</ref> provides insight by quantifying program bloat. It does not do so in application-neutral terms, nor does it provide predictive models. There are many powerful memory profiling tools <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> that identify suspicious data 12 A preliminary version of the work presented in this paper appeared in <ref type="bibr" target="#b16">[17]</ref>.</p><p>types, or memory leaks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>. None of them study the underlying design causes of memory bloat or its asymptotic behavior. Others have worked on graph summarization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15]</ref>. Each of these works has its strengths, such as for execution time profiling <ref type="bibr" target="#b0">[1]</ref> or for more close coupling with source code <ref type="bibr" target="#b19">[20]</ref>. We are aware of no summarization techniques that place the focus on containers and contained data structures that the content schematic offers. Automatic heap sizing <ref type="bibr" target="#b26">[27]</ref> assumes the program's health as written. Conventional asymptotic analysis of algorithms <ref type="bibr" target="#b5">[6]</ref> says nothing about the underlying causes of poor asymptotic behavior.</p><p>Finally, there are a number of works that characterize various aspects of the program, whether its configuration complexity <ref type="bibr" target="#b4">[5]</ref>, its defects <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref> or general behavior <ref type="bibr" target="#b9">[10]</ref>, or its performance bloat <ref type="bibr" target="#b17">[18]</ref>. Many of these use applicationneutral categories, but none is targeted to memory consumption, nor provides an asymptotic formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future Directions</head><p>Analyzing heap snapshots biases our analysis towards the longer-lived objects; we are extending this work to cover shorter-lived data. We can also leverage static information, and richer type information such as given by generic type systems, to possibly infer an approximation of health signatures from source code. We also see interesting possibilities in using health signatures and asymptotic analysis in development tools, to assist in better design, earlier in the development lifecycle.</p><p>One limitation of the current work is its inability to infer certain kinds of design intent. For example, to populate a structure may require random insertion and deletion. It is possible that, by incorporating knowledge about runtime access patterns, we can aid tool users in teasing out these cases. For example, it would be helpful to point out cases where a bloated structure is used well beyond the point where its construction has been completed. The user may then consider having two distinct forms: one optimized for construction, and a second optimized for read-only access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>A development team should assess the runtime consequences of their design and implementation choices. Ideally, this assessment should happen early and often, especially since ameliorating memory bloat may not be possible later on. In reality, teams often feel a sense of fatalism with respect to memory consumption, that this is an inexorable consequence of the object oriented paradigm. Fortunately, this is not the case, but it requires knowing whether an application's memory consumption is out of line with its needs. As a step in that direction, we presented applicationneutral characterizations of the health of a design. They offer the ability to compare one implementation to another and to known ideas of goodness, and to study the limits of the health of a design. By describing the nature of bloat, they can also serve to educate developers and managers of the trade-offs inherent in data structure design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example of instance health.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An example of collection health.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2 shows an example collection health categorization. This heap contains a HashMap whose keys and values are Java Strings, and the Strings in turn contain character arrays. We consider the case where the key and value point to the same object, 3 and show only a single edge to the Strings, to reduce clutter in the figure. The top-most node in the figure, representing the HashMap , falls into the head category.The Strings also fall into the head category, since they refer to the arrays of primitive characters. Using instance sizing information from IBM Java 1.5, this heap consumes a total of 364 bytes. Of that, 144 bytes, 40% of the total, belongs to the head category: three strings, each 32 bytes, and one hash map; 76 bytes belongs to the array category, from the four-element array; 96 bytes, 26% of the total, belong to the entry category: three hash map entry objects; and 48 bytes, only 13%, belong to the contained category: three 2-element primitive arrays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Four example health signatures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The overhead health judgments for the four largest data structures of application O .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The overhead judgment scheme illustrates the asymptotic behavior of memory health. The three rows show three implementations of a collection of collections of bytes. The four columns show inner collections of size 10, 100, 1000, and 1000. The x-axis of each plot corresponds to increasing the size of the outer collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The largest regions in a 40MB data structure of application S. The average number of elements of each region is shown on the incoming edge label. Each region is labeled with the size of objects that fall into its equivalence class. Small regions are drawn as circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Table 14 .</head><label>14</label><figDesc>Two limit studies for Figure7(subset of application S).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The categories of instance health, with sizes for a typical 32-bit Java VM. An data structure composed of an array of strings, showing one string with its three primitive fields and that string's 2-element character array.</figDesc><table><row><cell cols="3">category explanation</cell><cell></cell><cell></cell><cell># bytes</cell></row><row><cell cols="5">primitive prim. array elements, prim. fields</cell><cell>1-8</cell></row><row><cell>header</cell><cell cols="3">space imposed by VM</cell><cell></cell><cell>12-16</cell></row><row><cell>pointer</cell><cell cols="3">references between objects</cell><cell></cell><cell>4</cell></row><row><cell>null</cell><cell cols="2">unused pointers</cell><cell></cell><cell></cell><cell>4</cell></row><row><cell></cell><cell>header</cell><cell>pointer</cell><cell>null pointers</cell><cell></cell></row><row><cell>String[]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>String</cell><cell></cell><cell></cell><cell></cell></row><row><cell>header</cell><cell></cell><cell></cell><cell></cell><cell cols="2">primitives</cell></row><row><cell></cell><cell></cell><cell></cell><cell>header</cell><cell cols="2">(from array)</cell></row><row><cell>pointer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>char[]</cell></row><row><cell>primitives</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(from fields)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">(a) primitive header pointer null total</cell></row><row><cell></cell><cell>16</cell><cell>40</cell><cell>8</cell><cell>8</cell><cell>72</cell></row></table><note><p>(b) The instance health categorization of this data structure, with category sizes in bytes.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The overhead health judgment scheme.</figDesc><table><row><cell></cell><cell cols="5">primitive header pointer null</cell></row><row><cell>contained head array entry</cell><cell>data primitive overhead data</cell><cell>small objects</cell><cell>glue</cell><cell>pointer</cell><cell>overhead</cell></row><row><cell></cell><cell cols="3">overhead judgment size</cell><cell></cell><cell></cell></row><row><cell></cell><cell>data</cell><cell></cell><cell>24</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">primitive overhead</cell><cell>56</cell><cell></cell><cell></cell></row><row><cell></cell><cell>small objects</cell><cell></cell><cell>156</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">pointer overhead</cell><cell>104</cell><cell></cell><cell></cell></row><row><cell></cell><cell>collection glue</cell><cell></cell><cell>24</cell><cell></cell><cell></cell></row><row><cell></cell><cell>total</cell><cell></cell><cell>364</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Our corpus of Java heap snapshots, taken from real applications and benchmarks. The real applications have 50MB to 2GB of live objects. The DaCapo and SPECJVM benchmarks are both relatively tiny in this regard.</figDesc><table><row><cell>application</cell><cell>description</cell></row><row><cell>A, P</cell><cell>Eclipse-based programs</cell></row><row><cell cols="2">C, G, H, I, L, M financial services server</cell></row><row><cell>E, Q</cell><cell>catalog management server</cell></row><row><cell>B, N, F</cell><cell>Java clients</cell></row><row><cell>D, J, K, O, R</cell><cell>collaboration servers</cell></row><row><cell>S</cell><cell>standalone Java program</cell></row><row><cell>serverbench</cell><cell>application server benchmark</cell></row><row><cell>dacapo</cell><cell>the DaCapo benchmark suite</cell></row><row><cell>specjvm</cell><cell>the SPECJVM98 benchmark suite</cell></row></table><note><p><p>Figure 4</p>. A judgment scheme, in this case the overhead judgment scheme, allows for quick comparison of the health of disparate applications. For example, application Q has 780M of live objects, of which only 234M is actual data.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The scaling health judgment scheme.</figDesc><table><row><cell>scaling judgment</cell><cell>size</cell></row><row><cell>data</cell><cell>12</cell></row><row><cell>data overhead</cell><cell>36</cell></row><row><cell>fixed collection overhead</cell><cell>156</cell></row><row><cell cols="2">variable collection overhead 160</cell></row><row><cell>total</cell><cell>364</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>The scaling judgment scheme applied to the example in Figure2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>The indexes are implemented as HashMaps (the left-most The scaling judgment scheme applied to a large data structure of application S. The size of each category represents the sum of bytes categorized that way for all objects under the given collection.HashMaps in the figure).The key is a 2-element ArrayList, containing an Integer level number and a pointer to a vertex. The value is a HashSet of Edges. In this run, there were approximately 148,000 edges.</figDesc><table><row><cell>scaling judgment</cell><cell cols="3">outer HashMap Arrays$ArrayList HashSet</cell></row><row><cell>data</cell><cell>2.6M</cell><cell>0.3M</cell><cell>2.4M</cell></row><row><cell>data overhead</cell><cell>9.0M</cell><cell>0.8M</cell><cell>8.2M</cell></row><row><cell>fixed collection overhead</cell><cell>7.4M</cell><cell>3.0M</cell><cell>4.4M</cell></row><row><cell>variable collection overhead</cell><cell>24.5M</cell><cell>0.8M</cell><cell>21.2M</cell></row><row><cell>total</cell><cell>43.5M</cell><cell>4.9M</cell><cell>36.2M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table 9 shows the scaling judgment over the values: the HashSets and the Edges they</figDesc><table><row><cell></cell><cell>(current)</cell><cell>(proposed)</cell></row><row><cell>overhead judgment</cell><cell>Arrays$ArrayList</cell><cell>Pair</cell></row><row><cell>data</cell><cell>0.3M</cell><cell>0.3M</cell></row><row><cell>primitive overhead</cell><cell>0.3M</cell><cell>0M</cell></row><row><cell>small objects</cell><cell>3.2M</cell><cell>1.1M</cell></row><row><cell>pointer overhead</cell><cell>0.3M</cell><cell>0.3M</cell></row><row><cell>collection glue</cell><cell>0.8M</cell><cell>0M</cell></row><row><cell>total</cell><cell>4.9M</cell><cell>1.7M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>The overhead judgment scheme helps to compare current and proposed implementations of a HashMap's keys in a large data structure of application S.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table 13 shows four simple examples of scaling formulas, generated automatically by our implementation. The rows of the table show formulas for data structures that contain Strings in various ways. On its own, a String with d char-acters has a scaling formula of 1 + 28 d , while an n-element ArrayList of 2-element HashSets of Strings has a formula 1 + 84 d + 8 nd .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 .</head><label>13</label><figDesc>give the results of these data structure scaling formula (S) vary n with d = 20 d necessary for S &lt; 1.2 Four example scaling formulas and two limit studies: for Strings with 20 bytes of characters the bounds of health that each structure can achieve, and the size of the Strings necessary in order to achieve good health (S &lt; 1.2).two studies for our simple examples. Observe that a String must have at least 140 characters in order to achieve an S &lt; 1.2 (i.e. no more than 80% actual data). When Strings are placed in a standard Java HashSet, they must have at least 270 characters to achieve this level of health. On the flip side, placing 10-character Strings into a HashSet will result in an S of no less than 3.7 (i.e. no more than 27% actual data), no matter how many Strings are placed into the HashSet.</figDesc><table><row><cell>String with d bytes of characters</cell><cell>1 + 28 d</cell><cell>S = 2.4</cell><cell>d &gt; 140</cell></row><row><cell>n-elt. ArrayList of Strings</cell><cell>1 + 32 d + 16 nd</cell><cell>2.6 ≤ S ≤ 3.4</cell><cell>d &gt; 160</cell></row><row><cell>n-elt. HashSet of Strings</cell><cell>1 + 52 d + 16 nd</cell><cell>3.7 ≤ S ≤ 4.4</cell><cell>d &gt; 270</cell></row><row><cell>n-elt. ArrayList of 2-elt. HashSets of Strings</cell><cell>1 + 84 d + 8 nd</cell><cell>5.2 ≤ S ≤ 5.6</cell><cell>d &gt; 420</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>There are two other cases of per-instance variation of object header size: fragmentation due to object alignment, and hashcodes stored in object headers. This information is available only in some JVMs. In cases where we do have it we include it in the object header category.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>It also includes the relatively few instances outside any collection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This is a common situation used to pool objects, since the Java HashSet does not support a lookup() method.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The Java LinkedList, Hashtable, etc. use distinct data types for these two roles; the GNU Trove<ref type="bibr" target="#b8">[9]</ref> collection classes often do not. A collection health categorization that better distinguished these scenarios would allow for more precise health judgment for entry-primitive bytes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>In cases where one snapshot was marginally smaller but considerably more unhealthy, we chose to present, in Figure4, the unhealthy one.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>In Java there are no per-array primitive fields. If there were, they would be included in this category.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>We use framework knowledge to classify the HashSet as head of collection, though it would not normally be classified as such solely on the basis of structure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>In the case of diamond structures or root sharing, we refer the reader to<ref type="bibr" target="#b15">[16]</ref> and<ref type="bibr" target="#b14">[15]</ref>. For these situations, we chose to use the heuristic for handling shared ownership described in<ref type="bibr" target="#b15">[16]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>The composition of these two equivalence relations is similar to the calling context tree<ref type="bibr" target="#b0">[1]</ref>, except in how it deals with recursive structures, and how it folds contained structures together.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Tim Klinger, Palani Kumanan, and Edith Schonberg for their help with this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting hardware performance counters with flow and context sensitive profiling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ammons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Programming Language Design and Implementation</title>
		<imprint>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Shape analysis for composite data structures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berdine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Calcagno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Distefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>O'hearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno>MSR-TR-2007- 13</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The DaCapo benchmarks: Java benchmarking development and analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA &apos;06: Proceedings of the 21st annual ACM SIGPLAN conference on Object-Oriented Programing, Systems, Languages, and Applications</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bell: Bit-encoding online memory leak detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A model of configuration complexity and its application to a change management system</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Integrated Management</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to Algorithms</title>
		<imprint>
			<publisher>MIT Press and McGraw-Hill</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Maxima Book</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Fateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yapp</surname></persName>
		</author>
		<ptr target="http://maxima.sourceforge.net" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic metrics for java</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Driesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hendren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Verbrugge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Object-oriented Programming, Systems, Languages, and Applications</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="149" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="http://trove4j.sourceforge.net" />
		<title level="m">GNU Trove: High performance collections for Java</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical investigation of program spectra</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGPLAN/SIGSOFT Workshop on Program Analysis For Software Tools and Engineering</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Purify -fast detection of memory leaks and access errors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Joynce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Proceedings</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Holzner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-04">Apr. 2004</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
	<note>first edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cork: dynamic memory leak detection for garbage-collected languages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jump</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Principles of Programming Languages</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Handbook of Software Reliability Engineering, chapter 9 (Orthogonal Defect Classification)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>IEEE Computer Society Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The runtime structure of object ownership</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Object-Oriented Programming</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006-07">July 2006</date>
			<biblScope unit="volume">4067</biblScope>
			<biblScope unit="page" from="74" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Leakbot: An automated and lightweight tool for diagnosing memory leaks in large Java applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sevitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Object-Oriented Programming</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003-07">July 2003</date>
			<biblScope unit="volume">2743</biblScope>
			<biblScope unit="page" from="351" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data structure health</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sevitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schonberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Dynamic Analysis</title>
		<meeting><address><addrLine>Minneapolis, Minnesota, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling runtime behavior in framework-based applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sevitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Object-Oriented Programming</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006-07">July 2006</date>
			<biblScope unit="volume">4067</biblScope>
			<biblScope unit="page" from="429" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic data structure analysis for java programs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Verbrugge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Program Comprehension</title>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A dynamic analysis for revealing object ownership and sharing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rayside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Dynamic Analysis</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic removal of array memory leaks in java</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Kolodner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sagiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Complexity</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="50" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="http://java.sun.com/j2ee" />
		<title level="m">Java 2 Platform, Enterprise Edition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<ptr target="http://www.quest.com/jprobe" />
	</analytic>
	<monogr>
		<title level="j">Quest Software. JProbe R Memory Debugger</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<ptr target="http://www.spec.org/osg/jvm98" />
		<title level="m">The SPEC JVM Client98 benchmark suite</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>SPEC Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Llc</forename><surname>Yourkit</surname></persName>
		</author>
		<ptr target="http://www.yourkit.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Checking inside the black box: Regression testing based on value spectra differences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Notkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Maintenance</title>
		<meeting><address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">Sept. 2004</date>
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic heap sizing: Taking real memory into account</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E B</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Memory Management</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
