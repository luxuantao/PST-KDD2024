<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regular Iterative Algorithms and their Implementation on Processor Arrays</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">T</forename><surname>H O M A</surname></persName>
						</author>
						<author>
							<persName><roleName>FELLOW, lEEE</roleName><forename type="first">S</forename><surname>Kailath</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Systems Laboratory</orgName>
								<orgName type="institution">Bell Laboratories</orgName>
								<address>
									<addrLine>Stanford IEEE Log Number 8716427</addrLine>
									<postCode>AF83-0228, 07733</postCode>
									<settlement>Holmdel</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regular Iterative Algorithms and their Implementation on Processor Arrays</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1C4DA59DEA23A8E2251308B93ED150B</idno>
					<note type="submission">received August 31,1986; revised June 25,1987.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we summarize some recent results concerning a class of algorithms known as Regular lterative Algorithms, particularly with respect to their implementations on processor arrays.</p><p>Regular Iterative Algorithms contain all algorithms executed by sys- tolic arrays as a proper subclass and are therefore of considerable importance in real-time signal processing applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The advent of VLSl has encouraged research into the design and development of special-purpose processor arrays for use in various applications, notable in signal processing. Such research was given a major impetus by the work of Kung and Leiserson [I], especially when it was reproduced in the pioneering textbook of Mead and Conway [2] on VLSl design. Kung and Leiserson introduced a class of parallellpipelined arrays that they dubbed with the attractive name of "systolic" arrays. Since then systolic solutions have been derived for various problems: notably, in signal processing, numerical linear algebra, and in graphtheoretic applications. This intense research activity was motivated by the apparent simplicity of the hardware design: one merely patterns the layout for a single processing element in the array and replicates this pattern appropriately.</p><p>It may be noted that the idea of using a "regular iterative" array of processors for solving various problems actually dates back to the late 1950s and the early 1960s (see, e.g., Unger [3], [4], McCluskey [5], Hennie [6]). Moreover, such iterative layout patterns have already been effectively used for silicon memories, array multipliers, transversal filters, etc. The important contribution of Kung and Leiserson was in pointing out the applicability of these layout strategies Manuscript</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for problems in signal processing, numerical computing, graph theory, and other areas.</p><p>However, despite fairly intense research activity in this field, the design of systolic arrays is still largely done on a heuristic and intuitive basis. This can be illustrated by discussing a set of natural questions that arise in this field:</p><p>Question 7. Given an algorithm, is there a systolic array implementation for it? The current answer is maybe, or maybe not. One has to try to find a systolic implementation by some means or other. If this can be done, then the answer, of course, is yes. On the other hand, one can hardly say no for any particular problem, because some more skillfull designer might find a systolic implementation. For example, in their original work, <ref type="bibr">Kung and Leiserson [I]</ref> presented a systolic array for solving linear equations of Gaussian elimination, but without pivoting. Of course, pivoting is essential for obtaining meaningful numerical results for most problems, with the notable exception of equations with positive-definite coefficient matrices. However, no one has yet succeeded in finding a systolic array for Gaussian elimination with pivoting. It would be nice to have a test for whether a particular algorithm has a systolic implementation or not, without having to first find an actual implementation. A reason that such a test has not been available so far is that the existing "definitions" of systolic arrays have been too qualitative. For example: "A systolic array is a one-or two-dimensional array, the body of which i s composed of identical functional modules arranged in ageometric latticeeach interconnected to its nearest neighbors and utilizing common control synchronous timing."</p><p>The problem with such a definition is that it is not too hard to find examplesof arrays that meet all of the stated requirements but that most people would refuse to accept as systolic, because they failed to have some property (e.g., efficiency) not explicitly stated above. We could elaborate on this theme further, but let us continue with our questions.</p><p>Question 2. If the answer to Question 1 is yes, is there more than one systolic implementation? More specifically, how many different implementations are there, really? As an example, a much studied problem is that of matrix multiplication, and by now about four or five different systolic implementations have been presented [I], [7- <ref type="bibr">[9]</ref>. Are there more? The point, of course, i s that these alternative implementations might have better properties of one kind or another, e.g., higher throughput, lower latency, fewer processors, etc. Question 3. Following up Question 2, the big question is whether there are systematic procedures for finding one or all systolic implementations? There are some partial answers to this question [IO]-[19], but many gaps still remain, including, in particular, whether some "reasonable'' generalization of the concept of systolic arrays might exist that would still be appropriate for VLSl technology.</p><p>In our recent efforts [9], [20]-[22], we have begun to find some satisfactory answers to the above questions. For example, a mathematical formulation has been obtained that shows, inter alia, that there is no true systolic implementation for Gaussian elimination with pivoting, but that there is an array of identical processors together with register pipelines and Last-In-First-Out (LIFO) buffers that can implement the pivoting algorithm. We have called such more general arrays Regularlterative Arrays (RIAs) and have shown that systolic arrays, precisely defined, form a proper subclass.</p><p>In this paper we shall briefly describe the general theory we have developed for RlAs and point out their relationships to earlier work, especially a seminal paper of <ref type="bibr">Karp,</ref><ref type="bibr">Miller,</ref><ref type="bibr">and Winograd [23]</ref>.</p><p>In Section II, some general concepts concerning the design of parallel architectures will be introduced and the importance of devising special techniques that exploit any available structure in the algorithm will be highlighted.</p><p>Section Ill contains a generic description of the existing methodologies for the systematic design of systolic arrays. Using some simple examples, the limitations of these methods will be brought out.</p><p>In Section IV, a formal methodology is proposed that overcomes the difficulties in the existing procedures. This methodology is based on some fundamental concepts and results contained in [23].</p><p>Finally, Section V contains the concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ON THE EXPLOITATION OF PARALLELISM IN ALGORITHMS</head><p>An algorithm takes concrete form only through the language expressing it. It has been recognized for some time that for the purpose of extracting the parallelism in an algorithm, standard sequential programming languages such as Fortran and Pascal are ill-suited vehicles for expressing the algorithm. An algorithm written in these languages has a built-in ordering of the computations which most often obscures any parallelism present in the algorithm. Furthermore, ever since the days when core memory was a costly resource to be sparingly used, one has been conditioned to think in terms of minimizing the storage required by the program, and hence encouraged to overwrite on variables as much as possible. Such overwriting further compounds the problem of extracting the parallelism from the program.</p><p>The so-called Single Assignment Language [24], for example, provides the means for overcoming the difficulties mentioned above by requiring that every variable defined in the program take on a unique value during the course of thecomputation.Thusassignment statements of the form "(a : = a + b)" are not allowed since a appears on both sides of the statement. If an algorithm is expressed as a Single Assignment Algorithm, viz., as a program in the single assignment language, then one can conceive of automated procedures for extracting the parallelism in the algorithm, with no further effort required of the user.</p><p>Given a single assignment algorithm, it is possible to capture the information regarding the parallelism in the algorithm by means of a dependencegraph. This graph has one nodeforeachofthevariables in thealgorithmandadirected arc from node x to node y if and only if variable y is computed using the value of x in the algorithm. The dependence graph of a single assignment algorithm specifies a partial ordering among the computations in the algorithm; that is, if there is a directed path in the dependence graph from node x to node y, then the computation represented by node y must be executed after the computation represented by node x is completed, no matter how many processors are brought to bear upon the problem. In such a case,onewould saythatyisdependent uponx,and ifapath from x to y is an edge, this dependence is direct. From this observation, one can infer that the length o f the longest path in the dependence graph is a lower bound for the total time required for executing the algorithm, independent of the number of processors used in this execution.</p><p>Suppose that one wishes to obtain an implementation of the algorithm that is optimal with respect to the total time required forexecutingthealgorithm. Onesimpleand bruteforce method for achieving this objective is to use a distinct processor for executing the computation represented by every node in the dependence graph. This, in general, leads to a very inefficient use of the computational resources, since each processor is active only for a constant period of time, which could be a minute fraction of the time required forcompletingthealgorithm.Toachieve a better utilization of these resources, it is necessary to reuse the same procesor for handling a large number of computations. In general, the set of computations can be arbitrarily partitioned and assigned to different processors.</p><p>In determining an implementation for the algorithm, one must not only specify the processor at which each computation is to be performed, but also assign a time at which it is to be executed by the processor. This mapping of computations into time slots is referred to as the construction of aschedulefor the computations. A schedule must satisfy the precedence constraints imposed by the dependence graph of the algorithm and must also be such that no two computations assigned to the same processor are expected to be executed at the same time. A schedule must also take into account the communication constraints among the processors. That is, if variable x is computed by processor px and if x is required as an input to the computation of variable y at processor pr then the schedule for the execution ofymust includethetime required tocommunicate the value of x from px top,,. If the processors are arranged in a general-purpose communication fabric, such as a crossbar switch, then the time required for this communication will also depend upon the prevalent congestion in the switch. Clearly, for different partitions of the nodes in the dependence graph, the interprocessor communications required will differ in general.</p><p>The problem of determining an optimal schedule, i.e., one that minimizes the total time for the execution of the algorithm, is extremely hard even if the interprocessor communication among the processors is assumed to be instantaneous. Indeed, it has been proved to be NP-completeeven in the presence of many simplifying assumptions (see, e.g., <ref type="bibr">Ullman [25]</ref>). If communication constraints have to betaken into account as well, then the problem becomes even more intractable, thereby forcing one to seek ways of exploiting any available structures in the algorithm.</p><p>Regular Iterative Algorithms (RIAs) are a special subclass of single assignment algorithms for which many of these difficultiescan be successfully overcome. Indeed, for a Regular Iterative Algorithm, one can ensure that all computations assigned to the same processor can be described bythe same simple instruction. For instance, if this instruction is a multiply operation, then one can replace this processor by a simple serial multiplier element. Furthermore, for a Regular Iterative Algorithm, one can ensure that the interprocessor communication required is fixed and can be implemented using a few dedicated links. A further attraction of this class is that the schedule for the algorithm can be constructed to be "periodic" so that the necessary delays on the interprocessor links can be implemented using shift registers and Last-In-First-Out buffers alone, without any additional control circuitry. Finally, Regular Iterative Algorithms form an extremely useful subclass of single assignment algorithms, in so far as signal processing applications are concerned, as shown in [9].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">AN INFORMAL DESCRIPTION OF RlAs AND THEIR IMPLEMENTATIONS</head><p>Aformal definition of Regular Iterative Algorithms is given in <ref type="bibr">[9]</ref> (see Appendix). Forsthe present discussion, it i s best to explore the various features of an RIA by means of some simple examples.</p><p>Example I An urn contains N red balls and N green balls. The following experiment is conducted repeatedly until either the urn is empty or exactly one ball remains: Two balls are picked at random from the urn. If they are ofthesamecolor,thenoneoftheseisreplaced intheurn. If they are of different colors, then both are discarded.</p><p>To determine the probability that the urn is empty at the end of the experiment, one can derive a recursive algorithm, using elementary counting arguments. Let p(i, j ) denote the probability that the urn becomes empty if there are i red balls and j green balls to begin with. Then j t h instance of the list be given by {x(i, j ) } , where i ranges from 1 to (Nj ) . In addition, let m(i, j ) be the largest number in the segment {x(k, j ) , k = 1 to i} of the list. Then m(i, i , = r ( i t i)!</p><formula xml:id="formula_0">i f i = I otherwise max {m(i -1, j ) , x(i, j ) } , undefined i f i = 1 min {m(i -1, j ) , x(i, j ) } , otherwise<label>(2)</label></formula><p>where the first equation follows from the definition of m(i, j ) and the second equation creates the ( j + 1)th instance of the list from the j t h instance. The calculations in (2) must be carried out for j = 1 to N -1, and i = 1 to (N -j ) .</p><p>i</p><formula xml:id="formula_1">x(i -1, j + 1) = Example 3 U, find the filtered image Y such that</formula><p>Given an image represented by the (N + 1 x N + 1) matrix n n y;j = c aky;-k,j-k + c bkU;-k,j-k.</p><p>(3)</p><p>Many possible RlAs can be obtained for solving this problem, by applying a simple transformation on known onedimensional filtering algorithms. However, most of these RlAs are known to be numerically unstable and can thus be ignored. Among the numerically stable algorithms, the ones due to Deprettere and Dewilde [32], <ref type="bibr">Vaidyanathan and Mitra [33]</ref>, <ref type="bibr">and Fettweis [34]</ref> can be written in the form:</p><formula xml:id="formula_2">k = l k=O For i = 0 to n do For j , k = 0 to N do x(i, j + 1, k + 1) = fX,;(x(i, j , k), y(i, j , k), w(i, i, k)) w(i -1, j , k) = fw,i(x(i, j , k), w(i, j , k)) (4)</formula><p>where fx,i, fY,;, fW,; are some linear functions that are determined by a synthesis procedure.</p><p>x(i, 0, k), x(i, j , 0) and w(N, j , k) = 0, for all j , k. <ref type="bibr">(5)</ref> The actual inputs are made available as This set of equations is initialized with</p><formula xml:id="formula_3">~( 0 , j , k) = ujk<label>(6)</label></formula><p>with p(0, 0) = 1, p(i, 0) = 0 for all i &gt; 0 and p(0, j ) = 0 for all j &gt; 0.</p><p>Example 2</p><p>Consider the following simple sorting algorithm referred toasse/ectionsorf[26].Givenalistof N numbers {x(i)},first determine the largest number in the list and delete it from the list. Then, from the (N -1) numbers in the remaining list, delete the largest number and soon iteratively until the list is empty.</p><p>To write this algorithm in single assignment form, let the and the output of the filter is obtained as</p><formula xml:id="formula_4">Y(N 1, / t k) = Yjk. (<label>7</label></formula><formula xml:id="formula_5">)</formula><p>All the algorithms described in the above examples are Regular Iterative Algorithms because they have the following features: i) They can be easily verified to be in the single assignment format.</p><p>ii) Each variable in these RlAs is identified by a label ( p in Example 1, for instance) and an index vector (k = [i, jl', in Example 1). The range of the index vector, which in gen-RA0 AND KAILATH: ITERATIVE ALGORITHMS AND THEIR IMPLEMENTATION era1 can be S-dimensional with S 2 1, is known as the index space. For instance, in Example 1, the index space is two- integer point in the index space, a set of Vlabels is used to denote the distinct variables (V = 1,2,3 for Examples 1-3, respectively). The number of integer points in the index space in the above examples is governed by the size parameter N (though in general, there may be several such size parameters).</p><p>iii) Finally, the main feature of these algorithms is the regularity of the direct dependences among the variables with respect to the index points. That is, if x(k) is computed using the value of y(kd), then the index displacement vector d, corresponding to this direct dependence, is the same regardless of the index point k. In Example 1 for instance,p(i, j ) is directly dependent on, say, p(i -I, j ) irrespective of the particular value of iand j . As a consequence of this regularity, the dependence graph of an RIA has an iterative structure, which can be clearly demonstrated by drawing the dependence graph within the index space (see Figs. 3 and 4). Each node in this dependence graph can be identified as (x, k) to represent the variable x(k) and is (physically) located at the point k in order to exhibit this regularity.</p><p>Though the direct dependences among the variables in an RIAare required to be iterative, the actual computations carried out to evaluate thesevariables can depend upon the index point. This is reflected in the above examples, both through the use of conditional expressions and through the use of values of i and j in the instructions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementations of RlAs</head><p>We shall now review, in our language, the present status of systematic methods for parallel implementation of the above algorithms. The three examples given above have been chosen to successively illustrate some of the limitations of the present methods.</p><p>Given an RIA, suppose that one wishes to exploit the maximum available parallelism in the RIA, while at the same time minimizing the computational resources used in the implementation. To achieve this globally optimal implementation, in general, the set of computations assigned to distinct processors might have to be arbitrary disjoint subsets of the set of all computations. However, the problem of optimally scheduling the computations that are assigned to a given processor becomes extremely hard if the partitioning is arbitrary. Hence, to renderthis problem tractable, one can restrict attention to linear partitions. Here, a set of parallel lines is drawn through the index space, so that all computations that correspond to index points that lie on the same line are handled by the same processor. In this manner, the processor array (including the interprocessor communication links) itself can be obtained by projecting the embedded dependence graph of the RIA along these lines on to a lower dimensional lattice of points known as the processor space (see Fig. <ref type="figure">5</ref>). The direction along which this projection is made is represented by an integer vector U, and is defined as the iteration vector.</p><p>Once the processor space is decided upon, one must attempt to schedule the computations that are mapped on to a given processor, i.e., assign some "time slot" for each variable (with respect to a global reference frame) during Fig. <ref type="figure">5</ref>. A processor array obtained for the RIA in Example 1 using the projection method. which its computation is performed by the processor. As noted before, thechoiceof the schedule is constrained both by the dependences in the algorithm and by the choice of the processor space. Once again, determining this schedule appears to be difficult, unless one imposes further restrictions on its nature. In keepingwith the useof a linear projection to determine the processor space, one can attempt to determine a schedule that is linear with respect to the index vectors. In a linear schedule, a set of (S -1)dimensional parallel hyperplanes must be drawn through the index space so that all computations that correspond to index points that lie on the same hyperplane are executed at the same time (necessarily, by different processors). Thus a linear schedule corresponds to isotemporal hyperplanes that are drawn through the index space, so that the progression of time is along the direction normal to these hyperplanes. For Example 1, S is two, and hence the linear schedule consists of a set of parallel isotemporal lines as shown in Fig. <ref type="figure">6</ref>.</p><formula xml:id="formula_6">L J I 2 1 I=? 1=s 1 =)v</formula><p>Fi g. 6. A linear schedule for the RIA in Example 1, applicable to the processor array in Fig. <ref type="figure">5</ref>.</p><p>Next, we note that a linearly scheduled RIA must be such that all edges in its dependencegraph areoriented in directions along which time strictly increases. Thus if the normal to the isotemporal hyperplanes is given by the S-dimensional vector 1, then every distinct index displacement vector d i n the RIA must satisfy 1 ' d 2 1. Furthermore, in order to ensure that all computations assigned to the same processor are scheduled at different times, 1 ' u must be required to be nonzero. Any vector 1 ' that satisfies these constraints simultaneously, defines a valid linear schedule forthe algorithm.Thisapproach can be successfullyapplied fortheRlAin Example1 andtheresultinglinearlyscheduled implementation is illustrated in Fig. <ref type="figure">7</ref>. The procedure informally described above has been independently derived by several authors both in the geometric framework used here [IO] and as an algebraic methodology [Ill- [19]. Though it is intuitively appealing, it has some very serious drawbacks that severely limit its applicability. Consider, for instance, the RIA in Example 2, for which the distinct index displacement vectors are given by Surely, it is impossible to find a linear schedule for this RIA, since there exists no vector 1 that can satisfy the constraint imposed by the first index displacement vector displayed above. One can, of course, overcome this difficulty simply by introducing the change of variables RlAs in which all variables except for a distinguished one, are computed using an assignment statement of the form</p><formula xml:id="formula_7">x(k) = ~( k -d) (11)</formula><p>and hence the RIA in Example 2 is not an URE.</p><p>The difficulty in this approach is even more serious in the case of the RIA in Example 3. Here, there are two distinct self-loops for which the index displacement vectors are the negative of each other, and thus any choice of the linear schedule, i.e., h', will violate the constraints imposed by at least one of these vectors. It can be easily verified that in this case, a simple change of variables as suggested above cannot be used to overcome this difficulty.</p><p>The failure of the above procedure in these two examples may betraced toacertain implicit assumption that it i s based upon: a l l computations that belong to the same index point in the RIA can be scheduled to begin at the same time. This may not be possible in general. What if there are directed paths in the dependence graph from some node at a particular index point to some other node at the same index point? Surely then, one cannot schedule these two computations at the same time and hence the second assumption can never be satisfied. Next, what if the coarse version of the dependence graph obtained by coalescing all distinct nodes in the original graph at every index point, is cyclic? Then also, the procedure is not applicable.</p><p>To rescue the above procedure, one must therefore pay attention to the fine structure at every index point in the dependence graph of a multivariable RIA (V &gt; 1). Most systematic methodologies proposed thus far fail to take this into account, thereby implicity treating a multivariable RIA as a single-variable one. It must, however, be noted that as part of their analysis procedures, <ref type="bibr">Karp,</ref><ref type="bibr">Miller,</ref><ref type="bibr">and Winograd [23]</ref> clearly pointed out this important distinction between single-variable RlAs and multivariable RIAs. Indeed, one must truly credit these authors for first pointing out the fact that single-variable RlAs can always be linearly scheduled as shown above, and that this need not be the case for multivariable RIAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. A FORMAL APPROACH TO THE DESIGN OF PROCESSOR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARRAYS</head><p>In a multivariable RIA, each indexed variable defines a lattice of points by itself within the index space. Before defining the isotemporal lines within the index space, one may have to translate each of these lattices to a different origin, independently. Sometimes, mere translations may not suffice and one may also have to rotate each of the lattices independently. In the extreme case, translations and rotations may not suffice by themselves, at which point our geometrical interpretation breaks down. It must be noted here that this extreme case includes many useful, numerically stable algorithms <ref type="bibr">[22]</ref>, and is hence, not just of academic interest.</p><p>While the geometric approach is not powerful enough for our purposes, a formal algebraic framework has been proposed that can deal with any well-posed RIA and generate an optimal implementation for it [9]. Though this framework was developed recently, it is based on some fundamental concepts and techniques that were reported two decades ago in the seminal paper of <ref type="bibr">Karp,</ref><ref type="bibr">Miller,</ref><ref type="bibr">and Winograd [23]</ref>.</p><p>The fine structure in the dependence graph of an RIA is concisely captured in the concept of a Reduced Dependence Graph (RDG)' that was introduced in [23]. In general, the R D G of an RIA has Vnodes, one for each of the indexed variables in RIA; it has a directed arc from node x to node y, if y(k) is computed using the value of x(k -d ) for some d; finally, each directed arc is assigned a vector weight representing the displacement of the index point across the direct dependence. Thus in theaboveexample, thearc from</p><p>x to y has weight d.</p><p>The RDGs for the RlAs in Examples 1-3 are shown in Fig. <ref type="figure">7</ref>. <ref type="bibr">Karp, Miller, and</ref> Winograd used this notation of an R D G representation in their study of RlAs defined over a semiinfinite index space. In their case, since the index spacewas assumed to be the set of all nonnegative integer vector sand was hence known to begin with, the R D G constituted a complete description of the dependence graph of the RIA. However, for many of our problems, the index space will naturally be finite, and hence the R D G together with the specification of the index space, will combine to form a complete description of the dependence graph. (The distinction between finite and semi-infinite index spaces is a nontrivial one; the latter assumption forces one to impose certain "causality" constraints on the dependence graph that cannot be met in several numerical algorithms.)</p><p>Given the R D G and a specification of the index space, one has all the information necessary to determine an efficient implementation for the RIA. Before developing the necessary algebraic framework, however, let us attempt to translate the geometric procedure informally described in the previous section into a more formal setting. Once this formalization is in place, one can then try to generalize it so as to be able to overcome the difficulties mentioned earlier.</p><p>The S-dimensional iteration vector U , introduced earlier, defines the topology of the processor array completely: the computations at two index points &amp;' and k2 are mapped on to the same processor if and only if</p><formula xml:id="formula_8">-k 2 = CYU (<label>12</label></formula><formula xml:id="formula_9">)</formula><p>where CY is some scalar integer. (Interpretation: This means thatk,and k2mapon tothesame pointwhen the indexspace is projected along the direction defined by u.) Let P be any (S -1 x S)-dimensional integer matrix of rank (S -1) that is orthogonal to U , i.e., Pu = 0.</p><p>Then, the processor array is defined by the lattice of points obtained by mapping the index space according to</p><formula xml:id="formula_11">p = Pk, k E index space (<label>14</label></formula><formula xml:id="formula_12">)</formula><p>where p defines the location of the processor that carries out the computation at the index point k. Indeed, in the above mapping, two index points k, and k2 are mapped on the same processor (i.e., the same p vector) if and only if they satisfy (12). The necessary interprocessor communication links are then defined by the vector weights on the for all k.</p><p>It must be noted here that while some authors use the iterationvectoru, inordertoderivethe processor array(e.g., Cappello and Steiglitz [IO]), many prefer to use the transformation matrix Pas originally proposed by Moldovan [Ill,  [13]. Both these techniques are equivalent: given U, one can determine the equivalent transformation matrix P to be any (S -1 x S) matrix that forms a basis for the null space of U; conversely, given P, the corresponding iteration vector is the unique right null vector for P. However, one must be careful in the latter case, since two matrices Pl and P2 represent the same processor array if they have the same row space. Of course, the same consideration applies to the former technique if one is dealing with multidimensional iteration spaces, rather than a one-dimensional iteration vector.</p><p>Once the processor array is obtained as above, one must now schedule the computations. If the processor array is to be implemented in a globally synchronous fashion, then this schedule might consist of specifying duringwhich cycle of the global clock a particular computation must be started at the processor. But this requires a detailed knowledge of the capabilities of the processor-whether it is bit-serial or bit-parallel and whether it can begin several computations simultaneouslyor not. To avoid having to knowthese details and still be able to derive useful results, one can conceive of a schedule that partitions the computations into global steps with the following restrictions: ii) All computations at step 7 are completed by every processor in the array before step (7 + 1) is begun.</p><p>iii) At each step, each processor must be assigned a "small" number of computations.</p><p>The second restriction is only conceptual: a processor may begin step (7 + 1) when all its neighbors (from which it receives input values) have completed step rwithout waiting for the rest of the processors in the array. Hence, even asynchronous implementations can be devised using this schedule. The third restriction needs to be clarified somewhat: we shall assume that a "small" number of computations is assigned at each step to every processor if and only if at most oneof each of the Vindexed variables in the algorithm is assigned to be computed at the processor. This restricts the number of computations to be at most V per step per processor.</p><p>In a linear schedule, as assumed in the previous section, the variable x(k) is assigned to step b'k for some constant vector b that is independent of x(k). That is</p><formula xml:id="formula_13">sx(k) = xJk, for all x (1 7)</formula><p>which, as we demonstrated earlier, is not general enough to handle several interesting problems. Indeed, in this case, if v(k) is computed using x(kd), then from (16), we must have i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I'd 2 1.</head><p>(1 9)</p><p>This must be true for every distinct index displacement vector d in the RIA, which may or may not be possible for the given RIA. In the sorting example, a zero index displacement vector exists for which it is impossible to satisfy A natural generalization of the above linear scheduling strategy is a uniform affine schedule. Here, we hypothesize that</p><formula xml:id="formula_14">(20)</formula><p>where li. is a constant vector, independent of x, whereas yx is a scalar that is specific to x. Then, (16) translates into (21 1 and this must be true for all such dependences, i.e., forevery directed edge in the RDG. When compiled together in matrix form, these constrains can be written as</p><formula xml:id="formula_15">s,(k) = I r k + yx yy -y x + L' d 2 1 y'c + 1 ' 0 2 [I 1 * * * I] (22)</formula><p>where i) C is the familiar edge-vertex incidence matrix or the connection matrix, commonly found in many circuit analysis text books [28]. It has € columns, one for each of the edges in the RDG, and V rows, one for each of the nodes in theRDG.The(i,j)thelementof Cis +I ifedgejterminates in node i, is -1 if edge j originates from node i and is zero otherwise (if edge j both originates and terminates at node i, then also ci i is zero). For example, the RDG for the RIA for sorting has a connection matrix given by c = c ; -I ' I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">-(23)</head><p>ii) D is the (S x €)-dimensional index displacement matrix, in which thejth column is the vector weight on the jth edge in the RDG. For the sorting example</p><formula xml:id="formula_16">D = [<label>(24)</label></formula><p>iii) yisthevectorobtained by stacking {yx) in theappropriate order, consistent with the arrangement of the rows in the connection matrix.</p><p>Before proceeding to attempt to solve the set of constraints in (22), a third restriction that a schedule must meet must be taken into consideration. If x(kl) and x(k2) are computed by the same processor, then they must not be assigned to the same step in the schedule. This implies that an additional constraint that is dependent upon our choice of the iteration vector U. Fortunately, though, from elementary linear programming considerations, it can be shown that if there exists a feasible solution to the set of constraints in ( <ref type="formula">22</ref>), then there always exists a feasible solution that meets the additional constraint in (26) as well [9],</p><formula xml:id="formula_17">(25) I- 1 -1 0 0 0 1 0 1 yx + b'k, # yx + b'k2, for all (k, -k2) = QU</formula><p>[20] (see Appendix).</p><p>An affine schedule exists as defined above if and only if the set of constraints in ( <ref type="formula">22</ref>) has a feasible solution. Indeed, for the sorting example, (29) for all x. Then, in the transformed domain, the RIA can be linearly scheduled simply because the corresponding transformation of the scheduling functions is given by</p><formula xml:id="formula_18">yx + X'k + yx -yx(X'r) + lL' k = X'k. (<label>30</label></formula><formula xml:id="formula_19">)</formula><p>It can be shown that everysystolicarray, properlydefined, executes an RIA for which a uniform affine schedule can be found as described above. Conversely, every such RIA can be implemented on a systolic array [9], [20] (the Appendix contains a partial exposition on this result).</p><p>What if, for the given RIA, there is no solution to the set of constraints in ( <ref type="formula">22</ref>)? In this case, of course, one must further relax the restriction on the scheduling functions. It is easy to show that the RIA in Example 3 falls in this category, for which one must work harder in order to obtain a schedule. The interested reader can verify that</p><formula xml:id="formula_20">Sx(i, j, k) = nj + k S,,(i, j, k) = i + nj + k SJi, j, k) = -i + nj + k (31)</formula><p>constitute a valid set of scheduling functions for the RIA in thisexample; thesescheduling functionsarealsoaffine, but for each x, the 1 vector is different, i.e., <ref type="bibr">(32)</ref> Due to space limitations, we will not discuss this case, but we must mention here that it has been proved that every RIA can be scheduled using affine scheduling functions, provided the X-vectors are allowed to be different for different variables. Furthermore, the parameters { 1 , , yx} must be allowed to depend upon the finite extent of the index space <ref type="bibr">[9]</ref>. In addition, these affine scheduling functions can also be obtained by solving a series of integer linear programming problems defined on some specific subgraphs of the RDG of the RIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUDING REMARKS</head><p>With the brief overview presented in this paper, we wish to leave the reader with a list of thevarious issues that must be addressed in order to provide a complete theoretical framework for the design of processor arrays, from a given RIA description. While these issues have been fairly well resolved for the case of RlAs defined over a finite index space [9], many of them are as yet unsolved for the semiinfinite case, though some partial results can be found in 1231.</p><p>Analysis Issues a) Is the given RIA computable, that is, is there some variable in the RIA that is circularly defined? If there is such a variable, then the algorithm cannot be executed in the manner proposed, and hence must be discarded (this problem was completely solved in <ref type="bibr">[23]</ref>).</p><p>b) Given acomputable RIA,what is the minimum achievable execution time for completing the algorithm? This information is crucial for the designer as it lets him decide how many processors should be used for executing the algorithm. For instance, if the algorithm requires O(N3) computations and the minimum achievable execution time is OW), then onewould wish to use OW2) processors, each working on disjoint sets of O(N) computations, to complete the algorithm in O(N) time. On the other hand, if the minimum achievable execution time is O(N2), then acollection of OW) processors must be sought in order to achieve maximum utilization. (The iteration vector may now have to be generalized to an iteration space.) c) How much storage should be provided for executing the algorithm? This would provide an indication of how much local memory should be associated with each processor in the array. lmplementational issues a) Given a computable RIA, how should one choose the processor space? Is this choice restricted by some properties of the RIA? b) Once the processor space is chosen, how should one determine the schedule? Is it possible to choose the schedule so that every processor is active most of the time? Synthesis Issues a) Given a problem, is it possible to find an RIA for solving it? Is thereatheoretical basis for determining such RIAs? b) Presumingthattherearea lotof RlAsfor solvingagiven problem, is there some systematic method for weeding out the inefficient ones?</p><p>This paper has attempted to provide some insight into these issues, and to show the virtues of a formal analytical approach. While only an outline has been given here, a detailed presentation can be found in a forthcoming monograph [29]. However, as a partial indication of the formalization that is possible, the Appendix gives the formal description of systolic arrays and some deductions thereupon.</p><p>While RlAs themselves form a useful class of algorithms, some attempts have been made in [35], [36] to generalize this class. However, it can be shown that the so-called a fhe recurrence equations, defined therein, can be systematically transformed into an RIA format, and thus can be handled using the techniques outlined above <ref type="bibr">[38]</ref>.</p><p>Another question that has not been addressed in this paper is that of "optimality." First, there are many different criteria for optimality of the array that are not necessarily synergistic. Secondly, even if one were to fix upon some cost function for defining optimality, the best known techniques rely essentially upon exhaustive searches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>This Appendix contains some formal definitions of the relevant concepts introduced in the paper and some deductions thereupon.</p><p>A Regular Iterative Algorithm is defined by the triple {/, X, F } where I is the index space which is the set of all lattice points enclosed within a specified region in S-dimensional Euclidean space, Xis the set of Vvariables that are defined at every point in the index space, where the variable xi defined at the index pointkwill bedenoted asxi(k)and takeson aunique value in any particular instance of algorithm, and F is the set of functional relations among the variables, restricted to be such that if xi#) is computed using xj(k -djJ, then 4; is a constant vector independent of kand the extent of the index space, and for every 0 contained in the index space, xi (&amp;? is computed using xj(e -d,J (if xi (! -dij) falls outside the index space, then this is an external input to the algorithm).</p><p>Note that the functional relations among the variables in Fcan involve conditional branches. However, in this case, the model essentially assumes that the dependence includes all the variables in every branch of the conditionals. While this assumption is a limitation in certain cases, it is not restrictive at all for a majorityof useful algorithms. Some related concepts can now be formalized as follows: An index vector is any integer vector that represents a lattice point within the index space.</p><p>The dependence graph of an RIA is a directed graph in which the node set is defined by the ordered pair (xi , k)</p><p>wherexiEXandkEland theedge setconsistsof all directed edges drawn from node (x,, k -dIl) to node (x,, k) if and only if x,(k) is computed using x,(k -d,J in the RIA.</p><p>The Reduced Dependence Graph of an RIA is a directed, vector edge-weighted graph</p><p>Vis the set of V nodes in the graph, one for each of the indexed variables in the RIA, E is the set of directed edges in the graph, such that a directed edge exists from node j to node i if and only if</p><p>x,(k) is computed using x,(kd,,) for some d, , , D is the set of index displacement vectors defined on the edges in the graph so that the edge from j to i above has an index displacement vector of d,,.</p><p>With these definitions in place, we now turn to the definition of a systolic array. Unfortunately, though, there is considerable confusion in the literature as to what exactly is a systolic array. Some authors, for instance, Leiserson, Rose, and Saxe [37l, consider every synchronous circuit to be a systolic array, while others, e.g., Kung [30], define systolic arrays as a restricted class of processor arrays with certain attributable properties. The following formal definition is consistent with the four properties (modularity, spatial locality, temporal locality, and efficiency) qualitatively described in [30]. Based on this definition, onecan then easily characterize the algorithm executed by a systolic array as a member of a proper subclass of Regular Iterative Algorithms.</p><p>Definition o f a Systolic Array: A systolic array is characterized by the sets {P, 7 , X, Dp, F } where Pis the processorspacewhich is the set of all lattice points enclosed within a specified region in p-dimensional Euclidean space, 7 represents the beats of the systolic clock, Xis the set of Vvariables that is computed by every processor in the processor space and at every beat of the systolic clock during the execution of the array, Dp is the set of processor displacements that defines the interconnecting links in the processor array so that if d is a member of Dp, then there is an interconnecting link from the processor at location p to the processor at location ( p + d ) irrespective of the particular value of p, and if variablexcomputed at beat 7 by the processor at location p is transferred across the link to the processor at location ( p + d), then this data transfer occurs regardless of the particular values of k and p, and Fis the set of functional dependences that relate thecomputation of a variable x at processor p during beat 7, as a function of the variables computed during the previous beat at the neighboring processors.</p><p>Once again, it must be noted that the functional dependences in F can involve conditional branches. Sometimes these conditional branches may be such that some of the data transfers involved in D may be unnecessary.</p><p>A detailed justification for this model can be found in With these formalizations in place, any algorithm executed by a systolic array can be characterized as follows.</p><p>= {V, E, D} where Theorem: A systolic array executes a Regular Iterative Algorithm that has a uniform affine schedule. Conversely, every Regular Iterative Algorithm with a uniform affine schedule can be implemented on a systolic array.</p><p>Proof: To show that a systolic array executes a Regular Iterative Algorithm, we define the index space to be / = = r], p E P, 7 = systolic beat .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>Next, let the variable x computed by the processor at location p at beat 7 be denoted as x(k). Then, by the definition of a systolic array, if x(k) is computed using y(e), then Therefore, it must be possible to determine 1 such that the greatest common divisor of its elements is 1. This implies (by the Bezoutian identity <ref type="bibr">[31]</ref>) that there exists a vector U such that bTu = 1.</p><p>Next, each indexed variable x can be redefined to be X(k) = x(k + yxu).</p><p>Then, the displacement matrix in the new domain can be written as D = D + ubTC.</p><p>Choose u to be the iteration vector and define the processor space according to f = { p : p = f k }</p><p>where Pis defined as in (13). To complete the systolic array implementation, define 7 = Irk so that X(k) is computed by the processor at location Pk during the 7th beat of the systolic clock.</p><p>The class of systolic algorithms, as characterized in the abovetheorem, is preciselythesubclassof Regular Iterative Algorithms that have O(N) 1 1 0 latency and constant storage requirements at each processor <ref type="bibr">[29]</ref>. For instance, the RIA in Example 3 is not a systolic algorithm, since it does not meet the latency bound. In this case, one can show that the schedulegiven in(31) is"tight"and thatthenumberof steps required to execute the RIA is at least nN regardless of the number of processors used. Thus the latency here has to be a polynomial of degree 2 in the size parameters n and N. Similarly, the RIA for Gaussian elimination with partial pivoting <ref type="bibr">[22]</ref> is not systolic since it requires OW*) time for its execution, no matter how many processors are used in its implementation.</p><p>Thomas <ref type="bibr">Kailath (Fellow, IEEE)</ref>   Dr. Kailath is a member of the National Academy of Engineering, a Life Fellow of Churchill College, Cambridge, England, and a Fellow of the Institute of Mathematical Statistics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>IEEE, VOL. 76, NO. 3, MARCH 1988</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. The index space for the RlAs in Examples 1 and 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The dependence graph of the RIA in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The RDGs for the RlAs in Examples 1-3, displayed as (a), (b), and (c), respectively. In (c), all edges without any weights explicitly displayed have zero weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, j ) = m(i -1, j ) since in the new domain, the index displacement vectors are given by dxiii = [ ; I 4Eii = [@ ukx = [ -; I dx, = [ -; I (IO) and 1 ' = [I 21 is a valid schedule for the algorithm. But then, how does one determine the necessary change of variables in general? Among the references listed above, only Quinton [I21 recognizes the need for such modifications in the description of the RIA, but the subclass of RlAs studied by Quinton does not include the one in Example 2. Indeed the Uniform Recurrence Equations (UREs) of Quinton are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>'</head><label></label><figDesc>This is our terminology. Karp, Miller, and Winograd refer to the RDC as the'ldependence graph,"which might causeconsiderable confusion at the present time. Incidentally, Waite [27l also independently introduced the RDC concept in a somewhat different setting in the same journal. edges in the RDG: if y(k) is dependent on x(kd), then there must be a directed link in the array from p(kd ) -+ Pk (1.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>i) If variable y(k) is computed using variable x(kd ) , then the step s,(k) at which y(k) is computed must be strictly larger than the step s,(k -d ) to which x(kd) is assigned, i.e., ~, ( k ) L sX(kd )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>'</head><label></label><figDesc>As noted earlier, a shift in a particular index can be used to overcome this difficulty in the sorting example. However, no such trick exists for the problem in Example 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The processor array for sorting obtained by projecting the dependence graph along the j direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>For the present, we shall only be interested in characterizing the algorithm executed by a systolic array.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>ke = [:I which is independent of kand the extent of the index space. Moreover, the systolic array operates according to the uniform affine schedule y = [O bTl = [ O O . . . 11. To prove the converse statement, let {y, b } constitute the parameters of a uniform affine schedule for the RIA. Then, if C be the connection matrix of the RDG of the RIA and D, its index displacement matrix, one must have yTC + bTD L [ I 1 * * * I].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(1983), and the 1986 Education Award of The American Automatic Control Council. He is on the editorial board of several engineering and mathematics journals. He is the author of Linear Systems (Englewood Cliffs, NJ: Prentice-Hall, 1980), Lectures on Wiener and Kalman Filtering (New York, NY: Springer-Verlag, 1981), as well as editor of Modern Signal Processing (New York, NY Hemisphere-Springer-Verlag, 1985), and co-editor of VLSl and Modern Signal Processing (Englewood Cliffs, NJ: Prentice-Hall, 1985).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>was born in Poona, India, on June 7,1935. He received the B.E. degree from the Universityof Poona in 1956and the S.M. and Sc.D. degreesfrom the Massachusetts Institute of Technology, Cambridge, in 1959 and 1961, respectively. During 1961-1962 he worked at the Jet Propulsion Laboratories, Pasadena, CA, where he also taught part time at the California Instituteof Technology. He has held shorter term amointments at several institutions around the world, including ihe Ford Fellowship in 1963 at the University of California at Berkeley, a Guggenheim Fellowship in 1970 at the Indian Institute of Science, a Churchill Fellowship in 1977 at the Statistical Laboratory, Cambridge University, England, and a Michael Fellowship in 1984 at the Department of Theoretical Mathematics of the Weizmann Institute, Israel. His research and teaching have been in statistical communications, control, information theory, linear systems, and signal processing. From 1971 to 1978, he was a member of the Board of Governors of the IEEE Professional Group on Information Theory and the IEEE Control Systems Society. During 1975 he served as President of the IEEE Information Theory Group. He has received outstanding paper prizes from the IEEE Information Theory Group (I%), the IEEE Acoustics, Speech, and Signal Processing Society</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMEN~</head><p>The authors wish to thank H. V. Jagadish for his comments during the early course of this work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Joint Services Program at Stanford University (U.S. Army, U.S. Navy, and U.S. Airforce) under Contract DAAC29-85-K-0048, the National Science Foundation under Grant DCI-84-21315-A1, the U.S. Army Research Office under Contract DAAL03-86-K-0045, and the Air Force Office of Scientific Research, Air Force Systems Command, under Contract</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Systolic arrays for VLSI</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sparse Matrix Proceedings</title>
		<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="245" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">lntroduction to VLSl Systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Mead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Conway</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computer oriented towards spatial problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IRE</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1744" to="1750" />
			<date type="published" when="1958-10">Oct. 1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pattern recognition using two-dimensional bilateral iterative combinational switching circuits</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Polytechnic lnstitute of Brooklyn Symp. on Mathematical Theory of Automata</title>
		<meeting>Polytechnic lnstitute of Brooklyn Symp. on Mathematical Theory of Automata<address><addrLine>Brooklyn, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Polytechnic Press</publisher>
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative combinational switching networks-General design consideration</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Mccluskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. flectro. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Iterative Arrays of Logical Circuits</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Hennie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
			<publisher>MIT Press and Wiley</publisher>
			<pubPlace>Cambridge, MA, and New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Awavefront notational tool for VLSl Array design</title>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CMU Conf. on VLSl on Systems a n d Computations</title>
		<meeting><address><addrLine>Pittsburgh, PA Comput</addrLine></address></meeting>
		<imprint>
			<publisher>Sci. Press</publisher>
			<date type="published" when="1981-10">Oct. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">presented at theconferenceof Advanced Research in Integrated Circuits</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>VLSI array processor sf or signal processing</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Regular iterative algorithms and their implementations on processor arrays</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cappelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Steiglitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="23" to="65" />
			<date type="published" when="1984">Oct. 1985. 1984</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Information Systems. Lab., Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note>Unifying VLSl array designs with linear transformations of space time</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the analysis and synthesis of VLSl algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">/€€E Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1121" to="1126" />
			<date type="published" when="1982">1982</date>
			<publisher>NOV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The systmatic design of systolic arrays</title>
		<author>
			<persName><forename type="first">P</forename><surname>Quinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INRIA Rep</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On thedesign of algorithms for VLSl systolic arrays</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. I€€€</title>
		<meeting>I€€€</meeting>
		<imprint>
			<date type="published" when="1983-01">Jan. 1983</date>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Concurrentalgorithmsasspacetime recursion equations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chenand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modern Signal Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A transformational model for VLSl systolic design</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the lnt. Symp. on Hardware Description Languages a n d their Applications</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Uehara</surname></persName>
		</editor>
		<editor>
			<persName><surname>Barbacci</surname></persName>
		</editor>
		<meeting>of the lnt. Symp. on Hardware Description Languages a n d their Applications<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>North-Holland</publisher>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="65" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Thedesign of optimal systolic arrays</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Wah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I€€€ Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="77" />
			<date type="published" when="1985-01">Jan. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Toward a formal treatment of VLSl arrays</title>
		<author>
			<persName><forename type="first">L</forename><surname>Johnsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lennart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Caltech Conf. on VLSI</title>
		<meeting>2nd Caltech Conf. on VLSI</meeting>
		<imprint>
			<date type="published" when="1981-01">Jan. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Space-time representation of computational structures</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Mirankler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="93" to="114" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gracefully degradeable processor arrays</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A B</forename><surname>Fortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPl E (Highly Parallel Signal Processing Architectures)</title>
		<meeting>SPl E (Highly Parallel Signal essing Architectures)</meeting>
		<imprint>
			<date type="published" when="1986-01">Jan. 1986</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="34" to="48" />
		</imprint>
	</monogr>
	<note>What i s a systolic algorithm?</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Techniques for the design of parallel and pipelined VLSl systems for numerical computations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985-12">Dec. 1985</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Information Systems Laboratory, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Regular Processor Arrays for Matrix Algorithms with Pivoting</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Roychowdury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>ISL Preprint, Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The organization of computations for uniform recurrence equations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winograd</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SAL-A single-assignment language for parallel algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Acm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Celoni</surname></persName>
		</author>
		<author>
			<persName><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NOV</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="563" to="590" />
			<date type="published" when="1967">July1981. 226-234. 1985. 1967</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Systems Laboratory, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NP-complete scheduling problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">IO</biblScope>
			<biblScope unit="page" from="384" to="393" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Art of Computing Programming: Sorting and Searching</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Path detection in multidimensional iterative arrays</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Waite</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="300" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hayt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kemmerly</surname></persName>
		</author>
		<title level="m">Engineering Circuit Analysis</title>
		<meeting><address><addrLine>New-York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Design of Processor Arrays</title>
		<title level="s">Prentice-Hall Systems Sciences Series</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On supercomputing with systoliclwavefront array processors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">froc. IEEE</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="867" to="884" />
			<date type="published" when="1984-07">July 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Theory of Matrices</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Macduffee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950">1950</date>
			<publisher>Chelsea Publishing</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Orthorgonal cascade realization of real multi-port digital filters</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deprettere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dewilde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
		<respStmt>
			<orgName>Net-workTheory Section, Delft Univ. of Technol., Delft, The Netherlands</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ageneral theory and synthesis procedure for low sensitivity digital filter structures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECE Rep., Dept. of Elec. and Comput. Eng., Univ. of California</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<pubPlace>Santa Barbara, Sept</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Digital filter structures related to classical filter networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fettweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Elek. Ubertragung</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An illustration of a methodology for the construction of efficient systolic architectures in VLSI</title>
		<author>
			<persName><forename type="first">J-M</forename><surname>Delosme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C F</forename><surname>Ipsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Symp. on VLSl Technology</title>
		<meeting>2nd Int. Symp. on VLSl Technology<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Design methodology for systolic arrays</title>
	</analytic>
	<monogr>
		<title level="j">froc. SPIE</title>
		<imprint>
			<biblScope unit="volume">696</biblScope>
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimizing synchronous circuitry by retiming</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd CalTech Conf. on VLSI</title>
		<meeting>3rd CalTech Conf. on VLSI</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">1981,1982, and 1985 respectively, all in electrical engineering. He is currently with the VLSl Systems Research Department in thecomputer and Robotics Systems Laboratory at AT&amp;T Bell Laboratories</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sailesh Rao was born in Madras</title>
		<meeting><address><addrLine>Stanford, CA; India; Madras, India, theM; Stanford, CA, in; Holmdel, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1959">1987. June 13. 1959</date>
		</imprint>
		<respStmt>
			<orgName>ISL Preprint, Stanford University ; State University of New York, Stony Brook, and the Ph.D. degree from Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>He received the B.Tech. degree from the Indian Institute of Technology</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
