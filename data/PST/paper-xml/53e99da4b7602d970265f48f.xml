<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Nearest Neighbor Method for Efficient ICP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Greenspan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Information Technology Group Institute for Information Technology</orgName>
								<orgName type="institution">National Research Council</orgName>
								<address>
									<addrLine>Bldg. M50, 1500 Montreal Rd</addrLine>
									<postCode>K1A 0R6</postCode>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guy</forename><surname>Godin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Information Technology Group Institute for Information Technology</orgName>
								<orgName type="institution">National Research Council</orgName>
								<address>
									<addrLine>Bldg. M50, 1500 Montreal Rd</addrLine>
									<postCode>K1A 0R6</postCode>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Nearest Neighbor Method for Efficient ICP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">38A7FC798760E0D9CBFB1870026F9D49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>nearest neighbor problem</term>
					<term>correspondence</term>
					<term>iterative closest point</term>
					<term>triangle inequality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel solution is presented to the Nearest Neighbor Problem that is specifically tailored for determining correspondences within the Iterative Closest Point Algorithm. The reference point set È is preprocessed by calculating for each point Ô ¾ È that neighborhood of points which lie within a certain distance ¯of Ô . The points within each ¯-neighborhood are sorted by increasing distance to their respective Ô . At runtime, the correspondences are tracked across iterations, and the previous correspondence is used as an estimate of the current correspondence. If the estimate satisfies a constraint, called the Spherical Constraint, then the nearest neighbor falls within the ¯-neighborhood of the estimate. A novel theorem, the Ordering Theorem, is presented which allows the Triangle Inequality to efficiently prune points from the sorted ¯-neighborhood from further consideration.</p><p>The method has been implemented and is demonstrated to be more efficient than both the k-d tree and Elias methods. After ¼ iterations, fewer than 2 distance calculations were required on average per correspondence, which is close to the theoretical minimum of 1. Furthermore, after 20 iterations the time expense per iteration was demonstrated to be negligibly more than simply looping through the points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Iterative Closest Point Algorithm is overwhelmingly the most widely utilized range data processing method. First formulated by <ref type="bibr">Besl and</ref> McKay <ref type="bibr" target="#b0">[1]</ref> and extended by Chen and Medioni <ref type="bibr" target="#b1">[2]</ref>, ICP iteratively improves the registration of two overlapping surfaces by calculating the unique transformation that minimizes the mean square distance of the correspondences (the mse) between the two surfaces. At each iteration, new correspondences are determined based upon the current relative position of the surfaces.</p><p>Calculating the correspondences has been reputed to account for the bulk of the computational expense of ICP. In their implementation, <ref type="bibr">Besl and</ref> McKay reported that determining the correspondences consumed a full ± of the runtime. If the two surfaces are represented as point sets, then establishing the correspondence for each point is known as the Nearest Neighbor Problem. At each iteration, the correspondent of each point in the floating (i.e. data) surface is defined as that point in the reference (i.e. model) surface which it is nearest to. Correspondence can be defined equivalently when the surfaces are represented otherwise.</p><p>A number of classical general solutions exist to the Nearest Neighbor Problem, including binning methods such as k-d tree <ref type="bibr" target="#b2">[3]</ref> and Elias <ref type="bibr" target="#b3">[4]</ref>, and those which make use of the Triangle Inequality <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Implementations of the ICP typically employ one of these general methods, with the k-d tree likely being the most widely used.</p><p>We present a solution to the Nearest Neighbor Problem which is specifically tailored to ICP. It is motivated by two observations. The first is that the correspondence distances tend to become smaller with each successive ICP iteration. In the aggregate, this is described by the convergence theorem <ref type="bibr" target="#b0">[1]</ref>, which states that the mse will reduce monotonically with each iteration. The second observation is that the Triangle Inequality tends to be a more powerful pruning constraint as the correspondence distances reduce.</p><p>The method has been formulated here for the case where both surfaces are represented as point sets, and it is believed that there are generalizations to other surface representations. The reference point set È is preprocessed by calculating for each Ô ¾ È that neighborhood of points which lie within a certain distance ¯of Ô . The points within each ¯-neighborhood are sorted by increasing distance to their respective Ô . At runtime, the correspondences are tracked across iterations. If a floating point ever falls closer than ¯ ¾ to the reference point to which it corresponded in the previous iteration, then its new correspondence will lie within the ¯-neighborhood of this pre-vious correspondence. The Triangle Inequality is applied to the sorted ¯-neighborhood to efficiently prune out neighborhood points from further consideration <ref type="bibr" target="#b6">[7]</ref>.</p><p>Despite the understanding that the correspondence calculation is the rate limiting step, there has been little previous work reported on techniques for efficient Nearest Neighbor determination in the context of ICP. An exception is Simon et al. <ref type="bibr" target="#b7">[8]</ref> who used a caching method which is similar to the Spherical Constraint presented here. For a given iteration, rather than determining the single nearest neighbor for each query point, they identified small sets of nearest neighbors, which they stored in caches associated with each query point. At the next iteration, assuming that the incremental transformation was small, it was necessary only to search the respective cache to determine the correspondence for each transformed query point. They found this technique to improve the runtime performance of the k-d tree method.</p><p>Blais and Levine <ref type="bibr" target="#b8">[9]</ref> used the inverse calibration parameters of the acquisition sensor to project the points from one image onto the reference frame of the other image. While this method was efficient, it did not identify the true nearest neighbor correspondences between images.</p><p>Other context specific solutions to the Nearest Neighbor problem include Vidal Ruiz et al. <ref type="bibr" target="#b5">[6]</ref> which was based upon the Triangle Inequality. Their method was effective for moderately small point sets, and was applied to a speech recognition problem <ref type="bibr" target="#b9">[10]</ref>. Nene and Nayar <ref type="bibr" target="#b10">[11]</ref> presented a method which was effective mainly for large dimensional spaces.</p><p>The paper continues as follows: Section 2 covers the mathematical preliminaries, and the method is described in detail in Section 3. Section 4 reports the results of an example designed to illustrate its performance. The paper concludes in Section 5 with a summary and some future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mathematical Preliminaries</head><p>In this section, a variation of the Nearest Neighbor Problem is defined which applies directly to ICP. A condition called the Spherical Constraint is presented which limits the neighborhood of points which need be considered in determining the nearest neighbor. The Triangle Constraint is also presented which, according to the Ordering Theorem, further reduces the number of potential points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Nearest Neighbor Problem</head><p>Let È Ô Ò ½ be a set of Ò points in a -dimensional space, and let Õ be a query point. A statement of the Nearest Neighbor Problem is:</p><formula xml:id="formula_0">Find the point Ô in È which is the minimum dis- tance from Õ , i.e. Õ Ô Õ Ô Ô ¾ È (1) p c p i p c S q S q Figure 1: Spherical Constraint</formula><p>We consider here the following variation of the Nearest Neighbor Problem which, as we shall see, will be useful for improving the time efficiency of ICP:</p><formula xml:id="formula_1">Find the point Ô in È which is the minimum dis- tance from Õ , given an initial estimate Ô Ô .</formula><p>One option is to simply ignore Ô and solve the conventional Nearest Neighbor Problem using classical methods, such as k-d tree or Elias. To be relevant, any method which makes use of Ô must improve upon the efficiency of conventional solution methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spherical Constraint</head><p>A property which we shall call the Spherical Constraint holds that any point which is closer to Õ than is the current estimate Ô must lie within the sphere centered on Ô with radius ¾ Õ Ô .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1 (Spherical Constraint)</head><formula xml:id="formula_2">Õ Ô Õ Ô µ Ô Ô ¾ Õ Ô<label>(2)</label></formula><p>Proof: Let Ë Õ be the sphere centered at Õ with radius Õ Ô . From the LHS of Eq.( <ref type="formula" target="#formula_2">2</ref>), Ô must also lie within</p><formula xml:id="formula_3">Ë Õ ; Õ Ô Õ Ô µ Ô ¾ Ë Õ (3)</formula><p>Now let Ë Ô be the sphere centered at Ô with radius</p><formula xml:id="formula_4">¾ Õ Ô , such that Ë Ô completely contains Ë Õ , i.e. Ë Õ Ë Ô . Then, Ô ¾ Ë Õ Ë Ô</formula><p>, rendering the RHS of Eq.( <ref type="formula" target="#formula_2">2</ref>).</p><p>A 2D diagram of the proof of Proposition 1 is illustrated in Figure <ref type="figure">1</ref>.</p><p>The Spherical Constraint describes a necessary but not sufficient condition for closest point identification: there may well be points which satisfy only the RHS of Eq.( <ref type="formula" target="#formula_2">2</ref>). It is nevertheless hypothesized that when the estimate Ô is sufficiently close to Ô , the constraint can be used to rule out many contenders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Triangle Inequality</head><p>Let points Ô , Ô , and Ô be defined in a metric space, and let Ô Ô denote the Euclidean distance between Ô and Ô . A statement of the Triangle Inequality is:</p><formula xml:id="formula_5">Ô Ô Ô Ô Ô Ô Ô Ô • Ô Ô (4)</formula><p>The equality limit on the lower bound holds when the 3 points are collinear, and Ô does not lie between Ô and Ô .</p><p>The equality limit on the upper bound holds when the 3 points are collinear and Ô does lie between Ô and Ô .</p><p>Proposition 2 (Triangle Constraint)</p><formula xml:id="formula_6">Õ Ô Ô Ô Õ Ô µ Õ Ô Õ Ô<label>(5)</label></formula><p>Proof: Reversing the lower bound of Eq.( <ref type="formula">4</ref>) gives</p><formula xml:id="formula_7">Õ Ô Õ Ô Ô Ô</formula><p>. Substituting this into the LHS of Eq.( <ref type="formula" target="#formula_6">5</ref>) gives:</p><formula xml:id="formula_8">Õ Ô Õ Ô Ô Ô Õ Ô</formula><p>which yields RHS.</p><p>The Triangle Constraint in the form of Eq.( <ref type="formula" target="#formula_6">5</ref>) has been well exploited in solutions to the Nearest Neighbor Problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. We present here a novel treatment toward this end which is based upon the following Ordering Theorem. The main idea of the Ordering Theorem is to arrange the points by increasing distance to Ô . Any point which violates the Triangle Constraint can be discounted as a possible nearest neighbor. Further, as the points are arranged in increasing order, all subsequent points will necessarily also violate the Triangle Constraint, and therefore need not be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1 (Ordering Theorem) Let È Ô Ò ½ be a set of points which are ordered by increasing distance to some point</head><formula xml:id="formula_9">Ô : ´µ Ô Ô Ô Ô (6)</formula><p>Let there be a point Ô which is further to Ô than the query point Õ, i.e. Õ Ô Ô Ô . Then the following relation holds:</p><formula xml:id="formula_10">Õ Ô Ô Ô Õ Ô µ Õ Ô Õ Ô<label>(7)</label></formula><p>Figure <ref type="figure">2</ref>: Ordering Theorem Proof: From the lower bound of the Triangle Inequality (Eq.( <ref type="formula">4</ref>)):</p><formula xml:id="formula_11">Õ Ô Ô Ô Õ Ô (8)</formula><p>The ordering of È ensures that Ô and Ô are further from Ô than is Õ, so that:</p><formula xml:id="formula_12">Ô Ô Õ Ô Ô Ô Õ Ô Ô Ô Õ Ô<label>(9)</label></formula><p>Substituting Eqs.( <ref type="formula" target="#formula_12">9</ref>) and ( <ref type="formula">8</ref>) into the LHS of Eq. <ref type="bibr" target="#b6">(7)</ref> gives:</p><formula xml:id="formula_13">Õ Ô Ô Ô Õ Ô Ô Ô Õ Ô Õ Ô µ RHS Corollary 1 Õ Ô Ô Ô Õ Ô µ Õ Ô Õ Ô</formula><p>The above corollary was used in <ref type="bibr" target="#b6">[7]</ref> to accelerate the performance of binning Nearest Neighbor methods. It is not applied in the method presented here, and is included for completeness.</p><p>A geometric interpretation of the Ordering Theorem is illustrated in 2D in Figure <ref type="figure">2</ref>. The points are shown ordered by increasing distance to Ô , i.e. Ô ½ Ô ¾ Ô Ô Ô .</p><p>For convenience we denote the distance to Ô as Ê Ô Ô</p><p>and Ê Õ Õ Ô , and the distance to Õ as Ô Õ . Note that Ô and Ô are not necessarily consecutive, as in Eq.( <ref type="formula">6</ref>). Also, for the sake of illustration the points are shown emanating from Ô fairly linearly. It is, however, only their distance from Ô that is important: they may equivalently be distributed otherwise, as long as their distance from Ô remains the same.</p><p>The three spheres illustrated in Figure <ref type="figure">2</ref> are all centered at Ô . Sphere Ë ¼ has radius Ê Õ . Sphere Ë has radius Ê Õ</p><p>, and sphere Ë • has radius Ê Õ • . By the Ordering Theorem and its Corollary, all points which lie outside of the shaded annulus defined by Ë • Ë will be further from Õ than is Ô . That is, all points which lie either outside of Ë • or inside of Ë need not be considered further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Application to ICP</head><p>The above results can be used to improve the efficiency of ICP. The main idea is to store the value of the correspondences within an iteration so that they are available at the next iteration. In preprocessing, a neighborhood is calculated for each point Ô in the reference set È. At runtime, for each point Õ in the floating set É, the correspondence which was determined at the previous iteration is used as an initial estimate of its current nearest neighbor. The Spherical Constraint is applied to determine whether or not the nearest neighbor falls within the neighborhood of this estimate, and if so, the Triangle Constraint and the Ordering Theorem are applied to the neighborhood to quickly identify the correspondence. A pseudocode representation of the method, which we call Spherical Triangle Constraint Nearest Neighbor (STCNN), is listed in Figure <ref type="figure" target="#fig_0">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>For each Ô ¾ È a neighborhood is calculated comprising those points within È that fall within a distance ¯of Ô . This can be accomplished using a k-d tree, Elias, or other conventional Nearest Neighbor method. We call this neighborhood the ¯-neighborhood of Ô , and store it in a list which is sorted by increasing distance to Ô , denoted by the array Ô ¡ . Each element stores both the point identity Ô and its distance Ô to Ô . For example, if the Ø element of Ô ¡ is Ô , then Ô Ô and Ô Ô Ô . In summary, the properties of an ¯-neighborhood are:</p><formula xml:id="formula_14">(membership) Ô ¾ Ô ¡ ´µ Ô Ô (order) Ô Ô ´µ 3.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Runtime</head><p>At runtime, for each Õ ¾É, the nearest neighbor which was found for Õ in the previous iteration is used as the initial estimate Ô . As the location of Õ has changed since the last iteration, the value Õ Ô must be recalculated.   ¾ Õ Ô then the initial estimate was not tight enough to restrict the search only to Ô ¡ , and it is necessary to apply a conventional method to find the nearest neighbor of Õ . It is also necessary to apply a conventional method in the first iteration where no estimates from previous iterations exist.</p><formula xml:id="formula_15">Ñ Ò Õ Ô if ( ¾ Ñ Ò ¯) // Spherical Constraint satisfied ½ , Ô Ô , ÓÒ Ä Ë while ( ÓÒ ÄË and size of Ô ¡ ) if ( Õ Ô Ô Ñ Ò ) // Triangle Constraint not satisfied Ô Ô ÓÒ Ì Ê Í else // Triangle Constraint satisfied if ( Õ Ô Ñ Ò ) Ñ Ò Õ Ô Ô Ô • • Ô Ô else // Spherical</formula><p>We call this conventional method the companion method.</p><p>Once the Spherical Constraint has been satisfied, one option is to simply calculate the distance to each point in Ô ¡ . As ¯is relatively small, the cardinality of the ¯neighborhood will be small compared to that of È, and so this will be quite efficient.</p><p>The efficiency can be further improved upon by applying the Triangle Constraint to the ordered points of Ô ¡ .</p><p>The nearest neighbor of Õ is initially set to be Ô , and we store its identity in the temporary value Ô Ô . Using the information precomputed and stored in Ô ¡ we then test the Triangle Constraint for successive elements, starting at the first element. If for any</p><formula xml:id="formula_16">Õ Ô Ô Õ Ô</formula><p>then by Eq.( <ref type="formula" target="#formula_6">5</ref>)</p><formula xml:id="formula_17">Õ Ô Õ Ô</formula><p>and so the nearest neighbor cannot be Ô . Furthermore, as the elements of Ô ¡ are sorted by increasing distance to Ô , from the Ordering Theorem the Triangle Constraint will also be violated for all subsequent elements ( ) of Ô ¡ . The nearest neighbor of Õ has therefore been identi- fied as Ô . Alternately, if for any element the Triangle Constraint is satisfied, i.e.</p><formula xml:id="formula_18">Õ Ô Ô Õ Ô</formula><p>then Ô may indeed be closer to Õ than is Ô . Its distance is calculated, and if it is less than the current value of Õ Ô , then the value of Ô is updated to Ô Ô . The test then continues with the next list element.</p><p>The search terminates for this Õ when either the Triangle Constraint is violated, or all elements of Ô ¡ have been visited.</p><p>There is a property of ICP which works in favour of the above conditions being satisfied. The Convergence Theorem <ref type="bibr" target="#b0">[1]</ref> states that the mean square distance of all correspondences (mse) will monotonically reduce with successive iterations. On average, the values of Õ Ô and Õ Ô will therefore tend to reduce with each iteration. Furthermore, it is the nature of ICP that the first few iterations converge very quickly, resulting in large transformations of É, whereas later iterations converge more gradually, resulting in smaller transformations. When a highly accurate result is desired (as often is the case) the majority of the time is spent on iterations where the transformations are small, which is exactly the situation where STCNN becomes most efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Example</head><p>We have implemented the STCNN method and tested it on a number of data sets. We present here details of the results on one representative example, which we believe typified the performance. The example reference point set È was a range image containing 94884 points that was acquired with a Biris range sensor. This image is illustrated in The performance of STCNN was compared with implementations of both the k-d tree and Elias methods. The k-d tree is a strictly binary tree in which each node represents a partition of the -dimensional space. The root of the tree represents the entire space, and the leaf nodes represent subspaces containing mutually exclusive small subsets of È. At any node, only one of the dimensions is used as a discriminator, or key, to partition the space. When the tree is traversed, the single scalar value of the key of Õ is compared with the node key value, and the appropriate branch is followed. When a leaf node is encountered, all of the points resident in the leaf's bin are tested against Õ. Depending on the closeness of the match at a leaf node, and the boundaries of the space partition, the traversal may backtrack to explore alternate branches.</p><p>Whereas the k-d tree method partitions space hierarchically and irregularly, the Elias method groups È into subsets that form regular (congruent and non-overlapping) subregions. The nearest point to the query point Õ is found by searching the subregions in order of their proximity to Õ, until the distance to any remaining region is greater than a) average vs. iterations b) time vs. iterations . The only parameter of the Elias method is the number of bins Î Ø along each axis Ø. For our implementation we assume an equal number of bins per axis, (i.e. Î Ü Î Ý Î Þ Î) so that the total number of bins is equal to Î ¿ .</p><p>The STCNN uses Elias as its companion method, which is required both for initialization of STCNN and whenever the Spherical Constraint is not satisfied.</p><p>The comparison of the implemented methods was based upon two metrics. The first metric is denoted , and is a measure of the number of distance computations which are explicitly executed. For È of cardinality Ò and query point Õ, Ò Ñ , where Ñ is the number of points for which Õ Ô was actually computed.</p><p>The second metric is the measured runtime to complete a number of iterations. While is a direct measure of computational complexity, the runtime is dependent upon implementation specifics. Each implementation was executed on the same platform (a 400 MHz Pentium II) and compiled with the same compiler options, and a moderate and equivalent amount of attention was afforded to the optimization of each. For example, square root calculations were avoided wherever possible, but no assembly level coding or other machine specific techniques were used. It is believed that these implementations (and therefore the measured time values) are representative of what a practitioner in the field would typically reproduce. It is also likely that these results would scale equivalently with any other implementation optimizations, so long as they were applied equally across all methods.</p><p>That the same data set was used for both È and É did not bias the performance of the ICP either for or against any of the methods. It did produce the effect that exact correspondences existed for each point, so that it was possible to identify exactly when the ICP had converged. This occurred at the iteration where each correspondence remained unchanged and the mse vanished. In the test case, complete convergence occurred at iteration 86.</p><p>Each method was tested over a range of parameter values. For the k-d tree, the number of points per leaf node was varied over ¾ ½ ¿¼¼ . The Elias Î parameter was varied over a range of Î ¾ ¼ ½¼¼ . The value of the STCNN neighborhood epsilon was varied over a range of ¯¾ ¾ mm. For these values, the average cardinalities of the ¯-neighborhoods varied respectively between 13 and 117 points.</p><p>The range of results of these tests were plotted in graph. The first is that, after ¾¼ iterations, STCNN is more efficient than either k-d tree or Elias in both and time. The second benefit is that STCNN is less sensitive to the selection of the parameter values Î and ¯. Indeed, most of the variation between the STCNN trials occurred in the earlier iterations, which were more dependent upon the Elias companion method. In later iterations, when most or all points fell within the ¯-neighborhoods, all of the STCNN trials had similar performance.</p><p>The best performing trials from Figure <ref type="figure" target="#fig_2">5</ref>, were replotted in Figure <ref type="figure">6</ref>. This occurs at ¼ for k-d tree, Î ¼ for Elias, and Î ¼ and ¯ ¿ for STCNN. Note that the y-axis in part a) comprises two regions; ¼ ¼¼¼ and ¼¼¼ ½¼¼¼¼¼ . That the optimal value of the number of bins is less for STCNN than for Elias is attributed to the role of the companion method. In STCNN initialization, Elias is used to generate the ¯-neighborhoods, an operation which is slightly more efficient for larger bins. The average cardinality of the ¯-neighborhood for the value ¯ ¿ mm was 28 points.</p><p>After about 40 iterations, ¾. Given that the image contained 94884 points, this meant that on average the distance was explicitly calculated for fewer than 2 points per Õ. This is significantly close to the theoretical minimum of 1.</p><p>Two other trials were also included in this plot, SCNN and NULL. The SCNN method was the same as STCNN, with the exception that it applied only the Spherical Constraint, and not the Triangle Constraint. When the Spherical Constraint was satisfied, the distance was therefore tested for every point in the ¯-neighborhood. In part a), this variation is observed to be only slightly more efficient than Elias by the metric. In part b), it can be seen that the time efficiency of SCNN is between that of STCNN and Elias. This confirms that the Triangle Constraint serves to practically improve the time performance of the method.</p><p>The final curve on the plot of Figure <ref type="figure">6b</ref>), the NULL method, provides an explanation as to why the difference in the time performance between STCNN and Elias is not as great as the orders of magnitude difference observed in . For each iteration, the NULL method simply loops through all points Õ ¾ É and assigns the correspondence of Õ as Ô . As È É, these are in fact the true correspondences, so that the correct transformation is calculated on the first iteration, and the identity transformation thereafter. The significance of the NULL method is that there is no search for the correspondences, so that the computational expense is only the overhead of simply looping through the points in É. After about 20 iterations, it can be seen that the slope of STCNN is roughly the same as the slope of NULL. This means that, within that range, the correspondence computation of STCNN is contributing a negligible expense over that of simply looping through the points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a novel solution to the Nearest Neighbor Problem which is specifically tailored for use with ICP. The method is based upon the application of the Spherical and Triangle Constraints and uses the correspondences determined in the previous iteration as initial estimates. While the Triangle Inequality has been used in many other Nearest Neighbor methods, to the authors' knowledge the Ordering Theorem and its application are a novel contribution.</p><p>The STCNN method has been implemented and demonstrated to be faster than fairly lean versions of the k-d tree and Elias methods. At full convergence, which occurred in the presented example at iteration 86, the time to complete the k-d tree, Elias and STCNN were 943, 251 and 176 seconds respectively. This represented a time savings of ¿¼± over Elias, and greater than ¼¼± over k-d tree. A comparison of the metric, which is a measure of the average number of times the distance was actually calculated, weighed even more in favour of STCNN. After ¼ iterations, fewer than 2 distance calculations where required per point, which is close to the theoretical minimum of 1. Further, after 20 iterations, the time expense per iteration was demonstrated to be only trivially greater than simply looping through the points without executing any correspondence calculations. We conclude that the cost of the correspondence calculation in later iterations has been reduced so as to be negligible.</p><p>In future work, we plan to analyze the complexity expression of the method. We would also like to generalize the results to other surface representations, particularly triangles.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: STCNN Pseudocode guaranteed to be a member of Ô ¡ . Alternately, if ¯</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The floating point set É was an exact copy of Èwhich had been perturbed to a slightly different initial position. The rotational offset was 5 degrees about all 3 axes, and the translational offset in all 3 directions was 10 mm, which was approximately 1±, 7.5±, and 3± of the extent of the image along the Ü-, Ý-, and Þ-axes respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of STCNN, k-d tree, and Elias: ¾ ½ ¿¼¼ , Î ¾ ¼ ½¼¼ , ¯¾ ¾ the distance to the nearest point found, or until all subregions have been processed. Points in each searched subregion are exhaustively examined. Simple modifications to the method allow the search for the Ã nearest neighbors, or for all points closer than a threshold distance. The partitions are axis-aligned, forming squares in 2-D, cubes in 3-D, or hypercubes in -D space for. The only parameter of the Elias method is the number of bins Î Ø along each axis Ø. For our implementation we assume an equal number of bins per axis, (i.e. Î Ü Î Ý Î Þ Î) so that the</figDesc><graphic coords="6,58.47,90.48,480.98,221.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig- ure 5 .Figure 6 :</head><label>56</label><figDesc>Figure 6: Comparison of Best Results</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A method for registration of 3D shapes</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992-02">February 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object modeling by registration of multiple range images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="155" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bentley</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975-09">September 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the optimality of Elias&apos;s algorithm for performing best-match searches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><surname>Rivest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing</title>
		<imprint>
			<date type="published" when="1974">1974</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="678" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A branch and bound algorithm for computing k-nearest neighbors</title>
		<author>
			<persName><forename type="first">Keinosuke</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrenahalli</forename><forename type="middle">M</forename><surname>Narendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="page" from="750" to="753" />
			<date type="published" when="1975-07">July 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An algorithm for finding nearest neighbors in (approximately) constant time</title>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Acceleration of binning nearest neighbor methods</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Michael Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Interface 2000</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">May 14-17 2000</date>
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Realtime 3-D pose estimation using a high-speed range sensor</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. Robotics and Automation</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date>May 8-13 1994</date>
			<biblScope unit="page" from="2235" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Registering multiview range data to create 3D computer objects</title>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="820" to="824" />
			<date type="published" when="1995-08">August 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the use of a metric-space search algorithm (AESA) for fast DTW-based recognition of isolated words</title>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">M</forename><surname>Rulot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Casacumerta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose-Miguel</forename><surname>Benedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="651" to="660" />
			<date type="published" when="1988-05">May 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple algorithm for nearest neighbor search in high dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sameer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shree</forename><forename type="middle">K</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="989" to="1003" />
			<date type="published" when="1997-09">September 1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
