<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DABERT: Dual Attention Enhanced BERT for Semantic Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
							<email>wangsirui@meituan.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Liang</surname></persName>
							<email>liangdi04@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Song</surname></persName>
							<email>songjian20@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Yuntao</forename><surname>Li</surname></persName>
							<email>liyuntao04@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DABERT: Dual Attention Enhanced BERT for Semantic Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based pre-trained language models such as BERT have achieved remarkable results in Semantic Sentence Matching. However, existing models still suffer from insufficient ability to capture subtle differences. Minor noise like word addition, deletion, and modification of sentences may cause flipped predictions. To alleviate this problem, we propose a novel Dual Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention module, which measures soft word matches by introducing a new dual channel alignment mechanism to model affinity and difference attention. (2) Adaptive Fusion module, this module uses attention to learn the aggregation of difference and affinity features, and generates a vector describing the matching details of sentence pairs. We conduct extensive experiments on well-studied semantic matching and robustness test datasets, and the experimental results show the effectiveness of our proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic Sentence Matching (SSM) is a fundamental NLP task. The goal of SSM is to compare two sentences and identify their semantic relationship. In paraphrase identification, SSM is used to determine whether two sentences are paraphrase or not <ref type="bibr" target="#b17">(Madnani et al., 2012)</ref>. In natural language inference task, SSM is utilized to judge whether a hypothesis sentence can be inferred from a premise sentence <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>. In the answer sentence selection task, SSM is employed to assess the relevance between query-answer pairs and rank all candidate answers <ref type="bibr" target="#b30">(Wang et al., 2020)</ref>.</p><p>Across the rich history of semantic sentence matching research, there have been two main streams of studies for solving this problem. One 1 These authors contributed equally to this work. 2 Corresponding author.</p><p>The secretaries knew the students.</p><p>The secretaries knew the students slept .  is to utilize a sentence encoder to convert sentences into low-dimensional vectors in the latent space, and apply a parameterized function to learn the matching scores between them <ref type="bibr" target="#b22">(Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b30">Wang et al., 2020)</ref>. Another paradigm adopts attention mechanism to calculate scores between tokens from two sentences, and then the matching scores are aggregated to make a sentence-level decision <ref type="bibr" target="#b2">(Chen et al., 2016;</ref><ref type="bibr" target="#b25">Tay et al., 2017)</ref>. In recent years, pre-trained models, such as BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, RoBERTa <ref type="bibr" target="#b16">(Liu et al., 2019)</ref>, have became much more popular and achieved outstanding performance in SSM. Recent work also attempts to enhance the performance of BERT by injecting knowledge into it, such as Sem-BERT <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>, UER-BERT <ref type="bibr" target="#b33">(Xia et al., 2021)</ref>, Syntax-BERT <ref type="bibr" target="#b0">(Bai et al., 2021)</ref> and so on.</p><p>Although previous studies have provided some insights, those models do not perform well in distinguishing sentence pairs with high literal similarities but different semantics. Figure <ref type="figure" target="#fig_1">1</ref> demonstrates several cases suffering from this problem. Although the sentence pairs in this figure are semantically different, they are too similar in literal for those pre-trained language models to distinguish accurately. This could be caused by the self-attention architecture itself. Self-attention mechanism focuses on using the context of a word to understand the semantics of the word, while ignoring model-ing the semantic difference between sentence pairs. De-attention <ref type="bibr" target="#b24">(Tay et al., 2019)</ref> and Sparsegen <ref type="bibr" target="#b19">(Martins and Astudillo, 2016)</ref> have proved that equipping with attention mechanism with more flexible structure, models can generate more powerful representations. In this paper, we also focus on enhancing the attention mechanism in transformerbased pre-trained models to better integrate difference information between sentence pairs. We hypothesize that paying more attention to the finegrained semantic differences, explicitly modeling the difference and affinity vectors together will further improve the performance of pre-trained model. Therefore, two systemic questions arise naturally: Q1: How to equip vanilla attention mechanism with the ability on modeling semantics of fine-grained differences between a sentence pair? Vanilla attention, or named affinity attention, less focuses on the fine-grained difference between sentence pairs, which may lead to error predictions for SSM tasks. An intuitive solution to this problem is to make subtraction between representation vectors to harvest their semantic differentiation. In this paper, we propose a dual attention module including a difference attention accompanied with the affinity attention. The difference attention uses subtraction-based cross-attention to aggregate word-and phrase-level interaction differences. Meanwhile, to fully utilize the difference information, we use dual-channel inject the difference information into the multi-head attention in the transformer to obtain semantic representations describing affinity and difference respectively. Q2: How to fuse two types of semantic representations into a unified representation? A hard fusion of two signals by extra structure may break the representing ability of the pre-trained model. How to inject those information softly to pre-trained model remains a hard issue. In this paper, we propose an Adaptive Fusion module, which uses an additional attention to learn the difference and affinity features to generate vectors describing sentence matching details. It first inter-aligns the two signals through distinct attentions to capture semantic interactions, and then uses gated fusion to adaptively fuse the difference features. Those generated vectors are further scaled with another fuse-gate module to reduce the damage of the pretrained model caused by the injection of difference information. The output final vectors can better describe the matching details of sentence pairs. Our main contributions are three fold: Finally, a vector is output including more fine-grained matching details. In the following sections, we explain each component in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dual Attention Module</head><p>In this module, we use two distinct attention functions, namely affinity attention and difference attention, to compare the affinities and differences of vectors between two sentences. The input of the dual attention module is a triple of K, Q, V ∈ R dseq×dv , where d v is the latent dimension and d seq is the utterance length. Dual attention module calculate the latent relationship between K, Q and V via two separate attention mechanism to measure their affinity and difference. As a result, two set of attention representations are generated by the dual attention module, which will be fused by the following adaptive fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Affinity Attention</head><p>The affinity attention module is the part of the dual attention module, which is the standard dot-product attention that operates following Transformer's default operation. The input of affinity attention module consists of queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the values. For the sake of simplicity, the formulations of BERT not be repeated here, please refer to <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> for more details. We denote the output affinity vector as:</p><formula xml:id="formula_0">A = sof tmax( QK T √ d k ) * V,<label>(1)</label></formula><p>where A = {a 1 , ..., a l } ∈ R d l ×dv denotes the vector describing affinity expressions generated by the Transformer original attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Difference Attention</head><p>The second part of dual attention module is a difference attention module that capture and aggregate the difference information between sentence pairs. The difference attention module adopts a subtraction-based cross-attention mechanism, which allows model to pay attention to dissimilar parts between sentence pairs by element-wise subtraction as:</p><formula xml:id="formula_1">D = sof tmax( β √ d k ) * V,<label>(2)</label></formula><formula xml:id="formula_2">β = ∥Q − K∥ + M,<label>(3)</label></formula><formula xml:id="formula_3">∥Q − K∥ ij = d k k=0 Q ik − K jk ,<label>(4)</label></formula><p>where ∥Q − K∥ ∈ R d l ×d l and d l is the input sequence length. We use</p><formula xml:id="formula_4">D = {d 1 , ..., d l } ∈ R d l ×dv</formula><p>to denote the representation generated by the difference attention. The M ∈ R d l ×d l is a masking operation. Both the affinity attention and the difference attention are utilized to fit the semantic relationship of sentence pairs, and obtain the representations with the same dimension from the perspective of affinity and difference respectively. This dual channel mechanism can obtain more detailed representations describing sentence matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive Fusion Module</head><p>After obtaining the affinity signals A and the difference signals D, we introduce a novel adaptive fusion module to fuse these two signals instead of direct fusion (i.e., average embedding vector), since direct fusion may compromise the original representing ability of the pre-trained model. The fusion process includes three steps. First, it flexibly interacts and aligns these two signals via affinity-guided attention and difference-guided attention. Second, multiple gate modules are adopted to selectively extract interaction semantic information. Finally, to alleviate the damage of the pre-trained model by the difference signal, we utilize filter gates to adaptively filter out noisy information and finally generate vectors that better describe the details of sentence matching. Firstly, we update the difference vectors through affinity-guided attention. We use a i and d i to denote the i-th dimension of A and D respectively. We provide each affinity vector a i to interact with the difference signals matrix D and obtain the new difference feature d * i . Then, based on d * i , we can in turn acquire the new Affinity feature a * i through difference-guided attention. The calculation process is as follows:</p><formula xml:id="formula_5">δ i = tanh(W D D ⊕ (W a i a i + b a i )), d i = D * sof tmax(W d i δ i + b d i ), γ i = tanh(W A A ⊕ (W d i d i + b d i )), a i = A * sof tmax(W a i γ i + b a * i ), d * i = tanh(W d * i ([d i ; d i ]) + b d * i )), a * i = tanh(W a * i ([a i ; a i ]) + b a * i )),<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">W D , W A , W a i , W d i ∈ R d l * dv ; W d i , W a i ∈ R 1 * 2d l ; b d * i , b a i , b d i , b a *</formula><p>i are weights and bias of our model, and ⊕ denotes the concatenation of signal matrix and feature vector. Secondly, to adaptively capture and fuse useful information from Affinity and difference features, we introduce our gate fusion modules:</p><formula xml:id="formula_7">di = tanh(W di d * i + b di ), âi = tanh(W âi a * i + b âi ), g i = σ(W g i ( di ⊕ âi )), v i = g i âi + (1 − g i ) di , (6) where W di , W âi ∈ R d h * dv ; W g i ∈ R 1 * 2d h ; b di , b âi are</formula><p>parameters and d h is the size of hidden layer. σ is the sigmoid activation function and g i is the gate that determines the transmission of these two distinct representations. By the way, we get the fusion feature v i .</p><p>Eventually, considering the potential noise problem, we propose a filtering gate to selectively leverage the fusion feature. When v i tends to be beneficial, the filtration gate will incorporate the fusion features and the original features. Otherwise, the fusion information will be filtered out:</p><formula xml:id="formula_8">f i = σ(W f i ,a i (a i ⊕ (W v i v i + b v i ))), l i = f i * tanh(W l i v i + b l i ),<label>(7)</label></formula><p>where </p><formula xml:id="formula_9">W f i ,a i ∈ R 1 * 2dv ; W v i , W l i ∈ R dv * d h ; b v i ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>To evaluate the effectiveness of our proposed DABERT in SSM, we mainly introduce BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, SemBERT <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>, SyntaxBERT <ref type="bibr" target="#b15">(Liu et al., 2020)</ref>, UERBERT <ref type="bibr" target="#b33">(Xia et al., 2021)</ref> and multiple other PLMs <ref type="bibr" target="#b21">(Radford et al., 2018;</ref><ref type="bibr" target="#b5">Devlin et al., 2018)</ref> for comparison. In addition, we also select several competitive models without pre-training as baselines, such as ESIM <ref type="bibr" target="#b2">(Chen et al., 2016)</ref>, Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> , etc <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b31">Wang et al., 2017;</ref><ref type="bibr" target="#b25">Tay et al., 2017)</ref>. In robustness experiments, we compare the performance of multiple pre-trained models <ref type="bibr" target="#b23">(Sanh et al., 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2016;</ref><ref type="bibr" target="#b5">Devlin et al., 2018;</ref><ref type="bibr" target="#b10">Lan et al., 2019)</ref>  and SemBERT,UERBERT and Syntax-BERT on the robustness test datasets. For simplicity, the compared models are not described in detail here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>DABERT is based on BERT-base and BERT-large. For distinct targets, our hyper-parameters are different. We use AdamW in the BERT and set the learning rate in {1e − 5, 2e − 5, 3e − 5, 8e − 6}. As for the learning rate decay, we use a warmup of 0.1 and L2 weight decay of 0.01. Furthermore, we set the epoch to 5 and the batch size is selected in {16, 32, 64}. We also set dropout at 0.1-0.3. To prevent gradient explosion, we set gradient clipping in {7.5, 10.0, 15.0}. All the experiments are conducted by Tesla V100 and PyTorch platform. In addition, to ensure that the experimental results are statistically significant, we conduct each experiment five times and report the average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Performance</head><p>In our experiments, we implement DABERT in the initial transformer layer of BERT. First, we fine-tune our model on 6 GLUE datasets. Table <ref type="table" target="#tab_2">1</ref> shows the performance of DABERT and other competitive models. It can be seen that using only non-pretrained models performs obviously worse than PLMs due to their strong context awareness and data fitting capabilities. When the backbone model is BERT-base or BERT-large, the average accuracy of DABERT respectively improves by 1.7% and 2.3% than vanilla BERT. Such great improvement demonstrates the benefit of fusion difference attention for mining semantics and proves that our framework can help BERT perform much better in SSM.</p><p>Moreover, compared with some previous works such as SemBERT, UERBERT and SyntaxBERT, DABERT achieves the best performance without injecting external knowledge. Specifically, our model outperforms SyntaxBERT, the best performing model in previous work leveraging external knowledge, with an average relative improvement of 0.86% based on BERT-large. On the QQP dataset, the accuracy of DABERT is significantly improved by 2.4% over SyntaxBERT. There are two main reasons for such results. On the one hand, we use dual-channel attention to enhance the ability of DABERT to capture difference features. This enables DABERT to obtain more fine-grained interaction matching features. On the other hand, for the potential noise problem introduced by external structures, our adaptive fusion module can selectively filter out inappropriate information to suppress the propagation of noise, and previous work does not seem to pay enough attention to this problem. However, we still notice that SyntaxBERT achieves slightly better accuracy on a few datasets. We argue that this is a result of the intrinsic correlation of syntactic and dependent knowledge.</p><p>Second, to verify the general performance of our method, we also conduct experiments on other popular datasets. The results are shown in Table 2. DABERT still outperforms vanilla BERT and other models on almost all datasets. It is worth noting that DABERT performs worse than SyntaxBERT on SICK. This may be because the data volume of SICK is relatively small, and Syn-taxBERT uses syntactic prior knowledge, which makes SyntaxBERT more advantageous on small datasets. but DABERT still shows a very competitive performance on SICK, which also shows from the side that our method can enhance the difference capture ability of BERT and make up for the lack of generalization ability with fewer parameters.</p><p>Overall, our method has competitive performance in judging semantic similarity compared to previous work. Extensive performance improvements also validate our point, soft ensemble difference information based on BERT's powerful contextual representation capability is useful for sentence matching tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Robustness Test Performance</head><p>In order to examine the performance of DABERT and competitive models in their ability to capture subtle differences in sentence pairs. We perform robustness tests on three extensively studied datasets.</p><p>Table <ref type="table" target="#tab_3">3</ref> lists the accuracy of DABERT and six baseline models on the three datasets. We can observe that SwapAnt leads to a drop in maximum performance, and our model outperforms the best model SemBert nearly 10% on SwapAnt(QQP), which indicates that DABERT can better handle semantic contradictions caused by antonyms than baseline models. And the model performance drops to 56.96% on NumWord transformation, while DABERT outperforms BERT by nearly 6% because it requires the model to capture subtle numerical differences for correct linguistic inference. In SwapSyn transformation, UERBERT significantly outperforms other baseline models because it explicitly uses the synonym similarity matrix to calibrate the attention distribution, while our model can still achieve comparable performance to UER-BERT without adding external knowledge. On TwitterType and AddPunc, the performance of Syn-taxBERT by injecting syntax trees degrades significantly, probably because converting text to twitter type or adding punctuation breaks the normal syntactic structure of sentences. And DABERT still achieves the competitive performance in these two transformations. In other scenarios, DABERT also achieve better performance due to capturing subtle differences in sentence pairs. Meanwhile, ESIM has the worst performance, the results reflect that the pre-training mechanism benefits from rich external resources and provides better generalization ability than de novo trained models. And the improved pre-trained model SyntaxBERT performs better than the original BERT model, which reflects that sufficient pre-trained corpus and suitable external knowledge fusion strategies can help improve the generalization performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To evaluate the contribution of each component in our method, we conduct ablation experiments on the QQP and QNLI datasets based on BERT. The experimental results are shown in the table <ref type="table" target="#tab_5">5</ref>. Above all, the dual attention module consists of two core components that use a two-channel mechanism to model affinity and difference attention. First, after removing affinity attention, the performance of the model on the two datasets drops by 1.8% and 0.7%. Affinity attention can capture the dynamic alignment relationship between word pairs, which is crucial for SSM tasks. Next,after removing difference attention from the model, the performance on the two datasets dropped by 1.5% and 0.6%, respectively. The difference information can further describe the interaction between words, and can provide more fine-grained comparison information for the pre-trained model, so that the model can obtain a better representation.</p><p>The above experiments show that the performance drops sharply after the submodule is removed, which demonstrates the effectiveness of the internal components of the dual attention module.</p><p>Next, in the adaptive fusion module, we also conducted several experiments to verify the effect of the fusion of affinity and difference vectors. On the QQP dataset, we first remove the guide attention module, and the performance drops to 90.4%. Since guide attention can capture the interaction between two signals, this interaction information is crucial for fusing two different information. Second, after removing the fusion gate, we only integrate two signals by simple averaging. The accuracy dropped to 91.4%, indicating that dynamically merging the affinity and difference vectors according to different weights can improve the performance of the model. Then, when the filter gate is removed, the accuracy drops by 0.4%, indicating that the ability of the model to suppress noise is weakened without the filter gate. Finally, we also replaced the overall aggregation and Regulation module with simple average, and the performance dropped sharply to 89.4%. While difference information is crucial for judging sentence-pair relations, hard-integrating the difference information into the PLMs will destroy its Pre-existing knowledge, and soft aggregation and governance can make better use of difference signals.</p><p>Overall, due to the effective combination of each component, DABERT can adaptively fuse difference features into pretrained models and leverage its powerful contextual representation to better inference about semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>To visualize how DABERT works, we use three cases from the table 4 for qualitative analysis. In the first case, the non-pretrained language model ESIM is difficulty capturing the semantic conflicts caused by the difference words. Therefore, ESIM  gives wrong prediction results in case 1. BERT can identify the semantic difference in case 1 with the help of context representation . But in case 3, BERT cannot capture the difference between the numbers "12" and "24" and give wrong prediction. SyntaxBERT enhances text understanding by introducing syntactic trees. Since case 2 and case 3 have the same syntactic structure, Syn-taxBERT also gives wrong predictions. Our model made correct predictions in all of the above cases.</p><p>Because DABERT explicitly focuses on different parts of sentence pairs through difference attention and adaptively aggregates affinity and difference information in the adaptive fusion module, it can identify semantic differences caused by subtle differences within sentence pairs. Attention Distribution. To verify the fusion effect of subtraction-based attention on the difference information, we display the weights distribution of BERT and DABERT in Figure <ref type="figure" target="#fig_3">3</ref> for comparison. It can be seen that the attention distribution after dual attention becomes more reasonable, especially the attention weight between "hardware" and "software" increases significantly. This reveals that DABERT pays more attention to different parts of sentence pairs rather than the same words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Semantic Sentence Matching is a fundamental task in NLP. Thanks to the appearance of largescale annotated datasets <ref type="bibr" target="#b1">(Bowman et al., 2015;</ref><ref type="bibr" target="#b32">Williams et al., 2017)</ref>, neural network models have made great progress in SSM <ref type="bibr" target="#b20">(Qiu and Huang, 2015;</ref><ref type="bibr" target="#b28">Wan et al., 2016)</ref>, mainly fell into two categories. The first <ref type="bibr" target="#b4">(Conneau et al., 2017;</ref><ref type="bibr" target="#b3">Choi et al., 2018)</ref> focuses on encoding sentences into corresponding vectors without cross-interaction and applies a classifier to obtain similarity. The second <ref type="bibr" target="#b31">(Wang et al., 2017;</ref><ref type="bibr" target="#b2">Chen et al., 2016;</ref><ref type="bibr" target="#b12">Liang et al., 2019a)</ref> utilizes cross-features as an attention module to express the word-or phrase-level alignments of two texts, and aggregates it into prediction layer to acquire sim- ilarity. Recently, the pre-training paradigm has achieved great results in SSM. Some work attempt to introduce other methods to enhance pre-trained models. For example, SemBERT <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref> explicitly absorbs contextual semantics over a BERT backbone. AMAN <ref type="bibr" target="#b13">(Liang et al., 2019b)</ref> uses answers knowledge to enhance language representation. UER-BERT <ref type="bibr" target="#b33">(Xia et al., 2021)</ref> injects synonym knowledge to enhance BERT. Syntax-BERT <ref type="bibr" target="#b0">(Bai et al., 2021</ref>) also integrates the syntax tree into transformer models. Robustness Although neural network models have achieved human-like or even superior results in multiple tasks, they still face the insufficient robustness problem in real application scenarios <ref type="bibr" target="#b6">(Gui et al., 2021)</ref>. Tiny literal changes may cause misjudgments. Therefore, recent work starts to focus on robustness research from multiple perspectives. TextFlint <ref type="bibr" target="#b6">(Gui et al., 2021)</ref> incorporates multiple transformations to provide comprehensive robustness analysis. <ref type="bibr" target="#b11">Li et al. (2021)</ref> provide an overall benchmark for current work on adversarial attacks. And <ref type="bibr" target="#b14">Liu et al. (2021)</ref> propose a more comprehensive evaluation system and add more detailed output analysis indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a novel Dual Attention Enhanced BERT (DABERT), which can efficiently aggregate the difference information in sentence pairs and soft-integrate it into a pretrained model. Based on BERT's powerful contextual representation capability, DABERT enables the model to learn more fine-grained comparative information and enhances the sensitivity of PLMs to subtle differences. Experimental results on 10 public datasets and robustness dataset show that our method can achieve better performance than several strong baselines. Since DABERT is an end-to-end training component, it is expected to be applied to other large-scale pre-trained models in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example sentences with similar text but different semantics. S1 and S2 are sentence pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall architecture of Dual Attention Enhanced BERT (DABERT). The left side is the Dual attention module, and the right side is the Adaptive Fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of BERT (a) and our method (b).</figDesc><graphic url="image-1.png" coords="8,341.06,85.75,76.09,74.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The performance comparison of DABERT with other methods. We report Accuracy × 100 on 6 GLUE datasets. Methods with † indicate the results from their papers, while methods with ‡ indicate our implementation.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">Pre-train</cell><cell cols="3">Sentence Similarity</cell><cell cols="2">Sentence Inference</cell><cell>Avg</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">MRPC QQP SST-B MNLI-m/mm QNLI RTE</cell></row><row><cell>BiMPM †(Wang et al., 2017)</cell><cell></cell><cell></cell><cell cols="2">79.6</cell><cell>85.0</cell><cell>-</cell><cell>72.3/72.1</cell><cell>81.4</cell><cell>56.4</cell><cell>-</cell></row><row><cell>CAFE †(Tay et al., 2017)</cell><cell></cell><cell></cell><cell cols="2">82.4</cell><cell>88.0</cell><cell>-</cell><cell>78.7/77.9</cell><cell>81.5</cell><cell>56.8</cell><cell>-</cell></row><row><cell>ESIM †(Chen et al., 2016)</cell><cell></cell><cell></cell><cell cols="2">80.3</cell><cell>88.2</cell><cell>-</cell><cell>-</cell><cell>80.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Transformer †(Vaswani et al., 2017)</cell><cell></cell><cell></cell><cell cols="2">81.7</cell><cell>84.4</cell><cell>73.6</cell><cell>72.3/71.4</cell><cell>80.3</cell><cell>58.0 74.53</cell></row><row><cell cols="2">BiLSTM+ELMo+Attn †(Devlin et al., 2018)</cell><cell></cell><cell cols="2">84.6</cell><cell>86.7</cell><cell>73.3</cell><cell>76.4/76.1</cell><cell>79.8</cell><cell>56.8 76.24</cell></row><row><cell cols="2">OpenAI GPT †(Radford et al., 2018)</cell><cell></cell><cell cols="2">82.3</cell><cell>70.2</cell><cell>80.0</cell><cell>82.1/81.4</cell><cell>87.4</cell><cell>56.0 77.06</cell></row><row><cell>UERBERT ‡(Xia et al., 2021)</cell><cell></cell><cell></cell><cell cols="2">88.3</cell><cell>90.5</cell><cell>85.1</cell><cell>84.2/83.5</cell><cell>90.6</cell><cell>67.1 84.19</cell></row><row><cell>SemBERT †(Zhang et al., 2020)</cell><cell></cell><cell></cell><cell cols="2">88.2</cell><cell>90.2</cell><cell>87.3</cell><cell>84.4/84.0</cell><cell>90.9</cell><cell>69.3 84.90</cell></row><row><cell>BERT-base ‡(Devlin et al., 2018)</cell><cell></cell><cell></cell><cell cols="2">87.2</cell><cell>89.0</cell><cell>85.8</cell><cell>84.3/83.7</cell><cell>90.4</cell><cell>66.4 83.83</cell></row><row><cell cols="2">SyntaxBERT-base †(Bai et al., 2021)</cell><cell></cell><cell cols="2">89.2</cell><cell>89.6</cell><cell>88.1</cell><cell>84.9/84.6</cell><cell>91.1</cell><cell>68.9 85.20</cell></row><row><cell>DABERT-base ‡</cell><cell></cell><cell></cell><cell cols="2">89.1</cell><cell>91.3</cell><cell>88.2</cell><cell>84.9/84.7</cell><cell>91.4</cell><cell>69.5 85.58</cell></row><row><cell>BERT-large ‡(Devlin et al., 2018)</cell><cell></cell><cell></cell><cell cols="2">89.3</cell><cell>89.3</cell><cell>86.5</cell><cell>86.8/85.9</cell><cell>92.7</cell><cell>70.1 85.80</cell></row><row><cell cols="2">SyntaxBERT-large †(Bai et al., 2021)</cell><cell></cell><cell cols="2">92.0</cell><cell>89.5</cell><cell>88.5</cell><cell>86.7/86.6</cell><cell>92.8</cell><cell>74.7 87.26</cell></row><row><cell>DABERT-large ‡</cell><cell></cell><cell></cell><cell cols="2">91.4</cell><cell>91.9</cell><cell>89.5</cell><cell>87.1/86.9</cell><cell>94.8</cell><cell>75.3 88.12</cell></row><row><cell>Model</cell><cell cols="3">SNLI Sci SICK Twi</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ESIM †(Chen et al., 2016)</cell><cell>88.0 70.6</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CAFE †(Tay et al., 2017)</cell><cell cols="2">88.5 83.3 72.3</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CSRAN †(Tay et al., 2018)</cell><cell>88.7 86.7</cell><cell>-</cell><cell>84.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base ‡(Devlin et al., 2018)</cell><cell cols="3">90.7 91.8 87.2 84.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UERBERT ‡(Xia et al., 2021)</cell><cell cols="3">90.8 92.2 87.8 86.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SemBERT †(Zhang et al., 2020)</cell><cell cols="3">90.9 92.5 87.9 86.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SyntaxBERT-base †(Bai et al., 2021) 91.0 92.7 88.7 87.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DABERT-base ‡</cell><cell cols="3">91.3 93.6 88.6 87.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-large ‡(Devlin et al., 2018)</cell><cell cols="3">91.0 94.4 91.1 91.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SyntaxBERT-large †(Bai et al., 2021) 91.3 94.7 91.4 92.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DABERT-large ‡</cell><cell cols="3">91.5 95.3 92.5 92.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 2: The performance comparison of DABERT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">with other methods on 4 popular datasets, including</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">SNLI, Scitail(Sci), SICK and TwitterURL(Twi).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The robustness experiment results of DABERT and other models. The data transformation methods we</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>Quora</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SNLI</cell><cell></cell></row><row><cell></cell><cell>SA</cell><cell>NW</cell><cell>IA</cell><cell>Al</cell><cell>BT</cell><cell>AS</cell><cell>SA</cell><cell>TT</cell><cell>SN</cell><cell>SW</cell></row><row><cell>ESIM †(Chen et al., 2016)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">64.00 84.22 78.32 53.76 65.38</cell></row><row><cell>BERT ‡(Devlin et al., 2018)</cell><cell cols="5">48.58 56.96 86.32 85.48 83.42</cell><cell cols="5">79.66 94.84 83.56 50.45 76.42</cell></row><row><cell>ALBERT ‡(Lan et al., 2019)</cell><cell cols="5">51.08 55.24 81.87 78.94 82.37</cell><cell cols="5">45.17 96.37 81.62 57.66 74.93</cell></row><row><cell>UERBERT ‡(Xia et al., 2021)</cell><cell cols="5">48.57 54.86 84.72 80.88 82.71</cell><cell cols="5">73.24 94.78 85.36 57.54 80.81</cell></row><row><cell>SemBERT ‡(Zhang et al., 2020)</cell><cell cols="5">50.92 53.15 85.19 82.04 82.40</cell><cell cols="5">76.81 95.31 84.60 56.28 77.86</cell></row><row><cell>SyntaxBERT ‡(Bai et al., 2021)</cell><cell cols="5">49.30 56.37 86.43 84.62 84.19</cell><cell cols="5">78.63 95.02 86.91 58.26 76.90</cell></row><row><cell>DABERT ‡</cell><cell cols="5">60.43 62.76 87.50 85.48 87.49</cell><cell cols="5">81.06 96.85 85.14 60.58 80.92</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MNLI-m/mm</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>AS</cell><cell></cell><cell>SA</cell><cell></cell><cell>AP</cell><cell>TT</cell><cell></cell><cell>SN</cell><cell></cell><cell>SW</cell></row><row><cell>BERT ‡(Devlin et al., 2018)</cell><cell cols="2">55.32/55.25</cell><cell>52.76/55.69</cell><cell cols="2">82.30/82.31</cell><cell>77.08/77.22</cell><cell></cell><cell>51.97/51.84</cell><cell cols="2">76.41/77.05</cell></row><row><cell>ALBERT ‡(Lan et al., 2019)</cell><cell cols="2">53.09/53.58</cell><cell>50.25/50.20</cell><cell cols="2">83.98/83.68</cell><cell>77.98/78.03</cell><cell></cell><cell>56.43/50.03</cell><cell cols="2">76.63/77.43</cell></row><row><cell>UERBERT ‡(Xia et al., 2021)</cell><cell cols="2">54.99/54.84</cell><cell>52.29/53.80</cell><cell cols="2">79.80/79.18</cell><cell>75.46/74.93</cell><cell></cell><cell>55.21/55.96</cell><cell cols="2">82.23/82.74</cell></row><row><cell>SemBERT ‡(Zhang et al., 2020)</cell><cell cols="2">55.38/55.12</cell><cell>54.07/54.62</cell><cell cols="2">78.70/78.16</cell><cell>73.90/73.47</cell><cell></cell><cell>53.43/53.76</cell><cell cols="2">78.09/78.93</cell></row><row><cell>SyntaxBERT ‡(Bai et al., 2021)</cell><cell cols="2">54.92/54.63</cell><cell>53.54/54.73</cell><cell cols="2">77.01/76.71</cell><cell>70.38/70.13</cell><cell></cell><cell>57.11/51.95</cell><cell cols="2">78.57/79.31</cell></row><row><cell>DABERT ‡</cell><cell cols="2">60.14/59.25</cell><cell>60.89/61.37</cell><cell cols="2">83.23/83.19</cell><cell>77.94/78.10</cell><cell></cell><cell>60.12/59.83</cell><cell cols="2">82.15/82.97</cell></row></table><note>utilized mainly include SwapAnt (SA), NumWord (NW), AddSent (AS), InsertAdv (IA), Appendlrr (Al), AddPunc (AP), BackTrans (BT), TwitterType (TT), SwapNamedEnt (SN), SwapSyn-WordNet (SW).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The example sentence pairs of our cases. Red and Blue are difference phrases in sentence pair.</figDesc><table><row><cell>Case</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ESIM</cell><cell>BERT</cell><cell>SyntaxBERT</cell><cell>DABERT</cell></row><row><cell cols="5">S1:How done you solve this aptitude question? S2:How does I solve aptitude questions on cube?</cell><cell>label:1</cell><cell>label:0</cell><cell>label:0</cell><cell>similarity:10.87% label:0</cell></row><row><cell cols="4">S1:How can I tell if this girl loves me? S2:How can I tell if this boy loves me?</cell><cell></cell><cell>label:1</cell><cell>label:1</cell><cell>label:1</cell><cell>similarity:12.06% label:0</cell></row><row><cell cols="5">S1:How many 12 digits number have the sum of 4? S2:How many 42 digits number have the sum of 4?</cell><cell>label:1</cell><cell>label:1</cell><cell>label:1</cell><cell>similarity:18.63% label:0</cell></row><row><cell>Model</cell><cell cols="2">Quora</cell><cell cols="2">QNLI</cell></row><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>DABERT</cell><cell>92.1</cell><cell>91.3</cell><cell>92.9</cell><cell>91.4</cell></row><row><cell>w/o Affi-attention</cell><cell>90.1</cell><cell>89.5</cell><cell>91.8</cell><cell>90.7</cell></row><row><cell>w/o Diff-attention</cell><cell>90.6</cell><cell>89.8</cell><cell>92.0</cell><cell>90.8</cell></row><row><cell>w/o Guide-attention</cell><cell>91.3</cell><cell>90.4</cell><cell>92.1</cell><cell>91.0</cell></row><row><cell>w/o Gate fusion</cell><cell>91.7</cell><cell>90.6</cell><cell>92.5</cell><cell>91.1</cell></row><row><cell>w/o Gate filter</cell><cell>91.8</cell><cell>90.9</cell><cell>92.6</cell><cell>91.2</cell></row><row><cell>w/o Regulation</cell><cell>89.9</cell><cell>89.4</cell><cell>91.5</cell><cell>90.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of component ablation experiment.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The statistics of all 10 datasets.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://huggingface.co/datasets/glue</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.textflint.io</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Syntaxbert: Improving pre-trained transformers with syntax trees</title>
		<author>
			<persName><forename type="first">Jiangang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04350</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06038</idno>
		<title level="m">Enhanced lstm for natural language inference</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to compose task-specific tree structures</title>
		<author>
			<persName><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02364</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinzhuo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11441</idno>
		<title level="m">Textflint: Unified multilingual robustness evaluation toolkit for natural language processing</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scitail: A textual entailment dataset from science question answering</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A continuously growing dataset of sentential paraphrases</title>
		<author>
			<persName><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00391</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiehang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12777</idno>
		<title level="m">Searching for an effective defender: Benchmarking defense against adversarial word substitution</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Asynchronous deep interaction network for natural language inference</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fubao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive multi-attention network incorporating answer information for duplicate question detection</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fubao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaicheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihuiwen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06387</idno>
		<title level="m">Explainaboard: An explainable leaderboard for nlp</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
				<meeting><address><addrLine>Reykjavik</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1614" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for communitybased question answering</title>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth international joint conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compositional deattention networks</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A compare-propagate architecture with alignment factorization for natural language inference</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00102</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">154</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Costack residual affinity networks with multi-level attention refinement for matching text sequences</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02938</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-level head-wise match and aggregation in transformer for textual sequence matching</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9209" to="9216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using prior knowledge to guide bert&apos;s attention in semantic textual matching tasks</title>
		<author>
			<persName><forename type="first">Tingyu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2466" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantics-aware bert for language understanding</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuailiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9628" to="9635" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
