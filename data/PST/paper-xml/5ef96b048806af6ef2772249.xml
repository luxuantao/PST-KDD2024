<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Multi-Scale Gaussian Transformer for Stock Movement Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qianggang</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sifan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jiadong</forename><surname>Guo</surname></persName>
							<email>guoj@pcl.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><forename type="middle">Cheng</forename><surname>Laboratory</surname></persName>
						</author>
						<title level="a" type="main">Hierarchical Multi-Scale Gaussian Transformer for Stock Movement Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting the price movement of finance securities like stocks is an important but challenging task, due to the uncertainty of financial markets. In this paper, we propose a novel approach based on the Transformer to tackle the stock movement prediction task. Furthermore, we present several enhancements for the proposed basic Transformer. Firstly, we propose a Multi-Scale Gaussian Prior to enhance the locality of Transformer. Secondly, we develop an Orthogonal Regularization to avoid learning redundant heads in the multi-head self-attention mechanism. Thirdly, we design a Trading Gap Splitter for Transformer to learn hierarchical features of high-frequency finance data. Compared with other popular recurrent neural networks such as LSTM, the proposed method has the advantage to mine extremely long-term dependencies from financial time series. Experimental results show our proposed models outperform several competitive methods in stock price prediction tasks for the NASDAQ exchange market and the China A-shares market.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the development of stock markets all around the world, the overall capitalization of stock markets worldwide has exceed 68 trillion U.S. dollar by 2018<ref type="foot" target="#foot_0">1</ref> . Recent years, more and more quantitative researchers get involved in predicting the future trends of stocks, and they help investors make profitable decisions using state-of-the-art trading strategies. However, the uncertainty of stock prices make it an extremely challenging problem in the field of data science.</p><p>Prediction of stock price movement belongs to the area of time series analysis which models rich contextual dependencies using statics or machine learning methods. Traditional approaches for stock price prediction are mainly based on fundamental factors technical indices or statistical time series models, which captures explicit or implicit patterns from historical financial data. However, the performance of those methods are limited by two aspects. Firstly, they usually require expertise in finance. Secondly, these methods only capture simple patterns and simple dependence structures of financial time series. With the rise of artificial intelligence technology, more and more researchers attempt to solve this problem using machine learning algorithms, such as SVM <ref type="bibr" target="#b3">[Cortes and Vapnik, 1995]</ref>, Nearest Neighbors <ref type="bibr" target="#b0">[Altman, 1992]</ref>, Random Forest <ref type="bibr" target="#b1">[Breiman, 2001]</ref>. Recently, since deep neural networks empirically exhibited its powerful capabilities in solving highly uncertain and nonlinear problems, the stock prediction research based on deep learning technique has become more and more popular in recent years and show significant advantages over traditional approaches.</p><p>The stock prediction research based on deep learning technique can roughly be grouped to two categories: (1) Fundamental analysis, and (2) Technical analysis. Fundamental analysis constructs prediction signals using fundamental information such as news text, finance report and analyst report. For example, <ref type="bibr" target="#b11">[Schumaker and Chen, 2009;</ref><ref type="bibr">Xu and Cohen, 2018;</ref><ref type="bibr" target="#b2">Chen et al., 2019]</ref> use natural language processing approaches to predict stock price movement by extracting latent features from market-related texts information,such as news, reports, and even rumors. On the other hand, technical analysis predicts finance market using historical data of stocks. One natural choice is the RNN family, such as <ref type="bibr">RNN [Rumelhart et al., 1986]</ref>, LSTM <ref type="bibr" target="#b6">[Hochreiter and Schmidhuber, 1997]</ref>, Conv-LSTM <ref type="bibr" target="#b14">[Xingjian et al., 2015]</ref>, and ALSTM <ref type="bibr" target="#b11">[Qin et al., 2017]</ref>. However, the primary drawback of these methods is that RNN family struggles to capture extremely long-term dependencies <ref type="bibr" target="#b9">[Li et al., 2019]</ref>, such as the dependencies across several months on financial time series.</p><p>Recently, a well-known sequence-to-sequence model called Transformer <ref type="bibr" target="#b12">[Vaswani et al., 2017]</ref> has achieved great success on natural machine translation tasks. Distinct from RNN-based models, Transformer employs a multi-head selfattention mechanism to learn the relationship among different positions globally, thereby the capacity of learning long-term dependencies is enhanced. Nevertheless, canonical Transformer is designed for natural language tasks, and therefore it has a number of limitations in tackling finance prediction:</p><p>(1) Locality imperception: the global self-attention mecha-nism in canonical Transformer is insensitive to local context, whose dependencies are much important in financial time series. (2) Hierarchy poverty: the point-wise dot-product selfattention mechanism lacks the capability of utilizing hierarchical structure of financial time series (e.g. learning intraday, intra-week and intra-month features in financial time series independently). Intuitively, addressing those drawbacks will improve the robustness of the model and lead to better performance in the task of financial time series prediction.</p><p>In this paper, we propose a new Transformer-based method for stock movement prediction. The primary highlight of the proposed model is the capability of capturing long-term, short-term as well as hierarchical dependencies of financial time series. For these aims, we propose several enhancements for the Transformer-based model: (1) Multi-Scale Gaussian Prior enhances the locality of Transformer. (2) Orthogonal Regularization avoids learning redundant heads in the multimead self-attention mechanism. (3) Trading Gap Splitter enables Transformer to learn intra-day features and intra-week features independently. Numerical results comparing with other competitive methods for time series show the advantages of the proposed method.</p><p>In summary, the main contributions of our paper include: Technical Analysis On the other hand, technical analysis methods extract price-volume information from historical trading data and use machine learning algorithms for prediction. For instances, <ref type="bibr" target="#b10">[Lin et al., 2013]</ref> proposes an SVMbased approach for stock market trend prediction. Meanwhile, LSTM neural network <ref type="bibr" target="#b6">[Hochreiter and Schmidhuber, 1997]</ref> has been employed to model stock price movement.</p><p>[ <ref type="bibr" target="#b10">Nelson et al., 2017]</ref> proposes an LSTM model to predict stock movement based on the technical analysis indicators.</p><p>[ <ref type="bibr" target="#b15">Zhang et al., 2017]</ref> proposes an LSTM model on historical data to discover multi-frequency trading. <ref type="bibr">[Wang et al., 2019]</ref> proposes a ConvLSTM-based Seq2Seq framework for stock movement prediction. <ref type="bibr" target="#b11">[Qin et al., 2017]</ref> proposes an Attentive-LSTM model with an attention mechanism to predict stock price movement and <ref type="bibr" target="#b4">[Feng et al., 2019]</ref> further introduces an data augmentation approach with the idea of adversarial training. However, <ref type="bibr" target="#b9">[Li et al., 2019]</ref> points out that LSTM can only distinguish 50 positions nearby with an effective context size of about 200. That means that LSTM-based models suffer from the difficulty in capturing extremely longterm dependencies in time series. To tackle this issue, we propose a Transformer-based method to better mine the intrinsic long-term and complex structures in financial time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>Since the exact price of a stock is extremely hard to be predicted accurately, we follow the setup of <ref type="bibr" target="#b13">[Walczak, 2001]</ref> and predict the stock price movement instead. Usually the stock movement prediction is treated as a binary classification problem -e.g., discretizing the stock movement into two classes (Rise or Fall). Formally, given the stock features </p><formula xml:id="formula_0">X = [x T âˆ’âˆ†t+1 , x T âˆ’âˆ†t+2 , ..., x T ] âˆˆ R âˆ†tÃ—F (also repre- sented as X = [x 1 , x 2 , ..., x N ] âˆˆ R N Ã—F in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Method</head><p>In this section, we first describe the basic Transformer model we designed. Then we introduce the proposed enhancements of Transformer for financial time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basic Transformer for Stock Movement Prediction</head><p>In our work, we instantiate f Î¸ (â€¢) with Transformer-based model. To adapt the stock movement prediction task which takes time series as inputs, we design a variant of Transformer with encoder-only structure which consists of L blocks of multi-head self-attention layers and position-wise feed forward layers (see Figure <ref type="figure" target="#fig_1">1</ref>). Given the input time series</p><formula xml:id="formula_1">X = [x 1 , x 2 , ..., x N ] âˆˆ R N Ã—F</formula><p>, we first add the position encoding and adopt an linear layer with tanh activation function as follows:</p><formula xml:id="formula_2">X = Ïƒ tanh W (I) h [PositionEncoding(X)].<label>(1)</label></formula><p>Then multi-head self-attention layers take X as input, and are computed by</p><formula xml:id="formula_3">Q h = W (Q) h X, K h = W (K) h X, V h = W (V ) h X,<label>(2)</label></formula><p>where h = 1, ..., H and W</p><formula xml:id="formula_4">(Q) h , W (K) h and W (V ) h</formula><p>are learnable weight matrices for Query, Key and Value, respectively</p><formula xml:id="formula_5">ğ’™ 1 ğ’™ 2 ğ’™ 3 â€¦ ğ’™ ğ‘ à´¥ ğ’™ 1 à´¥ ğ’™ 2 à´¥ ğ’™ 3 â€¦ à´¥ ğ’™ ğ‘ ğ’ 1 ğ’ 2 ğ’ 3 â€¦ ğ’ ğ‘ ğ’› 1 ğ’› 2 ğ’› 3 â€¦ ğ’› ğ‘ Ã—L Head-1 Head-2 Head-3 â€¦ ğ’ â€¦ à·œ ğ‘¦</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Attention Aggregation</head><p>Multi-Head Self-Attention Layers (refers to <ref type="bibr" target="#b12">[Vaswani et al., 2017]</ref> for more details). Then the attention score matrix a h âˆˆ R N Ã—N of the h th head is computed by</p><formula xml:id="formula_6">a h = softmax( Q h K T h âˆš d k â€¢ M),<label>(3)</label></formula><p>where M is a position-wise mask matrix to filter out temporal attention, so as to avoid future information leakage. Afterwards, the output of the h th head is a weighted sum defined as follows:</p><formula xml:id="formula_7">[O h ] i = N j=1 (a h ) i,j â€¢ [V h ] j .<label>(4)</label></formula><p>The final output of multi-head attention layers is the concatenation of all heads by O = [O 1 , O 2 , ..., O H ]. Afterward, the position-wise feed forward layer takes O as input and transforms it to Z by two fully-connected layers and a ReLU activation layer. Upon the output z i of the last selfattention layer, a temporal attention layer <ref type="bibr" target="#b11">[Qin et al., 2017]</ref> is deployed to aggregate the latent features from each position as m = N i=1 Î± i z i . Then the scalar prediction score Å· is computed by a fully-connected layer and a sigmoid transformation: Å· = sigmoid(W f c m).</p><p>(5) Our ultimate goal is to maximize the log-likelihood between Å· and y via the following loss function:</p><formula xml:id="formula_8">L CE = (1 âˆ’ y)log(1 âˆ’ Å·) + ylog(Å·) (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Enhancing Locality with Multi-Scale Gaussian Prior</head><p>Recently, Transformer exhibits its powerful capability of extracting global patterns in natural language processing fields. However, the self-attention mechanism in Transformer considers the global dependencies with very weak position information. Note that, the position information serves as the temporal variant patterns in time series, which is much important. To address it, we incorporate Multi-Scale Gaussian Prior into the canonical multi-head self-attention mechanism with the intuition that the relevance of data in two positions is directly proportional to the temporal distance between them.</p><p>To pay more attention to the closer time-steps, we add biases of Gaussian prior to the attention score matrices based on the assumption that such scores would obey Gaussian distributions. Note that this operation is equivalent to multiplying the original attention distribution with a Gaussian distribution mask (See <ref type="bibr" target="#b5">[Guo et al., 2019]</ref> for the proof). In details, we transform Eq.3 to the following form by adding Gaussian biases:</p><formula xml:id="formula_9">a h = softmax[( Q h K T h âˆš d k + B (G) h ) â€¢ M],<label>(7)</label></formula><p>where</p><formula xml:id="formula_10">B (G) h âˆˆ R N Ã—N is a matrix computed by [B (G) h ] i,j = exp(âˆ’ (jâˆ’i) 2 2Ïƒ 2 h ) j â‰¤ i; 0 j &gt; i.<label>(8)</label></formula><p>Note that we allow Ïƒ h in B</p><p>(G)</p><p>h are different for different heads in the multi-head self-attention layer.</p><p>Besides, we also give an empirical approximation for Ïƒ h . Suppose we want to pay more attention to the D h closest time-steps, the variance can be empirically set as Ïƒ h = D h . By this way, we allow different D h in different attention heads in order to provide Multi-Scale Gaussian Prior.</p><p>In finance, the temporal features from last 5-day, 10-day, 20-day or 40-day are usually considered in trading strategies. That means, for a 4-head self-attention layer, we can empirically assign the window-size set D = {5, 10, 20, 40} to Ïƒ h with h = 1, ..., 4, respectively as is shown in Figure <ref type="figure" target="#fig_2">2</ref>. In conclusion, the proposed Multi-Scale Gaussian Prior enables Transformer to learn multi-scale localities from financial time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Orthogonal Regularization for Multi-Head Self-Attention Mechanism</head><p>With the proposed Multi-Scale Gaussian Prior, we let different heads learn different temporal patterns in the multi-head attention layer. However, some previous research <ref type="bibr" target="#b12">[Tao et al., 2018</ref><ref type="bibr" target="#b8">][Li et al., 2018</ref><ref type="bibr" target="#b7">][Lee et al., 2019]</ref> claims that canonical multi-head self-attention mechanism tend to learn redundant heads. To enhance the diversity between each head, we induce an orthogonal regularization with regard to the weight tensor</p><formula xml:id="formula_11">W (V ) h</formula><p>in Eq.2. Specifically, we first calculate the </p><formula xml:id="formula_12">â€¦ ğ’ # ğ’ $ â€¦ ğ’ $% ! ğ’™ # ! ğ’™ $ â€¦ ! ğ’™ $% ğ’ $&amp; â€¦ ğ’ '$ ! ğ’™ $&amp; â€¦ ! ğ’™ '$ ğ’ #(' â€¦ ğ’ #)( ! ğ’™ #(' â€¦ ! ğ’™ #)( â€¦ ğ’ #)# ğ’ #)$ â€¦ ğ’ #'% ! ğ’™ #)# ! ğ’™ #)$ â€¦ ! ğ’™ #'% ğ’ $)' â€¦ ğ’ $%( ! ğ’™ $)' â€¦ ! ğ’™ $%( â€¦ â€¦ Block-1 ğ’ # ğ’ $ ğ’ #)( ! ğ’™ # ! ğ’™ $ ! ğ’™ #)( ğ’ #)# ğ’ #)$ ğ’ $%( ! ğ’™ #)# ! ğ’™ #)$ ! ğ’™ $%( â€¦ Block-2 ğ’ # ğ’ $ â€¦ ğ’ #)( ! ğ’™ # ! ğ’™ $ â€¦ ! ğ’™ #)( ğ’ #)# ğ’ #)$ â€¦ ğ’ ,-# ! ğ’™ #)# ! ğ’™ #)$ â€¦ ! ğ’™ ,-# ğ’ , ! ğ’™ , Block-3 â€¦ â€¦ â€¦ â€¦ â€¦ 130Ã—130 130Ã—130 130Ã—130 â€¦ â€¦ â€¦ â€¦ â€¦ 26Ã—26 26Ã—26 26Ã—26 26Ã—26 26Ã—26</formula><p>ğŒ (0) for Block-2 ğŒ (0) for Block-1 (a) Hierarchical features of NASDAQ 15-minute data (b) Hierarchical self-attention mechanism (c) Hierarchical masks tensor</p><formula xml:id="formula_13">ğ’ $% ! ğ’™ $% ğ’ $&amp; â€¦ ğ’ '$ ! ğ’™ $&amp; â€¦ ! ğ’™ '$ â€¦ â€¦ ğ’ #'% ! ğ’™ #'% ğ’ $)' ! ğ’™ $)' â€¦ â€¦ â€¦ â€¦ ğ’ #(' ! ğ’™ #(' â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦</formula><formula xml:id="formula_14">W (V ) = [W (V ) 1 , W (V ) 2 , ..., W (V ) H ] by concatenating W (V ) h of all heads. Note that the size of W (V ) is H Ã— F Ã— d v</formula><p>where d v denotes the last dimension of V h . Then we flatten the tensor W (V ) to a matrix A with size of H Ã— (F * d v ), and further normalize it as A = A/||A|| 2 . Finally, the penalty loss is computed by</p><formula xml:id="formula_15">L p = || A A T âˆ’ I|| F ,<label>(9)</label></formula><p>where || â€¢ || F denotes the Frobenius norm of a matrix and I stands for an identity matrix. We briefly add the penalty loss to the original loss with a trade-off hyper-parameter Î³ as follows:</p><formula xml:id="formula_16">L = L CE + Î³L p .<label>(10)</label></formula><p>For simplicity in expression, here we omit the number of multi-head self-attention layers in the model. In our experiment, we sum up the penalty losses from each multi-head self-attention layer as the final penalty loss as follows:</p><formula xml:id="formula_17">L p = L (1) p + L (2) p + ... + L (L) p .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Trading Gap Splitter</head><p>As is known that the input of the model is a continuous time series. However, due to the trading gaps, the input time series is essentially NOT continuous. Takes the 15-minute data from NASDAQ stock market 2 as an example, of which one trading day contains 26 15-minute time-steps and one trading week contains 5 trading days. This means there are inter-day and inter-week trading gaps. However, when the basic Transformer model is applied to this data, the self-attention layer 2 the NASDAQ Stock Exchange Market is open 5 days per week for 6.5 hours per day inside treats all time-steps equally and omit the implicit interday and inter-week trading gaps. To solve this problem, we design a new hierarchical self-attention mechanism for the Transformer model to learn the hierarchical features of stock data (see Figure <ref type="figure" target="#fig_3">3 (a)</ref>).</p><p>Takes a 3-block Transformer model as an example, we aim to learn the hierarchical features of stock data by the order "intra-dayâ†’intra-weekâ†’global". In order to do so, we set two extra position-wise masks to the first and second selfattention blocks respectively in order to limit the attention scopes. Formally, we modify Eq.7 to the following form:</p><formula xml:id="formula_18">a h = softmax[( Q h K T h âˆš d k + B (G) h ) â€¢ M (H) â€¢ M],<label>(12)</label></formula><p>where M (H) is an N Ã— N matrix filled with âˆ’inf whose diagonal is composed of continuous sub-matrices filled with 0. The M (H) for the first and second self-attention blocks are shown in Figure <ref type="figure" target="#fig_3">3 (c)</ref>. Specifically, the size of sub-matrices in M (H) for the first block is 26 Ã— 26 since one trading day contains 26 15-minute time-steps, and the size of sub-matrices for the second block changes to 130 Ã— 130 (26 * 5) since one trading week contains 5 trading days. By this way, the first and second self-attention blocks will learn the intra-day and intra-week features of stock data, respectively. Moreover, for the last self-attention block, we keep the original attention mechanism without M (H) to learn global features of stock data. As a result, the Transformer model with the proposed hierarchical attention mechanism avoids suffering from the trading gaps. Note that, all attention heads in the same multihead self-attention layer share the same M (H) . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To evaluate the proposed methods, we use two stock data: one from NASDAQ market and the other from China A-shares market. The details of the two data are listed in Table <ref type="table" target="#tab_2">1</ref>. In the following subsections, we will introduce the data collection process and show our empirical results from numerical experiments. We also conduct an incremental analysis to explore the effectiveness of each proposed enhancements for Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Collection</head><p>We collect the daily quote data of all 3243 stocks from NAS-DAQ stock market from July 1 st , 2010 to July 1 st , 2019 and the 15-min quote data of 500 CSI-500 component stocks from China A-shares market from December 1 st , 2015 to December 1 st , 2019. We move a lag window with size of N timesteps along these time series to construct candidate examples. For the NASDAQ data, we construct 5 datasets with window sizes N = 5, 10, 20, 40 (denoting 5-, 10-, 20-, 40day, respectively), and the lag strides are all fixed to 1. For the China A-shares data, we also construct 5 datasets with window sizes N = 5 * 16, 10 * 16, 20 * 16, 40 * 16 (denoting 5-, 10-, 20-, 40-day since one trading day contains 16 15-minute in China A-shares market), and the lag strides are all fixed to 16 (i.e. 1 day). The features used in all datasets consist of open, high, low, close and volume, which are all adjusted and normalized following <ref type="bibr" target="#b4">[Feng et al., 2019;</ref><ref type="bibr">Xu and Cohen, 2018]</ref>. We initially label both datasets by the strategy mentioned in Section 3.1 (i.e y = I(p</p><formula xml:id="formula_19">T +k &gt; p T )),</formula><p>where k is set to 1 and 16 (both denote 1 day) for the NAS-DAQ data and the China A-shares data, respectively. Furthermore, we set two threshold parameter Î² rise , Î² f all to the labels in each dataset in order to balance the number of positive and negative samples to roughly 1 : 1 as follows:</p><formula xml:id="formula_20">y = ï£± ï£´ ï£² ï£´ ï£³ 1 p T +k âˆ’p T p T &gt; Î² rise ; âˆ’1 p T +k âˆ’p T p T &lt; Î² f all ; abandon otherwise. (13)</formula><p>To avoid the data leakage problem, we strictly follow the sequential order to split training/validation/test sets. For instances, we split the NASDAQ data and the China A-shares data into training/validation/test sets by 8-year/1-year/1-year and 3-year/6-month/6-month, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>Following previous research <ref type="bibr" target="#b4">[Feng et al., 2019;</ref><ref type="bibr">Xu and Cohen, 2018]</ref>, we evaluate the prediction performance with two metrics: Accuracy (Acc) and Matthews Correlation Coefficient (MCC), which is defined as below:</p><formula xml:id="formula_21">MCC = tp Ã— tn âˆ’ fp Ã— fn (tp + fp)(tp + fn)(tn + fp)(tn + fn)<label>(14)</label></formula><p>where tp, tn, fp, fn denote the number of samples classified as true positive, true negative, false positive and false negative, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Numerical Experiments</head><p>Approaches in comparison. We compare our approaches B-TF, MG-TF and HMG-TF with the baselines CNN, LSTM and ALSTM:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Incremental Analysis</head><p>To explore the effectiveness of the proposed components Multi-Scale Guassian Prior, Orthogonal Regularization and Trading Gap Splitter, we further conduct an incremental analysis on different settings of the Transformer-based models.  As is shown in Table <ref type="table" target="#tab_4">3</ref>, these components all contribute to the performance of Transformer-based method. Moreover, we can further observe that the performance improvement mainly benefits from Multi-Scale Guassian Prior and Trading Gap Splitter, while the gain from Orthogonal Regularization is relatively insignificant. Besides, we illustrate the effectiveness of Trading Gap Splitter by visualizing the attention score matrix learned by HMG-TF model with N = 160 (is equivalent to K = 10-day or 2-week). As is shown in Figure <ref type="figure" target="#fig_7">5</ref>, the attention scope becomes larger gradually (16â†’80â†’160) from (a) to (c). That means, with the proposed hierarchical self-attentiom mechanism, the time-steps only attend to those belonging to the same day in the first self-attention block, the time-steps only attend to those belonging to the same week in the second block, and the attention scopes of time-steps are no limit in the last block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion &amp; Future Work</head><p>In this paper, we propose to apply Transformer model for stock movement prediction in which the attention mechanism can help to capture extremely long-term dependencies of finance time series. Furthermore, equipped with the proposed enhancements Multi-Scale Gaussian Prior, Orthogonal Regularization and Trading Gap Splitter, our Transformer-based model achieves significant gains over several state-of-the-art baselines on two real-world market dataset. In the future, except for the model itself, the following aspects can be investigated for further improvements: (1) cross-sectional features of financial data can be engaged to improve the model, (2) regularization methods can be explored to avoid suffering from overfitting on financial data, (3) data augmentation such as adversarial and stochastic perturbations can be adopted to improve the robustness of the model. Also, it is significant to investigate the theoretical guarantee for our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the rest of the paper for simplicity) in the latest âˆ†t time-steps, the prediction model f Î¸ (X) with parameters Î¸ can output the predicted movement label y = I(p T +k &gt; p T ), where T denotes the target trading time, F denotes the dimension of stock features and p t denotes the close price at time-step t. Briefly, the proposed model utilizes the historical data of a stock s in the lag [T âˆ’ âˆ†t + 1, T ] (where âˆ†t is a fixed lag size) to predict the movement class y (0 for Fall, 1 for Rise) of the future k time-steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed Basic Transformer overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of [B (G) h ]i,j in Eq. 8 with window-size set D = {5, 10, 20, 40}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Trading Gap Splitter overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The accuracy and MCC trends along with the window size K-day on the China A-shares data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>â€¢</head><label></label><figDesc>Multi-Scale Gaussian Transformer (MG-TF): The B-TF model with enhancements of Multi-Gaussian Prior and Orthogonal Regularization introduced in Section 4.2 and Section 4.3, respectively. â€¢ Hierarchical Multi-Scale Gaussian Transformer (HMG-TF): The MG-TF model with the enhancement of Trading Gap Splitter introduced in Section 4.4. Settings. We implement B-TF, MG-TF and HMG-TF with PyTorch framework on Nvidia Tesla V100 GPU. We use Adam optimizer with an initial learning-rate of 1e âˆ’ 4. The size of mini-batch is set to 256. The trade-off hyperparameter Î³ is set to 0.05. All TF-based models have 3 multihead self-attention blocks each with 4 heads. We train the model in an end-to-end manner from raw quote data without any data augmentation. Since Trading Gap Splitter is less appropriate for low-frequency data like daily quote data, we only perform MG-TF on our NASDAQ dataset. Results. The performance of comparison experiments on both data are shown in Table 2, from which we have the following observations: â€¢ The proposed approaches B-TF, MG-TF and HMG-TF show significant gains on both metrics Acc and MCC compared with baselines in all cases. It exhibits that Transformer-based approaches have significant performance advantages over RNN-and CNN-based approaches. â€¢ Transformer-based approaches have better capabilities of learning long-term dependencies. Especially on China A-shares data, in which the window size of 40day comtains 640 (40*16) time-steps, and it is hard for RNN-based approaches to learn the dependencies across so many time-steps (see Figure 4). While the advantages of the self-attention mechanism enables Transformerbased approaches achieve consistently better performance as the window size becomes larger. â€¢ The modified Transformer model MG-TF and HMG-TF have better performance than the basic Transformer model. More detailed analysis will be shown in the next section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Hierarchical attention scores. (a)(b)(c) represents the attention score matrices learned by Block-1, Block-2 and Block-3, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Data description.</figDesc><table><row><cell>Property</cell><cell></cell><cell>Data</cell></row><row><cell></cell><cell>Daily</cell><cell>15-min</cell></row><row><cell>Market</cell><cell>NASDAQ</cell><cell>China A-shares</cell></row><row><cell>Start Date</cell><cell>2010/07/01</cell><cell>2015/12/01</cell></row><row><cell>End Date</cell><cell>2019/07/01</cell><cell>2019/12/01</cell></row><row><cell>Time Frequency</cell><cell>1 day</cell><cell>15 minutes</cell></row><row><cell>Total Stocks</cell><cell>3243</cell><cell>500</cell></row><row><cell>Total Records</cell><cell>9749098</cell><cell>7928000</cell></row><row><cell>Rising Threshold Î²rise</cell><cell>0.55%</cell><cell>-0.5%</cell></row><row><cell>Falling Threshold Î² f all</cell><cell>-0.1%</cell><cell>0.105%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The results of comparison experiments. All values are average results from 5 repeated experiments. The best results are in bold and â€  denotes our methods.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Accuracy (%)/MCC (Ã—10 âˆ’2 )</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">with window size of K-day</cell></row><row><cell></cell><cell>K = 5</cell><cell>K = 10</cell><cell>K = 20</cell><cell>K = 40</cell></row><row><cell></cell><cell cols="3">NASDAQ Daily Data</cell></row><row><cell>CNN</cell><cell cols="4">52.33/3.16 52.02/2.68 51.84/2.28 52.60/2.52</cell></row><row><cell>LSTM</cell><cell cols="4">53.86/7.73 53.89/7.72 53.59/7.15 53.81/7.48</cell></row><row><cell>ALSTM</cell><cell cols="4">54.06/8.35 53.94/7.92 54.05/8.11 54.19/8.56</cell></row><row><cell>B-TF  â€ </cell><cell cols="4">54.78/8.48 54.84/8.89 54.90/9.13 56.01/9.45</cell></row><row><cell>MG-TF  â€ </cell><cell cols="4">55.10/8.98 56.18/9.74 56.77/10.39 57.30/11.46</cell></row><row><cell></cell><cell cols="3">China A-shares 15-min Data</cell></row><row><cell>CNN</cell><cell cols="4">53.53/2.62 52.25/1.80 52.03/1.81 51.61/1.77</cell></row><row><cell>LSTM</cell><cell cols="4">56.59/6.42 56.70/6.19 56.18/3.74 54.93/2.98</cell></row><row><cell>ALSTM</cell><cell cols="4">57.03/8.23 57.42/9.16 55.69/6.65 55.68/6.65</cell></row><row><cell>B-TF  â€ </cell><cell cols="4">57.14/9.68 57.42/9.52 57.32/9.14 57.55/10.41</cell></row><row><cell cols="5">HMG-TF  â€  57.36/10.52 57.79/9.98 57.90/10.33 58.70/14.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experimental results of incremental analysis. MG, OR and TS denote Multi-Scale Gaussian Prior, Orthogonal Regularization and Trading Gap Splitter, respectively. * denotes wo/ OR.</figDesc><table><row><cell>â€¢ CNN [Selvin et al., 2017]: Here we use 1D-CNN with</cell></row><row><cell>the kernel size of 1 Ã— 3.</cell></row><row><cell>â€¢ LSTM [Nelson et al., 2017]: A variant of recurrent neu-</cell></row><row><cell>ral network with feedback connections.</cell></row><row><cell>â€¢ Attentive LSTM (ALSTM) [Feng et al., 2019]: A vari-</cell></row><row><cell>ant of LSTM model with a temporal attentive aggrega-</cell></row><row><cell>tion layer.</cell></row><row><cell>â€¢ Basic Transformer (B-TF): The proposed Basic Trans-</cell></row><row><cell>former model introduced in Section 4.1.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://data.worldbank.org/indicator/CM.MKT.TRAD.CD?end=</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2018" xml:id="foot_1">2018&amp;start=2018&amp;view=bar</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20) Special Track on AI in FinTech</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3">Proceedings of Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20) Special Track on AI in FinTech</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An introduction to kernel and nearest-neighbor nonparametric regression</title>
		<author>
			<persName><forename type="first">Naomi</forename><forename type="middle">S</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="185" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Leo Breiman. Random forests</title>
				<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Incorporating fine-grained events in stock movement prediction</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05078</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">Vapnik</forename><surname>Cortes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995. 1995</date>
			<publisher>Corinna Cortes and Vladimir Vapnik</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhancing stock movement prediction with adversarial training</title>
		<author>
			<persName><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5843" to="5849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaussian transformer: a lightweight approach for natural language inference</title>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Schmidhuber</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and JÃ¼rgen Schmidhuber</title>
				<imprint>
			<date type="published" when="1997">1997. 1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Orthogonality constrained multi-head attention for keyword spotting</title>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04500</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-head attention with disagreement regularization</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10183</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5244" to="5254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stock market&apos;s price movement prediction with lstm neural networks</title>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2013 international joint conference on neural networks (IJCNN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013. 2017. 2017</date>
			<biblScope unit="page" from="1419" to="1426" />
		</imprint>
	</monogr>
	<note>2017 International Joint Conference on Neural Networks (IJCNN)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vijay Krishna Menon, and KP Soman. Stock price prediction using lstm, rnn and cnnsliding window model</title>
		<author>
			<persName><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02971</idno>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1986">2017. 2017. 1986. 1986. 2009. 2017</date>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="1643" to="1647" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Textual analysis of stock market prediction using breaking financial news: The azfin text system</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Get the point of my utterance! learning towards effective responses with multi-head attention mechanism</title>
		<author>
			<persName><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<editor>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2018. 2018. 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clvsa: A convolutional lstm based variational sequence-to-sequence model with attention for predicting trends of financial markets</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Walczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Walczak</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2001">2001. 2001. 2019. 2019. 2017. 2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
	<note>Stock market one-day ahead movement prediction using disparate data sources. Expert Systems with Applications</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Yumo Xu and Shay B Cohen. Stock movement prediction from tweets and historical prices</title>
		<author>
			<persName><surname>Xingjian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2018. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1970" to="1979" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stock price prediction via discovering multi-frequency trading patterns</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2141" to="2149" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
