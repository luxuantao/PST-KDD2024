<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Particle Swarm Optimization in Dynamic Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tim</forename><surname>Blackwell</surname></persName>
							<email>t.blackwell@gold.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Goldsmiths College London</orgName>
								<address>
									<postCode>SE14 6NW</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Particle Swarm Optimization in Dynamic Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C298455E88A8FA5AB586E4FBAAFE386A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Particle Swarm Optimization (PSO) is a versatile population-based optimization technique, in many respects similar to evolutionary algorithms (EAs). PSO has been shown to perform well for many static problems <ref type="bibr" target="#b30">[30]</ref>. However, many real-world problems are dynamic in the sense that the global optimum location and value may change with time. The task for the optimization algorithm is to track this shifting optimum. It has been argued <ref type="bibr" target="#b14">[14]</ref> that EAs are potentially well-suited to such tasks, and a review of EA variants tested in the dynamic problem is given in <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15]</ref>. It might be wondered, therefore, what promise PSO holds for dynamic problems.</p><p>Optimization with particle swarms has two major ingredients, the particle dynamics and the particle information network. The particle dynamics are derived from swarm simulations in computer graphics <ref type="bibr" target="#b21">[21]</ref>, and the information sharing component is inspired by social networks <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b25">25]</ref>. These ingredients combine to make PSO a robust and efficient optimizer of real-valued objective functions (although PSO has also been successfully applied to combinatorial and discrete problems too). PSO is an accepted computational intelligence technique, sharing some qualities with Evolutionary Computation <ref type="bibr" target="#b0">[1]</ref>.</p><p>The application of PSO to dynamic problems has been explored by various authors <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">24]</ref>. The overall consequence of this work is that PSO, just like EAs, must be modified for optimal results on dynamic environments typified by the moving peaks benchmark (MPB). (Moving peaks, arguably representative of real world problems, consist of a number of peaks of changing with and height and in lateral motion <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b7">7]</ref>.) The origin of the difficulty lies in the dual problems of outdated memory due to environment dynamism, and diversity loss, due to convergence.</p><p>Of these two problems, diversity loss is by far the more serious; it has been demonstrated that the time taken for a partially converged swarm to re-diversify, find the shifted peak, and then re-converge is quite deleterious to performance <ref type="bibr" target="#b2">[3]</ref>. Clearly, either a re-diversification mechanism must be employed at (or before) function change, and/or a measure of diversity can be maintained throughout the run. There are four principle mechanisms for either re-diversification or diversity maintenance: randomization <ref type="bibr" target="#b23">[23]</ref>, repulsion <ref type="bibr" target="#b4">[5]</ref>, dynamic networks <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b36">36]</ref> and multi-populations <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Multi-swarms combine repulsion with multi-populations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">7]</ref>. Interestingly, the repulsion occurs between particles, and between swarms. The multipopulation in this case is an interacting super-swarm of charged swarms . A charged swarm is inspired by models of the atom: a conventional PSO nucleus is surrounded by a cloud of 'charged' particles. The charged particles are responsible for maintaining the diversity of the swarm. Furthermore, and in analogy to the exclusion principle in atomic physics, each swarm is subject to an exclusion pressure that operates when the swarms collide. This prohibits two or more swarms from surrounding a single peak, thereby enabling swarms to watch secondary peaks in the eventuality that these peaks might become optimal. This strategy has proven to be very effective for MPB environments.</p><p>This chapter starts with a description of the canonical PSO algorithm and then, in Section 3, explains why dynamic environments pose particular problems for unmodified PSO. The MPB framework is also introduced in this section. The following section describes some PSO variants that have been proposed to deal with diversity loss. Section 5 outlines the multi-swarm approach and the subsequent section presents new results for a self-adapting multi-swarm, a multi-population with swarm birth and death.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Canonical PSO</head><p>In PSO, population members (particles) possess a memory of the best (with respect to an objective function) location that they have visited in the past, pbest, and of its fitness. In addition, particles have access to the best location of any other particle in their own network. These two locations (which will coincide for the best particle in any network) become attractors in the search space of the swarm. Each particle will be repeatedly drawn back to spatial neighborhoods close to these two attractors, which themselves will be updated if the global best and/or particle best is bettered at each particle update. Several network topologies have been tried, with the star or fully connected network remaining a popular choice for unimodal functions. In this network, every particle will share information with every other particle in the swarm so that there is a single gbest global best attractor representing the best location found by the entire swarm.</p><p>Particles possess a velocity which influences position updates according to a simple discretization of particle motion</p><formula xml:id="formula_0">v(t + 1) = v(t) + a(t + 1)<label>(1)</label></formula><formula xml:id="formula_1">x(t + 1) = x(t) + v(t + 1)<label>(2)</label></formula><p>where a, v, x and t are acceleration, velocity, position and time (iteration counter) respectively. Eqs. 1, 2 are similar to particle dynamics in swarm simulations, but PSO particles do not follow a smooth trajectory, instead moving in jumps, in a motion known as a flight <ref type="bibr" target="#b28">[28]</ref> (notice that the time increment dt is missing from these rules). The particles experience a linear or springlike attraction, weighted by a random number, (particle mass is set to unity) towards each attractor. Convergence towards a good solution will not follow from these dynamics alone; the particle flight must progressively contract. This contraction is implemented by Clerc and Kennedy with a constriction factor χ, χ &lt; 1, <ref type="bibr" target="#b20">[20]</ref>. For our purposes here, the Clerc-Kennedy PSO will be taken as the canonical swarm; χ replaces other energy draining factors extant in the literature such as a decreasing 'inertial weight' and velocity clamping. Moreover the constricted swarm is replete with a convergence proof, albeit about a static attractor (although there is some experimental and theoretical support for convergence in the fully interacting swarm where particles can move attractors <ref type="bibr" target="#b10">[10]</ref>). Explicitly, the acceleration of particle i in Eq.1 is given by</p><formula xml:id="formula_2">a i = χ[cǫ • (p g -x i ) + cǫ • (p i -x i )] -(1 -χ)v i<label>(3)</label></formula><p>where ǫ are vectors of random numbers drawn from the uniform distribution U [0, 1], c &gt; 2 is the spring constant and p i , p g are particle and global attractors. This formulation of the particle dynamics has been chosen to demonstrate explicitly constriction as a frictional force, opposite in direction, and proportional to, velocity. Clerc and Kennedy derive a relation for χ(c): standard values are c = 2.05 and χ = 0.729843788. The complete PSO algorithm for maximizing an objective function f is summarized as Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PSO problems with moving peaks</head><p>As has been mentioned in Sect 1, PSO must be modified for optimal results on dynamic environments typified by the moving peaks benchmark (MPB). These modifications must solve the problems of outdated memory, and of lost diversity. This explains the origins of these problems in the context of MPB, and shows how memory loss is easily addressed. The following section then considers the second, more severe, problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Moving Peaks</head><p>The dynamic objective function of MPB, f (x, t), is optimized at 'peak' locations x * and has a global optimum at x * * = arg max{f (x * )} (once more, assuming optimization means maximizing). Dynamism entails a small movement of magnitude s, and in a random direction, of each x * . This happens every K evaluations and is accompanied by small changes of peak height and width. There are p peaks in total, although some peaks may become obscured. The peaks are constrained to move in a search space of extent X in each of the d dimensions, [0, X] d . This scenario, which is not the most general, nevertheless has been put forward as representative of real world dynamic problems <ref type="bibr" target="#b12">[12]</ref> and a benchmark function is publicly available for download from <ref type="bibr" target="#b11">[11]</ref>. Note that small changes in f (x * ) can still invoke large changes in x * * due to peak promotion, so the many peaks model is far from trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The problem of outdated memory</head><p>Outdated memory happens at environment change when the optima may shift in location and/or value. Particle memory (namely the best location visited in the past, and its corresponding fitness) may no longer be true at change, with potentially disastrous effects on the search.</p><p>The problem of outdated memory is typically solved by either assuming that the algorithm knows just when the environment change occurs, or that it can detect change. In either case, the algorithm must invoke an appropriate response. One method of detecting change is a re-evaluation of f at one or more of the personal bests p i <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b23">23]</ref>. A simple and effective response is to re-set all particle memories to the current particle position and f value at this position, and ensuring that p g = arg max f (p i ). One possible drawback is that the function has not changed at the chosen p i , but has changed elsewhere. This can be remedied by re-evaluating f at all personal bests, at the expense of doubling the total number of function evaluations per iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The problem of lost diversity</head><p>Equally troubling as outdated memory is insufficient diversity at change. The population takes time to re-diversify and re-converge, effectively unable to track a moving optimum.</p><p>It is helpful at this stage to introduce the swarm diameter |S|, defined as the largest distance, along any axis, between any two particles <ref type="bibr" target="#b1">[2]</ref>, as a measure of swarm diversity (Fig. <ref type="figure" target="#fig_0">1</ref>). Loss of diversity arises when a swarm is converging on a peak. There are two possibilities: when change occurs, the new optimum location may either be within or outside the collapsing swarm. In the former case, there is a good chance that a particle will find itself close to the new optimum within a few iterations and the swarm will successfully track the moving target. The swarm as a whole has sufficient diversity. However, if the optimum shift is significantly far from the swarm, the low velocities of the particles (which are of order |S|) will inhibit re-diversification and tracking, and the swarm can even oscillate about a false attractor and along a line perpendicular to the true optimum, in a phenomenon known as linear collapse <ref type="bibr" target="#b4">[5]</ref>. This effect is illustrated in Fig. <ref type="figure">2</ref>. These considerations can be quantified with the help of a prediction for the rate of diversity loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">10]</ref>. In general, the swarm shrinks at a rate determined by the constriction factor and by the local environment at the optimum. For static functions with spherical symmetric basins of attraction, the theoretical and empirical analysis of the above references suggest that the rate of shrinkage (and hence diversity loss) is scale invariant and is given by a scaling law</p><formula xml:id="formula_3">|S(t)| = Cα t<label>(4)</label></formula><p>for constants C and α &lt; 1, where α ≈ 0.92 and C is the swarm diameter at iteration t = 0. The number of function evaluations between change, K, can be converted into a period measured in iterations, L by considering the total number of function evaluations per iteration including, where necessary, the extra test-for-change evaluations. We might expect that, for peak shift distance s and box size X = |S(0)|, if s &gt;&gt; S L = Xα L , tracking will be very hard since the swarm has already converged to a very small ball at the first change. The experiments reported in <ref type="bibr" target="#b2">[3]</ref> show that canonical PSO fails to track a single peaked dynamic environment defined by s = 8.7, L = 100, X = 10. Since S L computes to 0.0024, this is hardly surprising.</p><p>Fig. <ref type="figure">2</ref>. Sequence of frames showing possible behavior when optimum shift is greater that swarm diversity. When the attractor T (square box, frame 1) shifts, particle a is at the global best, pg. a continues along trajectory v since it is not accelerated in this update (frame 2). Particle a continues to move along v, repositioning pg at each update, becoming in effect the swarm leader (frame 3). After a while, the swarm oscillates along v, about a point perpendicular to T (frame 4). Eventually random fluctuations will cause another particle to deviate from v and move closer towards the attractor. The swarm soon follows and converges on T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Diversity lost and diversity regained</head><p>There are two solutions in the literature to the problem of insufficient diversity. Either a diversity increasing mechanism can be invoked at change (or at predetermined intervals), or some permanent means can be put in place to ensure there is sufficient diversity at all times <ref type="bibr" target="#b7">[7]</ref>. These modifications are the subject of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Re-diversification</head><p>Hu and Eberhart <ref type="bibr" target="#b23">[23]</ref> study a number of re-diversification mechanisms. These all involve randomization of the entire, or part of, the swarm. This happens when re-evaluation of the objective function at one or several of the attractors detects change, or at a pre-set interval. Clearly the problem with this approach is the arbitrariness of the extra parameters. Since randomization implies information loss, there is a danger of erasing too much information and effectively re-starting the swarm. On the other hand, too little randomization might not introduce enough diversity to cope with the change. And, of course, if tests for change happen at pre-determined intervals, there is a danger of missing a shift. The arbitrariness of the extra parameters can only be solved if much prior knowledge about f 's dynamism is available, or some other higher-level modification mechanism scheme is implemented. Such a scheme could infer details about f 's dynamism during the run, making appropriate adjustments to the re-diversification parameters. So far, though, higher level modifications such as these have not been studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Maintaining diversity by repulsion</head><p>A constant, and hopefully good enough, degree of swarm diversity can be maintained at all times either through some type of repulsive mechanism, or by adjustments to the information sharing neighborhood. Repulsion can either be between particles, or from an already detected optimum. For example, Krink et al <ref type="bibr" target="#b34">[34]</ref> study finite-size particles as a means of preventing premature convergence. The hard sphere collisions produce a constant diversification pressure. Alternatively, Parsopoulos and Vrahatis <ref type="bibr" target="#b30">[30]</ref> place a repeller at an already detected optima, in an attempt to divert the swarm and find new optima. Neither technique, however, has been applied to the dynamic scenario. An example of repulsion that has been tested in a dynamic context is the atom analogy <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b5">6]</ref>. In this model, a swarm is comprised of a 'charged' and a 'neutral' sub-swarm. The model can be depicted as a cloud of charged particles orbiting a contracting, neutral, PSO nucleus, Fig. <ref type="figure">3</ref>. The charged particles can be either classical or quantum particles; either type are discussed in some depth in references <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">7]</ref> and in the following section. Charge enhances diversity in the vicinity of the converging PSO sub-swarm, so that optimum shifts within this cloud should be trackable. Good tracking (outperforming canonical PSO) has been demonstrated for unimodal dynamic environments of varying severities <ref type="bibr" target="#b2">[3]</ref>. Fig. <ref type="figure">3</ref>. The Atom Analogy. The situation depicted here shows a PSO sub-swarm of neutral particles (filled circles), converging at an optimum. The neutral swarm diameter, |S 0 |, is shrinking by a factor of 0.92 at each iteration. This sub-swarm is surrounded by a number of charged particles with constant diversity |S -|. Both sub-swarms share the same global attractor pg. Optimum moves to locations within the charged sub-swarm will be rapidly re-optimized by the swarm as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Maintaining diversity with dynamic network topology</head><p>Adjustments to the information sharing topology can be made with the intention of reducing, maybe temporarily, the desire to move towards the global best position, thereby enhancing population diversity. Li and Dam use a gridlike neighborhood structure, and Jansen and Middendorf test a hierarchical structure, reporting improvements over unmodified PSO for unimodal dynamic environments <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b36">36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Maintaining diversity with multi-populations</head><p>A number of research groups have considered multi-populations as a means of enhancing diversity. The multi-population idea is particularly helpful in multi-modal environments such as many peaks. The aim here is to allow each population to converge on a promising peak. Then, if any secondary peak becomes the global optimum as a result of change, a population is close at hand. Multi-population techniques include niching, speciation and multi-swarms.</p><p>In the static context, the niching PSO of Brits et al <ref type="bibr" target="#b16">[16]</ref> can successfully optimize some static benchmark problems. In nichePSO, if a particle's fitness changes very little (the variance in fitness is less than a threshold) over a small number of iterations, a two particle sub-swarm is created from this particle and its nearest spatial neighbor. This technique, as the authors point out, would fail in a dynamic environment because niching depends on a homogeneous distribution of particles in the search space, and on a training phase.</p><p>A speciation PSO variation, known as clearing <ref type="bibr" target="#b31">[31]</ref> has been adopted by Li in the static context and generalized by Parrot and Li to dynamic functions <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b29">29]</ref>. Under clearing, the number and size of swarms is adjusted dynamically by constructed an ordered list of particles, ranked according to their fitness, with spatially close particles joining a particular species. This method relies on a speciation radius and has no further diversity mechanism. Other related work includes using different swarms in cooperation to optimize different parts of a solution <ref type="bibr" target="#b35">[35]</ref>, a two swarm min-max optimization algorithm <ref type="bibr" target="#b33">[33]</ref> and iteration by iteration clustering of particles into sub-swarms <ref type="bibr" target="#b26">[26]</ref>. Apart form Parrot and Li's speciation, none of these multi-population techniques have been generalized as dynamic optimizers. The multi-swarm approach of Blackwell and Branke is described in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multi-swarms</head><p>A combined approach might be to incorporate the virtues of the multipopulation approach and of swarm diversity enhancing mechanisms such as repulsion. Such an optimizer would be well suited to the many peaks environment. Multi-swarms, first proposed in a non-optimization context <ref type="bibr" target="#b8">[8]</ref> would seem to do just this. The extension of multi-swarms to a dynamic optimizer was made by Blackwell and Branke <ref type="bibr" target="#b5">[6]</ref>, and is inspired by Branke's own selforganizing scouts (SOS) <ref type="bibr" target="#b13">[13]</ref>. The scouts have been shown to give excellent results on the many peaks benchmark.</p><p>A multi-swarm is a colony of charged swarms interacting locally via exclusion and globally by anti-convergence. The motivation for these operators is that a mechanism must be found to prevent two or more swarms from trying to optimize the same peak (exclusion) and also to maintain multi-swarm diversity, that is to say the diversity amongst the population of swarms as a whole (anti-convergence). Multi-swarms have been compared very favorably to both hierarchical swarms and to self-organizing scouts.</p><p>We consider below the main ingredients of the multi-swarm algorithm in more depth. In particular we will assess the values of parameters with relation to the many peaks benchmark. A complete discussion of parameter choices is given in <ref type="bibr" target="#b7">[7]</ref>. The multi-swarm algorithm is presented in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Atom Analogy</head><p>In the atom analogy, each swarm is pictured as an atom with a contracting nucleus of neutral PSO particles, and an enveloping cloud of charged particles. All particles are in fact members of the same information network so that they all (in the star topology) have access to p g . The mutual repulsions between the charged particles may follow a deterministic, classical, rule (Coulomb repulsion, parameterized by particle charge Q). Alternatively, in a quantum atom, the particles are positioned within a hypersphere of radius r cloud centered on p g according to a probability distribution. So far, two uniform distributions have been tested. References <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">7]</ref> consider a uniform shell distribution, p(r, dr) = ρ(r)dr = const, where ρ is a probability density, r is a shell radius, dv is a volume element and p is a probability. Recent work has investigated a uniform volume distribution, p(x, dv) = ρ(x)dv = const. In both cases, the distributions are normalized so that p(r &gt; r cloud ) = 0. Other, non-uniform and possibly dynamic distributions might be favorable for the quantum swarm in some cases, but such distributions remain unexplored.</p><p>The quantum atom has the advantages of lower complexity and an easily controllable distribution: the Coulomb repulsion has quadratic complexity and highly fluctuating electron orbits <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. An order of magnitude estimation for the parameters r cloud (for quantum swarms), or Q, for classically charged clouds, can be made by supposing that good tracking will occur if the mean charged particle separation &lt; |x -p g | &gt; is comparable to s. This separation is easy to compute for the quantum swarm: only empirical data is available for classical charged particles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Exclusion</head><p>In order to demonstrate the necessity for exclusion, first consider an assembly of non-interacting swarms, also known as a many-swarm. A many-swarm has M swarms, and each swarm, for symmetrical configurations has N 0 neutral and N -charged particles. Such a many-swarm is written M * (N 0 + N -). Since the swarms do not interact -either dynamically through the particle velocity and positions updates, or by sharing information -the M swarms are completely independent, and any number of them may try to optimize the same peak. This is undesirable because it is clearly inefficient to have two or more swarms on the same peak, and in any case, we wish to distribute the swarms throughout the search space for peak watching. Hence the swarms must interact in some way. One possibility is to allow the swarms to interact topologically and share a single information network. The multi-swarm approach is to seek a spatial interaction between swarms. Such an interaction might repel entire swarms from already occupied peaks. However, Coulomb repulsion, or some such similar physics-inspired repulsion would not be satisfactory, because the attractive pull towards the peak might be balanced by the repulsive force away from other nearby swarms. In such an equilibrium, no swarm would be able to optimize the peak.</p><p>Exclusion is inspired by the exclusion principle in atomic and molecular physics. This principle states that no two electrons may occupy the same state. The exclusion principle provides an effective repulsive force between two gas molecules with overlapping electron clouds <ref type="bibr" target="#b22">[22]</ref>. However the effective force does not arise from any deterministic equation that governs the electron motion, but is a rule imposed on the probability distributions of the electron positions. A version of this principle for interacting swarms is a rule that forbids two swarms moving to within r excl of each other, where the distance between swarms is defined as the distance between their p g 's. The exclusion operator simple randomizes, in the entire search space, the worse swarm in any collision, as judged by the current best value determined by the swarm, f (p g ). The configuration of the interacting multi-swarm is written M (N 0 + N -).</p><p>An order of magnitude estimation for r excl can be made by assuming that all p peaks are evenly distributed in X d . The linear diameter of the basin of attraction of a peak is then, on average, d boa = X/p 1/d . It is reasonable to assume that swarms that are closer than this distance should experience exclusion, since the overall strategy is to place one swarm on each peak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Anti-Convergence</head><p>Anti-convergence is a simple operator that is designed to ensure there is at least one free swarm in the multi-swarm at all times. A free swarm is one that is patrolling the search space rather than converging on a peak. A swarm is assumed to be converging when the neutral swarm diameter is less than a convergence diameter, 2r conv . The idea is that if the number of swarms is less than the number of peaks, all swarms may converge, leaving some peaks unwatched. One of these unwatched peaks may later become optimal. The presence of free swarms maintains multi-swarm diversity and encourages response to peak promotion.</p><p>Estimations of r conv are difficult, but some progress can be made by considering the rate of convergence of the neutral swarm, as given by Equation <ref type="formula" target="#formula_3">4</ref>. A lower bound on r conv can be estimated from the ideal case that a swarm immediately tracks a shifted peak. This means that the swarm size at the shift is about s and the swarm has K function evaluations worth of time to contract around the peak. On the other hand, r conv should certainly be less than r excl because exclusion occurs before convergence.</p><p>Note that there are two levels of diversity. Diversity at the swarm level , as enforced by exclusion, enables a single swarm to track a single moving peak and diversity at the multi-swarm level enables the multi-swarm as a whole to find new peaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multi-swarm cardinality</head><p>The multi-swarm cardinality M can be estimated from p. If possible we would expect that M &gt; p is undesirable since free swarms absorb valuable function evaluations and there is no need to have many more swarms than peaks. Anticonvergence is expected to be beneficial for M &lt; p. Optimally, we suppose that M = p, and in this case anti-convergence can be switched off, since the multi-swarm has just the right number of swarms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results</head><p>A exhaustive series of multi-swarm experiments has been conducted for the many peaks benchmark. The standard settings for MPB are number of peaks, p = 10, change period in function evaluations, K = 5000, peak shift severity, s = 1.0, dimensionality, d = 5 and search space range X = 100. The peak heights and widths vary randomly but are constrained to <ref type="bibr" target="#b30">[30,</ref><ref type="bibr">70]</ref> and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">12]</ref> respectively. Each experiment actually consists of 50 runs for a given set of multi-swarm parameters. Each run uses a different random number generator seed for initialization of the swarms, the swarm update algorithm and the MPB generator. Other non-standard MPB's were also tested for comparisons. Multi-swarm performance is quantified by the offline error which is the average, at any point in time, of the error of the best solution found since the last environment change. This measure is commonly used for scenarios such as the MPB and is zero for perfect tracking.</p><p>Various values of multi-swarm cardinality M were tested for fixed total number of particles as given by the expression N total = M (N 0 + N -). As expected, M = p was found to be optimal. The multi-swarm coped well with shift severities between 1.0 and 6.0. The multi-swarm offline errors for MPB's with different numbers of peaks were lass than 3.0 for 5 ≤ p ≤ 200. Anticonvergence was found to bring a significant improvement when M &lt; p. The robustness of the algorithm, and its generalizability into higher dimensions, was also tested by taking d = 10 and varying the predicted optimal parameter settings (i.e. r excl , r conv , r cloud and Q) by <ref type="bibr" target="#b20">20</ref> In all cases, the multi-swarms with charge outperform many-swarms, PSO and multi-swarms without charge. Furthermore, quantum swarms perform better than classical charged swarms. The offline error, as compared to hierarchical swarms and self-organizing scouts, is cut by a half, and the improvement over a randomization scheme is about an order of magnitude. It seems, therefore, that the multi-swarm is a very promising approach for problems of the MPB class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Self-adapting multi-swarms</head><p>The multi-swarm model of the previous section introduced a number of new parameters. Although recommendations can be made for these settings, this analysis depends on prior knowledge of the environment and on many test runs. A laudable goal for any optimization technique is the reduction of handtunable parameters. Although parameter adjustments might improve performance for any particular problem, a general purpose method that might perform reasonably well across a spectrum of problems is certainly attractive. We therefore wonder to what extent PSO and PSO-variants can find their own best parameter settings during a single run. Such self-adapting algorithms might not return the best performance on any particular problem instance. However to make fair comparisons, the total number of function evaluations (or iterations) involved in all the trials of the hand-tuned method must be taken into account. This point is emphasized by Clerc in his explorations of 'Tribes', a self-adapting PSO <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>Here we will describe self-adaptations at the level of the multi-swarm. Future work will seek to incorporate adaptations of individual swarms into this scheme. The following will assume (5 0 + 5 -) swarms with canonical PSO neutral particles and quantum charged particles and with the swarm diversity parameter, r cloud , determined by the peak shift severity. This recipe gave the best results for the environments studied in the previous section. The parameters at the multi-swarm level are the number of swarms M and the exclusion and convergence radii r excl and r conv . It is assumed in the following that the multi-swarm has access to the dimensionality d and the extent of the search space X, but not the MPB parameters p or K. (Previously, r excl , r conv and M were determined with knowledge of p and K.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Swarm birth and death</head><p>The basic idea is to allow the multi-swarm to regulate its size by bringing new swarms into existence, or by removing redundant swarms. The aim, as before, is to place a swarm on each peak, and to maintain multi-swarm diversity with (at least one) patrolling swarm. The multi-swarm therefore needs a new swarm if all swarms are currently converging. Alternatively, if there are too many free swarms (i.e. those that fail the convergence criterion), a free swarm should be removed. If there is more than one free swarm, the choice for removal is arbitrary and a simple rule might be to remove the worst of the free swarms, as judged by f (p g ).</p><p>This simple birth/death mechanism removes the need for the anti-convergence operator, and for specifying the multi-swarm cardinality. The selfadapting version of Algorithm 2 is given below in Algorithm 3, where M f ree is the number of free swarms at iteration t, and identical steps in Algorithm 2 have been abbreviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Self-adapting multi-swarm</head><p>Begin with a single free swarm, randomized in X d At each iteration t: For generality, Algorithm 3 also specifies a redundancy parameter n excess which is set to the desired number of free swarms. A simple choice is to suppose that n excess = 1, but this may not give sufficient diversity if there are many peaks. Alternatively, n excess = ∞ means that no swarm can ever be removed. Intermediate values control the amount of multi-swarm diversity. Part of the purpose of the experiments reported below is to assess the algorithm for robustness in n excess .</p><formula xml:id="formula_4">IF M f ree = 0,</formula><p>The multi-swarm size M (t) is dynamic and at any iteration t is given by</p><formula xml:id="formula_5">M (0) = 1 M (t) = M (t -1) + 1, M f ree = 0 M (t -1) -1, M f ree &gt; n excess (5)</formula><p>Swarm convergence and exclusion are now determined by a dynamic convergence radius r(t) defined by</p><formula xml:id="formula_6">r(t) = X 2M 1/d<label>(6)</label></formula><p>which has been chosen to ensure a mean volume per swarm of (2r) d = X d M , a condition which might be expected to be if the peaks are, on average, uniformly distributed in X d . The number of free swarms at any time is the difference between the multi-swarm size and the number of converging swarms. A swarm is defined as 'converging' if its diameter is less than 2r. The exclusion radius is replaced by r(t). Hence two parameters, M and r excl and one operator, anti-convergence, have been removed from the multi-swarm algorithm, at the expense of introducing a new parameter, n excess .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>A number of experiments using the MPB of Section 5.5 with 10 and 200 peaks were conducted to test the efficacy of the self-adapting multi-swarm for various values of n excess . The uniform volume distribution described in Section 5.1 was used. An empirical investigation revealed that r cloud = 0.5s for shift length s yields optimum tracking for these MPB's, and this was the value used here. Table <ref type="table">6</ref>.2 shows the raw and rounded offline errors for 1 ≤ n excess ≤ 7 and for n excess = ∞. Only the rounded errors are significant, but the pre-rounded values have been reported in order to examine algorithm functionality. For comparison, the best performance of an unadapted 10(10 0 +10 -) multi-swarm for the p = 10 and p = 200 environments is, respectively, 1.75(0.06) and 2.26(0.03) <ref type="bibr" target="#b7">[7]</ref>. The best self-adapting multi-swarm errors are 1.77(0.05) and 2.37(0.03), only slightly higher than the hand-tuned values. The constancy of the raw offline error for n excess ≥ 5 shows that the algorithm never tries to generate 6 or more swarms: n excess = 5 is equivalent to setting n excess to ∞. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>Theoretically, n excess = 1 would appear to be an ideal setting, allowing the multi-swarm to adapt to the number of peaks, whilst always maintaining a single free swarm. However, inspection of the numbers of free and converged swarms for a single function instance revealed that the self-adaptation mechanism at n excess = 1 frequently adds a swarm, only to remove one at the subsequent iteration. The explanation is believed to be the following: suppose a swarm (swarm A) has started to converging on a peak. A new free swarm, swarm B, will be created. Since A is just at the edge of convergence, fluctuations in the swarm size may cause the swarm to appear to be free at the next iteration. Hence there will be two free swarms, and one must be removedalmost certainly swarm B, since this has had little chance to improve its p g . Swarm A will again start to converge (according to the criterion), causing the creation of a new free swarm at the next iteration. Such repeated creations and annihilations of the free swarm waste valuable function evaluations. Another possible setting is n excess = ∞. This effectively turns swarm removal off. Although there is no check to the number of swarms, it is not unreasonable to suppose that given enough time, the multi-swarm would stabilize at M conv = p and M f ree = 1. The convergence criterion is rather naive and may mark a free swarm as converging even though it is not associated with a peak. This will cause the generation of another free swarm, with no means of removal. This will only be a problem when M conv &gt; p, a situation that might not even happen within the time-scale of the run. For example, Figures <ref type="figure" target="#fig_1">4</ref> and<ref type="figure" target="#fig_2">5</ref> show convergence data for a single run at p = 10 and p = 200. For p = 10, the eleventh swarm is generated at function evaluation, n eval , = 254054. There are 500000 evaluations in a run, and in the remaining evaluations the multi-swarm size is, at most 13, and remains fairly steady after the 400000th evaluation. The p = 200 trial shows a steadily growing multi-swarm, which never attains complete coverage of all peaks, ending with 47 converging swarms and 1 free swarm. The flexibility of a swarm removal mechanism is desirable for a number of reasons. For example, two peaks might move within r excl of each other, causing a previously converged swarm to vaporize through exclusion (the better swarm remains). Or maybe a peak i becomes invisible if its height is smaller than other peak heights at its optimizer i.e. if f (x * i ) &lt; f (x j ) for some j = i. In either case, superfluous free swarms will consume function evaluations.</p><p>The redundancy n excess can be set at any value between the two extremes of n excess = 1 and n excess = ∞. (n excess = 0 gives very bad performance, no swarms at all may be added and the single swarm converges, and remains on, the first peak it finds.) The results for p = 10 and p = 200 indicate that tuning of n excess can improve performance for runs where the multi-swarm has found all the peaks. Tuning can prevent the multi-swarm from generating too many free swarms -for example 3 free swarms are optimal for p = 10. However, setting n excess at either extreme still produces good performance, and better than the comparison algorithms cited in Section 5.5. Perhaps the multi-swarm itself could tune n excess during a run. A more sophisticated convergence criterion would also have to be devised. For example, the convergence criterion could take into account both the swarm diameter and the rate of improvement of f (p g ). This chapter has reviewed the application of particle swarms to dynamic optimization. The canonical PSO algorithm must be modified for good performance in environments such as many peaks. In particular the problem of diversity loss must be addressed. The most promising PSO variant to date is the multi-swarm; a multi-swarm is a colony of swarms, where each swarm, drawing from an atomic analogy, consists of a canonical PSO surrounded by a cloud of charged particles. The underlying philosophy behind multi-swarms is to place a separate PSO on the best peaks, and to maintain a population of patrolling particles for the purposes of identifying new peaks. Movement of any peak that is being watched by a swarm is tracked by the charged particles. An exclusion operator ensures that only one swarm can watch any one peak, and anti-convergence seeks to maintain a free, patrolling swarm.</p><p>New work on self-adaptation has also been presented here. Self-adaptation aims at reducing the number of tunable parameters and operators. Some progress has been made at the multi-swarm level, where a mechanism for swarm birth and death has been suggested; this scheme eliminates one operator and allows the number of swarms and an exclusion parameter to adjust dynamically. One free parameter, the number of patrolling swarms, still exists, but results suggest that the algorithm is not overly sensitive to this number. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The swarm diameter</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Convergence of the self-adapting nexcess = ∞ multi-swarm for a single instance of the 10 peak MPB environment. Upper plot shows offline error, lower plot shows number of converged and free swarms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Convergence of the self-adapting Mexcess = ∞ multi-swarm for a single instance of the 200 peak MPB environment. Upper plot shows offline error, lower plot shows number of converged and free swarms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Variation of offline error with nexcess for 10 and 200 dynamic peaks. The raw data demonstrates identical algorithm behavior for nexcess ≥ 5</figDesc><table><row><cell></cell><cell>Raw</cell><cell></cell><cell cols="2">Rounded (standard error)</cell></row><row><cell>nexcess</cell><cell>p = 10</cell><cell>p = 200</cell><cell>p = 10</cell><cell>p = 200</cell></row><row><cell>1</cell><cell cols="3">1.9729028458116666 2.5383187958098343 1.97(0.09)</cell><cell>2.54(0.04)</cell></row><row><cell>2</cell><cell cols="3">1.879641879674056 2.398361911062971 1.88(0.07)</cell><cell>2.40(0.02)</cell></row><row><cell>3</cell><cell cols="3">1.7699076299648027 2.386396596554201 1.77(0.05)</cell><cell>2.39(0.03)</cell></row><row><cell>4</cell><cell cols="3">1.8033988974332964 2.372590853208213 1.80(0.06)</cell><cell>2.37(0.03)</cell></row><row><cell>5</cell><cell cols="3">1.8013758537004643 2.365026825401844 1.80(0.06)</cell><cell>2.37(0.03)</cell></row><row><cell>6</cell><cell cols="3">1.8010120393728533 2.3651325663361167 1.80(0.06)</cell><cell>2.37(0.03)</cell></row><row><cell>7</cell><cell cols="3">1.8010120393728533 2.3651325663361167 1.80(0.06)</cell><cell>2.37(0.03)</cell></row><row><cell cols="4">infinity 1.8010120393728533 2.3651325663361167 1.80(0.06)</cell><cell>2.37(0.03)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-adaptations at the level of each swarm, in particular allowing particles to be born and to die, and self-regulation of the charged cloud radius remain unexplored.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Computational Intelligence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Engelbrecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>John Wiley and sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Particle swarms and population diversity I: Analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
		<ptr target="http://www.ubka.uni-karlsruhe.de/cgi-bin/psview?document=2003%2Fwiwi%2F1" />
	</analytic>
	<monogr>
		<title level="m">GECCO Workshop on Evolutionary Algorithms for Dynamic Optimization Problems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="9" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">GECCO Workshop on Evolutionary Algorithms for Dynamic Optimization Problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
		<ptr target="http://www.ubka.uni-karlsruhe.de/cgi-bin/psview?document=2003%2Fwiwi%2F1" />
		<editor>J. Branke</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="14" to="18" />
		</imprint>
	</monogr>
	<note>Particle swarms and population diversity II: Experiments</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t push me! collision avoiding swarms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1691" to="1696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic search with charged swarms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computation Conference</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Langdon</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multi-swarm optimization in dynamic environments</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Applications of Evolutionary Computing</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Raidl</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3005</biblScope>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-swarms, exclusion and anti-convergence in dynamic environments</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Evolutionary Computation</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Swarm music: Improvised music with multi-swarms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc AISB &apos;03 Symposium on artificial intelligence and creativity in arts and science</title>
		<meeting>AISB &apos;03 Symposium on artificial intelligence and creativity in arts and science</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Swarms in dynamic environments</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computation Conference</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Cantu-Paz</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2723</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Particle swarms and population diversity</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Blackwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="793" to="802" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<ptr target="http://www.aifb.uni-karlsruhe.de/jbr/movpeaks" />
		<title level="m">The moving peaks benchmark website</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Memory enhanced evolutionary algorithms for changing optimization problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<ptr target="ftp://ftp.aifb.uni-karlsruhe.de/pub/jbr/brankecec1999.ps.gz" />
	</analytic>
	<monogr>
		<title level="m">Congress on Evolutionary Computation CEC99</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date>199</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1875" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evolutionary approaches to dynamic environments -updated survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<ptr target="http://www.aifb.uni-karlsruhe.de/jbr/EvoDOP/Papers/gecco-dyn2001.pdf" />
	</analytic>
	<monogr>
		<title level="m">GECCO Workshop on Evolutionary Algorithms for Dynamic Optimization Problems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Evolutionary Optimization in Dynamic Environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<ptr target="http://www.aifb.uni-karlsruhe.de/jbr/book.html" />
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Designing evolutionary algorithms for dynamic optimization problems. Theory and application of evolutionary computation: recent trends</title>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmeck</surname></persName>
		</author>
		<editor>S. Tsutsui and A. Ghosh</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="239" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A niching particle swarm optimizer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Den Bergh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Asia-Pacific conference on simulated evolution and learning</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="692" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adapting particle swarm optimisationto dynamic environments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carlisle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dozier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of int conference on artificial intelligence</title>
		<meeting>of int conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="429" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Think locally act locally -a framework for adaptive particle swarm optimizers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Clerc</surname></persName>
		</author>
		<ptr target="http://clerc.maurice.free.fr/pso/" />
		<imprint>
			<date type="published" when="2002-06-29">2002. June 29, 2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Particle Swarm Optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Clerc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ISTE publishing company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The particle swarm: explosion, stability and convergence in a multi-dimensional space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="158" to="173" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Flocks, herds and schools: a distributed behavioral model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="25" to="34" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An introduction to quantum physics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>W.W. Norton and Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive particle swarm optimisation: detection and response to dynamic systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proc Congress on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="1666" to="1670" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A hierachical particle swarm optimizer for dynamc optimization problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Janson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Middendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Raidl</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3005</biblScope>
			<biblScope unit="page" from="513" to="524" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1995 IEEE International Conference on neural networks</title>
		<meeting>the 1995 IEEE International Conference on neural networks</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stereotyping: improving particle swarm performance with cluster analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennnedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1507" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptively choosing neighborhood bests in a particle swarm optimizer for multimodal function optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Copmutation Conference, GECCO-2004</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</editor>
		<meeting>the Genetic and Evolutionary Copmutation Conference, GECCO-2004</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3102</biblScope>
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Fractal Geometry of Nature</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mandelbrot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>W. H. Freeman and Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A particle swarm model for tracking multiple peaks in a dynamic environment using speciation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parrott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="98" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recent approaches to global optimization problems through particle swarm optimization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Computing</title>
		<imprint>
			<biblScope unit="page" from="235" to="306" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A clearing procedure as a niching method for genetic algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Petrowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Evolutionary Computation</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Grefenstette</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="798" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<title level="m">Swarm intelligence</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Co-evolutionary particle swarm optimization to solve min-max problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khrohling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1682" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Particle swarm optimisation with spatial particle extension</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vesterstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">14741479</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A cooperative approach to particle swarm optimization</title>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Englebrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="225" to="239" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Comparing particle swarms for tracking extrema in dynamic environments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Dam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1772" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
