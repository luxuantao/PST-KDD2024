<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Programmable Parallel Accelerator for Learning and Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Srihari</forename><surname>Cadambi</surname></persName>
							<email>cadambi@nec-labs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NEC Laboratories America</orgName>
								<address>
									<addrLine>Inc. 4 Independence Way</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abhinandan</forename><surname>Majumdar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NEC Laboratories America</orgName>
								<address>
									<addrLine>Inc. 4 Independence Way</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michela</forename><surname>Becchi</surname></persName>
							<email>mbecchi@nec-labs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NEC Laboratories America</orgName>
								<address>
									<addrLine>Inc. 4 Independence Way</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Srimat</forename><surname>Chakradhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NEC Laboratories America</orgName>
								<address>
									<addrLine>Inc. 4 Independence Way</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NEC Laboratories America</orgName>
								<address>
									<addrLine>Inc. 4 Independence Way</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Programmable Parallel Accelerator for Learning and Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A55275853CB3405A53C5A3C071A3A24A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.3 [Computer Systems Organization]: Special-purpose and Application-based Systems -Microprocessor/microcomputer applications Design</term>
					<term>Experimentation</term>
					<term>Measurement</term>
					<term>Performance Accelerator-based systems</term>
					<term>parallel computing</term>
					<term>heterogeneous computing</term>
					<term>machine learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For learning and classification workloads that operate on large amounts of unstructured data with stringent performance constraints, general purpose processor performance scales poorly with data size. In this paper, we present a programmable accelerator for this workload domain. To architect the accelerator, we profile five representative workloads, and find that their computationally intensive portions can be formulated as matrix or vector operations generating large amounts of intermediate data, which are then reduced by a secondary operation such as array ranking, finding max/min and aggregation. The proposed accelerator, called MAPLE, has hundreds of simple processing elements (PEs) laid out in a twodimensional grid, with two key features. First, it uses in-memory processing where on-chip memory blocks perform the secondary reduction operations. By doing so, the intermediate data are dynamically processed and never stored or sent off-chip. Second, MAPLE uses banked off-chip memory, and organizes its PEs into independent groups each with its own off-chip memory bank. These two features together allow MAPLE to scale its performance with data size. This paper describes the MAPLE architecture, explores its design space with a simulator, and illustrates how to automatically map application kernels to the hardware. We also implement a 512-PE FPGA prototype of MAPLE and find that it is 1.5-10x faster than a 2.5 GHz quadcore Xeon processor despite running at a modest 125 MHz.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Applications that examine raw, unstructured data in order to draw conclusions and make decisions are becoming ubiquitous. Banks and credit cards companies, for instance, analyze withdrawal and spending patterns to prevent fraud or identity theft. Online retailers study website traffic patterns in order to predict customer interest in products and services based upon prior purchases and viewing trends. Semantic querying of text and images, which has wide-ranging, mass market uses such as advertisement placement <ref type="bibr" target="#b0">[1]</ref> and content-based image retrieval <ref type="bibr" target="#b1">[2]</ref>, is another fast growing application domain.</p><p>Such applications extensively use learning and classification techniques. With increasing amounts of data, the computational load imposed by these techniques becomes severe as they must be executed under stringent performance constraints. Scaling application performance with data assumes importance. As an example, for semantic text search, a server using a learning algorithm such as Supervised Semantic Indexing <ref type="bibr" target="#b2">[3]</ref> must search millions of documents at a few milliseconds per query. Another example is face and object recognition in high resolution video that is often done with Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b3">[4]</ref>. A server performing this task must search VGA (640x480) or higher resolution images at rates of 24 or more frames per second. Often, economic considerations dictate that multiple video streams be processed simultaneously on one server. Our fastest parallelized software implementation on a quad-core 2.5 GHz Xeon server processes about 7 VGA frames per second, while GPU implementations <ref type="bibr" target="#b10">[11]</ref> can reach 10 frames per second, both falling far short of requirements. Other similar workloads include digital pathology <ref type="bibr" target="#b14">[15]</ref>, automotive applications to predict failures and reduce recalls, financial analytics and cognitive databases.</p><p>Motivated by this gap between workloads and state-of-theart computing platforms, we investigate a parallel accelerator for learning and classification applications, and an accompanying tool to automatically map application kernels to the accelerator hardware. To design the accelerator, we profile five representative workloads: Supervised Semantic Indexing <ref type="bibr" target="#b2">[3]</ref>, Convolutional Neural Networks <ref type="bibr" target="#b3">[4]</ref>, K-means <ref type="bibr" target="#b4">[5]</ref>, Support Vector Machines <ref type="bibr" target="#b5">[6]</ref> and Generalized Learning Vector Quantization <ref type="bibr" target="#b6">[7]</ref>, and find that their computational kernels exhibit two common characteristics. First, they can be formulated as matrix or vector operations producing large intermediate data (potentially leading to many off-chip memory accesses), that are then reduced by a secondary operation such as array ranking, finding min/max and aggregation. Second, they exhibit coarse-grained as well as finegrained parallelism, i.e., the computations can be partitioned into parallel streams with little communication between them, with each stream processed by hundreds of simple parallel processing elements.</p><p>With this in mind, we architect MAPLE (MAssively Parallel Learning/Classification Engine), an accelerator with hundreds of simple vector processing elements (PEs) and two key features that directly address the above workload characteristics. First, MAPLE's on-chip memories are capable of in-memory processing which allows the large intermediate data to be processed on-the-fly thereby reducing off-chip memory accesses. Second, MAPLE uses banked off-chip memories with each memory bank serving a separate group of PEs, thereby creating processor-memory channels that can process the coarsegrained, independent computation streams. These two features make MAPLE's performance scale more easily with problem and data size.</p><p>While several prior efforts have developed FPGA and GPU implementations of individual algorithms such as SVMs <ref type="bibr" target="#b7">[8]</ref>[9], CNNs <ref type="bibr" target="#b9">[10]</ref>[11] and K-means <ref type="bibr" target="#b13">[14]</ref>, to the best of our knowledge, a more general, programmable architecture that is optimized across a range of learning and classification workloads has not yet been published. We believe the study and development of accelerators for this domain will become necessary as learning and classification techniques become ubiquitous.</p><p>To this end, we make the following contributions in this paper. We present the architecture of MAPLE, a parallel accelerator for learning and classification, and evaluate the use of in-memory processing for learning and classification applications. We present a strategy to automatically map application kernels to MAPLE. Using an FPGA prototype, we compare MAPLE's performance against parallel, optimized software implementations of learning and classification algorithms on multi-cores and GPUs.</p><p>The rest of the document is organized as follows. We discuss related work in Section 2, and describe our workloads in Section 3. In Section 4, we describe the MAPLE architecture, explore its design space and present a compilation strategy. In Section 5, we present our FPGA prototype and performance measurements. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Prior work in accelerating learning and classification workloads can be classified broadly into four categories: (i) optimized, parallel libraries for multi-core CPUs, (ii) optimized implementations on graphics processors (GPUs) <ref type="bibr" target="#b8">[9]</ref>[10] <ref type="bibr" target="#b10">[11]</ref>[14], (iii) algorithm-specific accelerators on FPGAs <ref type="bibr" target="#b7">[8]</ref> and (iv) other embedded and analog hardware implementations.</p><p>Multi-core CPUs and many-core GPUs <ref type="bibr" target="#b17">[18]</ref>[21] accommodate diverse learning and classification workloads through programmability. However multi-cores cannot avail of the fine-grained data parallelism inherent in these workloads due to thread synchronization overheads and inadequate memory bandwidth. In addition, GPUs do not have banked memoryprocessor channels, and require multiple independent parallel streams to be coalesced and synchronized. Neither CPUs nor GPUs have enough on-chip storage to handle the large intermediate data generated by these applications. In this paper, we quantitatively compare MAPLE to both CPU and GPU implementations, using optimized software libraries such as Intel MKL BLAS and NVIDIA's CUBLAS.</p><p>Several prior efforts have developed algorithm-specific implementations of SVMs <ref type="bibr" target="#b26">[27]</ref>, CNNs <ref type="bibr" target="#b22">[23]</ref> and deep learning <ref type="bibr" target="#b25">[26]</ref>. There are also architectures <ref type="bibr" target="#b18">[19]</ref> and FPGA implementations that accelerate matrix computations <ref type="bibr" target="#b23">[24]</ref> <ref type="bibr" target="#b24">[25]</ref>. MAPLE is not algorithm-specific, not restricted to matrix operations, and can be programmed for different learning and classification algorithms. We compare MAPLE's performance with published algorithm-specific numbers from <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WORKLOAD ANALYSIS</head><p>We use five learning and classification workloads to help architect MAPLE. In this section we (i) profile these workloads to identify computational bottlenecks and make the case for an accelerator, (ii) study the nature of the computational bottlenecks (compute or memory bound), (iii) reformulate the computational bottlenecks using a set of common primitives and (iv) identify broader characteristics common to all the reformulated computational bottlenecks that the accelerator architecture must support.</p><p>The five algorithms we use are Supervised Semantic Indexing (SSI) <ref type="bibr" target="#b2">[3]</ref>, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b3">[4]</ref>, K-means <ref type="bibr" target="#b4">[5]</ref>, Support Vector Machines (SVMs) <ref type="bibr" target="#b5">[6]</ref> and Generalized Learning Vector Quantization (GLVQ) <ref type="bibr" target="#b6">[7]</ref>. SSI ranks a large number of documents based on their semantic similarity to the queries. CNNs are 2-dimensional neural networks used for pattern recognition in applications such as object and face detection <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref>, and recently even semantic text search <ref type="bibr" target="#b11">[12]</ref>. K-means clusters points into K clusters, and is commonly used in computer vision for image segmentation. SVM training finds support vectors that separate given training data into distinct classes indicated by the training data labels. GLVQ is a supervised learning algorithm to classify an input into one of several classes.</p><p>We profile each algorithm using typical data set sizes (Table <ref type="table" target="#tab_0">1</ref>, column 3), and summarize the characteristics in Figure <ref type="figure" target="#fig_1">1</ref>. The table shows the core computations in each workload and the fraction of the total running time they are responsible for. The execution profiles were measured on a 2.5 GHz quad core Xeon. It is clear that significant speedups are achievable by accelerating the core computations. The table also shows whether the workload is compute or memory bound, and the number of computations per memory operation. A memory bound workload performs one or fewer computations per memory load or store. MAPLE targets these core computations, providing adequate processing and I/O resources for both compute and memory bound workloads.</p><p>We now examine the computational bottlenecks of these workloads in more detail to find common characteristics and a set of primitives that may be used to design the accelerator. Figure <ref type="figure" target="#fig_1">1</ref> shows the five workloads, their typical parameters and how the computational bottleneck may be transformed into a common set of primitives.</p><p>In SSI <ref type="bibr" target="#b2">[3]</ref>, given D documents, we find K semantic best matches for each of Q concurrent queries. This amounts to a series of dot-products between the document and query vectors, followed by a ranking process to extract the top K matches. These operations may be transformed into matrix multiplications (by reorganizing the document and query vectors into matrices) which produces a large intermediate result matrix, and array ranking to rank each column of the intermediate matrix and produce the final result.</p><p>CNN <ref type="bibr" target="#b3">[4]</ref> convolves images with "kernels", which are small weight matrices that are part of a given CNN network. We express convolutions as matrix operations by creating matrices out of different parts of the input images, multiplying with the kernels and using the result matrices to update different portions of the output image. This requires specialized memory access patterns that mimic a convolution operation.</p><p>In K-means, the computational bottleneck is finding the closest of mean for N points. This can also be expressed as a matrix multiplication followed by a procedure to find the minimum element in each row of the intermediate result matrix. SVM's core computation is a large matrix-vector multiplication, where the matrix is typically too large for on-chip caches. Finally, GVLQ requires a matrix-vector multiplication followed by a minimum finding operation.</p><p>From Figure <ref type="figure" target="#fig_1">1</ref> we note that: (i) matrix operations are a common primitive, but matrix sizes vary from very small (CNN kernels) to very large (SVM), (ii) one matrix operand is constant while the other changes, (iii) a large intermediate result is produced before being reduced to a relatively small final output, (iv) the primitives used to reduce the intermediate result (array rank, find minimum) can be implemented using in-memory processing and (v) specialized memory access patterns are required (e.g., CNN). We architect MAPLE with these requirements in mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MAPLE ARCHITECTURE AND COMPILATION SCHEME</head><p>In this section, we present the MAPLE architecture, explore its design space and sensitivities with a simulator and present ways of automatically mapping application kernels to the hardware. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kmeans</head><p>Given  </p><formula xml:id="formula_0">DOC MATRIX (D x c) QUERY MATRIX (c x Q) X INTERM. RESULT (D x Q) FINAL (K x Q) MATMUL ARRAY RANK IMAGE MATRICES KERNEL MATRICES X OUTPUT MATRICES POINTS MATRIX (N x d) MEANS MATRIX (d x K) X MATMUL MATMUL DIST. MATRIX (N x K) FIND MIN TRAINING DATA MATRIX (N x d) X MATVEC MUL d x 1 N x 1 REF VECTORS (N x d) X MATVEC MUL FIND MIN N x 1 TEST VECTOR d x 1 N x 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>From the workload analysis, we find the architecture must support matrix and vector operations (both large and small matrices), handle large intermediate data and perform reduction operations such as array ranking, finding max/min and aggregation. These requirements lead us to the following design decisions.</p><p>First, matrix and vector operations are implemented by streaming data through a two-dimensional array of fine-grained vector processing elements (PEs). This allows minimizing instruction overhead and accelerating operations involving small matrices as well large matrices. Second, we use in-memory processing to handle the intermediate data on-the-fly. By performing reduction operations using on-chip memory blocks, we obviate the need for off-chip accesses to store and reload intermediate data.</p><p>We spatially lay out the PEs so that each PE produces a few elements of the output matrix. Each PE has its own local storage. By distributing the columns of one matrix across all PEs and streaming the rows of the other matrix through each PE, matrix multiplication is performed naturally (with each PE performing a multiply-accumulate of the streaming row and its stored column). PEs stream results into "smart memories" that perform inmemory processing of the intermediate data (e.g., finding minimums, ranking arrays, etc). Finally, to support complex access patterns that may result when applications such as CNN are cast as matrix operations, we provide an input buffer that may be addressed by the processing fabric and a memory controller that can support custom access patterns from off-chip.</p><p>Figure <ref type="figure">2</ref> shows the details of the core architecture of MAPLE. A core has P vector PEs organized as H processing chains of M PEs each (P=H*M). Each chain has a bi-directional nearest neighbor interconnect ("intra-chain") along which inputs are propagated from left to right and outputs from right to left. The first PE in every chain accepts inputs from an input buffer (labeled input local store). Each PE also has a private local store which can be written with data from off-chip. A PE chain sends its outputs to the smart memory block, one of which is available to each processing chain, which performs in-memory processing like array ranking, finding min/max or aggregation. Each PE takes two vector operands as inputs, one from its local store, and the other streaming from the input buffer.</p><p>MAPLE (Figure <ref type="figure">3</ref>) has C processing cores each connected to two off-chip memory banks. It is connected to a generalpurpose host computer via a communication interface such as PCI. A high bandwidth bus connects each memory bank to its corresponding core. A switch enables the core to alternate between memory banks for inputs and outputs, or use both banks for inputs or outputs. Each core also has a separate instruction memory bank that is written by the host. The host can also write to the data memory banks via a slower bank-to-bank interconnection network. The architecture is tailored to applications that can be parallelized into separate memoryprocessor core "channels", with infrequent communications across the channels. The memory architecture can scale by increasing the number of banks.</p><p>Processing Elements and their Interconnection: The PEs, shown in Figure <ref type="figure" target="#fig_3">4</ref>(A), perform standard ALU, multiply and multiply-accumulate operations in a single cycle. A PE is a simple vector processor with two operands -one from the PE on its left via the intra-chain interconnect, and the other from its private local store. The intra-chain interconnect bus is M words wide and matches the number of PEs in the chain. Thus the PE chain can perform up to M vector operations at a time. A PE can select any word from its intra-chain interconnect bus, leading to  Unless the smart memory block issues a stall, a store takes M cycles. This latency can be hidden by the next vector operation if the vector size is large enough. To facilitate indexing of results (a feature required for K-means and GVLQ), a PE also sends its ID along with the results to be stored. Smart Memory Blocks: Each chain in a MAPLE core has a memory block capable of two atomic store operations. The first is a variable latency store for selection and ranking operations in large arrays, and the second a read-modify-write. The memory block can be written to only by the processing chain, and is read by the reduce network. compared to the array size, there are more discards than insertions. In the event of an insertion, the store operation stalls the processor in order to scan LIST and update the filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mapping applications onto MAPLE</head><p>In order to program MAPLE, the user expresses application kernels in terms of a primary operation (i.e., matrix multiplication) and a reduction function (e.g., finding max/min for K-means and GLVQ, ranking arrays for SSI). At a low level, MAPLE is programmed using specialized assembly. In order to free the programmer from low level programming issues, we provide a tool that, given the input matrices and the reduction function: (i) maps the data onto input-and PE-local stores, (ii) maps data and reduction operations onto the smart memory blocks, and (iii) generates the assembly used to program MAPLE.</p><p>MAPLE's design goals are to handle matrices of various sizes, and minimize intermediate data by performing reduction operations using smart memory blocks. Data placement and smart memory configuration are key aspects. The mapping algorithm determines the data placement by analyzing the sizes of input matrices A and B. Assuming that A is streamed from the input local store and B is stored in the PE local stores, a fundamental output of the mapping process is the parallelism mode parameter that determines how B is split across PEs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From PE i+1</head><p>To PE i-1 Figure <ref type="figure" target="#fig_5">5</ref> illustrates how the parallelism mode pm affects the mapping. If pm is greater than one, each column of the matrix B is replicated on pm PEs. Those PEs will process different rows of matrix A concurrently, and the results will relate to different rows of the output matrix. If pm is less than one, each column of B is split into 1/pm portions stored on different PEs. Rows of A are split as well and properly distributed to the PEs. In this case, the smart memory will need to accumulate results from the different PEs processing the same column before performing the reduction operation. Therefore, the parallelism modes affects: (i) the data placement in the PE local stores, (ii) the data distribution from the input local store to the PEs, (iii) the smart memory configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: The MAPLE (A) PE and (B) smart memory block</head><formula xml:id="formula_1">A i A i A i+1 A i+1 B 1 B 1 B 2 B 2 PE 2 PE 3 PE 4 PE 1 A iY A iY A iX A iX B 1X B 1Y B 2X B 2Y PE 2 PE 3 PE 4 PE 1 A 1 X Y X Y A 2 A 3 A 4 B 1 B 2 B 3 B 4 A 1X A 2X A 3X A 4X A 1Y A 2Y A 3Y A 4Y B 1X B 2X B 3X B 4X B 1Y B 2Y B 3Y B 4Y parallelism mode = 2 parallelism mode = ⅟₂ Input LS Input LS (a) (c) A i A i A i A i B 1 B 2 B 3 B 4 PE 2 PE 3 PE 4 PE 1 A 1 A 2 A 3 A 4 B 1 B 2 B 3 B 4 parallelism mode = 1 Input LS (b)</formula><p>Figure <ref type="figure">6</ref> shows how two matrices are mapped onto MAPLE. The mechanism can be generalized to multiple matrices. It is based on the larger matrix (A) streaming row-wise from the input local store, and the smaller matrix (B) mapped column-wise onto the PE local stores. Either matrix may be blocked if it is too large to fit its memory. For matrix B, each local store will potentially accommodate one or more columns. If B is small, the same column is mapped onto different PEs (pm &gt; 1). In this case, PEs containing the same column of B will process different rows of A concurrently. If the columns of B are too large to fit in a single PE local store, they are split over multiple PEs (pm &lt; 1). During operation, the rows of A will be split and directed to the correct PE. ; increments the address in input local store to read from } } DUMP_SM ; dumps the content of smart memory }</p><formula xml:id="formula_2">Input: matrixes M 1 (m,k) and M 2 (k,n) m*k &gt;k*n A(A R ,B R ) = M 1 B(B R ,B C )= M 2 A(A R ,B R ) = M 2 T B(B R ,B C )= M 1 T Column B i fits</formula><p>The pseudo-code above shows the assembly code generation. Bold keywords represent generated assembly directives, and italicized keywords represent configuration variables provided by the user or parameters produced by the mapping tool. A_blocks and B_blocks are row-wise and columnwise portions of A and B matrixes fitting the input-and the PElocal stores, respectively. The example assumes partial results computed on A-blocks fit the smart memory. SET_PARALLEL_MODE affects how B is mapped onto the PE local stores, potentially with column replication (pm &gt; 1) or splitting (pm &lt; 1). It also affects how the rows of A are distributed to the PEs, as well as the smart memory configuration. SET_SM_ADDR instructs the first PE of each chain; the remaining PEs are automatically configured based on pm. If B is a (BR,BC) matrix, b_col_size is equal to BR if pm is greater than one, and to BR*pm otherwise. Taking SSI as an example, the user may provide the following inputs: document matrix of size 2M document x 64 categories, query matrix of size 64 categories x 64 queries, and reduction operation equal to top-64 ranking. The mapping phase will set matrix A to be document matrix, and matrix B to be the query matrix. Additionally, assuming 4B data, 32 chains of 8 PEs each, 2KB PE local stores and 64KB input local stores, the mapping algorithm will produce the following parameters to configure the assembly generator code: pm = 8, B_block = 1, A_block = 7813, a_num_rows = 256 (zero-padding is performed in the last A_block), b_col_size = 64 and b_num_cols = 2. Figure <ref type="figure" target="#fig_6">7</ref> summarizes the outcome of the mapping process on the considered workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Architectural exploration</head><p>We developed a C++ cycle-accurate simulator that takes as input assembly code for applications mapped to MAPLE and an architectural configuration file that specifies the off-chip memory architecture (banks and bandwidth) and processor layout, and produces an estimate of MAPLE's execution time.</p><p>Table <ref type="table" target="#tab_4">2</ref> shows the different parameters of MAPLE. We seek to find, given an off-chip memory organization and processor budget, the processor layout (chain size and number of chains) that maximizes performance for different applications. We use different instances of SSI, CNN and K-means as examples to explore the architectural design space.</p><p>We used an SSI instance of 2M documents, each expressed as a vector of size 100, and extracted the top 32, 64 and 128 best matches (i.e., K=32, 64, 128). We simulated this for 1024 queries across various chain lengths (Figure <ref type="figure" target="#fig_7">8(A)</ref>). Because of MAPLE's smart memories that rank arrays dynamically, the number of cycles was largely insensitive to K. The processor layout with the best performance was when the chain size was 6-8 processors. This is because in our SSI mapping, each PE chain compares the same set of documents with a different query. Since the documents are broadcast from the off-chip memory bank, the   performance is best when the chain can consume as many documents as the memory can provide. Figure <ref type="figure" target="#fig_7">8</ref>(B) shows MAPLE's performance for 4 CNN networks. The best performer ranges between M=4 to M=16. This is due to the fact that CNNs are compute-bound, and the amount of parallelism across networks. For instance, networks with many kernels can be converted into a large matrix-matrix multiplication, for which a larger chain is better.</p><p>We present K-means in Figure <ref type="figure" target="#fig_7">8</ref>(C). The best chain sizes ranges from 64 down to 16 as K varies from 32 to 128. This is because when we map K-means to MAPLE, the number of points that can be multiplied with the means in parallel decreases as the number of means increases. Therefore a "tall, skinny" configuration with small chain sizes suits large values of K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of in-memory processing</head><p>The primary advantage of in-memory processing is reduced offchip accesses and, as a consequence, improved performance. Using the simulator, we evaluate the extent of off-chip accesses reduced, as well as the performance boost this provides MAPLE. Table <ref type="table" target="#tab_6">3</ref> shows the number of bytes loaded and stored to off-chip memory with and without in-memory processing (i.e., with and without the smart memory). It also shows the number of bus transactions. We consider SSI, CNN and K-means, and average the results across different instances of each application. For SSI, the smart memory performs array ranking, while for CNN it performs aggregation and for K-means, it computes the minimum. The reduction in the number of off-chip accesses ranges from 1.6x to 76x.</p><p>We also use the simulator to compute the actual execution time of SSI and K-means with and without in-memory processing. Table <ref type="table" target="#tab_7">4</ref> shows that without performing in-memory processing for array ranking, the SSI execution time on MAPLE increases by a factor of 17. For K-means, if the minimum computation is not performed in-memory, the execution time increases by almost 2.5x. The speedups can be attributed to reducing off-chip memory accesses as well as to overlapping the secondary and primary operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PROTOTYPE AND EXPERIMENTAL RESULTS</head><p>In this section, we present the MAPLE prototype and its measured performance. We built the prototype using an off-theshelf FPGA board from AlphaData. Our architectural design space exploration, along with FPGA constraints, determined the specific prototype architecture. We implemented each of the five workloads on the prototype.</p><p>We compare the prototype to (i) optimized parallel software implementations, (ii) available and published GPU implementations of SSI, CNN and SVM from <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b8">[9]</ref> and (iii) FPGA-based algorithm-specific implementations of CNN and SVM from <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b26">[27]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SSI and CNN</head><p>We implemented SSI in software using BLAS for matrix multiplication, and an optimized multi-threaded implementation for array ranking. We compared this to a MAPLE implementation of SSI. Figure <ref type="figure" target="#fig_8">9</ref>(A) shows the performance in ms/query for document database sizes ranging from 256K to 10M, and for K=32 and K=128. For each, the 2 bars show optimized software speed and measured prototype speed to process 64 text queries. We find the MAPLE prototype to be up to 50% faster than the optimized software, We also used the prototype to search 1.8 million Wikipedia documents, a dataset from <ref type="bibr" target="#b2">[3]</ref>, and obtained a speed of 4.63 to 4.88 ms/query for K=32 and K=128 respectively. We measured speeds of five CNNs performing face and digit recognition, surveillance and automotive safety for 640x480 images. Figure <ref type="figure" target="#fig_8">9</ref> (B) shows the speed in milliseconds per frame measured with the parallel software and the MAPLE prototype. Now we compare MAPLE's performance with available GPU implementations of SSI and CNN. Table <ref type="table" target="#tab_9">6</ref> compares software, GPU and the MAPLE prototype. For SSI, we used NVIDIA's CUBLAS library and array rank routines, while for CNN, we use GPU numbers from <ref type="bibr" target="#b10">[11]</ref>. Compared to the GPU, the MAPLE prototype is 3x faster for SSI and about 50% faster for CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">K-means</head><p>Figure <ref type="figure" target="#fig_9">10</ref> shows data for K-means. MAPLE finds the closest mean for every point and transfers that information to the host, which averages all the closest points to each mean. Since successive iterations cannot be overlapped, we consider the data transfer time and the host component to compute the effective speedup. Figure <ref type="figure" target="#fig_9">10</ref>(A) breaks down the running time of K-means on MAPLE into the core MAPLE execution, data transfer and host execution. The data transfer time increases with the number of points, but the host execution is larger and responsible for reducing speedups. Figure <ref type="figure" target="#fig_9">10 (B)</ref> shows speedups of the prototype over parallel software. The data shows that MAPLE's speedup is largely independent of the number of points (and that is due to the fact that K-means can be easily parallelized by partitioning the points), but increases with the number of means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">GLVQ</head><p>We implemented GLVQ <ref type="bibr" target="#b6">[7]</ref> training and testing on the prototype, and used it for eye detection in images. The data had 128 images each represented as reference vectors of dimension 512. 64 vectors represented different eye images and 64 non-eye images. The training data set had 5400 images, and the testing data 240 images. Training images are processed sequentially, as each incrementally modifies the model. This also means considerable host-accelerator communication. Table <ref type="table" target="#tab_11">7</ref> shows the performance for the eye detection data. Considering the substantial transfer time, the projected speedup for training is 3x, but is much higher (9.5x) for testing where data may be transferred in bulk.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">SVM</head><p>In SVM, the "kernel computation", which involves multiplication of test or training vectors with the large training or support vector matrix, is mapped to MAPLE. We compare MAPLE with a recent GPU implementation of SVM <ref type="bibr" target="#b8">[9]</ref> as well as an FPGA implementation <ref type="bibr" target="#b26">[27]</ref>. An iteration involves multiplying the training set matrix with 2 training vectors; typically tens of thousands of iterations are required to complete training. Table <ref type="table" target="#tab_10">8</ref> shows SVM training performance in milliseconds per iteration for six data sets <ref type="bibr" target="#b8">[9]</ref>. The large size of the training matrix renders this problem memory bound. While the prototype underperforms the GPU implementation, we note the GPU data sets from <ref type="bibr" target="#b8">[9]</ref> are small. For instance, MNIST uses only 60K training vectors. As we show in the next section, MAPLE's performance scales well for MNIST with 2M training vectors. Further, compared to the FPGA prototype, the GPU has a higher memory bandwidth and faster, custom circuitry. If a custom MAPLE processor were built, it could benefit from similar considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Algorithm Specific Implementations</head><p>We compare the MAPLE prototype performance with algorithmspecific FPGA-based implementations of SVM and CNN from <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b22">[23]</ref> (Table <ref type="table" target="#tab_12">9</ref>). For SVM, <ref type="bibr" target="#b26">[27]</ref> reports 9.3 billion MACs per second for the MNIST dataset with 2M training vectors. The MAPLE prototype achieves nearly half that speed. For CNN, the MAPLE prototype matches the speed reported in <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We described a programmable parallel accelerator that can handle several learning and classification algorithms. We profile and analyze five representative workloads to identify their computational bottlenecks. We find the core computations of these workloads can be transformed into a matrix or vector operation producing large intermediate data which is then reduced by a secondary operation. We architect the accelerator to leverage this characteristic by provisioning many simple, parallel PEs and in-memory processing. The in-memory processing obviates the need for off-chip memory loads and stores. We also present a compilation scheme to automatically map application kernels to the accelerator. An FPGA-based prototype of the  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>GLVQ</head><label></label><figDesc>Class of input vector = class of closest of N reference vectors N : 100-1000sVector Dim d: 100s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Transforming each workload's bottlenecks to a common set of primitives</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FROMFigure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Architecture of a MAPLE processing core</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 (</head><label>4</label><figDesc>B) shows relevant architectural components of the smart memory architecture for selecting the top k elements in an array given a function to compare two elements: (i) a filter with the programmed compare function (CMP), a threshold value (VAL) and threshold address (ADDR), (ii) a list of k elements (LIST) and (iii) a hardware list scanner. The array elements are streamed in and compared with threshold VAL. If the comparison succeeds for an array element, it replaces VAL located at ADDR in LIST. The scanner then scans LIST to find a new threshold value and address and updates the filter. If the comparison fails, the element is discarded. When k is small</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Mapping matrices to MAPLE with different parallelism modes: (a) each column of B duplicated across 2 PEs (parallelism doubled), (b) 1 column of B per PE and (c) columns of B split into X and Y to fit the PE local stores (parallelism halved).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Mapping our workloads onto MAPLE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: MAPLE architectural exploration for SSI, CNN and K-means.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: MAPLE prototype performance vs optimized software for (A) SSI and (B) CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: K-means performance on MAPLE prototype</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Workload characteristics</head><label>1</label><figDesc></figDesc><table><row><cell>Workload</cell><cell>Core computations</cell><cell>% time (profile)</cell><cell>Characteristic</cell><cell>Compute ops per memory operation</cell></row><row><cell>SSI</cell><cell>Series of dot products, array rank</cell><cell>&gt; 99%</cell><cell>Dot prod: compute bound Array rank: memory bound</cell><cell>Dotprods: 25-50 Array rank: 0.001</cell></row><row><cell>CNN</cell><cell>1D, 2D, 3D convolutions</cell><cell>&gt; 99%</cell><cell>Compute bound</cell><cell>16-100</cell></row><row><cell>K-means</cell><cell>Minimum Euclidean dist.</cell><cell>~96%</cell><cell>Marginally compute bound</cell><cell>1-3</cell></row><row><cell>SVM</cell><cell>Large matrix-vector mult.</cell><cell>85-95% [27]</cell><cell>Memory bound</cell><cell>1</cell></row><row><cell>GLVQ</cell><cell>Minimum Euclidean dist.</cell><cell>&gt; 99%</cell><cell>Memory bound</cell><cell>&lt;1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Figure 6: Block diagram showing the mapping of matrices M1 and M2 onto the accelerator</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>T</cell><cell>F</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Matrix A will be streamed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>from input-LS, matrix B</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>will be loaded onto PE_LS</cell></row><row><cell></cell><cell></cell><cell></cell><cell>T</cell><cell>PE_LS</cell><cell>F</cell></row><row><cell cols="3">Each PE_LS will be assigned one or more columns B i</cell><cell></cell><cell></cell><cell cols="2">Each column B i will be split across different PE_LS</cell></row><row><cell>F</cell><cell cols="2">Does whole B fit on-chip PE_LS</cell><cell>T</cell><cell></cell><cell>F</cell><cell>Does B i fit a PE chain</cell><cell>T</cell></row><row><cell>parallelism_mode=1</cell><cell></cell><cell cols="2">parallelism mode = 2 X (maximum amount of replication of columns B i</cell><cell cols="3">parallelism mode = 1 / (PE_per_chain * chain_per_col)</cell><cell>parallelism mode = 1 / 2 X (2 X being min number of PE_LS that store B i )</cell></row><row><cell cols="2">B_blocks = B C /(col_per_PE*num_PE)</cell><cell cols="3">across different PEs allowing</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">B to fit on-chip LS)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">B_blocks = 1</cell><cell></cell><cell cols="2">B_blocks = B C /(parallelism_mode*num_PE)</cell></row><row><cell cols="3">SET_PARALLEL_MODE parallelism_mode</cell><cell></cell><cell cols="2">; sets the parallelism mode</cell></row><row><cell cols="3">SET_SM_REDUCTION reduction_operation</cell><cell></cell><cell cols="3">; configures the reduction performed by smart memories</cell></row><row><cell cols="2">SET_A_NUM_ROWS a_num_rows</cell><cell></cell><cell></cell><cell cols="3">; sets number of A rows present in each A_block</cell></row><row><cell cols="2">SET_B_COL_SZ b_col_size</cell><cell></cell><cell></cell><cell cols="3">; sets the size of the portion of B column fitting a PE local store</cell></row><row><cell cols="2">SET_B_NUM_COLS b_num_cols</cell><cell></cell><cell></cell><cell cols="3">; sets the number of B columns stored in each PE local store</cell></row><row><cell>for each A_block {</cell><cell></cell><cell></cell><cell></cell><cell cols="2">; A_blocks consist of A rows</cell></row><row><cell>WRITE_A A_block</cell><cell></cell><cell></cell><cell></cell><cell cols="3">; transfers an A_block from DRAM into input local store</cell></row><row><cell>for each B_block {</cell><cell></cell><cell></cell><cell></cell><cell cols="2">; B_blocks consist of B columns</cell></row><row><cell>WRITE_B B_block</cell><cell></cell><cell></cell><cell></cell><cell cols="3">; transfers a B_block from DRAM into input local store</cell></row><row><cell cols="2">SET_INPUT_LS_ADDR 0</cell><cell></cell><cell></cell><cell cols="3">; resets the active address in input local store</cell></row><row><cell cols="2">for each A_row_group {</cell><cell></cell><cell></cell><cell cols="3">; A_row_group consists of A rows processed concurrently</cell></row><row><cell cols="2">for b_col: 0..b_num_cols {</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SET_PE_LS_ADDR b_col*b_col_size</cell><cell cols="3">; sets the address in PE_mem to load the B data from</cell></row><row><cell cols="3">SET_SM_ADDR result_addr</cell><cell></cell><cell cols="3">; sets the address in smart memory for partial results</cell></row><row><cell cols="3">MULT_ACC_DUMP b_col_size</cell><cell></cell><cell cols="3">; performs b_col_size MACC and sends result to smart mem</cell></row><row><cell>}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">INC_INPUT_LS_ADDR sz(A_row_group)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Distribute test vectors: 1 test vector per PE chain • Stream reference vectors through all PE chains• Each PE chain computes distance between test vector and all reference vectors • Smart memory identifies closest and farthest reference vector to each point</figDesc><table><row><cell>WORK LOAD</cell><cell>Off-chip Data</cell><cell>On-chip Data</cell><cell>Computational Bottleneck (after transformation)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>• Distribute query matrix: 1 column per PE chain</cell></row><row><cell></cell><cell>D documents</cell><cell>Q queries</cell><cell>• Stream document matrix through all PE chains</cell></row><row><cell>SSI</cell><cell>stored as</cell><cell>stored as</cell><cell>• Each PE chain computes distance between query and all documents</cell></row><row><cell></cell><cell>D x C matrix</cell><cell>C x Q matrix</cell><cell>• Each PE chain streams results into its smart memory</cell></row><row><cell></cell><cell></cell><cell></cell><cell>• Smart memory extracts top K on-the-fly</cell></row><row><cell></cell><cell></cell><cell></cell><cell>• Distribute kernels column-wise across PEs</cell></row><row><cell>CNN</cell><cell>Input images</cell><cell>CNN kernels</cell><cell>• Stream images through all PEs</cell></row><row><cell></cell><cell></cell><cell></cell><cell>• Program smart memory to aggregate results in-place</cell></row><row><cell>K-means</cell><cell>N points stored as N x D matrix</cell><cell>K means stored as D x K matrix</cell><cell>• Distribute means: 1 mean per PE chain • Stream points matrix through all PE chains • Each PE chain streams distance of mean to all points into its smart memory • Smart memory identifies closest mean to each point</cell></row><row><cell></cell><cell>N training</cell><cell></cell><cell>• Vector stored in all PEs</cell></row><row><cell>SVM</cell><cell>vectors stored as N x D</cell><cell>1 vector of size D</cell><cell>• Training vector streamed / broadcast to all PEs • Each PE computes one result of the output vector</cell></row><row><cell></cell><cell>matrix</cell><cell></cell><cell>• Smart memory collects results in batches and sends off-chip</cell></row><row><cell>GLVQ</cell><cell>N reference matrix as N x D vectors stored</cell><cell>T x D matrix stored as T test vectors</cell><cell>•</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 : Architectural exploration setup</head><label>2</label><figDesc></figDesc><table><row><cell>Type</cell><cell>Parameter</cell><cell>Value</cell></row><row><cell>Off-chip Memory</cell><cell>Number of banks</cell><cell>4</cell></row><row><cell>Organization</cell><cell>Bandwidth per bank</cell><cell>8 words/cycle</cell></row><row><cell></cell><cell>PE budget</cell><cell>512</cell></row><row><cell>Processor</cell><cell>Cores (C) Chains (H)</cell><cell>2 Variable</cell></row><row><cell></cell><cell>Processors/chain (M)</cell><cell>Variable</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>shows the experimental</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 : Effect of MAPLE's in-memory processing on off-chip load/stores</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Off-chip Accesses</cell><cell cols="2">Off-chip Bus</cell><cell></cell></row><row><cell>Workload</cell><cell>Parameters</cell><cell></cell><cell>(Bytes)</cell><cell cols="2">Transactions</cell><cell>Reduction</cell></row><row><cell></cell><cell></cell><cell>No SM</cell><cell>With SM</cell><cell>No SM</cell><cell>With SM</cell><cell></cell></row><row><cell>SSI</cell><cell>2M docs, 64 queries, 32-128 top K</cell><cell>687M</cell><cell>419M</cell><cell>42M</cell><cell>26M</cell><cell>1.64x</cell></row><row><cell>CNN</cell><cell>5 networks (Figure 9(B))</cell><cell>993M</cell><cell>12.9M</cell><cell>62M</cell><cell>0.81M</cell><cell>76x</cell></row><row><cell>K-means</cell><cell>200K points, 32-128 means</cell><cell>64.7M</cell><cell>25.2M</cell><cell>40.5M</cell><cell>1.57M</cell><cell>25.7x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 : Speedup due to in-memory processing</head><label>4</label><figDesc></figDesc><table><row><cell>Workload</cell><cell>Parameters</cell><cell cols="2">Execution Time (sec) No SM With SM</cell><cell>Speedup</cell></row><row><cell>SSI</cell><cell>2M docs, 64 queries, 32-128 top K</cell><cell>4.12</cell><cell>0.24</cell><cell>17.2x</cell></row><row><cell>K-means</cell><cell>200K points, 32-128 means</cell><cell>1.52</cell><cell>0.61</cell><cell>2.46x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 : Experimental setup MAPLE FPGA Prototype</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>PEs (total)</cell><cell>512 organized as 2 cores, 32 chains/core, 8 PEs/chain</cell></row><row><cell></cell><cell>Clock speed</cell><cell>125 MHz</cell></row><row><cell></cell><cell>Memory</cell><cell>4 banks DDR2, bandwidth 8 words per bank (8GB/s total)</cell></row><row><cell></cell><cell>FPGA</cell><cell>Xilinx Virtex 5 SX240T</cell></row><row><cell></cell><cell cols="2">Host interface 64-bit, 66MHz PCI</cell></row><row><cell>Software</cell><cell cols="2">2.5 GHz quad-core Xeon, Intel MKL library</cell></row><row><cell>GPU</cell><cell cols="2">NVIDIA Tesla C870, 1.3GHz, 128 cores, CUDA 2.3 with CUBLAS library</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 : SSI and CNN performance comparisons Workload Details</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Software GPU</cell><cell></cell><cell>MAPLE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prototype</cell></row><row><cell cols="2">SSI: 2M docs, 32</cell><cell>52.8</cell><cell>11.4</cell><cell></cell><cell>3.76</cell></row><row><cell cols="2">queries, 128 top K</cell><cell>ms/query</cell><cell cols="2">ms/query</cell><cell>ms/query</cell></row><row><cell>CNN:</cell><cell>Face</cell><cell>5 fps</cell><cell>9.5</cell><cell>fps</cell><cell>13 fps</cell></row><row><cell cols="2">Recognition [11]</cell><cell></cell><cell>([11])</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 : SVM training performance (millisec / iteration)</head><label>8</label><figDesc></figDesc><table><row><cell>Data Set</cell><cell>Training</cell><cell>Dim</cell><cell>MAPLE</cell><cell>[9]</cell></row><row><cell></cell><cell>Set Size</cell><cell></cell><cell>Prototype</cell><cell></cell></row><row><cell>Adult</cell><cell>32,561</cell><cell>123</cell><cell>3.2</cell><cell>0.76</cell></row><row><cell>Web</cell><cell>49,749</cell><cell>300</cell><cell>9.27</cell><cell>3.56</cell></row><row><cell>MNST</cell><cell>60,000</cell><cell>784</cell><cell>25.69</cell><cell>12.76</cell></row><row><cell>USPS</cell><cell>7,291</cell><cell>256</cell><cell>1.22</cell><cell>0.15</cell></row><row><cell>Forest</cell><cell>561,012</cell><cell>54</cell><cell>35.26</cell><cell>6.09</cell></row><row><cell>Face</cell><cell>6,977</cell><cell>381</cell><cell>1.6</cell><cell>0.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 : GLVQ training and testing performance (in seconds) for the eye-detection data set</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell># Vectors</cell><cell>Vector Size</cell><cell>Classes</cell><cell>Vectors per Class</cell><cell>Software Time (s)</cell><cell>Data Xfer</cell><cell cols="2">MAPLE Prototype 512 PEs, 125 MHz SW HW</cell><cell>Speedup</cell></row><row><cell>Training</cell><cell>5400</cell><cell>512</cell><cell>2</cell><cell>64</cell><cell>1.7304</cell><cell>0.5196</cell><cell>0.03</cell><cell>0.55</cell><cell>2.97</cell></row><row><cell>Testing</cell><cell>230</cell><cell>512</cell><cell>2</cell><cell>64</cell><cell>0.2357</cell><cell>0.0118</cell><cell>0</cell><cell>0.01</cell><cell>9.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 : MAPLE prototype vs algorithm specific accelerators Algorithm specific MAPLE Proto.</head><label>9</label><figDesc></figDesc><table><row><cell>SVM: MNIST 9.3</cell><cell>GMACs/sec</cell></row><row><cell>[27]</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VideoSense: towards effective online video advertising</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15 th International Conference on Multimedia 2007</title>
		<meeting>15 th International Conference on Multimedia 2007</meeting>
		<imprint>
			<biblScope unit="page" from="1075" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date>Apr 08</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to Rank with (a lot of) word features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sadamasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Issue: Learning to Rank for Information Retrieval</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Berkeley Symp. on Math. Stat. and Prob</title>
		<meeting>Berkeley Symp. on Math. Stat. and Prob</meeting>
		<imprint>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast Training of Support Vector Machines Using Sequential Minimal Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods -Support Vector Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized learning vector quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jakkula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sankaradass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Chakradhar</surname></persName>
		</author>
		<title level="m">A Massively Parallel Digital Learning Processor</title>
		<imprint>
			<publisher>Neural Information Processing. Systems</publisher>
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast Support Vector Training and Classification on Graphics Processors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, 25th International Conference on</title>
		<imprint>
			<date type="published" when="2008-07">2008. Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High Performance Convolutional Neural Networks for Document Processing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chellapilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Workshop on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face Detection Using GPU-Based Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Images and Patterns, 13th International Conference</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th International Conference on Machine Learning</title>
		<meeting>of the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008-07">Jul 2008</date>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GPU Acceleration of Iterative Clustering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACM Workshop on General Purpose Computing on Graphics Processors and SIGGRAPH 2004 poster</title>
		<imprint>
			<date type="published" when="2004-08">Aug 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grading Nuclear Pleomorphism on Histological Micrographs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognition</title>
		<meeting>Int. Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face recognition: a convolutional neural-network approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997-01">Jan 1997</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Raw Microprocessor: A Computational Fabric for Software Circuits and General-Purpose Programs</title>
		<author>
			<persName><forename type="first">M D</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2002-04">Mar./Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of general-purpose computation on graphics hardware</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Purcell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="113" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling to the End of Silicon with EDGE Architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High performance dense linear algebra on a spatially distributed processor</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Robatmili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Geijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th ACM SIGPLAN PPoPP</title>
		<meeting>13th ACM SIGPLAN PPoPP</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Larrabee: a many-core x86 architecture for visual computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Programmable stream processors</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">J</forename><surname>Kapasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ho Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="54" to="62" />
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Massively Parallel Coprocessor for Convolution Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sankaradas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th IEEE International Conference on Application-specific Systems, Architectures and Processors</title>
		<meeting>20th IEEE International Conference on Application-specific Systems, Architectures and essors<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High Performance Linear Algebra Operations on Reconfigurable Systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Conference on Supercomputing, Proc. of the 2005</title>
		<imprint>
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A High Performance FPGA-Based Accelerator for BLAS Library Implementation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rousseaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hubaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guisset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Legat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Third Annual Reconfigurable Systems Summer Institute (RSSI&apos;07)</title>
		<meeting>of the Third Annual Reconfigurable Systems Summer Institute (RSSI&apos;07)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale deep unsupervised learning using graphics processors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annual international Conference on Machine Learning</title>
		<meeting>26th Annual international Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Massively Parallel FPGA-based Coprocessor for Support Vector Machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cadambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on FCCM 2009</title>
		<meeting>IEEE Symposium on FCCM 2009<address><addrLine>Napa, CA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
