<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Stochastic Graph Evolution Framework for Robust Multi-target Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bi</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92521</postCode>
									<settlement>Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting-Yueh</forename><surname>Jeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92521</postCode>
									<settlement>Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elliot</forename><surname>Staudt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92521</postCode>
									<settlement>Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amit</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92521</postCode>
									<settlement>Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Stochastic Graph Evolution Framework for Robust Multi-target Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6812EF537416D42F58633E88FCD159CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Maintaining the stability of tracks on multiple targets in video over extended time periods remains a challenging problem. A few methods which have recently shown encouraging results in this direction rely on learning context models or the availability of training data. However, this may not be feasible in many application scenarios. Moreover, tracking methods should be able to work across different scenarios (e.g. multiple resolutions of the video) making such context models hard to obtain. In this paper, we consider the problem of long-term tracking in video in application domains where context information is not available a priori, nor can it be learned online. We build our solution on the hypothesis that most existing trackers can obtain reasonable short-term tracks (tracklets). By analyzing the statistical properties of these tracklets, we develop associations between them so as to come up with longer tracks. This is achieved through a stochastic graph evolution step that considers the statistical properties of individual tracklets, as well as the statistics of the targets along each proposed long-term track. On multiple real-life video sequences spanning low and high resolution data, we show the ability to accurately track over extended time periods (results are shown on many minutes of continuous video).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multiple object tracking is the most fundamental task for higher level automated video content analysis. Although a large number of trackers exist, stable, longterm tracking is still a challenging problem. Common reasons which cause tracking failure are occlusion, illumination change, clutter and sensor noise. Moreover, for multiple targets, we have to consider the interaction between the targets which may cause errors like switching between tracks, missed detections and false detections. Therefore, detection and correction of the errors in the tracks is the key to robust long term tracking.</p><p>Many state-of-the-art tracking algorithms focus on how to avoid losing track. They usually rely on training data or learning context models (e.g. some recent papers like <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>). In many situations, there may not be enough data for training or learning context models. For example, videos downloaded from Youtube are usually a few minutes in length and from a variety of contexts. Analysis of these videos requires tracking and there is no separate data available to learn models.</p><p>In this paper, we consider the problem of long-term tracking in video in application domains where context information is not available a priori, nor can it be learned online. We are not proposing our method as an alternative to learning models, rather as an approach for applications where such data is not available. Building on the hypothesis that most existing trackers can obtain reasonable short-term tracks (tracklets), we propose a stochastic graph evolution framework to understand the association between tracklets so as to come up with longer tracks by analyzing the statistical properties of individual tracklets, as well as the statistics of the targets along each proposed long-term track.</p><p>Our approach is original in the following ways.</p><p>-We come up with a measure of the accuracy of the tracking, so that we can determine when the tracking error is increasing and identify the tracklets. -We propose a prediction-based affinity modeling approach by searching for optimal associations in the target feature space using a stochastic sampling method. We show that this provides higher accuracy as opposed to heuristically selecting a fixed affinity model. This process leads to a weighted graph with the tracklets as nodes and affinity scores as weights. -We consider long-term interdependencies between the target tracklet features and use it to correct for wrong correspondences. This is achieved by evolving the graph weights through a stochastic sampling approach. The underlying hypothesis for this step is that along a correct track the variation of the target features will be lower than along a wrong track.</p><p>Through this process, we are able to get stable long-term tracks of multiple targets without the need for extra training data. Our method analyzes the video in a time-window (maximum duration of a few minutes) in a batch process; thus there is a delay in the analysis, which is often a non-issue in many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>To track multiple objects, a lot of effort has been devoted to making data association based on the results of object detection. Multi-Hypothesis Tracking (MHT) <ref type="bibr" target="#b12">[13]</ref> and Joint Probabilistic Data Association Filters (JPDAF) <ref type="bibr" target="#b1">[2]</ref> are two representative methods. In order to overcome the large computational cost of MHT and JPDAF, various optimization algorithms such as Linear Programming <ref type="bibr" target="#b8">[9]</ref>, Quadratic Boolean Programming <ref type="bibr" target="#b9">[10]</ref>, and Hungarian algorithm <ref type="bibr" target="#b11">[12]</ref> are used for data association. In <ref type="bibr" target="#b16">[17]</ref>, data association was achieved through a MCMC sampling based framework. These methods rely on the precision of object detection, which can not be guaranteed in complex scenarios. On the other hand, some statical tracking methods (e.g. Kalman filter and particle filter <ref type="bibr" target="#b7">[8]</ref>) and kernel tracking algorithm (e.g. mean-shift tracker <ref type="bibr" target="#b2">[3]</ref>) release the requirement for object detection in every frame, but they are not powerful for tracking multiple objects by themselves. In <ref type="bibr" target="#b6">[7]</ref>, particle filters were used to track multiple objects by incorporating probabilistic MHT for data association. Many state-of-the-art tracking algorithms focus on how to avoid errors in tracking. In <ref type="bibr" target="#b17">[18]</ref>, the authors proposed a min-cost flow framework for global optimal data association. A tracklet association based tracking method was presented in <ref type="bibr" target="#b4">[5]</ref>, which fixed the affinity model heuristically and focused on searching for optimal associations. A HybridBoosted affinity model was learned in <ref type="bibr" target="#b10">[11]</ref>. The method is built on the availability of training data under a similar environment, which may not be always feasible. The authors in <ref type="bibr" target="#b0">[1]</ref> addressed the problem of learning an adaptive appearance model for object tracking. Context information was considered in <ref type="bibr" target="#b15">[16]</ref> to help in tracking, by integrating a set of auxiliary objects which are learned online. Unfortunately, except for high resolution video, it is not easy to find these auxiliary objects.</p><p>We would like to clearly differentiate our approach with traditional Data Association Tracking (DAT) approaches which perform the tracking by detection instead of running a tracking algorithm. Unlike the DAT methods, our data association is done on the tracking results, not the detection result. Moreover, in most methods, there is very little attention paid on error recovery, i.e., if errors happen, how to detect and correct them. It is, however, at the heart of the proposed strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of Solution Strategy</head><p>Our system is initialized when new targets are detected. A basic tracker using particle filter is applied to generate the initial tracks. It can be replaced by any existing tracker, without affecting the other modules. However, errors cannot be avoided in the tracks generated by the basic trackers, especially in the presence of occlusions, disappearance of targets and close proximity of targets. In order to correct the errors, we propose a stochastic tracklet association and adaptation strategy.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows an overview of our long-term tracking system. We begin by identifying tracklets, i.e., the short-term fragments with low probability of error, which are estimated from the initial tracks by evaluating the tracking performance. Details on estimation of tracklets are provided in Section 3.</p><p>The tracklets are then associated based on their affinities. Although an optimal affinity model could be learned <ref type="bibr" target="#b10">[11]</ref>, it requires the availability of training data. Instead of using a heuristically selected fixed affinity model, we propose a prediction based affinity modeling approach by searching for optimal predictions in the feature space based on Markov chain Monte Carlo (MCMC) sampling methods as detailed in Section 4. The tracklets are first extended in space and time through new predicted positions generated using the Metropolis Hastings algorithm. The affinity between two tracklets is modeled by the distance (in a suitable feature space) of the predicted ending of one tracklet to the starting of another. Using the affinity model, we create a tracklet association graph (TAG) with the tracklets as nodes and affinity scores as weights. The association of the tracklets can be found by computing the optimal paths in the graph. The optimal path computation is based on the principles of dynamic programming and gives the maximum a posteriori (MAP) estimate of tracklets' connections as the long-term tracks for each target. This is explained in Section 4.1.</p><p>The tracking problem could be solved optimally by the above tracklet association method if the affinity scores were known exactly and assumed to be independent. However, this can be a big assumption due to well known low-level image processing challenges, like poor lighting conditions or unexpected motion of the targets. The prediction based affinity model may not be enough to capture the variation. This leads us to develop a graph evolution scheme as described in Section 5. The affinities (i.e., the weights on the edges of TAG) are stochastically adapted by considering the distribution of the features along possible paths in the association graph to search for the global optimum. We design a loss function and an efficient optimization strategy for this process. The overall approach is able to track stably over minutes of video in challenging domains with no learning and context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tracklet Identification</head><p>As mentioned earlier, we identify the tracklets from the initial tracks generated from the basic tracker. Then the problem of tracking over long-term video is equivalent to finding the best association between the tracklets. Note that although the particle filter based basic tracker is replaceable, it was chosen because the observation model is nonlinear and the posterior can temporarily become multimodal due to background clutter. We now describe our implementation of the basic tracker using a particle filter and the tracklet estimation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Particle Filter Based Basic Tracker</head><p>Initialization: We use motion detection to automatically detect moving objects. The background modeling algorithm in <ref type="bibr" target="#b14">[15]</ref> is used for its adaptability to illumination change, and to learn the multimodal background through time. Using the learned background model, the moving objects can be detected. However, the background model may not be precise due to noise, which could produce false detections. By observing that most of our interested targets, like people and vehicles, are on ground plane, we estimate the rough ground plane area using the method proposed in <ref type="bibr" target="#b5">[6]</ref>. Based on the ground plane information, false alarms can be removed significantly. We reiterate that this process is just one choice based on the current literature. It can be replaced and we do not assume that this step should work perfectly. In fact, the following stages are designed to correct for the errors here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System model:</head><p>The target regions are represented by rectangles with the state vector X t = [x, y, ẋ, ẏ, l x , l y ], where (x, y) and ( ẋ, ẏ) are the position and velocity of a target in the x and y directions respectively, and (l x , l y ) denote the size of the rectangle. We consider a linear dynamic model:</p><formula xml:id="formula_0">X t = AX t-1 + n t ,</formula><p>where A defines the deterministic system model and n t is zero mean white Gaussian noise (n t ∼ N (0, Σ t )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation model:</head><p>The observation process is defined by the likelihood distribution, p(I t |X t ), where X t is the state vector and I t is the image observation at t. Our observation models were generated by combining an appearance and a foreground response model, i.e.,</p><formula xml:id="formula_1">p(I t |X t ) = p(I a t , I f t |X t ),<label>(1)</label></formula><p>where I a t is the appearance information of I t and I f t is the foreground response of I t using the learned background model as described above. I f t is a binary image with "1" for foreground and "0" for background.</p><p>It is reasonable to assume that I a t and I f t are independent and thus (1) becomes</p><formula xml:id="formula_2">p(I t |X t ) = p(I a t |X t )p(I f t |X t ).</formula><p>The appearance observation likelihood is defined as p(I a t |X t ) ∝ exp{-B(ch(X t ), ch 0 ) 2 }, where ch(X t ) is the color histogram associated with the rectangle region of X t and ch 0 is color histogram of the initialized target. B(.) is the Bhattachayya distance between two color histograms. The foreground response observation likelihood is p(</p><formula xml:id="formula_3">I f t |X t ) ∝ exp{-(1-#F (Xt) #Xt ) 2 }</formula><p>, where #F (X t ) is the number of foreground pixels in the rectangular region of X t and #X t is the total number of pixels in that rectangle. #F (Xt) #Xt represents the percentage of the foreground in that rectangle. The observation likelihood would be higher if more pixels in the rectangular region of X t belong to the foreground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tracklet Estimation</head><p>Errors cannot be avoided in the tracks generated by any basic tracker. There are two common errors: lost track (when the track is no longer on any target, but on the background) and track switching (when targets are close and the tracks are on the wrong target). This leads us to the rules for tracklet estimation. We estimate when these errors happen and identify their spatio-temporal location, leading to the tracklets. An example is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection of lost track:</head><p>The tracking error (TE) <ref type="bibr" target="#b1">[2]</ref> or prediction error is the distance between the current observation and its prediction based on past observations. TE will increase when the tracker loses track and can be used to detect the unreliability of the track result. In our observation model, TE of tracked target Xt is calculated by</p><formula xml:id="formula_4">T E( Xt , I t ) = T E a ( Xt , I t ) + T E f ( Xt , I t ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">T E a ( Xt , I t ) =B(ch(X t ), ch 0 ) 2 and T E f ( Xt , I t ) = 1 - #F (X t ) #X t 2 .</formula><p>If a lost track is detected, it means the tracking result after this point is not reliable; in the tracking procedure, we stop doing tracking after this point and identify a tracklet. In the case of false detection (i.e., the detected target is a part of background), or target passes through a region with similar color, or a target stops, the background modeling algorithm will adapt to treat this as a part of the background, and thus T E f will eventually increase. Then a lost track will be detected.</p><p>Track Switching: When targets are close to each other, a track switch can happen with high probability especially if the appearances of targets are similar. Thus, we inspect the distances between targets, and break the tracks into tracklets at the points where targets are getting close, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prediction Based Tracklet Affinity Modeling</head><p>As mentioned in <ref type="bibr" target="#b10">[11]</ref>, in most previous work, simple affinity models are used by heuristically selecting parameters. The approach in <ref type="bibr" target="#b10">[11]</ref> is able to automatically select among features and corresponding non-parametric models based on training data. However, without the availability of training data, searching in such an affinity function space is not trivial. Under this condition, rather than directly search in the affinity function space, we propose a prediction based affinity modeling approach by searching for optimal predictions in the feature space based on MCMC sampling methods and using the predicted features to come up with the affinity measurements. This provides more robustness compared to using a fixed affinity measure, as shown in Table <ref type="table" target="#tab_3">3</ref> in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tracklet Prediction and Association</head><p>The tracklet occurring earlier in time is referred to as the base-tracklet, while a tracklet beginning after the base-tracklet ended is referred to as the targettracklet. In order to measure tracklet affinity, the base-tracklet is extended in the image motion/appearance-space M steps, where M could represent the number of frames that separate the end of the base-tracklet from the beginning of the target-tracklet or a fixed number of pre-determined steps. In order to choose new points for the base-tracklet, a form of MCMC called the Metropolis Hastings Algorithm is used to generate chains of random samples. MCMC is a versatile tool for generating random samples that can be used in determining statistical estimates. By using this sampling method, the algorithm is able to take advantage of the base object's motion and appearance information while also considering its relationship to the target-tracklet via the target distribution p tl (z). The target distribution relates points surrounding the starting point of the target-track to a probability measure. MCMC has the advantage of not requiring perfect knowledge of the target distribution p tl (z) -it is enough to be able to evaluate it a particular point, but not sample from it. The Proposal Distribution: The proposal distribution q tl (y|z) allows us to generate samples from a distribution that is easy to sample from. Our proposal distribution was based on a combination of motion and appearance of each target. The direction of motion of each target is modeled using the von Mises distribution. The von Mises distribution has close ties to the normal distribution, however it is limited to angles about the unit circle as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The pdf for the von Mises distribution takes the following form:</p><formula xml:id="formula_6">v(θ|μ θ , κ) = e κ cos(θ-μ θ ) 2πI 0 (κ) . (<label>3</label></formula><formula xml:id="formula_7">)</formula><p>Here, I 0 (.) is the modified Bessel function of order zero. The parameters μ θ and κ correspond to mean and variance in a normal distribution, which are learned within each base tracklet. The speed of each target is modeled with a Normal distribution N (μ D , σ D ), where the mean μ D and variance σ D are learned within each base tracklet. The appearance model is described using a normal distribution on the color histogram of each target as N (μ A , Σ A ), where the parameters are also learned within each base tracklet.</p><p>So our proposal distribution is</p><formula xml:id="formula_8">q tl (w|x) ∝ v(Θ(w -x)|μ θ , κ)N (D(w -x)|μ D , σ D )N (A(w)|μ A , Σ A ),<label>(4)</label></formula><p>where Θ(wx) and D(wx) represent the angle and distance between the proposed point w and the end point of tracklet x respectively, and A(w) represents the color histogram of proposed point w. A new point is proposed by randomly producing motion direction, speed and appearance vector as shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>The Target Distribution: Proposed points from the base-tracklet were related to the starting point of the target-tracklet through the target distribution. The target distribution, p tl (z), was chosen as</p><formula xml:id="formula_9">p tl (z) ∝ e -dz ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_10">d z = d 2 a + d 2</formula><p>m is a Euclidean combination of the normalized distance in the motion-space, d m , and the Bhattacharyya distance, d a , between the image histograms of the average base and target appearances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-H Algorithm:</head><p>Given the proposal distribution, q tl (w|x), where w was the proposed point and x was the last point in the tracklet and the target distribution p tl (w), the probability that a point was accepted was given as, ρ tl (x, w) = min p tl (w)q tl (x|w) p tl (x)q tl (w|x) , 1 .</p><p>This process results in a sequence of accepted points for M time steps. The affinity between a base and target tracklet is computed as the distance d z in (5) between the end of the predicted extension of the base tracklet and the beginning of the target tracklet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracklet Association</head><p>We can now define a Tracklet Association Graph where the nodes are the identified tracklets and the weights on the edges are the affinity scores. By splitting the beginning and end of each tracklet into two subsets, the problem of the tracklet association can be formulated as a maximum matching problem in a weighted bipartite graph. In this paper, we use the Hungarian algorithm <ref type="bibr" target="#b11">[12]</ref> to find the maximum matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tracklet Adaptation</head><p>If the affinity scores (edge weights) of the bipartite graph were known exactly and assumed to be independent, the tracking problem could be solved optimally by the tracklet association method described above. However, it is not uncommon for some of the similarities to be estimated wrongly since they depend on detected features which is not a perfect process. As we show in Fig. <ref type="figure" target="#fig_3">4</ref>, if the similarity estimation is incorrect for one pair of tracklets, the overall inferred long track may be wrong even if all the other tracklets are connected correctly. We address this issue by constructing a graph evolution strategy, in which the weights (i.e., affinity scores) on the edges of the tracklet association graph are adapted by measuring the similarity of observed features along a path that is generated after tracklet association. We adopt the affinity adaptation method proposed in <ref type="bibr" target="#b13">[14]</ref>, but instead of adapting deterministically which may be stuck at a local optimum, we propose a Metropolis-Hastings based adaptation scheme with the potential to reach the global optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tracklet Association Cost Function</head><p>To model the spatio-temporal variation of the observed features along a path, a Tracklet Association Cost (TAC) is defined motivated by <ref type="bibr" target="#b13">[14]</ref>. Given an estimated track for the q th target, λ q , TAC is defined on each edge e ij ∈ λ q . The feature vector of the tracklets before (in time) e ij on λ q and those after e ij are treated as two clusters. An illustration of TAC calculation is shown in Fig. <ref type="figure" target="#fig_3">4 (a)</ref>.</p><p>Let {X} be the set of feature (e.g., appearance) of all N tracklets along the path and let them be clustered into {X (1) } and {X (2) } with respect to each edge e ij ∈ λ q . Let the mean m of the features in {X} be m = 1</p><formula xml:id="formula_12">N x∈{X} x. Let m i be the mean of N i data points of class {X (i) }, i = 1, 2, such that m i = 1 Ni x∈{X (i) } x.</formula><p>Let S T be the variance of the all observed feature x along the path, i.e., S T = x∈{X} |x -m| 2 and S W be the sum of the variances along each sub-path, {X (1) } and {X (2) </p><formula xml:id="formula_13">}, i.e., S W = 2 i=1 S i = 2 i=1 x∈{X (i) } |x -m i | 2 .</formula><p>The TAC for e ij is defined as</p><formula xml:id="formula_14">T AC(e ij ) = |S T -S W | |S W | |S B | |S W | . (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>Thus the TAC is defined from Fisher's linear discriminant function <ref type="bibr" target="#b3">[4]</ref> and measures the ratio of the distance between different clusters, S B , over the distances between the members within each cluster S W . If all the feature nodes along a path belong to the same target, the value of TAC at each edge e ij ∈ λ q should be low, and thus the variance of TAC over all the edges along the path should also be low. If the feature nodes belonging to different people are connected wrongly, we will get a higher value of TAC at the wrong link, and the variance of TAC along the path will be higher. Thus, the distribution of TAC along a path can be used to detect if there is a wrong connection along that path.</p><p>We can now design a loss function for determining the final tracks by analyzing features along a path. We specify the function in terms of the Tracklet Association Cost (TAC) function. Thus, we adapt the affinity scores to minimize</p><formula xml:id="formula_16">L(λ q ) = λq V ar(T AC(e ij ∈ λ (n) q )).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metropolis-Hastings Based Adaptation of Tracklet Association</head><p>Whenever there is a peak<ref type="foot" target="#foot_0">1</ref> in the TAC function for some edge along a path, the validity of the connections between the features along that path is under doubt. As per the Metropolis-Hastings method, we will propose a new candidate affinity score s ij on this edge where the peak occurs using a proposal distribution q af (s ij |s ij ), where s ij is the affinity score on edge e ij . The proposal distribution q af (s ij |s ij ) is chosen to be an uniform distribution of width 2δ, i.e., U (s ijδ, s ij + δ), since without additional information, uniform distribution can be a reasonable guess of the new weights. Any other distribution can be chosen based on the application. We then recalculate the maximum matching paths, λ q , of the new feature graph. The target probability p af (.) is defined as p af (s ij ) ∝ exp(-L(λ q )), and p af (s ij ) ∝ exp(-L(λ q )). The candidate weight s ij is accepted with probability ρ af (s ij , s ij ) as</p><formula xml:id="formula_17">ρ af (s ij , s ij ) = min p af (s ij )q af (s ij |s ij ) p af (s ij )q af (s ij |s ij ) , 1 . (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>Our adaptation scheme is summarized below.</p><p>1. Construct a weighted graph G = (V, E, S), where the vertices are the tracklets and edge weights are set as described in Section 4. 2. Estimate the optimal paths, λq based on bipartite graph matching. 3. Compute the TAC for each e ij ∈ λq . 4. Propose a weight s ij on the link where the TAC peak occurs based on a proposal distribution. 5. Recalculate the maximum matching paths, λ q , of the new feature graph. We accept the new graph with probability ρ af (s ij , s ij ) in ( <ref type="formula" target="#formula_17">9</ref>). 6. Repeat Steps 4 and 5 until either a predefined iteration number is reached or the system reaches some predefined stopping criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>To evaluate the performance of our system, we show results on two different data sets. The CAVIAR (http:// homepages.inf.ed.ac.uk/rbf/CAVIARDATA1) is captured in a shopping mall corridor with heavy inter-object occlusion. The Videoweb dataset (http://vwdata.ee.ucr.edu) is a wide area multi-camera dataset consisting of low and high resolution videos. We consider two subsets of videos. The first is a outdoor low resolution parking lot scene, and the second is a relatively high resolution courtyard scene with intensive occlusion and clutter.</p><p>To evaluate the performance of our system quantitatively, we adopt the evaluation metrics for tracking defined in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b17">[18]</ref>. In addition, we define RS and RL to evaluate the ability of recovering from occlusion (see Table <ref type="table" target="#tab_0">1</ref>). Although we show results on datasets that others have worked with, it should be noted that we are not proposing our method as an alternative to those that use/learn context models, rather as an approach to be used when such models are not available. Therefore, our results should be analyzed with the ground truth, rather than against those that rely on such knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on CAVIAR dataset:</head><p>In CAVIAR dataset, the inter-object occlusion is high and includes long term partial occlusion and full occlusion. Moreover, frequent interactions between targets such as multiple people talking and walking in a group make tracking more challenging. We show our results on the relatively more challenging part of the dataset which contains 7 videos (TwoEnterShop3, TwoEnterShop2, ThreePastShop2, ThreePastShop1, TwoEnterShop1, OneSho-pOneWait1, OneStopMoveEnter1)<ref type="foot" target="#foot_2">2</ref> . Table <ref type="table" target="#tab_1">2</ref> shows the comparison among the proposed method, the min-cost flow approach in <ref type="bibr" target="#b17">[18]</ref>, HybridBoosted affinity  <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b17">[18]</ref> are reported on 20 sequences; basic particle filter and proposed method are reported on 7 most challenging sequences of the dataset. Our test data has totally 12308 frames for about 500 sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT MT ML FG IDS RS RL</head><p>Zhang et al. <ref type="bibr" target="#b17">[18]</ref>   modeling approach in <ref type="bibr" target="#b10">[11]</ref> and a basic particle filter. The results in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> are reported on 20 sequences in CAVIAR. It can be seen that our method achieves similar performance as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>. It should also be noted that <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> are built on the availability of training data under similar environment (e.g. other 6 sequences in CAVIAR are used for training in <ref type="bibr" target="#b17">[18]</ref>), while our method does not rely on any training; also our results are for the most challenging sub-part of the dataset. Some sample frames with results are shown in Fig. <ref type="figure" target="#fig_5">5</ref> (a). In the supplementary material, we show results on continuously tracking this data.</p><p>In order to show the achievement of each step (i.e., the prediction based affinity modeling and tracklet adaptation) of our proposed method, we compare the performances of the basic particle filter, a simple affinity model followed by bipartite graph match, prediction based affinity model without tacklet adaptation step, and the complete proposed approach on one of the sequences (the one shown in the supplementary material). The simple affinity model is constructed by directly using the average angle and speed of motion and average color histogram similar to <ref type="bibr" target="#b17">[18]</ref>. It is clearly shown in Table <ref type="table" target="#tab_3">3</ref> that our method has much less Fragments (FG) and ID Switches (IDS) and the adaptation part can further correct the wrong connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Videoweb dataset -Low-resolution Example:</head><p>The first part of Videoweb dataset we use is a low resolution parking lot scene. The target categories include people, cars and motorcycle (any object which is below 15 pixels in width is not taken into account). The low resolution makes tracking more challenging, especially in outdoor scenes since the illumination is always unstable and the appearance is hard to extract. The results of our methods are shown in Table <ref type="table" target="#tab_4">4</ref>. Some sample frames and tracking results are shown in 5 (b).    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Videoweb dataset -High Occlusion and Clutter Example:</head><p>The second part of Videoweb dataset consists of multiple people interacting in a courtyard. It is almost impossible to track with a basic tracker because of very high occlusion. Also, an adaptive background model is hard to build for this level of occlusion. The tracking result shows our method using the proposed strategy can get reasonable results even at this level of occlusion. The performance on this dataset is shown in Table <ref type="table" target="#tab_5">5</ref>. Some sample frames with tracking results are shown in 5 (c). Results on tracking about 45 seconds of this scene are shown in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we considered the problem of long-term tracking in video in application domains where context information is not available a priori, nor can it be learned online. We built our solution on the hypothesis that most existing trackers can obtain reasonable short-term tracks (tracklets). We then developed associations between them so as to come up with longer tracks. Finally, we proposed a graph evolution method to search for optimal association, then providing robustness to inaccuracies in feature similarity estimation. Promising results are shown on challenging data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of proposed approach</figDesc><graphic coords="3,66.54,59.20,293.03,116.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of tracklet identification. The ground truth trajectories are represented by brown dotted lines. The estimated tracklets due to detection of a lost track (track of the person in lower left corner due to occlusion) and targets' close proximity (the persons moving around the cars) are clearly shown in different colors.</figDesc><graphic coords="6,124.72,60.21,182.50,123.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>D~Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustration of proposing a new point based on the proposal distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Tracklets of two targets obtained from Videoweb courtyard dataset of Section 6: ground truth track of the person in green T-shirt is shown with orange line, and the association results before adaptation are shown with blue line. (b)-(c): TAC values along the incorrect and correct association results respectively, (note that the range of the y-axis in (c) is much smaller than (b)). It is clear that TAC has a peak at the wrong link; thus the variance of TAC along the wrongly associated tracklets is higher than the correct one.</figDesc><graphic coords="9,47.91,56.93,117.18,90.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) CAVIAR scene (b) Videoweb: Low Resolution (c) Videoweb: High Occlusion and Clutter</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a): Tracking results on CAVIAR dataset. (b): Tracking results on Videoweb dataset -low resolution parking lot scene. (c): Tracking results on Videoweb datasethigh clutter and occlusion courtyard scene.</figDesc><graphic coords="13,45.62,412.65,340.27,78.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation metrics</figDesc><table><row><cell cols="2">Name Definition</cell></row><row><cell>GT</cell><cell>Num of ground truth trajectories</cell></row><row><cell>MT%</cell><cell>Mostly tracked: Percentage of GT trajectories which are covered by tracker output more</cell></row><row><cell></cell><cell>than 80% in time</cell></row><row><cell>ML%</cell><cell>Mostly lost: Percentage of GT trajectories which are covered by tracker output less than</cell></row><row><cell></cell><cell>20% in time</cell></row><row><cell>FG</cell><cell>Fragments: The total Num of times that the ID of a target changed along a GT trajectory</cell></row><row><cell>IDS</cell><cell>ID switches: The total Num of times that a tracked target changes its ID with another target</cell></row><row><cell>RS%</cell><cell>Recover from short term occlusion</cell></row><row><cell>RL%</cell><cell>Recover from long term occlusion</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Tracking Results on CAVIAR data set. Results of</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Tracking Results on one sequence of CAVIAR dataset. Proposed approach is a combination of basic particle filter, prediction based affinity model and track adaptation.</figDesc><table><row><cell></cell><cell>GT MT</cell><cell cols="2">ML FG IDS RS RL</cell></row><row><cell>Basic particle fitler</cell><cell cols="2">18 44.4% 22.2% 7</cell><cell>6 4/14 0/5</cell></row><row><cell>Simple Affinity model</cell><cell cols="2">18 66.6% 5.6% 2</cell><cell>4 12/14 2/5</cell></row><row><cell cols="3">Prediction-Based Affinity model 18 72.2% 0.0% 2</cell><cell>3 13/14 3/5</cell></row><row><cell>Proposed method</cell><cell cols="2">18 83.3% 0.0% 2</cell><cell>1 13/14 4/5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Tracking Results on parking lot scene of Videoweb dataset. 4 sequences of totally 14673 frames (980 sec.) were used. GT MT ML FG IDS RS RL Basic particle filter 90 80% 10.0% 20 6 5/19 1/8</figDesc><table><row><cell>Proposed method 90 90% 4.4% 8</cell><cell>3 15/19 5/8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Tracking Results on courtyard scene of Videoweb dataset, 4 sequences of totally 8254 frames (550 sec.) were used.</figDesc><table><row><cell>GT MT</cell><cell cols="2">ML FG IDS RS</cell><cell>RL</cell></row><row><cell cols="4">Basic particle fitler 48 41.7% 14.6% 9 17 10/35 2/15</cell></row><row><cell cols="2">Proposed method 48 66.7% 6.25% 5</cell><cell cols="2">8 29/35 12/15</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The peak is detected if it is above a threshold, which is defined as E{T AC(eij ∈ λq)} +</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>V ar(T AC(eij ∈ λq)).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Compared with other sequences in CAVIAR (e.g. TwoLeaveShop2, OneStopNoEn-ter1 and OneStopMoveNoEnter1), the challenge of the set we test on is obvious.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>More extensive tracking results are available on the author's webpage.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by NSF grant IIS-0712253 and subcontract from Mayachitra Inc., through a DARPA STTR award (#W31P4Q-08-C-0464).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual Tracking with Online Multiple Instance Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tracking and Data Association</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bar-Shalom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fortmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Academic Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kernel-based Object Tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Pattern Classification. Wiley Interscience</publisher>
			<pubPlace>Hoboken</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-target Data Association by Tracklets with Unsupervised Parameter Estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric Context from a Single Image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequential Monte Carlo Methods for Multiple Target Tracking and Data Fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Cadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Condensation -Conditional Density Propagation for Visual Tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Linear Programming Approach for Multiple Object Tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coupled Detection and Trajectory Estimation for Multi-Object Tracking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to Associate: HybridBoosted Multi-Target Tracker for Crowded Scene</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-Object Tracking Through Simultaneous Long Occlusions and Split-Merge Conditions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brooksby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Algorithm for Tracking Multiple Targets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automatic Control</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="843" to="854" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic Adaptive Tracking in a Camera Network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive Background Mixture Models for Real-time Tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context-Aware Visual Tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple Target Tracking Using Spatio-Temporal Markov Chain Monte Carlo Data Association</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global Data Association for Multi-Object Tracking Using Network Flows</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
