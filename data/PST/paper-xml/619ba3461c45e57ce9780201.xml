<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Physics-informed machine learning</title>
				<funder ref="#_kuCPR6y">
					<orgName type="full">DOE</orgName>
				</funder>
				<funder ref="#_Jxfkk3F">
					<orgName type="full">US Department of Energy</orgName>
				</funder>
				<funder ref="#_68tJdCu">
					<orgName type="full">OSD/AFOSR MURI</orgName>
				</funder>
				<funder ref="#_TwkWmh6">
					<orgName type="full">DOE-ARPA</orgName>
				</funder>
				<funder>
					<orgName type="full">UCSB</orgName>
				</funder>
				<funder ref="#_MtsjZfz">
					<orgName type="full">AFOSR</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_w8KeTp4">
					<orgName type="full">DARPA PAI programme</orgName>
				</funder>
				<funder ref="#_sQMSrdP">
					<orgName type="full">Air Force Office of Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
							<email>george_karniadakis@brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Division of Applied Mathematics</orgName>
								<orgName type="institution">Brown University Providence</orgName>
								<address>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Brown University Providence</orgName>
								<address>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Chemical and Biomolecular Engineering</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Applied Mathematics and Statistics</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Lu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Mechanical Engineering and Applied Mechanics</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sifan</forename><surname>Wang</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Graduate Group in Applied Mathematics and Computational Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Applied Mathematics</orgName>
								<orgName type="institution">Brown University Providence</orgName>
								<address>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Physics-informed machine learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1038/s42254-021-00314-5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Observational bias Inductive bias Learning bias Symmetry Conservation laws Dynamics</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modelling and forecasting the dynamics of multiphysics and multiscale systems remains an open scientific problem. Take for instance the Earth system, a uniquely complex system whose dynamics are intricately governed by the interaction of physical, chemical and biological processes taking place on spatiotemporal scales that span 17 orders of magnitude <ref type="bibr" target="#b0">1</ref> . In the past 50 years, there has been tremendous progress in understanding multiscale physics in diverse applications, from geophysics to biophysics, by numerically solving partial differential equations (PDEs) using finite differences, finite elements, spectral and even meshless methods. Despite relentless progress, modelling and predicting the evolution of nonlinear multiscale systems with inhomogeneous cascades-of-scales by using classical analytical or computational tools inevitably faces severe challenges and introduces prohibitive cost and multiple sources of uncertainty. Moreover, solving inverse problems (for inferring material properties in functional materials or discovering missing physics in reactive transport, for example) is often prohibitively expensive and requires complex formulations, new algorithms and elaborate computer codes. Most importantly, solving real-life physical problems with missing, gappy or noisy boundary conditions through traditional approaches is currently impossible. This is where and why observational data play a crucial role. With the prospect of more than a trillion sensors in the next decade, including airborne, seaborne and satellite remote sensing, a wealth of multi-fidelity observations is ready to be explored through data-driven methods. However, despite the volume, velocity and variety of available (collected or generated) data streams, in many real cases it is still not possible to seamlessly incorporate such multi-fidelity data into existing physical models. Mathematical (and practical) data-assimilation efforts have been blossoming; yet the wealth and the spatiotemporal heterogeneity of available data, along with the lack of universally acceptable models, underscores the need for a transformative approach. This is where machine learning (ML) has come into play. It can explore massive design spaces, identify multi-dimensional correlations and manage ill-posed problems. It can, for instance, help to detect climate extremes or statistically predict dynamic variables such as precipitation or vegetation productivity <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3</ref> . Deep learning approaches, in particular, naturally provide tools for automatically extracting features from massive amounts of multi-fidelity observational data that are currently available and characterized by unprecedented spatial and temporal coverage <ref type="bibr" target="#b3">4</ref> . They can also help to link these features with existing approximate models and exploit them in building new predictive tools. Even for biophysical and biomedical modelling, this synergistic integration between ML tools and multiscale and multiphysics models has been recently advocated <ref type="bibr" target="#b4">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0123456789();:</head><p>A common current theme across scientific domains is that the ability to collect and create observational data far outpaces the ability to assimilate it sensibly, let alone understand it 4 (Box 1). Despite their towering empirical promise and some preliminary success <ref type="bibr" target="#b5">6</ref> , most ML approaches currently are unable to extract interpretable information and knowledge from this data deluge. Moreover, purely data-driven models may fit observations very well, but predictions may be physically inconsistent or implausible, owing to extrapolation or observational biases that may lead to poor generalization performance. Therefore, there is a pressing need for integrating fundamental physical laws and domain knowledge by 'teaching' ML models about governing physical rules, which can, in turn, provide 'informative priors'that is, strong theoretical constraints and inductive biases on top of the observational ones. To this end, physics-informed learning is needed, hereby defined as the process by which prior knowledge stemming from our observational, empirical, physical or mathematical understanding of the world can be leveraged to improve the performance of a learning algorithm. A recent example reflecting this new learning philosophy is the family of 'physics-informed neural networks' (PINNs) <ref type="bibr" target="#b6">7</ref> . This is a class of deep learning algorithms that can seamlessly integrate data and abstract mathematical operators, including PDEs with or without missing physics (Boxes 2,3). The leading motivation for developing these algorithms is that such prior knowledge or constraints can yield more interpretable ML methods that remain robust in the presence of imperfect data (such as missing or noisy values, outliers and so on) and can provide accurate and physically consistent predictions, even for extrapolatory/generalization tasks.</p><p>Despite numerous public databases, the volume of useful experimental data for complex physical systems is limited. The specific data-driven approach to the predictive modelling of such systems depends crucially on the amount of data available and on the complexity of the system itself, as illustrated in <ref type="bibr">Box 1.</ref> The classical paradigm is shown on the left side of the figure in <ref type="bibr">Box 1,</ref><ref type="bibr"></ref> where it is assumed that the only data available are the boundary conditions and initial conditions whereas the specific governing PDEs and associated parameters are precisely known. On the other extreme (on the right side of the figure), a lot of data may be available, for instance, in the form of time series, but the governing physical law (the underlying PDE) may not be known at the continuum level <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> . For the majority of real applications, the most interesting category is sketched in the centre of the figure, where it is assumed that the physics is partially known (that is, the conservation law, but not the constitutive relationship) but several scattered measurements (of a primary or auxiliary state) are available that can be used to infer parameters and even missing functional terms in the PDE while simultaneously recovering the solution. It is clear that this middle category is the most general case, and in fact it is representative of the other two categories, if the measurements are too few or too many. This 'mixed' case may lead to much more complex scenarios, where the solution of the PDEs is a stochastic process due to stochastic excitation or an uncertain material property. Hence, stochastic PDEs can be used to represent these stochastic solutions and uncertainties. Finally, there are many problems involving long-range spatiotemporal interactions, such as turbulence, visco-elasto-plastic materials or other anomalous transport processes, where non-local or fractional calculus and fractional PDEs may be the appropriate mathematical language to adequately describe such pheno mena as they exhibit a rich expressivity not unlike that of deep neural networks (DNNs).</p><p>Over the past two decades, efforts to account for uncertainty quantification in computer simulations have led to highly parameterized formulations that may include hundreds of uncertain parameters for complex problems, often rendering such computations infeasible in practice. Typically, computer codes at the national labs and even open-source programs such as OpenFOAM <ref type="bibr" target="#b9">10</ref> or LAMMPS <ref type="bibr" target="#b10">11</ref> have more than 100,000 lines of code, making it almost impossible to maintain and update them from one generation to the next. We believe that it is possible to overcome these fundamental and practical problems using physics-informed learning, seamlessly integrating data and mathematical models, and implementing them using PINNs or other nonlinear regression-based physics-informed networks (PINs) <ref type="bibr">(Box 2)</ref>.</p><p>In this Review, we first describe how to embed physics in ML and how different physics can provide guidance to developing new neural network (NN) architectures. We then present some of the new capabilities of physics-informed learning machines and highlight relevant applications. This is a very fast moving field, so at the end we provide an outlook, including some thoughts on current limitations. A taxonomy of several existing physics-based methods integrated with ML can also be found in ref. <ref type="bibr" target="#b11">12</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to embed physics in ML</head><p>No predictive models can be constructed without assumptions, and, as a consequence, no generalization performance can be expected by ML models without appropriate biases. Specific to physics-informed learning, there are currently three pathways that can be followed separately or in tandem to accelerate training and enhance generalization of ML models by embedding physics in them <ref type="bibr">(Box 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observational biases</head><p>Observational data are perhaps the foundation of the recent success of ML. They are also conceptually the simplest mode of introducing biases in ML. Given sufficient data to cover the input domain of a learning task, ML methods have demonstrated remarkable power in achieving accurate interpolation between the dots, even for high-dimensional tasks. For physical systems in particular, thanks to the rapid development of sensor networks, it is now possible to exploit a wealth of variable fidelity observations and monitor the evolution of complex phenomena across several spatial and temporal scales. These observational data ought to reflect the underlying physical principles that dictate their generation, and, in principle, can be used as a weak mechanism for embedding these principles into an ML model during its training phase. Examples include NNs proposed in refs 13-16 . However, especially for over-parameterized deep learning models, a large volume of data is typically necessary to reinforce these biases and generate predictions that respect certain symmetries and conservation laws. In this case, an immediate difficulty relates to the cost of data acquisition, which for many applications in the physical and engineering sciences could be prohibitively large, as observational data may be generated via expensive experiments or large-scale computational models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inductive biases</head><p>Another school of thought pertains to efforts focused on designing specialized NN architectures that implicitly embed any prior knowledge and inductive biases associated with a given predictive task. Without a doubt, the most celebrated example in this category are convolutional NNs <ref type="bibr" target="#b16">17</ref> , which have revolutionized the field of computer vision by craftily respecting invariance along the groups of symmetries and distributed pattern representations found in natural images <ref type="bibr" target="#b17">18</ref> . Additional representative examples include graph neural networks (GNNs) <ref type="bibr" target="#b18">19</ref> , equivariant networks <ref type="bibr" target="#b19">20</ref> , kernel methods such as Gaussian processes <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> , and more general PINs <ref type="bibr" target="#b26">27</ref> , with kernels that are directly induced by the physical principles that govern a given task. Convolutional networks can be generalized to respect more symmetry groups, including rotations, reflections and more general gauge symmetry transformations <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20</ref> . This enables the development of a very general class of NN architectures on manifolds that depend only on the intrinsic geometry, leading to very effective models for computer vision tasks involving medical images <ref type="bibr" target="#b27">28</ref> , climate pattern segmentation <ref type="bibr" target="#b19">20</ref> and others. Translation-invariant representations can also be constructed via wavelet-based scattering transforms, which are stable to deformations and preserve high-frequency information <ref type="bibr" target="#b28">29</ref> . Another example includes covariant NNs <ref type="bibr" target="#b29">30</ref> , tailored to conform with the rotation and translation invariances present Box 2 | Principles of physics-informed learning making a learning algorithm physics-informed amounts to introducing appropriate observational, inductive or learning biases that can steer the learning process towards identifying physically consistent solutions (see the figure).</p><p>? observational biases can be introduced directly through data that embody the underlying physics or carefully crafted data augmentation procedures. Training a machine learning (ml) system on such data allows it to learn functions, vector fields and operators that reflect the physical structure of the data. ? Inductive biases correspond to prior assumptions that can be incorporated by tailored interventions to an ml model architecture, such that the predictions sought are guaranteed to implicitly satisfy a set of given physical laws, typically expressed in the form of certain mathematical constraints. one would argue that this is the most principled way of making a learning algorithm physics-informed, as it allows for the underlying physical constraints to be strictly satisfied. However, such approaches can be limited to accounting for relatively simple symmetry groups (such as translations, permutations, reflections, rotations and so on) that are known a priori, and may often lead to complex implementations that are difficult to scale. ? learning biases can be introduced by appropriate choice of loss functions, constraints and inference algorithms that can modulate the training phase of an ml model to explicitly favour convergence towards solutions that adhere to the underlying physics. By using and tuning such soft penalty constraints, the underlying physical laws can only be approximately satisfied; however, this provides a very flexible platform for introducing a broad class of physics-based biases that can be expressed in the form of integral, differential or even fractional equations. These different modes of biasing a learning algorithm towards physically consistent solutions are not mutually exclusive and can be effectively combined to yield a very broad class of hybrid approaches for building physics-informed learning machines. 0123456789();: in many-body systems (fig. <ref type="figure" target="#fig_1">1a</ref>). A similar example is the equivariant transformer networks <ref type="bibr" target="#b30">31</ref> , a family of differentiable mappings that improve the robustness of models for predefined continuous transformation groups. Despite their remarkable effectiveness, such approaches are currently limited to tasks that are characterized by relatively simple and well-defined physics or symmetry groups, and often require craftsmanship and elaborate implementations. Moreover, their extension to more complex tasks is challenging, as the underlying invariances or conservation laws that characterize many physical systems are often poorly understood or hard to implicitly encode in a neural architecture.</p><p>Generalized convolutions are not the only building blocks for designing architectures with strong implicit biases. For example, anti-symmetry under the exchange of input variables can be obtained in NNs by using the determinant of a matrix-valued function <ref type="bibr" target="#b31">32</ref> . Reference <ref type="bibr" target="#b32">33</ref> proposed to combine a physics-based model of bond-order potential with an NN and divide structural parameters into local and global parts to predict interatomic potential energy surface in large-scale atomistic modelling. In another work <ref type="bibr" target="#b33">34</ref> , an invariant tensor basis was used to embedded Galilean invariance into the network architecture, which significantly improved the NN prediction accuracy in turbulence modelling. For the problem of identifying Hamiltonian systems, networks are designed to preserve the symplectic structure of the underlying Hamiltonian system <ref type="bibr" target="#b34">35</ref> For example, ref. <ref type="bibr" target="#b35">36</ref> modified an auto-encoder to represent a Koopman operator for identifying coordinate transformations that recast nonlinear dynamics into approximately linear ones.</p><p>Specifically for solving differential equations using NNs, architectures can be modified to satisfy exactly the required initial conditions <ref type="bibr" target="#b36">37</ref> , Dirichlet boundary conditions <ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38</ref> , Neumann boundary conditions <ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40</ref> , Robin boundary conditions <ref type="bibr" target="#b40">41</ref> , periodic boundary conditions <ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43</ref> and interface conditions <ref type="bibr" target="#b40">41</ref> . In addition, if some features of the PDE solutions are known a priori, it is also possible to encode them in network architectures, for example, multiscale features <ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45</ref> , even/odd symmetries and energy conservation <ref type="bibr" target="#b45">46</ref> , high frequencies <ref type="bibr" target="#b46">47</ref> and so on.</p><p>For a specific example, we refer to the recent work in ref. <ref type="bibr" target="#b47">48</ref> , which proposed new connections between NN architectures and viscosity solutions to certain Hamilton-Jacobi PDEs (HJ-PDEs). The two-layer architecture depicted in fig. <ref type="figure" target="#fig_1">1b</ref> </p><formula xml:id="formula_0">defines R R ? f : ?[0, + ?) n as follows ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? f x t t L x u t a ( , ) : = min - + ,<label>(1)</label></formula><formula xml:id="formula_1">i m i i {1,?, }</formula><p>which is reminiscent of the celebrated Lax-oleinik formula.</p><p>Here, x and t are the spatial and temporal variables, L is a convex and Lipschitz activation function, R ? a i and R ? u i n are the NN parameters, and m is the number of neurons. It is shown in ref. <ref type="bibr" target="#b47">48</ref> that f is the viscosity solution to the following HJ-PDE</p><formula xml:id="formula_2">? ? ? ? ? ? ? ? ? R R ? ? ? ? (2) f t x t H f x t x t f x J x x ? ? ( , ) + ( ( , )) = 0 , (0, + ?), ( , 0) = ( ) ,</formula><p>x n n where both the Hamiltonian H and the initial data J are explicitly obtained by the parameters and the activation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lax-Oleinik formula</head><p>A representation formula for the solution of the Hamilton-Jacobi equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Box 3 | Physics-informed neural networks</head><p>Physics-informed neural networks (PInns) <ref type="bibr" target="#b6">7</ref> seamlessly integrate the information from both the measurements and partial differential equations (PDes) by embedding the PDes into the loss function of a neural network using automatic differentiation. The PDes could be integer-order PDes 7 , integro-differential equations <ref type="bibr" target="#b153">154</ref> , fractional PDes <ref type="bibr" target="#b102">103</ref> or stochastic PDes <ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b101">102</ref> .</p><p>Here, we present the PInn algorithm for solving forward problems using the example of the viscous Burgers' equation</p><formula xml:id="formula_3">? ? ? + ? ? = ? ? u t u u x u x 2 2</formula><p>with a suitable initial condition and Dirichlet boundary conditions. In the figure, the left (physics-uninformed) network represents the surrogate of the PDe solution u(x, t), while the right (physics-informed) network describes the PDe residual ? + -</p><formula xml:id="formula_4">? ? ? ? ? ? u u t u x u x 2 2 .</formula><p>The loss function includes a supervised loss of data measurements of u from the initial and boundary conditions and an unsupervised loss of PDe:</p><formula xml:id="formula_5">= + L L L w w ,<label>(3)</label></formula><formula xml:id="formula_6">data data PDE PDE where ? ? ? = - = ? ? + ? ? - ? ? . = = ? ? ? ? ? ? ? ? ? ? ? ? L L | N u x t u N u t u u x u x 1 (<label>( , ) ) and 1</label></formula><formula xml:id="formula_7">i N i i i j N x t data data 1 2 PDE PDE 1 2<label>2 2 ( , ) j j data PDE</label></formula><p>Here {(x i , t i )} and {(x j , t j )} are two sets of points sampled at the initial/boundary locations and in the entire domain, respectively, and u i are values of u at (x i , t i ); w data and w PDe are the weights used to balance the interplay between the two loss terms. These weights can be user-defined or tuned automatically, and play an important role in improving the trainability of PInns <ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b172">173</ref> .</p><p>The network is trained by minimizing the loss via gradient-based optimizers, such as Adam <ref type="bibr" target="#b195">196</ref> and l-BFGS <ref type="bibr" target="#b205">206</ref> , until the loss is smaller than a threshold ?. The PInn algorithm is shown below, and more details about PInns and a recommended Python library DeepXDe can be found in ref. <ref type="bibr" target="#b153">154</ref> .</p><p>Algorithm 1: The PINN algorithm. Construct a neural network (nn) u(x, t; ?) with ? the set of trainable weights w and biases b, and ? denotes a nonlinear activation function. Specify the measurement data {x i , t i , u i } for u and the residual points {x j , t j } for the PDe. Specify the loss L in eq. ( <ref type="formula" target="#formula_5">3</ref>) by summing the weighted losses of the data and PDe. Train the nn to find the best parameters ?* by minimizing the loss L.</p><formula xml:id="formula_8">Done Y N &lt; ?? Loss u t x ? ? ? ? ? ? ? ? PDE (?) NN (x, t; ?) ? ?t ? ?x ? 2 ?x 2 ?u ?t u ?u ?x ? 2 u ?x 2 + ? - nATure revIewS | PhySIcS</formula><p>functions of the networks. The Hamiltonian H must be convex, but the initial data J are not. Note that the results of ref. <ref type="bibr" target="#b47">48</ref> do not rely on universal approximation theorems established for NNs. Rather, the NNs in ref. 48 show that the physics contained in certain classes of HJ-PDEs can be naturally encoded by specific NN architectures without any numerical approximation in high dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning bias</head><p>Yet another school of thought approaches the problem of endowing an NN with prior knowledge from a different angle. Instead of designing a specialized architecture that implicitly enforces this knowledge, current efforts aim to impose such constraints in a soft manner by appropriately penalizing the loss function of conventional NN approximations. This approach can be viewed as a specific use-case of multi-task learning, in which a learning algorithm is simultaneously constrained to fit the observed data, and to yield predictions that approximately satisfy a given set of physical constraints (for example, conservation of mass, momentum, monotonicity and so on). Representative examples include the deep galerkin method 49 and PINNs and their variants <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref> . The framework of PINNs is further explained in <ref type="bibr">Box 3,</ref><ref type="bibr"></ref> as it accurately reflects the key advantages and limitations of enforcing physics via soft penalty constraints.</p><p>The flexibility of soft penalty constraints allows one to incorporate more general instantiations of domain-specific knowledge into ML models. For example, ref. <ref type="bibr" target="#b52">53</ref> presented a statistically constrained generative adversarial network (GAN) by enforcing constraints of covariance from the training data, which results in an improved ML-based emulator to capture the statistics of the training data generated by solving fully   <ref type="bibr" target="#b203">204</ref> . The architecture is based on graph neural networks <ref type="bibr" target="#b18">19</ref> and is constructed by decomposing into a hierarchy of sub-graphs (middle) and forming a neural network in which each 'neuron' corresponds to one of the sub-graphs and receives inputs from other neurons that correspond to smaller sub-graphs (right). The middle panel shows how this can equivalently be thought of as an algorithm in which each vertex receives and aggregates messages from its neighbours. Also depicted on the left are the molecular graphs for C 18 H 9 N 3 OSSe and C 22 H 15 NSeSi from the Harvard Clean Energy Project (HCEP) data set <ref type="bibr" target="#b204">205</ref> with their corresponding adjacency matrices. b | A neural network with the Lax-Oleinik formula represented in the architecture. f is the solution of the Hamilton-Jacobi partial differential equations, x and t are the spatial and temporal variables, L is a convex and Lipschitz activation function, ? R a i and ? R u i n are the neural network parameters, and m is the number of neurons. Panel a is adapted with permission from ref. <ref type="bibr" target="#b203">204</ref> , AIP Publishing. Panel b image courtesy of J. Darbon and T. Meng, Brown University.</p><formula xml:id="formula_9">f 1 f 2 f 3 f 4 n 1 f 5 n 5 f 8 n 8 f 9 n 9 f 10 n 10 n r f 6 n 6 f 7 n 7 n 2 n 3 n 4 Min Pooling f(x,t) x t L L L 1/t Weight: t Bias: a i Weight: 1/t Bias: -u i /t x -u 1 t x -u i t x -u m t x -u 1 t L x -u i t L x -u m t L ..... ..... ..... ..... ..... ..... x -u i t tL + a i x -u 1 t tL + a 1 x -u m t tL + a m N Se</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Galerkin method</head><p>A physics-informed neural network-like method with random sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0123456789();:</head><p>resolved PDEs. Other examples include models tailored to learn contact-induced discontinuities in robotics <ref type="bibr" target="#b53">54</ref> , physics-informed auto-encoders <ref type="bibr" target="#b54">55</ref> , which use an additional soft constraint to preserve the Lyapunov stability, and InvNet 56 , which is capable of encoding invariances by soft constraints in the loss function. Further extensions include convolutional and recurrent architectures, and probabilistic formulations <ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b56">57</ref> . For example, ref. <ref type="bibr" target="#b51">52</ref> includes a Bayesian framework that allows for uncertainty quantification of the predicted quantities of interest in complex PDE dynamical systems.</p><p>Note that solutions obtained via optimization with such soft penalty constraints and regularization can be viewed as equivalent to the maximum a-posteriori estimate of a Bayesian formulation stemming from physics-based likelihood assumptions. Alternatively, a fully Bayesian treatment using Markov chain Monte Carlo methods or variational inference approximations can be used to quantify the uncertainty arising from noisy and gappy data, as discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid approaches</head><p>The aforementioned principles of physics-informed ML have their own advantages and limitations. Hence, it would be ideal to use these different principles together, and indeed different hybrid approaches have been proposed. For example, non-dimensionalization can recover characteristic properties of a system, and thus it is beneficial to introduce physics bias via appropriate non-dimensional parameters, such as Reynolds, Froude or Mach numbers. Several methods have been proposed to learn operators that describe physical phenomena <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59</ref> . For example, DeepONets <ref type="bibr" target="#b12">13</ref> have been demonstrated as a powerful tool to learn nonlinear operators in a supervised data-driven manner. What is more exciting is that by combining DeepONets with physics encoded by PINNs, it is possible to accomplish real-time accurate predictions with extrapolation in multiphysics applications such as electro-convection 60 and hypersonics <ref type="bibr" target="#b60">61</ref> . However, when a low-fidelity model is available, a multi-fidelity strategy <ref type="bibr" target="#b61">62</ref> can be developed to facilitate the learning of a complex system. For example, ref. <ref type="bibr" target="#b62">63</ref> combines observational and learning biases through the use of large-eddy simulation data and constrained NN training methods to construct closures for lower-fidelity Reynolds-averaged Navier-Stokes models of turbulent fluid flow.</p><p>Additional representative use-cases include the multi-fidelity NN used in ref. <ref type="bibr" target="#b63">64</ref> to extract material properties from instrumented indentation data, the PINs in ref. <ref type="bibr" target="#b64">65</ref> used to discover constitutive laws of non-Newtonian fluids from rheological data, and the coarse-graining strategies proposed in ref. <ref type="bibr" target="#b65">66</ref> . Even if it is not possible to encode the low-fidelity model into the learning directly, the low-fidelity model can be used through data augmentation -that is, generating a large amount of low-fidelity data via inexpensive low-fidelity models, which could be simplified mathematical models or existing computer codes, such as ref. <ref type="bibr" target="#b63">64</ref> . Other representative examples include FermiNets <ref type="bibr" target="#b31">32</ref> and graph neural operator methods <ref type="bibr" target="#b57">58</ref> . It is also possible to enforce the physics to an NN by embedding a network into a traditional numerical method (such as finite element). This approach was applied to solve problems in many different fields, including nonlinear dynamical systems <ref type="bibr" target="#b66">67</ref> , computational mechanics to model constitutive relations <ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69</ref> , subsurface mechanics <ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref> , stochastic inversion <ref type="bibr" target="#b72">73</ref> and more <ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connections to kernel methods</head><p>Many of the presented NN-based techniques have a close asymptotic connection to kernel methods, which can be exploited to produce new insight and understanding. For example, as demonstrated in <ref type="bibr">refs 76,77</ref> , the training dynamics of PINNs can be understood as a kernel regression method as the width of the network goes to infinity. More generally, NN methods can be rigorously interpreted as kernel methods in which the underlying warping kernel is also learned from data <ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79</ref> . Warping kernels are a special kind of kernels that were initially introduced to model non-stationary spatial structures in geostatistics <ref type="bibr" target="#b79">80</ref> and have been also used to interpret residual NN models <ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b79">80</ref> . Furthermore, PINNs can be viewed as solving PDEs in a reproducing kernel Hilbert space spanned by a feature map (parametrized by the initial layers of the network), where the latter is also learned from data. Further connections can be made by studying the intimate connection between statistical inference techniques and numerical approximation. Existing works have explored these connections in the context of solving PDEs and inverse problems <ref type="bibr" target="#b80">81</ref> , optimal recovery <ref type="bibr" target="#b81">82</ref> and Bayesian numerical analysis <ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref> . Connections between kernel methods and NNs can be established even for large and complicated architectures, such as attention-based transformers <ref type="bibr" target="#b88">89</ref> , whereas operator-valued kernel methods <ref type="bibr" target="#b89">90</ref> could offer a viable path of analysing and interpreting deep learning tools for learning nonlinear operators. In summary, analysing NN models through the lens of kernel methods could have considerable benefits, as kernel methods are often interpretable and have strong theoretical foundations, which can subsequently help us to understand when and why deep learning methods may fail or succeed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connections to classical numerical methods</head><p>Classical numerical algorithms, such as Runge-Kutta methods and finite-element methods, have been the main workhorses for studying and simulating physical systems in silico. Interestingly, many modern deep learning models can be viewed and analysed by observing an obvious correspondence and specific connections to many of these classical algorithms. In particular, several architectures that have had tremendous success in practice are analogous to established strategies in numerical analysis. Convolutional NNs, for example, are analogous to finite different stencils in translationally equivariant PDE discretizations <ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b91">92</ref> and share the same structures as the multigrid method <ref type="bibr" target="#b92">93</ref> ; residual NNs (ResNets, networks with skip connections) <ref type="bibr" target="#b93">94</ref> are analogous to the basic forward Euler discretization of autonomous ordinary differential equations <ref type="bibr" target="#b94">[95]</ref><ref type="bibr" target="#b95">[96]</ref><ref type="bibr" target="#b96">[97]</ref><ref type="bibr" target="#b97">[98]</ref> ; inspection of simple Runge-Kutta schemes (such as an RK4) immediately brings forth the analogy with recurrent NN architectures (and even with Krylov-type matrix-free</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lyapunov stability</head><p>Characterization of the robustness of dynamic behaviour to small perturbations, in the neighbourhood of an equilibrium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gappy data</head><p>sets with regions of missing data.</p><p>linear algebra methods such as the generalized minimal residual method) <ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b98">99</ref> . Moreover, the representation of DNNs with the reLU activation function is equivalent to the continuous piecewise linear functions from the linear finite-element method <ref type="bibr" target="#b99">100</ref> . Such analogies can provide insights and guidance for cross-fertilization, and pave the way for new 'mathematics-informed' meta-learning architectures. For example, ref. <ref type="bibr" target="#b6">7</ref> proposed a discrete-time NN method for solving PDEs that is inspired by an implicit Runge-Kutta integrator: using up to 500 latent stages, this NN method can allow very large time-steps and lead to solutions of high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merits of physics-informed learning</head><p>There are already many publications on physicsinformed ML across different disciplines for specific applications. For example, different extensions of PINNs cover conservation laws <ref type="bibr" target="#b100">101</ref> as well as stochastic and fractional PDEs for random phenomena and for anomalous transport <ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103</ref> . Combining domain decomposition with PINNs provides more flexibility in multiscale problems, while the formulations are relatively simple to implement in parallel since each subdomain may be represented by a different NN, assigned to a different GPU with very small communication cost <ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b104">105</ref> . Collectively, the results from these works demonstrate that PINNs are particularly effective in solving ill-posed and inverse problems, whereas for forward, well-posed problems that do not require any data assimilation the existing numerical grid-based solvers currently outperform PINNs. In the following, we discuss in more detail for which scenarios the use of PINNs may be advantageous and highlight these advantages in some prototypical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incomplete models and imperfect data</head><p>As shown in Box 1, physics-informed learning can easily combine both information from physics and scattered noisy data, even when both are imperfect. Recent research <ref type="bibr" target="#b105">106</ref> demonstrated that it is possible to find meaningful solutions even when, because of smoothness or regularity inherent in the PINN formulation, the problem is not perfectly well posed. Examples include forward and inverse problems, where no initial or boundary conditions are specified or where some of the parameters in the PDEs are unknown -scenarios in which classical numerical methods may fail. When dealing with imperfect models and data, it is beneficial to integrate the Bayesian approach with physics-informed learning for uncertainty quantification, such as Bayesian PINNs (B-PINNs) <ref type="bibr" target="#b106">107</ref> . Moreover, compared with the traditional numerical methods, physics-informed learning is mesh-free, without computationally expensive mesh generation, and thus can easily handle irregular and moving-domain problems <ref type="bibr" target="#b107">108</ref> . Lastly, the code is also easier to implement by using existing open-source deep learning frameworks such as TensorFlow and PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strong generalization in small data regime</head><p>Deep learning usually requires a large amount of data for training, and in many physical problems it is difficult to obtain the necessary data at high accuracy. In these situations, physics-informed learning has the advantage of strong generalization in the small data regime. By enforcing or embedding physics, deep learning models are effectively constrained on a lower-dimensional manifold, and thus can be trained with a small amount of data. To enforce the physics, one can embed the physical principles into the network architecture, use physics as soft penalty constraints or use data augmentation as discussed previously. In addition, physics-informed learning is capable of extrapolation, not only interpolation: that is, it can perform spatial extrapolation in boundary-value problems <ref type="bibr" target="#b106">107</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Understanding deep learning</head><p>In addition to enhancing the trainability and generalization of ML models, physical principles are also being used to provide theoretical insight and elucidate the inner mechanisms behind the surprising effectiveness of deep learning. For example, in refs <ref type="bibr" target="#b108">[109]</ref><ref type="bibr" target="#b109">[110]</ref><ref type="bibr" target="#b110">[111]</ref><ref type="bibr" target="#b111">[112]</ref> , the authors use the jamming transition of granular media to understand the double-descent phenomenon of deep learning in the over-parameterized regime. Shallow NNs can also be viewed as interacting particle systems and hence can be analysed in the probability measure space with mean-field theory, instead of the high-dimensional parameter space <ref type="bibr" target="#b112">113</ref> .</p><p>Another work <ref type="bibr" target="#b113">114</ref> rigorously constructed an exact mapping from the variational renormalization group to deep learning architectures based on restricted Boltzmann machines. Inspired by the successful density matrix renormalization group algorithm developed in physics, ref. <ref type="bibr" target="#b114">115</ref> proposed a framework for applying quantum-inspired tensor networks to multi-class supervised learning tasks, which introduces considerable savings in computational cost. Reference 116 studied the landscape of deep networks from a statistical physics viewpoint, establishing an intuitive connection between NNs and the spin-glass models. In parallel, information propagation in wide DNNs has been studied based on dynamical systems theory <ref type="bibr" target="#b116">117,</ref><ref type="bibr" target="#b117">118</ref> , providing an analysis of how network initialization determines the propagation of an input signal through the network, hence identifying a set of hyper-parameters and activation functions known as the 'edge of chaos' that ensure information propagation in deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tackling high dimensionality</head><p>Deep learning has been very successful in solving high-dimensional problems, such as image classification with fine resolution, language modelling, and high-dimensional PDEs. One reason for this success is that DNNs can break the curse of dimensionality under the condition that the target function is a hierarchical composition of local functions <ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b119">120</ref> . For example, in ref. <ref type="bibr" target="#b120">121</ref> the authors reformulated general high-dimensional parabolic PDEs using backward stochastic differential equations, approximating the gradient of the solution with DNNs, and then designing the loss based on the discretized stochastic integral and the given terminal condition. In practice, this approach was used to solve high-dimensional Black-Scholes, Hamilton-Jacobi-Bellman and Allen-Cahn equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReLU activation function</head><p>rectified linear unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Double-descent phenomenon</head><p>increasing model capacity beyond the point of interpolation resulting in improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restricted Boltzmann machines</head><p>generative stochastic artificial neural networks that can learn a probability distribution over their set of inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0123456789();:</head><p>GANs <ref type="bibr" target="#b121">122</ref> have also proven to be fairly successful in generating samples from high-dimensional distributions in tasks such as image or text generation <ref type="bibr" target="#b122">[123]</ref><ref type="bibr" target="#b123">[124]</ref><ref type="bibr" target="#b124">[125]</ref> . As for their application to physical problems, in ref. <ref type="bibr" target="#b101">102</ref> the authors used GANs to quantify parametric uncertainty in high-dimensional stochastic differential equations, and in ref. <ref type="bibr" target="#b125">126</ref> GANs were used to learn parameters in high-dimensional stochastic dynamics. These examples show the capability of GANs in modelling high-dimensional probability distributions in physical problems. Finally, in <ref type="bibr">refs 127,128</ref> it was demonstrated that even for operator regression and applications to PDEs, deep operator networks (DeepONets) can tackle the curse of dimensionality associated with the input space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty quantification</head><p>Forecasting reliably the evolution of multiscale and multiphysics systems requires uncertainty quantification. This important issue has received a lot of attention in the past 20 years, augmenting traditional computational methods with stochastic formulations to tackle uncertainty due to the boundary conditions or material properties <ref type="bibr" target="#b128">[129]</ref><ref type="bibr" target="#b129">[130]</ref><ref type="bibr" target="#b130">[131]</ref> . For physics-informed learning models, there are at least three sources of uncertainty: uncertainty due to the physics, uncertainty due to the data, and uncertainty due to the learning models.</p><p>The first source of uncertainty refers to stochastic physical systems, which are usually described by stochastic PDEs (SPDEs) or stochastic ordinary differential equations (SODEs). The parametric uncertainty arising from the randomness of parameters lies in this category. In ref. <ref type="bibr" target="#b131">132</ref> the authors demonstrate the use of NNs as a projection function of the input that can recover a low-dimensional nonlinear manifold, and present results for a problem on uncertainty propagation in an SPDE with uncertain diffusion coefficient. In the same spirit, in ref. <ref type="bibr" target="#b132">133</ref> the authors use a physics-informed loss function -that is, the expectation of the energy functional of the PDE over the stochastic variables -to train an NN parameterizing the solution of an elliptic SPDE. In ref. <ref type="bibr" target="#b50">51</ref> , a conditional convolutional generative model is used to predict the density of a solution, with a physics-informed probabilistic loss function so that no labels are required in the training data. Notably, as a model designed to learn distributions, GANs offer a powerful approach to solving stochastic PDEs in high dimensions. The physics-informed GANs in <ref type="bibr">refs 102,134</ref> represent the first such attempts. Leveraging data collected from simultaneous reads at a limited number of sensors for the multiple stochastic processes, physics-informed GANs are able to solve a wide range of problems ranging from forward to inverse problems using the same framework. Also, the results so far show the capability of GANs, if properly formulated, to tackle the curse of dimensionality for problems with high stochastic dimensionality.</p><p>The second source of uncertainty, in general, refers to aleatoric uncertainty arising from the noise in data and epistemic uncertainty arising from the gaps in data. Such uncertainty can be well tackled in the Bayesian framework. If the physics-informed learning model is based on Gaussian process regression, then it is straightforward to quantify uncertainty and exploit it for active learning and resolution refinement studies in PDEs <ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b134">135</ref> , or even design better experiments <ref type="bibr" target="#b135">136</ref> . Another approach was proposed in ref. <ref type="bibr" target="#b106">107</ref> using B-PINNs. The authors of ref. 107 showed that B-PINNs can provide reasonable uncertainty bounds, which are of the same order as the error and increase as the size of noise in data increases, but how to set the prior for B-PINNs in a systematic way is still an open question.</p><p>The third source of uncertainty refers to the limitation of the learning models -for example, the approximation, training and generalization errors of NNs -and is usually hard to rigorously quantify. In ref. 137 , a convolutional encoder-decoder NN is used to map the source term and the domain geometry of a PDE to the solution as well as the uncertainty, trained by a probabilistic supervised learning procedure with training data coming from finite-element methods. Notably, a first attempt to quantify the combined uncertainty from learning was given in ref. <ref type="bibr" target="#b137">138</ref> , using the dropout method of ref. <ref type="bibr" target="#b138">139</ref> and, due to physical randomness, using arbitrary polynomial chaos. An extension to time-dependent systems and long-time integration was reported in ref. <ref type="bibr" target="#b41">42</ref> : it tackled the parametric uncertainty using dynamic and bi-orthogonal modal decomposition of the stochastic PDE, which are effective methods for long-term integration of stochastic systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications highlights</head><p>In this section, we discuss some of the capabilities of physics-informed learning through diverse applications. Our emphasis is on inverse and ill-posed problems, which are either difficult or impossible to solve with conventional approaches. We also present several ongoing efforts on developing open-source software for scientific ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some examples</head><p>Flow over an espresso cup. In the first example, we discuss how to extract quantitative information on the 3D velocity and pressure fields above an espresso coffee cup <ref type="bibr" target="#b139">140</ref> . The input data is based on a video of temperature gradient (fig. <ref type="figure" target="#fig_3">2</ref>). This is an example of the 'hidden fluid mechanics' introduced in ref. <ref type="bibr" target="#b105">106</ref> . It is an ill-posed inverse problem as no boundary conditions or any other information are provided. Specifically, 3D visualizations obtained using tomographic background-oriented Schlieren (Tomo-BOS) imaging that measures density or temperature are used as input to a PINN, which seamlessly integrates the visualization data and the flow and passive scalar governing equations, to infer the latent quantities. Here, the physical assumption is that of the Boussinesq approximation, which is valid if the density variation is relatively small.</p><p>The PINN uses the space and time coordinates as inputs and infers the velocity and pressure fields; it is trained by minimizing a loss function including a data mismatch of temperature and the residuals of the conservation laws (mass, momentum and energy). Independent experimental results from particle image velocimetry have verified that the Tomo-BOS/PINN approach is able to provide continuous, high-resolution and accurate 3D flow fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aleatoric uncertainty</head><p>Uncertainty due to the inherent randomness of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epistemic uncertainty</head><p>Uncertainty due to limited data and knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arbitrary polynomial chaos</head><p>A type of generalized polynomial chaos with measures defined by data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boussinesq approximation</head><p>An approximation used in gravity-driven flows, which ignores density differences except in the gravity term. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physics-informed deep learning for 4D-flow MRI.</head><p>Next, we discuss the use of PINNs in biophysics using real magnetic resonance imaging (MRI) data. Because it is non-invasive and proves a range of structural and physiological contrasts, MRI has become an indispensable tool for quantitative in-vivo assessment of blood flow and vascular function in clinical scenarios involving patients with cardiac and vascular disease. However, MRI measurements are often limited by the very coarse resolution and may be heavily corrupted by noise, leading to tedious and empirical workflows for reconstructing vascular topologies and associated flow conditions. Recent developments on physics-informed deep learning can greatly enhance the resolution and information content of current MRI technologies, with a focus on 4D-flow MRI. Specifically, it is possible to construct DNNs that are constrained by the Navier-Stokes equations in order to effectively de-noise MRI data and yield physically consistent reconstructions of the underlying velocity and pressure fields that ensure conservation of mass and momentum at an arbitrarily high spatial and temporal resolution. Moreover, the filtered velocity fields can be used to identify regions of no-slip flow, from which one can reconstruct the location and motion of the arterial wall and infer important quantities of interest such as wall shear stresses, kinetic energy and dissipation (fig. <ref type="figure" target="#fig_4">3</ref>). Taken together, these methods can considerably advance the capabilities of MRI technologies in research and clinical scenarios. However, there are potential pitfalls related to the robustness of PINNs, especially in the presence of high signal-to-noise ratio in the MRI measurements and complex patterns in the underlying flow (for example, due to boundary layers, high-vorticity regions, transient turbulent bursts through a stenosis, tortuous branched vessels and so on). That said, under physiological conditions, blood flow is laminar, a regime under which current PINN models usually remain effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncovering edge plasma dynamics via deep learning from partial observations.</head><p>Predicting turbulent transport on the edge of magnetic confinement fusion devices is a longstanding goal spanning several decades, currently presenting significant uncertainties in the particle and energy confinement of fusion power plants. In ref. <ref type="bibr" target="#b140">141</ref>   learn turbulent field dynamics consistent with the twofluid theory from just partial observations of a synthetic plasma, for plasma diagnosis and model validation in challenging thermonuclear environments. figUre <ref type="figure" target="#fig_6">4</ref> displays the turbulent radial electric field learned by PINNs from partial observations of a 3D synthetic plasma's electron density and temperature <ref type="bibr" target="#b140">141</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Studying transitions between metastable states of a distribution.</head><p>Next, we discuss how physics-informed learning can be creatively used to tackle high-dimensional problems. In ref. <ref type="bibr" target="#b141">142</ref> , the authors proposed to use physicsinformed learning to study transitions between two metastable states of a high-dimensional probability distribution. In particular, an NN was used to represent the committor function, trained with a physics-informed loss function defined as the variational formula for the committor function combined with a soft penalty on the boundary conditions. Moreover, adaptive importance sampling was used to sample rare events that dominate the loss function, which reduces the asymptotic variance of the solution and improves generalization. Results for a probability distribution in a 144-dimensional Allen-Cahn type system are illustrated in fig. <ref type="figure" target="#fig_7">5</ref>. Although these computational results suggest that this approach is effective for high-dimensional problems, the application of the method to more complicated systems and the selection of the NN architecture in adapting it to a given system remain challenging.</p><p>Thermodynamically consistent PINNs. The physics regularization generally pursued in PINNs admits an interpretation as a least-squares residual of point evaluations using an NN basis. For hyperbolic problems involving shocks, where point evaluation of the solution is ill-defined, it is natural to consider alternative physics stabilization requiring reduced regularity. The control volume PINN (cvPINN) pursued by ref. <ref type="bibr" target="#b142">143</ref> generalizes traditional finite-volume schemes to deep learning settings. In addition to offering increased accuracy due to reduced regularity requirements, connections to traditional finite-volume schemes allow natural adaptation of total variation diminishing limiters and recovery of entropy solutions. This framework has allowed the estimation of black-box equations of state for shock hydrodynamics models appropriate for materials such as metals. For scenarios such as phase transitions at extreme pressures and temperatures, DNNs provide an ideal means of addressing unknown model form, whereas the finite-volume structure provided by cvPINNs allows enforcement of thermodynamic consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application to quantum chemistry.</head><p>In some other applications, researchers have also used the physics to design specific new architectures together with the principles of physics-informed learning. For example, in ref. <ref type="bibr" target="#b31">32</ref> , a fermionic NN (FermiNet) was proposed for the ab initio calculation of the solution of the many-electron Schr?dinger equation. FermiNet is a hybrid approach for embedding physics. First, to parameterize the wavefunction, the NN has a specialized architecture that obeys Fermi-Dirac statistics: that is, it is anti-symmetric under the exchange of input electron states and the boundary conditions (decay at infinity). Second, the training of FermiNet is also physics-informed: that is, the loss function is set as the variational form of the energy expectation value, with the gradient estimated by the Monte Carlo method. Although the application of NNs leads to eliminating the basis-set extrapolation, which is a common source of error in computational quantum chemistry, the performance of NNs, in general, depends on many factors, including the architectures and optimization algorithms, which require further systematic investigation.</p><p>Application to material sciences. In applications to materials, from the characterization of the material properties to the non-destructive evaluation of their strength, physics-informed learning can play an important role </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Committor function</head><p>A function used to study transitions between metastable states in stochastic systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Allen-Cahn type system</head><p>A type of system with both reaction and diffusion.</p><p>nATure revIewS | PhySIcS as the underlying problems are typically ill-posed and of inverse type. In ref. <ref type="bibr" target="#b143">144</ref> , the authors introduced an optimized PINN trained to identify and precisely characterize a surface breaking crack in a metal plate. The PINN was supervised with realistic ultrasonic surface acoustic wave data acquired at a frequency of 5 MHz and physically informed by the acoustic wave equation, with the unknown wave speed function represented as an NN. A key element in training was the use of adaptive activation functions, which introduced new trainable hyper-parameters and substantially accelerated convergence even in the presence of significant noise in the data. An alternative approach to introducing physics into ML is through a multi-fidelity framework as in ref. <ref type="bibr" target="#b63">64</ref> for extracting mechanical properties of 3D-printed materials via instrumented indentation. By solving the inverse problem of depth-sensing indentation, the authors could determine the elastoplastic properties of 3D-printed titanium and nickel alloys. In this framework, a composite NN consisting of two ResNets was used. One is a lowfidelity ResNet that uses synthetic data (a lot of finiteelement simulations) and the other is a high-fidelity ResNet that uses as input the sparse experimental data and the output of the low-fidelity data. The objective was to discover the nonlinear correlation function between the low-and high-fidelity data, and subsequently predict the modulus of elasticity and yield stress at high fidelity. The results reported in ref. <ref type="bibr" target="#b63">64</ref> show impressive performance of the multi-fidelity framework, reducing the inference error for the yield stress from over 100% with existing techniques to lower than 5% with the multi-fidelity framework.</p><p>Application to molecular simulations. In ref. <ref type="bibr" target="#b144">145</ref> , an NN architecture was proposed to represent the potential energy surfaces for molecular dynamics simulations, where the translational, rotational and permutational symmetry of the molecular system is preserved with proper pre-processing. Such an NN representation could be further improved in deep potential molecular dynamics (DeePMD) <ref type="bibr" target="#b145">146</ref> . With traditional artificially designed potential energy functions replaced by the NN trained with data from ab initio simulations, DeePMD achieves an ab initio level of accuracy at a cost that scales linearly with the system size. In ref. <ref type="bibr" target="#b146">147</ref> , the limit of molecular dynamics simulations was pushed with ab initio accuracy to simulating more than 1-ns-long trajectories of over 100 million atoms per day, using a highly optimized code for DeePMD on the Summit supercomputer. Before this work, molecular dynamics simulations with ab initio accuracy were performed in systems with up to 1 million atoms <ref type="bibr" target="#b146">147,</ref><ref type="bibr" target="#b147">148</ref> .</p><p>Application to geophysics. Physics-informed learning has also been applied to various geophysical inverse problems. The work in ref. <ref type="bibr" target="#b70">71</ref> estimates subsurface properties, such as rock permeability and porosity, from seismic data by coupling NNs with full-waveform inversion, subsurface flow processes and rock physics models. Furthermore, in ref. <ref type="bibr" target="#b148">149</ref> , it was demonstrated that by combining DNNs and numerical PDE solvers as we discussed in the section on hybrid approaches, physics-informed learning is capable of solving a wide class of seismic inversion problems, such as velocity estimation, fault rupture imaging, earthquake location and source-time function retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software</head><p>To implement PINNs efficiently, it is advantageous to build new algorithms based on the current ML libraries, such as TensorFlow 150 , PyTorch 151 , Keras 152 and JAX <ref type="bibr" target="#b152">153</ref> . Several software libraries specifically designed for physics-informed ML have been developed and are contributing to the rapid development of the field (TABLe <ref type="table" target="#tab_1">1</ref>).   <ref type="bibr" target="#b159">160</ref> . Because Python is the dominant programming language for ML, it is more convenient to use Python for physics-informed ML, and thus most of these libraries are written in Python, except the NeuralPDE 158 and ADCME <ref type="bibr" target="#b159">160</ref> , which are written in Julia. All these libraries use the automatic differentiation mechanism provided in other softwares such as TensorFlow <ref type="bibr" target="#b149">150</ref> . Some of these libraries (such as DeepXDE <ref type="bibr" target="#b153">154</ref> and SimNet <ref type="bibr" target="#b154">155</ref> ) can be used as a solver, that is, users only need to define the problem and then the solver will deal with all the underlying details and solve the problem, whereas some (such as SciANN <ref type="bibr" target="#b158">159</ref> and ADCME <ref type="bibr" target="#b159">160</ref> ) only work as a wrapper, meaning they wrap low-level functions of other libraries (such as TensorFlow) into relatively high-level functions for easier implementation of physics-informed learning and users still need to implement all the steps to solve the problem. Software packages such as GPyTorch <ref type="bibr" target="#b160">161</ref> and Neural Tangents 162 also enable the study of NNs and PINNs through the lens of kernel methods. This viewpoint has produced new understanding of the training dynamics of PINNs, subsequently motivating the design of new effective architectures and training algorithms <ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77</ref> .</p><p>DeepXDE not only solves integer-order ODEs and PDEs, but it can also solve integro-differential equations and fractional PDEs. DeepXDE supports complex domain geometries via the technique of constructive solid geometry, and enables the user code to stay compact, resembling closely the mathematical formulation. DeepXDE is also well-structured and highly configurable, since all its components are loosely coupled. We note that in addition to being used as a research tool for solving problems in computational science and engineering, DeepXDE can also be used as an educational tool in diverse courses. Although DeepXDE is suitable for education and research, SimNet <ref type="bibr" target="#b154">155</ref> developed by Nvidia is specifically optimized for Nvidia GPUs for large-scale engineering problems.</p><p>In PINNs <ref type="bibr">(Box 3)</ref>, one needs to compute the derivatives of the network outputs with respect to the network inputs. One can compute the derivatives using automatic differentiation provided by ML packages such as TensorFlow <ref type="bibr" target="#b149">150</ref> . For example, U t ? ? can be computed using TensorFlow as tf.gradients(U, t), and secondorder derivatives can be computed by applying tf. gradients twice. DeepXDE provides a more convenient way to compute higher-order derivatives, for example using dde.grad.hessian to compute the Hessian matrix. Moreover, there are two extra advantages to using dde.grad.hessian: first, it is lazy evaluation, meaning it will only compute an element in the Hessian matrix until that element is needed, rather than computing the whole Hessian matrix. Second, it memorizes all the gradients that have already been computed to avoid duplicate computation, even if the user calls the function multiple times in different parts of the code. These two features could speed up the computation in problems where one needs to compute the gradients many times, for example in a system of coupled PDEs.</p><p>Most of these libraries (such as DeepXDE and SimNet) use physics as the soft penalty constraints (Box 3), and ADCME embeds DNNs in standard scientific numerical schemes (such as Runge-Kutta methods for ODEs, and the finite-difference, finite-element and finite-volume methods for PDEs) to solve inverse problems. ADCME was recently extended to support implicit schemes and nonlinear constraints <ref type="bibr" target="#b162">163,</ref><ref type="bibr" target="#b163">164</ref> . To enable truly large-scale scientific computations on large meshes, support for MPI-based domain decomposition methods is also available and was demonstrated to scale very well on complex problems <ref type="bibr" target="#b164">165</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Which model, framework, algorithm to use?</head><p>With a growing collection of methodologies and software tools, a series of questions naturally arises: given a physical system and/or governing law and some observational data, which ML framework should one use? Which training algorithm to choose? How many training samples to consider? Although at present there are no rule-of-thumb strategies for answering these questions, and some degree of experience is required to set up a physics-informed ML model properly, meta-learning techniques <ref type="bibr" target="#b165">[166]</ref><ref type="bibr" target="#b166">[167]</ref><ref type="bibr" target="#b167">[168]</ref> could automate this process in the Increasing q(x) ? q = 1 q = 0 Optimization of the 'committor function' Dirichlet boundary conditions ? two metastable states future. The choices intimately depend on the specific task that needs to be tackled. In terms of providing a high-level taxonomy, we note that PINNs are typically used to infer a deterministic function that is compatible with an underlying physical law when a limited number of observations is available (either initial/boundary conditions or other measurements). The underlying architecture of a PINNs model is determined by the nature of a given problem: multi-layer perceptron architectures are generally applicable but do not encode any specialized inductive biases, convolutional NN architectures are suitable for gridded 2D domains, Fourier feature networks are suitable for PDEs whose solution exhibits high frequencies or periodic boundaries, and recurrent architectures are suitable for non-Markovian and time-discrete problems. Moreover, probabilistic variants of PINNs can also be used to infer stochastic processes that can allow capturing epistemic/model uncertainty (via Bayesian inference or frequentist ensembles) or aleatoric uncertainty (via generative models such as variational auto-encoders and GANs). However, the DeepONet framework can be used to infer an operator (instead of a function). In DeepONet, the choice of the underlying architecture can also vary depending on the nature of available data, such as scattered sensor measurements (multi-layer perceptron), images (convolutional NNs) or time series (recurrent NNs). In all the aforementioned cases, the required sample complexity is typically not known a priori and is generally determined by: the strength of inductive biases used in the architecture; the compatibility between the observed data, and the underlying physical law used as regularization; and the complexity of the underlying function or operator to be approximated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current limitations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiscale and multiphysics problems</head><p>Despite the recent success of physics-informed learning across a range of applications, multiscale and multiphysics problems require further developments. For example, fully connected NNs have difficulty learning high-frequency functions, a phenomenon referred to in the literature as the 'F-principle' <ref type="bibr" target="#b168">169</ref> or 'spectral bias' <ref type="bibr" target="#b169">170</ref> . Additional work <ref type="bibr" target="#b170">171,</ref><ref type="bibr" target="#b171">172</ref> rigorously proved the existence of frequency bias in DNNs and derived convergence rates of training as a function of target frequency. Moreover, high-frequency features in the target solution generally result in steep gradients, and thus PINN models often struggle to penalize accurately the PDE residuals <ref type="bibr" target="#b44">45</ref> . As a consequence, for multiscale problems, the networks struggle to learn high-frequency components and often may fail to train <ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b172">173</ref> . To address the challenge of learning high-frequency components, one needs to develop new techniques to aid the network learning, such as domain decomposition <ref type="bibr" target="#b104">105</ref> , Fourier features <ref type="bibr" target="#b173">174</ref> and multiscale DNN <ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b174">175</ref> . However, learning multiphysics simultaneously could be computationally expensive. To address this issue, one may first learn each physics separately and then couple them together. In the method of DeepM&amp;M for the problems of electro-convection <ref type="bibr" target="#b59">60</ref> and hypersonics <ref type="bibr" target="#b60">61</ref> , several DeepONets were first trained for each field separately and subsequently learned the coupled solutions through either a parallel or a serial DeepM&amp;M architecture using supervised learning based on additional data for a specific multiphysics problem. It is also possible to learn the physics at a coarse scale by using the fine-scale simulation data only in small domains <ref type="bibr" target="#b175">176</ref> .</p><p>Currently in NN-based ML methods, the physicsinformed loss functions are mainly defined in a pointwise way. Although NNs with such loss functions can be successful in some high-dimensional problems, they may also fail in some special low-dimensional cases, such as the diffusion equation with non-smooth conductivity/permeability <ref type="bibr" target="#b176">177</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New algorithms and computational frameworks</head><p>Physics-informed ML models often involve training large-scale NNs with complicated loss functions, which generally consist of multiple terms and thus are highly non-convex optimization problems <ref type="bibr" target="#b177">178</ref> . The terms in the loss function may compete with each other during training. Consequently, the training process may not be robust and sufficiently stable, and thus convergence to the global minimum cannot be guaranteed <ref type="bibr" target="#b178">179</ref> . To resolve this issue, one needs to develop more robust NN architectures and training algorithms for diverse applications. For example, <ref type="bibr">refs 76,77,173</ref> have identified two fundamental weaknesses of PINNs, relating spectral bias <ref type="bibr" target="#b169">170</ref> to a discrepancy in the convergence rate of different components in a PINN loss function. The latter is manifested by training instabilities leading to vanishing back-propagated gradients. As discussed in these refs <ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b172">173</ref> , these pathologies can be mitigated by designing appropriate model architectures and new training algorithms for PINNs. Also, ref. <ref type="bibr" target="#b103">104</ref> used the weak form of the PDE and hp-refinement via decomposition to enhance the approximation capability of networks. Other examples include adaptively modifying the activation functions <ref type="bibr" target="#b179">180</ref> or sampling the data points and the residual evaluation points during training <ref type="bibr" target="#b180">181</ref> , which accelerate convergence and improve the performance of physics-informed models. Moreover, the design of effective NN architectures is currently done empirically by users, which could be very time-consuming. However, emerging meta-learning techniques can be used to automate this search <ref type="bibr" target="#b165">[166]</ref><ref type="bibr" target="#b166">[167]</ref><ref type="bibr" target="#b167">[168]</ref> . What is interesting here is that the architecture may be changing as the bifurcation parameters of the system (such as the Reynolds number) increase. The training and optimization of deep learning models is expensive, and it is crucial to speed up the learning, for instance through transfer learning via DeepONets as in the example of crack propagation reported in ref. <ref type="bibr" target="#b181">182</ref> . In addition, scalable and parallel training algorithms should be developed by using hardware like GPUs and tensor processing units, using both data-parallel and model-parallel algorithms. Unlike classic classification or regression tasks, where the first-order derivative is required for gradient descent, physics-informed ML usually involves higher-order derivatives. Currently, their efficient evaluation is not well supported in popular software frameworks such as TensorFlow and PyTorch. An ML software library that is more efficient for computing high-order derivatives (for example, via Taylor-mode automatic differentiation) <ref type="bibr" target="#b182">183,</ref><ref type="bibr" target="#b183">184</ref> could greatly reduce the computational cost and boost the application of physics-informed ML across different disciplines. In addition to integer-order derivatives, other operators such as integral operators and even fractional-order derivatives <ref type="bibr" target="#b102">103</ref> are very useful in physics-informed learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data generation and benchmarks</head><p>In the ML community dealing with imaging, speech and natural language processing problems, the use of standard benchmarks is very common in order to assess algorithm improvement, reproducibility of results, and expected computational cost. The UCI Machine Learning Repository <ref type="bibr" target="#b184">185</ref> , which was created over three decades ago, is a collection of databases and data generators that are often used to compare the relative performance of new algorithms. Currently, they also include experimental data sets in the physical sciences, for example noise generated by an aerofoil, ocean temperature and current measurements related to El Ni?o, and hydrodynamic resistance related to different yacht designs. These data sets are useful and are intended for data-driven modelling in ML, but in principle they can also be used for benchmarking physics-informed ML methods, assuming that proper parameterized physical models can be explicitly included in the databases. However, in many different applications in physics and chemistry, full-field data are required, which cannot be obtained experimentally (for example in density-functional theory and molecular dynamics simulation or in direct numerical simulations of turbulence), and which tax computational resources heavily both in terms of time and memory. Hence, careful consideration should be given to how to make these data publicly available, how to curate such valuable data, and how to include the physical models and all parameters required for the generation of these databases. In addition, it will take a concerted effort by researchers to design meaningful benchmarks that test accuracy and speed-up of the new proposed physics-informed algorithms, which is a non-trivial task. Indeed, even for the aforementioned imaging and other established ML applications, there are still new developments on refining existing benchmarks and metrics, especially if software and hardware considerations are also factored in such evaluations (for example, an in-depth analysis for image recognition) <ref type="bibr" target="#b185">186</ref> . In physical systems, these difficulties are exacerbated by the fact that the aim is to predict dynamics, and it will be complicated, for example, to determine how to capture or identify bifurcations in dynamical systems and chaotic states.</p><p>However, new metrics such as the valid-timeprediction introduced in ref. <ref type="bibr" target="#b186">187</ref> may be appropriate and offer a promising direction to follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New mathematics</head><p>Despite the empirical success of physics-informed learning models, little is known about the theoretical foundation of such constrained NNs. A new theory is required to rigorously analyse the capabilities and limitations of physics-informed learning (for example, the learning capacity of NNs). More specifically, a fundamental question is: can a network find solutions to PDE via gradient-based optimization? To answer this question, one should analyse the total error in deep learning, which can be decomposed into three types of errors: approximation error (can a network approximate a solution to PDE with any accuracy?), optimization error (can one attain zero or very small training loss?) and generalization error (does smaller training error mean more accurate predicted solution?). It is important to analyse the well-posedness of the problem and the stability and convergence in terms of these errors. In particular, if the operator to be solved is (possibly partially) learned by the data themselves, establishing how well-posed any problem involving this operator is becomes an exciting mathematical challenge. The challenge is exacerbated when the initial/boundary/internal conditions are provided themselves as (possibly uncertain) data. This well-posedness issue must be analysed mathematically, aided by ML computational exploration.</p><p>The first mathematical analysis for PINNs in solving forward problems appeared in ref. <ref type="bibr" target="#b187">188</ref> , where the H?lder regularization was introduced to control generalization error. Specifically, ref. <ref type="bibr" target="#b187">188</ref> analysed the second-order linear elliptic and parabolic type PDEs and proved the consistency of results. References <ref type="bibr" target="#b188">189,</ref><ref type="bibr" target="#b189">190</ref> used quadrature points in the formulation of the loss and provided an abstract error estimate for both forward and inverse problems. However, no convergence results were reported, as the use of quadrature points does not quantify the generalization error. In subsequent work, ref. <ref type="bibr" target="#b190">191</ref> studied linear PDEs and proposed an abstract error estimates framework for analysing both PINNs 7 and variational PINNs <ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b191">192</ref> . Based on the compactness assumptions and the norm equivalence relations, sufficient conditions for convergence to the underlying PDE solution were obtained. The generalization error was handled by the rademacher complexity. For the continuous loss formulation, <ref type="bibr">refs 49,193-195</ref> derived some error estimates based on the continuous loss formulations of PINNs. Although known error bounds involved with continuous norms (from PDE literature) may serve as error bounds for (continuous) PINNs, data samples have to be taken into account to quantify the generalization error.</p><p>In general, NNs are trained by gradient-based optimization methods, and a new theory should be developed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H?lder regularization</head><p>A regularization term associated with H?lder constants of differential equations that controls the derivatives of neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rademacher complexity</head><p>A quantity that measures richness of a class of real-valued functions with respect to a probability distribution.</p><p>to better understand their training dynamics (gradient descent, stochastic gradient descent, Adam 196 and so on). In ref. <ref type="bibr" target="#b196">197</ref> , over-parameterized two-layer networks were analysed, and it was proved that the convergence of gradient descent for second-order linear PDEs, but the boundary conditions were not included in the analysis.</p><p>In ref. <ref type="bibr" target="#b75">76</ref> , the neural tangent kernel theory <ref type="bibr" target="#b197">198</ref> was extended to PINNs, and it was shown that the training dynamics of PINNs sometimes can be regarded as a kernel regression as the width of network goes to infinity.</p><p>It is also helpful to understand the training process of networks by visualizing the landscape of loss function of different formulations (strong form, weak form and so on). Furthermore, more methods are being rapidly developed nowadays, and thus it is also important to understand the equivalence between models and the equivalence between different loss functions with different norms.</p><p>Analysing the physics-informed ML models based on rigorous theory calls for a fruitful synergy between deep learning, optimization, numerical analysis and PDE theory that not only has the potential to lead to more robust and effective training algorithms, but also to build a solid foundation for this new generation of computational methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outlook</head><p>Physics-informed learning integrates data and mathematical models seamlessly even in noisy and highdimensional contexts, and can solve general inverse problems very effectively. Here, we have summarized some of the key concepts in Boxes 1-3 and provided references to frameworks and open-source software for the interested reader to have a head start in exploring physics-informed learning. We also discussed current capabilities and limitations and highlighted diverse applications from fluid dynamics to biophysics, plasma physics, transition between metastable states and other applications in materials. Next, we present possible new directions for applications of physics-informed learning machines as well as research directions that will contri bute to their faster training, more accurate predictions, and better interpretability for diverse physics applications and beyond.</p><p>Although there have been tools like TensorBoard to visualize the model graph, track the variables and metrics, and so on, for physical problems, extended requirements may include incorporating multiple physics and complicated geometry domain into the learning algorithm, visualizing the solution field (even high-dimensional ones), as in traditional computing platforms such as FEniCS 199 , OpenFOAM 10 and others. A user-friendly, graph-based ML development environment that can address the above issues could help more practitioners to develop physics-informed ML algorithms for applications to a wide range of diverse physical problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future directions</head><p>Digital twins. 'Digital twins' , a concept first put forth by General Electric to describe the digital copy of an engine manufactured in their factories, are now becoming a reality in a number of industries. By assimilating real measurements to calibrate computational models, a digital twin aims to replicate the behaviour of a living or non-living physical entity in silico. Before these emerging technologies can be translated into practice, a series of fundamental questions need to be addressed. First, observational data can be scarce and noisy, are often characterized by vastly hetero geneous data modalities (images, time series, lab tests, historical data, clinical records and so on), and may not be directly available for certain quantities of interest. Second, physics-based computational models heavily rely on tedious pre-processing and calibration procedures (such as mesh generation or calibration of initial and boundary conditions) that typically have a considerable cost, hampering their use in real-time decisionmaking settings. Moreover, physical models of many complex natural systems are, at best, 'partially' known as conservation laws, and do not provide a closed system of equations unless appropriate constitutive laws are postulated. Thanks to its natural capability of blending physical models and data as well as the use of automatic differentiation that removes the need for mesh generation, physics-informed learning is well placed to become an enabling catalyst in the emerging era of digital twins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and model transformations, fusion and interpretability.</head><p>As the interactions between physics-based modelling and ML intensify, one will encounter -with increasing frequency -situations in which different researchers arrive at different data-driven models of the same phenomenon, even if they use the same training data (or equally informative data, observed through different sensors). For example, two research groups using the same or equivalent alternative data may end up having differently trained networks (differently learned latent spaces, differently learned operators) even though their predictions are practically indistinguishable on the training set. Recognizing that there is often no unique physical interpretation of an observed phenomenon, here we foresee the importance of building ML-based transformations between predictive models, models at different fidelities, and theories, that are one-to-one (transformable, 'dual' , calibratable) to each other in a verifiable manner. Researchers are increasingly discovering such transformations (for example from nonlinear dynamics to the corresponding Koopman model; from a Poisson system to the corresponding Hamiltonian one; from Nesterov iterations to their corresponding ODEs) in a data-driven manner. Such transformations will allow data and models to be systematically fused. Transformations between the ML latent space features and the physically interpretable observables, or ML-learned operators and closed-form equations, will obviously bolster the interpretability of the ML model. Ultimately, one needs to test how far these transformations generalize: for what range of observations an ML model can be mapped to a different ML model, or to a physical model, and what the generalization limit is, beyond which they cannot be transformed or calibrated to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Koopman model</head><p>Linear model of a (nonlinear) dynamical system obtained via a Koopman operator theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nesterov iterations</head><p>iterations of an algorithm for the numerical computation of equilibria.</p><p>www.nature.com/natrevphys R e v i e w s 0123456789();:</p><p>Searching for intrinsic variables and emergent, useful representations. Most of the current physics-informed ML methods follow this paradigm: first define a set of (humanly interpretable) observables/variables; then collect data; formulate the physics completely or incompletely using a 'reasonable' dictionary of operators based on the chosen observables; and finally apply the learning algorithm of choice. An emerging paradigm fuelled by advances in ML is to use observations and learning methods to automatically determine good/ intrinsic variables and to also find useful or informative physical model formulations. Stepping beyond principal component analysis, manifold learning techniques (from isoMAP to t-sNe and diffusion maps) and their deep learning counterparts of generative models and (possibly variational) auto-encoders are used to embed raw observations in reduced, mathematically useful latent spaces, in which evolution rules can be learned. Remarkably, these useful representations can go beyond embedding the relevant features, the dependent vari ables in a PDE. For spatiotemporally disordered data, one can also create ML-driven emergent spaces <ref type="bibr" target="#b199">200,</ref><ref type="bibr" target="#b200">201</ref> in terms of ML-learned independent variables: emergent 'spacetimes' in which the model operators will be learned. The DARPA Shredder Challenge 202 of 2011 recreated space by effectively solving puzzles: documents shredded using a variety of paper shredding techniques. Today, disorganized spatiotemporal observations can be embedded in informative 'independent variable' emergent spaces. For example, evolution operators in the form of PDEs or SODEs will then be learned in terms of these new, emergent space -even possibly time -independent variables; there is a direct analogy here with the discussion of emergent space-time in modern physics <ref type="bibr" target="#b202">203</ref> .</p><p>Such new paradigms could play a critical role in design optimization or in building a digital twin for complicated systems, even systems of systems, where humans can hardly write down a neat physical formulation in closed form. Moreover, instead of collecting data from experiments first and then performing the learning algorithm, it becomes important to integrate both in an active learning framework. In this way, a judicious selection of new and informative data can be aided by exploiting the geometry of latent space of the learning algorithms, while the algorithms can gradually improve the choice of latent space descriptors, as well as the mathematical formulation governing the physics, so as to yield realistic predictions as the experiments go on.</p><p>Ultimately, the main element we see changing is what we mean by 'understanding' . Up to now, understanding meant that, say, each term in a PDE had a physical or mechanistic interpretation operated on some physically meaningful observables (dependent variables) and also operated in terms of some physically meaningful space/ time (independent) variables. Now, it becomes possible to make accurate predictions without this type of mechanistic understanding -and 'understanding' is something that may be redefined in the process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published online 24 May 2021</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion maps</head><p>A nonlinear dimensionality reduction technique for embedding intrinsically low-dimensional data from high-dimensional representations to lower-dimensional spaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 |</head><label>1</label><figDesc>Fig.1| Physics-inspired neural network architectures. a | Predicting molecular properties with covariant compositional networks<ref type="bibr" target="#b203">204</ref> . The architecture is based on graph neural networks<ref type="bibr" target="#b18">19</ref> and is constructed by decomposing into a hierarchy of sub-graphs (middle) and forming a neural network in which each 'neuron' corresponds to one of the sub-graphs and receives inputs from other neurons that correspond to smaller sub-graphs (right). The middle panel shows how this can equivalently be thought of as an algorithm in which each vertex receives and aggregates messages from its neighbours. Also depicted on the left are the molecular graphs for C 18 H 9 N 3 OSSe and C 22 H 15 NSeSi from the Harvard Clean Energy Project (HCEP) data set<ref type="bibr" target="#b204">205</ref> with their corresponding adjacency matrices. b | A neural network with the Lax-Oleinik formula represented in the architecture. f is the solution of the Hamilton-Jacobi partial differential equations, x and t are the spatial and temporal variables, L is a convex and Lipschitz activation function, ? R a i and ? R u i n are the neural network parameters, and m is the number of neurons. Panel a is adapted with permission from ref.<ref type="bibr" target="#b203">204</ref> , AIP Publishing. Panel b image courtesy of J. Darbon and T. Meng, Brown University.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 |</head><label>2</label><figDesc>Fig. 2 | Inferring the 3D flow over an espresso cup based using the Tomo-BOS imaging system and physics-informed neural networks (PINNs). a | Six cameras are aligned around an espresso cup, recording the distortion of the dot-patterns in the panels placed in the background, where the distortion is caused by the density variation of the airflow above the espresso cup. The image data are acquired and processed with LaVision's Tomographic BOS software (DaVis 10.1.1). b | 3D temperature field derived from the refractive index field and reconstructed based on the 2D images from all six cameras. c | Physics-informed neural network (PINN) inference of the 3D velocity field (left) and pressure field (right) from the temperature data. The Tomo-BOS experiment was performed by F. Fuest, Y. J. Jeon and C. Gray from LaVision. The PINN inference and visualization were performed by S. Cai and C. Li at Brown University. Image courtesy of S. Cai and C. Li, Brown University.</figDesc><graphic url="image-5.png" coords="9,353.74,240.15,134.18,122.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 |</head><label>3</label><figDesc>Fig. 3 | Physics-informed filtering of in-vivo 4D-flow magnetic resonance imaging data of blood flow in a porcine descending aorta. Physics-informed neural network (PINN) models can be used to de-noise and reconstruct clinical magnetic resonance imaging (MRI) data of blood velocity, while constraining this reconstruction to respect the underlying physical laws of momentum and mass conservation, as described by the incompressible Navier-Stokes equations. Moreover, a trained PINN model has the potential to aid the automatic segmentation of the arterial wall geometry and to infer important biomarkers such as blood pressure and wall shear stresses. a | Snapshot of in-vivo 4D-flow MRI measurements. b-d | A PINN reconstruction of the velocity field (panel b), pressure (panel c), arterial wall surface geometry and wall shear stresses (panel d). The 4D-flow MRI data were acquired by E. Hwuang and W. Witschey at the University of Pennsylvania. The PINN inference and visualization were performed by S. Wang, G. Kissas and P. Perdikaris at the University of Pennsylvania.</figDesc><graphic url="image-6.png" coords="10,42.52,525.29,122.88,129.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 |</head><label>4</label><figDesc>Fig. 4 | Uncovering edge plasma dynamics. One of the most intensely studied aspects of magnetic confinement fusion is edge plasma behaviour, which is critical to reactor performance and operation. The drift-reduced Braginskii two-fluid theory has for decades been widely used to model edge plasmas, with varying success. Using a 3D magnetized two-fluid model, physics-informed neural networks (PINNs) can be used to accurately reconstruct 141 the unknown turbulent electric field (middle panel) and underlying electric potential (right panel), directly from partial observations of the plasma's electron density and temperature from a single test discharge (left panel). The top row shows the reference target solution, while the bottom row depicts the PINN model's prediction. These 2D synthetic measurements of electron density and temperature over the duration of a single plasma discharge constitute the only physical dynamics observed by the PINNs from the 3D collisional plasma exhibiting blob-like filaments. ?, electric potential; E r , electric field; n e , electron density; T e , electron temperature. Figure courtesy of A. Matthews, MIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 |</head><label>5</label><figDesc>Fig. 5 | Transitions between metastable states. Results obtained from studying transitions between metastable states of a distribution in a 144-dimensional Allen-Cahn type system. The top part of the figure shows the two metastable states. The lower part of the figure shows, from left to right, a learned sample path with the characteristic nucleation pathway for a transition between the two metastable states. Here, q is the committor function. Figure courtesy of G. M. Rotskoff, Stanford University, and E. Vanden-Eijnden, Courant Institute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>it was demonstrated that PINNs can accurately</figDesc><table><row><cell>Tomo-BOS setup</cell><cell>3D temperature data</cell></row><row><cell></cell><cell>330 325 320 315 310 305 300 295</cell></row><row><cell></cell><cell>Physics-informed</cell></row><row><cell></cell><cell>neural network</cell></row><row><cell>3D velocity</cell><cell>3D pressure</cell></row><row><cell>0.34 0.30 0.26 0.22 0.18 0.14 0.10 0.02</cell><cell>0.006 0.005 0.004 0.003 0.002 0.001 0 -0.001 -0.002</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 | Major software libraries specifically designed for physics-informed machine learning</head><label>1</label><figDesc></figDesc><table><row><cell>Software name</cell><cell>Usage</cell><cell>Language</cell><cell>Backend</cell><cell>Ref.</cell></row><row><cell>DeepXDE</cell><cell>Solver</cell><cell>Python</cell><cell>TensorFlow</cell><cell>154</cell></row><row><cell>SimNet</cell><cell>Solver</cell><cell>Python</cell><cell>TensorFlow</cell><cell>155</cell></row><row><cell>PyDEns</cell><cell>Solver</cell><cell>Python</cell><cell>TensorFlow</cell><cell>156</cell></row><row><cell>NeuroDiffEq</cell><cell>Solver</cell><cell>Python</cell><cell>PyTorch</cell><cell>157</cell></row><row><cell>NeuralPDE</cell><cell>Solver</cell><cell>Julia</cell><cell>Julia</cell><cell>158</cell></row><row><cell>SciANN</cell><cell>Wrapper</cell><cell>Python</cell><cell>TensorFlow</cell><cell>159</cell></row><row><cell>ADCME</cell><cell>Wrapper</cell><cell>Julia</cell><cell>TensorFlow</cell><cell>160</cell></row><row><cell>GPyTorch</cell><cell>Wrapper</cell><cell>Python</cell><cell>PyTorch</cell><cell>161</cell></row><row><cell>Neural Tangents</cell><cell>Wrapper</cell><cell>Python</cell><cell>JAX</cell><cell>162</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>www.nature.com/natrevphys</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">H. Owhadi</rs> (<rs type="affiliation">Caltech</rs>) for his insightful comments on the connections between NNs and kernel methods. G.E.K. acknowledges support from the <rs type="funder">DOE</rs> <rs type="projectName">PhILMs</rs> project (no. <rs type="grantNumber">DE-SC0019453</rs>) and <rs type="funder">OSD/AFOSR MURI</rs> grant <rs type="grantNumber">FA9550-20-1-0358</rs>. I.G.K. acknowledges support from <rs type="funder">DARPA</rs> (PAI and ATLAS programmes) as well as an <rs type="funder">AFOSR</rs> <rs type="grantName">MURI grant</rs> through <rs type="funder">UCSB</rs>. P.P. acknowledges support from the <rs type="funder">DARPA PAI programme</rs> (grant <rs type="grantNumber">HR00111890034</rs>), the <rs type="funder">US Department of Energy</rs> (grant <rs type="grantNumber">DE-SC0019116</rs>), the <rs type="funder">Air Force Office of Scientific Research</rs> (grant <rs type="grantNumber">FA9550-20-1-0060</rs>), and <rs type="funder">DOE-ARPA</rs> (grant <rs type="grantNumber">1256545</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_kuCPR6y">
					<idno type="grant-number">DE-SC0019453</idno>
					<orgName type="project" subtype="full">PhILMs</orgName>
				</org>
				<org type="funding" xml:id="_68tJdCu">
					<idno type="grant-number">FA9550-20-1-0358</idno>
				</org>
				<org type="funding" xml:id="_MtsjZfz">
					<orgName type="grant-name">MURI grant</orgName>
				</org>
				<org type="funding" xml:id="_w8KeTp4">
					<idno type="grant-number">HR00111890034</idno>
				</org>
				<org type="funding" xml:id="_Jxfkk3F">
					<idno type="grant-number">DE-SC0019116</idno>
				</org>
				<org type="funding" xml:id="_sQMSrdP">
					<idno type="grant-number">FA9550-20-1-0060</idno>
				</org>
				<org type="funding" xml:id="_TwkWmh6">
					<idno type="grant-number">1256545</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head><p>Authors are listed in alphabetical order. G.E.K. supervised the project. All authors contributed equally to writing the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peer review information</head><p>Nature Reviews Physics thanks the anonymous reviewers for their contribution to the peer review of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's note</head><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Environmental sensor networks: a revolution in the earth system science?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Earth Sci. Rev</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="177" to="191" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exascale deep learning for climate analytics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kurth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prediction of vegetation dynamics using NDVI time series data and LSTM</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R C</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Model. Earth Syst. Environ</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="409" to="419" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning and process understanding for data-driven earth system science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reichstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">566</biblScope>
			<biblScope unit="page" from="195" to="204" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integrating machine learning and multiscale modeling -perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digit. Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discovering physical concepts with neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Metger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wilming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Del Rio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Renner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">10508</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distilling free-form natural laws from experimental data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="page" from="81" to="85" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering governing equations from data by sparse identification of nonlinear dynamical systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="3932" to="3937" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">OpenFOAM: A C++ library for complex physics simulations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jasak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Workshop Coupled Methods Numer. Dyn</title>
		<imprint>
			<biblScope unit="volume">1000</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast parallel algorithms for short-range molecular dynamics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Plimpton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Physics-guided machine learning for scientific discovery: an application in simulating lake temperature profiles</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2001.11086" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="218" to="229" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A point-cloud deep learning framework for prediction of fluid flow fields on irregular geometries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kashefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Fluids</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">27104</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fourier neural operator for parametric partial differential equations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional deep surrogate models for stochastic, high-dimensional, and multi-fidelity systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mech</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="417" to="434" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handb. Brain Theory Neural Netw</title>
		<imprint>
			<biblScope unit="volume">3361</biblScope>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding deep convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page">20150203</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond Euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral CNN</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multigrid with rough coefficients and multiresolution operator decomposition from hierarchical information games</title>
		<author>
			<persName><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="99" to="149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inferring solutions of differential equations using noisy multi-fidelity data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="page" from="736" to="746" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Numerical Gaussian processes for time-dependent and nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="172" to="A198" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian numerical homogenization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="812" to="828" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning dynamical systems from data: a simple cross-validation perspective, part I: parametric kernel flows</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hamzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">421</biblScope>
			<biblScope unit="page">132817</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning equivariant functions with matrix valued kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reisert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="385" to="408" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kernel flows: from learning kernels from data into the abyss</title>
		<author>
			<persName><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">389</biblScope>
			<biblScope unit="page" from="22" to="47" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved semantic segmentation for histopathology using rotation equivariant convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Med. Imaging Deep Learn</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1801.02144" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Equivariant transformer networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6086" to="6095" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ab initio solution of the manyelectron Schr?dinger equation with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M C</forename><surname>Foulkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">33429</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Physically informed artificial neural networks for atomistic modeling of materials</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramprasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mishin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reynolds averaged turbulence modelling using deep neural networks with embedded invariance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurzawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Templeton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fluid Mech</title>
		<imprint>
			<biblScope unit="volume">807</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SympNets: intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="166" to="179" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning for universal linear embeddings of nonlinear dynamics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4950</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Artificial neural networks for solving ordinary and partial differential equations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Lagaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Fotiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="987" to="1000" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PFNN: A penalty-free neural network method for solving a class of second-order boundary-value problems on complex geometries</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">428</biblScope>
			<biblScope unit="page">110085</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Artificial neural network method for solution of boundary value problems with exact satisfaction of arbitrary boundary conditions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mcfall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Mahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transac. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1221" to="1233" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Solving initial-boundary value problems for systems of partial differential equations using neural networks and optimization techniques</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Beidokhti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Franklin Inst</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="page" from="898" to="913" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Systematic construction of neural forms for solving partial differential equations inside rectangular domains, subject to initial, boundary and interface conditions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Lagari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Tsoukalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Safarkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Lagaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Artif. Intell. Tools</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">2050009</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning in modal space: solving time-dependent stochastic PDEs using physics-informed neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="639" to="A665" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A method for representing periodic functions and enforcing exactly periodic boundary conditions with deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="page">110242</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-scale deep neural network (MscaleDNN) methods for oscillatory stokes flows in complex domains</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2139" to="2157" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale deep neural network (MscaleDNN) for solving Poisson-Boltzmann equation in complex domains</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1970" to="2001" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Physical symmetries embedded in neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mattheakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Protopapas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sondak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kaxiras</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.08991" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A phase shift deep neural network for high frequency approximation and wave problems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="3285" to="A3312" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On some neural network architectures that can represent viscosity solutions of certain high dimensional Hamilton-Jacobi partial differential equations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Darbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">425</biblScope>
			<biblScope unit="page">109907</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DGM: a deep learning algorithm for solving partial differential equations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sirignano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Spiliopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="page" from="1339" to="1364" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Machine learning in cardiovascular flows modeling: predicting arterial blood pressure from non-invasive 4D flow MRI data using physicsinformed neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kissas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page">112623</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koutsourelakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="page" from="56" to="81" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling the dynamics of PDE systems with physics-constrained deep auto-regressive networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Geneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page">109056</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="page">109209</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Contactnets: learning of discontinuous contact dynamics with smooth, implicit representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pfrommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Posa</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2009.11193" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Physics-informed autoencoders for Lyapunov-stable fluid flow prediction</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Erichson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muehlebach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.10866" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Encoding invariances in deep generative models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shah</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="https://arxiv.org/abs/1906.01626" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Transformers for modeling physical systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Geneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="https://arxiv.org/abs/2010.03957" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multipole graph neural operator for parametric partial differential equations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The random feature model for input-output maps between Banach spaces</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Nelsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.10224" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">DeepM&amp;Mnet: inferring the electroconvection multiphysics fields based on operator approximation by neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">436</biblScope>
			<biblScope unit="page">110296</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">DeepM&amp;Mnet for hypersonics: predicting the coupled flow and finite-rate chemistry behind a normal shock using neural-network approximation of operators</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Marxen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2011.03349" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A composite neural network that learns from multi-fidelity data: application to function approximation and inverse PDE problems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page">109020</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">DPM: a deep learning PDE augmentation method with application to large-eddy simulation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sirignano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Macart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Freund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page">109811</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Extraction of mechanical properties of materials through deep learning from instrumented indentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="7052" to="7062" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning unknown physics of non-Newtonian fluids</title>
		<author>
			<persName><forename type="first">B</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tartakovsky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2009.01658" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Coarse-graining auto-encoders for molecular dynamics</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Continuous-time nonlinear signal processing: a neural network based approach for gray box identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rico-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kevrekidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Learning constitutive relations using symmetric positive definite neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.00265" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Predictive modeling with learned constitutive laws from indirect observations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.12530" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Inverse modeling of viscoelasticity materials using physics constrained learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tartakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.04384" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Coupled time-lapse full-waveform inversion for subsurface flow problems using intrusive automatic differentiation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<idno>WR027032</idno>
	</analytic>
	<monogr>
		<title level="j">Water Resour. Res</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Physics-informed deep neural networks for learning parameters and constitutive relationships in subsurface flow problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tartakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Marrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tartakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barajas-Solano</surname></persName>
		</author>
		<idno>WR026731</idno>
	</analytic>
	<monogr>
		<title level="j">Water Resour. Res</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Adversarial numerical analysis for inverse problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.06936" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Bayesian differential programming for robust systems identification under uncertainty</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bhouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">476</biblScope>
			<biblScope unit="page">20200290</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Universal differential equations for scientific machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rackauckas</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2001.04385" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">When and why PINNs fail to train: a neural tangent kernel perspective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.14527" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">On the eigenvector bias of Fourier feature networks: from regression to solving multi-scale PDEs with physics-informed neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2012.10047" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Neural-netinduced Gaussian process regression for function approximation and PDE solution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="page" from="270" to="288" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep kernel learning. Proc. Int. Conf</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Do ideas have shape? Plato&apos;s theory of forms as the continuous limit of artificial neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.03920" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Operator-Adapted Wavelets, Fast Solvers, and Numerical Homogenization: From a Game Theoretic Approach to Numerical Approximation and Algorithm Design</title>
		<author>
			<persName><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scovel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge Univ. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Rivlin</surname></persName>
		</author>
		<title level="m">Optimal Estimation in Approximation Theory</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Rivlin</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="1" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Linear Approximation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Mathematical Surveys</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1963">1963</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Gaussian measure in Hilbert space and applications in numerical analysis. Rocky Mt</title>
		<author>
			<persName><forename type="first">F</forename><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="379" to="421" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Wiener measure and its applications to approximation methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Sul'din</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I. Izv. Vyssh. Uchebn. Zaved. Mat</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="145" to="158" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Bayesian numerical analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Decision Theory Relat. Top. IV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="163" to="175" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A correspondence between Bayesian estimation on stochastic processes and smoothing by splines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Kimeldorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="495" to="502" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Statistical numerical approximation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scovel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sch?fer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Not. Am. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1608" to="1617" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Transformer dissection: a unified understanding of transformer&apos;s attention via the lens of kernel</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="https://arxiv.org/abs/1908.11775" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Operator-valued kernels for learning from functional response data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Identification of distributed parameter systems: a neural net based approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gonz?lez-Garc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rico-Mart?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Chem. Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="965" to="S968" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">PDE-Net: learning PDEs from data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3208" to="3216" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">MgNet: a unified framework of multigrid and convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China Math</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1331" to="1354" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Discrete-vs. continuous-time nonlinear signal processing of Cu electrodissolution data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rico-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Eng. Commun</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="25" to="48" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A proposal on machine learning via dynamical systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="6571" to="6583" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Neural jump stochastic differential equations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9847" to="9858" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Rico-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krischer</surname></persName>
		</author>
		<title level="m">Neural Networks for Chemical Engineers</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Bulsari</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="409" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">ReLU deep neural networks and linear finite elements</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="502" to="527" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Conservative physics-informed neural networks on discrete domains for conservation laws: applications to forward and inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jagtap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kharazmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page">113028</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Physics-informed generative adversarial networks for stochastic differential equations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="292" to="A317" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">fPINNs: fractional physics-informed neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2603" to="A2626" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">hp-VPINNs: variational physics-informed neural networks with domain decomposition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kharazmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page">113547</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Extended physicsinformed neural networks (XPINNs): a generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jagtap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2002" to="2041" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Hidden fluid mechanics: learning velocity and pressure fields from flow visualizations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page" from="1026" to="1030" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<author>
			<persName><surname>B-Pinns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page">109913</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Deep learning of free boundary and Stefan problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">428</biblScope>
			<biblScope unit="page">109914</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A jamming transition from under-to over-parametrization affects generalization in deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Spigler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">474001</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Scaling description of generalization with number of parameters in deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Mech. Theory Exp</title>
		<imprint>
			<biblScope unit="page">123456789</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Reconciling modern machine-learning practice and the classical bias-variance trade-off</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci</title>
		<meeting>Natl Acad. Sci<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15849" to="15854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Jamming transition as a paradigm to understand the loss landscape of deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">12115</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">A mean field view of the landscape of two-layer neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="7665" to="E7671" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">An exact mapping between the variational renormalization group and deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1410.3831" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Supervised learning with tensor networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Stoudenmire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4799" to="4807" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Artif. Intell. Stat</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="192" to="204" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Exponential expressivity in deep neural networks through transient chaos</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3360" to="3368" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Mean field residual networks: on the edge of chaos</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="7103" to="7114" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Why and when can deep -but not shallownetworks avoid the curse of dimensionality: a review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Autom. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="503" to="519" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">A proof that artificial neural networks overcome the curse of dimensionality in the numerical approximation of Black-Scholes partial differential equations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jentzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Wurstemberger</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="https://arxiv.org/abs/1809.02362" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Solving highdimensional partial differential equations using deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jentzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="8505" to="8510" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Seqgan</surname></persName>
		</author>
		<title level="m">sequence generative adversarial nets with policy gradient</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Generative ensemble-regression: learning particle dynamics from observations of ensembles with physics-informed deep generative models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.01915" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Error estimates for DeepONets: a deep learning framework in infinite dimensions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lanthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.09618" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Convergence rate of DeepONets for learning operators arising from advection-diffusion equations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.10621" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">The Wiener-Askey polynomial chaos for stochastic differential equations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="619" to="644" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Stochastic spectral methods for efficient Bayesian solution of inverse problems</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Marzouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Najm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">224</biblScope>
			<biblScope unit="page" from="560" to="586" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Inverse problems: a Bayesian perspective</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">451</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Deep UQ: learning deep neural network surrogate models for high dimensional uncertainty quantification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bilionis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="page" from="565" to="588" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Simulator-free solution of high-dimensional stochastic elliptic partial differential equations using deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karumuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bilionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Panchal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="page">109120</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Adversarial uncertainty quantification in physics-informed neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="page" from="136" to="152" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Machine learning of linear differential equations using Gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="page" from="683" to="693" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A robotic intelligent towing tank for learning complex fluid-structure dynamics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5063</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">ConvPDE-UQ: convolutional neural networks with quantified uncertainty for heterogeneous elliptic partial differential equations on varied domains</title>
		<author>
			<persName><forename type="first">N</forename><surname>Winovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="page" from="263" to="279" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">397</biblScope>
			<biblScope unit="page">108850</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1050" to="1059" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Flow over an espresso cup: inferring 3-D velocity and pressure fields from tomographic background oriented Schlieren via physics-informed neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fluid Mech</title>
		<imprint>
			<biblScope unit="volume">915</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Uncovering edge plasma dynamics via deep learning from partial observations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Francisquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hatch</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2009.05005" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Learning with rare data: using active importance sampling to optimize objectives dominated by rare events</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Rotskoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.06334" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Thermodynamically consistent physics-informed neural networks for hyperbolic systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Patel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2012.05343" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Physics-informed neural network for ultrasound nondestructive quantification of surface breaking cracks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Di Leoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blackshire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sparkman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nondestruct. Eval</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Generalized neural-network representation of high-dimensional potential-energy surfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Behler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parrinello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">146401</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Car</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">143001</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Pushing the limit of molecular dynamics with ab initio accuracy to 100 million atoms with machine learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.00223" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Large scale and linear scaling DFT with the CONQUEST code</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page">164112</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">A general approach to seismic inversion with automatic differentiation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Beroza</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.06027" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Keras -Deep learning library</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Compiling machine learning programs via high-level tracing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syst</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">DeepXDE: a deep learning library for solving differential equations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="208" to="228" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">NVIDIA SimNet: an AI-accelerated multi-physics simulation framework</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hennigh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2012.07938" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">PyDEns: a Python framework for solving differential equations with neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koryagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khudorozkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsimfer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.11544" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">NeuroDiffEq: A python package for solving differential equations with neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Source Softw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1931</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">jla performant and feature-rich ecosystem for solving differential equations in Julia</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rackauckas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><surname>Differentialequations</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Res. Softw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">SciANN: a Keras/ TensorFlow wrapper for scientific computations and physics-informed deep learning using artificial neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Haghighat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Juanes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Appl. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="page">113552</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">ADCME: Learning spatially-varying physical fields using deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2011.11955" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Gpytorch: blackbox matrix-matrix Gaussian process inference with GPU acceleration</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7587" to="7597" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Neural Tangents: fast and easy infinite neural networks in Python</title>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Conf. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Physics constrained learning for data-driven inverse modeling from sparse observations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.10521" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1901.07758" />
		<title level="m">The neural network approach to inverse problems in differential equations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Distributed machine learning for computational engineering using MPI</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Darve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2011.01349" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Neural architecture search: a survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">AutoML: a survey of the state-of-the-art</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">106622</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Meta-learning in neural networks: a survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.05439" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Frequency principle: Fourier analysis sheds light on deep neural networks</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1746" to="1767" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">On the spectral bias of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rahaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5301" to="5310" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">The convergence rate of neural networks for learned functions of different frequencies</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kritchman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4761" to="4771" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Towards understanding the spectral bias of deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.01198" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Understanding and mitigating gradient pathologies in physics-informed neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2001.04536" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Multi-scale deep neural networks for solving high dimensional PDEs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q J</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.11710" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Linking machine learning with multiscale numerics: data-driven discovery of homogenized equations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Arbabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Bunder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Samaey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOM</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="4444" to="4457" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Metric-based upscaling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="675" to="723" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Training a 3-node neural network is NP-complete</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Gradient descent only converges to minimizers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Conf. Learn. Theory</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1246" to="1257" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Locally adaptive activation functions with slope recovery for deep and physics-informed neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jagtap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Em Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">476</biblScope>
			<biblScope unit="page">20200334</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">Solving Allen-Cahn and Cahn-Hilliard equations using the adaptive physics informed neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://arXiv.org/abs/2007.04542" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Transfer learning enhanced physics informed neural network for phase-field modeling of fracture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anitescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rabczuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Appl. Fract. Mech</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">102447</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">A geometric theory of higher-order automatic differentiation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="https://arxiv.org/abs/1812.11592" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Taylor-mode automatic differentiation for higher-order derivatives in JAX</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Conf. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hettich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/~mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Benchmark analysis of representative deep neural network architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Celona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="64270" to="64277" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Vlachas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Darbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2042" to="2074" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<title level="m" type="main">Estimates on the generalization error of physics informed neural networks (PINNs) for approximating PDEs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molinaro</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.16144" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Estimates on the generalization error of physics informed neural networks (PINNs) for approximating PDEs II: a class of inverse problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molinaro</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.01138" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Error estimates of residual minimization using neural networks for linear PDEs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.08019" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Variational physics-informed neural networks for solving partial differential equations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kharazmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karniadakis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.00873" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Deep neural network approach to forward-inverse problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Netw. Heterog. Media</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="247" to="259" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">An energy-based error bound of physics-informed neural network solutions in elasticity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Haghighat</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.09088" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">The model reduction of the Vlasov-Poisson-Fokker-Planck system to the Poisson-Nernst-Planck system via the deep neural network approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="https://arxiv.org/abs/2009.13280" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">Two-layer neural networks for partial differential equations: optimization and generalization theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.15733" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="8571" to="8580" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Alnaes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The FEniCS project version 1.5. Arch. Numer. Softw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="23" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">An emergent space for distributed data with hidden internal order through manifold learning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Kemeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="77402" to="77413" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">Learning emergent PDEs in a learned emergent space</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Kemeth</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2012.12738" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Defense Advanced Research Projects Agency. DARPA shredder challenge rules</title>
		<ptr target="https://web.archive.org/web/20130221190250/http://archive.darpa.mil/shredderchallenge/Rules.aspx" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Forget time</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rovelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Phys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">1475</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Predicting molecular properties with covariant compositional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page">241745</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">The Harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hachmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. Chem. Lett</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2241" to="2251" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/simnetTensorFlow:www.tensorflow.org" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>ReLAted Links ADCMe</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
