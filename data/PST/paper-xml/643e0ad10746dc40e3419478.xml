<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-17">17 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
							<email>xiao_wang20@fudan.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Weikang</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Can</forename><surname>Zu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianze</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuansen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
							<email>tgui@fudan.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jihua</forename><surname>Kang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingsheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunsai</forename><surname>Du</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Modern Languages and Linguistics</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-17">17 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2304.08085v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models have unlocked strong multi-task capabilities from reading instructive prompts. However, recent studies have shown that existing large models still have difficulty with information extraction tasks. For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance. In this paper, we propose Instruc-tUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency. To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) <ref type="bibr" target="#b1">(Brown et al., 2020;</ref><ref type="bibr" target="#b40">Ouyang et al., 2022;</ref><ref type="bibr" target="#b39">OpenAI, 2023)</ref> show tremendous promise in generalization within the set of observed tasks through multi-task training and unified encoding <ref type="bibr" target="#b38">(Mishra et al., 2022;</ref><ref type="bibr">Wang et al., 2022c;</ref><ref type="bibr" target="#b33">Longpre et al., 2023)</ref>. Recent research has revealed a significant performance gap in LLMs when it comes to information extraction (IE) tasks <ref type="bibr" target="#b68">(Ye et al., 2023;</ref><ref type="bibr" target="#b59">Chen et al., 2023)</ref>. For instance, gpt-3.5-turbo achieves an 18.22 F1 score on the Ontonotes dataset, which is far from satisfactory. Therefore, it is necessary to explore how to build a unified information extraction (UIE) model with LLMs.</p><p>Recently, <ref type="bibr" target="#b36">Lu et al. (2022)</ref> proposed UIE, which uniformly encodes different extraction structures via a structured extraction language, and captures the common IE abilities via a large-scale pretrained text-to-structure model (shown in Figure <ref type="figure" target="#fig_0">1a</ref>). However, UIE requires separate finetune for different downstream tasks. This lead to the poor performance of UIE in low resource settings or facing new label schema, which greatly restricts the application of UIE in real scenarios. <ref type="bibr" target="#b34">Lou et al. (2023)</ref> proposed USM, which decouple IE into two basic tasks, token-token linking to extract labelagnostic substructures, and label-token linking to attach substructures to corresponding semantic concepts (shown in Figure <ref type="figure" target="#fig_0">1b</ref>). However, USM presents two major limitations. Firstly, it converts IE into a semantic matching task, which makes it difficult to integrate with generative language model. Secondly, the method requires semantic matching for each word, which leads to a significant increase in training and inference time.</p><p>In this work, we introduce a unified information extraction framework based on multi-task instruction tuning, named InstructUIE (shown in Figure 1c). Specifically, we reformulate IE tasks as a natural language generation problem. For the source sentence, we design descriptive instructions to enable the model to understand different tasks and employ an option mechanism including all candidate categories as constraints of output space. Then, a pre-trained language model is required to generate the target structure and the corresponding type in the form of natural language. We believe that unrestricted decoding would stimulate the latent knowledge of LLMs to complete IE tasks to a larger extent. We further propose auxiliary tasks, which enable the model to capture common structure information and deepen the understanding of diverse semantics. To evaluate the effectiveness of the proposed model, we have developed a new benchmark called IE INSTRUCTIONS. The benchmark consists of 32 diverse information extraction datasets that have been unified into a text-to-text format, allowing for a consistent and standardized evaluation of various IE tasks<ref type="foot" target="#foot_0">1</ref> . Based on the benchmark, we conduct experiments on three main IE tasks under the supervised and zero-shot settings.</p><p>Our main contributions are summarized as follows:</p><p>? We propose an end-to-end framework for universal information extraction -InstructUIE, which leverages natural language instructions to guide large language models for IE tasks.</p><p>? We introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions.</p><p>? Experimental results demonstrate that Instruc-tUIE achieves comparable performance to Bert in a supervised setup. Notably, our method significantly outperforms the current state-of-theart and GPT-3.5 in a zero-shot setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we first briefly introduce the setup of instruction tuning. Then, we discuss the task metainformation schema and how IE tasks are mapped into our schema. Next, we discuss the framework of InstructUIE, which consists of two major parts: task schema and auxiliary tasks. Finally, we explain how IE INSTRUCTION is constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instruction Tuning Background</head><p>Instruction tuning is a multi-task learning framework that enables the use of human-readable instructions to guide the output of LLMs. Given a source text and task-specific instructions, the model is trained to generate a sequence of tokens representing the desired output structure and its corresponding labels. In a supervised setup, the instructions are provided during training for all tasks, and the model is fine-tuned on a set of labeled data for each task. This allows the model to learn task-specific features and optimize for each task. In a zero-shot setup, the instructions are only provided for a subset of tasks during training, and the model is evaluated on unseen tasks without additional fine-tuning. This requires the model to generalize across tasks and use the shared features learned from the instruction tuning framework to infer the output structures for new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework</head><p>In this section, we discuss the task metainformation schema and how IE tasks are mapped into our schema. Next, we propose auxiliary tasks, which enable the model to capture common structure information and deepen the understanding of diverse semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Task Schema</head><p>To better transfer and utilize the knowledge learned in pre-trained language models, we reformulate the IE tasks to the seq2seq form and solve it through fine-tuning LLMs, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Every task instance is formatted with four properties: task instruction, options, text, and output.</p><p>Task Instruction provides a detailed guide on how to extract the relevant information from the input text and produce the desired output structure. It includes information such as the type of information to be extracted, the format of the output structure, and any additional constraints or rules that need to be followed during the extraction process. The task instruction acts as a bridge between the raw input text and the structured output representation, enabling the model to understand the extraction task and generate accurate and meaningful output. In Table <ref type="table">8</ref> in the Appendix we present the list of instructions for each task.</p><p>Options are the output label constraints for a task, which represent the set of possible outputs that can be generated by the model for a given input. These label constraints are specific to each task and provide information on how to map the predicted outputs to the corresponding semantic concepts. For instance, in NER, options could be entity tags such as person, organization, location, or miscellaneous. Similarly, in RE, options could represent the types of relations that can be extracted, such as "works for", "born in", "married to", and so on. In EE, options could represent the event tags that correspond to different types of events, such as "beginning", "end", "occurring", "ceasing", and so on. The options provide a structured output space for the model, allowing it to generate outputs that are consistent with the underlying semantic structure of the task.</p><p>Text is the input sentence of a task instance. This sequence is then fed into the pre-trained language model along with the task instruction and options, enabling the model to generate the desired output sequence for the given task.</p><p>Output is the sentence converted from the original tags of the sample. Specifically, for NER, the output format is "entity tag: entity span". For RE, the output format is "relationship: head entity, tail entity". For EE, the output format is "event tag: trigger word, argument tag: argument span". In cases where the input does not contain structural information that matches any of the provided options, we assign a value of "None" to the corresponding output sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Auxiliary Tasks</head><p>To boost the performance in a more fine-grained level, we further design auxiliary tasks to be trained in conjunction with the main task. The auxiliary tasks provide additional information that complements the main task, enabling the model to capture common structures better and deepen the understanding of diverse semantics.</p><p>For the named entity recognition task, we introduce a span extraction task and an entity typing task. The span extraction task is designed to extract the entity span from the input sentence, while the entity typing task is aimed at identifying the type of entity.</p><p>For the relation extraction task, we have introduced an entity pair extraction task and a relation classification task. The entity pair extraction task aims to extract the entity pairs involved in the relationship, while the relation classification task is designed to classify the type of relationship between the entity pairs.</p><p>For the event extraction task, we have introduced a trigger extraction task and an argument extraction task. The trigger extraction task is designed to extract the trigger word that triggers the event, while the argument extraction task aims to extract the associated arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">IE INSTRUCTIONS</head><p>IE INSTRUCTIONS collects 32 publicly available datasets covering three types of IE tasks: NER, RE, and EE. To ensure the diversity of the datasets, we include corpora from various domains, such as science, healthcare, social media, and transportation, in addition to general-domain sources, such as news and Wikidata. Figure <ref type="figure" target="#fig_2">3</ref> shows the breakdown of the benchmark by task, domain, and size. For detailed dataset statistics and train/test split methods, please refer to Appendix Table <ref type="table" target="#tab_7">7</ref>.</p><p>We carry out the following data processing steps: (1) To address the issue of inconsistent label schemas across different tasks, we unify the names of labels with identical semantics but different names in various datasets. (2) To better test the semantic understanding capabilities of the LLM, we convert labels with underscores, abbreviations, or special formats into natural language formats. For example, we renamed the label "people person place_of_birth" to "place of birth." (3) Following the guidelines outlined in the section 2.2.1, we transform all datasets into a text-to-text format, which ensures a consistent representation of the input-output pairs across all tasks.</p><p>Our benchmark provides a standardized evaluation platform for LLMs' performance on IE tasks. This will facilitate a more accurate comparison of various models and contribute to the development of more effective and robust models for IE tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Baselines</head><p>We compare the proposed InstructUIE with the following strong baseline models:</p><p>? UIE <ref type="bibr" target="#b36">(Lu et al., 2022</ref>) is a unified text-tostructure generation framework that can universally model different IE tasks and adaptively generate targeted structures;</p><p>? USM <ref type="bibr" target="#b34">(Lou et al., 2023</ref>) is a unified IE tasks framework, which converts IE tasks to a semantic matching problem;</p><p>? Bert <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, which are widely used as text encoders for various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Evaluation Metrics</head><p>We use span-based offset Micro-F1 as the primary metric to evaluate the model. For NER task, we follow a span-level evaluation setting, where the entity boundary and entity type must be correctly predicted. For RE task, a relation triple is correct if the model correctly predicts the boundaries of the subject entity, the object entity, and the entity relation. For EE task, we report two evaluation metrics: (1) Event Trigger: an event trigger is correct if the event type and the trigger word are correctly predicted. (2) Event Argument: an event argument is correct if its role type and event type match a reference argument mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Results</head><p>Tabel 1, Tabel 2 and 3 show the performance of different models for the NER, RE, and EE tasks.   <ref type="bibr">et al., 2020)</ref>, MIT Movie Review, and MIT Restaurant Review <ref type="bibr" target="#b31">(Liu et al., 2019)</ref> to test the zero-shot capability of the model. For RE task, we test the zero-shot capability on FewRel <ref type="bibr" target="#b12">(Han et al., 2018)</ref> and Wiki-ZSL <ref type="bibr" target="#b2">(Chen and Li, 2021)</ref>. For FewRel and Wiki-ZSL data sets, we follow the previous work <ref type="bibr" target="#b6">(Chia et al., 2022)</ref> and randomly select 5 unseen labels which do not appear in the training set as the test set. In order to reduce the effect of experimental noise, the unseen label selection process is repeated for five different random seeds to produce the test set. Since the training and testing tasks do not overlap at all and across various domains as well, this setting is challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Baselines</head><p>For zero-shot Named Entity Recognition and Relational Extraction, we compare InstructUIE with the following strong baselines:</p><p>? ZETT <ref type="bibr" target="#b49">(Kim et al., 2022)</ref> is a novel framework based on end-to-end generative transformers and outperform previous state-of-the-art models;</p><p>? ChatGPT <ref type="bibr" target="#b40">(Ouyang et al., 2022)</ref> is also called GPT-3.5-turbo, which is the most capable GPT-3.5 model and optimized for chat;</p><p>? UIE and USM have been introduced in 3.1.2. For the NER task, we can observe that Instruc-tUIE outperforms the current sota model USM in Micro-F1 score on all the datasets except Cross-NER_Literature, ranging from 5.21% to 25.27%. For example, compared with the USM model, InstructUIE performs over 20 points better on the MIT Movie Review dataset and the CrossNER_AI dataset. Noted that USM is trained on the same task corpus and tested on the label held out, while our model has never seen the task corpus. For the RE task, under the setting of 5 unseen labels, InstructUIE outperforms the current sota model ZETT on both the FewRel and Wiki-ZSL datasets by 5.84% and 3.46% respectively. When compared to the GPT series model, In-structUIE significantly outperforms Davinci for the NER task but still falls some way short of Chatgpt's results for the NER task. However, for the RE task, our model performs much better than these two GPT series models. Both Davinci and Chatgpt perform poorly, especially with Davinci completely unable to output correct results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4 and</head><p>It is worth mentioning that since Chatgpt is not open source, we have no way of knowing whether the model has seen the two data sets used by the zero-shot setting during training, and we think the huge difference in results for NER and RE tasks may be due to this reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Instruction Tuning</head><p>Instruction tuning <ref type="bibr" target="#b38">(Mishra et al., 2022;</ref><ref type="bibr">Wang et al., 2022c;</ref><ref type="bibr" target="#b33">Longpre et al., 2023)</ref>, a novel paradigm that leverages natural language instructions to guide large language models for downstream tasks, shows tremendous promise in generalization within the set of observed tasks. Most recent work <ref type="bibr">(Wang et al., 2022c;</ref><ref type="bibr" target="#b33">Longpre et al., 2023)</ref> on instruction tuning has focused on general NLP tasks such as question answering and text classification, but not specifically on IE tasks. While some work such as <ref type="bibr">(Wang et al., 2022a;</ref><ref type="bibr" target="#b43">Parmar et al., 2022)</ref> includes a few IE tasks, those tasks do not provide good coverage of IE tasks and domains. No prior work has examined how training a model on a wide range of IE tasks with various instructions. In this paper, we propose a unified framework for information extraction that involves auxiliary task design as well as specific tuning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Information Extraction</head><p>Information extraction is fundamental in natural language processing systems, aiming to extract structured information from unstructured or semistructured data sources automatically. Traditional methods <ref type="bibr">(Wang et al., 2022b;</ref><ref type="bibr" target="#b66">Yan et al., 2021;</ref><ref type="bibr" target="#b15">Huguet Cabot and Navigli, 2021;</ref><ref type="bibr" target="#b65">Xie et al., 2021)</ref> for IE typically require the design of specific architectures for different IE tasks, and the models are trained separately. However, training dedicated models for different IE tasks requires a significant amount of labeled data, which can be costly and time-consuming to obtain. Secondly, knowledge learned from one IE task cannot be easily applied to another task, even if the tasks have similar characteristics. Recently, <ref type="bibr" target="#b36">Lu et al. (2022)</ref> proposed UIE, which uniformly encodes different extraction structures via a structured extraction language and captures the common IE abilities via a large-scale pre-trained text-to-structure model. However, UIE requires separate finetune for different downstream tasks. This lead to the poor performance of UIE in low resource settings or facing new label schema. <ref type="bibr" target="#b34">Lou et al. (2023)</ref> proposed USM, which decouples IE into two basic tasks, token-token linking and label-token linking. Unfortunately, USM requires semantic matching for each word, which leads to a significant increase in training and inference time. InstructUIE addresses these challenges by utilizing instructive guidance to direct pre-trained large models toward the task, facilitating the efficient and adaptive generation of target structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose an end-to-end framework for universal information extraction -InstructUIE, which leverages natural language instructions to guide large language models for IE tasks. We further introduce a new benchmark dataset. The benchmark consists of 32 diverse information extraction datasets that have been unified into a text-to-text format, allowing for a consistent and standardized evaluation of various IE tasks. Experimental results demonstrate that InstructUIE achieves state-of-the-art results under supervised and zero settings and solves massive tasks using a single multi-task model.</p><p>GENIA <ref type="bibr">(Kim et al., 2003a)</ref> and PHEE <ref type="bibr" target="#b49">(Sun et al., 2022)</ref> are used.</p><p>For the data set with only training set as the original data, we divided it into training set, validation set and test set according to the ratio of 8:1:1. For the data set with only training set and validation set as the original data, we randomly select half of the data in the validation set as the test set and the other half as the new validation set. For other datasets, we adopt the official split.</p><p>Tabel 7 shows detailed datasets statistics. NER refers to Named Entity Recognition task, RE refers to Relation Extraction task, and EE refers to Event Extraction task. |Labels| indicates the number of labels, and # is the number of sentences in the specific subset. For the |Labels| of event extraction, the number outside the parenthesis indicates the number of event types and the number inside the parenthesis indicates the number of argument types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Instruction Details</head><p>Table <ref type="table">8</ref> shows prompts for different tasks. NER refers to the named entity recognition task, the object of which is the entity in the output sentence and its corresponding entity type. RE refers to the relation extraction task, the object of which is to extract the relation triplet in the sentence, including the relation name, the head entity and the tail entity. EE refers to the event extraction task. The task objective is to extract the event types, trigger word and arguments in the sentence. ES refers to entity span, the task target is given sentence and entity category options, and output entities that conform to the entity category, but there is no need to output the entity type of each entity; ET refers to entity type identification. The task target is a given sentence, which contains entity and entity category options, and outputs the entity category corresponding to each entity. EP refers to entity pair identification (entity pair). The task target is given sentence and relation category options, and output entity pairs that conform to relation category, but do not need to output its relation category; EPR refers to entity pair relationship identification. The task target is a given sentence, which contains entity pair and relationship category options, and outputs the corresponding relationship category for each entity pair. ES and ET are auxiliary tasks of NER, EP and EPR are auxiliary tasks of RE, and EEA and EET are auxiliary tasks of EE. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of 3 different paradigms for solving unified information extraction task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FindFigure 2 :</head><label>2</label><figDesc>Figure 2: The overview framework of InstructUIE. The input consists of task instructions, options, and text. The output is a more understandable sentence converted from the original label structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of IE INSTRUCTIONS.</figDesc><graphic url="image-1.png" coords="4,72.00,70.87,215.99,173.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overall results of InstructUIE on NER task. The evaluation metric is Entity F1. For 20 NER datasets, InstructUIE outperforms the Bert model on 17 of them.</figDesc><table><row><cell>Dataset</cell><cell cols="4">UIE USM Bert-base Ours</cell></row><row><cell>ACE2005</cell><cell cols="2">85.78 87.14</cell><cell>87.30</cell><cell>86.66</cell></row><row><cell>AnatEM</cell><cell>-</cell><cell>-</cell><cell>85.82</cell><cell>90.89</cell></row><row><cell>bc2gm</cell><cell>-</cell><cell>-</cell><cell>80.90</cell><cell>85.16</cell></row><row><cell>bc4chemd</cell><cell>-</cell><cell>-</cell><cell>86.72</cell><cell>90.30</cell></row><row><cell>bc5cdr</cell><cell>-</cell><cell>-</cell><cell>85.28</cell><cell>89.59</cell></row><row><cell>broad twitter</cell><cell>-</cell><cell>-</cell><cell>58.61</cell><cell>83.14</cell></row><row><cell>CoNLL2003</cell><cell cols="2">92.99 93.16</cell><cell>92.40</cell><cell>92.94</cell></row><row><cell>FabNER</cell><cell>-</cell><cell>-</cell><cell>64.20</cell><cell>76.20</cell></row><row><cell>FindVehicle</cell><cell>-</cell><cell>-</cell><cell>87.13</cell><cell>89.47</cell></row><row><cell>GENIA-Ent</cell><cell>-</cell><cell>-</cell><cell>73.3</cell><cell>74.71</cell></row><row><cell>HarveyNER</cell><cell>-</cell><cell>-</cell><cell>82.26</cell><cell>88.79</cell></row><row><cell>MIT Movie</cell><cell>-</cell><cell>-</cell><cell>88.78</cell><cell>89.01</cell></row><row><cell>MIT Restaurant</cell><cell>-</cell><cell>-</cell><cell>81.02</cell><cell>82.55</cell></row><row><cell>multiNERD</cell><cell>-</cell><cell>-</cell><cell>91.25</cell><cell>92.32</cell></row><row><cell>ncbi-disease</cell><cell>-</cell><cell>-</cell><cell>80.20</cell><cell>90.23</cell></row><row><cell>Ontonotes</cell><cell>-</cell><cell>-</cell><cell>91.11</cell><cell>90.19</cell></row><row><cell>polyglot-NER</cell><cell>-</cell><cell>-</cell><cell>75.65</cell><cell>70.15</cell></row><row><cell>tweetNER7</cell><cell>-</cell><cell>-</cell><cell>56.49</cell><cell>64.97</cell></row><row><cell>wikiann</cell><cell>-</cell><cell>-</cell><cell>70.60</cell><cell>85.13</cell></row><row><cell>wikineural</cell><cell>-</cell><cell>-</cell><cell>82.78</cell><cell>91.36</cell></row><row><cell>Avg</cell><cell>-</cell><cell>-</cell><cell>80.09</cell><cell>85.19</cell></row><row><cell cols="2">3 Experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">This section conducted extensive experiments un-</cell></row><row><cell cols="5">der supervised and zero-shot settings to validate</cell></row><row><cell cols="5">the effectiveness of InstructUIE. We select 11B</cell></row><row><cell cols="5">FlanT5 (Chung et al., 2022) as our backbone model</cell></row><row><cell cols="5">because prior research (Longpre et al., 2023) has</cell></row><row><cell cols="5">demonstrated that models fine-tuned on instruction-</cell></row><row><cell cols="5">based tasks offer a computationally efficient start-</cell></row><row><cell cols="5">ing point for new tasks. The details of the exper-</cell></row><row><cell cols="5">imental setup, datasets, and comparison methods</cell></row><row><cell cols="4">are described in the following parts.</cell><cell></cell></row><row><cell cols="5">3.1 Experiments on Supervised Settings</cell></row><row><cell>3.1.1 Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">We conduct supervised experiments on IE IN-</cell></row><row><cell cols="5">STRUCTIONS, including three tasks (named entity</cell></row><row><cell cols="5">extraction, relation extraction, and event extrac-</cell></row><row><cell cols="5">tion). Details of the dataset splitting methods and</cell></row><row><cell cols="4">statistics can be found in Appendix 6.1.</cell><cell></cell></row><row><cell cols="5">To balance the dataset, we apply a sampling</cell></row><row><cell cols="5">strategy (Poolsawad et al., 2014). Specifically,</cell></row><row><cell cols="5">we sample 10,000 examples for each dataset and</cell></row><row><cell cols="5">include all examples for datasets with fewer than</cell></row><row><cell>10,000 samples.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overall results of InstructUIE on RE task. The evaluation metric is Relation Strict F1. Our model reaches an average F1 of 67.98% on the eight datasets of the RE task and is comparable to the baseline.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Overall results of InstructUIE on EE task.</figDesc><table><row><cell>Dataset</cell><cell cols="4">UIE USM Bert-base Ours</cell></row><row><cell cols="3">ACE2005 73.36 72.41</cell><cell>72.5</cell><cell>77.13</cell></row><row><cell>CASIE</cell><cell cols="2">69.33 71.73</cell><cell>68.98</cell><cell>67.80</cell></row><row><cell>PHEE</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.14</cell></row><row><cell>Avg</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.69</cell></row><row><cell></cell><cell cols="3">a. Event Trigger F1</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">UIE USM Bert-base Ours</cell></row><row><cell cols="3">ACE2005 54.79 55.83</cell><cell>59.9</cell><cell>72.94</cell></row><row><cell>CASIE</cell><cell cols="2">61.30 63.26</cell><cell>60.37</cell><cell>63.53</cell></row><row><cell>PHEE</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.91</cell></row><row><cell>Avg</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.46</cell></row><row><cell></cell><cell cols="3">b. Event Argument F1</cell><cell></cell></row><row><cell cols="5">The evaluation metric is Event Trigger F1 and Event</cell></row><row><cell cols="5">Argument F1. Our model outperformed USM and UIE</cell></row><row><cell cols="2">on some datasets.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Named Entity Recognition Our model achieves</cell></row><row><cell cols="5">an average F1 score of 85.19% on 20 NER datasets,</cell></row><row><cell cols="5">surpassing Bert's 80.09%. The best performance</cell></row><row><cell cols="5">is on the CoNLL2003 dataset, where InstructUIE</cell></row><row><cell cols="5">achieved an F1 score of 92.94%. For 20 NER data</cell></row><row><cell cols="5">sets, InstructUIE outperforms the Bert model on</cell></row><row><cell cols="5">17 of them. Among them, our model outperforms</cell></row><row><cell cols="5">Bert by more than 5 points on eight datasets. The</cell></row><row><cell cols="5">dataset with the biggest gap is the broad twitter</cell></row><row><cell cols="5">dataset, where InstructUIE outperforms Bert by</cell></row><row><cell cols="2">about 25 points.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">In the ACE2005, Ontonotes, and Polyglot-NER</cell></row><row><cell cols="5">datasets, our model performs slightly worse than</cell></row><row><cell cols="5">Bert. We speculate that this is due to our strat-</cell></row><row><cell cols="5">egy of sampling only 10,000 training examples</cell></row><row><cell cols="5">for each dataset. The original corpora for these</cell></row><row><cell cols="5">three datasets contain a larger number of training</cell></row><row><cell cols="5">examples, such as 420,000 for Polyglot-NER, of</cell></row><row><cell cols="5">which we only used around 20%. The detailed</cell></row><row><cell cols="5">number of training sets for all datasets can be seen</cell></row><row><cell cols="2">in the appendix.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Compared with UIE and USM, our model</cell></row><row><cell cols="5">also achieves comparable results on ACE2005</cell></row><row><cell cols="5">and CoNLL2003, which are two commonly used</cell></row><row><cell cols="5">datasets. Due to the UIE and USM only test</cell></row><row><cell cols="5">their modelson a small number of commonly used</cell></row><row><cell cols="5">datasets, we are unable to compare our model with</cell></row><row><cell cols="4">these two models on other datasets.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Micro-F1 scores of zero-shot NER on 7 datasets. The best results are in bold. InstructUIE outperforms SOTA by a wide margin on most datasets ranging from 5.21% to 25.27%.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="2">FewRel Wiki-ZSL</cell></row><row><cell>Baselines</cell><cell>ZET T T 5-small ZET T T 5-base</cell><cell>30.53 33.71</cell><cell>31.74 31.17</cell></row><row><cell>Ours</cell><cell>InstructUIE</cell><cell>39.55</cell><cell>35.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Micro-F1 scores of zero-shot RE on FewRel and Wiki-ZSL. The best results are in bold. InstructUIE outperforms SOTA on both datasets.</figDesc><table><row><cell>Relational Extraction Our model reaches an</cell></row><row><cell>average F1 of 67.98% on the eight datasets of the</cell></row><row><cell>RE task, among which the NYT data set reaches</cell></row><row><cell>90.47% F1 score. Among the eight datasets,</cell></row><row><cell>CoNLL2004 and SciERC datasets are also tested</cell></row><row><cell>by UIE and USM models. We focus on the analysis</cell></row><row><cell>of the results of these two datasets. For the SciERC</cell></row><row><cell>dataset, InstructUIE significantly outperforms UIE</cell></row><row><cell>and USM by 8.62% and 7.79% respectively. For</cell></row><row><cell>the CoNLL2004 dataset, InstructUIE outperforms</cell></row><row><cell>UIE by more than three points, and lag USM by</cell></row><row><cell>less than 0.5%. Moreover, noted that as BERT is</cell></row><row><cell>usually used for relation classification tasks rather</cell></row><row><cell>than relation extraction. Therefore, we did not use</cell></row><row><cell>this baseline in the RE task.</cell></row><row><cell>Event Extraction Our model achieve sota on all</cell></row><row><cell>datasets except for the Event Trigger F1 metric of</cell></row><row><cell>the CASIE dataset. On the Event Trigger F1 metric,</cell></row><row><cell>InstructUIE reaches an average of 71.69% on these</cell></row><row><cell>three datasets, with ACE2005 reaching 77.13%,</cell></row><row><cell>significantly surpassing UIE's 73.36%, USM's</cell></row><row><cell>72.41% and Bert's 72.5%. On the Event Argument</cell></row><row><cell>F1 metric, InstructUIE beats three baseline models</cell></row><row><cell>to reach sota on all three datasets. In particular,</cell></row><row><cell>ACE2005 dataset reaches 72.94%, 18 points higher</cell></row><row><cell>than the UIE and 17 points higher than the USM.</cell></row><row><cell>3.2 Experiments on Zero-shot Settings</cell></row><row><cell>3.2.1 Dataset</cell></row></table><note><p>To evaluate InstructUIE's zero-shot performance, we train the model on 18 NER datasets and 6 RE datasets and test it on 7 NER datasets and 2 RE datasets. Specifically, we eliminate the datasets for zero-shot experimental testing during the training phase. For the NER task, We use five CrossNER subsets(AI, literature, music, politics, science) (Liu</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Table5show the performance of NER and RE tasks under the zero-shot setting. Micro-F1 scores of davinci and chatgpt under zero-shot setting.</figDesc><table><row><cell cols="3">Model Movie Restaurant</cell><cell>AI</cell><cell cols="6">Literature Music Politics Science FewRel Wiki-ZSL</cell></row><row><cell>davinci</cell><cell>0.84</cell><cell>2.94</cell><cell>2.97</cell><cell>9.87</cell><cell>13.83</cell><cell>18.42</cell><cell>10.04</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell cols="2">chatgpt 41.00</cell><cell>37.76</cell><cell>54.40</cell><cell>54.07</cell><cell>61.24</cell><cell>59.12</cell><cell>63.00</cell><cell>9.96</cell><cell>13.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Detailed datasets statistics.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell cols="2">|labels| #Train</cell><cell>#Val</cell><cell>#Test</cell></row><row><cell></cell><cell>ACE2004</cell><cell>7</cell><cell>6202</cell><cell>745</cell><cell>812</cell></row><row><cell></cell><cell>ACE2005</cell><cell>7</cell><cell>7299</cell><cell>971</cell><cell>1060</cell></row><row><cell></cell><cell>broad_twitter_corpus</cell><cell>3</cell><cell>5334</cell><cell>2000</cell><cell>2001</cell></row><row><cell></cell><cell>CoNLL2003</cell><cell>4</cell><cell>14041</cell><cell>3250</cell><cell>3453</cell></row><row><cell></cell><cell>multiNERD</cell><cell>16</cell><cell cols="3">134144 10000 10000</cell></row><row><cell></cell><cell>Ontonotes</cell><cell>18</cell><cell>59924</cell><cell>8528</cell><cell>8262</cell></row><row><cell></cell><cell>polyglot-NER</cell><cell>3</cell><cell cols="3">393982 10000 10000</cell></row><row><cell></cell><cell>tweetNER7</cell><cell>7</cell><cell>7111</cell><cell>886</cell><cell>576</cell></row><row><cell></cell><cell>wikiann</cell><cell>3</cell><cell cols="3">20000 10000 10000</cell></row><row><cell></cell><cell>wikineural</cell><cell>3</cell><cell cols="3">92720 11590 11597</cell></row><row><cell></cell><cell>AnatEM</cell><cell>1</cell><cell>5861</cell><cell>2118</cell><cell>3830</cell></row><row><cell></cell><cell>bc2gm</cell><cell>1</cell><cell>12500</cell><cell>2500</cell><cell>5000</cell></row><row><cell>NER</cell><cell>bc4chemd bc5cd</cell><cell>1 2</cell><cell cols="3">30682 30639 26364 4560 4581 4797</cell></row><row><cell></cell><cell>CrossNER_AI</cell><cell>14</cell><cell>100</cell><cell>350</cell><cell>431</cell></row><row><cell></cell><cell>CrossNER_literature</cell><cell>12</cell><cell>100</cell><cell>400</cell><cell>416</cell></row><row><cell></cell><cell>CrossNER_music</cell><cell>13</cell><cell>100</cell><cell>380</cell><cell>465</cell></row><row><cell></cell><cell>CrossNER_politics</cell><cell>9</cell><cell>199</cell><cell>540</cell><cell>650</cell></row><row><cell></cell><cell>CrossNER_science</cell><cell>17</cell><cell>200</cell><cell>450</cell><cell>543</cell></row><row><cell></cell><cell>FabNER</cell><cell>12</cell><cell>9435</cell><cell>2182</cell><cell>2064</cell></row><row><cell></cell><cell>FindVehicle</cell><cell>21</cell><cell cols="3">21565 20777 20777</cell></row><row><cell></cell><cell>GENIA</cell><cell>5</cell><cell>15023</cell><cell>1669</cell><cell>1854</cell></row><row><cell></cell><cell>HarveyNER</cell><cell>4</cell><cell>3967</cell><cell>1301</cell><cell>1303</cell></row><row><cell></cell><cell>MIT Movie Review</cell><cell>12</cell><cell>9774</cell><cell>2442</cell><cell>2442</cell></row><row><cell></cell><cell>MIT Restaurant Review</cell><cell>8</cell><cell>7659</cell><cell>1520</cell><cell>1520</cell></row><row><cell></cell><cell>ncbi-disease</cell><cell>1</cell><cell>5432</cell><cell>923</cell><cell>940</cell></row><row><cell></cell><cell>ADE corpus</cell><cell>1</cell><cell>3417</cell><cell>427</cell><cell>428</cell></row><row><cell></cell><cell>CoNLL2004</cell><cell>5</cell><cell>922</cell><cell>231</cell><cell>288</cell></row><row><cell></cell><cell>GIDS</cell><cell>4</cell><cell>8526</cell><cell>1417</cell><cell>4307</cell></row><row><cell></cell><cell>kbp37</cell><cell>18</cell><cell>15917</cell><cell>1724</cell><cell>3405</cell></row><row><cell>RE</cell><cell>NYT NYT11 HRL</cell><cell>24 12</cell><cell>56196 62648</cell><cell>5000 149</cell><cell>5000 369</cell></row><row><cell></cell><cell>SciERC</cell><cell>7</cell><cell>1366</cell><cell>187</cell><cell>397</cell></row><row><cell></cell><cell>semeval RE</cell><cell>10</cell><cell>6507</cell><cell>1493</cell><cell>2717</cell></row><row><cell></cell><cell>ACE2005</cell><cell>33(22)</cell><cell>3342</cell><cell>327</cell><cell>293</cell></row><row><cell>EE</cell><cell>CASIE GENIA</cell><cell>5(26) 5(0)</cell><cell>3751 15023</cell><cell>788 1669</cell><cell>1500 1854</cell></row><row><cell></cell><cell>PHEE</cell><cell>2(16)</cell><cell>2898</cell><cell>961</cell><cell>968</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The dataset, code, and models can be found at https://github.com/BeyonderXX/InstructUIE</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NER</head><p>Please list all entity words in the text that fit the category. Output format is "type1: word1; type2: word2".</p><p>Please find all the entity words associated with the category in the given text. Output format is "type1: word1; type2: word2".</p><p>Please tell me all the entity words in the text that belong to a given category. Output format is "type1: word1; type2: word2".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RE</head><p>Given a phrase that describes the relationship between two words, extract the words and the lexical relationship between them. The output format should be "relation1: word1, word2; relation2: word3, word4".</p><p>Find the phrases in the following sentences that have a given relationship. The output format is "relation1: word1, word2; relation2: word3, word4". Given a sentence, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of "relation1: word1, word2; relation2: word3, word4".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EE</head><p>Locate the role in the text that participated in the event based on the event type and return it in the event list.</p><p>Extract the event information in the text and return them in the event list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ES</head><p>Please list all entity words in the text that fit the category. Output format is word1, word2.</p><p>ET Given options, please tell me the categories of all the listed entity words.Output format is "type1: word1; type2: word2".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EP</head><p>Please list all entity pairs containing a certain relationship in the given options.Output format is "word1, word2; word3, word4".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EPR</head><p>Given options, please tell me the relationships of all the listed entity pairs.Output format is "relation1: word1, word2; relation2: word3, word4".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEA</head><p>Given event type and trigger, please tell me the arguments of all the listed option. Output format is "name: role".</p><p>EET Please tell me event type and its trigger word from given type options. Output format is "event type: trigger".  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">POLYGLOT-NER: massive multilingual named entity recognition</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno>CoRR, abs/1410.3791</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Zs-bert: Towards zero-shot relation extraction with attribute representation learning</title>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Te</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2022a. Crossroads, buildings and neighborhoods: A dataset for fine-grained location recognition</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3329" to="3339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2022b. Crossroads, buildings and neighborhoods: A dataset for fine-grained location recognition</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3329" to="3339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xuanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.00293</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relationprompt: Leveraging prompts to generate synthetic data for zero-shot relation triplet extraction</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><surname>Si ; Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Dehghani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
	</analytic>
	<monogr>
		<title level="m">Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Hyung Won Chung</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Broad Twitter corpus: A diverse named entity recognition resource</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1169" to="1179" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: A resource for disease name recognition and concept normalization</title>
		<author>
			<persName><forename type="first">Rezarta</forename><surname>Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Findvehicle and vehiclefinder: A ner dataset for a text-image cross-modal vehicle retrieval system</title>
		<author>
			<persName><forename type="first">Runwei</forename><surname>Guan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drugrelated adverse effects from medical case reports</title>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Mateen Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2012.04.008</idno>
	</analytic>
	<monogr>
		<title level="m">Text Mining and Natural Language Processing in Pharmacogenomics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="885" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><surname>Szpakowicz</surname></persName>
		</author>
		<editor>*SEMEVAL</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ontonotes: The 90% solution</title>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>In North American Chapter of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">REBEL: Relation extraction by end-to-end language generation</title>
		<author>
			<persName><forename type="first">Pere-Llu?s</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cabot</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.204</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2370" to="2381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving distantly supervised relation extraction using word and entity based attention</title>
		<author>
			<persName><forename type="first">Sharmistha</forename><surname>Jat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhesh</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<idno>ArXiv, abs/1804.06987</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Estevam Hruschka, and Ndapa Nakashole. 2022. Zero-shot triplet extraction by template infilling</title>
		<author>
			<persName><forename type="first">Bosung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayate</forename><surname>Iso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Bhutani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">2003a. Genia corpus -a semantically annotated corpus for bio-textmining</title>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="182" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Yuka Tateisi, and Jun&apos;ichi Tsujii. 2003b. Genia corpus-a semantically annotated corpus for bio-textmining</title>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btg1023</idno>
	</analytic>
	<monogr>
		<title level="m">Bioinformatics</title>
		<meeting><address><addrLine>Oxford, England</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note>Suppl 1:i180-2</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Biomedical named entity recognition at scale</title>
		<author>
			<persName><forename type="first">Veysel</forename><surname>Kocaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Talby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR Workshops</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Biomedical named entity recognition at scale</title>
		<author>
			<persName><forename type="first">Veysel</forename><surname>Kocaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Talby</surname></persName>
		</author>
		<idno>CoRR, abs/2011.06315</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Biomedical named entity recognition at scale</title>
		<author>
			<persName><forename type="first">Veysel</forename><surname>Kocaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Talby</surname></persName>
		</author>
		<idno>CoRR, abs/2011.06315</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">2022a. Accurate clinical and biomedical named entity recognition at scale. Software Impacts</title>
		<author>
			<persName><forename type="first">Veysel</forename><surname>Kocaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Talby</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.simpa.2022.100373</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">100373</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">2022b. Accurate clinical and biomedical named entity recognition at scale. Software Impacts</title>
		<author>
			<persName><forename type="first">Veysel</forename><surname>Kocaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Talby</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.simpa.2022.100373</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">100373</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The chemdner corpus of chemicals and drugs and its annotation principles</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Obdulia</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Sayle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riza</forename><surname>Batista-Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Rak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?rgio</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfonso</forename><surname>Valencia</surname></persName>
		</author>
		<idno type="DOI">10.1186/1758-2946-7-S1-S2</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">S2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">2021a. &quot;fabner&quot;: information extraction from manufacturing process science domain literature using named entity recognition</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binil</forename><surname>Starly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Manufacturing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2393" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2021b. &quot;fabner&quot;: information extraction from manufacturing process science domain literature using named entity recognition</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binil</forename><surname>Starly</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10845-021-01807-x</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Manufacturing</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SUNNYNLP at SemEval-2018 task 10: A supportvector-machine-based method for detecting semantic difference using taxonomy and word embedding features</title>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwong</forename><surname>Sak Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><surname>Leung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S18-1118</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation</title>
		<meeting>the 12th International Workshop on Semantic Evaluation<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="741" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database: The Journal of Biological Databases and Curation</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dice loss for dataimbalanced NLP tasks</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1911.02855</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">GCDT: A global context enhanced deep transition architecture for sequence labeling</title>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1906.02437</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Crossner: Evaluating crossdomain named entity recognition</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The flan collection: Designing data and methods for effective instruction tuning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<idno>ArXiv, abs/2301.13688</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Universal information extraction as unified semantic matching</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Text2event: Controllable sequence-tostructure generation for end-to-end event extraction</title>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoyi</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.09232</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unified structure generation for universal information extraction</title>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-task generalization via natural language crowdsourcing instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.244</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3470" to="3487" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno>ArXiv, abs/2303.08774. openbiocorpora. 2015</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>openbiocorpora anatem</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Francis Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Lowe</surname></persName>
		</author>
		<idno>ArXiv, abs/2203.02155</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">In-BoXBART: Get instructions into biomedical multitask learning</title>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirali</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murad</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-naacl.10</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<publisher>United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="112" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Balancing class for performance of classification with a clinical dataset</title>
		<author>
			<persName><surname>Poolsawad</surname></persName>
		</author>
		<author>
			<persName><surname>Kambhampati</surname></persName>
		</author>
		<author>
			<persName><surname>Cleland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the World Congress on Engineering</title>
		<meeting>the World Congress on Engineering</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Tjong Kim Sang and Fien De Meulder</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">FLERT: document-level features for named entity recognition</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<idno>CoRR, abs/2011.06993</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Phee: A dataset for pharmacovigilance event extraction from text</title>
		<author>
			<persName><forename type="first">Zhao-Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bino</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<idno>ArXiv, abs/2210.12560</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DoSEA: A domain-specific entity-aware framework for cross-domain named entity recogition</title>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiexi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Minghao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongquan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxiu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpeng</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2022</date>
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
	<note>AAAI Conference on Artificial Intelligence. International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">WikiNEuRal: Combined neural and knowledge-based silver data creation for multilingual NER</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Tedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentino</forename><surname>Maiorca</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.215</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2521" to="2533" />
		</imprint>
	</monogr>
	<note>Niccol? Campolungo, Francesco Cecconi, and Roberto Navigli. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MultiN-ERD: A multilingual, multi-genre and fine-grained dataset for named entity recognition (and disambiguation)</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Tedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-naacl.60</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="801" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MultiN-ERD: A multilingual, multi-genre and fine-grained dataset for named entity recognition (and disambiguation)</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Tedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-naacl.60</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="801" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">An information extraction study: Take in mind the tokenization!</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Theodoropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">T-NER: An all-round python library for transformerbased named entity recognition</title>
		<author>
			<persName><forename type="first">Asahi</forename><surname>Ushio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-demos.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics: System Demonstrations. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Named entity recognition in twitter: A dataset and analysis on short-term temporal shifts</title>
		<author>
			<persName><forename type="first">Asahi</forename><surname>Ushio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitor</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Named entity recognition in twitter: A dataset and analysis on short-term temporal shifts</title>
		<author>
			<persName><forename type="first">Asahi</forename><surname>Ushio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitor</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Walker and Linguistic Data Consortium</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACE 2005 Multilingual Training Corpus. LDC corpora. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Deepstruct: Pretraining of language models for structure prediction</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">2022a. Instructionner: A multi-task instructionbased generative framework for few-shot ner</title>
		<author>
			<persName><forename type="first">Liwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><forename type="middle">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<idno>ArXiv, abs/2203.03903</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">MINER: Improving out-of-vocabulary named entity recognition from an information theoretic perspective</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihan</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.383</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5590" to="5600" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">2021a. Automated concatenation of embeddings for structured prediction</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2643" to="2660" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">2021b. UniRE: A unified label space for entity relation extraction</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="220" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">2022c. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atharva</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arut</forename><surname>Selvan Dhanasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjana</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Stap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Karamanolakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishani</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirby</forename><surname>Kuznia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krima</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Kuntal</forename><surname>Kumar Pal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maitreya</forename><surname>Patel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mehrad</forename><surname>Moradshahi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mihir</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mirali</forename><surname>Purohit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neeraj</forename><surname>Varshney</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rohitha</forename><surname>Phani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pulkit</forename><surname>Kaza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravsehaj</forename><surname>Verma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rushang</forename><surname>Singh Puri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Savan</forename><surname>Karia</surname></persName>
		</editor>
		<editor>
			<persName><surname>Doshi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Keyur</forename><surname>Shailaja</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siddhartha</forename><surname>Sampat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sujan</forename><surname>Mishra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Reddy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sumanta</forename><surname>Patro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tanay</forename><surname>Dixit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xudong</forename><surname>Shen</surname></persName>
		</editor>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="5085" to="5109" />
		</imprint>
	</monogr>
	<note>United Arab Emirates</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Revisiting the negative data of distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<idno>CoRR, abs/2105.10158</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A unified generative framework for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.188</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2416" to="2429" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Pack together: Entity and relation extraction with levitated marker</title>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/2109.06067</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.10420</idno>
		<title level="m">A comprehensive capability analysis of gpt-3 and gpt-3.5 series models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Relation classification via recurrent neural network</title>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Optimizing bi-encoder for named entity recognition via contrastive learning</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">A frustratingly easy approach for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2010.12812</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">For NER(named entity extraction) task, the 21 used datasets includes ACE</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Ner</surname></persName>
		</author>
		<author>
			<persName><surname>Ee ; Derczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FabNER(Kumar and Starly, 2021a), FindVehicle(Guan, 2022), GENIA(Kim et al., 2003b)</title>
		<title level="s">FewRel(Han et al., 2018) and Wiki-ZSL</title>
		<editor>
			<persName><surname>Gids(jat</surname></persName>
		</editor>
		<imprint>
			<publisher>Chen and Li</publisher>
			<date type="published" when="2004">2004. 2005. 2016. 2006. 2014. 2017. 2021. 2015. 2016. 2019. 2019. 2014. 2012. 2004. 2018. 2015. 2010. 2018. 2018. 2010. 2021</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
		<respStmt>
			<orgName>NYT</orgName>
		</respStmt>
	</monogr>
	<note>semeval RE(Hendrickx et al.. For task EE(event extraction), ACE2005(Walker and Consortium, 2005), CASIE(Lu et al., 2021</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
