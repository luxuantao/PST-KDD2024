<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RSD: Relational subgroup discovery through first-order feature construction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nada</forename><surname>Lavrač</surname></persName>
							<email>nada.lavrac@ijs.si</email>
							<affiliation key="aff0">
								<orgName type="institution">Institute Jožef Stefan</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Filip</forename><surname>Železný</surname></persName>
							<email>zelezny@fel.cvut.cz</email>
							<affiliation key="aff1">
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Flach</surname></persName>
							<email>peter.flach@bristol.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RSD: Relational subgroup discovery through first-order feature construction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">67A7B62A64BA3E394964C90467E4C40F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relational rule learning is typically used in solving classification and prediction tasks. However, relational rule learning can be adapted also to subgroup discovery. This paper proposes a propositionalization approach to relational subgroup discovery, achieved through appropriately adapting rule learning and first-order feature construction. The proposed approach, applicable to subgroup discovery in individualcentered domains, was successfully applied to two standard ILP problems (East-West trains and KRK) and a real-life telecommunications application.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Developments in descriptive induction have recently gained much attention.These involve mining of association rules (e.g., the APRIORI association rule learning algorithm <ref type="bibr" target="#b0">[1]</ref>), subgroup discovery (e.g., the MIDOS subgroup discovery algorithm <ref type="bibr" target="#b21">[22]</ref>), symbolic clustering and other approaches to non-classificatory induction.</p><p>The methodology presented in this paper can be applied to relational subgroup discovery. As in the MIDOS approach, a subgroup discovery task can be defined as follows: given a population of individuals and a property of those individuals we are interested in, find population subgroups that are statistically 'most interesting', e.g., are as large as possible and have the most unusual statistical (distributional) characteristics with respect to the property of interest. This paper aims at solving a slightly modified subgroup discovery task that can be stated as follows. Again, the input is a population of individuals and a property of those individuals we are interested in, and the output are population subgroups that are statistically 'most interesting': are as large as possible, have the most unusual statistical (distributional) characteristics with respect to the property of interest and are sufficiently distinct for detecting most of the target population.</p><p>Notice an important aspect of the above two definitions. In both, there is a predefined property of interest, meaning that both aim at characterizing popu-lation subgroups of a given target class. This property indicates that rule learning may be an appropriate approach for solving the task. However, we argue that standard propositional rule learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> and relational rule learning algorithms <ref type="bibr" target="#b18">[19]</ref> are unsuitable for subgroup discovery. The main drawback is the use of the covering algorithm for rule set construction. Only the first few rules induced by a covering algorithm may be of interest as subgroup descriptions with sufficient coverage, thus representing a 'chunk' of knowledge characterizing a sufficiently large population of covered examples. Subsequent rules are induced from smaller and strongly biased example subsets, i.e., subsets including only positive examples not covered by previously induced rules. This bias prevents a covering algorithm to induce descriptions uncovering significant subgroup properties of the entire population. A remedy to this problem is the use of a weighted covering algorithm, as demonstrated in this paper, where subsequently induced rules with high coverage allow for discovering interesting subgroup properties of the entire population.</p><p>This paper investigates how to adapt classification rule learning approaches to subgroup discovery, by exploiting the information about class membership in training examples. This paper shows how this can be achieved by appropriately modifying the covering algorithm (weighted covering algorithm) and the search heuristics (weighted relative accuracy heuristic). The main advantage of the proposed approach is that each rule with high weighted relative accuracy represents a 'chunk' of knowledge about the problem, due to the appropriate tradeoff between accuracy and coverage, achieved through the use of the weighted relative accuracy heuristic.</p><p>The paper is organized as follows. In Section 2 the background for this work is explained: propositionalization through first-order feature construction, irrelevant feature elimination, the standard covering algorithm used in rule induction, the standard heuristics as well as the weighted relative accuracy heuristic, probabilistic classification and rule evaluation in the ROC space. Section 3 presents the proposed relational subgroup discovery algorithm. Section 4 presents the experimental evaluation in two standard ILP problems (East-West trains and KRK) and a real-life telecommunications application. Section 5 concludes by summarizing the results and presenting plans for further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>This section presents the backgrounds: propositionalization through first-order feature construction, irrelevant feature elimination, the standard covering algorithm used in rule induction, the standard heuristics as well as the weighted relative accuracy heuristic, probabilistic classification and rule evaluation in the ROC space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Propositionalization through First-order Feature Construction</head><p>The background knowledge used to construct hypotheses is a distinctive feature of relational rule learning (and inductive logic programming, in general). It is well known that relevant background knowledge may substantially improve the results of learning in terms of accuracy, efficiency, and the explanatory potential of the induced knowledge. On the other hand, irrelevant background knowledge will have just the opposite effect. Consequently, much of the art of inductive logic programming lies in the appropriate selection and formulation of background knowledge to be used by the selected ILP learner.</p><p>By devoting enough effort to the construction of features, to be used as background knowledge in learning, even complex relational learning tasks can be solved by simple propositional rule learning systems. In propositional learning, the idea of augmenting an existing set of attributes with new ones is known under the term constructive induction. A first-order counterpart of constructive induction is predicate invention. This work takes the middle ground: we perform a simple form of predicate invention through first-order feature construction, and use the constructed features for relational rule learning, which thus becomes propositional.</p><p>Our approach to first-order feature construction can be applied in the socalled individual-centered domains, where there is a clear notion of individual, and learning occurs at the level of individuals only. Such domains include classification problems in molecular biology, for example, where the individuals are molecules. Often, individuals are represented by a single variable, and the target predicates are either unary predicates concerning boolean properties of individuals, or binary predicates assigning an attribute-value or a class-value to each individual. It is however also possible that individuals are represented by tuples of variables.</p><p>In our approach to first-order feature construction, described in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10]</ref>, local variables referring to parts of individuals are introduced by so-called structural predicates. The only place where nondeterminacy can occur in individualcentered representations is in structural predicates. Structural predicates introduce new variables. In a given language bias for first-order feature construction, a first-order feature is composed of one or more structural predicates introducing a new variable, and of utility predicates as in LINUS <ref type="bibr" target="#b12">[13]</ref> (called properties in <ref type="bibr" target="#b8">[9]</ref>) that 'consume' all new variables by assigning properties of individuals or their parts, represented by variables introduced so far. Utility predicates do not introduce new variables.</p><p>Individual-centered representations have the advantage of a strong language bias, because local variables in the bodies of rules either refer to the individual or to parts of it. However, not all domains are amenable to the approach we use in this paper -in particular, we cannot learn recursive clauses, and we cannot deal with domains where there is not a clear notion of individual (e.g., many program synthesis problems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Irrelevant Feature Elimination</head><p>Let L denote the set of all features constructed by a first-order feature construction algorithm. Some features defined by the language bias may be irrelevant for the given learning task. Irrelevant features can be detected and eliminated in preprocessing. Besides reducing the hypothesis space and facilitating the search for the solution, the elimination of irrelevant features may contribute to a better understanding of the problem domain.</p><p>It can be shown that if a feature l ∈ L is irrelevant then for every complete and consistent hypothesis H = H(E, L), built from example set E and feature set L whose description includes feature l , there exists a complete and consistent hypothesis H = H(E, L ), built from the feature set L = L\{l } that excludes l . This theorem is the basis of an irrelevant feature elimination algorithm proposed in <ref type="bibr" target="#b14">[15]</ref>.</p><p>Note that usually the term feature is used to denote a positive literal (or a conjunction of positive literals; let us, for the simplicity of the arguments below, assume that a feature is a single positive literal). In the hypothesis language, the existence of one feature implies the existence of two complementary literals: a positive and a negated literal. Since each feature implies the existence of two literals, the necessary and sufficient condition for a feature to be eliminated as irrelevant is that both of its literals are irrelevant.</p><p>This observation directly implies the approach taken in this work. First we convert the starting feature set to the corresponding literal set which has twice as many elements. After that, we eliminate the irrelevant literals and, in the third step, we construct the reduced set of features which includes all the features which have at least one of their literals in the reduced literal set.</p><p>It must be noted that direct detection of irrelevant features (without conversion to and from the literal form) is not possible except in the trivial case where two (or more) features have identical values for all training examples. Only in this case a feature f exists whose literals f and ¬f cover both literals g and ¬g of some other feature. In a general case if a literal of feature f covers some literal of feature g then the other literal of feature g is not covered by the other literal of feature f . But it can happen that this other literal of feature g is covered by a literal of some other feature h. This means that although there is no such feature f that covers both literals of feature g, feature g can still turn out to be irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Rule Induction Using the Covering Algorithm</head><p>Rule learning typically consists of two main procedures: the search procedure that performs search in order to find a single rule and the control procedure that repeatedly executes the search. In the propositional rule learner CN2 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, for instance, the search procedure performs beam search using classification accuracy of the rule as a heuristic function. The accuracy of rule H ← B is equal to the conditional probability of head H, given that the body B is satisfied:</p><formula xml:id="formula_0">Acc(H ← B) = p(H|B).</formula><p>The accuracy measure can be replaced by the weighted relative accuracy, defined in Equation <ref type="formula" target="#formula_1">1</ref>. Furthermore, different probability estimates, like the Laplace <ref type="bibr" target="#b3">[4]</ref> or the m-estimate <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>, can be used for estimating the above probability and the probabilities in Equation <ref type="formula" target="#formula_1">1</ref>.</p><p>Additionally, a rule learner can apply a significance test to the induced rule. The rule is considered to be significant, if it locates a regularity unlikely to have occurred by chance. To test significance, for instance, CN2 uses the likelihood ratio statistic <ref type="bibr" target="#b5">[6]</ref> that measures the difference between the class probability distribution in the set of examples covered by the rule and the class probability distribution in the set of all training examples. Empirical evaluation in <ref type="bibr" target="#b3">[4]</ref> shows that applying a significance test reduces the number of induced rules (but also slightly degrades the predictive accuracy).</p><p>Two different control procedures are used in CN2: one for inducing an ordered list of rules and the other for the unordered case. When inducing an ordered list of rules, the search procedure looks for the best rule, according to the heuristic measure, in the current </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The Weighted Relative Accuracy Heuristic</head><p>Weighted relative accuracy can be meaningfully applied both in the descriptive and predictive induction framework; in this paper we apply this heuristic for subgroup discovery.</p><p>We use the following notation. Let n(B) stand for the number of instances covered by rule H ← B, n(H) stand for the number of examples of class H, and n(H.B) stand for the number of correctly classified examples (true positives). We use p(H.B) etc. for the corresponding probabilities. We then have that rule accuracy can be expressed as Acc(H ← B) = p(H|B) = p(H.B) p(B) . Weighted relative accuracy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>, a reformulation of one of the heuristics used in MIDOS <ref type="bibr" target="#b21">[22]</ref>, is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WRAcc(H ← B) = p(B).(p(H|B) -p(H)).</head><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>Weighted relative accuracy consists of two components: generality p(B), and relative accuracy p(H|B) -p(H). The second term, relative accuracy, is the accuracy gain relative to the fixed rule H ← true. The latter rule predicts all instances to satisfy H; a rule is only interesting if it improves upon this 'default' accuracy. Another way of viewing relative accuracy is that it measures the utility of connecting rule body B with a given rule head H. However, it is easy to obtain high relative accuracy with highly specific rules, i.e., rules with low generality p(B). To this end, generality is used as a 'weight', so that weighted relative accuracy trades off generality of the rule (p(B), i.e., rule coverage) and relative accuracy (p(H|B) -p(H)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Probabilistic Classification</head><p>The induced rules can be ordered or unordered. Ordered rules are interpreted as a decision list <ref type="bibr" target="#b19">[20]</ref> in a straight-forward manner: when classifying a new example, the rules are sequentially tried and the first rule that covers the example is used for prediction.</p><p>In the case of unordered rule sets, the distribution of covered training examples among classes is attached to each rule. Rules of the form:</p><formula xml:id="formula_2">H ← B [ClassDistribution]</formula><p>are induced, where numbers in the ClassDistribution list denote, for each individual class H, how many training examples of this class are covered by the rule. When classifying a new example, all rules are tried and those covering the example are collected. If a clash occurs (several rules with different class predictions cover the example), a voting mechanism is used to obtain the final prediction: the class distributions attached to the rules are summed to determine the most probable class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Area Under the ROC Curve Evaluation</head><p>A point on the ROC curve <ref type="foot" target="#foot_0">1</ref> shows classifier performance in terms of false alarm or false positive rate F P r = F P T N +F P (plotted on the X-axis) that needs to be minimized, and sensitivity or true positive rate T P r = T P T P +F N (plotted on the Y -axis) that needs to be maximized. In the ROC space, an appropriate tradeoff, determined by the expert, can be achieved by applying different algorithms, as well as by different parameter settings of a selected data mining algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relational Subgroup Discovery</head><p>We have devised a relational subgroup discovery system RSD on principles that employ the following main ingredients: exhaustive first-order feature construction, elimination of irrelevant features, implementation of a relational rule learner, use of the weighted covering algorithm and incorporation of example weights into the weighted relative accuracy heuristic.</p><p>The input to RSD consists of -a relational database (further called input data) containing one main table (relation) where each row corresponds to a unique individual and one attribute of the main table is specified as the class attribute, and -a mode-language definition used to construct first-order features.</p><p>The main output of RSD is a set of subgroups whose class-distributions differ substantially from those of the complete data-set. The subgroups are identified by conjunctions of symbols of pre-generated first-order features. As a by-product, RSD also provides a file containing the mentioned set of features and offers to export a single relation (as a text file) with rows corresponding to individuals and fields containing the truth values of respective features for the given individual. This table is thus a propositionalized representation of the input data and can be used as an input to various attribute-value learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RSD First-Order Feature Construction</head><p>The design of an algorithm for constructing first-order features can be split into three relatively independent problems.</p><p>-Identifying all first-order literal conjunctions that by definition form a feature (see <ref type="bibr" target="#b8">[9]</ref>, briefly described in Section 2.1), and at the same time comply to user-defined constraints (mode-language). Such features do not contain any constants and the task can be completed independently of the input data. -Extending the feature set by variable instantiations. Certain features are copied several times with some variables substituted to constants 'carefully' chosen from the input data. -Detecting irrelevant features (see <ref type="bibr" target="#b14">[15]</ref>, briefly described in Section 2.2) and generating propositionalized representations of the input data using the generated feature set.</p><p>Identifying features. Motivated by the need to easily recycle language-bias declarations already present for numerous ILP problems, RSD accepts declarations very similar to those used by the systems Aleph <ref type="bibr" target="#b1">[2]</ref> or Progol <ref type="bibr" target="#b16">[17]</ref>, including variable typing, moding, setting a recall parameter etc, used to syntactically constrain the set of possible features. For example, a structural predicate declaration in the well-known domain of East-West trains would be such as :-modeb(1, hasCar(+train, -car)).</p><p>where the recall number 1 determines that a feature can address at most one car of a given train. Property predicates are those with no output variables (labeled with the minus sign). The head declaration always contains exactly one variable of the input mode (e.g. +train in our example). Various settings such as the maximal feature length denoting the maximal number of literals allowed in a feature, maximal variable depth etc. can also be specified, otherwise their default value is assigned.</p><p>RSD will produce the exhaustive set of features satisfying the mode and setting declarations. No feature produced by RSD can be decomposed into a conjunction of two features.</p><p>Employing constants. As opposed to Aleph or Progol declarations, RSD does not use the # sign to denote an argument which should be a constant. In the mentioned systems, constants are provided by a single saturated example, while RSD extract constants from the whole input data. The user can instead utilize the reserved property predicate instantiate/1, which does not occur in the background knowledge, to specify a variable which should be substituted with a constant. For example, out of the following declarations :-modeh(1, train(+train)). :-modeb(1, hasCar(+train, -car)). :-modeb(1, hasLoad(+car, -load)). :-modeb(1, hasShape(+load, -shape). :-modeb(3, instantiate(+shape)).</p><p>exactly one feature will be generated:<ref type="foot" target="#foot_1">2</ref> f1(A) :-hasCar(A,B),hasLoad(B,C),hasShape(C,D),instantiate(D).</p><p>However, in the second step, after consulting the input data, f1 will be substituted by 3 features (due to the recall parameter 3 in the last declaration) where the instantiate/1 literal is removed and the D variable is substituted with 3 most frequent constants out of all values of D which make the body of f1 provable in the input data. One of them will be f1(A) :-hasCar(A,B),hasLoad(B,C),hasShape(C,rectangle).</p><p>Filtering and applying features. RSD implements the simplest scheme of feature filtering from <ref type="bibr" target="#b14">[15]</ref>. This means that features with complete of empty coverage on the input data will be retracted from the feature set. In the current implementation, RSD does not check for irrelevance of a feature caused by the presence of other features. As the product of this third, final step of feature construction, the system exports an attribute representation of the input data based on the truth values of respective features, in a file of parametrizable format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RSD Rule Induction Algorithm</head><p>A part of RSD is a subgroup discovery program which can accept data propositionalized by the feature constructor described above. The algorithm acquires some basic principles of the CN2 rule learner <ref type="bibr" target="#b5">[6]</ref>, which are however adapted in several substantial ways to meet the interests of subgroup discovery. The principal modifications are outlined below.</p><p>The Weighted Covering Algorithm. In the classical covering algorithm for rule set induction, only the first few induced rules may be of interest as subgroup descriptors with sufficient coverage, since subsequently induced rules are induced from biased example subsets, i.e., subsets including only positive examples not covered by previously induced rules. This bias constrains the population for subgroup discovery in a way that is unnatural for the subgroup discovery process which is, in general, aimed at discovering interesting properties of subgroups of the entire population. In contrast, the subsequent rules induced by the proposed weighted covering algorithm allow for discovering interesting subgroup properties of the entire population.</p><p>The weighted covering algorithm performs in such a way that covered positive examples are not deleted from the current training set. Instead, in each run of the covering loop, the algorithm stores with each example a count how many times (with how many rules induced so far) the example has been covered. Weights derived from these example counts then appear in the computation of WRAcc. Initial weights of all positive examples e j equals w(e j , 0) = 1. Weights of covered positive examples decrease according to the formula w(e j , i) = 1 i+1 , where w(e j , i) is the weight of example e j being covered i times. <ref type="foot" target="#foot_2">3</ref>Modified WRAcc Heuristic with Example Weights. The modification of CN2 reported in <ref type="bibr" target="#b20">[21]</ref> affected only the heuristic function: weighted relative accuracy was used as search heuristic, instead of the accuracy heuristic of the original CN2, while everything else stayed the same. In this work, the heuristic function was further modified to enable handling example weights, which provide the means to consider different parts of the instance space in each iteration of the weighted covering algorithm.</p><p>In the WRAcc computation (Equation <ref type="formula" target="#formula_1">1</ref>) all probabilities are computed by relative frequencies. An example weight measures how important it is to cover this example in the next iteration. The initial example weight w(e j , 0) = 1 means that the example hasn't been covered by any rule, meaning 'please cover this example, it hasn't been covered before', while lower weights mean 'don't try too hard on this example'. The modified WRAcc measure is then defined as follows</p><formula xml:id="formula_3">WRAcc(H ← B) = n (B) N ( n (H.B) n (B) - n (H) N ). (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where N is the sum of the weights of all examples, n (B) is the sum of the weights of all covered examples, and n (H.B) is the sum of the weights of all correctly covered examples.</p><p>To add a rule to the generated rule set, the rule with the maximum WRAcc measure is chosen out of those rules in the search space, which are not yet present in the rule set produced so far (all rules in the final rule set are thus distinct, without duplicates).</p><p>Probabilistic classification and Area Under the ROC curve evaluation. The method, which is used for evaluation of predictive performance of the subgroup discovery results employs the combined probabilistic classifications of all subgroups (a set of rules as a whole). If we always choose the most likely predicted class, this corresponds to setting a fixed threshold 0.5 on the positive probability: if the positive probability is larger than this threshold we predict positive, else negative. A ROC curve can be constructed by varying this threshold from 1 (all predictions negative, corresponding to (0,0) in the ROC space) to 0 (all predictions positive, corresponding to (1,1) in the ROC space). This results in n + 1 points in the ROC space, where n is the total number of examples. Equivalently, we can order all examples by decreasing the predicted probability of being positive, and tracing the ROC curve by starting in (0,0), stepping up when the example is actually positive and stepping to the right when it is negative, until we reach (1,1). <ref type="foot" target="#foot_3">4</ref> The area under this ROC curve indicates the combined quality of all subgroups. This method can be used with a test set or in cross-validation: the resulting curve is not necessarily convex, but the area under the curve still indicates the quality of the combined subgroups for probabilistic prediction. <ref type="foot" target="#foot_4">5</ref>4 Experimental evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Materials</head><p>We have performed experiments with RSD on two popular ILP data sets: the King-Rook-King illegal chess endgame positions (KRK) and East-West trains.</p><p>We applied RSD also to a real-life problem in telecommunications. The data (described in detail in <ref type="bibr" target="#b22">[23]</ref>) represent incoming calls to an enterprise, which were transferred to a particular person by the telephone receptionist. The company has two rather independent divisions ('datacomm', 'telecomm') and people in each of them may have a technical or commercial role. These two binary divisions define four classes of incoming calls ('data tech', 'data comm', 'tele tech', 'tele comm'), depending on the person the call was redirected to. The fifth class ('other') labels the calls (of smaller interest) going to other employees than those mentioned. The problem is to define subgroups of incoming calls usually falling into a given class. The motivation for subgroup discovery in this domain is to select people within a given class that can substitute each other in helping the caller.</p><p>Table <ref type="table" target="#tab_0">1</ref> lists the basic properties of the experimental data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Procedures</head><p>We have applied the RSD algorithm with each of the data sets in the following manner.</p><p>-First, we generated a set of features and, except for the KRK domain, we expanded the set with features containing variable instantiations. -Then the feature sets were used to produce a propositional representation of each of the data sets. -From propositional data we induced rules with the RSD subgroup discovery algorithm as well as with the standard coverage approach. -Finally, we compared the pairs of rule sets in terms of properties that are of interest for subgroup discovery. -In one domain we evaluated the induced rule sets also from the point of view of predictive classification. To do so, we performed a classification test on the unseen part of the data, where rules were interpreted by the method of probabilistic classification, suing a ROC plot to evaluate the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Feature construction. Table <ref type="table" target="#tab_1">2</ref> shows the values of the basic parameters of the language-bias declarations provided to the feature constructor for each given domain as well as the number of generated features before and after the expansion due to variable instantiations.  The resulting rule sets are evaluated on the basis of average rule significance and average rule coverage. The significance of a rule is measured as in the CN2 algorithm <ref type="bibr" target="#b5">[6]</ref>, i.e., as the value of</p><formula xml:id="formula_5">i f i log f i p i (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where for each class i, p i denotes the number of instances classified into i within the whole data set, and f i denotes the number of such classified instances in the subset, where the rule's body holds true. Table <ref type="table" target="#tab_4">4</ref> shows the average significance and coverage values of rules produced in the three respective domains by each of the four combination of techniques, while Figure <ref type="figure">1</ref> shows the significance values for all individual rules.</p><p>By making pairwise comparisons we can investigate the influence of replacing the standard covering algorithm (Cov) by the weighted covering algorithm We), and the affects of replacing the standard accuracy heuristic (Acc) with the weighted relative accuracy heuristic (WRAcc). The following observations can be made from the quantitative results.</p><p>-Example weighting increases the average rule significance with respect to the standard covering algorithm for both cases of combination with the accuracy or weighted relative accuracy measure, in all three data domains. This follows from Table <ref type="table" target="#tab_4">4</ref> above as well as from Figure <ref type="figure">1</ref>. -The use of weighted relative accuracy heuristic increases the average rule coverage with respect to the case of using the accuracy measure. This holds for both cases of combination with the standard covering algorithm or the technique of example weighting, in all three data domains. This follows from Table <ref type="table" target="#tab_4">4</ref> (it is particularly striking in the Telecommunication domain). -A consequence of the previous observation is that in the standard covering algorithm, the use of weighted relative accuracy leads to smaller output rule sets. This can be verified in Figure <ref type="figure">1</ref>. -It may seem surprising that the combination Cov + WRAcc yields a higher average rule coverage than We + WRAcc for Trains and Telecommunication. However, note that it does not make sense to compare coverage results across Cov/We. The reason is that the rule generation process using We (gradually tending to produce more specific rules) is stopped at an instant dictated by the user-specified threshold parameter for the sum of all examples' weights.</p><p>Thus the coverage values are to be compared between methods both using Cov, and between methods both using We with the same threshold value setting.</p><p>Classification. Although the aim of RSD is to merely discover rules representing interesting subgroups, such rules may as well be used for classification purposes. For a binary classification problem, we can interpret the rule set as explained in Section 3.2 to obtain a probabilistic classifier which is then evaluated by means of the area under the ROC curve value. For the two binary-class KRK data set we have thus repeated the rule induction process with only a part of the propositionalized data (750 examples in KRK) and compare RSD rules with the standard covering algorithm.</p><p>We have made the comparisons of the performance of the four methods in terms of the area under the ROC curve only in the KRK domain. The reasons for this decision are that -the Trains domain is too small for splitting the training examples into a training set and a separate test set, and -the Telecommunications domain is a multi-class problem where the area under ROC curve elevation can not be applied in a straight-forward way.</p><p>Table <ref type="table">5</ref> lists the area under ROC values achieved by each of the four combinations of methods on the KRK domain. <ref type="foot" target="#foot_5">6</ref> The values have been obtained by using the subgroup description rules for probabilistic classification as described in Section 3.2. The corresponding ROC curves are shown in Figure <ref type="figure">2</ref>. It can be observed the using the WRAcc heuristic improved the predictive performance both with the standard coverage approach and the example weighting approach. We also observe that using example weighting yielded smaller predictive accuracies than standard coverage in both combinations with WRAcc and Acc. Our explanation is that due to the chosen parameterization of slow weight decrease and low total weight threshold we obtained an overly complex predictive model, composed of too many rules, leading to poorer predictive performance. The chosen parameterization was motivated by the aim of obtaining an exhaustive description of possibly interesting subgroups, but alternative, prediction-aimed settings as well as filtering based on significance measures are part of the future experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KRK</head><p>Cov + WRAcc We + WRAcc Cov + Acc We + Acc Area Under ROC 0.93 0.84 0.92 0.80 Table <ref type="table">5</ref>. The area under ROC values for 4 possible combinations of methods on the KRK domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have developed the system RSD which discovers interesting subgroups in classified relational data. The strategy followed by the system begins with converting the relational data into a single relation via first-order feature construction. RSD can 'sensitively' extract constants from the input data and employ them in the feature definitions. It can also detect and remove some irrelevant features, but so far only in a relatively simplified way. Finally, to identify interesting subgroups, RSD takes advantage of the method of example-weighting</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>set of training examples. The rule predicts the most frequent class in the set of examples, covered by the induced rule. Before starting another search iteration, all examples covered by the induced rule are removed. The control procedure invokes a new search, until all the examples are covered. In the unordered case, the control procedure is iterated, inducing rules for each class in turn. For each induced rule, only covered examples belonging to that class are removed, instead of removing all covered examples, like in the ordered case. The negative training examples (i.e., examples that belong to other classes) remain and positives are removed in order to prevent CN2 finding the same rule again.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Basic properties of the experimental data.</figDesc><table><row><cell>Domain</cell><cell>Individual</cell><cell cols="2">No. of examples No. of classes</cell></row><row><cell>KRK</cell><cell>KRK position</cell><cell>1000</cell><cell>2</cell></row><row><cell>Trains</cell><cell>Train</cell><cell>20</cell><cell>2</cell></row><row><cell cols="2">Telecommunication Incoming call</cell><cell>1995</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The language bias and number of generated features for each of the experimental domain.</figDesc><table><row><cell>KRK Trains Tele</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>below presents a few examples of features generated for each of the three domains together with a verbal interpretation.</figDesc><table><row><cell>KRK f6(A):-king1 rank(A,B),rook rank(A,C),adj(C,B).</cell></row><row><cell>meaning first king's rank adjacent to rook's rank</cell></row><row><cell>Trains f5(A):-hasCar(A,B),carshape(B,ellipse),carlength(B,short).</cell></row><row><cell>meaning has a short elliptic car</cell></row><row><cell>Tele f3(A):-call date(A,B),day is(B,mon).</cell></row><row><cell>meaning call accepted on Monday</cell></row><row><cell>f301(A):-ext number(A,B),prefix(B,[0,4,0,7]).</cell></row><row><cell>meaning caller's number starts with 0407</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Examples of generated features.Subgroup discovery. The following experiment compares the results of the search for interesting subgroups by means of four techniques described in detail in the previous sections:</figDesc><table><row><cell>-Standard covering algorithm with rule accuracy as a selection measure (Cov</cell></row><row><cell>+ Acc).</cell></row><row><cell>-Example weighting (i.e., weighted covering algorithm) with rule accuracy as</cell></row><row><cell>a selection measure (We + Acc).</cell></row><row><cell>-Standard covering algorithm with weighted relative accuracy as a selection</cell></row><row><cell>measure (Cov + WRAcc).</cell></row><row><cell>-Example weighting with weighted relative accuracy as a selection measure</cell></row><row><cell>(We + WRAcc).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Average values of significance and coverage of the resulting rules of subgroup discovery conducted by four different combinations of techniques.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">KRK -Rule Significance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cov WRAcc We WRAcc</cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cov Acc We Acc</cell></row><row><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="24">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">Trains -Rule Significance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3,5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2,5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cov WRAcc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>We WRAcc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cov Acc</cell></row><row><cell>1,5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>We Acc</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0,5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>13</cell><cell>15</cell><cell>17</cell><cell>19</cell><cell>21</cell><cell>23</cell><cell>25</cell><cell>27</cell><cell>29</cell><cell>31</cell><cell>33</cell><cell>35</cell><cell>37</cell><cell>39</cell><cell>41</cell><cell>43</cell><cell>45</cell><cell>47</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="15">Telecommunication -Rule Significance</cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cov WRAcc</cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>We WRAcc Cov Acc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>We Acc</cell></row><row><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>6</cell><cell>11</cell><cell>16</cell><cell>21</cell><cell>26</cell><cell>31</cell><cell></cell><cell>36</cell><cell>41</cell><cell>46</cell><cell>51</cell><cell>56</cell><cell>61</cell><cell>66</cell><cell cols="2">71</cell><cell>76</cell><cell>81</cell><cell>86</cell><cell>91</cell><cell>96</cell><cell>101</cell><cell>106</cell></row></table><note><p><p><p>Fig.</p>1</p>. The significance of individual rules produced by each of the four combinations of techniques in the three data domains. Rules are sorted decreasingly by their significance value. Note that the number of rules produced by the example weighting algorithm (We) is parameterizable by the setting of the weight-decay rate and the total weight threshold for stopping the search. We used the same threshold for all We-based experiments.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>ROC stands for Receiver Operating Characteristic<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Strictly speaking, the feature is solely the body of the listed clause. However, clauses such as the one listed will also be called features in the following as this will cause no confusion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Whereas this approach is referred to as additive, another option is the multiplicative approach, where, for a given parameter γ &lt; 1, weights of covered examples decrease as follows: w(ej, i) = γ i . Note that the weighted covering algorithm with γ = 1 would result in finding the same rule over and over again, whereas with γ = 0 the algorithm would perform the same as the standard CN2 algorithm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>In the case of ties, we make the appropriate number of steps up and to the right at once, drawing a diagonal line segment.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>A description of this method applied to decision tree induction can be found in<ref type="bibr" target="#b7">[8]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Note that we have used a separate test set for evaluating the results -the more elaborate evaluation using cross-validation is left for further work.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work reported in this paper was supported by the Slovenian Ministry of Education, Science and Sport, the IST-1999-11495 project Data Mining and Decision Support for Business Competitiveness: A European Virtual Enterprise, the British Council project Partnership in Science PSP-18, and the ILPnet2 Network of Excellence in Inductive Logic Programming.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(used in the so-called weighted covering algorithm) and employing the WRAcc measure as a heuristic for rule evaluation.</p><p>In three experimental domains, we have shown that example weighting improves the average significance of discovered rules with respect to the standard covering algorithm. Also, the use of weighted relative accuracy increases the average coverage (generality) of resulting rules. The combined effect of the two techniques is thus an important contribution to the problem of subgroup discovery.</p><p>From the classification point of view, we observed that the WRAcc heuristic improved the predictive performance on the tested domain compared to the simple accuracy measure heuristic. On the other hand, example-weighting yielded a smaller predictive performance compared to the standard coverage approach. This is in our opinion due to the chosen parameterization of the algorithm motivated by the need of exhaustive description. In future work we will use test the results using cross-validation. We will also experiment with alternative, prediction-aimed settings.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast discovery of association rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Verkamo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Piatetski-Shapiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Uthurusamy</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="307" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feature construction with Inductive Logic Programming: A study of quantitative predictions of biological activity aided by structural attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="57" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimating probabilities: A crucial task in machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cestnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th European Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Aiello</surname></persName>
		</editor>
		<meeting>of the 9th European Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Pitman</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="147" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rule induction with CN2: Some recent improvements</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th European Working Session on Learning</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Kodratoff</surname></persName>
		</editor>
		<meeting>of the 5th European Working Session on Learning</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="151" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Induction in noisy domains</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niblett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Machine Learning (Proc. of the 2nd European Working Session on Learning</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Bratko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</editor>
		<imprint>
			<publisher>Sigma Press</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="11" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The CN2 induction algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niblett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="283" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using the m-estimate in rule induction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cestnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Petrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computing and Information Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning decision trees using the area under the ROC curve</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri-Ramírez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernandez-Orallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 19th International Conference on Machine Learning</title>
		<meeting>of the 19th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BC: A first-order Bayesian classifier</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lachiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th International Workshop on Inductive Logic Programming</title>
		<meeting>of the 9th International Workshop on Inductive Logic Programming</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="92" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Propositionalization approaches to relational data mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relational Data Mining</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="262" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analysing and improving the diagnosis of ischaemic heart disease with machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grošelj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kralj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Fettich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine, special issue on Data Mining Techniques and Applications in Medicine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="25" to="50" />
			<date type="published" when="1998">1998</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An extended transformation approach to Inductive Logic Programming</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computational Logic</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="458" to="494" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inductive Logic Programming: Techniques and Applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Ellis Horwood</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rule evaluation measures: A unifying view</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th International Workshop on Inductive Logic Programming</title>
		<meeting>of the 9th International Workshop on Inductive Logic Programming</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="74" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A study of relevance for learning in deductive databases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jovanoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic Programming</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="249" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The multi-purpose incremental learning system AQ15 and its testing application on three medical domains</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mozetič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th National Conference on Artificial Intelligence</title>
		<meeting>5th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="1041" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inverse entailment and Progol</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generation Computing, Special issue on Inductive Logic Programming</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="245" to="286" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust classification for imprecise environments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="231" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Logical definitions from relationa</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="266" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning decision lists</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="229" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predictive performance of weighted relative accuracy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Todorovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th European Conference on Principles of Data Mining and Knowledge Discovery</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Zighed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Komorowski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zytkow</surname></persName>
		</editor>
		<meeting>of the 4th European Conference on Principles of Data Mining and Knowledge Discovery</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An algorithm for multi-relational discovery of subgroups</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First European Symposium on Principles of Data Mining and Knowledge Discovery</title>
		<meeting>First European Symposium on Principles of Data Mining and Knowledge Discovery</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A learning system for decision support in telecommunications</title>
		<author>
			<persName><forename type="first">F</forename><surname>Železný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Štěpánková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First International Conference on Computing in an Imperfect World</title>
		<meeting>First International Conference on Computing in an Imperfect World</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="88" to="101" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
