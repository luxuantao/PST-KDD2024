<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Neuroinformatics Department</orgName>
								<orgName type="department" key="dep2">Faculty of Tech-nology</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2AADAA462A1F36ABA2318749ACDCF4C6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Recognition of Continuous Hand Postures Claudia NÃ¶lker and Helge Ritter</head><p>Abstract-This paper describes GREFIT (Gesture REcognition based on FInger Tips), a neural-network-based system which recognizes continuous hand postures from gray-level video images (posture capturing). Our approach yields a full identification of all finger joint angles (making, however, some assumptions about joint couplings to simplify computations). This allows a full reconstruction of the three-dimensional (3-D) hand shape, using an articulated hand model with 16 segments and 20 joint angles. GREFIT uses a two-stage approach to solve this task. In the first stage, a hierarchical system of artificial neural networks (ANNs) combined with a priori knowledge locates the two-dimensional (2-D) positions of the finger tips in the image. In the second stage, the 2-D position information is transformed by an ANN into an estimate of the 3-D configuration of an articulated hand model, which is also used for visualization. This model is designed according to the dimensions and movement possibilities of a natural human hand. The virtual hand imitates the user's hand to an remarkable accuracy and can follow postures from gray scale images at a frame rate of 10 Hz.</p><p>Index Terms-Hand model, hand posture, human-computer intraction (HCI), inverse kinematics, local linear mapping (LLM) network, neural network, self-organizing map (SOM), visual learning, visual recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H UMAN-COMPUTER-INTERACTION (HCI) today is still mainly based on means of communication which are not natural for us. Although keyboard and mice as input devices have grown to be familiar, they inherently limit the speed and naturalness with which we can interact with the computer. But rather than continuing to demand that humans acquire machine-like skills, it is much better to enable machines to acquire human-like skills, such as recognizing speech and gesture. The use of hand gestures is a common means of nonverbal interaction among people and can, therefore, also provide an attractive alternative to cumbersome interface devices.</p><p>To exploit the use of gestures in HCI, it is required that configurations of the hand and/or arm become measurable by the machine. First attempts to solve this problem resulted in glove-based devices <ref type="bibr" target="#b0">[1]</ref>. Whereas these permit a fast and reliable measurement of the joint angles, they are not quite accepted by the user due to the awkwardness of wearing them and being connected to the machine by cables.</p><p>In contrast, visual interpretation of hand gestures is more convenient for the user and can thus help in achieving the ease and naturalness desired for HCI <ref type="bibr" target="#b1">[2]</ref>. The first approaches using video-based recognition of hand gestures simplified the computational problem by attaching colored markers on selected body landmarks. The next step was a classification of the hand shape into one of several predefined posture classes. This could be done on nonadorned hands but limited the type of applications severely. For future applications, it will become more and more necessary to achieve the exact parameters of the hand in order to evaluate the hand posture and with it the intention of the user. This is also evident with the development of new technologies, like MPEG for the transmission of multimedia data.</p><p>With the fast convergence of computers and telecommunication, economic coding of multimedia scenes becomes important. Along this line, the international MPEG-4 standard version 2 includes provision for body animation, which enables efficient representation and synthesis of human gesture and facial expressions in the transmission of video data. This is done by providing body objects and respective body animation parameters (BAPs), which describe the topology of a human skeleton. The body contains a total of 186 DOF, where 25 DOF already pertain to each hand. When given angular values corresponding to these BAPs, they can be applied to any MPEG-4 compliant body and will produce the same animation. Therefore, the model is capable for various applications, from realistic simulation of human motions to virtual reality games.</p><p>A further step will be taken by the MPEG-7 standard, formally named "Multimedia Content Description Interface," which allows not only for a description of the multimedia content data, but will also allow to encode some of the data's meaning.</p><p>The GREFIT system introduced in this paper is a video-based noncontact interface which computes the angular values of the human hand needed for the description of the BAPs of the MPEG-4 version 2 standard. This continuous parameterization of a hand posture instead of the common classification opens up new application areas, where gestures are used to indicate extents of quantity, as required, e.g., for object manipulation (sculpting, smoothing, joining, rotating), telecontrol of a robot, or navigation. This paper is structured as follows: A short survey of related work is presented in Section II. Then, we describe the selected approach of using the finger tips as a natural representation of the hand posture (Section III) and the setup of the system for locating the finger tips (Section IV). After the preprocessing (Section V), the GREFIT system performs a global, then a local processing of the hand images which is described in Sections VI and VII. An integration of knowledge and context information (Section VIII) improves the results of locating the finger tips in the image, before the reconstruction of the hand posture is performed. We conclude with the accuracy of the GREFIT system in Section X and a discussion in Section XI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Previous work on reconstructing hand posture from characteristic landmarks on the hand includes <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[12]</ref>. In <ref type="bibr" target="#b7">[8]</ref>, a continuous parameterization of hand posture is obtained by providing a small set of basis postures from which a larger subset of actually occurring configurations can be linearly combined. A neural network is employed to identify the posture of a simulated robot hand from its image.</p><p>In <ref type="bibr" target="#b8">[9]</ref>, the user wears a glove with color-coded markers on joints and finger tips in order to facilitate feature recognition. Dorner and Hagen's skeleton-like three-dimensional (3-D) model of the hand contains joints (their location and range of possible motion) and a mathematical representation of joint-angle interdependencies that are determined by the hand's physiology. They use a three-step iterative scheme to fit their model to the image data in order to recover the pose of the hand.</p><p>The hand model of DigitEyes <ref type="bibr" target="#b9">[10]</ref> consists of 16 cylinders forming the fingers and the palm arranged as a kinematic tree, as familiar from robotics. The kinematics and geometry of the target hand must be known in advance. DigitEyes uses a set of 27 DOF. Starting with the hand in a known initial configuration, a state estimation algorithm first predicts feature positions for the next frame. Afterwards the difference between measured and predicted states is minimized. A high image sampling rate is essential for this tracking algorithm.</p><p>Lee and Kunii <ref type="bibr" target="#b10">[11]</ref> present a hand model which is based on natural movement constraints of a human hand. With the help of color-coded gloves and stereo vision, seven "characteristic points" in 3-D (all five finger tips, the wrist, and an extra point on the palm) are extracted. An iterative algorithm using external "image forces" then fits their model to the hand image. Due to the time-consuming solving of the inverse kinematics of the fingers, the algorithm is rather slow.</p><p>Millar and Crawford <ref type="bibr" target="#b11">[12]</ref> employ a geometric model of the human hand which fulfills both the physical form and the kinematic constraints of the fingers. An analysis algorithm fits this model to six 3-D marker positions at the finger tips and the center of the wrist. Since the fitting of the model is done directly, the computation is fast. The algorithm requires an initial standardization of the hand data into a predefined position and orientation. It generates a skeletal model of the hand using points and lines as its output.</p><p>Heap and Hogg <ref type="bibr" target="#b12">[13]</ref> built a hand model with 498 vertices from a set of training examples from 3-D MRI data. Afterwards, their systems constructs a point distribution model and performs a PCA to find the modes of variation. The model is based on internal physical forces (to hold the model in shape) as well as external forces (to fit the model to the image data). The system can track an unmarked hand moving with six DOF in real time using a single video camera.</p><p>Shimada et al. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> use a 3-D hand model with 23 DOF. Their system can be divided into two phases for matching of the hand posture. First, possible candidate configurations using the model are generated, second, the degree of matching with the silhouette is evaluated and extended Kalman filters are used for fitting the model to the image.</p><p>Ouhaddi and Horain <ref type="bibr" target="#b15">[16]</ref> use an articulated 3-D model of the hand that integrates biomechanical and anthropometric constraints and conforms to the MPEG-4 specifications, but includes only 12 DOF. The system minimizes the difference between features of the projected hand model and those extracted from the images by minimizing the nonoverlapping surface in silhouette images and contour distances with classical optimization methods. The tracking system requires the hand to be in a fixed starting position and does not work in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH AND GREFIT SYSTEM ARCHITECTURE</head><p>We aim at a visual system for the recognition and reconstruction of a large, continuous subset of all hand postures instead of performing the usual classification into a number of discrete fixed postures. Therefore, we need an expressive, continuous-valued representation of the hand shape for the further processing with an hierarchical approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Finger Tips as Natural Determinants of Hand Posture</head><p>How can we describe the posture of the hand in an image by parameters? In this paper, we suggest to first localize the positions of characteristic landmarks in the image, and then to reconstruct the appropriate hand posture on the basis of these parameters.</p><p>The finger tips are a well-known set of characteristic landmarks for the human hand. This is motivated by results from Poizner et al. <ref type="bibr" target="#b2">[3]</ref>, who performed experiments with point light displays attached to different positions on arm and hand and found out, that solely the movements of the finger tips are necessary for the recognition of the American Sign Language (ASL). Also, research using an eye-tracker to evaluate the fixation points of subjects recognizing hand postures showed, that the majority of the fixation points are near the finger tips <ref type="bibr" target="#b3">[4]</ref>. Thus, finger tips work as natural determinants of hand posture.</p><p>This explains why finger tips are frequently used features in hand and gesture recognition. The task of locating the finger tips can be simplified by attaching specially colored markers on the hand of the user and by using color histograms for processing the images (e.g., <ref type="bibr" target="#b4">[5]</ref>). Other approaches to find the finger tips without visual aids include different kinds of finger tip templates (e.g., cylindrical models <ref type="bibr" target="#b5">[6]</ref> or images of a prototype <ref type="bibr" target="#b6">[7]</ref>). These methods do all assume that the contrast of the image region in the area of the finger tip is high, i.e., that the fingers of the hand are (nearly) stretched-but this is a severe limitation to the huge number of possible hand postures.</p><p>Contrary to that, the GREFIT system is able to locate the finger tips even in areas with low contrast, i.e., when the finger tips are in front of the palm. No constraints of the possible hand postures are necessary and gray-level video images are sufficient in this setup.</p><p>A big disadvantage of the usage of finger tips as features is their susceptibility to occlusions. This problem can be tackled by either using multiple cameras with different viewing directions, by tracking, or by integrating context information, e.g., the locations of the other finger tips. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hierarchical System Architecture</head><p>We use an hierarchical approach consisting of several layers. The first layer performs a coarse processing of the entire pixel image to find an initial estimate of the location of each finger tip. Each higher layer then refines the position estimates of the previous (lower) layer by restricting processing to smaller subregions centered at the position estimates of the previous layer. Also, these subregions are processed at a higher spatial resolution (coarse-to-fine strategy). In this way, interesting areas of the image gradually get more focal attention. This is also a principle that can be observed in the human visual system, where the fovea is the area with the highest resolution of the image. Altogether, concentrating on the relevant data and neglecting the irrelevant parts results in a more robust recognition, and due to the reduced amount of data, the processing on the lower level can be done much faster.</p><p>In the GREFIT system, the position estimates are computed with the aid of neural networks of the local linear mapping (LLM) type, see Fig. <ref type="figure" target="#fig_0">1</ref>. We additionally use a priori knowledge to locate the finger tip of each finger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. System Overview</head><p>After grabbing and preprocessing the image, we apply the global processing algorithm described below and then the local processing algorithm for each finger tip separately. This yields five finger tip positions in the image. For a more precise location, information about the shape and the context are included. As the next step, the obtained feature vector consisting of the five finger tip positions is transformed into the finger joint angles of an articulated hand model. A visualization of the results with this model completes the system. These steps are schematically depicted in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SETUP OF THE SYSTEM</head><p>The setup of GREFIT is a "handbox" (see Fig. <ref type="figure" target="#fig_2">3</ref>) containing a camera and a lamp for illumination of the scene. The box has the approximate size 30 27 50 cm. The user inserts his hand through an opening at the side into the handbox, an armrest prevents weariness. This setup guarantees a constant distance between the hand and the cameras. As the finger tip detection algorithm of GREFIT depends on the visibility of all five finger tips, the palm of the hand is required to face the camera, because this orientation reduces the possibility of occlusions.</p><p>The employed cameras of type Sony EVI-371DG are CCD cameras equipped with auto-focus lens and a controllable zoom function. A built-in shutter control normalizes the lighting. Additionally, a wide conversion lens is attached to each camera to shorten the distance between hand and cameras.</p><p>An ordinary energy-saving lamp (not shown in Fig. <ref type="figure" target="#fig_2">3</ref>), which does not get hot when used, is placed above the cameras to illuminate the scene.</p><p>The inside of the box is painted black, facilitating segmentation sufficiently to allow the use of gray-scale images together with a simple threshold based segmentation algorithm.</p><p>The images are grabbed using a Matrox Meteor framegrabber card, the processing is done by an off-the-shelf computer (PentiumPro processor with 200 MHz) running Linux.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PREPROCESSING</head><p>The input to the system is a stream of monocular gray-scale images with a resolution of 192 144 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Segmentation</head><p>The background is blackened by performing a threshold operation on the image (object-background separation). This threshold is also used to obtain the silhouette image shown in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>Different skin colors did not raise a problem, since the automatic camera iris is able to compensate for differences in brightness. For the same reason, additional light sources (e.g., room illumination when the lid did not cover the box) do not disturb the detection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hand-Arm-Separation and Centroid-Computation</head><p>A masking of the arm is needed to meet the requirements for the subsequent centering of the hand. The masking method is pixel-based: We count the number of object pixels of each vertical image scan line. When the variation of the values exceeds a threshold, we assume that the wrist has been detected, because the hand is wider than the arm, see Fig. <ref type="figure" target="#fig_4">5</ref> for details.</p><p>As a last step, we compute the centroid of the pixels of the separated hand. This position will become the center for the following computation of the feature vector. In this way, we can compensate for translations. Fig. <ref type="figure" target="#fig_3">4</ref> illustrates these preprocessing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Calibration to Hand Size</head><p>Whereas normalization of illumination, background and distance between hand and camera is guaranteed by the setup, the hands of the users are varying. Although the anatomical system of the human hand is fixed, hand shapes, structure, and surfaces show considerable individual differences. Therefore, a normalization is an important step to achieve good performance. Since an exact adaptation to the specific hand of the user would be presumptuous, we refrain from this and perform only a simple normalization according to the size of the hand by adapting the zoom value of the camera. We use the total number of object   pixels in the silhouette image as the criterion. Therefore, we ask the user to stretch out his/her fingers during the short calibration process. The zoom value of the camera is adjusted until the percentage of hand pixels in the image is approximately 25%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Edge Detection</head><p>As a further preprocessing step, we perform edge detection by applying a Laplace filter to the image. (Other, e.g., gradient-like filters are also suitable. The edge image is used for the local processing in Section VII. The edge image is also used in order to significantly reduce the number of possible finger tip positions through the integration of a priori knowledge, since we require the finger tip position to coincide with an edge pixel (see Section VIII).</p><p>We applied an orientation-invariant Laplacian operator on the image data, and thresholded the results to obtain binary edge maps. The thresholds are chosen so that the edge detection yields good results especially in areas with low contrast, e.g., if the finger tips are in front of the palm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. GLOBAL PROCESSING</head><p>In order to get a first estimate of the finger tip positions, we compute a lower dimensional representation of the image by a set of Gabor filters, and apply an artificial neural network (ANN) to the resulting feature vector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gabor-Feature-Vector</head><p>The input vector of 6912 pixel intensities of an image (subsampled at half resolution of 96 72 pixels) is much too high-dimensional for a direct use as a feature vector. To get a low-dimensional representation of the image, we apply Gabor filters at several positions in the image. Two-dimensional (2-D) Gabor filters have been found by Jones and Palmer <ref type="bibr" target="#b16">[17]</ref> to resemble the response profile of simple cells in the visual cortex of cats. The 2-D-Gabor filter kernels have the shape of a Gaussian modulated by a sinusoidal plane wave <ref type="bibr" target="#b0">(1)</ref> with or , where is the width of the Gaussian and , are parameters to specify the frequency and orientation of the filter. Fig. <ref type="figure">6</ref> depicts the typical shape of a Gabor function and the filter kernels of a Cosine-and a Sine-Gabor filter. Due to possible ambiguities in the filter response, Gabor filters are usually applied as a filter pair, for instance Cosine and Sine filter.</p><p>In order to compute the feature vector, a cross-correlation of the filter with the image is performed. High resemblance of the picture with the filter kernel yields high values in the filter response. As Gabor filters are sensitive to directions, a number of filters with different orientations are combined ("Gabor jet").</p><p>At five points (arranged like the "Five" on a dice in order to best cover the image) in the image, <ref type="foot" target="#foot_0">1</ref> we applied a Gabor jet consisting of Cosine-and Sine-Gabor filters of three different orientations each (0 , 60 , 120 ) plus an additional isotropic Gaussian. This results in a 35-dimensional feature vector. The frequencies as well as the width of the Gaussian was adjusted to the size of the image, so that the filters cover the area of the hand. In Fig. <ref type="figure" target="#fig_7">7</ref>, we take the 35 Gabor jet coefficients, multiply each coefficient by its corresponding filter and sum all the weighted filters to produce the images which are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LLM Net</head><p>For our approach we make use of an LLM network (for further explanation see <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b22">[23]</ref>), which is related to self-organizing maps <ref type="bibr" target="#b25">[26]</ref>, to generate a smooth mapping from the input variables (in our application a feature vector ) onto the output variables (position in the image). An LLM net has a fixed number of units (or nodes), each of which consists of a reference vector in the input space and an associated reference vector in the output space together with a matrix defining a local linear mapping from to . The task of the matrices is to approximate the mapping linearly in the vicinity of the prototype vectors (cf. is determined.</p><p>In the winner-take-all variant of the net, merely the bestmatch unit determines the answer in the output space. Smoother results can be achieved if the output of the network is a weighted superposition of the outputs of the individual nets. In both cases, the difference between the input value and the reference vector in the input space is converted into a linear correction term (see Fig. <ref type="figure" target="#fig_8">8</ref>). Therefore, the answer of the net is   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training and Results</head><p>For each finger, separate networks were trained on the same feature vectors. The input space is 35-dimensional (Gabor feature vector), the output space is 2-D and represents the -position in the image. The LLM-networks have nodes, the learning step sizes , , were chosen to be 0.8 at the beginning and gradually decreased to 0.0001. The training examples were presented in random order for five epochs.</p><p>A data set of 1000 example images consisting of ten sequences with manually marked finger tip positions was prepared for training and testing. These images were divided up into ten different partitionings consisting of 900 images for training and 100 images for testing. We then trained ten neural networks for each training/test-set and computed the average from these data as the overall result.</p><p>All errors-distances between the correct position and the detected positions-are measured in pixels (192 144 image size). Fig. <ref type="figure" target="#fig_9">9</ref> presents a box plot of the results of the training and the test runs. The average error is denoted with a solid line inside each box, the variance is indicated by the dashed lines. The size of the box denotes the 95%-limit which means that the error of 95% of all examples is below this value. The maximum error is shown as a whisker in the plot. The training results are displayed with white boxes, the test results with gray boxes. The results in Fig. <ref type="figure" target="#fig_9">9</ref> show that the maximum error in the test is always smaller than the same value in the training, the other values are quite similar. This signifies that the networks did not just specialize on the training images, but achieved a good generalization. Only slight differences occurred within the ten different training/test-sets.</p><p>The networks for the thumb achieve the best results, whereas the positions of the middle finger and ring finger seem to be the most difficult to learn. A possible reason may be that a movement of the thumb produces more pronounced changes in the feature vector than a movement of the fingers: Since we computed orientation-dependent Gabor filter features, the usually parallel movements of the fingers have less effect in the feature vector the predominantly perpendicular movements of the thumb in the image. Fig. <ref type="figure" target="#fig_10">10</ref> displays the positions of the finger tips in a typical sequence of 100 images relative to the image centroids. It shows that, after training, the reference vectors in the output space are arranged so that they cover the movement area of the finger tips quite well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. LOCAL PROCESSING</head><p>The finger tip position estimates from the preceding, global processing stage are now used to define smaller regions of interest (ROIs) for the subsequent local processing stage. Each ROI is centered at one of the estimated finger tip positions and is a sample with the original, full pixel resolution. Again, we first compute a feature vector from which then a second ANN (one for each ROI) computes a correction to the initial position estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Size of Region-of-Interest</head><p>An optimally chosen size of each ROI is crucial for the further processing, since it determines the search-area and should cover the uncertainty range of the estimates of the first (global) processing stage. Therefore, we chose the sizes so that they are in line with the error and its standard deviation of the results of the global processing. As can be seen in Fig. <ref type="figure" target="#fig_11">11</ref>, error and  variance for the finger position estimates<ref type="foot" target="#foot_1">2</ref> in the -direction are approximately three times the size as in the -direction, therefore we selected ROIs of 36 12 pixel size.</p><p>For the thumb, the -coordinate contains the largest error and variance, therefore, the thumb ROI size was set to be 16 24 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Vector</head><p>Similar as in the global processing step, we again apply Gabor jets to the edge image of each ROI in order to compute a feature vector. The Gabor jets were arranged on the subimages so that they fit to the respective size and shape. Again, we use Gabor jets consisting of seven filters at five positions to obtain a 35-dimensional feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training and Results</head><p>Since it is not possible to distinguish the fingers by their tips alone (their local shape and size are nearly identical), we used only two LLM-networks for the local stage: one network is trained to find the tip of the thumb, another network for the remaining fingers, which are sufficiently similar to allow the use of a single, shared network.</p><p>To this end, we generated the data for the training and test of the LLM-networks by randomly selecting subimage patches containing a finger tip, where the average distance of the finger tip position to the center of the image was set to seven pixels a variance of three pixels. The size of the training set was 2400 subimages, the test set contained 600 subimages.</p><p>As in the global processing, we partitioned the subimages into ten different training/test-sets. For each set ten neural networks three nodes were trained by presenting training examples in random order for five epochs. The reported data are average values for all sets and network results. The display in Fig. <ref type="figure" target="#fig_12">12</ref> is analogous as in Fig. <ref type="figure" target="#fig_9">9</ref>.</p><p>The improvements (see Fig. <ref type="figure" target="#fig_6">13</ref>) achieved by the local processing for a number of example images are illustrated in Fig. <ref type="figure" target="#fig_13">14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. INTEGRATION OF KNOWLEDGE AND CONTEXT</head><p>In this section, we discuss improvements in the GREFIT system by making use of additional information sources, namely a priori knowledge about the shape of the hand and context information about the special setup. As this is also a local processing step, we are working on subimages which are centered on the detected finger tip position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prior Knowledge</head><p>No individual technique of computer vision is universal. Instead, only the combination of several-typically complementary-information channels leads to reliable and robust systems <ref type="bibr" target="#b19">[20]</ref>. In the present case, we can improve performance by combining complementary results of the processing of the subimage into a "confidence image" representing our confidence for the presence of a finger tip at each of the pixels of the image.</p><p>Three channels of information are integrated in the GREFIT system: The results of the global local computation is represented as a Gaussian activity profile with maximum value (i.e., the highest confidence) at the center of the subimage and decreasing values for the more distant pixels, see Fig. <ref type="figure" target="#fig_14">15</ref>, second row, for the Gaussian combined with the edge image. Second, since the finger tips are on the borders of the fingers, we restricted the possible positions to the edge pixels. To this end, we used the edge images computed in Section V, which limited the number of possible points severely. Third, a simple matching with a "finger tip filter" of the size 5 5 square pixels for the shape of the finger tips evaluated the conformity of the edges with a typical "finger tip edge," i.e., a short vertical line with a rounding at one or both sides (see Fig. <ref type="figure" target="#fig_14">15</ref>, second and third row).</p><p>All three information channels are normalized to yield images with confidence values in . These solitary confidence Fig. <ref type="figure" target="#fig_6">13</ref>. Improvements by the hierarchical approach for the thumb (TH) and fore finger (FF). The position errors of the global processing stage (glb) are significantly reduced by the subsequent local processing (+loc). Additional integration of domain knowledge and context information (+know) leads to further gains in accuracy (graphs as in Fig. <ref type="figure" target="#fig_9">9</ref>).  images are pixel-wise multiplied to get the overall result. The two highest values in the product image with a minimum separation of four pixels are then used as the most probable candidate and an alternative position. Fig. <ref type="figure" target="#fig_14">15</ref> visualizes the original subimage and its confidence images with the most reliable finger tip locations indicated by squares.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Context</head><p>The detection of features can be further improved, if constraints about the scene or the object(s) can be integrated in the process of analysis. This additional knowledge can for instance help with the interpretation of the scene by providing background information about typical shapes, patterns of movement, etc.</p><p>In the GREFIT system, an implicit restriction to the area is already achieved by the trained global neural networks, each of which has learned the typical area of movement for its finger, see Fig. <ref type="figure" target="#fig_10">10</ref>. As additional context information, we impose the constraint that two finger tips cannot be at the same position. To this end, we replace the best-guess candidates with alternative positions whose confidence values are high whenever the chosen candidates of adjacent fingers would be less than four pixels apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. RECONSTRUCTION OF HAND POSTURE</head><p>In order to reconstruct the joint angles of the hand from the positions of the finger tips, we employ a geometric model of the hand. This requires us to solve the inverse kinematics. As we will show below, a very convenient way to solve this task is the use of a parameterized self-organizing map (PSOM), which is trained in the forward direction and can then be easily inverted. Finally, a plausibility test checks if some additional movement constraints of adjacent fingers are fulfilled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Articulated Model of the Hand</head><p>For the reconstruction and visualization of the hand posture, we developed a virtual model of the hand. It represents the shape and incorporates constraints based on the natural movement of the human hand. With the hand model any hand posture can be imitated, which is essential for the computation of the finger angles as well as the output of GREFIT.</p><p>Our hand model is composed of 16 rigid elements, one for the palm, and three segments for each finger and the thumb. The sizes of the segments have been taken according to a medium sized human hand and are depicted in Fig. <ref type="figure" target="#fig_15">16</ref>. In order to achieve a great variety of finger movements, we provide four DOF for each finger and the thumb (see Fig. <ref type="figure" target="#fig_15">16</ref>).</p><p>The fingers and the thumb have two DOF at the anchoring joint (or the metacarpophalangeal joint, MP) allowing flexion with an angle as well as spreading movement expressed by . The proximal interphalangeal (PIP) and the distal interphalangeal (DIP) joint have one DOF each. The angles and represent their degree of flexion. Altogether, our hand model has 20 DOF.</p><p>Indeed, not all of these joint angles of a human hand are capable of fully independent actuation, but are correlated (see <ref type="bibr" target="#b20">[21]</ref>). An example are the last two joints of the fingers, i.e., the PIP and DIP, which are driven by the same tendon. Approximately, their relation is <ref type="bibr" target="#b5">(6)</ref> Another simplification is to equate the joint angles and that determine the flexion of the fingers: <ref type="bibr" target="#b6">(7)</ref> This constraint does not exactly correspond to the situation of the human hand, but is a good approximation.</p><p>With these constraints, a ten-dimensional state vector represents the hand posture. Using this 2-D position data, we can reconstruct the 3-D hand posture.</p><p>We also included the convergence of the fingers when the fist is clenched (see <ref type="bibr" target="#b20">[21]</ref>): The axes of the fingers are not perpendicular to the finger segments, but are slightly oblique. As a consequence, the fingers are closely side-by-side at complete flexion.</p><p>Even though the allowable ranges of finger joint angles vary from person to person, general ranges as in Table I can be specified. Here, we only consider the active movements of the joints. These are activated by the tendons and muscles of the hand, whereas the passive movement is externally forced. The zero position of the thumb is defined as the state where no thumb muscles are active. The fingers are stretched when in zero position, their axes are parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PSOM-Net</head><p>We employ a PSOM <ref type="bibr" target="#b21">[22]</ref> to compute for each finger the appropriate finger joint angles from the 2-D position of the finger tip in the image. Here, the employed neural network can only be described briefly, for details see <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b24">[25]</ref>.</p><p>The PSOM is an extension of the well-known SOM by Kohonen <ref type="bibr" target="#b25">[26]</ref>. It uses a set of nonlinear basis manifolds to construct a mapping through a number of "topologically ordered" reference vectors. The advantage of the PSOM in comparison to the SOM is that fewer reference vectors are required for a good result. Also, learning is very fast.</p><p>The PSOM replaces the discrete lattice of a SOM by a continuous "mapping manifold" . Therefore, a reference vector is assigned to each grid point (see Fig. <ref type="figure" target="#fig_16">17</ref>). The reference vectors have to be topologically ordered, i.e., neighbors on the lattice are neighbors in the data space, too. A basis function is also associated with each grid point . The basis functions realize a smooth interpolation of intermediate positions between the grid points. The manifold in the embedding space can then be described as the image of under where <ref type="bibr" target="#b7">(8)</ref> The basis functions have to meet two conditions. The first orthonormality condition <ref type="bibr" target="#b8">(9)</ref> ensures that the manifold given by ( <ref type="formula">8</ref>) passes through all desired support points , . Second, the sum of all contributing weights should be one <ref type="bibr" target="#b9">(10)</ref> This ensures that a translation of all reference vectors entails a corresponding translation of the PSOM manifold without changing its shape.</p><p>During application, an initial point in the data space is mapped onto the nearest point with <ref type="bibr" target="#b10">(11)</ref> This minimization is done using an iterative procedure: We start with the point on the lattice which has the smallest distance to , take the associated reference vector , and apply gradient descent along the manifold to find the best approximation. The output of the PSOM is then computed using <ref type="bibr" target="#b7">(8)</ref>. The PSOM can be applied to map an input vector onto a map coordinate . Beside this mapping task, it is also able to perform an associative completion of fragmentary input. We took advantage of this ability and applied the PSOM to the inverse kinematics task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inverse Kinematics With the PSOM</head><p>One PSOM for each finger is trained by providing the training data in topological order. The training data are generated by varying the joint angles and (here, the lattice is 2-D) in their ranges of movement (see Table <ref type="table" target="#tab_0">I</ref>) and computing the resulting finger tip positions by forward kinematics with the hand model. This produces four-dimensional (4-D) reference vectors of joint angles and the respective finger tip positions (here,</p><p>), which can be split into a 2-D subset of input With this learned mapping, the PSOM completes the partially known vector to a completely known vector. The topological order of the training data is ensured by varying the joint angles on a grid structure, e.g., . Depicting the resulting finger tip positions yielded the shape of an "horizontal egg-timer" (see Fig. <ref type="figure" target="#fig_17">18</ref>, left) with an intersection in the middle. This particular 2-D position can be achieved with many different joint angle combinations and represents a singular point of the kinematic transformation. Due to this ambiguity in the joint angles, the transformation between the 2-D position and the angles cannot be solved by the PSOM at this singularity.</p><p>So far, our modeling of the ranges of motion was based on the assumption that the angles and of the fingers are not correlated. However, a closer look indicated that the abduction and adduction angle decreases as the flexion angle increases. In other words, the freedom of motion in can only be fully exploited when the fingers are roughly stretched.</p><p>Since the grid structure had to be preserved for the application of the PSOM, we merged those grid points with large values in the flexion angle. The range of movement of the fingers is now limited to a "broom"-shaped area (see Fig. <ref type="figure" target="#fig_17">18</ref>, right). As this changed structure has no singularity, the PSOMs perform well. The internal grid structure of the PSOM was maintained, but we provide fewer different reference vectors during training than points on the grid exist.</p><p>The thumb is the exception of this method. Since no singularities occur for its range of movement, the training of the PSOM is based upon the original grid. The resulting ranges of motion  for all four fingers and the thumb are illustrated in Fig. <ref type="figure" target="#fig_18">19</ref> for a grid of size 3 3.</p><p>In addition, we disabled the ability of the PSOM to extrapolate, i.e., to infer values outside of the movement area. This ensures that the fingers do not take on "unhealthy" values for the joint angles. This constraint equips the PSOM with implicit model information, anatomically injurious postures cannot occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Application</head><p>We tested the PSOMs with 100 randomly chosen test examples in the movement area of the respective finger. For evaluation, the results of the PSOMs were again transformed into the 2-D positions and directly compared with the input values. A comparison of PSOMs with different dimensions of the axes showed that a 4 4 grid yields the best results. Independent of the finger (and thumb), the average error is approximately 0.14 cm and the maximum error is always below 0.4 cm. (The length of the hand is 19 cm, the width of a finger 1.5 cm.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Restrictions of Movements of Adjacent Fingers</head><p>Whereas the qualitative facts are known, quantitative data for the couplings between angular values of adjacent fingers is rare. However, it is exactly these couplings which determine whether the resulting hand posture looks natural or not. In the GREFIT system, a plausibility test for the compliance of the posture is done on the basis of a number of inequalities proposed by Lee and Kunii <ref type="bibr" target="#b10">[11]</ref>. With the help of the static joint angle limits in Table <ref type="table" target="#tab_0">I</ref>, they describe the dynamic joint angle limits for the flexion of the MP joint depending on the values of adjacent fin- Due to their structure, nonfulfilled inequalities always occur at least as a pair (e.g., and . If exactly two inequalities are not fulfilled, this can be fixed by sole changing of the joint angle value of either fore finger or little finger. Otherwise, at least one of the joint angles of the middle or ring finger has to be adjusted. In rare cases, where four inequalities are not fulfilled, a second checking-pass is necessary. The effect of this plausibility test is shown in Fig. <ref type="figure" target="#fig_19">20</ref> for some example postures.</p><p>We included this plausibility test as the last step before displaying the hand posture with the articulated model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. RESULTS</head><p>Here, we illustrate the performance of the GREFIT system. Fig. <ref type="figure" target="#fig_20">21</ref> (upper rows) depicts a number of different hand postures offered to the system. The lower rows shows the corresponding 3-D reconstructions using the articulated hand model described schematically in Fig. <ref type="figure" target="#fig_15">16</ref>. While Fig. <ref type="figure" target="#fig_20">21</ref> can only illustrate the identification of a discrete set of hand postures, we would like to stress that the system indeed can process a continuous sequence of hand postures of which the shown samples are only "snapshots." See also website <ref type="bibr" target="#b26">[27]</ref> for a short movie and an example application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XI. DISCUSSION</head><p>As Fig. <ref type="figure" target="#fig_20">21</ref> shows, the original hand posture and the reconstruction match very closely. However, a comparison can only be done visually. This is standard practice. An exact, quantitative analysis would require a measurement with a DataGlove in order to compare the achieved values of the joint angles. But the external finger sensors will also disturb the visual recognition significantly and falsify the results. Therefore, such an evaluation is not feasible.</p><p>The GREFIT system is able to reliably reconstruct the hand posture at a frame rate of 10 Hz. Currently, each image is processed separately. A further improvement could be achieved by including information about the preceding images in the sequence. The perception of motion is often the first processing step in the visual system. Consequently, spatio-temporal variation in an image sequence is frequently used in computer vision, since it can reduce problems from temporary occlusions and restricts the search-area in the image, which results in less processing time. However, this demands the processing of image sequences with a frame-rate that is suitable for the moving object.</p><p>The movement of the fingers is potentially very fast. Therefore, the images have to be grabbed with a high frame rate in order to include sequence information. Recent hardware developments open up this possibility at reasonable cost.</p><p>The current vision-based interface provides one main building block toward making manual operations understandable for machines. Additional important steps will be the recognition of grasping postures and the analysis of bimanual hand-object configurations. Achieving this by purely visual means constitutes a major challenge for neural-network-based adaptive vision systems and is the objective of future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Hierarchical processing: a first neural network computes a coarse estimate of finger tip position based on global image data. This specifies a region of interest for a subsequent network which then performs a local analysis to provide more accurate finger tip coordinates.</figDesc><graphic coords="3,62.10,62.28,203.04,142.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the GREFIT system architecture. Signal paths transmitting images are denoted with a thick line, data paths with a thin line.</figDesc><graphic coords="4,86.10,62.28,421.20,63.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Schematical outline of the handbox of GREFIT. The fixed arrangement of hand, camera and an integrated illumination provides controlled viewing conditions for the camera.</figDesc><graphic coords="4,302.64,164.28,253.44,174.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Preprocessing steps from left to right: original image, object-background-separation, hand-arm-separation, and centroid of the silhouette pixels, resulting final image.</figDesc><graphic coords="4,302.64,405.12,252.00,75.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Hand-arm-separation. The graphs in the lower row depict the number of hand pixels n in each image column x. The horizontal position of the wrist is assumed to be at that column location x for which n 0 n &gt; 1, where 1 &gt; 0 is a prespecified threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(3) below). During training, the net gets new training examples each of which consists of a feature vector and the associated output vector . By comparison of and the reference vectors of the input space, the reference vector with the smallest Euclidean distance where (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 3 )</head><label>3</label><figDesc>At the end of each training step an adaptation of the reference vectors , and the matrix is performed according to the equations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (top) Projection of input images into a 35-dimensional subspace that is spanned by the chosen Gabor functions. (bottom) The projected images.</figDesc><graphic coords="5,308.04,62.28,237.36,121.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. LLM network: A number of units (nodes) compete for influence regions in the feature (input) space. Each node learns a linear mapping that approximates within its influence region the required transformation from feature to output space.</figDesc><graphic coords="5,305.28,223.80,242.88,88.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Accuracy of finger tip position estimates (in pixels) achieved with LLM network of six nodes, using a 35-dimensional feature vector. The average error is indicated by the solid horizontal line inside each box (dark/bright = test/training data). Dotted lines and "whiskers" indicate standard deviation and max error (TH = thumb, FF, MF, RF, LF = fore, middle, ring, little finger). For more details, see text.</figDesc><graphic coords="6,63.96,62.28,202.32,137.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Movement areas of the finger tips (relative to the centroid) in a typical sequence of 100 images (tiny symbols), and positions of the six reference vectors of the LLM-networks in the output-space for all five fingers.</figDesc><graphic coords="6,332.22,62.28,192.00,135.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Error and standard deviation of the pixel error of the global processing by the LLM-networks for all fingers in training and test data.</figDesc><graphic coords="6,330.84,248.16,194.64,136.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Local feature extraction in region of interest identified by first processing stage: application of a Gauss operator yields an edge image from which Gabor features are computed.</figDesc><graphic coords="7,68.34,62.28,190.56,86.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Accuracy of finger tip localization achieved with global networks alone (rows 1 and 3) and position refinement by subsequent networks operating on local regions of interest (rows 2 and 4).</figDesc><graphic coords="7,325.86,244.14,201.60,200.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Combination of different information channels (from left to right): Original image, Gaussian activity profile combined with the edge image, finger tip filter and the product of these confidence images. The pixel with the highest confidence for a finger tip is indicated by a big square, a possible alternate position with a small square.</figDesc><graphic coords="7,325.86,494.16,201.60,170.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Geometric arrangement of the 20 joint DOFs of the hand model used for recovering 3-D posture from 2-D finger tip position data (for abbreviations see text).</figDesc><graphic coords="8,336.36,62.28,183.60,101.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. PSOM used to interpolate (manifold M) a discrete self-organizing map that associates joint angles and finger tip locations in the cartesian product space X.</figDesc><graphic coords="9,323.58,62.28,206.16,169.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Topology of pixel position of single finger tip for the nine combinations of full, medium, and zero flexion ( ) and adduction/abduction ( ), resp. To remove the ambiguity associated with the intersection close to intermediate flexions, the original topology (left) has been replaced by the simplified configuration depicted in the right image.</figDesc><graphic coords="10,49.08,62.28,232.08,69.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 19 .</head><label>19</label><figDesc>Fig.<ref type="bibr" target="#b18">19</ref>. Movement areas of of the tips of the thumb and of the four remaining fingers after applying the simplification of Fig.18. Note that the simplification needs not be applied to the thumb.</figDesc><graphic coords="10,100.74,205.32,128.88,130.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Effect of including the plausibility test on reconstruction of 3-D hand posture from finger tip data. Top (bottom) row: 3-D configuration without (with) plausibility test.</figDesc><graphic coords="10,327.48,62.28,201.36,93.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. The 3-D hand posture reconstruction from hand motion sequence by integrating global processing, local refinement, domain knowledge, context, learned inverse kinematics and plausibility test.</figDesc><graphic coords="11,61.62,62.28,204.00,308.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ALLOWABLE</head><label>I</label><figDesc>RANGES OF JOINT ANGLES OF THE FINGERS (FF, MF, RF, LF) AND THUMB (TH). (AFTER [21])</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The central point is placed on the centroid of the object pixels (see Fig.4and Section V).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>No significant difference occurred within the results for each finger, so the results were summarized.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Claudia NÃ¶lkerreceived the Diploma degree in mathematics in1996 and the Ph.D. degree in computer science in 2001, both from the University of Bielefeld, Bielefeld, Germany.She became a Research Assistant at the Neuroinformatics Research Group in the University's Computer Science Department. Since then, she has carried out research in computer vision, neural networks and human computer interfaces. Subsequently, she joined a startup company in the field of multimedia applications, where she is currently working as a Technical Consultant.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of glove-based input</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sturman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Applicat</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual interpretation of hand gestures for human-computer interaction: A review</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Â´</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="677" to="695" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perception of American Sign Language in dynamic point-light displays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Poizner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bellugi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lutes-Driscoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Experimental Psychol.: Human Performance Perception</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="432" to="440" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fixation behavior in observation and imitation of human movement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mataric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pomplun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Brain Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gesture recognition using colored gloves</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Iwai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Pattern Recognition &apos;96</title>
		<imprint>
			<biblScope unit="page" from="662" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model-based tracking of self-accluding articulated objects</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="612" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast tracking of hands and fingertips in infrared images for augmented desk interface</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IEEE Int. Conf. Automatic Face and Gesture Recognition (FG 2000)</title>
		<meeting>4th IEEE Int. Conf. Automatic Face and Gesture Recognition (FG 2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning 3D-shape perception with local linear maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meyering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Networks</title>
		<meeting>Int. Joint Conf. Neural Networks</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="432" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward an American Sign Language interface</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dorner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="235" to="253" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual tracking of high DOF articulated structures: An application to human hand tracking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vision ECCV&apos;94</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">801</biblScope>
			<biblScope unit="page" from="35" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-based analysis of hand posture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Kunii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Applicat</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="77" to="86" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A mathematical model for hand-shape analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Millar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Gestural Interaction-Proc. Gesture Workshop&apos;96</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Harling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Edwards</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="235" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D deformable hand models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Heap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Gesture Workshop</title>
		<meeting>Gesture Workshop</meeting>
		<imprint>
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3-D hand pose estimation and shape model refinement from a monocular image sequence</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shirai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VSMM&apos;96 GIFU</title>
		<meeting>VSMM&apos;96 GIFU</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="423" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hand gesture estimation and model refinement using monocular camera-Ambiguity limitation by inequality constraints</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Miura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Automatic Face Gesture Recognition</title>
		<meeting>3rd Int. Conf. Automatic Face Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="268" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D hand gesture tracking by model registration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ouhaddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Horain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Synthetic-Natural Hybrid Coding Three Dimensional Imaging</title>
		<meeting>Int. Workshop Synthetic-Natural Hybrid Coding Three Dimensional Imaging</meeting>
		<imprint>
			<date type="published" when="1999-09">Sep 1999</date>
			<biblScope unit="page" from="74" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1233" to="1258" />
			<date type="published" when="1987-12">Dec. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning with the self-organizing map</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<editor>Artificial Neural Networks, T. Kohonen, K. MÃ¤kisara, O. Simula, and J. Kangas</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="379" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Self-Organizing Maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An appearance-based approach to gesturerecognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Image Anal. Processing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</editor>
		<meeting>9th Int. Conf. Image Anal. essing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-09">Sept. 1997</date>
			<biblScope unit="volume">1311</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kapandji</surname></persName>
		</author>
		<title level="m">The Physiology of the Joints: Upper Limbs</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Churchill Livingstone</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parametrized self-organizing maps</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="568" to="577" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation and Self-Organizing Maps</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rapid Learning in Robotics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Cuvillier-Verlag</publisher>
			<pubPlace>GÃ¶ttingen, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rapid learning with parametrized self-organizing maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="131" to="153" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The self-organizing map</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 78</title>
		<meeting>IEEE 78</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="1464" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>NÃ¶lker</surname></persName>
		</author>
		<ptr target="http://www.TechFak.Uni-Bielefeld.de/~claudia/vishand.html" />
		<title level="m">GREFIT website</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
