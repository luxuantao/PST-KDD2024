<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Walk with Dual Agents for Knowledge Graph Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Denghui</forename><surname>Zhang</surname></persName>
							<email>denghui.zhang@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zixuan</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
							<email>liuh@ust.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology (HKUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Lin</surname></persName>
							<email>lin@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>hxiong@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Walk with Dual Agents for Knowledge Graph Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph walking based on reinforcement learning (RL) has shown great success in navigating an agent to automatically complete various reasoning tasks over an incomplete knowledge graph (KG) by exploring multi-hop relational paths. However, existing multi-hop reasoning approaches only work well on short reasoning paths and tend to miss the target entity with the increasing path length. This is undesirable for many reasoning tasks in real-world scenarios, where short paths connecting the source and target entities are not available in incomplete KGs, and thus the reasoning performances drop drastically unless the agent is able to seek out more clues from longer paths. To address the above challenge, in this paper, we propose a dual-agent reinforcement learning framework, which trains two agents (GIANT and DWARF) to walk over a KG jointly and search for the answer collaboratively. Our approach tackles the reasoning challenge in long paths by assigning one of the agents (GIANT) searching on cluster-level paths quickly and providing stage-wise hints for another agent (DWARF). Finally, experimental results on several KG reasoning benchmarks show that our approach can search answers more accurately and efficiently, and outperforms existing RL-based methods for long path queries by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge graphs (KGs) have become an essential building block of various knowledge-driven services, such as relation extraction <ref type="bibr" target="#b15">(Mintz et al. 2009</ref><ref type="bibr">), question answering (Cui et al. 2019)</ref>, and recommender systems <ref type="bibr" target="#b35">(Zhang et al. 2016)</ref>. A KG is usually defined as a directed graph G = (E, R), where E is a collection of entity nodes, and R is a set of relation edges. Due to the highly incomplete nature, in practice, KGs often fail to include sufficient fact triples to satisfy the long-tail scenarios in various tasks. To this end, we focus our study in the context of automatic KG reasoning, also known as knowledge graph completion (KGC), i.e., constructing f (e s , r q , ?|G) or f (?, r q , e t |G) to infer missing facts by synthesizing information from multi-hop paths between the source and target nodes. One example is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, where no direct link can be found on between the target node e t = "U.S." and the source node e s = "Boston". However, by leveraging existing indirect links and the query relation r q = "Locate-dIn", one is possible to infer the fact (BOSTON, LOCATEDIN, U.S.).</p><p>In the past several years, extensive research has been conducted on learning latent representations of entities (e ∈ E) and relations (r ∈ R) for knowledge graph reasoning by using tensor factorization or neural networks <ref type="bibr" target="#b26">(Wang et al. 2014;</ref><ref type="bibr" target="#b30">Yang et al. 2014;</ref><ref type="bibr" target="#b21">Trouillon et al. 2016)</ref>. Such embeddingbased approaches mainly focus on preserving the structural information in the KG and are effective for single-hop reasoning. Also, recent works have considered exploiting reinforcement learning (RL) <ref type="bibr" target="#b3">(Das et al. 2017;</ref><ref type="bibr" target="#b29">Xiong, Hoang, and Wang 2017;</ref><ref type="bibr" target="#b17">Shen et al. 2018)</ref> for KGC reasoning tasks, where a walking agent is leveraged over KG paths to compose single-hop triplets into multi-hop reasoning chains. For instance, MINERVA <ref type="bibr" target="#b3">(Das et al. 2017</ref>) is an end-to-end model that adopts REINFORCE algorithm <ref type="bibr" target="#b18">(Sutton et al. 1999)</ref> to train the RL agent to search over KGs starting from the source and arrive at the candidate answers.</p><p>However, a noteworthy issue of these walking-based models is that they rely heavily on short reasoning chains (e.g., maximum path length=3 in MINERVA), where the performance drops drastically if short indirect paths are also absent. Indeed, such single-agent approaches often get stuck when reasoning on a long path. The reasons are two-fold. First, KGs consist of massive entities and relations, the dimension of the discrete action space at each step is typically large <ref type="bibr" target="#b3">(Das et al. 2017)</ref>. As a result, the difficulty of reasoning (i.e., making right decisions constantly) increases drastically with the increasing number of reasoning steps. Without narrowing down the scope of representative entities and relations, the underlying agent may conduct unnecessary traverse among similar objects, and thus has low efficiency for path finding. Second, prior approaches train the agent with sparse rewards. Specifically, they only return a positive reward when the agent reaches the target entity by the end of a walking, and penalizes all actions within the path otherwise. This may result in false-negative rewards to the intermediate actions which are reasonable, and hinders the policy network from learning trustworthy long-term patterns.</p><p>To tackle the above problems, in this paper, we propose a Collaborative Dual-agent Reinforcement Learning framework, named CURL. Unlike existing walking-based RL models, which rely on one single agent to explore reasoning paths arXiv:2112.12876v1 [cs.AI] 23 Dec 2021 over entities and relations in KGs, we design two agents, GIANT agent and DWARF agent, to perform reasoning at different granularities and search for the answer collaboratively. Specifically, GIANT performs coarse-grained reasoning by walking rapidly over pre-defined abstractive KG clusters, while DWARF carefully traverses entities within each cluster to perform fine-grained reasoning. By leveraging a Collaborative Policy Network, two agents can share historical path information with each other to enhance their state representations. Moreover, we propose a Mutual Reinforcement Reward System to overcome the sparse reward issue. Instead of using a static final reward, we allow DWARF to borrow weighted reward from GIANT for its intermediate steps and vice versa. Intuitively, GIANT provides abstract and milestone-like hints to guide DWARF's behavior and reduce the search space. On the other hand, DWARF provides regularization over GIANT's behavior to help avoid less informative cluster-level reasoning chains. By training two agents jointly, our framework exploits the KG structure more thoroughly, i.e., from both global and local views, long and short reasoning paths, macro and micro trajectories, to improve the effectiveness of KG reasoning tasks. We conduct extensive experimental studies on three real-world KG datasets. The results demonstrate that our approach can search answers more accurately and efficiently than existing embedding-based approaches as well as traditional RL-based methods, and outperforms them on long path queries significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we first review the formal problem definition of knowledge graph reasoning and the single-agent reinforcement learning approach by <ref type="bibr" target="#b3">(Das et al. 2017</ref>). Then we introduce our dual-agent based approach that explores path patterns at two granularity levels to tackle the long-path challenge and sparse reward problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><p>Formally, a knowledge graph G is represented as a directed graph: G = {(e s , r, e o ), e s , e o ∈ E, r ∈ R}, where E is the set of entities and R is the set of relations. Each directed link in the knowledge graph l = (e s , r, e o ) ∈ G corresponds to a real-world fact tuple, e.g., (JOE BIDEN, PRESIDENT OF, UNITED STATES). For the knowledge graph reasoning task, we follow the definition in prior graph walking literature, a.k.a, query answering task <ref type="bibr" target="#b3">(Das et al. 2017)</ref>. Specifically, given a query (e s , r q , ?), where e s is the source entity and r q is the relation of interest, the goal of query answering is to perform efficient search over G and find the possible answers (i.e., correct target entities) E o = {e o } where (e s , r q , e o ) / ∈ G due to incompleteness of KG.</p><p>Although being a promising way to discover new fact knowledge, this task is challenging as it requires an algorithm to be capable of sophisticated multi-hop reasoning over the incomplete KG. Note that we follow the end-to-end walking for reasoning paradigm as in MINERVA, to avoid undesirable requirements such as agent pre-training, path features precomputing, and all entities ranking in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous Single-Agent Reinforcement Walking</head><p>The process of walking (searching) on KG can be viewed as a Markov Decision Process (MDP) <ref type="bibr">(Sutton and Barto 2018)</ref>: given the query entity e s and relation r, the agent departs from e s , sequentially selects an outgoing edge l and traverses to a new entity until it arrives at a target answer or reaches maximum path length. The MDP can be expressed by the following essential components (we eliminate OBSERVATION part in MINERVA <ref type="bibr" target="#b3">(Das et al. 2017)</ref> for simplicity, yet the formulation is equivalent).</p><p>State, Action, Transition, Reward Each state s t = (e t , (e s , r q )) ∈ S is a tuple where e t is the entity visited at step t and (e s , r q ) are the source entity and query relation. e t can be viewed as state-dependent information while (e s , r q ) are the global context shared by all states. The set of possible actions A t ∈ A of at step t consists of the outgoing edges of e t in G. Concretely, A t = {(r , e )|(e t , r , e ) ∈ G}.</p><p>To grant the agent an option to terminate a search, a selfloop edge is added to every A t . Because search is unrolled for a fixed number of steps T , the self-loop acts similarly to a "stop" action. A transition function δ : S × A → S is defined by δ(s t , A t ) = δ(e t , (e s , r q ), A t ). Action probability is predicted by a policy network π θ , which takes as input the state information. Popular choices for π θ includes simple models like MLP and sequence-aware models like RNN. In the default formulation, the agent receives a terminal reward of 1 if it arrives at a correct target entity at the end of search (i.e., within maximal steps) and 0 otherwise, i.e., R b (s T ) = 1{(e s , r q , e T ) ∈ G}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Dual-Agent Reinforcement Walking</head><p>The above single-agent approach can effectively explore short paths on KG and discover short chains of reasoning. However, when the path length increases, the agent tends to miss the target entity and fails to catch meaningful long chains of reasoning. Contrarily, our approach launches two agents: GIANT AGENT and DWARF AGENT (short as GIANT and DWARF respectively), to collaboratively explore paths at different granularity levels and search for the answer. GIANT walks rapidly over inner clusters of the KG, DWARF slowly traverses by entities inside the clusters, while meantime, they share essential path and reward information to each other, taking advantage of a more comprehensive view (i.e., both cluster/global view and entity/local view) of KG to enhance reasoning. Figure <ref type="figure" target="#fig_0">1</ref> presents a concise illustration of our approach CURL.</p><p>Mapping KG to Clusters We first divide an original KG into N clusters of nodes using K-means <ref type="bibr" target="#b15">(MacQueen et al. 1967)</ref> on the pre-trained entity embeddings<ref type="foot" target="#foot_0">1</ref> . Based on these clusters, we also build a cluster-wise connection graph G c , where two clusters will be connected if there is at least one entity-level edge between them. It can be viewed as a denser mapping of the original KG. GIANT aims to walk over G c to reach a "fuzzy answer", i.e., the target cluster in which the end entity lies. GIANT AGENT: Cluster-Level Exploration Following the RL paradigm, GIANT makes moves based on the state. Each state s c t = (c t , c s ) ∈ S c is a tuple where c t ∈ G c represents the cluster visited at step t and c s is the start cluster which the source entity belongs to. Specifically, c t contains state-dependent information while c s are the global context shared by all states. At each step t, the possible actions for GIANT consists of neighbor clusters of c t in G c . Concretely,</p><formula xml:id="formula_0">A c t = {c |(c t , c ) ∈ G c }.</formula><p>In other words, it traverses the KG in a fashion of cluster by cluster. Since cluster-level paths are normally shorter than entity-level paths (e.g., in Figure <ref type="figure" target="#fig_0">1</ref>, length cluster = 3, length entity = 7), GIANT should be allowed to "stay" in some clusters during walking in order to synchronize with DWARF. To fulfill this purpose, a special action, i.e., STOP, is also added into every action space A c t . Each action a c t ∈ A c t is made based on the prediction of the policy network of GIANT: π c θ . After walking multiple required steps, GIANT receives a terminal reward of 1 if it arrives at the cluster c T where a correct entity answer lies in e T ∈ c T , and 0 otherwise.</p><p>DWARF AGENT: Entity-Level Exploration Similar to the single-agent approach, DWARF agent walks over the original KG G to reach an accurate answer, i.e., a target entity. Specifically, each state s e t = (e t , (e s , r q )) is a tuple consisting of the current entity being visited e t , and the query entity, relation, e s and r q . At each step t, to make an action, DWARF selects one from all the outgoing edges of the current entity, A e t = {(r , e )|(e t , r , e ) ∈ G}. The selection is predicted by its own policy network π e θ . Within a maximum number of steps, if the agent arrives at a correct target entity, it will receive a final reward of 1 and 0 otherwise.</p><p>Collaborative Graph Walking: Jointly Train π e θ and π c θ Under the framework of dual-agent, to maximize the benefit of both sides, i.e., cluster-level and entity-level exploration, we propose two advances to tame the two distinct agents to walk in a mutually beneficial way, namely, Collaborative Policy Networks and Mutual Reinforcement Rewards.</p><p>Collaborative Policy Networks Agents make moves based on the output of policy network, we use two separate networks π e θ and π c θ respectively 2 to model the action selection of GIANT and DWARF. Specifically, every entity, 2 The superscript c, e stands for cluster/entity. relation in G, as well as cluster in G c is assigned a dense vector embedding e ∈ R d , r ∈ R d , c ∈ R 2d . We use these embeddings to represent RL actions and states of two agents. For DWARF, each action a e t = (r t , e t ) ∈ A e t consisting of the next outgoing relation and entity, is represented as the concatenation of the relation embedding and the end node embedding a e t = [r t ; e t ] ∈ R 2d . For GIANT, the action corresponds to the next outgoing cluster and we directly use the cluster embedding to represent it, i.e., a c t = c t ∈ R 2d .</p><p>For both π e θ and π c θ , we first use two separate LSTMs to encode their search history h c t = (c 1 , . . . , c t ) ∈ H c , h e t = (e s , r 1 , e 1 , . . . , r t , e t ) ∈ H e according to the recurrent dynamics,</p><formula xml:id="formula_1">h c 0 = LSTM c (0, c 0 ), h e 0 = LSTM e (0, [r 0 ; e s ]),<label>(1)</label></formula><formula xml:id="formula_2">h c t = LSTM c (W c [h c t−1 ; h e t−1 ], a c t−1 ), t &gt; 0,<label>(2)</label></formula><formula xml:id="formula_3">h e t = LSTM e (W e [h e t−1 ; h c t−1 ], a e t−1 ), t &gt; 0,<label>(3)</label></formula><p>where we modify their internal structures to allow state sharing between LSTM c of GIANT and LSTM e of DWARF. Specifically, at each step t &gt; 0, we calculate the concatenation of the two raw states h c t , h e t ∈ R 2d as the new states</p><formula xml:id="formula_4">[h c t ; h e t ], [h e t ; h c t ] ∈ R 4d .</formula><p>In addition, we apply two transforming matrices W c , W e ∈ R 2d×4d to reduce the dimension of the new states, otherwise, their dimension will increase exponentially w.r.t the number of steps. With the modification, each hidden state h c t (h e t ) of an agent is conditioned on: its own previous state h c t−1 (h e t−1 ), the other agent's previous state h e t−1 (h c t−1 ), its previous action a c t−1 (a e t−1 ). This ensures sharing essential path information between GIANT and DWARF, as to some extent, cluster-level are complementary to entity-level paths, it can consequently enable more robust action selection for both of them.</p><p>To predict the next action (i.e., the next cluster for GIANT and next relation-entity edge for DWARF), we further apply a two-layer feedforward network with ReLU nonlinearity on the concatenation of their last LSTM states and current RL state embeddings, </p><formula xml:id="formula_5">π c θ (a c t |s c t ) = σ(A c t × W c 2 ReLU(W c 1 [c t ; h c t ])),<label>(</label></formula><formula xml:id="formula_6">; h e t ])), (5) where W c 1 , W c 2 ∈ R 4d×4d , W e 1 , W e 2 ∈ R 6d×6d are the ma- trices of learnable weights, A c t ∈ R |A c t |×4d , A e t ∈ R |A e t |×6d</formula><p>represent the embeddings of all next possible actions for GIANT and DWARF. σ denotes the softmax operator.</p><p>Mutual Reinforcement Rewards The default rewards for GIANT and DWARF only consider whether their own agent arrives at the target cluster or entity, leaving two problems limiting the dual agent to collaborate effectively: (i) As GI-ANT walks over the cluster graph which is more densely connected, it leads to more diverse trajectories and hard to be consistent with entity-level paths. (ii) DWARF does not learn any stage-wise knowledge from GIANT explicitly. To alleviate the issue, we provide a new mutual reinforcement reward system, which can (1) constrain GIANT to generate cluster trajectories consistent with entity-level paths;</p><p>(2) allow GIANT to provide stage-wise hints for DWARF to follow. Specifically, both agents' final reward consists of two parts, i.e., their own default reward and an auxiliary weighted reward borrowed from their partner,</p><formula xml:id="formula_7">R c (s c t ) = r c (s c T ) default reward + Φ(s c t , s e t ) • r e (s e T )</formula><p>partner reward</p><formula xml:id="formula_8">, t ∈ [1, T ],<label>(6)</label></formula><p>R e (s e t ) = r e (s e T )</p><p>default reward</p><formula xml:id="formula_9">+ Φ(s e t , s c t ) • r c (s c T ) partner reward , t ∈ [1, T ],<label>(7)</label></formula><p>where r c (s c T ) and r e (s e T ) denote the default final rewards for GIANT and DWARFT, defined as r c (s c T ) = 1{∃e a ∈ c T , (e s , r q , e a ) ∈ G}, r e (s e T ) = 1{(e s , r q , e T ) ∈ G}. More- over, Φ(s c t , s e t ) is an evaluation function measuring the consistency of each action made by two agents. In practice, it is calculated as the cosine similarity between the pre-trained embeddings 3 of the current traversed cluster and entity, i.e., Φ(s c t , s e t ) = c t et ||ct||2||et||2 . For GIANT, its partner reward will be valid only if DWARF reaches the target entity by the end, and meantime, the current cluster visited must be close to the corresponding entity at step t so that the weight is sufficient. Similarly, for DWARF, the partner reward will be valid only if GIANT reaches the target cluster by the end and the consistency weight is sufficient. This ensures that both agents learn knowledge from partner only at the right time, i.e., when their partner succeeds to reach the correct target. The measuring coefficient Φ(s c t , s e t ) controls the strength of partner reward based on the path consistency, i.e., the overlap between the cluster-level path and entity-level path.</p><p>The detailed training procedure of CURL is described in Algorithm 1. During inference, we use the same procedure of lines 3-7 below to calculate the action probabilities at each step. To find the target answer, we do a beam search with a beam width of 50 on DWARF agent and rank entities by the probabilities of the trajectory that DWARF took to reach the entity, all other unreachable entities get a rank of ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experiment Setup</head><p>We evaluate the effectiveness of CURL 4 by performing two fundamental tasks using three real-world KG datasets, i.e., FB15K-237, WN18RR, and NELL-995. The WN18RR <ref type="bibr" target="#b4">(Dettmers et al. 2018</ref>) and FB15K-237 <ref type="bibr" target="#b19">(Toutanova et al. 2015)</ref> 3 Each cluster embedding is obtained by averaging all entity embeddings within it. 4  Set current entity and cluster nodes e0 = es, c0 = cs 5:</p><p>for t = 0, . . . , T − 1 do 6:</p><p>Predict the next cluster ct+1 for GIANT and next relationentity edge (rt+1, et+1) for DWARF based on Eq. ( <ref type="formula">4</ref>) -(5) 7:</p><p>end for 8:</p><p>Set default cluster-level reward rc = 1 if the end of the path cT = co otherwise rc = 0 9:</p><p>Set default entity-level reward re = 1 if the end of the path eT = eo otherwise re = 0 10:</p><p>Repeat lines 5 -9 for running L rollouts (see the expectation in Eq. ( <ref type="formula">8</ref>) -( <ref type="formula">9</ref>)) to update the cluster-level and entity-level policies 11:</p><p>Compute the mutual reinforcement rewards Re(s e t ), Rc(s c t ) based on Eq. ( <ref type="formula" target="#formula_8">6</ref>) -( <ref type="formula" target="#formula_9">7</ref>) 12:</p><p>Update the model parameters with REINFORCE: Link Prediction (Query Answering) Given an incomplete KG, the link prediction task aims to predict the missing entities in the unknown links. In our settings, for a query (e 1 , r, ?) or (?, r, e 2 ), we run multiple rollouts to search for  Model FB15K-237 WN18RR NELL-995 @1 @3 @10 MRR @1 @3 @10 MRR @1 @3 @10 MRR answer node based on query relation and source entity, and then do a beam search with a beam width of 50 to rank the entities by the probability of their trajectories reaching the correct entity. Here, we use Hits@1, 3, 10 and Mean Reciprocal Ranking (MRR) as standard ranking metrics <ref type="bibr" target="#b29">(Xiong, Hoang, and Wang 2017;</ref><ref type="bibr">Sutton and Barto 2018)</ref>.</p><formula xml:id="formula_10">θ c ← θ c + α • ∇ θ c E A c 1 ,...,A c T ∼π c θ T −1 t=0 [Rc(s c t )|s c 0 ] (8) θ e ← θ e + α • ∇ θ e E A e</formula><p>Fact Prediction Subtly different from link prediction, fact prediction task targets at inferring whether an unknown fact (triple) holds or not. According to <ref type="bibr" target="#b29">(Xiong, Hoang, and Wang 2017)</ref>, the true test triples are ranked with some generated false triples. In the experiments, we first remove all links of groundtruth relations in the raw KG. Then the dual agents try to infer and walk through the KG to reach the target entity. Since we share a similar query-answering mechanism as MINERVA <ref type="bibr" target="#b3">(Das et al. 2017)</ref>, CURL can directly locate the correct entity node for a given query, and eliminate the need to evaluate negative samples any particular relation. Note that if CURL fails to reach any of the entities in the set of correct and negative entities, it then falls back to a random ordering of the entities. Here, we report Mean Average Precision (MAP) scores for various relation tasks of NELL-995 (corresponding to different subsets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Performance</head><p>As demonstrated in Table <ref type="table" target="#tab_3">1</ref>, we first report the performances of CURL and all baselines on the link prediction task. The results of NeuralLP are not included on NELL-995 because it can not scale to the size larger dataset. We observe that on FB15K-237, the embedding based methods dominate over several neural multi-hop reasoning approaches. With deeper investigation, we discover that the structural characteristics of FB15K-237 differ significantly from WN18RR and NELL-995, since it contains much larger number of 1-to-M than the M-to-1 relation instances <ref type="bibr" target="#b22">(Wan et al. 2020)</ref>. This indicates that the search process of multi-hop reasoning methods is prone to be stuck in the local nodes with high-degree centrality, renders it hard to reach the correct entity. Note that on WN18RR, neural symbolic methods (MINERVA, Neu-ralLP, M-Walk) generally beat embedding based methods, with CURL achieving the highest score on Hits@1, 3 and MRR metrics. On NELL-995, our approach delivers comparable performance to embedding based methods, further outperforms MINERVA by a clear margin on all metrics.</p><p>After averaging results on three datasets, we find that our dual-agent based approach leads to overall improvements relative to the single-agent approach with similar settings (MIN-ERVA) by 3.1%, 2.1%, 1.7%, 2.3%, in terms of Hits@1, 3, 10 and MRR. Table <ref type="table" target="#tab_4">2</ref> reports the performance of fact prediction on 10 relation tasks of NELL-995. Our approach produces an encouraging result in most tasks, contributing an average gain of 7.8% relative to the multi-hop neural methods (PRA, Deep-Path, MINERVA, and M-walk) and 14.8% gain compared to the embedding-based approaches <ref type="bibr">(TransR and TransE)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of CURL and Case Studies</head><p>Based on the above results, we conducted the analysis to discuss the superiority of our dual-agent framework on KG reasoning. First, the above quantitative results show that our dual-agent design contributes significant gains relative to the      DWARF-solely method (i.e., MINERVA) on all datasets, confirming the effectiveness of our high-level motivation that using cluster-level reasoning to guide entity-level reasoning can alleviate the long path challenge and sparse reward issue. To further examine this, in Figure <ref type="figure" target="#fig_2">2</ref>, we show the positive reward rate (i.e., the percentage of trajectories with positive reward during training) on the NELL-995 tasks. Compared to MINERVA under the same training and testing procedure, CURL is capable of generating trajectories with more positive rewards, and this continues to improve as training progresses. Additionally, in Figure <ref type="figure">5</ref> (in Appendix B.1), we show the Hits@1 performance change w.r.t the increasing path length, where we find that CURL maintains a relatively stable result or slower ratio of performance degradation. These two evidences prove that introducing GIANT agent by our Collaborative Walking and Mutual Reinforcement Reward can indeed benefit the entity-level KG reasoning. Finally, we present some illustrative examples of paths found by CURL in Table <ref type="table">3</ref>. Example (i) and (ii) illustrate that CURL is capable of capturing both short and long reasoning chains for diverse tasks. Example (iii) displays the ability to correct a previously taken decision even in the long paths, where our model took an incorrect decision at the first step but was able to revert the decision because of the presence of inverted edges. This property is similar to MINERVA, which however cannot well handle the long-path reasoning compared to us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Path Reasoning Performance</head><p>While short chains (length≤ 3) can partially support KG reasoning and navigate the agent to find target answers <ref type="bibr" target="#b3">(Das et al. 2017)</ref>, we consider a more practical and rigorous scenario in real-world KGs, where short paths are mostly absent from incomplete KGs. For effective evaluation, we compare our model with MINERVA in NELL-995 dataset, where we remove the most frequently-visited short paths found by the bi-directional search (BiS) <ref type="bibr" target="#b29">(Xiong, Hoang, and Wang 2017)</ref> inside KG. In particular, given a triplet (e s , r, e o ) in the training or testing set, we sample an intermediate entity node e i and use the breadth-first search algorithm <ref type="bibr" target="#b2">(Bundy and Wallen 1984)</ref> to verify the traversability from e s to e i and from e i to e o inside KG. After conducting the BiS on each task 50 times, we choose the walkable paths with a length smaller than 3 (self-included), and eliminate their traversed edges inside original KGs. Figure <ref type="figure" target="#fig_8">4</ref> plots the MAP scores on varying path lengths in various tasks. As can be seen, CURL outperforms MINERVA across all three tasks with different path lengths, and our gains are much more prominent when path length is 5. This explains that the cluster-level exploration is essential to lead entity-level agent rather than doing purely random walks in the neighborhood of the source entity. Overall, CURL is much more robust to queries where a longer reasoning path is required, showing minimal degradation in performance for even the longest path setting. Additional experiments on FB15K-237, WN18RR, and NELL-995 can be found in Appendix B.2 of the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity Test on Cluster Size</head><p>Figure <ref type="figure" target="#fig_6">3</ref> shows the Hits@10 scores of CURL under different cluster number N during training on four NELL-995 tasks. We observe a performance improvement when we increase N from 50 to 75 and performance degrade when we further increase N from 75 to 500. These results illustrate 75 cluster number is powerful enough to capture high-level semantic information. In other words, parametrized by a suitable N , our approach is generally stable when the path lengths increase, indicating that proper clustering of KG entities does refine a series of meaningful high-level semantics to facilitate the low-level path searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Knowledge Graph Representation Learning Representation learning has shown great success in a wide range of fields <ref type="bibr">(Zhang et al. 2019</ref><ref type="bibr" target="#b33">(Zhang et al. , 2017;;</ref><ref type="bibr" target="#b11">Li et al. 2021;</ref><ref type="bibr" target="#b32">Yuan et al. 2021)</ref> and KG reasoning is no exception. Recent advances in this area have proposed a variety of embeddingbased methods that project the entities and relations into low-dimensional continuous vector space by exploiting entity types <ref type="bibr" target="#b5">(Guo et al. 2015;</ref><ref type="bibr" target="#b17">Ouyang et al. 2017</ref>), relation paths <ref type="bibr" target="#b14">(Lin et al. 2015a;</ref><ref type="bibr" target="#b20">Toutanova et al. 2016;</ref><ref type="bibr" target="#b11">Li et al. 2018a;</ref><ref type="bibr" target="#b36">Zhang et al. 2018)</ref>, textual descriptions <ref type="bibr" target="#b38">(Zhong et al. 2015;</ref><ref type="bibr" target="#b28">Xiao et al. 2017)</ref>, and logical rules <ref type="bibr" target="#b16">(Omran, Wang, and Wang 2018;</ref><ref type="bibr" target="#b6">Hamilton et al. 2018)</ref>. For instance, TransE <ref type="bibr" target="#b26">(Wang et al. 2014</ref>) first encoded the entities and relations into latent vectors by following translational principle in point-wise Euclidean space. Besides, ComplEx <ref type="bibr" target="#b21">(Trouillon et al. 2016)</ref> introduced complex vector space to capture both symmetric and anti-symmetric relations. However, such models ignored the symbolic compositionality of KG relations, making them unable to discover complex reasoning paths with one-hop distance-based measure <ref type="bibr" target="#b23">(Wang et al. 2017;</ref><ref type="bibr">Ji et al. 2021)</ref>. Furthermore, it is hard to interpret the traversal paths, and these models can be computationally expensive to access the entire graph in memory.</p><p>Deep Reinforcement Learning for KG Reasoning The emergence of deep reinforcement learning (RL) enables many path-based approaches to learn symbolic inference rules from relational paths inside KG <ref type="bibr">(Ji et al. 2021)</ref>. By formulating KG reasoning as a sequential decision problem and taking multi-hop random walks, existing studies improve the empirical performance of various tasks <ref type="bibr" target="#b29">(Xiong, Hoang, and Wang 2017;</ref><ref type="bibr" target="#b17">Shen et al. 2018;</ref><ref type="bibr" target="#b13">Lin, Socher, and Xiong 2018;</ref><ref type="bibr" target="#b3">Das et al. 2017;</ref><ref type="bibr" target="#b22">Wan et al. 2020;</ref><ref type="bibr" target="#b6">Hildebrandt et al. 2020</ref>) including KG completion, fact prediction, and query answering. Specifically, DeepPath <ref type="bibr" target="#b29">(Xiong, Hoang, and Wang 2017)</ref> first introduced RL to search for diversified representative paths between entity pairs. M-Walk (Shen et al. 2018) further proposed to solve the reward sparsity problem in MCTS-based query answering in an off-policy manner. Reward shaping based approach <ref type="bibr" target="#b13">(Lin, Socher, and Xiong 2018</ref>) leveraged pretrained embeddings to estimate the likelihood of unseen facts, with the purpose of reducing the impact of false-negative supervision as well as facilitating the path inference. Our work aligns with the RL formulation of MINERVA <ref type="bibr" target="#b3">(Das et al. 2017)</ref>, i.e., learning to walk and search for answer entities of a particular KG query in an end-to-end fashion. Note that MINERVA solely exploited the entity-level KG information to update the policy network. In comparison, we trained the entity-level policy of the DWARF AGENT using the cluster-level semantics and trajectories generated by the GIANT AGENT, such that the dual agents can collaborative to reach the optimal answers given queries.</p><p>Regarding the dual-agent structure, our work is closely related to the hierarchical RL <ref type="bibr" target="#b0">(Barto and Mahadevan 2003;</ref><ref type="bibr" target="#b25">Wang, Li, and He 2018;</ref><ref type="bibr" target="#b12">Li et al. 2018b;</ref><ref type="bibr" target="#b22">Wan et al. 2020;</ref><ref type="bibr" target="#b24">Wang et al. 2020;</ref><ref type="bibr" target="#b27">Wen et al. 2020)</ref> in the sense of leveraging both low-level and high-level policies to solve long-horizon problems with sparse rewards. However, instead of requiring high-level policy to specify subgoals for low-level skills, CURL enables both policies to achieve the subgoals possessed by their counterparts. That is, with the faster convergence in a reduced search space, GIANT AGENT can provide high-level stepwise guidance for DWARF AGENT to follow, while DWARF AGENT walks along the real-existing paths, thus checking the correctness of abstractive cluster-level paths found by GIANT AGENT. Such a learning scheme can avoid extra design efforts for complex subgoal space, which is not always trustworthy and tractable <ref type="bibr" target="#b37">(Zhang, Yao, and Chen 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We proposed a dual-agent framework (CURL) that learns to walk over a KG for searching desired answer nodes given query relation and source entity. Specifically, we first leveraged LSTM to project trajectory history into latent vectors of different granularities and semantics. To facilitate the entitylevel exploration, CURL launched two agents: GIANT and DWARF to collaboratively explore paths at different granularity levels and search for the answer. GIANT walks rapidly over inner clusters of the KG, which can guide DWARF to smoothly traverse through the entities inside the clusters. We later developed the collaborative policy network for sharing historical path information between two agents, and established the mutual reinforcement reward system for handling sparse reward issue. Experimental results on several knowledge graph reasoning benchmarks show that our approach can search for answers more accurately and efficiently. Furthermore, we compared with the DWARF-solely MINERVA in the long-path experiment. We found that our method is more accurate in long path reasoning, which can be explained by that the stage-wise signals provided by GIANT do play a critical role in leading the DWARF towards the target node.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative diagram of the dual-agent reinforcement walking approach. Two agents work collaboratively to find the target answer (GIANT walks by clusters, DWARF walks by entities).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The positive reward rate on three NELL-995 relation tasks (a -c) and all tasks (d), and our agent is consistently better than MINERVA.</figDesc><graphic url="image-10.png" coords="5,82.11,63.96,105.84,95.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The effect of different cluster numbers used by GIANT. We present the link prediction performance on four relation tasks on NELL-995. Lines in different colors indicate results by different cluster numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The long-path performance: CURL significantly outperforms MINERVA on NELL-995.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Initial entity and cluster nodes es, cs; Entity-level query rq; Target entity and cluster nodes eo, co; Maximum path length T ; Episode size P , Rollout size L 2: Output:Well-trained policy networks π c θ , π e θ of GIANT and</figDesc><table /><note>Source code: https://github.com/RutgersDM/DKGR/tree/master Algorithm 1: CURL Training Algorithm 1: Input: KG G; DWARF 3: for episode p in {1, . . . , P } do 4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>TransE 24.8 40.1 45.0 36.1 28.9 46.4 53.4 35.9 51.4 67.8 75.1 45.6 DistMult 27.5 41.7 56.8 37.0 41.0 44.1 47.5 43.3 61.0 73.3 79.5 68.0 ComplEx 30.3 43.4 57.2 39.4 38.2 43.3 48.0 41.5 61.2 76.1 82.1 68.4 NeuralLP 16.6 24.8 34.8 22.7 37.6 46.8 65.7 45.9 ----MINERVA 19.2 30.7 42.6 27.1 41.3 45.6 51.3 44.8 58.8 74.6 81.3 67.5 M-Walk 16.8 24.5 40.3 23.4 41.5 44.7 54.3 43.7 63.2 75.7 81.9 70.7 CURL 22.4 34.1 47.0 30.6 42.9 47.1 52.3 46.0 66.7 78.6 84.3 73.8Query answering performance compared to state-ofthe-art embedding based approaches (top part) and multi-hop reasoning approaches (bottom part). The Hits@1, 3, 10 and MRR metrics were multiplied by 100. We highlight the best approach in each category.</figDesc><table><row><cell>Task</cell><cell cols="6">TransR TransE PRA DeepPath MINERVA M-Walk CURL</cell></row><row><cell>AthletePlaysInLeague</cell><cell>91.2</cell><cell>77.3 84.1</cell><cell>96.0</cell><cell>94.0</cell><cell>96.1</cell><cell>97.1</cell></row><row><cell>AthletePlaysForTeam</cell><cell>67.3</cell><cell>62.7 54.7</cell><cell>75.0</cell><cell>80.0</cell><cell>84.7</cell><cell>82.9</cell></row><row><cell>AthleteHomeStadium</cell><cell>72.2</cell><cell>71.8 85.9</cell><cell>89.0</cell><cell>89.8</cell><cell>91.9</cell><cell>94.3</cell></row><row><cell>TeamPlaysSports</cell><cell>81.4</cell><cell>76.1 79.1</cell><cell>73.8</cell><cell>88.0</cell><cell>88.4</cell><cell>88.7</cell></row><row><cell>AthletePlaysSport</cell><cell>96.3</cell><cell>87.6 47.4</cell><cell>95.7</cell><cell>98.0</cell><cell>98.3</cell><cell>98.4</cell></row><row><cell cols="2">OrganizationHiredPerson 73.7</cell><cell>71.9 59.9</cell><cell>74.2</cell><cell>85.6</cell><cell>88.8</cell><cell>87.6</cell></row><row><cell>PersonBornInLocation</cell><cell>81.2</cell><cell>71.2 66.8</cell><cell>75.7</cell><cell>78.0</cell><cell>81.2</cell><cell>82.1</cell></row><row><cell>WorksFor</cell><cell>69.2</cell><cell>67.7 68.1</cell><cell>71.1</cell><cell>81.0</cell><cell>83.2</cell><cell>82.1</cell></row><row><cell cols="2">OrgHeadquarteredInCity 65.7</cell><cell>62.0 81.1</cell><cell>79.0</cell><cell>94.0</cell><cell>94.3</cell><cell>94.8</cell></row><row><cell cols="2">PersonLeadsOrganization 77.2</cell><cell>75.1 70.0</cell><cell>79.5</cell><cell>87.7</cell><cell>88.3</cell><cell>88.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance on fact prediction, MAP scores for different relation tasks in NELL-995 dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We apply TransE(Bordes et al.  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2013" xml:id="foot_1">) for its efficiency in encoding the structural closeness information of KG.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">We obtain and report the best results using the code and hyperparameters released by the authors of the baseline models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by the National Science Foundation through award 2006387, 1814510, 2040799.   </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>US Government</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Parameter Sensitivity on Beam Search Size</head><p>We also check the influence of different beam search size during testing, as reported in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Long Path Recovery</head><p>We conduct further long-path experiments on three datasets, including FB15K-237, WN18RR, and NELL-995. Here, we investigate the ability of recovering from mistakes in the long-path traversal. Unlike Section "Long Path Reasoning Performance", we retain their original KGs and set the walkable path length to be relatively larger (&gt; 3), since short chains in these datasets can usually produce good empirical results <ref type="bibr" target="#b3">(Das et al. 2017;</ref><ref type="bibr" target="#b22">Wan et al. 2020)</ref>, and the longer ones are more likely to walk through unnecessary or incorrect links. As reported in Figure <ref type="figure">5</ref>, CURL outperforms the other DWARF-only method (i.e., MINERVA) in all datasets. This observation demonstrates that dual-agent design is able to stabilize the entity-level searching in long paths by utilizing the cluster-level reasoning information. One possible reason is that GIANT converges much faster in a reduced search space, consistently providing high-level stage-wise guidance for DWARF to follow.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Recent Advances in Hierarchical Reinforcement Learning. Discrete event dynamic systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="41" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-Relational Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Breadth-First Search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wallen</surname></persName>
		</author>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02419</idno>
	</analytic>
	<monogr>
		<title level="m">KBQA: Learning Question Answering over QA Corpora and Knowledge Bases</title>
				<imprint>
			<date type="published" when="1984">1984. 2019</date>
			<biblScope unit="page" from="13" to="13" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Catalogue of artificial intelligence tools</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05851</idno>
		<title level="m">Go For A Walk And Arrive At The Answer: Reasoning Over Paths In Knowledge Bases Using Reinforcement Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantically Smooth Knowledge Graph Embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="84" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reasoning on Knowledge Graphs with Debate Dynamics</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A Q</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ringsquandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01445</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4123" to="4131" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Embedding logical queries on knowledge graphs</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m">A Survey on Knowledge Graphs: Representation, Acquisition, and Applications. IEEE Transactions on Neural Networks and Learning Systems</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A Method for Stochastic Optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random Walk Inference and Learning in A Large Scale Knowledge Base</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 conference on empirical methods in natural language processing</title>
				<meeting>the 2011 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Link Prediction in Knowledge Graphs: A Hierarchy-Constrained Approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08179</idno>
	</analytic>
	<monogr>
		<title level="m">Interpretable Time-series Representation Learning With Multi-Level Disentanglement</title>
				<imprint>
			<date type="published" when="2018">2018a. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Path reasoning over knowledge graph: A multi-agent and reinforcement learning based method</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining Workshops (ICDMW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="929" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10568</idno>
		<title level="m">Multi-Hop Knowledge Graph Reasoning with Reward Shaping</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling Relation Paths for Representation Learning of Knowledge Bases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00379</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015a. 2015b</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning Entity and Relation Embeddings for Knowledge Graph Completion</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Some Methods for Classification and Analysis of Multivariate Observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
				<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="1967">1967. 2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable Rule Learning via Learning Representation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2149" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation Learning with Entity Topics for Knowledge Graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<idno>arXiv:1802.04394</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Science, Engineering and Management</title>
				<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2017">2017. 2019. 2018. 2018</date>
			<biblScope unit="page" from="534" to="542" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Reinforcement learning: An introduction</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Policy Gradient Methods for Reinforcement Learning with Function Approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPs</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
				<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compositional Learning of Embeddings for Relation Paths in Knowledge Base and Text</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>-T.; Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1434" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reasoning Like Human: Hierarchical Reinforcement Learning for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence 2020, 1926-1932. Association for the Advancement of Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding: A Survey of Approaches and Applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">H2KGAT: Hierarchical Hyperbolic Knowledge Graph Attention Network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4952" to="4962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning for NLP</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On Efficiency in Hierarchical Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ibrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>-T.; He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08367</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Differentiable Learning of Logical Rules for Knowledge Base Reasoning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-Supervised Prototype Representation Learning for Event-Based Corporate Profiling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient parallel translating embedding for knowledge graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Web Intelligence</title>
				<meeting>the International Conference on Web Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="460" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Job2Vec: Job title benchmarking with collective multi-view representation learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2763" to="2771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discriminative Path-Based Knowledge Graph Embedding for Precise Link Prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="276" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07132</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aligning Knowledge and Text Embeddings by Entity Descriptions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="267" to="272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
