<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The best-so-far selection in Artificial Bee Colony algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-12-04">4 December 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anan</forename><surname>Banharnsakun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">King Mongkut&apos;s University of Technology Thonburi</orgName>
								<address>
									<settlement>Bangkok</settlement>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tiranee</forename><surname>Achalakul</surname></persName>
							<email>tiranee@cpe.kmutt.ac.th</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">King Mongkut&apos;s University of Technology Thonburi</orgName>
								<address>
									<settlement>Bangkok</settlement>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Booncharoen</forename><surname>Sirinaovakul</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">King Mongkut&apos;s University of Technology Thonburi</orgName>
								<address>
									<settlement>Bangkok</settlement>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The best-so-far selection in Artificial Bee Colony algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-12-04">4 December 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">42B811D0F658162687A6766471B4B343</idno>
					<idno type="DOI">10.1016/j.asoc.2010.11.025</idno>
					<note type="submission">Received 15 February 2010 Received in revised form 31 May 2010 Accepted 28 November 2010</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Artificial Bee Colony Swarm intelligence Optimization Image registration Mutual information</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Artificial Bee Colony (ABC) algorithm is inspired by the behavior of honey bees. The algorithm is one of the Swarm Intelligence algorithms explored in recent literature. ABC is an optimization technique, which is used in finding the best solution from all feasible solutions. However, ABC can sometimes be slow to converge. In order to improve the algorithm performance, we present a modified method for solution update of the onlooker bees in this paper. In our method, the best feasible solutions found so far are shared globally among the entire population. Thus, the new candidate solutions are more likely to be close to the current best solution. In other words, we bias the solution direction toward the best-so-far position. Moreover, in each iteration, we adjust the radius of the search for new candidates using a larger radius earlier in the search process and then reduce the radius as the process comes closer to converging. Finally, we use a more robust calculation to determine and compare the quality of alternative solutions. We empirically assess the performance of our proposed method on two sets of problems: numerical benchmark functions and image registration applications. The results demonstrate that the proposed method is able to produce higher quality solutions with faster convergence than either the original ABC or the current state-of-the-art ABC-based algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Swarm Intelligence is a meta-heuristic method in the field of artificial intelligence that is used to solve optimization problems. It is based on the collective behavior of social insects, flocks of birds, or schools of fish. These animals can solve complex tasks without centralized control.</p><p>Researchers have analyzed such behaviors and designed algorithms that can be used to solve combinatorial and numerical optimization problems in many science and engineering domains. Previous research <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> has shown that algorithms based on Swarm Intelligence have great potential. The algorithms that have emerged in recent years include Ant Colony Optimization (ACO) <ref type="bibr" target="#b4">[5]</ref> based on the foraging behavior of ants, and Particle Swarm Optimization (PSO) <ref type="bibr" target="#b5">[6]</ref> based on the behaviors of bird flocks and fish schools.</p><p>Exploration and exploitation are the important mechanisms in a robust search process. While exploration process is related on the independent search for an optimal solution, exploitation uses existing knowledge to bias the search. In the recent years, there are a few algorithms based on bee foraging behavior developed to improve both exploration and exploitation for solving the numerical optimization problems.</p><p>The Artificial Bee Colony (ABC) algorithm introduced by D. Karaboga <ref type="bibr" target="#b6">[7]</ref> is one approach that has been used to find an optimal solution in numerical optimization problems. This algorithm is inspired by the behavior of honey bees when seeking a quality food source. The performance of ABC algorithm has been compared with other optimization methods such as Genetic Algorithm (GA), Differential Evolution algorithm (DE), Evolution Strategies (ES), Particle Swarm Optimization, and Particle Swarm Inspired Evolutionary Algorithm (PS-EA) <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. The comparisons were made based on various numerical benchmark functions, which consist of unimodal and multimodal distributions. The comparison results showed that ABC can produce a more optimal solution and thus is more effective than the other methods in several optimization problems <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>Yang <ref type="bibr" target="#b13">[14]</ref> introduced an algorithm called the Virtual Bee Algorithm (VBA) for solving engineering optimizations that have multipeaked functions. In the VBA algorithm, the objectives or optimization functions are encoded as virtual foods. Virtual bees are used to search for virtual foods in the search space. The position of each virtual bee is updated via the virtual pheromone from the neighboring bees. The food with largest number of virtual bees or intensity of visiting bees corresponds to the optimal solution. However, the VBA algorithm was only tested using two-dimension functions.</p><p>An optimization algorithm inspired by the honey bee foraging behavior based on the elite bee method was proposed by Sundareswaran <ref type="bibr" target="#b14">[15]</ref>. The bee whose solution is the best possible solution in each simulation iteration is considered to be the elite bee. A probabilistic approach is used to control the movement of the other bees, so majority of bees will follow the elite bee's direction while a few bees may fly to other directions. This approach improves the capability of convergence to a global optimum.</p><p>To improve the exploration and exploitation of foraging behavior of honey bees for numerical function optimization, Akbari et al. <ref type="bibr" target="#b15">[16]</ref> presented an algorithm called Bee Swarm Optimization (BSO).</p><p>In this method, the bees of the swarm are sorted according to the fitness values of the most recently visited food source and these sorted bees are divided into three types. The bees that have worst fitness are classified as scout bees, while the rest of bees are divided equally as experienced foragers and onlookers. Different flying patterns were introduced for each type of bee to balance the exploration and exploitation in this algorithm. The experimental results from these algorithms show that the algorithms based on bee foraging behavior can successfully solve numerical optimization problems. However, in some cases the convergence speed can be an issue. This paper introduces a modified version of ABC in order to improve the algorithm's performance.</p><p>First, we test our modified algorithm using the same benchmarks as the previously cited research including the original ABC and the BSO algorithm. Then, we apply our method to optimize the mutual information value in order to measure similarity in an image registration process. We compare the registration results between the original ABC and our best-so-far method.</p><p>The paper is organized as follows. Section 2 describes the original ABC algorithm. Section 3 presents our best-so-far method. Section 4 describes the automated image registration problem. Section 5 presents the experiments. Section 6 compares and discusses the performance results of the best-so-far method with the original method and BSO algorithm. Finally, Section 7 offers our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The original ABC algorithm</head><p>The ABC algorithm assumes the existence of a set of computational agents called honey bees. The honey bees in this algorithm are categorized into three groups: employed bees, onlooker bees and scout bees. The colony is equally separated into employed bees and onlooker bees. Each solution in the search space consists of a set of optimization parameters which represent a food source position. The number of employed bees is equal to the number of food sources. In other words, there is only one employed bee on each food source. The quality of food source is called its "fitness value" and is associated with its position. The process of bees seeking for good food sources is the process used to find the optimal solution.</p><p>The employed bees will be responsible for investigating their food sources and sharing the information about these food sources to recruit the onlooker bees. The onlooker bees will make a decision to choose a food source based on this information. The food source that has higher quality will have a larger chance to be selected by onlooker bees than one of lower quality. An employed bee whose food source is rejected as low quality by employed and onlooker bees will change to a scout bee to search randomly for new food sources. By this mechanism, the exploitation will be handled by employed and onlooker bees while the exploration will be maintained by scout bee. The details of the algorithm are as follows.</p><p>First, randomly distributed initial food source positions are generated. The process can be represented by Eq. (2.1). After initialization, the population is subjected to repeated cycles of three major steps: updating feasible solutions, selecting feasible solutions, and avoiding suboptimal solutions.</p><formula xml:id="formula_0">F(x i ), x i ∈ R D , i∈ {1, 2, 3, . . . , SN},<label>(2.1)</label></formula><p>x i is a position of food source as a D-dimensional vector, F(x i ) is the objective function which determines how good a solution is, and SN is the number of food sources. </p><formula xml:id="formula_1">= D i=1 x 2 i Uni-modal, Separable -100 ≤ x i ≤ 100 0 Griewank f2( x) = 1 4000 D i=1 x 2 i - D i=1 cos x i √ 1 + 1 Multi-modal, Non-Separable -600 ≤ x i ≤ 600 0 Rastrigin f3( x) = D i=1 (x 2 i -10 cos(2 x i ) + 10)</formula><p>Multi-modal, Separable -5.12 ≤ x i ≤ 5.12 0</p><formula xml:id="formula_2">Rosenbrock f4( x) = D i=1 100(x 2 i -x i+1 ) 2 + (1 -x i ) 2 Uni-modal, Non-Separable -30 ≤ x i ≤ 30 0 Ackley f5( x) = 20 + e -20e -0.2 (1/D) D i=1 x 2 i -e (1/D) D i=1 cos(2 x i ) Multi-modal, Non-Separable -30 ≤ x i ≤ 30 0 Schaffer f6( x) = 0.5 + (sin x 2 1 +x<label>2</label></formula><p>2 ) 2 -0.5</p><formula xml:id="formula_3">(1+0.001(x 2 1 +x<label>2 2</label></formula><p>))</p><p>2</p><p>Multi-modal, Non-Separable -100 ≤ x i ≤ 100 0</p><p>In order to update feasible solutions, all employed bees select a new candidate food source position. The choice is based on the neighborhood of the previously selected food source. The position of the new food source is calculated from equation below.</p><formula xml:id="formula_4">ij = x ij + ij (x ij -x kj ) (2.2)</formula><p>In the Eq. (2.2), ij is a new feasible solution that is modified from its previous solution value (x ij ) based on a comparison with the randomly selected position from its neighboring solution (x kj ). ij is a random number between [-1,1] which is used to randomly adjust the old solution to become a new solution in the next iteration. k ∈ {1,2,3,. . ., SN} and j ∈ {1,2,3,. . ., D} are randomly chosen indexes. The difference between x ij and x kj is a difference of position in a particular dimension. The algorithm changes each position in only one dimension in each iteration. Using this approach, the diversity of solutions in the search space will increase in each iteration.</p><p>The old food source position in the employed bee's memory will be replaced by the new candidate food source position if the new position has a better fitness value. Employed bees will return to their hive and share the fitness value of their new food sources with the onlooker bees.</p><p>In the next step, each onlooker bee selects one of the proposed food sources depending on the fitness value obtained from the employed bees. The probability that a food source will be selected can be obtained from an equation below.</p><formula xml:id="formula_5">P i = fit i SN n=1 fit n (2.3)</formula><p>where fit i is the fitness value of the food source i, which is related to the objective function value (F(x i )) of the food source i.</p><p>The probability of a food source being selected by the onlooker bees increases as the fitness value of a food source increases. After the food source is selected, onlooker bees will go to the selected food source and select a new candidate food source position in the neighborhood of the selected food source. The new candidate food source can be expressed and calculated by Eq. (2.2).</p><p>In the third step, any food source position that does not improve the fitness value will be abandoned and replaced by a new position that is randomly determined by a scout bee. This helps avoid suboptimal solutions. The new random position chosen by the scout bee will be calculated from equation below.</p><formula xml:id="formula_6">x ij = x min j + rand[0, 1] * (x max j -x min j ) (2.4)</formula><p>where x min j is the lower bound of the food source position in dimension j and x max j is the upper bound of the food source position in dimension j.</p><p>The maximum number of cycles (MCN) is used to control the number of iterations and is a termination criterion. The process will be repeated until the output of the objective function reaches a defined threshold value or the number of iteration equals the MCN. A defined threshold value is set to be equal to the global minimum value or the global maximum value depending on the type of the optimization problems.</p><p>Although the activities of exploitation and exploration are well balanced and help to mitigate both stagnation and premature convergence in the ABC algorithm, the convergence speed still is the major issue in some situations in the ABC algorithm.</p><p>In our best-so-far ABC, the exploitation process is focused on the best-so-far food source. This food source is used in the comparison process for updating the new candidate food source to accelerate the convergence speed. The searching ability of the exploration process is also enhanced by the scout bee. It will randomly generate a new food source to avoid local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The best-so-far ABC algorithm</head><p>To enhance the exploitation and exploration processes, we propose to make three major changes by introducing the best-so-far method, an adjustable search radius, and an objective-value-based comparison method.</p><p>The computational complexity is also addressed in order to evaluate the computational complexity of the best-so-far ABC with the original ABC and the BSO algorithms.</p><p>The pseudo-code of the best-so-far ABC algorithm is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. In this pseudo-code, the best-so-far method is shown in line number 37 where the onlooker bees will register the best selected food source position so far within the new candidate generation function. The adjustable search radius is performed by a scout bee in line number 50 and the objective-value-based comparison method is used in line number 14, 27, 39 and 52. The full details of each change are described in section 3.1-3.3.</p><p>Note that the pseudo-code in Fig. <ref type="figure" target="#fig_0">1</ref> is used to find the minimum objective value. To find the maximum objective value, the condition which is used to compare the old solution and the new solution, in line number 14, 27, 39 and 52, must be changed from  where f(x(*)) is the objective function value.</p><formula xml:id="formula_7">f (x new ( * )) &lt; f (x old ( * ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The best-so-far method</head><p>In order to improve the efficiency of the onlooker bees, we proposed to modify the parameters that are used to calculate new candidate food sources. In the original algorithm, each onlooker bee selects a food source based on a probability that varies according to the fitness function explored by a single employed bee. Then the new candidate solutions are generated by updating the onlooker solutions as shown in Eq. (2.2). In our best-so-far method, from the pseudo-code line number 20 to 32, all onlooker bees use the information from all employed bees to make a decision on a new candidate food source. Thus, the onlookers can compare information from all candidate sources and are able to select the best-so-far position.</p><p>On the assumption that the best-so-far position will lead to the optimal solution, from the pseudo-code line number 33 to 46, we bias the solution direction by using the best-so-far solutions-based approach to update the new candidate solutions of onlooker bees. We then accelerate its convergence speed by using the fitness value of the best-so-far food source as the multiplier of the error correction for this solution update. The values in all dimensions of each food source are also updated in each iteration in order to increase the diversity of the feasible solutions in the search space.</p><p>The new method used to calculate a candidate food source is shown in Eq. (3.1)</p><formula xml:id="formula_8">v id = x ij + f b (x ij -x bj ) (3.1)</formula><p>where: v id = The new candidate food source for onlooker bee position i dimension d, d = 1,2,3,. . .D; x ij = The selected food source position i in a selected dimension j; = A random number between -1 and 1; f b = The fitness value of the best food source so far;</p><p>x bj = The best-so-far food source in selected dimension j.</p><p>This change should make the best-so-far algorithm converge more quickly because the solution will be biased towards best-sofar solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The adjustable search radius</head><p>Although our best-so-far method can increase the local search ability compared to the original ABC algorithm, the solution is easily entrapped in a local optimum. In order to resolve this issue, we introduce a global search ability for the scout bee.</p><p>From the pseudo-code line number 47 to 59, the scout bee will randomly generate a new food source by using Eq. (3.2) whenever the solution stagnates in the local optimum.</p><formula xml:id="formula_9">ij = x ij + ij ω max - iteration MCN (ω max -ω min ) x ij (3.2)</formula><p>where ij is a new feasible solution of a scout bee that is modified from the current position of an abandoned food source (x ij ) and ij is a random number between [-1,1]. The value of ω max and ω min represent the maximum and minimum percentage of the position adjustment for the scout bee. The value of ω max and ω min are fixed to 1 and 0.2, respectively. These parameters were chosen by the experimenter. With these selected values, the adjustment of scout bee's position based on its current position will linearly decrease from 100 percent to 20 percent in each experiment round.</p><p>Based on the assumption that the solution of scout bee will be far from the optimal solution in the first iteration and it will converge closely to the optimal solution in later iterations, Eq. (3.2) will  dynamically adjust the position of scout bee by allowing a scout bee in the first iteration to wander with a wider step size in the search space. As the number of the iteration increases, the step size for the wandering of a scout bee will decrease.</p><p>From the pseudo-code line number 7 to 19, the employed bees can thus still maintain diversity in producing a new food source (Eq. (2.2)). Using both the local and the global search methods, the convergence speed increases and solutions can be globally optimized more quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Objective-value-based comparison method</head><p>In this work, we also focus on the method that is used to compare and to select between the old solution and the new solution in each iteration. Basically, the comparison of the new solution and the old solution is done by the fitness value. If the fitness of the new solution is better than the fitness of the old solution, we select the new one and ignore the old solution. The fitness value can be obtained from the following equation.</p><p>For finding the minimum objective value</p><formula xml:id="formula_10">Fitness(f (x)) = ⎧ ⎨ ⎩ 1 1 + f (x) if f (x) ≥ 0 1 + |f (x)| if f (x) &lt; 0 (3.3)</formula><p>Based on Eq. ( <ref type="formula">3</ref>.3), we can see that when f(x) is larger than the zero but has a very small value, e.g. 1E-20, the fitness value of equation 1/(1 + 1E-20) is rounded up to be 1 (1E-20 is ignored). This will lead the fitness of all solutions to become equal to 1 in the later iterations. In other words, there is no difference between the fitness values that is equal to 1/(1 + 1E-20) and 1/(1 + 1E-120). Thus, a new solution that gives a better fitness value than the old solution will be ignored and the solution will stagnate at the old solution. In order to solve this issue, we directly use the objective value of function for comparison and selection of the better solution. The pseudo-code of the new comparison process is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Computational complexity</head><p>We can evaluate the computational complexity of the best-sofar ABC algorithm in comparison with the original ABC and the BSO algorithms. Let n be the total number of bees, d be the dimension of solutions, and m be the maximum iteration. The best-so-far ABC algorithm must spend time necessary to find the best-so-far solution in each iteration, so it uses the computational time equal to (n/2)d + m(3nd). This is larger than the computational time of the original ABC algorithm which is equal to (n/2)d + m(3/2nd + d). The BSO algorithm must spend time to sort all bees based on their fitness in each of iteration of algorithm, so the computational time is equal to nd + m(n + nd log n + 5/2nd)). However, when we focus on the upper bound of the complexity of problems, both Best-so-far ABC and original ABC algorithms use computational time O(mnd) while the BSO algorithm uses O(mnd log n), which is the highest complexity.</p><p>Although the computational complexity of the best-so-far ABC is higher than the original ABC at the same maximum iteration, the best-so-far ABC should find the optimal solution before it reaches the maximum iteration. Thus the actual computation time of bestso-far ABC may be reduced in relative to original ABC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Using best-so-far ABC in image registration</head><p>We applied our best-so-far method to applications in the image registration domain. The following subsections provide a brief background on image registration and the adaptation of our bestso-far method to this problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Background on image registration</head><p>Image registration is a process used to align two or more images of a scene. It plays an important role in many research fields such as the computer vision, remote sensing and medical imaging <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Usually a target image must be transformed to be geometrically coincident with a reference image. To do this, corresponding features must be identified in the two images or else the global intensity distributions must be compared. A similarity or fitness measure <ref type="bibr" target="#b18">[19]</ref> is also necessary. The registration process searches for  a transformation from the target to the reference feature geometry that maximizes this similarity measure.</p><p>The global optimization of the similarity measurement is the key to automate the registration process. There are two common approaches to measure the similarity, which are geometric distance and statistical similarity. Hessian and Hausdorff geometric distance have been used to measure the distance in computer vision researches <ref type="bibr" target="#b19">[20]</ref>. These methods measure the nearest neighbor of rank kth. The weakness of the methods is the fact that the presence of outliers may affect the accuracy. Mismatch <ref type="bibr" target="#b20">[21]</ref> is then introduced. It is a new measure that makes use of a Gaussian distribution as a weight function in order to be tolerant of outliers.</p><p>Least Square Error is the method that statistically determines the smallest deviation or distance between two data sets, such as the reference and target images. The drawback of this measure is its sensitivity to the presence of outliers. Cross Correlation measures similarity of the two images by convolving them. Unfortunately, this measure is also not robust in the presence of noise or outliers. Currently, most of the researchers in this field work with Mutual Information (MI) as a measure of similarity. It is quite similar to correlation except that the concept of MI stems from conditional probability. It is a measure of the presence of the information from one set in another set. It is now becoming a standard to measure the similarity in image registration.</p><p>In this paper, we use a statistical similarity approach where the Mutual Information <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> is used for measuring the similarity of the two images. An optimization solution in the mutual information approach can be expressed mathematically as follows:  where ˛is the set of transformation parameters, MI is an objective function, I R is a reference image and I T is a target image. The mutual information of I R and I T can be expressed in term of Shannon's entropy as follows:</p><formula xml:id="formula_11">˛ * = argmax ˛(MI(I R , I T )) (<label>4</label></formula><formula xml:id="formula_12">MI(I R , I T ) -H(I R ) + H(I T ) -H(I R , I T ) (4.2)</formula><p>where H(I R ) and H(I T ) are the Shannon's entropies, and H(I R , I T ) is the joint entropy of the two images. Shannon's entropy can be defined as: Various types of transformations can be applied in image registration. In our work, we used rigid transformations involving rotation and translation along the x-axis and y-axis. The transformation of images can be written as:</p><formula xml:id="formula_13">H(I R ) = - a p R (a) log(p R (a)) (4.</formula><formula xml:id="formula_14">x = x c + (x -x c ) × cos Â + (y -y c ) × sin Â + x (4.9) y = y c + (x -x c ) × sin Â + (y -y c ) × cos Â + y (4.10)</formula><p>where x and y are new coordinates, x and y are the current coordinates, x c and y c are the center coordinates of the target image, x and y are the displacements of a target image, and Â is the rotation angle of the target image around its center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimization solution with best-so-far ABC</head><p>We applied best-so-far ABC method to the registration problem described above. The goal is to find a global optimization of the similarity measure. In other words, we try to find the transformation variables that maximize the mutual information values of the images. The adopted algorithm is illustrated in Fig. <ref type="figure" target="#fig_9">3</ref>.</p><p>In Fig. <ref type="figure" target="#fig_9">3</ref>, initial solutions consisting of a rotation, and x and y-axis translation values are generated. These parameter sets are treated as the food sources for the employed bees. Each solution is used to transform the target image by re-sampling. Then we calculate the mutual information score between the transformed target and the reference image. The onlooker bees will then select the solutions that produce higher mutual information and update those solutions based on our best-so-far method. The process will be repeated until the mutual information reaches a threshold value or the number of iteration equals the MCN. Solutions that cannot improve the mutual information within a certain period will be abandoned and new solutions will be regenerated by the scout bee.</p><p>Eq. ( <ref type="formula" target="#formula_0">2</ref>.2) is used by employed bees in order to update variables and improve the solution quality in each iteration. However, due to the independence among the transformation parameters in image registration application, Eq. (3.1), which uses a randomly chosen dimension, is slightly adjusted. A fixed dimension is used instead as shown in Eq. <ref type="bibr">(4.11)</ref>.</p><formula xml:id="formula_15">v id = x id + f b (x id -x bd ) (4.11)</formula><p>In our algorithm, Eq. (4.11) is introduced to improve the solutions in each iteration. When the solutions cannot be further improved, Eq. (2.2) is used to calculate the new candidate solutions. Moreover, Eq. (3.2) is used by scout bees to randomly generate the new solutions when the mutual information cannot be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We validated the performance of the proposed technique compared with the original technique in two sets of experiments: numerical and image registration applications. The following subsections describe the experimental methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Numerical applications</head><p>The aim of this experiment is to compare the performance of the search process for the best-so-far method with the original algorithm and the Bee Swarm Optimization that is the state-of-the-art algorithm for solving numerical optimization based on behavior of bees proposed by R. Akbari <ref type="bibr" target="#b15">[16]</ref>. Unimodal and multimodal benchmark functions as shown in Table <ref type="table">1</ref> were used in this experiment. The objective of the search process is to find the solutions that can produce the minimum output value from these benchmark functions. In other words, the aim is to minimize f i ( x) in Table <ref type="table">1</ref>. In order to make a performance comparison, we implemented the original ABC based on an algorithm given in <ref type="bibr" target="#b24">[25]</ref> as well as our best-so-far method. Our original ABC code was also validated by comparing the benchmark results with the previous work proposed in <ref type="bibr" target="#b9">[10]</ref>. Since we could not obtain the source code of the BSO algorithm, we only use the results reported in <ref type="bibr" target="#b15">[16]</ref> to compare with our algorithm. As a result, the BSO algorithm will not be included in some comparison experiments.</p><p>The experiments were conducted in a similar fashion as described in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. The number of employed and onlooker bees were set to 100. The values of ω max and ω min are set to 1 and 0.2, respectively. Each of the experiments was repeated 100 times with different random seeds. All the experiments in this paper were run on the same hardware (Intel Core 2 Quad with 2.4 GHz CPU and 4 GB memory). The number of iterations to convergence, the runtime (execution time), and the mean and standard deviation of the output values of benchmark functions were recorded. Note that lower mean objective values indicate better solutions.</p><p>In order to investigate the effect of the iteration increment on the output quality, we divided the experiments into two sets, called ABC1 and ABC2. The maximum numbers of iterations proposed in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref> were also used in our work. In the ABC1 experiment, for all benchmark functions with exceptions of Schaffer function, the numbers of maximum iterations were 500, 750 and 1000 for the dimensions of 10, 20 and 30, respectively. In ABC2, the maximum numbers of iterations were 5000, 7500 and 10,000 for the dimensions of 10, 20 and 30, respectively. For Schaffer function in both of ABC1 and ABC2 experiment, the numbers of maximum iterations was set to 2000. Note that one iteration of our method requires a longer execution time than the original method because the values in all dimensions of each candidate food source of onlooker bees are updated on every iteration and the algorithm spends the time to find the best-so-far solution in each iteration. However, the number of iterations to convergence is smaller. Thus, we will look at both the number of iterations and runtime when we consider convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image registration application</head><p>In our image registration experiments, once again we compare both the solution quality and execution performance between our best-so-far and original ABC implementation. The objective function of the image registration application is Mutual Information or MI. MI is used to measure the similarity of the two input images after registration. Thus, the higher the MI value, the more accurate the registration process.</p><p>In our experiments we used four sample image pairs. For each pair the experiments were repeated 30 times. The number of employed and onlooker bees used were 50 and the maximum number of iterations was 200. The search space of each variable for rigid transformation process in term of rotation degree (Â), translation in x-axis (x) and y-axis (y) were defined as shown below. The ranges of these parameters were chosen based on a visual estimate of the maximum amount of change required.</p><formula xml:id="formula_16">Image Pair (Â) ( x)<label>( y)</label></formula><formula xml:id="formula_17">I -25 • ≤ Â ≤ 25 • -20 ≤ x ≤ 20 -20 ≤ y ≤ 20 II 0 • ≤ Â ≤ 50 • -210 ≤ x ≤ 210 -160 ≤ y ≤ 160 III -1 • ≤ Â ≤ 1 • -10 ≤ x ≤ 10 -50 ≤ y ≤ 0 IV -90 • ≤ Â ≤ 90 • -256 ≤ x ≤ 256 -256 ≤ y ≤ 256</formula><p>We tested the algorithms with two cases. In the first case, we used a controlled initial solution with the fixed initial values of Â, x, and y. We offset all the initial values from the lower bound displayed in the table above. However, the initial values may accidentally be fixed close to the optimum solution. Thus, in the second test case, we used randomly generated initial values.</p><p>In both test cases we investigated the solution quality of the two algorithms when the runtime is approximately the same as well as studying the algorithm performance when the solution quality is approximately the same. In other words, we would like to know which algorithm uses less computation time to create a solution of the equivalent quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Numerical results and discussion</head><p>The results obtained with the original and the best-so-far method based on the numerical benchmark functions are shown in Table <ref type="table" target="#tab_1">2</ref>. Note that there is no result of the BSO algorithm to compare in the ABC1 experiment.</p><p>The "Convergence Iteration" column shows the number of iterations needed for each algorithm to converge towards the optimal solution (value not to exceed the maximum number of iterations indicated in section 6.1). The "Runtime" column is the average runtime needed to reach the convergence state across a hundred runs. The "Mean" column is the average output values of benchmark functions. The "SD" column shows the standard deviation of the results. Mean and SD values which are smaller than 1E-304 are considered as 0.</p><p>In Table <ref type="table" target="#tab_1">2</ref>, we compare the results using approximately the same runtime value. The best-so-far method gives better solutions than the original algorithm in all cases of benchmark functions. Especially, the global minimal values of Ackley and Schaffer functions were found by the best-so-far method in ABC1 experiment.</p><p>The results from the ABC1 experiment showed that several benchmark functions did not converge within the specified number of iterations. The maximum number of iterations was then increased in the ABC2 experiment. The results are shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>In the ABC2 experiment in Table <ref type="table" target="#tab_2">3</ref>, when the number of iterations was increased, the solution quality was also improved in the original ABC as shown in mean of objective values column. However, our best-so-far method was still able to generate a better solution on all the benchmark functions. The average improvement on runtime for the best-so-far method when compared with the original ABC was 82%, and the maximum and minimum runtime improvement were 99% and 30% for Ackley and Schaffer function, respectively. For several benchmark functions, the original ABC never reached the mean value achieved by our best-so-far method even when the runtime was increased.</p><p>The results from the BSO method reported in <ref type="bibr" target="#b15">[16]</ref> were also used to compare with the Best-so-far method in the ABC2 experiment. The results showed that the mean value of Best-so-far method is better than the BSO method at the same number of the iterations on all benchmark functions. Fig. <ref type="figure" target="#fig_4">4</ref> shows the comparison of the convergence speed (Iterations) in term of number of iterations between the original and the best-so-far method. Notice that the best-so-far method converges substantially faster with a much smaller number of iterations needed.</p><p>Note that there is no result of the BSO algorithm to illustrate the convergence speed, so the comparison with the BSO algorithm is not included in here.</p><p>The rate of convergence is used to indicate the speed at which a converging sequence approaches its limit. A smaller value of the rate of convergence implies that the solution converges to an optimal value more quickly. Suppose that the sequence {x k } converges to the number L, so the rate of convergence can be calculated from equation as follows  The results in Table <ref type="table" target="#tab_3">4</ref> show that both the ABC and the Best-sofar ABC converge linearly to the limit (global minimum). However, when we compare the rates of convergence, we found that the Best-so-far ABC gives better results than the ABC on all benchmark functions, especially in the Griewank and Rastrigin functions.</p><formula xml:id="formula_18">rate of convergence = lim k→∞ |x k+1 -L| |x k -L| (6.1)</formula><p>In summary, the results in the numerical experiments indicate that the best-so-far ABC method can converge to the optimal solution more quickly on almost all benchmark functions when it is compared with the original ABC method. The results also show that the best-so-far ABC method can produce better solutions than the BSO algorithm that is the state-of-the-art algorithm. In other words, fewer iterations were needed to converge and thus, less computational time was required. Notice that the SD is relatively low, which implies consistency among experimental runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Image registration results and discussion</head><p>In this experiment, we used both the original and the best-so-far methods to register four sets of images. The two methods produced results that appeared indistinguishable to human eyes. However, analysis of MI for registered images suggests that the best-so-far offered advantages. The registered images are shown in Figs. <ref type="figure" target="#fig_5">5</ref><ref type="figure" target="#fig_6">6</ref><ref type="figure">7</ref><ref type="figure" target="#fig_8">8</ref>. However, if we investigate the resulting numbers closely, the best-so-far produces better results faster as discussed next.</p><p>First, we discuss the results of the experiments for the fixed initial solution. Table <ref type="table" target="#tab_4">5</ref> and Fig. <ref type="figure" target="#fig_10">9</ref> show the results in a case where the runtime of both methods is approximately the same. In Table <ref type="table" target="#tab_5">6</ref>, the means of MI values are fixed to be approximately the same.</p><p>In Table <ref type="table" target="#tab_4">5</ref>, we compare the results using approximately the same runtime value. The best-so-far method gives slightly better solutions than the original algorithm in all cases of sample image pairs, with somewhat faster convergence. Table <ref type="table" target="#tab_5">6</ref> shows the convergence speed at approximately the same mean value of MI. The results indicate that the best-so-far method solutions converged to an optimal solution more quickly than the original method in all image pairs. The maximum runtime improvement of 84% was found in the experiment with image pair II and the minimum of 39% was with image pair I. The average runtime improvement for all image pairs was 70%.</p><p>For the random initial solution experiments, in some cases, if the initial solution is close to the optimal solution, the results from the first few iterations of the original algorithm generate better results. However, when the number of iterations increases, the best-sofar method will adjust the solutions and converge to the optimal solutions more quickly. The results of these experiments are shown in Tables <ref type="table" target="#tab_6">7</ref> and<ref type="table" target="#tab_7">8</ref> and Fig. <ref type="figure" target="#fig_11">10</ref>.</p><p>In the random initial solutions experiment, although the MI values from the original method are equivalent to the MI values from the best-so-far method at approximately the same runtime value as shown in Table <ref type="table" target="#tab_6">7</ref>, the results in Table <ref type="table" target="#tab_7">8</ref> show that the solutions of the same case still converge to an optimal solution faster than the original algorithm when the approximate mean value is the same. The maximum and minimum percentage of the runtime improvement, found on the experiment of image pair III and image pair I, were equal to 81% and 58%, respectively. The average improvement on the runtime for the best-so-far method compared with the original model on all experiments of image pairs in this case was 68%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, a best-so-far method for solution updates in the Artificial Bee Colony algorithm was proposed. We have replaced the neighboring solutions-based approach with the best-so-far technique in order to increase the local search ability of the onlooker bees. The searching method based on a dynamic adjustment of search range depending on the iteration was introduced for scout bees. The comparison and the selection of the new solution were changed from a fitness-based comparison to an objective-valuebased comparison that can help to resolve round up issues in the computation of the floating point "goodness" value.</p><p>In our best-so-far method, onlooker bees compare the information from all employed bees to select the best-so-far candidate food source. This will bias the solution handled by onlooker bees towards the optimal solution. Moreover, if the solution appears to stagnate in a local optimum, the scout bee can randomly generate a new position in order to maintain the diversity of new food sources.</p><p>The performance of the best-so-far ABC method was then compared with the original ABC algorithm and the BSO algorithm using a set of benchmark functions. The computational complexity and the rate of convergence on both best-so-far ABC method and the original ABC method were addressed. The results from the experiments provide evidence that the best-so-far ABC outperforms all the mentioned algorithms in both quality and convergence rate. We further applied the best-so-far method to optimize the mutual information in an image registration application. Several image pairs were used in the experiments. The results showed that our algorithm can arrive at the convergence state more quickly. Lastly, the mutual information value produced by the best-sofar method is higher implying that the registration quality is also enhanced. Thus, we can conclude that our best-so-far ABC is efficient from both the perspective of solution quality and algorithm performance. The algorithm can serve as an alternative in several application domains in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The pseudo-code of the Best-so-far ABC algorithm.</figDesc><graphic coords="2,63.26,55.97,480.24,575.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The comparison between the old and the new method for comparing the solutions.</figDesc><graphic coords="3,50.95,55.52,482.40,128.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>new ( * )) &gt; f (x old ( * ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Iterations to convergence for original and best-so-far algorithms when dimension = 30 (dimension of Schaffer Function = 2).</figDesc><graphic coords="7,52.45,55.65,480.24,597.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Image Pair I: Brain image for registration.</figDesc><graphic coords="8,123.26,55.78,360.00,107.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Image Pair II: Dentistry image for registration.</figDesc><graphic coords="8,123.26,197.13,360.00,138.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>. 1 )Fig. 7 .</head><label>17</label><figDesc>Fig. 7. Image Pair III: Satellite image for registration.</figDesc><graphic coords="8,123.26,542.60,359.64,184.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Image Pair IV: Camera Man image for registration.</figDesc><graphic coords="9,120.45,55.66,343.44,104.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 )</head><label>3</label><figDesc>H(I T ) =b p T (b) log(p T (b)) (4.4)H(I R , I T ) =a b p R,T (a, b) log(p R,T (a, b))(4.5)where p R (a) and p T (b) are the marginal probability mass functions of the reference and the target image respectively, and p R,T (a,b) is the joint probability mass function of two images.The joint histogram of the image pair can be used to calculate these marginal probability mass functions:p R,T (a, b) = hT (a, b)(4.8)where h(a,b) is a number of corresponding pairs having intensity value a in the reference image and intensity value b in the target image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The mean of the mutual information across iterations in Image Pair I-IV based on fixed initial solutions.</figDesc><graphic coords="10,62.76,56.16,480.96,371.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The mean of the mutual information in Image Pair I-IV based on random initial solutions.</figDesc><graphic coords="12,62.26,55.87,481.32,372.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Calculate the probability values of each solution from employed bees Update the new solutions of the onlooker bees depending on the selected solutions from employed bees by the modified approach Start Automated Image Registration Process based on Best-so-far ABC method Initialize the solutions such as rotation value, x-axis translation value, y-axis translation value Criterion is satisfied ? Update new solutions for the employed bees Image Registration and calculation of Mutual Information Process End of Automated Image Registration Process based on Best-so-far ABC approach No Yes Abandoned solution for the scout ? replace it with a new randomly produced solution No Yes Memorize the best solution achieved so far Show Result Image Registration and calculation of Mutual Information Process Fig. 3. Automated</head><label></label><figDesc>image registration based on the best-so-far ABC method.</figDesc><table><row><cell>Table 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Numerical benchmark functions.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Function name</cell><cell>Function</cell><cell>Characteristic</cell><cell>Ranges</cell><cell>Global Minimum</cell></row><row><cell>Sphere</cell><cell>f1( x)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Result obtained in the ABC1 experiment.</figDesc><table><row><cell>ABC</cell></row><row><cell>Max iteration</cell></row><row><cell>Dimensio</cell></row><row><cell>Function</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Result obtained in the ABC2 experiment.</figDesc><table><row><cell>Max iteration ABC</cell></row><row><cell>Dimensio</cell></row><row><cell>Function</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Rate of convergence in the ABC2 experiment.</figDesc><table><row><cell>Function</cell><cell>Dimensions</cell><cell cols="2">Rate of convergence</cell></row><row><cell></cell><cell></cell><cell>ABC</cell><cell>Best-so-far ABC</cell></row><row><cell>f1 Sphere</cell><cell>10</cell><cell>0.9265</cell><cell>0.5856</cell></row><row><cell></cell><cell>20</cell><cell>0.9942</cell><cell>0.6691</cell></row><row><cell></cell><cell>30</cell><cell>0.9954</cell><cell>0.7124</cell></row><row><cell>f2 Griewank</cell><cell>10</cell><cell>0.9966</cell><cell>0.5358</cell></row><row><cell></cell><cell>20</cell><cell>0.9890</cell><cell>0.6032</cell></row><row><cell></cell><cell>30</cell><cell>0.9908</cell><cell>0.5829</cell></row><row><cell>f3 Rastrigin</cell><cell>10</cell><cell>0.9270</cell><cell>0.5622</cell></row><row><cell></cell><cell>20</cell><cell>0.9662</cell><cell>0.6073</cell></row><row><cell></cell><cell>30</cell><cell>0.9787</cell><cell>0.6246</cell></row><row><cell>f4 Rosenbrock</cell><cell>10</cell><cell>0.9965</cell><cell>0.8705</cell></row><row><cell></cell><cell>20</cell><cell>0.9972</cell><cell>0.9492</cell></row><row><cell></cell><cell>30</cell><cell>0.9978</cell><cell>0.9722</cell></row><row><cell>F5 Ackley</cell><cell>10</cell><cell>0.9928</cell><cell>0.5683</cell></row><row><cell></cell><cell>20</cell><cell>0.9948</cell><cell>0.6791</cell></row><row><cell></cell><cell>30</cell><cell>0.9964</cell><cell>0.7445</cell></row><row><cell>f6 Schaffer</cell><cell>2</cell><cell>0.9907</cell><cell>0.6849</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>The results obtained from the original ABC and best-so-far ABC method based on fixed initial solutions at approximately the same runtime value.</figDesc><table><row><cell>Image pair</cell><cell>Image size (H × W)</cell><cell>ABC</cell><cell></cell><cell></cell><cell></cell><cell>Best-so-far ABC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Convergence Iteration</cell><cell>Runtime (s)</cell><cell>Mean (MI)</cell><cell>SD (MI)</cell><cell>Convergence Iteration</cell><cell>Runtime (s)</cell><cell>Mean (MI)</cell><cell>SD (MI)</cell></row><row><cell>I</cell><cell>253 × 255</cell><cell>186</cell><cell>687.699</cell><cell>1.25875</cell><cell>9.55E-05</cell><cell>179</cell><cell>684.106</cell><cell>1.25883</cell><cell>8.46E-05</cell></row><row><cell>II</cell><cell>210 × 160</cell><cell>199</cell><cell>413.281</cell><cell>0.552284</cell><cell>3.31E-02</cell><cell>200</cell><cell>411.116</cell><cell>0.581292</cell><cell>1.76E-02</cell></row><row><cell>III</cell><cell>299 × 176</cell><cell>200</cell><cell>636.353</cell><cell>0.957611</cell><cell>9.28E-04</cell><cell>196</cell><cell>635.344</cell><cell>0.959218</cell><cell>2.37E-04</cell></row><row><cell>IV</cell><cell>256 × 256</cell><cell>200</cell><cell>723.533</cell><cell>1.3449</cell><cell>3.00E-02</cell><cell>188</cell><cell>719.947</cell><cell>1.36046</cell><cell>7.57E-05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>The results obtained from the original ABC and best-so-far ABC method based on fixed initial solutions at approximately the same mean MI value.</figDesc><table><row><cell>Image pair</cell><cell>Image size (H × W)</cell><cell>ABC</cell><cell></cell><cell></cell><cell></cell><cell>Best-so-far ABC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Convergence Iteration</cell><cell>Runtime (s)</cell><cell>Mean (MI)</cell><cell>SD (MI)</cell><cell>Convergence Iteration</cell><cell>Runtime (s)</cell><cell>Mean (MI)</cell><cell>SD (MI)</cell></row><row><cell>I</cell><cell>253 × 255</cell><cell>186</cell><cell>687.699</cell><cell>1.25875</cell><cell cols="2">9.55E-05 108</cell><cell>412.756</cell><cell>1.25875</cell><cell>8.53E-05</cell></row><row><cell>II</cell><cell>210 × 160</cell><cell>199</cell><cell>413.281</cell><cell>0.552284</cell><cell>3.31E-02</cell><cell>32</cell><cell>65.778</cell><cell>0.553137</cell><cell>1.81E-02</cell></row><row><cell>III</cell><cell>299 × 176</cell><cell>200</cell><cell>636.353</cell><cell>0.957611</cell><cell>9.28E-04</cell><cell>44</cell><cell>142.628</cell><cell>0.957646</cell><cell>3.14E-04</cell></row><row><cell>IV</cell><cell>256 × 256</cell><cell>200</cell><cell>723.533</cell><cell>1.3449</cell><cell>3.00E-02</cell><cell>37</cell><cell>141.691</cell><cell>1.34501</cell><cell>7.66E-05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>The results obtained from the original ABC and best-so-far ABC method based on random initial solutions by using approximately the same runtime value.</figDesc><table><row><cell>Image pair</cell><cell>Image size (H × W)</cell><cell>ABC</cell><cell></cell><cell></cell><cell></cell><cell>Best-so-far ABC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Convergence Iteration</cell><cell>Runtime (s)</cell><cell>Mean MI</cell><cell>SD MI</cell><cell>Convergence Iteration</cell><cell>Runtime (s)</cell><cell>Mean MI</cell><cell>SD MI</cell></row><row><cell>I</cell><cell>253 × 255</cell><cell>189</cell><cell>714.005</cell><cell>1.25867</cell><cell>9.53E-05</cell><cell>196</cell><cell>712.789</cell><cell>1.25878</cell><cell>1.13E-04</cell></row><row><cell>II</cell><cell>210 × 160</cell><cell>199</cell><cell>400.781</cell><cell>0.580997</cell><cell>2.89E-03</cell><cell>187</cell><cell>399.604</cell><cell>0.58439</cell><cell>3.26E-04</cell></row><row><cell>III</cell><cell>299 × 176</cell><cell>199</cell><cell>641.630</cell><cell>0.958047</cell><cell>8.09E-04</cell><cell>200</cell><cell>629.072</cell><cell>0.959175</cell><cell>6.32E-04</cell></row><row><cell>IV</cell><cell>256 × 256</cell><cell>199</cell><cell>759.200</cell><cell>1.35800</cell><cell>5.38E-03</cell><cell>200</cell><cell>758.391</cell><cell>1.36049</cell><cell>4.54E-05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>The results obtained from the original ABC and best-so-far ABC method based on random initial solutions by using approximately the same mean MI value.</figDesc><table><row><cell>Image pair</cell><cell>Image size (H × W)</cell><cell>ABC</cell><cell></cell><cell></cell><cell></cell><cell>Best-so-far ABC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Convergence Iteration</cell><cell>Runtime (s)</cell><cell>Mean MI</cell><cell>SD MI</cell><cell>Convergence Iteration</cell><cell>Runtime (s)</cell><cell>Mean MI</cell><cell>SD MI</cell></row><row><cell>I</cell><cell>253 × 255</cell><cell>189</cell><cell>714.005</cell><cell>1.25867</cell><cell>9.53E-05</cell><cell>82</cell><cell>298.207</cell><cell>1.25867</cell><cell>1.17E-04</cell></row><row><cell>II</cell><cell>210 × 160</cell><cell>199</cell><cell>400.781</cell><cell>0.580997</cell><cell>2.89E-03</cell><cell>65</cell><cell>138.899</cell><cell>0.581029</cell><cell>3.44E-04</cell></row><row><cell>III</cell><cell>299 × 176</cell><cell>199</cell><cell>641.630</cell><cell>0.958047</cell><cell>8.09E-04</cell><cell>38</cell><cell>119.524</cell><cell>0.958108</cell><cell>6.56E-04</cell></row><row><cell>IV</cell><cell>256 × 256</cell><cell>199</cell><cell>759.200</cell><cell>1.35800</cell><cell>5.38E-03</cell><cell>59</cell><cell>223.725</cell><cell>1.35838</cell><cell>4.62E-05</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Thailand Research Fund through the Royal Golden Jubilee Ph.D. Program under Grant No. PHD/0038/2552.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text feature selection using ant colony optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Aghdam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ghasem-Aghaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Basiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications: An International Journal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="6843" to="6853" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ant colony optimization for multi-objective flow shop scheduling problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yagmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Yenisey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Industrial Engineering</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="411" to="420" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Particle swarm approach for structural design optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Behdinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Structures</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="1579" to="1588" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Journal of Visual Communication and Image Representation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="143" to="162" />
		</imprint>
	</monogr>
	<note>Particle swarm optimization for point pattern matching</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ant Colony Optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stützle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1995 IEEE International Conference on Neural Networks</title>
		<meeting>the 1995 IEEE International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An idea based on honey bee swarm for numerical optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<idno>-TR06</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Turkey</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Erciyes University, Engineering Faculty, Computer Engineering Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comparative study of Artificial Bee Colony algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Akay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="108" to="132" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the performance of Artificial Bee Colony (ABC) algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Basturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="687" to="697" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A powerful and efficient algorithm for numerical function optimization: artificial bee colony (ABC) algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Basturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="459" to="471" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An artificial bee colony algorithm for the leaf-constrained minimum spanning tree problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="625" to="631" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structural inverse analysis by hybrid simplex artificial bee colony algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Structures</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="861" to="870" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new design method based on artificial bee colony algorithm for digital IIR filters</title>
		<author>
			<persName><forename type="first">N</forename><surname>Karaboga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="page" from="328" to="348" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Engineering optimizations via nature-inspired virtual bee algorithms</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="317" to="323" />
			<date type="published" when="2005">2005</date>
			<publisher>GmbH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Development of novel optimization procedure based on honey bee foraging behavior</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundareswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Sreedevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1220" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel bee swarm optimization algorithm for numerical function optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ziarati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Nonlinear Science and Number Simulation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mutual information based image registration for remote sensing data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3701" to="3706" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">image registration using focused mutual information for application in dentistry</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bottenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Truyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Groen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2D</biblScope>
			<biblScope unit="page" from="545" to="553" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Goshtasby</surname></persName>
		</author>
		<title level="m">2-D and 3-D Image Registration for Medical, Remote Sensing, and Industrial Applications</title>
		<meeting><address><addrLine>New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient algorithms for robust point pattern matching and applications to image registration</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lemoigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="17" to="38" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ratanasanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Achalakul</surname></persName>
		</author>
		<title level="m">Enhancment in Robust Feature Matching, ECTI-CON</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="505" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodality image registration by maximization of mutual information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="187" to="198" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Alignment by maximization of mutual information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Artificial Bee Colony (ABC) Algorithm Homepage</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<ptr target="http://mf.erciyes.edu.tr/abc" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Online], Available</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
