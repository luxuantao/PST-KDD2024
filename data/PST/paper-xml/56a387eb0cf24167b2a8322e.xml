<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-01-18">18 January 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Francisco</forename><forename type="middle">Javier</forename><surname>Ordóñez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sensor Technology Research Centre</orgName>
								<orgName type="institution" key="instit1">Wearable Technologies</orgName>
								<orgName type="institution" key="instit2">University of Sussex</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Daniel</forename><surname>Roggen</surname></persName>
							<email>daniel.roggen@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Sensor Technology Research Centre</orgName>
								<orgName type="institution" key="instit1">Wearable Technologies</orgName>
								<orgName type="institution" key="instit2">University of Sussex</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sensor Technology Research Centre</orgName>
								<orgName type="institution" key="instit1">Wearable Technologies</orgName>
								<orgName type="institution" key="instit2">University of Sussex</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wendong</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sensor Technology Research Centre</orgName>
								<orgName type="institution" key="instit1">Wearable Technologies</orgName>
								<orgName type="institution" key="instit2">University of Sussex</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han-Chieh</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sensor Technology Research Centre</orgName>
								<orgName type="institution" key="instit1">Wearable Technologies</orgName>
								<orgName type="institution" key="instit2">University of Sussex</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pony</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sensor Technology Research Centre</orgName>
								<orgName type="institution" key="instit1">Wearable Technologies</orgName>
								<orgName type="institution" key="instit2">University of Sussex</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-01-18">18 January 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">AB5954B0DC1CB2ED6FA06A1D63DEF771</idno>
					<idno type="DOI">10.3390/s16010115</idno>
					<note type="submission">Received: 30 November 2015; Accepted: 12 January 2016;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>human activity recognition</term>
					<term>wearable sensors</term>
					<term>deep learning</term>
					<term>machine learning</term>
					<term>sensor fusion</term>
					<term>LSTM</term>
					<term>neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4% on average; outperforming some of the previous reported results by up to 9%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters' influence on performance to provide insights about their optimisation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing human activities (e.g., from simple hand gestures to complex activities, such as "cooking a meal") and the context in which they occur from sensor data is at the core of smart assistive technologies, such as in smart homes <ref type="bibr" target="#b0">[1]</ref>, in rehabilitation <ref type="bibr" target="#b1">[2]</ref>, in health support <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, in skill assessment <ref type="bibr" target="#b4">[5]</ref> or in industrial settings <ref type="bibr" target="#b5">[6]</ref>. Some simple activity-aware systems are now commercial in the form of fitness trackers or fall detection devices. However, many scenarios of high societal value are still elusive, such as providing "memory prosthesis" to people with dementia, inserting subtle cues in everyday life in the right context to support voluntary behaviour change (e.g., to fight obesity), or enabling natural human-robot interaction in everyday settings. These scenarios require a minute understanding of the activities of the person at home and out and about.</p><p>This work is motivated by two requirements of activity recognition: enhancing recognition accuracy and decreasing reliance on engineered features to address increasingly complex recognition problems. Human activity recognition is challenging due to the large variability in motor movements employed for a given action. For example, the OPPORTUNITYchallenge that was run in 2011 aiming at recognising activities in a home environment showed that contenders did not reach an accuracy higher than 88% on the recognition of only 17 sporadic gestures <ref type="bibr" target="#b6">[7]</ref>. Thus, addressing scenarios, such as activity diarisation, will require further improving recognition performance for an even wider set of activities.</p><p>Human activity recognition (HAR) is based on the assumption that specific body movements translate into characteristic sensor signal patterns, which can be sensed and classified using machine learning techniques. In this article, we are interested in wearable (on-body) sensing, as this allows activity and context recognition regardless of the location of the user.</p><p>Wearable activity recognition relies on combinations of sensors, such as accelerometers, gyroscopes or magnetic field sensors <ref type="bibr" target="#b7">[8]</ref>. Patterns corresponding to activities are then detected within the streaming sensor data using either feature extraction on sliding windows followed by classification, template matching approaches <ref type="bibr" target="#b8">[9]</ref> or hidden Markov modelling <ref type="bibr" target="#b9">[10]</ref>. Sliding window approaches are commonly used for static and periodic activities, while sporadic activities lend themselves to template matching approaches or hidden Markov modelling <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Most recognition systems select features from a pool of "engineered" features <ref type="bibr" target="#b11">[12]</ref>. Identifying relevant features is time consuming and also leads to a difficulty in "scaling up" activity recognition to complex high level behaviours (e.g., hour-long, day-long or more), as engineered features do not relate to "units of behaviour", but are rather the result of convenient mathematical operations. For instance, statistical and frequency features do not relate to semantically meaningful aspects of human motion, such as "hand grasp".</p><p>Deep learning refers broadly to neural networks that exploit many layers of non-linear information processing for feature extraction and classification, organised hierarchically, with each layer processing the outputs of the previous layer. Deep learning techniques have outperformed many conventional methods in computer vision <ref type="bibr" target="#b12">[13]</ref> and audio classification <ref type="bibr" target="#b13">[14]</ref>.</p><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b14">[15]</ref> are a type of DNN (deep neural network) with the ability to act as feature extractors, stacking several convolutional operators to create a hierarchy of progressively more abstract features. Such models are able to learn multiple layers of feature hierarchies automatically (also called "representation learning"). Long-short-term memory recurrent (LSTMs) neural networks are recurrent networks that include a memory to model temporal dependencies in time series problems. The combination of CNNs and LSTMs in a unified framework has already offered state-of-the-art results in the speech recognition domain, where modelling temporal information is required <ref type="bibr" target="#b15">[16]</ref>. This kind of architecture is able to capture time dependencies on features extracted by convolutional operations.</p><p>Deep learning techniques are promising to address the requirements of wearable activity recognition. First, performance may chiefly be improved over existing recognition techniques. Second, deep learning approaches may have the potential to uncover features that are tied to the dynamics of human motion production, from simple motion encoding in lower layers to more complex motion dynamics in upper layers. This may be useful to scaling up activity recognition to more complex activities.</p><p>The contributions of this paper are the following:</p><p>• We present DeepConvLSTM: a deep learning framework composed of convolutional and LSTM recurrent layers, that is capable of automatically learning feature representations and modelling the temporal dependencies between their activation. • We demonstrate that this framework is suitable for activity recognition from wearable sensor data by using it on two families of human activity recognition problems, that of static/periodic activities (modes of locomotion and postures) and that of sporadic activities (gestures). • We show that the framework can be applied seamlessly to different sensor modalities individually and that it can also fuse them to improve performance. We demonstrate this on accelerometers, gyroscopes and combinations thereof. • We show that the system works directly on the raw sensor data with minimal pre-processing, which makes it particularly general and minimises engineering bias.</p><p>• We compare the performance of our approach to that reported by contestants participating to a recognised activity recognition challenge (OPPORTUNITY) and to another open dataset (Skoda). • We show that the proposed architecture outperforms published results obtained on the OPPORTUNITY challenge, including a deep CNN, which had already offered state-of-the-art results in previous studies <ref type="bibr" target="#b16">[17]</ref>. • We discuss the results, including the characterisation of key parameters' influence on performance, and outline venues for future research towards taking additional advantages of the characteristics of the deep architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">State of the Art</head><p>Neural networks are powerful for pattern classification and are at the base of deep learning techniques. We introduce the fundamentals of shallow recurrent networks in Section 2.1, in particular those built on LSTM units, which are well suited to model temporal dynamics. In Section 2.2, we review the use of deep networks for feature learning, in particular convolutional networks. In Section 2.3, we review the use of neural networks in deep architectures and their applications to domains related to human activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">From Feedforward to Recurrent Networks</head><p>A feedforward neural network, or multi-layer perceptron (MLP), is a computational model that processes information through a series of interconnected computational nodes. These computational nodes are grouped into layers and are associated with one another using weighted connections. The nodes of the layers are called units (or neurons) and transform the data by means of non-linear operations to create a decision boundary for the input by projecting it into a space where it becomes linearly separable.</p><p>MLPs have been successfully applied to classification problems by training them in a supervised manner. Every neuron can be viewed as a computational unit that behaves as a logistic regression classifier (see Figure <ref type="figure" target="#fig_0">1a</ref>). Formally, units are defined in terms of the following function:</p><formula xml:id="formula_0">a pl`1q " σpW l a l `bl q (1)</formula><p>where a l denotes the level of response (or activation value) for the units in layer l (a l i is the activation for the unit i in layer l), W is a weight matrix, where W l ij represents the parameter (or weight) associated with the connection between unit j in layer l, and unit i in layer l `1, b l is the bias associated with units in layer l and σ is the activation function (or non-linearity). For l " 1, we use a p1q " x, denoting the input data of the network (the sensor signal in our problem). The output of an MLP architecture is defined by the activations of the units in the deepest layer. MLPs use a fully-connected (or dense) topology, in which each unit in layer pl `1q is connected with every unit in layer l.</p><p>A limitation of the MLP architecture is that it assumes that all inputs and outputs are independent of each other. In order for an MLP to model a time series (such as a sensor signal), it is necessary to include some temporal information in the input data. Recurrent neural networks (RNNs) are neural networks specifically designed to tackle this problem, making use of a recurrent connection in every unit. The activation of a neuron is fed back to itself with a weight and a unit time delay, which provides it with a memory (hidden value) of past activations, which allows it to learn the temporal dynamics of sequential data. A representation of a single recurrent unit is shown in Figure <ref type="figure" target="#fig_0">1b</ref>. Given a temporal input sequence a l " pa l 1 , . . . , a l T q of length T (being a l t,i the activation of the unit i in hidden layer l at time t), an RNN maps it to a sequence of hidden values h l " ph l 1 , . . . , h l T q and outputs a sequence of activations a pl`1q " pa pl`1q 1 , . . . , a pl`1q T q by iterating the following recursive equation:</p><formula xml:id="formula_1">h l t " σpW l xh a l t `hl t´1 W l hh `bl h q (2)</formula><p>where σ is the non-linear activation function, b l h is the hidden bias vector and W terms denote weight matrices, W l xh being the input-hidden weight matrix and W l hh the hidden-hidden weight matrix. The activation for these recurrent units is defined by:</p><formula xml:id="formula_2">a pl`1q t " h l t W l ha `bl a<label>(3)</label></formula><p>where W l ha denotes the hidden-activation weight matrix and the b l a terms denote the activation bias vector. Notice that the weight matrix W l defined for the MLP is equivalent to the W l xh matrix in Equation <ref type="bibr" target="#b1">(2)</ref>.</p><p>These types of networks have Turing capabilities <ref type="bibr" target="#b17">[18]</ref> and, thus, are in principle suited for learning sequences. However, their memory mechanism makes learning challenging when dealing with real-world sequence processing <ref type="bibr" target="#b18">[19]</ref>.</p><p>LSTMs extend RNN with memory cells, instead of recurrent units, to store and output information, easing the learning of temporal relationships on long time scales. LSTMs make use of the concept of gating: a mechanism based on component-wise multiplication of the input, which defines the behaviour of each individual memory cell. The LSTM updates its cell state, according to the activation of the gates. The input provided to an LSTM is fed into different gates that control which operation is performed on the cell memory: write (input gate), read (output gate) or reset (forget gate). The activation of the LSTM units is calculated as in the RNNs (see Equation ( <ref type="formula">2</ref>)). The computation of the hidden value h t of an LSTM cell is updated at every time step t. The vectorial representation (vectors denoting all units in a layer) of the update of an LSTM layer is as follows:</p><formula xml:id="formula_3">i t " σ i pW ai a t `Whi h t´1 `Wci c t´1 `bi q (4) f t " σ f pW a f a t `Wh f h t´1 `Wc f c t´1 `b f q (5) c t " f t c t´1 `it σ c pW ac a t `Whc h t´1 `bc q (6) o t " σ o pW ao a t `Who h t´1 `Wco c t `bo q (7) h t " o t σ h pc t q<label>(8)</label></formula><p>where Networks using LSTM cells have offered better performance than standard recurrent units in speech recognition, where they gave state-of-the-art results in phoneme recognition <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature Learning with Convolutional Networks</head><p>Neural networks, whether recurrent or feedforward, can receive as input raw sensor signals. However, applying them to features derived from the raw sensor signals often leads to higher performance <ref type="bibr" target="#b20">[21]</ref>. Discovering adequate features requires expert knowledge, which necessarily limits a systematic exploration of the feature space <ref type="bibr" target="#b11">[12]</ref>. Convolutional networks (CNNs) have been suggested to address this <ref type="bibr" target="#b16">[17]</ref>. A CNN with a single layer extracts features from the input signal through a convolution operation of the signal with a filter (or kernel). In a CNN, the activation of a unit represents the result of the convolution of the kernel with the input signal. By computing the activation of a unit on different regions of the same input (using a convolutional operation), it is possible to detect patterns captured by the kernels, regardless of where the pattern occurs. In CNNs, the kernels are optimised as part of the supervised training process, in an attempt to maximize the activation level of kernels for subsets of classes. A feature map is an array of units (or layer) whose units share the same parameterization (weight vector and bias). Their activation yields the result of the convolution of the kernel across the entire input data.</p><p>The application of the convolution operator depends on the input dimensionality. With a temporal sequence of 2D images (e.g., a video), often 2D kernels are used in a 2D spatial convolution <ref type="bibr" target="#b21">[22]</ref>. With a one-dimensional temporal sequence (e.g., a sensor signal), often a 1D kernel is used in a temporal convolution <ref type="bibr" target="#b22">[23]</ref>. In the 1D domain, a kernel can be viewed as a filter, capable of removing outliers, filtering the data or acting as a feature detector, defined to respond maximally to specific temporal sequences within the timespan of the kernel. Formally, extracting a feature map using a one-dimensional convolution operation is given by:</p><formula xml:id="formula_4">a pl`1q j pτq " σ ¨bl j `Fl ÿ f "1 K l j f pτq ˚al f pτq ‚" σ ¨bl j `Fl ÿ f "1 " P l ÿ p"1 K l j f ppqa l f pτ ´pq  ‚<label>(9)</label></formula><p>where a l j pτq denotes the feature map j in layer l, σ is a non-linear function, F l is the number of feature maps in layer l, K l j f is the kernel convolved over feature map f in layer l to create the feature map j in layer pl `1q, P l is the length of kernels in layer l and b l is a bias vector. When processing sensor data, this computation is applied to each sensor channel at the input independently, as shown in Figure <ref type="figure" target="#fig_1">2</ref>; hence, the number of feature maps at the input level is F 1 " 1. In subsequent layers, the number of feature maps will be defined by the number of kernels within that layer (see Figure <ref type="figure" target="#fig_1">2</ref>). ). The deepest layer (layer pl `1q) is composed by a single feature map, resulting from temporal convolution in layer l of a two-dimensional kernel K l 1 . The time axis (which is convolved over) is horizontal.</p><p>A kernel whose weights would be able to capture a specific salient pattern of a gesture would act as a feature detector. A model with several convolutional layers, in a stacked configuration where the output of layer l ´1 is the input for the upper layer l, may be able to learn a hierarchical representation of the data, where deeper layers progressively represent the inputs in a more abstract way. Deep CNNs have had a major impact on fields, like content recommendation <ref type="bibr" target="#b23">[24]</ref>, speech recognition <ref type="bibr" target="#b24">[25]</ref> and in computer vision <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>, where they have become the de facto standard approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Application of Deep Networks for HAR</head><p>Deep networks are able to compute more complex transformations of the input than those networks defined by a small number or just a single hidden layer (shallow networks), offering a higher representational power.</p><p>DNNs have been applied to the wearable HAR domain, using network architectures where convolutional and non-recurrent layers are combined <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>. Raw signals obtained from wearable sensors were processed by convolutional layers to capture features, which were unified by dense layers to obtain a probability distribution over different human activities. Experiments on several benchmark datasets (the OPPORTUNITY, Skoda and Actitracker datasets) proved convolutional operators capable of capturing temporal signal structure within the kernel window. Results showed that these network architectures offer a model with more discriminative power, outperforming state-of-the-art approaches.</p><p>Further improvements in terms of time series classification have been obtained in the speech recognition domain, by combining convolutional and recurrent layers in a unified deep framework, which contains either standard recurrent units <ref type="bibr" target="#b28">[29]</ref> or LSTM cells <ref type="bibr" target="#b15">[16]</ref>. Results on different datasets, such as the TIMITphone recognition database, proved these architectures to offer a feature representation that is more easily separable and able to capture information even at different data resolutions.</p><p>The case of activity recognition in video is one of the closest problems to the HAR scenario addressed in this paper, since video data analysis can be seen as time series modelling. In the video domain, CNNs and LSTMs were shown to be suitable to combine temporal information in subsequent video frames to enable better video classification. Results of a comparative analysis on the Sports-1Mand UCF-101 datasets showed that LSTM cells were necessary to take full advantage of the motion information contained in the video and yielded the highest reported performance <ref type="bibr" target="#b30">[30]</ref>.</p><p>Several deep recurrent approaches for video gesture recognition were compared on the Montalbano dataset <ref type="bibr" target="#b21">[22]</ref>. Models that included recurrence and convolutions improved frame-wise gesture recognition significantly. Results proved how recurrent approaches are able to capture temporal information, which provides a more discriminative data representation. It allowed outperforming non-recurrent networks and segmenting more accurately the beginning and ending frames of gestures.</p><p>Other network topologies, such as the deep belief networks (DBN), have been also applied to the activity recognition domain. DBNs are a form of generative deep learning networks, whose hidden layers are trained in a greedy layer-wise fashion. They can be used to extract a deep hierarchical representation of the training data <ref type="bibr" target="#b31">[31]</ref>. Results on three wearable HAR datasets (OPPORTUNITY, Skoda and Darmstadt Daily Routines datasets) show how they offer a feature extraction framework with general applicability in HAR applications <ref type="bibr" target="#b32">[32]</ref>.</p><p>These related work illustrate the potential of using deep CNNs to learn features in time series and also show that LSTMs are suitable to learn temporal dynamics in sensor signals. While some work applied CNNs to activity recognition, the effective combination of convolutional and recurrent layers, which has already offered state-of-the-art results in other time series domains, such as speech recognition, has not yet been investigated in the HAR domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture</head><p>We introduce a new DNN framework for wearable activity recognition, which we refer to as DeepConvLSTM. This architecture combines convolutional and recurrent layers. The convolutional layers act as feature extractors and provide abstract representations of the input sensor data in feature maps. The recurrent layers model the temporal dynamics of the activation of the feature maps. In this framework, convolutional layers do not include a pooling operation (see Section 5.4 for a discussion of this choice). In order to characterise the benefits brought about by DeepConvLSTM, we compare it to a "baseline" non-recurrent deep CNN. Both approaches are defined according to the network structure depicted in Figure <ref type="figure" target="#fig_2">3</ref>. For comparison purposes, they share the same architecture, with four convolutional layers and three dense layers. The input is processed in a layer-wise format, where each layer provides the representation of the input that will be used as data for the next layer. The number of kernels in the convolutional layers and the processing units in the dense layers is the same for both cases. The main difference between DeepConvLSTM and the baseline CNN is the topology of the dense layers. In the case of DeepConvLSTM, the units of these layers are LSTM recurrent cells, and in the case of the baseline model, the units are non-recurrent and fully connected. Therefore, performance differences between the models are a product of the architectural differences and not due to better optimisation, preprocessing or ad hoc customisation.</p><p>The input to the network consists of a data sequence. The sequence is a short time series extracted from the sensor data using a sliding window approach (see Section 4.2 for details) composed of several sensor channels. The number of sensor channels is denoted as D. Within that sequence, all channels have the same number of samples S 1 . The length of feature maps S l varies in different convolutional layers. The convolution is only computed where the input and the kernel fully overlap. Thus, the length of a feature map is defined by: S pl`1q " S l ´Pl `1 <ref type="bibr" target="#b9">(10)</ref> where P l is the length of kernels in layer l. The length of the kernels is the same for every convolutional layer, being defined as P l " 5, @l " 2, . . . , 5. From the left, the signals coming from the wearable sensors are processed by four convolutional layers, which allow learning features from the data. Two dense layers then perform a non-linear transformation, which yields the classification outcome with a softmax logistic regression output layer on the right. Input at Layer 1 corresponds to sensor data of size D ˆS1 , where D denotes the number of sensor channels and S l the length of features maps in layer l. Layers 2-5 are convolutional layers. K l denotes the kernels in layer l (depicted as red squares). F l denotes the number of feature maps in layer l. In convolutional layers, a l i denotes the activation that defines the feature map i in layer l. Layers 6 and 7 are dense layers. In dense layers, a l t,i denotes the activation of the unit i in hidden layer l at time t. The time axis is vertical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DeepConvLSTM</head><p>DeepConvLSTM is a DNN, which comprises convolutional, recurrent and softmax layers. Firstly, sensor data are transformed through four convolutional operations, as defined in Equation <ref type="bibr" target="#b8">(9)</ref>. Convolutional layers process the input only along the axis representing time. The number of sensor channels is the same for every feature map in all layers. In Figure <ref type="figure" target="#fig_2">3</ref>, convolution operators are displayed as '˚', which is applied to a kernel whose size is delineated by the red rectangles. These convolutional layers employ rectified linear units (ReLUs) to compute the feature maps, whose non-linear function in Equation ( <ref type="formula" target="#formula_4">9</ref>) is defined as σpxq " maxp0, xq. Layers 6 and 7 are recurrent dense layers. The choice of the number of recurrent layers is made following the results presented in <ref type="bibr" target="#b33">[33]</ref>, where the authors showed that a depth of at least two recurrent layers is beneficial when processing sequential data. Recurrent dense layers adapt their internal state after each time step. Here, the inputs of Layer 6 at time t are the elements of all of the feature maps at Layer 5 at time t, with t " 1 . . . T and T " S 5 . The activation of the recurrent units is computed using the hyperbolic tangent function. The output of the model is obtained from a softmax layer (a dense layer with a softmax activation function), yielding a class probability distribution for every single time step t. Following the notation in <ref type="bibr" target="#b21">[22]</ref>, the shorthand description of this model is: Cp64q ´Cp64q ´Cp64q ´Cp64q ´Rp128q ´Rp128q ´Sm, where CpF l q denotes a convolutional layer l with F l feature maps, Rpn l q a recurrent LSTM layer with n l cells and Sm a softmax classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baseline Deep CNN</head><p>The baseline model is a deep CNN, which comprises convolutional, non-recurrent and softmax layers. This approach shares the convolutional layers of DeepConvLSTM. It receives the same input, a D ˆS1 sensor data sequence, and the features maps are extracted in the same way as in the DeepConvLSTM architecture. In this model, Layers 6 and 7 are non-recurrent dense layers, identical to those employed in MLPs. The activation of each unit in the first dense layer is computed using all of the feature maps from the last convolutional layer, with Equation (1). The units in the dense layers are ReLUs, with σpxq " maxp0, xq. The output of the model is obtained from a softmax layer (a dense layer with a softmax activation function), yielding a probability distribution over classes. For this model, the shorthand description is: Cp64q ´Cp64q ´Cp64q ´Cp64q ´Dp128q ´Dp128q ´Sm, where CpF l q denotes a convolutional layer l with F l feature maps, Dpn l q a dense layer with n l units and Sm a softmax classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Implementation and Training</head><p>The neural networks here described are implemented in Theano using Lasagne <ref type="bibr" target="#b34">[34]</ref>, a lightweight library to build and train neural networks. The model training and classification are run on a GPU with 1664 cores, 1050 MHz clock speed and 4 GB RAM.</p><p>Models are trained in a fully-supervised way, backpropagating the gradients from the softmax layer through to the convolutional layers. The network parameters are optimized by minimizing the cross-entropy loss function using mini-batch gradient descent with the RMSProp update rule <ref type="bibr" target="#b35">[35]</ref>. After experimenting with multiple per-parameter learning rate updates, we found that RMSProp consistently offered the best results with the widest tolerance to the learning rate setting. The number of parameters to optimize in a DNN varies according to the type of layers it comprises and has great impact in the time and computer power required to train the networks. The number and size of the parameters in the networks presented in Sections 3.1 and 3.2 are detailed in Table <ref type="table" target="#tab_1">1</ref>. </p><formula xml:id="formula_5">W hi , W h f , W hc , W ho : 128 ˆ128 b: 128 b i , b f , b c , b o : 128 W ci , W c f , W co : 128 c: 128 h: 128 7 W ai , W a f , W ac , W ao : 128 ˆ128 33,280 W: 128 ˆ128 16,512 W hi , W h f , W hc , W ho : 128 ˆ128 b: 128 b i , b f , b c , b o : 128 W ci , W c f , W co : 128 c: 128 h: 128 8 W: 128 ˆnc p128 ˆnc q `nc W: 128 ˆnc p128 ˆnc q `nc b: n c b: n c</formula><p>Total 996,800 `p128 ˆnc q `nc 7, 443, 136 `p128 ˆnc q `nc For the sake of efficiency, when training and testing, data are segmented on mini-batches of a size of 100 data segments. Using this configuration, an accumulated gradient for the parameters is computed after every mini-batch. Both models are trained with a learning rate of 10e ´3 and a decay factor of ρ " 0.9. Weights are randomly orthogonally initialized. We introduce a drop-out operator on the inputs of every dense layer, as a form of regularization. This operator sets the activation of randomly-selected units during training to zero with probability p " 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>We evaluate DeepConvLSTM on two human activity recognition datasets and compare the performance against the baseline CNN, which provides a performance reference for deep networks, and against results reported in the literature on these datasets using other machine learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Datasets</head><p>Human activities can be defined as periodic, such as walking and bicycling, static, such as being seated and standing still, or sporadic, such as goal-oriented gestures (e.g., drinking from a cup) <ref type="bibr" target="#b7">[8]</ref>. Benchmarking of activity recognition must be conducted on datasets comprising a variety of these types of activities. Furthermore, human activities (i.e., goal-oriented gestures, such as "fetching a cup") are often embedded in a large Null class (the Null class corresponds to the time spans that do not cover "interesting" activities, such as, e.g., when a user is not engaging in one of the activities that is relevant to the scenario at hand). The recognition of activities embedded in a Null class tends to be more challenging, as the recognition system must implicitly identify the start and end point of data comprising a gesture and then classify it. A number of datasets have been published for activity recognition, including the OPPORTUNITY <ref type="bibr" target="#b36">[36]</ref>, PAMAP <ref type="bibr" target="#b37">[37]</ref>, Skoda <ref type="bibr" target="#b38">[38]</ref> and mHealth <ref type="bibr" target="#b39">[39]</ref> datasets. In this paper, we selected two datasets for the evaluation of our approach based on the variety and variability of activities and their presence in the HAR literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">The OpportunityDataset</head><p>The OPPORTUNITY dataset <ref type="bibr" target="#b36">[36]</ref> comprises a set of complex naturalistic activities collected in a sensor-rich environment. Overall, it contains recordings of four subjects in a daily living scenario performing morning activities, with sensors of different modalities integrated in the environment, in objects and on the body. During the recordings, each subject performed a session five times with activities of daily living (ADL) and one drill session. During each ADL session, subjects perform the activities without any restriction, by following a loose description of the overall actions to perform (i.e., checking ingredients and utensils in the kitchen, preparing and drinking a coffee, preparing and eating a sandwich, cleaning up). During the drill sessions, subjects performed 20 repetitions of a predefined sorted set of 17 activities. The dataset contains about 6 hours of recordings in total.</p><p>The OPPORTUNITY dataset comprises both static/periodic and sporadic activities. It is available on the UCIMachine Learning repository and has been used by numerous third party publications (e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b40">40]</ref>). Most importantly, it has been used in an open activity recognition challenge where participants (listed in Table <ref type="table" target="#tab_3">3</ref>) competed to achieve the highest performance on the recognition of modes of locomotion, as well as sporadic gestures <ref type="bibr" target="#b6">[7]</ref>. This dataset is publicly available and can be downloaded from <ref type="bibr" target="#b41">[41]</ref>.</p><p>For this paper, we have used the same subset employed in the OPPORTUNITY challenge to train and test our models. We train the models on the data of all ADL and drill sessions for the first subject and on ADL1, ADL2 and drill sessions for Subjects 2 and 3. We report classification performance on a testing set composed of ADL4 and ADL5 for Subjects 2 and 3. ADL3 datasets for Subjects 2 and 3 were left for validation.</p><p>In terms of the sensor setting, we follow the OPPORTUNITY challenge guidelines, taking into account only the on-body sensors. This includes 5 commercial RS485-networked XSense inertial measurement units (IMU) included in a custom-made motion jacket, 2 commercial InertiaCube3 inertial sensors located on each foot (Figure <ref type="figure" target="#fig_3">4</ref>, left) and 12 Bluetooth acceleration sensors on the limbs (Figure <ref type="figure" target="#fig_3">4</ref>, right). Each IMU is composed of a 3D accelerometer, a 3D gyroscope and a 3D magnetic sensor, offering multimodal sensor information. Each sensor axis is treated as an individual channel yielding an input space with a dimension of 113 channels. The sample rate of these sensors is 30 Hz. In this study, sensor data were pre-processed to fill in missing values using linear interpolation and to do a per channel normalization to interval [0,1].</p><p>The OPPORTUNITY dataset includes several annotations of gestures and modes of locomotion/postures. In this paper, we have focused the models on two tasks defined in the OPPORTUNITY challenge:</p><p>• Task A: recognition modes of locomotion and postures. The goal of this task is to classify modes of locomotion from the full set of body-worn sensors. This is a 5-class segmentation and classification problem. • Task B: recognition of sporadic gestures. This task concerns recognition of the different right-arm gestures. This is an 18-class segmentation and classification problem.</p><p>The activities included in the dataset for each task are summarised in Table <ref type="table" target="#tab_2">2</ref>. The Skoda Mini Checkpoint dataset <ref type="bibr" target="#b38">[38]</ref> describes the activities of assembly-line workers in a car production environment. These gestures are similar to those performed at the quality assurance checkpoint of a production plant and are listed in Table <ref type="table" target="#tab_2">2</ref>.</p><p>In the study, one subject wore 20 3D accelerometers on both arms. We restrict our experiments to the 10 sensors placed on the right arm. The original sample rate of this dataset was 98 Hz, but it was decimated to 30 Hz for comparison purposes with the OPPORTUNITY dataset. The dataset contains 10 manipulative gestures. The recording is about 3 h long, comprising about 70 repetitions per gesture. This dataset is publicly available and can be downloaded from <ref type="bibr" target="#b42">[42]</ref>. The Skoda dataset has been employed to evaluate decision fusion techniques in sensor networks <ref type="bibr" target="#b38">[38]</ref> and deep learning techniques <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">43]</ref>, which makes it a suitable dataset to evaluate our proposed solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Measure</head><p>The OPPORTUNITY and Skoda datasets were recorded continuously. We use a sliding window of fixed length to segment the data. We refer to each window as a "sequence", which is the input of the network. Following the OPPORTUNITY challenge experimental setup, the length of the window is 500 ms, with a step size of 250 ms. The number of instances (segments) obtained after using this sliding window configuration is detailed per dataset in Table <ref type="table" target="#tab_2">2</ref>.</p><p>The class associated with each segment corresponds to the gesture that has been observed during that interval. Given a sliding window of length T, we choose the class of the sequence as the label at t " T, or in other words, the label of the last sample in the window, as depicted in Figure <ref type="figure" target="#fig_4">5</ref>. As stated in Section 3, DeepConvLSTM outputs a class probability distribution for every single time step t in the sequence (i.e., the 500-ms window of the sensor signal). However, we are interested in the class probability distribution once DeepConvLSTM has observed the entire 500-ms sequence. Several approaches exist for this <ref type="bibr" target="#b30">[30]</ref>: (1) using the prediction at the last time step T; (2) max-pooling the predictions over the sequence; (3) summing all of the sequence predictions over time and returning the most frequent. Since the memory of LSTM units tends to become progressively more informed as a function of the number of samples they have seen, DeepConvLSTM returns the class probability distribution only at the last time step T, when the full sequence has been observed. Thus, at the time of each sample of the original sensor signal, DeepConvLSTM provides a class probability distribution inferred from processing a 500-ms extract of the sensor signal prior to that time, as illustrated in Figure <ref type="figure" target="#fig_5">6</ref>. In terms of comparison, that value is also the most relevant, given that the sample at time T is the one defining the label of the sequence in the ground truth. Naturalistic human activity datasets are often highly unbalanced. Class imbalance occurs when some classes are represented by a large number of examples while others are represented by only a few <ref type="bibr" target="#b44">[44]</ref>. The gesture recognition task of the OPPORTUNITY dataset is extremely imbalanced, as the Null class represents more than 75% of the recorded data (76%, 82% and 76% for Subjects 1-3, respectively). The overall classification accuracy is not an appropriate measure of performance, since a trivial classifier that predicted every instance as the majority class could achieve very high accuracy. Therefore, we evaluate the models using the F-measure (F 1 ), a measure that considers the correct classification of each class equally important. The F 1 score combines two measures defined in terms of the total number of correctly-recognized samples and which are known in the information retrieval community as precision and recall. Precision is defined as TP TP`FP , and recall corresponds to TP TP`FN , where TP, FP are the number of true and false positives, respectively, and FN corresponds to the number of false negatives. Class imbalance is countered by weighting classes according to their sample proportion:</p><formula xml:id="formula_6">F 1 " ÿ i 2 ˚wi precision i ¨recall i precision i `recall i (<label>11</label></formula><formula xml:id="formula_7">)</formula><p>where i is the class index and w i " n i {N is the proportion of samples of class i, with n i being the number of samples of the i-th class and N being the total number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>In this section, we present the results and discuss the outcome. We show the performance of the approaches and also evaluate some of their key parameters to obtain some insights about the suitability of these approaches for the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance Comparison</head><p>The results of the proposed deep methods on the OPPORTUNITY dataset and the Skoda dataset are shown in Tables <ref type="table" target="#tab_4">4</ref> and<ref type="table">9</ref>, respectively. In the case of the OPPORTUNITY dataset, we report here the classification performance either including or ignoring the Null class. Including the Null class may lead to an overestimation of the performance given its large prevalence. By providing both results, we get better insights about the type of errors made by the models.</p><p>Table <ref type="table" target="#tab_3">3</ref> includes a comprehensive list of past published classification techniques employed on the datasets. The techniques competing in the OPPORTUNITY challenge were sliding window based and only differ in the classifier and features extracted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Description</head><p>CNN <ref type="bibr" target="#b16">[17]</ref> Results reported by Yang et. al. , in <ref type="bibr" target="#b16">[17]</ref>. The value is computed using the average performance for Subjects 1, 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skoda dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Description</head><p>CNN <ref type="bibr" target="#b22">[23]</ref> Results reported by Ming Zeng et. al. , in <ref type="bibr" target="#b22">[23]</ref>. Performance computed using one accelerometer on the right arm to identify all activities. CNN <ref type="bibr" target="#b43">[43]</ref> Results reported by Alsheikh et. al. , in <ref type="bibr" target="#b43">[43]</ref>. Performance computed using one accelerometer node (id #16) to identify all activities. From the results in Table <ref type="table" target="#tab_4">4</ref>, we can see that DeepConvLSTM consistently outperforms baselines on both tasks. When compared to the best submissions of the OPPORTUNITY challenge, it improves the performance by 6% on average. For some specific tasks, it can be noticed how DeepConvLSTM offers a striking performance improvement: there is more than a 9% improvement in the gesture recognition task without the Null class when compared to the OPPORTUNITY challenge models. DeepConvLSTM also improves by 6% over results previously reported by Yang et. al. <ref type="bibr" target="#b16">[17]</ref> using a CNN.</p><p>The baseline CNN also offers better results than the OPPORTUNITY submissions in the recognition of models of locomotion. However, in the case of gesture recognition, it obtains a similar recognition performance as the ensemble approach named CStar. These results of the baseline CNN are consistent with those obtained previously by <ref type="bibr">Yang et. al. in [17]</ref> using a CNN on raw signal data. Among deep architectures, DeepConvLSTM systematically performs better than the CNNs, improving the performance by 5% on average on the OPPORTUNITY dataset.</p><p>In Figure <ref type="figure" target="#fig_5">6</ref>, we illustrate the differences in the output predictions of the different architectures on the OPPORTUNITY dataset. One of the main challenges in this domain is the automatic segmentation of the activities. The baseline CNN approach tends to make more mistakes and has difficulties making crisp decisions about the boundaries of the gestures. It has troubles defining where the gesture starts or ends.</p><p>The confusion matrices on the OPPORTUNITY dataset for the gesture recognition task are illustrated in Tables <ref type="table" target="#tab_6">5</ref> and<ref type="table" target="#tab_8">7</ref> for the DeepConvLSTM approach and in Tables <ref type="table" target="#tab_7">6</ref> and<ref type="table" target="#tab_9">8</ref> for the baseline CNN. The confusion matrices contain information about actual and predicted gesture classifications done by the system, to identify the nature of the classification errors, as well as their quantities. Each cell in the confusion matrix represents the number of times that the gesture in the row is classified as the gesture in the column. Given the class imbalance in the data due to the presence of the dominant Null class, we report confusion matrices including and ignoring the Null class, in order to get better insights on the actual system performance.</p><p>When the Null class is included in the recognition task (see Tables <ref type="table" target="#tab_6">5</ref> and<ref type="table" target="#tab_7">6</ref>), most classification errors, both false positives and false negatives, are related to this class. This is the most realistic setup, where almost 75% of the data (see Table <ref type="table" target="#tab_2">2</ref>) processed is not considered as an activity of interest.</p><p>When the Null class is removed from the classification task (see Tables <ref type="table" target="#tab_8">7</ref> and<ref type="table" target="#tab_9">8</ref>), both approaches tend to misclassify gestures that are relatively similar, such as "Open Door 2"-"Close Door 2" or "Open Fridge"-"Close Fridge". This may be because these gestures involve the activation of the same type of sensors, but with a different sequentiality. In the case of gestures "Open Door 2"-"Close Door 2", one is misclassified as the other 44 times by the baseline CNN, while DeepConvLSTM made only 14 errors. Similarly, for gestures "Open Drawer 3"-"Close Drawer 3", the baseline CNN made 33 errors, while DeepConvLSTM misclassified only 14 sequences. The better performance of DeepConvLSTM for these similar gestures may be explained by the ability of LSTM cells to capture temporal dynamics within the data sequence processed. On the other hand, the baseline CNN is only capable of modelling time sequences up to the length of the kernels.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actual Gesture</head><p>Open  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actual Gesture</head><p>Open Door 1 73 From the results in Table <ref type="table">9</ref>, we can see that DeepConvLSTM outperforms other deep non-recurrent approaches on the Skoda dataset, improving the best reported result by 6%. The Skoda dataset has some specific characteristics: the gestures are long on average; it does not contain a Null class; and unlike the OPPORTUNITY dataset, it is quite well balanced. The greater length of the gestures does not diminish the performance of the model. These results corroborate our findings, supporting that the use of LSTM brings a significant advantage across very different scenarios.</p><formula xml:id="formula_8">0 23 0 0 0 0 0 0 0 0 0 0 0 1 0 1 Open Door 2 0 111 0 43 0 2 0 1 0 0 0 0 0 0 1 0 4 Close Door 1 22 0 63 2 0 0 0 0 0 0 0 0 0 0 0 0 1 Close Door 2 2 4 1 118 0 0 0 0 0 0 0 0 0 0 1 0 1 Open Fridge 1 1 0 0 304 59 17 1 4 0 0 0 1 0 1 0 2 Close Fridge 0 0 0 0 20 243<label>5</label></formula><p>Table <ref type="table">9</ref>. F 1 score performance on the Skoda dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CNN <ref type="bibr" target="#b22">[23]</ref> 0.861 CNN <ref type="bibr" target="#b43">[43]</ref> 0.893 Baseline CNN 0.884 DeepConvLSTM 0.958</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multimodal Fusion Analysis</head><p>Wearable activity recognition can make use of a variety of sensors. While accelerometers tend to be extremely small and low power, inertial measurement units are more complex (combining accelerometers, gyroscopes and magnetic sensors), but can provide accurate limb orientation. It is therefore important for an activity recognition framework to be applicable to a wide range of commonly-used sensor modalities to accommodate for the various size and power trade-offs.</p><p>We evaluate how the automated feature extraction provided by the kernels in convolutional layers is suitable to deal with signals of sensors of different modalities. In Table <ref type="table" target="#tab_10">10</ref>, we show the performance of DeepConvLSTM at recognizing gestures on the OPPORTUNITY dataset (without the Null class) for different selections of sensors. It can be noticed how,without any specific preprocessing, convolution operations can be interchangeably applied to individual sensor modalities. Starting from a 69% F 1 score using only the accelerometers on the dataset, the performance improves on average by 15% fusing accelerometers and gyroscopes and by 20% when fusing accelerometers, gyroscopes and magnetic sensors. As the number of sensor channels is increased, the performance of the model is consistently improved, regardless of the modalities of the sensors. These results demonstrate that the convolutional layers can extract features from sensor signals of different modalities without ad hoc preprocessing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Hyperparameters Evaluation</head><p>We characterise the influence of the key hyperparameters of the system. We evaluate the influence of two key architectural parameters: the sequence length processed by the network and the number of convolutional layers.</p><p>As previously stated, the input of the recurrent model is composed of a 500-ms data sequence. Therefore, the gradient signal is unable to notice time dependencies longer than the length of this sequence. Firstly we want to evaluate the influence of this parameter in the recognition performance of gestures with different durations, in particular if the gestures are significantly longer or shorter than the sequence duration.</p><p>The F 1 score on individual gestures of the dataset are shown in Figure <ref type="figure">7</ref>. This figure displays performance at recognizing individual gestures as a function of the ratio between the gesture length and the sequence length. Ratios under one represent performance for gestures whose durations are shorter than the sequence duration and, thus, that can be fully observed by the network before it provides an output prediction. Besides 500 ms, we carried out experiments with sequences of lengths of 400 ms, 1400 ms and 2750 ms. For most gestures, there are no significant performance changes when modifying the length of the sequence, although shorter gestures seem to benefit from being completely included in the sequence observed by the model. That is the case for several short gestures ("Open Drawer 1", "Close Drawer 1", "Open Drawer 2", "Close Drawer 2") when their ratio is under one. When the gesture duration is longer than the sequence duration, DeepConvLSTM can only come up with a classification result based on a partial view of the temporal unfolding of the features within the sequence. However, results show that DeepConvLSTM can nevertheless obtain good performance. For example, the gesture "drink from cup", which is 10-times longer than the sequence in one of the experiments, on average achieves a 0.9 F 1 score. We speculate that this is due to the fact that longer gestures (as "clean table" or "drink from cup" in this dataset) may be made of several shorter characteristic patterns, which allows DeepConvLSTM to spot and classify the gesture even without a complete view of it. The horizontal axis represents the ratio between the gesture length and the sequence length (ratios under one represent performance for gestures whose durations are shorter than the sequence duration).</p><p>We characterised the effect of the number of convolutional layers employed to automatically learn feature representations. Figure <ref type="figure" target="#fig_10">8</ref> shows that increasing the number of convolutional layers tends to increase the performance for the OPPORTUNITY dataset, improving by 1% when a new layer is added. Performance changes are not significant in the case of the Skoda dataset, showing a plateau. Results for the OPPORTUNITY dataset show that performance may be further improved if the number of convolution operations are increased. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Discussion</head><p>The main findings from the direct comparison of our novel DeepConvLSTM against the baseline model using standard feedforward units in the dense layer is that: (i) DeepConvLSTM reaches a higher F1 score; (ii) it offers better segmentation characteristics illustrated by clearer cut decision boundaries between activities; (iii) it is significantly better able to disambiguate closely-related activities, which tend to differ only by the ordering of the time series (e.g., "Open/Close Door 2" or "Open/Close Drawer 3"); and (iv) it is applicable even if the gestures are longer than the observation window. These findings support the hypothesis that the LSTM-based model takes advantage of learning the temporal feature activation dynamics, which the baseline model is not capable of modelling.</p><p>DeepConvLSTM is and eight-layer deep network. Other publications showed much deeper networks, such as "GoogLeNet", which is a 27-layer deep neural network applied to image classification <ref type="bibr" target="#b45">[45]</ref>. Indeed, findings illustrated in Figure <ref type="figure" target="#fig_10">8</ref> show that increasing further the number of layers may be beneficial, especially for the OPPORTUNITY dataset. Training time, however, increases with the number of layers, and depending on computational resources available, future work may consider how to find trade-offs between system performance and training time. This is especially important as a deep learning approach tends to be more suitable for training on a "cloud" infrastructure, possibly using data contributed by individual wearables (e.g., as in <ref type="bibr" target="#b46">[46]</ref>). Therefore, the choice of the number of layers is not solely a function of the desired performance, but also of the computational budget available.</p><p>In the literature, CNN frameworks often include convolutional and pooling layers successively, as a measure to reduce data complexity and introduce translation invariant features. Nevertheless, such an approach is not strictly part of the architecture, and in the time series domain, we can see some examples of CNNs where not every convolutional layer is followed by a pooling operation <ref type="bibr" target="#b15">[16]</ref>. DeepConvLSTM does not include pooling operations because the input of the network is constrained by the sliding window mechanism defined by the OPPORTUNITY challenge, and this fact limits the possibility of downsampling the data, given that DeepConvLSTM requires a data sequence to be processed by the recurrent layers. However, without the sliding window requirement, a pooling mechanism could be useful to cover different sensor data time scales at deeper layers. With the introduction of pooling layers, it would be possible to have different convolutional layers operating on sensor data sequences downsampled at different levels.</p><p>As general guidelines, we would recommend to focus the main effort on optimizing hyperparameters related to the network architecture, which have the major influence on performance. Indeed, parameters related to the learning and regularization processes seem to have less overall influence on the performance. For instance, we tested higher drop-out rates (p ą 0.5) with no difference in terms of performance. These results are consistent with those presented in <ref type="bibr" target="#b22">[23]</ref>. Nevertheless, there is a power-performance trade-off, and stacking more layers to augment the hierarchic representation of the features may not be relevant if one factors computational aspects.</p><p>We show how convolution operations are robust enough to be directly applied to raw sensor data, to learn features (salient patterns) that, within a deep framework, successfully outperformed previous results on the problem. A main benefit of using CNNs is that hand-crafted or heuristic features can be avoided, thus minimising engineering bias. This is particularly important, as activity recognition techniques are applied to domains that include more complex activities or open-ended scenarios, where classifiers must adaptively model a varying number of classes.</p><p>It is also noticeable, in terms of data, how the recurrent model is capable of obtaining a very good performance with relatively small datasets, since the largest training dataset used during the experiments (the one corresponding to the OPPORTUNITY dataset) is composed of ~80 k sensor samples, corresponding to 6 h of recordings. This seems to indicate that although deep learning techniques are often employed with large amounts of data, (e.g., millions of frames in computer vision <ref type="bibr" target="#b21">[22]</ref>), they may actually be applicable to problem domains where acquiring annotated data is very costly, such as in supervised activity recognition.</p><p>Although LSTM cells are composed of a much higher number of parameters per cell, the overall number of parameter values is significantly larger for the baseline CNN model than for DeepConvLSTM. For the specific case of the OPPORTUNITY dataset with a Null class and following the equation in Table <ref type="table" target="#tab_1">1</ref>, the parameters of DeepConvLSTM are composed of 999,122 values, while the baseline CNN parameters contain 7,445,458 values; this represents an increase of 600%. As illustrated in Table <ref type="table" target="#tab_1">1</ref>, this difference in size is due to the type of connection between the convolutional and dense layers (Layers 5 and 6). In the fully-connected architecture, the units in the dense layer (Layer 6) have to be connected with every value of the last feature map (Layer 5), needing a very large weight matrix to parametrize this connection. On the other hand, the recurrent model processes the feature map sample by sample, thus requiring a much reduced number of parameter values. Although DeepConvLSTM is a more complex approach, it is composed of much smaller parameters, and this has a direct beneficial effect in the memory and computational efforts required to use this approach.</p><p>However, in terms of training and classification time, there is not such a significant difference between the two models, despite the more complex computational units included in the dense layers of DeepConvLSTM. Training DeepConvLSTM on the OPPORTUNITY dataset requires 340.3 min to converge, while the baseline CNN requires 282.2 min. The classification time of the baseline CNN is 5.43 s, while DeepConvLSTM needs 6.68 s to classify the whole dataset. On average, within a second, DeepConvLSTM can classify almost 15 min of data. Thus, this implementation is suitable for online HAR on the GPU used in this work.</p><p>We have not yet implemented DeepConvLSTM on a wearable device. The GPU used in this work clearly outperforms the computational power available today even in a high-end wearable system (e.g., a multicore smartphone). However, DeepConvLSTM achieves a recognition speed of 900ˆreal-time using 1664 GPU cores at 1050 MHz. High-end mobile platforms already contain GPUs that can be used for general purpose processing <ref type="bibr" target="#b47">[47]</ref>. A mobile processor, such as the Qualcomm Snapdragon 820, comprises 256 GPU cores running at 650 MHz and supports OpenCL profiles for general purpose GPU computing. While cores differ in capabilities, the available computational power may well be sufficient for real-time recognition in upcoming mobile devices. Training, however, is best envisioned server-side (e.g., as in <ref type="bibr" target="#b46">[46]</ref>).</p><p>Removing the dependency on engineered features by exploiting convolutional layers is particularly important if the set of activities to recognise is changing over time, for instance as additional labelled data become available (e.g., through crowd-sourcing <ref type="bibr" target="#b48">[48]</ref>). In such an "open-ended" learning scenario, where the number of classes could be increased after the system is initially deployed, backpropagation of the gradient to the convolutional layer could be used to incrementally adapt the kernels according to the new data at runtime. Future work may consider the representational limits of such networks for open-ended learning and investigate rules to increase network size (e.g., adding new kernels) to maintain a desired representational power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we demonstrated the advantages of a deep architecture based the combination of convolutional and LSTM recurrent layers to perform activity recognition from wearable sensors. This new framework outperformed previous results in the OPPORTUNITY dataset of everyday activities by 4% on average and by 9% in an 18-class gesture recognition task. For the Skoda car manufacturing activity dataset, it outperformed previous deep non-recurrent approaches, improving the best reported scores by 6%. This extends the known domain of applicability for this unified framework of convolutional and recurrent units, which has never been reported on wearable sensor data.</p><p>In terms of time requirements, the recurrent architecture offers a very good trade-off between performance and training/recognition time when compared to a standard CNN. Indeed, the increase in training and recognition time for the recurrent architecture is only 20%.</p><p>We demonstrated that the recurrent LSTM cells are fundamental to allow one to distinguish gestures of a similar kind (e.g., "Open/Close Door 2" or "Open/Close Drawer 3"), which differ only by the ordering of the sensor samples. The baseline CNN model in comparison provided much worse performance on activities such as "Open/Close Door 2" or "Open/Close Drawer 3", where it made 2-3-times more errors. Convolution kernels are only able to capture temporal dynamics within the duration of the kernel. In comparison, the recurrent LSTM cells do not have this limitation and can learn the temporal dynamics on various (potentially much longer) time scales depending on their learned parameters. Furthermore, when directly compared, we have showed how a recurrent architecture offers better segmentation characteristics than a standard CNN, being capable of defining the boundaries of activities much more precisely. These findings support the hypothesis that the LSTM-based model takes advantage of learning the temporal feature activation dynamics, which CNNs are not fully capable of modelling.</p><p>From the results, we can also see how the model is able to learn from signals obtained from accelerometers, gyroscopes and magnetometers, fusing them without requiring any specific preprocessing. The model offers a 0.69 F 1 score performance using only accelerometers. This performance improves on average by 15% when fusing accelerometers and gyroscopes and by 20% when fusing accelerometers, gyroscopes and magnetic sensors. This offers a trade-off for applications where sensor data from different sources must be automatically processed.</p><p>As future work, we will investigate a transfer learning approach based on these models to perform activity recognition on large-scale data. We propose to reuse kernels trained on the benchmark datasets for feature extraction. This potential transfer of features will ease the deployment of activity recognizers on cloud infrastructures.</p><p>The code and model parameters of DeepConvLSTM are available at <ref type="bibr" target="#b49">[49]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Different types of units in neural networks. (a) MLP with three dense layers; (b) recurrent neural network (RNN) with two dense layers. The activation and hidden value of the unit in layer pl `1q are computed in the same time step t; (c) The recurrent LSTM cell is an extension of RNNs, where the internal memory can be updated, erased or read out.</figDesc><graphic coords="4,229.10,250.58,137.08,96.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Representation of a temporal convolution over a single sensor channel in a three-layer convolutional neural network (CNN). Layer pl ´1q defines the sensor data at the input. The next layer (l) is composed of two feature maps (a l 1 pτq and a l 2 pτq) extracted by two different kernels (K pl´1q 11</figDesc><graphic coords="6,142.87,127.12,309.54,247.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Architecture of the DeepConvLSTM (Conv, convolutional) framework for activity recognition.From the left, the signals coming from the wearable sensors are processed by four convolutional layers, which allow learning features from the data. Two dense layers then perform a non-linear transformation, which yields the classification outcome with a softmax logistic regression output layer on the right. Input at Layer 1 corresponds to sensor data of size D ˆS1 , where D denotes the number of sensor channels and S l the length of features maps in layer l. Layers 2-5 are convolutional layers. K l denotes the kernels in layer l (depicted as red squares). F l denotes the number of feature maps in layer l. In convolutional layers, a l i denotes the activation that defines the feature map i in layer l. Layers 6 and 7 are dense layers. In dense layers, a l t,i denotes the activation of the unit i in hidden layer l at time t. The time axis is vertical.</figDesc><graphic coords="8,98.65,160.89,397.97,123.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Placement of on-body sensors used in the OPPORTUNITYdataset (left: inertial measurements units; right: 3-axis accelerometers) [7].</figDesc><graphic coords="11,187.09,87.87,221.10,201.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Sequence labelling after segmenting the data with a sliding window. The sensor signals are segmented by a jumping window. The activity class within each sequence is considered to be the ground truth label annotated at the sample T of that window.</figDesc><graphic coords="12,131.81,331.93,331.65,197.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Output class probabilities for a ~25 s-long fragment of sensor signals in the test set of the OPPORTUNITY dataset, which comprises 10 annotated gestures. Each point in the plot represents the class probabilities obtained from processing the data within a sequence of 500 ms obtained from a sliding window ending at that point. The dashed line represents the Null class. DeepConvLSTM offers a better performance identifying the start and ending of gestures.</figDesc><graphic coords="13,98.65,87.88,397.99,115.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Performance of Skoda and OPPORTUNITY (recognizing gestures and with the Null class) datasets with different numbers of convolutional layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>i, f, o and c are respectively the input gate, forget gate, output gate and cell activation vectors, all of which are the same size as vector h defining the hidden value. Terms σ represent non-linear functions. The term a t is the input to the memory cell layer at time t. W ai , W hi , W ci , W a f , W h f , W c f , W ac , W hc , W ao , W ho and W co are weight matrices, with subscripts representing from-to relationships (W ai being the input-input gate matrix, W hi the hidden-input gate matrix, and so on). b i , b f , b c and b o are bias vectors. Layers' notation has been omitted for clarity.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Number and size of parameters for the DeepConvLSTM architecture and for the baseline model. The final number of parameters depends on the number of classes in the classification task, denoted as n c .</figDesc><table><row><cell>Layer</cell><cell>DeepConvLSTM</cell><cell></cell><cell cols="2">Baseline CNN</cell></row><row><cell></cell><cell>Size Per Parameter</cell><cell>Size Per Layer</cell><cell cols="2">Size Per Parameter Size Per Layer</cell></row><row><cell>2</cell><cell>K: 64 ˆ5 b: 64</cell><cell>384</cell><cell>K: 64 ˆ5 b: 64</cell><cell>384</cell></row><row><cell>3-5</cell><cell>K: 64 ˆ64 ˆ5 b: 64</cell><cell>20,544</cell><cell>K: 64 ˆ64 ˆ5 b: 64</cell><cell>20,544</cell></row><row><cell></cell><cell>W ai , W a f , W ac , W ao : 7232 ˆ128</cell><cell></cell><cell>W: 57, 856 ˆ128</cell><cell></cell></row><row><cell>6</cell><cell></cell><cell>942,592</cell><cell></cell><cell>7,405,696</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Class labels for the OPPORTUNITY and Skoda datasets. The OPPORTUNITY dataset is divided into activities belonging to Task A (modes of locomotion) and Task B (gesture recognition). For each class, we report the number of times an activity is performed and the number of instances obtained by the sliding window (all subjects combined). The Null class corresponds to the time intervals where there are no activities of interest.</figDesc><table><row><cell></cell><cell></cell><cell>OPPORTUNITY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Skoda</cell><cell></cell></row><row><cell></cell><cell>Gestures</cell><cell></cell><cell></cell><cell cols="2">Modes of Locomotion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Name</cell><cell># of Repetitions</cell><cell># of Instances</cell><cell>Name</cell><cell># of Repetitions</cell><cell># of Instances</cell><cell>Name</cell><cell># of Repetitions</cell><cell># of Instances</cell></row><row><cell>Open Door 1</cell><cell>94</cell><cell>1583</cell><cell>Stand</cell><cell>1267</cell><cell>38,429</cell><cell>Write on Notepad</cell><cell>58</cell><cell>20,874</cell></row><row><cell>Open Door 2</cell><cell>92</cell><cell>1685</cell><cell>Walk</cell><cell>1291</cell><cell>22,522</cell><cell>Open Hood</cell><cell>68</cell><cell>24,444</cell></row><row><cell>Close Door 1</cell><cell>89</cell><cell>1497</cell><cell>Sit</cell><cell>124</cell><cell>16,162</cell><cell>Close Hood</cell><cell>66</cell><cell>23,530</cell></row><row><cell>Close Door 2</cell><cell>90</cell><cell>1588</cell><cell>Lie</cell><cell>30</cell><cell>2866</cell><cell>Check Gaps Door</cell><cell>67</cell><cell>16,961</cell></row><row><cell>Open Fridge</cell><cell>157</cell><cell>196</cell><cell>Null</cell><cell>283</cell><cell>16,688</cell><cell>Open Door</cell><cell>69</cell><cell>10,410</cell></row><row><cell>Close Fridge</cell><cell>159</cell><cell>1728</cell><cell></cell><cell></cell><cell></cell><cell>Check Steering Wheel</cell><cell>69</cell><cell>12,994</cell></row><row><cell>Open Dishwasher</cell><cell>102</cell><cell>1314</cell><cell></cell><cell></cell><cell></cell><cell>Open and Close Trunk</cell><cell>63</cell><cell>23,061</cell></row><row><cell>Close Dishwasher</cell><cell>99</cell><cell>1214</cell><cell></cell><cell></cell><cell></cell><cell>Close both Doors</cell><cell>69</cell><cell>18,039</cell></row><row><cell>Open Drawer 1</cell><cell>96</cell><cell>897</cell><cell></cell><cell></cell><cell></cell><cell>Close Door</cell><cell>70</cell><cell>9783</cell></row><row><cell>Close Drawer 1 Open Drawer 2</cell><cell>95 91</cell><cell>781 861</cell><cell></cell><cell></cell><cell></cell><cell>Check Trunk</cell><cell>64</cell><cell>19,757</cell></row><row><cell>Close Drawer 2 Open Drawer 3</cell><cell>90 102</cell><cell>754 1082</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Close Drawer 3</cell><cell>103</cell><cell>1070</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clean Table</cell><cell>79</cell><cell>1717</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Drink from Cup</cell><cell>213</cell><cell>6115</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Toggle Switch</cell><cell>156</cell><cell>1257</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Null</cell><cell>1605</cell><cell>69,558</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1.2. The Skoda Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Baseline classifiers included in the datasets' comparative evaluation.</figDesc><table><row><cell cols="2">OPPORTUNITY Dataset</cell></row><row><cell></cell><cell>Challenge Submissions [7]</cell></row><row><cell>Method</cell><cell>Description</cell></row><row><cell>LDA</cell><cell>Linear discriminant analysis. Gaussian classifier that classifies on the assumption that the</cell></row><row><cell></cell><cell>features are normally distributed and all classes have the same covariance matrix.</cell></row><row><cell>QDA</cell><cell>Quadratic discriminant analysis. Similar to the LDA, this technique also assumes a normal</cell></row><row><cell></cell><cell>distribution for the features, but the class covariances may differ.</cell></row><row><cell>NCC</cell><cell>Nearest centroid classifier. The Euclidean distance between the test sample and the centroid</cell></row><row><cell></cell><cell>for each class of samples is used for the classification.</cell></row><row><cell>1NN</cell><cell>k nearest neighbour algorithm. Lazy algorithm where the Euclidean distances between a test</cell></row><row><cell></cell><cell>sample and the training samples are computed and the most frequently-occurring label of the</cell></row><row><cell></cell><cell>k-closest samples is the output.</cell></row><row><cell>3NN</cell><cell>See 1NN. Using 3 neighbours.</cell></row><row><cell>UP</cell><cell>Submission to the OPPORTUNITY challenge from U. of Parma. Pattern comparison using</cell></row><row><cell></cell><cell>mean, variance, maximum and minimum values.</cell></row><row><cell>NStar</cell><cell>Submission to the OPPORTUNITY challenge from U. of Singapore. kNN algorithm</cell></row><row><cell></cell><cell>using a single neighbour and normalized data.</cell></row><row><cell>SStar</cell><cell>Submission to the OPPORTUNITY challenge from U. of Singapore. Support vector machine</cell></row><row><cell></cell><cell>algorithm using scaled data.</cell></row><row><cell>CStar</cell><cell>Submission to the OPPORTUNITY challenge from U. of Singapore. Fusion of a kNN algorithm</cell></row><row><cell></cell><cell>using the closest neighbour and a support vector machine.</cell></row><row><cell>NU</cell><cell>Submission to the OPPORTUNITY challenge from U. of Nagoya. C4.5 decision tree algorithm</cell></row><row><cell></cell><cell>using mean, variance and energy.</cell></row><row><cell>MU</cell><cell>Submission to the OPPORTUNITY challenge from U. of Monash. Decision tree</cell></row><row><cell></cell><cell>grafting algorithm.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>F 1 score performance on OPPORTUNITY dataset for the gestures and modes of locomotion recognition tasks, either including or ignoring the Null class. The best results are highlighted in bold.</figDesc><table><row><cell>Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Modes of Locomotion Modes of Locomotion Gesture Recognition Gesture Recognition (No Null Class) (No Null Class)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">OPPORTUNITY Challenge Submissions</cell><cell></cell></row><row><cell>LDA</cell><cell>0.64</cell><cell>0.59</cell><cell>0.25</cell><cell>0.69</cell></row><row><cell>QDA</cell><cell>0.77</cell><cell>0.68</cell><cell>0.24</cell><cell>0.53</cell></row><row><cell>NCC</cell><cell>0.60</cell><cell>0.54</cell><cell>0.19</cell><cell>0.51</cell></row><row><cell>1 NN</cell><cell>0.85</cell><cell>0.84</cell><cell>0.55</cell><cell>0.87</cell></row><row><cell>3 NN</cell><cell>0.85</cell><cell>0.85</cell><cell>0.56</cell><cell>0.85</cell></row><row><cell>UP</cell><cell>0.84</cell><cell>0.60</cell><cell>0.22</cell><cell>0.64</cell></row><row><cell>NStar</cell><cell>0.86</cell><cell>0.61</cell><cell>0.65</cell><cell>0.84</cell></row><row><cell>SStar</cell><cell>0.86</cell><cell>0.64</cell><cell>0.70</cell><cell>0.86</cell></row><row><cell>CStar</cell><cell>0.87</cell><cell>0.63</cell><cell>0.77</cell><cell>0.88</cell></row><row><cell>NU</cell><cell>0.75</cell><cell>0.53</cell><cell></cell><cell></cell></row><row><cell>MU</cell><cell>0.87</cell><cell>0.62</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Deep architectures</cell><cell></cell><cell></cell></row><row><cell>CNN [17]</cell><cell></cell><cell></cell><cell></cell><cell>0.851</cell></row><row><cell>Baseline CNN</cell><cell>0.912</cell><cell>0.878</cell><cell>0.783</cell><cell>0.883</cell></row><row><cell>DeepConvLSTM</cell><cell>0.930</cell><cell>0.895</cell><cell>0.866</cell><cell>0.915</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Confusion matrix for OPPORTUNITY dataset using DeepConvLSTM.</figDesc><table><row><cell>Predicted Gesture</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Confusion matrix for OPPORTUNITY dataset using the baseline CNN.</figDesc><table><row><cell>Predicted Gesture</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Confusion matrix for OPPORTUNITY dataset using DeepConvLSTM (without the Null class).</figDesc><table><row><cell>Predicted Gesture</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Confusion matrix for OPPORTUNITY dataset using the baseline CNN (without the Null class).</figDesc><table><row><cell>Predicted Gesture</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Performance using different sensor modalities.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Accelerometers Accelerometers Opportunity</cell></row><row><cell></cell><cell>Accelerometers</cell><cell>Gyroscopes</cell><cell>+ Gyroscopes</cell><cell>+ Gyroscopes</cell><cell>Sensors Set</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+ Magnetic</cell><cell></cell></row><row><cell># of sensors channels</cell><cell>15</cell><cell>15</cell><cell>30</cell><cell>45</cell><cell>113</cell></row><row><cell>F 1 score</cell><cell>0.689</cell><cell>0.611</cell><cell>0.745</cell><cell>0.839</cell><cell>0.864</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>F 1 score performance of DeepConvLSTM on the OPPORTUNITY dataset. Classification performance is displayed individually per gesture, for different lengths of the input sensor data segments. Experiments carried out with sequences of length of 400 ms, 500 ms, 1400 ms and 2750 ms.</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Open Door 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Open Door 2</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Close Door 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Close Door 2</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Open Fridge</cell></row><row><cell>score</cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Close Fridge Open Dishwasher</cell></row><row><cell>F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Close Dishwasher</cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Open Drawer 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Close Drawer 1</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Open Drawer 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Close Drawer 2</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Open Drawer 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Close Drawer 3</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Clean Table Drink from Cup</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Toggle Switch</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Ratio gesture length/sequence length</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Figure 7.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work was partly funded by the Google Faculty Research Award grant "Is deep learning useful for wearable activity recognition?" and the U.K. EPSRCFirst Grant EP/N007816/1 "Lifelearn: Unbounded activity and context awareness". This publication is supported by multiple datasets, which are openly available at locations cited in the reference section.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: Francisco Javier Ordóñez conceptualized and implemented the deep frameworks, executed the experimental work, analysed the results, drafted the initial manuscript and revised the manuscript. Daniel Roggen conceptualized the deep frameworks, analysed the results, provided feedback, revised the manuscript and approved the final manuscript as submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The resident in the loop: Adapting the smart home to the user</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rashidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man. Cybern. J. Part A</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="949" to="959" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review of wearable sensors and systems with application in rehabilitation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rodgers</surname></persName>
		</author>
		<idno type="DOI">10.1186/1743-0003-9-21</idno>
	</analytic>
	<monogr>
		<title level="j">J. NeuroEng. Rehabil</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activity Recognition Using Inertial Sensing for Healthcare, Wellbeing and Sports Applications: A Survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Avci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marin-Perianu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marin-Perianu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Havinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architecture of Computing Systems (ARCS)</title>
		<meeting>the 23rd International Conference on Architecture of Computing Systems (ARCS)<address><addrLine>Hannover, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GaitAssist: A Daily-Life Support and Training System for Parkinson&apos;s Disease Patients with Freezing of Gait</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mazilu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Blanke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tröster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hausdorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems (SIGCHI)</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems (SIGCHI)<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05-01">26 April-1 May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The mobile fitness coach: Towards individualized skill assessment using personalized mobile devices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Roalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perv. Mob. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="203" to="215" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wearable Activity Tracking in Car Manufacturing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stiefmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ogris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tröster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Perv. Comput. Mag</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="42" to="50" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Opportunity challenge: A benchmark database for on-body sensor-based activity recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chavarriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sagha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Calatroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Digumarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Millán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tröster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2033" to="2042" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Tutorial on Human Activity Recognition Using Body-worn Inertial Sensors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Blanke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Limited-Memory Warping LCSS for Real-Time Low-Power Pattern Recognition in Wireless Nodes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Cuspinera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nguyen-Dinh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference Wireless Sensor Networks (EWSN)</title>
		<meeting>the 12th European Conference Wireless Sensor Networks (EWSN)<address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-02">February 2015</date>
			<biblScope unit="page" from="151" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In-Home Activity Recognition: Bayesian Inference for Hidden Markov Models</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Toledo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Kasteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perv. Comput. IEEE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Activity identification using body-mounted sensors: A review of classification techniques</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Preece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Goulermas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P J</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Crompton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiol. Meas</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Preprocessing techniques for context recognition from accelerometer data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Figo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Diniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M P</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perv. Mob. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="645" to="662" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th Annual International Conference on Machine Learning (ICML)<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the 22th Annual Conference on Advances in Neural Information Processing Systems (NIPS)<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12-10">10 December 2008</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chapter Convolutional Networks for Images, Speech, and Time Series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Brain Theory and Neural Networks</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="255" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory, fully connected Deep Neural Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>the 40th International Conference on Acoustics, Speech and Signal Processing (ICASSP)<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04">April 2015</date>
			<biblScope unit="page" from="4580" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks On Multichannel Time Series For Human Activity Recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="3995" to="4001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Turing computability with neural nets</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="77" to="80" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning precise timing with LSTM recurrent networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeedings of the 38th International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>eeedings of the 38th International Conference on Acoustics, Speech and Signal essing<address><addrLine>Vancouver, BC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysis of CNN-based Speech Recognition System using Raw Speech as Input</title>
		<author>
			<persName><forename type="first">D</forename><surname>Palaz</surname></persName>
		</author>
		<author>
			<persName><surname>Magimai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>-Doss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of International Speech Communication Association (Interspeech)</title>
		<meeting>the 16th Annual Conference of International Speech Communication Association (Interspeech)<address><addrLine>Dresden, Germany, 6-</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-10">10 September 2015</date>
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Herreweghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01911</idno>
		<title level="m">Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video. arXiv Preprint</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for human activity recognition using mobile sensors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Mengshoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th IEEE International Conference on Mobile Computing, Applications and Services (MobiCASE)</title>
		<meeting>the 6th IEEE International Conference on Mobile Computing, Applications and Services (MobiCASE)<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11">November 2014</date>
			<biblScope unit="page" from="197" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proeedings of the Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, NE, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">December 2013</date>
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for large-scale speech tasks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="39" to="48" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the 25th Conference on Advances in Neural Information Processing Systems (NIPS)<address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Cornell Univ. Lib</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">September 2014</date>
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th</title>
		<meeting>the 15th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m">Annual Conference of International Speech Communication Association (Interspeech)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09-18">14-18 September 2014</date>
			<biblScope unit="page" from="1915" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08909</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Cornell Univ. Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature Learning for Activity Recognition in Ubiquitous Computing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Olivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 22nd International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="1729" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Cornell Univ. Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">First Release</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Geneva, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">RMSProp and equilibrated adaptive learning rates for non-convex optimization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04390</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Collecting complex activity data sets in highly rich networked sensor environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Calatroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Holleczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Förster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tröster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pirkl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferscha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Networked Sensing Systems (INSS)</title>
		<meeting>the 7th IEEE International Conference on Networked Sensing Systems (INSS)<address><addrLine>Kassel, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Introducing a New Benchmarked Dataset for Activity Monitoring</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Symposium on Wearable Computers (ISWC)</title>
		<meeting>the 16th International Symposium on Wearable Computers (ISWC)<address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="page" from="108" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Activity recognition from on-body sensors: accuracy-power trade-off by dynamic sensor selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zappi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lombriser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Farella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tröster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European Conference on Wireless Sensor Networks (EWSN)</title>
		<meeting>the 5th European Conference on Wireless Sensor Networks (EWSN)<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-02-01">30 January-1 February 2008</date>
			<biblScope unit="page" from="17" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">mHealthDroid: a novel framework for agile development of mobile health applications</title>
		<author>
			<persName><forename type="first">O</forename><surname>Banos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Holgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Damas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pomares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Villalonga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Work-conference on Ambient Assisted Living an Active Ageing</title>
		<meeting>the 6th International Work-conference on Ambient Assisted Living an Active Ageing<address><addrLine>Belfast, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-05">2-5 December 2014</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Activity recognition for creatures of habit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Czerny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beigl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pers. Ubiquitous Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="205" to="221" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Opportunity</forename><surname>Dataset</surname></persName>
		</author>
		<ptr target="https://archive.ics.uci.edu/ml/datasets/OPPORTUNITY+Activity+Recognition" />
		<imprint>
			<date type="published" when="2012-11-19">2012. 19 November 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Skoda</forename><surname>Dataset</surname></persName>
		</author>
		<ptr target="http://www.ife.ee.ethz.ch/research/groups/Dataset" />
		<imprint>
			<date type="published" when="2008-11-19">2008. 19 November 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04664</idno>
		<title level="m">Deep Activity Recognition Models with Triaxial Accelerometers. arXiv preprint</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study. Intell. Data Anal</title>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="429" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Going Deeper With Convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Activity recognition service for mobile phones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Schmidtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beigl</surname></persName>
		</author>
		<author>
			<persName><surname>Actiserv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Wearable Computers (ISWC)</title>
		<meeting>the International Symposium on Wearable Computers (ISWC)<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Using mobile GPU for general-purpose computing: A case study of face recognition on smartphones</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on VLSI Design, Automation and Test (VLSI-DAT)</title>
		<meeting>the International Symposium on VLSI Design, Automation and Test (VLSI-DAT)<address><addrLine>Hsinchu, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04">April 2011</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">CrowdSignals: A call to crowdfund the community&apos;s largest mobile dataset</title>
		<author>
			<persName><forename type="first">E</forename><surname>Welbourne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Tapia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-09">September 2014</date>
			<biblScope unit="page" from="873" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><surname>Deepconvlstm</surname></persName>
		</author>
		<ptr target="https://github.com/sussexwearlab/DeepConvLSTM" />
		<imprint>
			<date type="published" when="2015-12-23">23 December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
