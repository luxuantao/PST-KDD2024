<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Long-Text Understanding with Short-Text Models</title>
				<funder>
					<orgName type="full">Len Blavatnik</orgName>
				</funder>
				<funder>
					<orgName type="full">Yandex Initiative for Machine Learning</orgName>
				</funder>
				<funder ref="#_Jp8HVa7">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Shashua Fellowship</orgName>
				</funder>
				<funder ref="#_rUuguaA">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">Blavatnik Family foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-01">1 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
							<email>maor.ivgi@cs.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
							<email>uri.shaham@cs.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
							<email>joberant@cs.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Long-Text Understanding with Short-Text Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-01">1 Aug 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2208.00748v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles and long documents, due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. Specifically, we partition the input into overlapping chunks, encode each with a short-text LM encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder).</p><p>We illustrate through controlled experiments that SLED offers a viable strategy for long text understanding and evaluate our approach on SCROLLS, a benchmark with seven datasets across a wide range of language understanding tasks. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer-based pretrained language models <ref type="bibr" target="#b42">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b9">Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Lewis et al., 2020;</ref><ref type="bibr">Raffel et al., 2020b;</ref><ref type="bibr" target="#b3">Brown et al., 2020)</ref> have been widely successful across all areas of natural language understanding (NLU). However, applying them over long texts (such as stories, scripts, or scientific articles) is prohibitive due to their quadratic complexity in the input length. To bridge this gap, recent work has developed more efficient transformer variants <ref type="bibr">(Kitaev et al., 2020a;</ref><ref type="bibr" target="#b2">Beltagy et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020a;</ref><ref type="bibr" target="#b12">Guo et al., 2021)</ref> and applied them over  <ref type="bibr" target="#b37">(Shaham et al., 2022)</ref> as a function of parameter count. Plugging existing pretrained LMs into the SLED framework dramatically improves their SCROLLS score (arrows from blue circles to pink stars). Gray triangles indicate models with dedicated pretraining for capturing long-range dependencies. BART large -SLED is competitive with LongT5 base <ref type="bibr" target="#b12">(Guo et al., 2021)</ref> and UL2 <ref type="bibr">(Tay et al., 2022b)</ref> (which has 50x more parameters), and slightly lags behind larger LongT5 models.</p><p>long-range language understanding tasks <ref type="bibr" target="#b28">(Mehta et al., 2022;</ref><ref type="bibr" target="#b37">Shaham et al., 2022)</ref>.</p><p>However, most efficient transformers use specialized architectures with custom implementations that are not guaranteed to scale as well as vanilla transformers <ref type="bibr">(Tay et al., 2022a)</ref>. Moreover, they require an expensive pretraining step and do not exploit off-the-shelf pretrained LMs that were trained for short texts. To date, the performance of efficient transformers on long texts has not matched the success of their short-range counterparts.</p><p>In this work, we present SLED: SLiding-Encoder and Decoder, a simple yet powerful method for applying off-the-shelf pretrained encoder-decoder models on long text problems, with a linear time and space dependency. Specifically (see Fig. <ref type="figure" target="#fig_1">2</ref>), we partition long documents into overlapping chunks of tokens of constant length and encode each chunk independently with an already-pretrained encoder. Then, a pretrained decoder attends to all contextualized input representations to generate the output. Our main assumption is that input tokens can be contextualized through their local surrounding (using a shorttext LM), and any global cross-chunk reasoning can be handled by the decoder, similar to fusionin-decoder <ref type="bibr" target="#b17">(Izacard and Grave, 2021)</ref>.</p><p>We evaluate SLED on a wide range of language understanding tasks. To substantiate SLED's adequacy for text processing, we perform controlled experiments over modified versions of SQuAD 1.1 <ref type="bibr" target="#b35">(Rajpurkar et al., 2016)</ref> and HotpotQA <ref type="bibr" target="#b49">(Yang et al., 2018)</ref> to show that SLED can (a) find relevant information that is embedded within a long text sequence and (b) fuse information from chunks that were encoded separately.</p><p>Our main evaluation is over SCROLLS, a recently-released benchmark that includes 7 longrange tasks across Question Answering (QA), summarization, and Natural Language Inference (NLI). We show (Fig. <ref type="figure" target="#fig_0">1</ref>) that taking a pre-trained encoder-decoder model, such as BART <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref> or T5 <ref type="bibr">(Raffel et al., 2020b)</ref>, and embedding it into SLED's framework results in dramatic improvement in performance (6 points on average across models). Moreover, BART large -SLED's performance is within one point difference from LongT5 base <ref type="bibr" target="#b12">(Guo et al., 2021)</ref>, a model that was specifically pretrained to handle longrange dependencies, and UL2 <ref type="bibr">(Tay et al., 2022b)</ref>, which contains 50x more parameters. Importantly, SLED-based models can use any future pretrained LM out-of-the-box without requiring additional pretraining to further improve performance.</p><p>Due to its simplicity, SLED can also be used as a diagnostic tool for analyzing long-range benchmarks. We analyze the seven datasets in SCROLLS through the lens of SLED and show which datasets require the input to be contextualized with remote tokens. Specifically, we find that in QA and NLI tasks, relatively local contextualization is sufficient for high performance.</p><p>To summarize, our main contributions are: 1. We present SLED, a simple and effective approach for processing long texts that leverages off-the-shelf encoder-decoder LMs based on fusion-in-decoder.</p><p>2. We demonstrate SLED's efficacy in both controlled experiments, as well as on the SCROLLS benchmark, which leads to competitive results compared to specialized models that include up to 10x more parameters.</p><p>3. We use SLED as a diagnostic tool for analyzing the long-range properties of datasets in the SCROLLS benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>We provide an open-source implementation of SLED,<ref type="foot" target="#foot_0">1</ref> which can be seamlessly integrated into the Transformers library <ref type="bibr" target="#b46">(Wolf et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Recent advances in natural language processing have been by and large fueled by the transformer architecture <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>. A core component of the transformer is the self-attention layer where every input token "attends" to every other token to produce its contextualized representation. This results in quadratic time and space dependency w.r.t. the length of the input, limiting the ability of transformers to process long sequences. This long-text limitation has sparked ample interest in developing efficient transformer variants. One prominent family of methods is based on sparse attention, where each token attends to a constant number of other tokens, overcoming the quadratic dependency. Tokens typically attend either to their local surrounding <ref type="bibr">(Zaheer et al., 2020a;</ref><ref type="bibr" target="#b2">Beltagy et al., 2020;</ref><ref type="bibr" target="#b0">Ainslie et al., 2020;</ref><ref type="bibr" target="#b14">Gupta and Berant, 2020)</ref> or to tokens that are semantically similar <ref type="bibr">(Kitaev et al., 2020b;</ref><ref type="bibr" target="#b36">Roy et al., 2021)</ref>. Moreover, a constant number of global tokens that attend to and are attended by all input tokens are often added to each attention sub-layer. Recent analyses <ref type="bibr" target="#b47">(Xiong et al., 2022)</ref> have shown that sparse transformers with local attention are competitive with other variants on multiple language understanding tasks.</p><p>Our method, SLED, falls into the family of local sparse attention variants. However, unlike prior work, SLED re-uses and extends existing shortrange encoder-decoder models, and does not require specialized pretraining or dedicated CUDA implementations.</p><p>In most local attention variants, e.g., LED <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>, attention is local per-layer, but the receptive field of tokens grows across layers.</p><p>In SLED, which we describe next, tokens have access to the same number of tokens, independent of a layer's depth, which enables better parallelization. For a survey on the families of efficient transformers, see <ref type="bibr" target="#b40">Tay et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this work, we propose a simple approach for avoiding transformer's quadratic complexity, motivated by the Locality of information assumption:</p><p>In an encoder-decoder architecture, the encoder can effectively contextualize input tokens with local context only, leaving long-range dependencies to be handled by the decoder. SLED relies on said modeling assumption to encode shorter chunks independently and perform fusion of information in the decoder <ref type="bibr" target="#b17">(Izacard and Grave, 2021)</ref>. We now describe the SLED model in detail.</p><p>Input SLED uses a pretrained encoder-decoder model M as a backbone. SLED receives a tokenized document of length n (blue squares in Fig. <ref type="figure" target="#fig_1">2</ref>), and an optional short tokenized prefix of length m n, typically representing a question about the document, an instruction to perform some generation task, or a hypothesis (orange squares in Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>Steps SLED follows the following steps:</p><p>(a) Document tokens are split into C chunks of length c (In Fig. <ref type="figure" target="#fig_1">2</ref>, c = 4). The middle (1 -?) ? c tokens in each chunk are contextualized from both the left and right by P := ??c 2 tokens, where ? ? [0, 0.5] (? = 0.5 in Fig. <ref type="figure" target="#fig_1">2</ref>). We call these middle tokens the effective chunk, since they will constitute the output of the encoder, and term the tokens on each side by context padding. (e) To give the decoder access to prefix tokens, we encode the prefix tokens with M enc , and prepend the result to the contextualized representation (leftmost chunk in Fig. <ref type="figure" target="#fig_1">2</ref>(a)-(d)).</p><p>(f) Finally, we generate the output with the backbone decoder, M dec , which uses standard cross-attention over the m + n encoded tokens (Fig. <ref type="figure" target="#fig_1">2(e)</ref>).</p><p>SLED requires handling a few edge cases, namely, dealing with the first and last chunk that do not have bidirectional context. We refer to App. A.1 for these details.</p><p>SLED's Complexity SLED divides an input of length n to C chunks of size c. Since ? ? [0, 0.5], it follows that C ? n c , 2n c . While the complexity of encoding each chunk is quadratic in c due to self-attention, c n is constant and thus the memory and compute dependency is linear in n.<ref type="foot" target="#foot_1">2</ref> </p><p>In particular, the complexity to encode the input with a model of l attention layers is:</p><formula xml:id="formula_0">O l ? c 2 ? 2n c = O (l ? c ? n) .</formula><p>Decoding is done as proposed by <ref type="bibr" target="#b42">Vaswani et al. (2017)</ref>, thus requiring O(nk + k 2 ) memory. Assuming a constant output sequence length k n, this remains linear in n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Efficacy of Fusion in Decoder</head><p>As mentioned ( ?3), SLED relies on the assumption that chunks can be encoded independently and fusion across them can be delegated to the decoder (Locality of information assumption). This is similar to the Fusion in Decoder (FiD) approach, introduced by <ref type="bibr" target="#b17">Izacard and Grave (2021)</ref> for opendomain question answering (ODQA). However, there, the encoder-decoder receives a set of independent passages and needs to generate an answer that can typically be extracted from a single passage. Here, we extend the scope of FiD by applying it over a single, long, and coherent input that potentially requires global contextualization.</p><p>To demonstrate the viability of FiD for long text language tasks, we design two controlled experiments that quantify the extent to which FiD can perform two operations at the heart of long-text processing. First, can FiD find a "needle-in-ahaystack", i.e., locate a piece of short information embedded in long text, disregarding irrelevant information. Second, can FiD "piece the puzzle" and fuse two pieces of information that are encoded independently when generating an output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Needle in a haystack</head><p>To check if SLED can ignore irrelevant text and locate a single piece of information, we cast SQuAD 1.1 <ref type="bibr" target="#b35">(Rajpurkar et al., 2016)</ref> as a sequenceto-sequence task with long input. SQuAD is a question answering dataset, where given a question-paragraph pair the goal is to generate the answer (which lies within the paragraph). For each question-paragraph pair, we randomly sample 9 other paragraphs from the the dataset and concatenate them to form a long document. <ref type="foot" target="#foot_2">3</ref> We then fine-  tune and evaluate our models in two settings: a) Ordered Distractors: the gold paragraph is the first one, and all other distractors are concatenated after it. b) Shuffled Distractors: we randomly shuffle the order of all paragraphs so the answer can be anywhere in the input document. Since this is a QA task, the prefix is the question.</p><formula xml:id="formula_1">S L E D L E D L E D B A R T<label>(</label></formula><p>We use BART base <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref> as our backbone model, M , throughout ?4, and compare SLED to an oracle BART base that is given the gold paragraph only with no distractor paragraphs. this is an oracle setup since BART base can take 1,024 tokens as input and all gold paragraphs are shorter. If SLED can match the oracle performance, we can infer that indeed the decoder can find a needle in a haystack. In addition, we compare SLED to BART base which is given  only the first 1K tokens and to LED <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>, which uses local sparse attention, similar to SLED (LED has the same backbone BART base ). However, as explained in ?2, the receptive field of LED layers linearly grows with the number of layers, and thus information can be fused in the encoder, unlike SLED where cross-chunk fusion must be delegated to the decoder. Last, for QA tasks, LED defines the question tokens as global tokens, and as an additional sanity test we evaluate LED L , where no global tokens are used. For both LED and SLED we use a chunk size c = 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Fig 3(a)</head><p>shows the results of our evaluation on the development set. SLED almost matches the performance of an oracle BART base that is not given any distractor paragraphs, reaching an F 1 score of 87.6 compared to the oracle F 1 of 88.1 (horizontal line in the figure). LED also achieves high performance (but lower than SLED in the shuffled setup), showing both models learn to ignore distracting information and find a needle in a haystack. As expected, both LED L and BART suffers a significant drop in performance when the passages are shuffled, as the gold paragraph is not contextualized with the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Piecing a puzzle</head><p>We now verify that SLED can fuse pieces of information from different chunks. To this end, we modify HotpotQA <ref type="bibr" target="#b49">(Yang et al., 2018)</ref>, a multihop question answering dataset, in which every question relies on two pieces of information (located in different paragraphs). While in the original setting, each input in HotpotQA has two gold paragraphs and 8 distractor paragraphs, we include only the two gold paragraphs in our experiments.</p><p>To ensure SLED and LED encode the relevant two pieces of information in separate chunks, we set the chunk size to c = 128. Similar to ?4.1, we compare SLED to an oracle BART base with full attention over 1,024 tokens,<ref type="foot" target="#foot_3">4</ref> to LED, and to LED L . Finally, past work has shown that many examples in HotpotQA can be answered with access to the "second" gold paragraph only, which contains the answer <ref type="bibr" target="#b18">(Jiang and Bansal, 2019)</ref>. Thus, we also evaluate a BART model that is given the second passage only.</p><p>Results Fig. <ref type="figure">4</ref>(a) shows that indeed, SLED's decoder can effectively fuse information from two separately encoded chunks, reaching an F 1 of 76.5, slightly lower than the oracle F 1 of 78.6. Notably, SLED substantially outperforms a BART model with access to the entire second paragraph, showing that information is fused by the decoder. LED slightly outperforms SLED, but when denied access to global tokens (LED L ) its performance drops sharply. This shows that the large receptive field of deep LED layers does not suffice for information fusion and interaction between the question and text is crucial for the decoder.</p><p>To summarize, our two controlled experiments show that SLED can perform the operations of retrieving and fusing information, which are fundamental for long text language tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations of design choices</head><p>We leverage our controlled experimental setup to further investigate the components of SLED.</p><p>Efficacy of the encoder While ?4.2 shows that SLED can fuse separate pieces of information in the decoder, it is not clear to what extent local contextualization is necessary. To check whether it is possible for all fusion to occur in the decoder, we finetune SLED with a chunk size of c = 1, such that input tokens do not observe any context in the encoder. As can be seen in the leftmost bars in Fig. <ref type="figure" target="#fig_4">3</ref> Contextualizing chunks with a prefix As explained, SLED does not use global tokens, but instead contextualizes each chunk with a preprended prefix. To verify this is indeed necessary, we finetune a SLED model that treats the prefix as another chunk and does not prepend it to document chunks. <ref type="foot" target="#foot_4">5</ref> The second bar in Fig. <ref type="figure" target="#fig_4">3</ref>(b) and Fig. <ref type="figure" target="#fig_2">4(b)</ref> shows a significant drop in performance for all settings, suggesting the prefix is needed during encoding.</p><p>As expected, there is practically no difference between the Ordered and Shuffled settings in Fig. <ref type="figure" target="#fig_4">3(b</ref>). In contrast, LED L which is similar in concept (due to the lack of global tokens) shows a significant drop when paragraphs are shuffled. This shows the possible effectiveness of the increased receptive field in LED, but only when the gold paragraph is relatively close to the prefix.</p><p>Encoding the prefix After showing the prefix is crucial for the encoder, we ask whether the decoder also needs access to the prefix or whether relevant information from the prefix can be infused into the chunk representations. To test that, we finetune SLED as usual, but remove the prefix tokens from the final representation given to the decoder. The last bar in Fig. <ref type="figure" target="#fig_4">3</ref>(b) and Fig. <ref type="figure" target="#fig_2">4(b)</ref> shows that providing the decoder with prefix representations makes little difference if any at all, suggesting that indeed the encoder infuses the important information from the prefix into the encoded document tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate SLED on SCROLLS <ref type="bibr" target="#b37">(Shaham et al., 2022)</ref>, a recently-proposed benchmark for evaluating long text understanding. SCROLLS contains seven benchmarks that span three different language understanding tasks:</p><p>1. Summariazation: GovReport <ref type="bibr" target="#b16">(Huang et al., 2021)</ref>  For each task, we use the official evaluation metrics defined in SCROLLS, which are based on the metrics from the original datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>We evaluate SLED with both BART <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref> and T5 <ref type="bibr">(Raffel et al., 2020b)</ref> as backbone models. For each backbone model, we compare performance within SLED, which can consume long sequences, vs. the backbone models alone that are fed with the first 1,024 tokens.</p><p>For each model-dataset pair, we run hyperparameter tuning (detailed in app. A.2) based on the development set. Additionally, we submit generated predictions over the test set to SCROLLS leaderboard, 6 and compare to the reported performance of other models at the time of submission.  <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref> scores were reported by <ref type="bibr" target="#b37">Shaham et al. (2022)</ref>, while the results for LongT5 <ref type="bibr" target="#b12">(Guo et al., 2021)</ref> and UL2 <ref type="bibr">(Tay et al., 2022b)</ref> were submitted by their authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tab</head><p>UL2 (less than one point difference on the average SCROLLS score), despite UL2's large parameter count (50x larger), and with no need for expensive pretraining geared towards long-range tasks. BART large -SLED's performance is moderately lower than the larger LongT5 models.</p><p>Barring QuALITY, SLED significantly improves performance across all tasks compared to the corresponding backbone models. All summarization datasets (GovReport, SummScreenFD and QMSum) show impressive gains of up to 35% compared to their baseline scores, across all metrics (Rouge-1/Rouge-2/Rouge-L <ref type="bibr" target="#b25">(Lin, 2004)</ref>) and for all three backbone models. Similarly, on Con-tractNLI <ref type="bibr" target="#b22">(Koreeda and Manning, 2021)</ref> we see large relative improvements. As the performance of the baseline models was already high, this boost in performance is even more significant. Finally, the QA datasets Qasper and NarrativeQA show the largest gains, improving by an average of 60%.</p><p>QuALITY In stark contrast to other datasets lies the multi-choice QA dataset QuALITY <ref type="bibr" target="#b29">(Pang et al., 2021)</ref>. While the performance of BART large -SLED is above chance, it barely improves the performance of its backbone model (BART large ), which observes only the first 1K to-kens, with a similar trend in other backbone models. Analyzing test scores in Tab. 1, we see that increasing model size consistently improves performance (up to 46% exact match), but increasing input length has a negligible effect. Since reported human accuracy on QuALITY is high (93.5%), this hints that QuALITY might require commonsense reasoning and knowledge that are absent from models with a lower parameter count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>We have shown that taking offthe-shelf pretrained LMs and embedding them into SLED leads to competitive performance on SCROLLS. Importantly, any future pretrained LM can be easily plugged into SLED, without the need for an expensive pretraining step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Datasets analysis</head><p>SLED's simplicity and modularity allow it to be used as a useful tool for dataset analyses. Specifically, we can vary the chunk size, c, and the number of tokens, n, across datasets to analyze a) how local are individual pieces of relevant information, and b) how far into the document is relevant information located.</p><p>Locality of information SLED relies on an assumption that information can be contextualized locally at encoding time. To analyze locality, we vary the chunk size, c, which defines the attention window, and measure the effect on SCROLLS datasets with input length 16K. Fig. <ref type="figure" target="#fig_7">5</ref> shows the results of this experiment, where the y-axis shows the relative improvement compared to BART base on a target metric as a function of the chunk size c for all datasets. We observe that in all datasets the best performing chunk size is relatively small (up to 256), and further increasing c even hurts the performance in some cases. However, the summarization datasets show a much larger gain in performance when increasing c up to that threshold. This coincides with a common hypothesis that QA and NLI require relatively local context, and thus increasing c can add noise and hurt optimization, while summarization may require a more highlevel view of information.</p><p>Distance from start of document We now analyze whether indeed the entire document is required for tasks in SCROLLS by varying the maximum document length, n. results of this experiment, where the y-axis shows relative improvement of BART base -SLED compared to BART base as a function of the first n tokens from the document (chunk size c = 256). As expected, all datasets (except QuALITY) show a roughly monotonic improvement in performance with n. This shows that (a) SLED is able to effectively use all of the information in a long sequence (up to 16K tokens),<ref type="foot" target="#foot_5">7</ref> , and that (b) observing the entire inputs from the SCROLLS datasets improves performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of context padding</head><p>In all experiments thus far, we used a conservative padding value ? = 0.5, that is, for every chunk of length c, the effective chunk size is c 2 and we use c 4 context padding tokens on each side. Since both memory and, more importantly, the number of forwards passes through the encoder are linear in the number of chunks, a natural question is how much padding and overlap are necessary to achieve satisfactory results.</p><p>To explore this, we finetune BART base -SLED on four datasets from the three dataset categories of SCROLLS, varying the value of ?, and fixing c = 256. Tab. 2 shows the results of this experiment, where we compare relative gain compared to BART base across different ? values as well as the effect on training time in hours.</p><p>As expected, decreasing the padding factor and consequently the number of chunks reduces training time significantly. When ? = 0.25 or ? = 0.05 training is faster by 2x-2.5x. Moreover, relative gain (i.e., improvement relative to the baseline results) is often similar or even higher with less padding (perhaps due to better encoding or more stable optimization). Thus, one can improve the efficiency and performance of SLED by tuning the hyperparameter ? for optimal behaviour, which we leave for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Efficient transformers Many efficient attention variants were proposed in recent years, to alleviate the quadratic complexity of dense attention <ref type="bibr" target="#b40">(Tay et al., 2020;</ref><ref type="bibr" target="#b10">Fournier et al., 2021)</ref>. Among those are clustering vectors to distinct buckets, calculating attention only within each one <ref type="bibr">(Kitaev et al., 2020a)</ref>, attending only to a fixed number of hidden vectors <ref type="bibr" target="#b26">(Ma et al., 2021)</ref>, using random features to approximate the attention matrix <ref type="bibr" target="#b5">(Choromanski et al., 2021;</ref><ref type="bibr" target="#b7">Peng et al., 2021)</ref>, and using lowrank factorizations <ref type="bibr" target="#b44">(Wang et al., 2020)</ref>. Despite achieving respectable performance when finetun-ing these models on the Long Range Arena benchmark <ref type="bibr" target="#b39">(Tay et al., 2021)</ref>, many of them were not yet proven to work well as a backbone for pretrained language models. In fact, recent work <ref type="bibr" target="#b48">(Xiong et al., 2021)</ref> on encoder-only models found many do not outperform a simple local attention sliding window on downstream language tasks. We discuss such methods next.</p><p>Sparse attention variants A popular and simple solution for allowing attention-based models to process long sequences is to use local attention, where each token attend to a local window around it. Longformer <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>, GMAT <ref type="bibr" target="#b14">(Gupta and Berant, 2020)</ref>  <ref type="bibr" target="#b39">(Tay et al., 2021)</ref>. State space models are now an active research field <ref type="bibr" target="#b13">(Gupta, 2022;</ref><ref type="bibr" target="#b28">Mehta et al., 2022)</ref>, but their efficacy on long-range language understanding tasks has not been tested yet.</p><p>Fusion-in-Decoder <ref type="bibr" target="#b17">Izacard and Grave (2021)</ref> proposed to encode multiple independent passages separately, and concatenate the encodings prior to the decoding phase. Despite encouraging empirical evidence <ref type="bibr" target="#b1">(Amouyal et al., 2022;</ref><ref type="bibr" target="#b50">Yavuz et al., 2022)</ref>, we are the first (to our knowledge) to analyze FiD's feasibility and limitations in a controlled setting. Importantly, we test FiD on longrange tasks over a single long document, rather than a collection of independent passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrained models with sliding windows</head><p>Wrapping a BERT encoder within a sliding window was proposed by <ref type="bibr" target="#b7">Cui and Hu (2021)</ref> in the context of a specialized architecture for summarization. <ref type="bibr" target="#b45">Wang et al. (2019)</ref> showed that sliding BERT across text improves performance on several QA datasets. In this work, we propose a sliding window approach that can be easily plugged into any existing encoder-decoder model without additional parameters or task-specific training, and show its efficacy for long-range text understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>We present SLED as a simple and effective method to extend the capabilities of pretrained short-text models to long-text tasks. Despite its impressive empirical performance on SCROLLS, SLED suffers from two disadvantages which may limit its applicability to some long-range tasks.</p><p>Long output To obtain linear complexity, SLED assumes the output length k is constant. This is since the decoder uses quadratic selfattention over the output, on top of O(nk) crossattention between the output and input. While most current long-text tasks follow this assumption, future tasks may include long text generation such as script writing, stories generation, etc. This limitation is not unique to SLED and affects other existing long-range efficient transformers including LongT5 and LED. Aside from finetuning on tasks that require long-text generation, this also affects pretraining long-range models on long inputs with self-supervised losses such as span-corruption <ref type="bibr">(Raffel et al., 2020b)</ref> or denoising <ref type="bibr" target="#b23">Lewis et al. (2020)</ref>, which require the decoder to process an output that is linear in the length of the input.</p><p>Co-reference resolution and fact retention An assumption at the heart of SLED is the Locality of information assumption. When the input text is long, this assumption may break if distant entity resolution or factual knowledge are required. For example, a chapter in a book may mention "they were walking into the room" when knowledge of what room or who walked is located a few chapters back. In such cases, the encoder used by SLED will not be able to access this information, moving more responsibility to the decoder and reducing the effectiveness of the contextual encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this work we present SLED, a simple approach for modeling long texts which slides a pretrained short-range encoder over a long input document and then generates an output by attending to the encoded tokens. We show SLED can perform core operations that are important for long text understanding, such as finding relevant pieces of information and fusing them at decoding time, and demonstrate competitive performance on the SCROLLS benchmark compared to larger models and models that employ a dedicated and expensive pretraining step.</p><p>One of SLED's most attractive features is that it can be readily used with any short-range pretrained LM. Thus, any future encoder-decoder model can be flexibly plugged into it to achieve further gains in performance on SCROLLS, some of its tasks, or any other long-range task.</p><p>We open source SLED and hope it encourages the research community to easily extend to longer inputs and push the borders of NLU models' applicability in real-world use-cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Models' SCROLLS score<ref type="bibr" target="#b37">(Shaham et al., 2022)</ref> as a function of parameter count. Plugging existing pretrained LMs into the SLED framework dramatically improves their SCROLLS score (arrows from blue circles to pink stars). Gray triangles indicate models with dedicated pretraining for capturing long-range dependencies. BART large -SLED is competitive with LongT5 base<ref type="bibr" target="#b12">(Guo et al., 2021)</ref> and UL2(Tay et al.,  2022b)  (which has 50x more parameters), and slightly lags behind larger LongT5 models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of SLED. (a) Input tokens (t 1 , . . . , t n ) are chunked into C overlapping chunks of length c (here, c = 4). Each chunk is made of P := ??c 2 context padding tokens at the right and left edges of the chunk, and (1 -?) ? c effective chunk tokens in the middle (here, ? = 0.5, P = 1). (b) We prepend the prefix tokens (p 1 , . . . , p m ) to each chunk (m n). (c) Each chunk is encoded independently using the already pretrained backbone encoder M enc . (d) We gather the encoded effective chunks tokens (yellow) and discard the context padding tokens (pink) (e) We pass the encoded input to the decoder to generate the final output sequence (o 1 , . . . , o k ).</figDesc><graphic url="image-1.png" coords="3,71.34,-26.70,500.08,214.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( b )</head><label>b</label><figDesc>Each chunk is prepended by (optional) prefix tokens (Fig. 2(b)). (c) Each chunk is encoded independently, using the backbone encoder M enc (see Fig. 2(c)). (d) To create a contextualized representation for each token, we keep from each chunk only the tokens from the effective chunk, and concatenate them (Fig. 2(d)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: F 1 results on our modified SQuAD 1.1's (Rajpurkar et al., 2016) development set evaluation: (a) the horizontal line gives the performance of an oracle BART base given the gold paragraph only. SLED matches oracle performance in both the ordered and shuffled setting (see text). LED slightly underperforms SLED in the shuffled setup. Both BART (given only the first 1K tokens) and LED with no global tokens (LED L ) performs poorly in the shuffled setup. (b) Ablations on SLED's architecture, see ?4.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) and Fig. 4(b), removing local contextualization results in poor performance, illustrating the importance of local contextualization on the encoder side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: BART base -SLED relative improvement compared to BART base results, when varying the SLED's chunk size (i.e. c), fixing the maximum input length to 16K. Top: Summarization datasets. The y-axis measures relative improvement of Rouge-2. Bottom: QA and NLI datasets. The y-axis measures relative improvement of exact match for QuALITY and Con-tractNLI and F 1 for NarrativeQA and Qasper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FigFigure 6 :</head><label>6</label><figDesc>Figure 6: BART base -SLED relative improvement compared to BART base results, when varying the input length fed to SLED, fixing c = 256. Top: Summarization datasets. The y-axis measures relative improvement of Rouge-2. Bottom: QA and NLI datasets. The y-axis measures relative improvement of exact match for QuALITY and ContractNLI and F 1 for Nar-rativeQA and Qasper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Main results on the SCROLLS benchmark. Chunk/Input refers to the chunk size used (c) and to the maximal input length (n). Avg is the average SCROLLS score as described in<ref type="bibr" target="#b37">Shaham et al. (2022)</ref>. Development scores for QuALITY are only for the full set (T). ? indicates reported results from SCROLLS public leaderboard. 6 LED L base</figDesc><table><row><cell>. 1 reports results over SCROLLS devel-</cell></row><row><cell>opment and test sets. Taking short-range pre-</cell></row><row><cell>trained LMs like BART and T5 and casting them</cell></row><row><cell>into SLED's framework allows them to process</cell></row><row><cell>long documents effectively, improving the aver-</cell></row><row><cell>age SCROLLS score by 5.3-7 points. Examining</cell></row><row><cell>BART base -SLED, we see a large improvement</cell></row><row><cell>compared to LED L base (29.2?35.3), and compet-</cell></row><row><cell>itive performance on multiple tasks compared to</cell></row><row><cell>LongT5 base and UL2. Moreover, adding SLED</cell></row><row><cell>to BART large results in a high-performing model</cell></row><row><cell>with results that are similar to LongT5 base and</cell></row><row><cell>6 https://www.scrolls-benchmark.com/</cell></row><row><cell>leaderboard</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Context padding ablations results. BART base -SLED relative improvement compared to BART base when varying ?. In all cases the maximum input length is 16K and c = 256. For QMSum and SummScreen (SumScr) we measure relative improvement of Rouge-2, for ContractNLI we measure the gain in exact match, and for Qasper in F 1 .</figDesc><table><row><cell>Padding (?)</cell><cell cols="4">Relative Gain QMSum Qasper QMSum Qasper Train Time (hrs.)</cell></row><row><cell>50%</cell><cell>22.8%</cell><cell>53.7%</cell><cell>8.5</cell><cell>7.0</cell></row><row><cell>25%</cell><cell>17.9%</cell><cell>54.7%</cell><cell>3.6</cell><cell>3.0</cell></row><row><cell>5%</cell><cell>23.5%</cell><cell>52.0%</cell><cell>3.0</cell><cell>2.8</cell></row><row><cell></cell><cell>CNLI</cell><cell>SumScr</cell><cell>CNLI</cell><cell>SumScr</cell></row><row><cell>50%</cell><cell>8.9%</cell><cell>21.0%</cell><cell>6.0</cell><cell>4.1</cell></row><row><cell>25%</cell><cell>10.1%</cell><cell>19.0%</cell><cell>3.4</cell><cell>2.8</cell></row><row><cell>5%</cell><cell>7.4%</cell><cell>15.9%</cell><cell>3.2</cell><cell>2.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/Mivg/SLED</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We assume the prefix length (m) is negligible compared to the input length n and thus its effect on asymptotic complexity is negligible.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>  3  We only consider paragraphs that are not within the gold document and do not contain the gold answer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>All examples have ?1,024 tokens, including the prefix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We add masked padding after the prefix to ensure chunking of the document remains identical.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>For ContractNLI, we do not show results for over</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>8K tokens, as the length of over 95% of the tokenized examples is less than 8K.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://github.com/tau-nlp/scrolls</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was partially supported by The <rs type="funder">Yandex Initiative for Machine Learning</rs>, the <rs type="funder">Shashua Fellowship</rs>, the <rs type="funder">Len Blavatnik</rs> and the <rs type="funder">Blavatnik Family foundation</rs>, and the <rs type="funder">European Research Council (ERC)</rs> under the <rs type="funder">European Union</rs> <rs type="programName">Horizons 2020 research and innovation programme</rs> (grant <rs type="funder">ERC</rs> <rs type="grantNumber">DELPHI 802800</rs>). This work was completed in partial fulfillment for the Ph.D. degree of the first author.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rUuguaA">
					<orgName type="program" subtype="full">Horizons 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_Jp8HVa7">
					<idno type="grant-number">DELPHI 802800</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 SLED Implementation Details While ?3 details SLED's method, in the sake of brevity, it leaves out a few edge cases in the implementation details. First, in the case that n ? c, we do not perform any chunking and thus SLED effectively behaves identical to its backbone model. Second, to impose symmetry in the contextualization, we require that ? ? c = 2z for some z ? N. Finally, encoding the first and last ??c 2 input tokens requires careful attention, as they lack bidirectional context. To preserve as much commonality between chunks, all first (2-?)?c 2 are considered the effective chunk tokens in the first chunk. To account for the final tokens, the last chunk will always start at token t n-c+1 so it would contain exactly c tokens, and its effective chunk tokens will be defined as all tokens that were not part of any previous effective chunk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Experimental details</head><p>All experiments were done with the transformers <ref type="bibr" target="#b46">(Wolf et al., 2020)</ref> library (version 4.10.0.dev0) and tracked using the Comet.ML infrastructure (Comet.ML, 2021). All pretraining and finetuning datasets were provided by the Datasets library <ref type="bibr">(Lhoest et al., 2021)</ref> (version 1.17.0) and the SCROLLS official repository 8 . The datasets inputs and splits remained as suggested by the authors of SCROLLS. We trained all models to the maximum number of epochs suggested for each models by <ref type="bibr" target="#b37">Shaham et al. (2022)</ref> and chose the checkpoint that performed best over the a subset of 1000 samples out of the evaluation set (i.e., posthoc early stopping) and then evaluated it over the entire evaluation set. To perform model selection, for each model-dataset pair we finetuned 9 models with LINEAR learning rate scheduling, AdamW optimized with the default settings, and setting the learning rate to one of {2e -5, 5e -5, 1e -4} and the effective batch size to one of {8, 16, 32}. Warmup was fixed at 10%.</p><p>Other packages used for training and analyzing the results are numpy <ref type="bibr" target="#b15">(Harris et al., 2020)</ref>, scipy <ref type="bibr" target="#b15">(Virtanen et al., 2020</ref><ref type="bibr">), pandas (Wes McKinney, 2010;</ref><ref type="bibr">pandas development team, 2020)</ref> and scikitlearn <ref type="bibr" target="#b31">(Pedregosa et al., 2011)</ref>. All models ran using PyTorch <ref type="bibr" target="#b30">(Paszke et al., 2019)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ETC: Encoding long and structured inputs in transformers</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Qampari: : An open-domain question answering benchmark for questions with many answers from multiple paragraphs</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Amouyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno>ArXiv, abs/2205.12665</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zewei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<title level="m">Summscreen: A dataset for abstractive screenplay summarization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam?s</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Benjamin</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Comet.ML. 2021. Comet.ML home page</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sliding selector network with dynamic memory for extractive summarization of long documents</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.470</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5881" to="5891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A dataset of information-seeking questions and answers anchored in research papers</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.365</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4599" to="4610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A practical survey on faster and lighter transformers</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?tan</forename><surname>Marceau Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Aloise</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><surname>Christopher R'e</surname></persName>
		</author>
		<idno>ArXiv, abs/2111.00396</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Longt5: Efficient textto-text transformer for long sequences</title>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Uthus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<idno>ArXiv, abs/2112.07916</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Diagonal state spaces are as effective as structured state spaces</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<idno>ArXiv, abs/2203.14343</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">GMAT: Global memory augmentation for transformers</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno>abs/2006.03274</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Jarrod</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">St?fan</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Picus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marten</forename><forename type="middle">H</forename><surname>Van Kerkwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Haldane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Fern?ndez Del R?o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>G?rard-Marchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hameer</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Gohlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-020-2649-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient attentions for long document summarization</title>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Parulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1419" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.74</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA</title>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1262</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2726" to="2736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The NarrativeQA reading comprehension challenge</title>
		<author>
			<persName><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?bor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00023</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ContractNLI: A dataset for document-level natural language inference for contracts</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Koreeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.164</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1907" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sylvain Gugger, Cl?ment Delangue, Th?o Matussi?re, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran?ois Lagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>?a?ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavitvya</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-demo.21</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linear unified nested attention</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Luna</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data Structures for Statistical Computing in Python</title>
		<author>
			<persName><forename type="first">Wes</forename><surname>Mckinney</surname></persName>
		</author>
		<idno type="DOI">10.25080/Majora-92bf1922-00a</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Python in Science Conference</title>
		<meeting>the 9th Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="56" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Long range language modeling via gated state spaces</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashok</forename><surname>Cutkosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno>ArXiv, abs/2206.13947</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Richard</forename><surname>Yuanzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelica</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishakh</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Quality: Question answering with long input texts, yes!</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.10683</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient contentbased sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00353</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Scrolls: Standardized comparison over long language sequences</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Haviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno>ArXiv, abs/2201.03533</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">2022a. Scaling laws vs model architectures: How does inductive bias influence scaling?</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<idno>ArXiv, abs/2207.10551</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Long range arena : A benchmark for efficient transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Efficient transformers: A survey</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">2022b. Unifying language learning paradigms</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3509134</idno>
		<idno>ArXiv, abs/2205.05131</idno>
		<imprint>
			<publisher>Pandas</publisher>
		</imprint>
	</monogr>
	<note>The pandas development team. 2020. pandasdev/pandas</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Pro-cessing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>St?fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Jarrod Millman</surname></persName>
		</author>
		<author>
			<persName><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?lhan</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ant?nio</forename><forename type="middle">H</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Pedregosa</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0686-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<publisher>Paul van Mulbregt</publisher>
		</imprint>
	</monogr>
	<note>and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Linformer: Selfattention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multipassage BERT: A globally normalized BERT model for open-domain question answering</title>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1599</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5878" to="5882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simple local attentions remain competitive for long-context tasks</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1975" to="1986" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Simple local attentions remain competitive for longcontext tasks</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Ouguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<idno>ArXiv, abs/2112.07210</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Modeling multi-hop question answering as single sequence prediction</title>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<idno>ArXiv, abs/2205.09226</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Nitish Shirish Keskar, and Caiming Xiong</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">QMSum: A new benchmark for query-based multi-domain meeting summarization</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mutethia</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.472</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5905" to="5921" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
