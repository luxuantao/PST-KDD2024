<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Highly Non-Rigid Object Tracking via Patch-based Dynamic Appearance Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Junseok</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Automation and Systems Research Institute</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<addrLine>599 Gwanak-ro, Gwanak-gu</addrLine>
									<postCode>151-744</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Automation and Systems Research Institute</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<addrLine>599 Gwanak-ro, Gwanak-gu</addrLine>
									<postCode>151-744</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Highly Non-Rigid Object Tracking via Patch-based Dynamic Appearance Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B2AD5A2F52EA4ED758E73A728477F073</idno>
					<idno type="DOI">10.1109/TPAMI.2013.32</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object Tracking</term>
					<term>Non-Rigid Object</term>
					<term>Local Patch-based Appearance Model</term>
					<term>Basin Hopping Sampling</term>
					<term>Markov Chain Monte Carlo</term>
					<term>Likelihood Landscape Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel tracking algorithm is proposed for targets with drastically changing geometric appearances over time. To track such objects, we develop a local patch-based appearance model and provide an efficient online updating scheme that adaptively changes the topology between patches. In the online update process, the robustness of each patch is determined by analyzing the likelihood landscape of the patch. Based on this robustness measure, the proposed method selects the best feature for each patch and modifies the patch by moving, deleting, or newly adding it over time. Moreover, a rough object segmentation result is integrated into the proposed appearance model to further enhance it. The proposed framework easily obtains segmentation results because the local patches in the model serve as good seeds for the semi-supervised segmentation task. To solve the complexity problem attributable to the large number of patches, the Basin Hopping (BH) sampling method is introduced into the tracking framework. The BH sampling method significantly reduces computational complexity, with the help of a deterministic local optimizer. Thus, the proposed appearance model could utilize a sufficient number of patches. The experimental results show that the present approach could track objects with drastically changing geometric appearance accurately and robustly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O BJECT tracking is one of the most important prob- lems in computer vision. Practically, it can be used in a large number of different applications including surveillance, intelligent robots, augmented reality, medical imaging, and so on. Recent research trends address challenging real-world tracking problems more than experiments in a simple lab-like environment <ref type="bibr" target="#b0">[1]</ref>. In real-world settings, objects are typically complex and difficult to track. To track an object robustly under difficult real-world settings, tracking algorithms need to consider the target object's appearance changes adaptively. Recently, numerous online learning algorithms have been proposed to address photometric appearance changes, and these algorithms have shown promising results <ref type="bibr" target="#b1">[2]</ref>[3][4] <ref type="bibr" target="#b4">[5]</ref>[6] <ref type="bibr" target="#b6">[7]</ref>[8] <ref type="bibr" target="#b8">[9]</ref>[10] <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b11">[12]</ref>. However, few studies focus on the geometric appearance changes in target objects. In this paper, we address the problem of tracking non-rigid objects, the geometric appearances of which change drastically over time. Although more generally applicable, the results in the present paper focus specifically on tracking the objects in scenes from movies and sports, which usually exhibit a large amount of extreme geometric appearance changes. Under aforementioned scenes, conventional tracking methods frequently fail to track the target object. Figure <ref type="figure" target="#fig_0">1</ref> shows a tracking example of such an object by the proposed method.</p><p>The philosophy of the proposed method lies in taking the advantages of both the histogram-based appearance <ref type="bibr" target="#b12">[13]</ref> <ref type="bibr" target="#b13">[14]</ref> and pixel-wise models <ref type="bibr" target="#b14">[15]</ref> <ref type="bibr" target="#b5">[6]</ref>. Note that the histogram-based appearance model covers geometric variations to some degree, but loses spatial information of target objects. On the other hand, the pixel-wise model preserves all spatial information, but typically fails to capture the extreme geometric changes of target objects. To cover the geometric changes without losing spatial information of target objects, we propose a local patch-based appearance model as in <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref> and present a new strategy for its online construction. The proposed local patch-based appearance model comprises a number of local patches and the topology between the patches. In the proposed model, the patch contains the local information of the target appearance. The topology exploits spatial relations of the patches. The model then preserves the spatial information of the target object using the topology between local patches. The model could also cover geometric variations well because the topology between local patches evolves through online update over time.</p><p>The present work has the following main contributions:</p><p>• A new local patch-based appearance model and its online update scheme for highly non-rigid objects are proposed. The appearance model comprises multiple local patches and the topology between those patches, which covers geometric changes while preserving spatial information of the target. The proposed model needs no specific object model and no training phase for learning the appearance or behavior of the object. Instead, the model evolves automatically through a novel update scheme, reflecting the photometric and geometric appearance changes of the target. A novel likelihood landscape analysis (LLA) is proposed for the update scheme and employed to measure the robustness of each patch. Using LLA, the robustness of a patch is measured by evaluating the degree of smoothness and steepness of the likelihood landscape of the patch.</p><p>• The adaptive Basin Hopping Monte Carlo (ABHMC)based tracking method <ref type="bibr" target="#b8">[9]</ref> is proposed to reduce the complexity in the proposed appearance model. The BH sampling method simplifies the landscape of a solution space by combining the Monte Carlo method with a deterministic local optimizer <ref type="bibr" target="#b18">[19]</ref>. In our tracking problem, the method gives an efficient way to reach the global optimum using a small number of samples, even in a huge solution space, that is associated with a large number of local patches. This method is extended to an adaptive version of ABHMC by adding an adaptive proposal density, which further improves the sampling efficiency.</p><p>• The ABHMC-based tracking method is further extended and ABHMC-F-and ABHMC-FS-based tracking methods are proposed. The ABHMC-based tracking method is designed to deal with geometric appearance changes in particular. This method is extended to cope with severe illumination conditions and scale changes as well. To accomplish the extension of the method, the appearance model is enhanced with multiple features, and the ABHMC-F-based tracking method is developed. In the enhanced appearance model, the local patches are constructed using different features, and a good feature for each patch is selected automatically. With adaptive feature selection, the method deals with local appearance changes of the target and tracks it robustly during local changes in the illumination conditions. The likelihood function is also improved using the rough segmentation results, and the ABHMC-FS-based tracking method is developed. Using the segmentation results, the redesigned likelihood function covers severe scale changes in the target. These extended works are described in detail in Sections 4.2 and 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The related works are grouped into five categories. Tracking methods with online appearance learning: By approximately estimating the pixel-wise color density in a sequential manner, Han et al. <ref type="bibr" target="#b5">[6]</ref> successfully track an object where lighting conditions, pose, scale, and view-point change over time. Ross et al. <ref type="bibr" target="#b9">[10]</ref> present an adaptive tracking method utilizing incremental principal component analysis. This adaptive tracking method shows robustness to large changes in pose, scale, and illumination. These two methods, however, do not consider extreme geometric changes of an object. The proposed method explicitly tackles these changes using a local patch-based online appearance model. Tracking methods for non-rigid objects: Schindler et al. <ref type="bibr" target="#b19">[20]</ref> represent an object as constellations of parts to track a bee accurately using the Rao-Blackwellized Particle Filter. This method focuses on fixing the topology of the constellation, whereas the proposed method evolves the topology via online updates. Ramanan et al. <ref type="bibr" target="#b20">[21]</ref> propose a tracking method operated by detecting the models of the target, the appearances of which should first be built. This method shows good results in tracking an articulated person. Using shape models of humans, Zhao et al. <ref type="bibr" target="#b21">[22]</ref> successfully track humans in crowded environments where occlusion occurs persistently. All these tracking methods, however, basically assume that specific models of the targets are given. By contrast, the proposed method utilizes no prior knowledge of the specific model for the target and no off-line training phase. Cehovin et al. <ref type="bibr" target="#b22">[23]</ref> combines local appearance with global appearance of the target using a novel coupled-layer visual model. Godec et al. <ref type="bibr" target="#b23">[24]</ref> employs a rough segmentation to describe global appearance of the target. The global appearance of these two methods help reduce noisy samples during the appearance updating process and thus help effectively prevent the trackers from drifting. By incorporating the global appearance, these two methods produced very accurate tracking results especially for the highly non-rigid objects. The proposed ABHMC-FS tracker also uses global appearance obtained from a rough segmentation. However, in contrast with aforementioned two methods, the ABHMC tracker includes the online feature selecting step. This enables that a different part of local appearance is described by a different feature. With the step, the ABHMC-FS tracker shows better tracking performance under the challenging tracking environments including illumination changes as well as severe deformation of the targets. Tracking methods using multiple patches: Adam et al. <ref type="bibr" target="#b24">[25]</ref> present a tracking method using multiple image fragments, where every fragment votes on the possible positions and scales of the object. By employing multiple fragments, the method is able to handle partial occlusions or pose changes efficiently. Nejhum et al. <ref type="bibr" target="#b25">[26]</ref> model the constantly changing foreground shape using multiple rectangular blocks, whose positions within the tracking window are adaptively determined. Using multiple rectangular blocks, the algorithm efficiently tracks articulated objects undergoing large variations in appearance and shape. Yang et al. <ref type="bibr" target="#b26">[27]</ref> proposed a novel attentional tracking method, which utilizes spatially attentional patches. The attentional patches include salient and discriminative regions of the targets. This method showed the robustness on a large variety of real-world video. Compared with these methods, the proposed local patch-based appearance model is more flexible, because This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</p><p>the patch may be removed, newly added, and moved by affine transformation and transition. Thus, the proposed method is able to track more severe non-rigid objects. Tracking methods using segmentation results: Chockalingam et al. <ref type="bibr" target="#b27">[28]</ref> represented the target by a Gaussian mixture model with multiple fragments and extracted accurate boundaries of the target using level sets. Then, the boundaries are used to learn the dynamic shape of the target over time. <ref type="bibr">Lu and Hager [29]</ref> treated the tracking problem as an online binary classification one using dynamic foreground/background appearance models. Using a temporal adaptive importance re-sampling procedure, they maintained temporally changing appearance model for both foreground and background. Theses two methods demonstrated the effectiveness of their approach on several challenging sequences. However, the methods did not consider geometric structure of the targets such as relations between patches or fragments. Compared with aforementioned methods, the proposed method covers the temporally changing geometric structure of the targets. Hence, the method robustly tracks the targets, of which geometric appearance severely change over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling-based tracking methods:</head><p>In tracking problems, the particle filter <ref type="bibr" target="#b29">[30]</ref> has shown efficiency in handling non-Gaussianity and multi-modality. The Markov Chain Monte Carlo (MCMC) method can be well applied to multi-object tracking problems because of its reduction of computational costs <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b31">[32]</ref>. When the dimension of a solution space increases, however, these methods still suffer from the problem of being trapped in deep local optima and handling a vast number of samples. The proposed method, based on the BH sampling method, solves these problems by combining a sampling method with a deterministic method and simplifying the landscape of a solution space. By doing this, our method can find a solution using smaller number of samples even in a very high-dimensional solution space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BAYESIAN OBJECT TRACKING APPROACH</head><p>The tracking problem can be interpreted as Bayesian filtering. Given the state at time t, X t and the observation up to time t, Y 1:t , the Bayesian filter updates the posteriori probability p(X t |Y 1:t ) with the following rule:</p><formula xml:id="formula_0">p(X t |Y 1:t ) ≈ p(Y t |X t ) p(X t |X t-1 )p(X t-1 |Y 1:t-1 )dX t-1 ,<label>(1)</label></formula><p>where p(Y t |X t ) is the observation model that measures the similarity between the observation at the estimated state and the given model, and; p(X t |X t-1 ) is the transition model that predicts the next state X t based on the previous state X t-1 . With the posteriori probability p(X t |Y 1:t ) computed by the observation and transition models, the Maximum a Posteriori (MAP) estimate over the N number of samples at each time t is obtained.</p><formula xml:id="formula_1">Xt = arg X (l) t max p(X (l) t |Y 1:t ) for l = 1, . . . , N,<label>(2)</label></formula><p>where X (l) t</p><p>represents the l-th sample of the object state X t , and Xt denotes the best sample of the object state, which explains the object configuration mostly well given the observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MCMC-based Tracking Method</head><p>The integration in (1) is not feasible in a large state space. To solve this problem, we utilize the Metropolis Hastings (MH) algorithm <ref type="bibr" target="#b32">[33]</ref>, a popular MCMC sampling method. The MH algorithm defines a single Markov chain and acquires samples over the chain. To obtain samples, two main steps are performed, namely, the proposal and acceptance steps. These two steps are iteratively executed until the number of iterations reaches a predefined value.</p><p>• Proposal Step: The proposal step proposes a new state given the previous state, based on some prior knowledge of the motion. The most commonly used prior knowledge of the motion is that the transition is governed by the Gaussian distribution. Thus, the proposal density is designed by: Q(X is accepted or not. This step can be calculated simply by the likelihood ratio a between the previous and new states:</p><formula xml:id="formula_2">(l+1) t ; X (l) t ) = G(X (l) t , σ 2 ),</formula><formula xml:id="formula_3">a = min 1, p(Yt|X (l+1) t )Q(X (l) t ;X (l+1) t ) p(Yt|X (l) t )Q(X (l+1) t ;X (l) t )</formula><p>, where p(Y t |X (l) t ) denotes the likelihood term over the state X (l) t , and Q(X (l+1) t ; X (l) t ) represents the proposal density. In the proposal and acceptance steps, the design of the state, X t is crucial to the success of MCMC-based tracking approaches because it significantly affects the performance of the MCMC sampling method. Ordinarily, the state at time t is represented as a threedimensional vector, X t = (x t , y t , s t ), where x t , y t , and s t indicate the xy center positions and scale of the target object, respectively. However, with conventional state representation, the tracking methods typically fail if the target is highly non-rigid because the representation cannot completely describe the geometric appearance of the target and cannot robustly cover its changes. To overcome these problems, the tracking methods require more advanced state representation, which describes the target's geometric appearance well. Thus, the proposed method presents a novel local patch-based appearance model in Section 4.</p><p>However, the local patch-based appearance model generates serious problems in conventional MCMCbased tracking approaches because the model should be represented as a very high-dimensional vectors. In the high-dimensional state space, the conventional MCMC method explained in Section 3.1 suffers from higher computational complexity because it requires an exponentially large number of samples to reach the global optimum. Additionally, the method becomes trapped in local optima more frequently because functions usually become rougher in a high-dimensional state space. To solve this problem, the proposed method presents the advanced MCMC method, the BH sampling method, to obtain samples efficiently, especially from the highdimensional state space in Section 6.</p><formula xml:id="formula_4">(a) Example of Xt = (xt, yt, st, X 1 t , X 2 t , X 3 t , X 4 t ) (b) Example of Rt = (R 1 t , R 2 t , R 3 t , R<label>4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN OF THE APPEARANCE MODEL</head><p>An object is represented by a local patch-based dynamic graph model as shown in Figure <ref type="figure" target="#fig_1">2</ref>. In the proposed model, the object state X t at time t is defined by</p><formula xml:id="formula_5">X t = (x t , y t , s t , X 1 t , • • • , X i t , • • • , X m t )</formula><p>where x t , y t , and s t denotes the xy center positions and scale of an object, respectively, X i t = (x i t , y i t ) indicates the center position of the i-th local patch, and m is the total number of local patches. Each local patch is assumed to be dependent only on the center of an object, such like the star model <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref>. The star model is frequently used because of its efficiency, which only considers the relation between center and each patch. As reported in <ref type="bibr" target="#b16">[17]</ref>, the star model has complexity O(NP ), while the fully connected model that considers all relations among patches has complexity O(N P ), where N and P denote the number of features and patches, respectively. Although the fully connected model may completely utilizes the geometric information of the target, the complexity of the model exponentially increase, as the number of patches in the model increases. The k-fan model <ref type="bibr" target="#b15">[16]</ref> has advantages of both the star and fully connected models. Although this model showed good performance for the recognition problem, the model did not produce good results for our tracking problem. The recognition problem typically considers a static scenario, in which the relations between patches undergo relatively small changes. On the other hand, the tracking problem should cover severe changes in relations between patches over time, especially if the target is highly non-rigid. To cover topology changes in the model both accurately and efficiently, the model should be designed with as simple relations between patches as possible, such like the star model, as addressed in <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref>. In our tracking problem, the star model produced more accurate results with lower complexity compared with the k-fan model.</p><p>Using the star model, the center position of the local patch, X i t , is determined by R i t , which represents the relative position between (x t , y t ) and X i t . In this manner, the topology of all local patches is constructed by</p><formula xml:id="formula_6">R t = (R 1 t , • • • , R i t , • • • , R m t ), as described in Fig- ure 2(b)</formula><p>. The objective of our tracking method is then finding the best sample of the object state, Xt =</p><formula xml:id="formula_7">(x t , ŷt , ŝt , X1 t , • • • , Xi t , • • • , Xm t )</formula><p>using the MAP estimation in <ref type="bibr" target="#b1">(2)</ref>, where the l-th state sample is represented by</p><formula xml:id="formula_8">X (l) t = (x (l) t , y (l) t , s (l) t , X 1(l) t , • • • , X i(l) t , • • • , X m(l) t</formula><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Photometric and Geometric Likelihoods</head><p>The likelihood is designed as:</p><formula xml:id="formula_9">p Y t |O(X (l) t ) ≈ m i=1 p p Y t |O(X i(l) t ) p g O(X i(l) t )|x (l) t , y (l) t , R i t ,<label>(3)</label></formula><p>where p p denotes the photometric likelihood and p g indicates the geometric likelihood. In (3), O(X i(l) t ) returns the state vector of the local mode of the i-th patch centered on X i(l) t , which indicates the locally best one among the states around X i(l) t . To obtain the state of the local mode, the proposed method utilizes the image registration algorithm in <ref type="bibr" target="#b18">[19]</ref>. In the image registration process, the ith local patch is warped via affine transformation so that it best matches to the model image of the i-th patch, M i t at time t.</p><p>The photometric likelihood is then defined as:</p><formula xml:id="formula_10">p p Y t |O(X i(l) t ) = exp -λpF1 I O(X i(l) t ) ,M i t ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_11">I[O(X i(l)</formula><p>t )] indicates the patch image described by O(X i(l) t ), the F 1 function returns the normalized sum of squared differences between the patch at the state of the local mode and its model image, and λ p denotes the weighting parameter set to 30. The i-th patch model M i t in (4) is updated by</p><formula xml:id="formula_12">M i t+1 = (1 -ω)M i(ref ) + ωM i(dyn) t ,<label>(5)</label></formula><p>where M i(ref ) indicates the i-th reference local patch model in the initial frame and M i(dyn) t</p><p>represents the model image obtained in the region of O( Xi t ) at time t. The local patch model in the initial frame, M i(ref ) , prevents from learning drastic appearance changes <ref type="bibr" target="#b33">[34]</ref>.</p><p>The geometric likelihood is defined by</p><formula xml:id="formula_13">p g O(X i(l) t )|x (l) t , y (l) t , R i t = exp -λg O(X i(l) t )-(x (l) t ,y (l) t ) -R i t 2 , (6) where [O(X i(l) t ) -(x (l) t , y (l) t )] -R i t 2 returns the 2-norm distance between two vectors [O(X i(l) t ) -(x (l) t , y (l)</formula><p>t )] and R i t , and λ g denotes the weighting parameter set to 1. In <ref type="bibr" target="#b5">(6)</ref></p><formula xml:id="formula_14">, [O(X i(l) t ) -(x (l) t , y (l)</formula><p>t )] is the relative position of the local mode of the proposed i-th local patch with respect to the center of an object. R i t is the reference position of the i-th local patch with respect to the center of an object, which is updated by</p><formula xml:id="formula_15">R i t+1 = (1 -ω)R i t + ω O( Xi t ) -(x t , ŷt ) ,<label>(7)</label></formula><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. where Xi t , xt , and ŷt denote the MAP states for X i t , x t , and y t , respectively. Note that this updating process is for unmodified local patches. For modified local patches, M i t+1 and R i t+1 are already determined in the modification process explained in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Advanced Likelihood with Rough Segmentation</head><p>The local patch-based appearance model enables the proposed method to employ a segmentation technique very easily. As aforementioned, the model automatically produces several foreground patches over time. These patches could be good seeds for the segmentation algorithms. Figure <ref type="figure" target="#fig_2">3</ref>(a) displays the construction of background patches. Since the number of background patches around the bounding box is large, our background model is similar to a conventional rectangular band. With the advantage of the conventional rectangular band, our background model also has good property of preserving the spatial information of background, while the rectangular band loses the spatial information of background.</p><p>With the foreground and background patches, the proposed method segments the target using the randomwalk algorithm in <ref type="bibr" target="#b34">[35]</ref>, as described in Figure <ref type="figure" target="#fig_2">3(b)</ref>. This algorithm finds out the target region probabilistically with a set of foreground patches and a set of background patches. Because the segmentation results are generative, the results are well integrated into our sampling based Bayesian tracking framework. The likelihood function is then enhanced by measuring the additional likelihood value on the segmented region,</p><formula xml:id="formula_16">p s (Y t |S[O(X (l) t )]): p Y t |O(X (l) t ) ≈ p s Y t |S O(X (l) t ) × m i=1 p p Y t |O(X i(l) t ) p s O(X i(l) t )|x (l) t , y (l) t , R i t , p s Y t |S O(X (l) t ) = exp -λsF2 I S O(X i(l) t ) ,Mt ,<label>(8)</label></formula><p>where S[O(X the image patch inside the bounding box B, which is defined by</p><formula xml:id="formula_17">B w = max {x i t-1 } m i=1 -min {x i t-1 } m i=1 , B h = max {ŷ i t-1 } m i=1 -min {ŷ i t-1 } m i=1 , B c = min {x i t-1 } m i=1 + B w 2 , min {ŷ i t-1 } m i=1 + B h 2 ,<label>(9)</label></formula><p>where B w , B h , and B c denote the width, height, and center of the imaginary bounding box B constructed by all local patches { Xi t-1 } m i=1 , respectively, and; xi t-1 and ŷi t-1 indicate the x and y positions of the MAP state of the i-th local patch at time t -1, respectively. In <ref type="bibr" target="#b7">(8)</ref>, our method uses the segmented region to compute the histogram. The function F 2 returns Bhattacharyya similarity <ref type="bibr" target="#b13">[14]</ref> between HSV histogram of the segmented region S[O(X (l) t )] and M t . Using (8), our method could cover severe scale changes in the target. Notably, the segmentation method reconstructs as much regions of the target as possible and provides the global information of the target appearance. Hence, the new likelihood function considers missed regions of the target while measuring the likelihood score, where the missed regions are typically made due to severe scale changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">UPDATE OF THE APPEARANCE MODEL</head><p>In this process, the local patches in our appearance model are newly added, deleted or moved to a different position via online update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Initialization of Patches</head><p>The initial positions of patches have to be chosen carefully for image alignment. Thus, the condition number K of the Hessian Matrix H is used for measuring the goodness of a patch.</p><formula xml:id="formula_18">K = σ max (H) σ min (H) ,<label>(10)</label></formula><p>where σ max (H) and σ min (H) denote the maximal and minimal singular values of H respectively. In <ref type="bibr" target="#b9">(10)</ref>, the small K means that the matrix is numerically stable<ref type="foot" target="#foot_1">1</ref> .</p><p>To initialize the patches, a bounding box around the target is drawn manually, and the center of the first local patch within the bounding box is chosen as the point with the least K value. The size of the patch is determined randomly. The second patch is chosen as the point with the next least K value. Hence, this patch does not overlap with the existing local patches. The procedure is repeated and terminated only when there is no space to make local patches or the number of local patches reaches a predefined value. Figure <ref type="figure" target="#fig_3">4</ref> shows the patch initialization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Examination of Patches by LLA</head><p>When the landscape of the local mode (LLM) of each patch has good properties, the proposed appearance model reliably estimates the likelihood in <ref type="bibr" target="#b7">(8)</ref>, which is important for the success of tracking. Smoothness and steepness are used to characterize LLM. Smooth (rough) LLM means that local modes are gathered (scattered) in a narrow (wide) region of a solution space, whereas steep (gradual) LLM indicates that these local modes have a steep (gradual) shape. Both smooth and steep LLM guarantee that there is a very strong local mode for the patch. Figure <ref type="figure" target="#fig_4">5</ref> shows examples of different LLM types.</p><p>To measure these properties quantitatively, a new method of measurement inspired by <ref type="bibr" target="#b5">[6]</ref> is designed. The degree of smoothness D sm of the i-th local patch is measured as the variance on the positions of the local modes of the patch:</p><formula xml:id="formula_19">D sm (i) = 1 1 N N l=1 O(X i(l) t ) -1 N N l =1 O(X i(l ) t ) 2 2 ,<label>(11)</label></formula><p>where O(•) finds local modes for the N number of samples of the i-th local patch. The degree of steepness D st of the i-th local patch is measured by the mean distance between the positions of samples and of local modes:</p><formula xml:id="formula_20">D st (i) = 1 1 N N l=1 O(X i(l) t ) -(x i(l) t , y i(l) t ) 2 . (<label>12</label></formula><formula xml:id="formula_21">)</formula><p>With the new methods of measurement in <ref type="bibr" target="#b10">(11)</ref> and ( <ref type="formula" target="#formula_20">12</ref>), the status of local modes, such as that in Table <ref type="table" target="#tab_1">1</ref>, could be determined. Figures <ref type="figure">6(a</ref>) and 6(b) respectively show the qualitative and quantitative results of LLM for 4 different cases in diving seq. In case 1, the proposed method can track the patch robustly because there is no ambiguous region around it, whose appearance is similar to that of the patch. However, the method frequently fails to track the patches in cases 2,3, and 4 because of background clutter, similar textures, and homogeneous regions around the patches. Therefore, these patches  should be modified. This modification is explained in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Online Feature Selection via LLA</head><p>In the real-world tracking environment, the photometric appearance of a target object also changes severely because of varying illumination conditions, as described in Figure <ref type="figure">7</ref>(a). To track the target robustly in this environment, the proposed method uses locally different features to describe the target, as shown in Figure <ref type="figure">7(b)</ref>. Thus, some portions of the target are expressed as a feature and other portions as other features. This can be done efficiently by allowing the local patches to have different features. Note that a different feature makes a different LLM. Therefore, the proposed method can choose a robust feature for each patch by analyzing the LLM of the patch. If the chosen feature describes the target's current appearance well, the corresponding LLM of the patch should be smooth and steep. Otherwise the LLM is rough and gradual, as illustrated in Figure <ref type="figure">7(c)</ref>. With this robust feature, the proposed method can cope with local appearance changes of the target efficiently and track it robustly, even with local illumination changes.</p><p>The improvement of the LLM of the patches by selecting features in the proposed method can be measured by</p><formula xml:id="formula_22">S LLM = 1 m m i=1 D sm (i) + 1 m m i=1 D st (i), where D sm (i)</formula><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. in <ref type="bibr" target="#b10">(11)</ref> and D st (i) in ( <ref type="formula" target="#formula_20">12</ref>) return the degree of smoothness and steepness of the i-th local patch, respectively. In Figure <ref type="figure">7</ref>, S LLM increases from 7.14 to 61.96 by adaptively selecting the Hue, Saturation, and Value features, compared with the Value feature only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Online Modification of Patches</head><p>According to the results of the likelihood landscape analysis in Sections 5.  <ref type="figure" target="#fig_2">3(a)</ref>. The second criterion is formulated by Xi t -X j t 2 ≥ θ C2 , for ∀j and j = i.</p><p>When the above two criteria are satisfied, modifications are performed by adding new patches, or by deleting or moving bad patches. First, the proposed method makes several attempts to find a patch that satisfies the above criteria, whereby a bad patch is moved via the Gaussian perturbation centered on the current position. If the moved patch satisfies the above criteria within the predefined number of iterations, the bad patch is replaced with the moved patch. If not, the bad patch is deleted. Second, a new patch that satisfies the aforementioned criteria is added by choosing a new position for the patch using the condition number explained in Section 5.1. If the new patch satisfies the aforementioned criteria within half the number of predefined iterations, the new patch is added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">INFERENCE VIA THE ABHMC SAMPLING</head><p>The solution space generally becomes larger as the number of local patches in our appearance model increases. Thus, the conventional MCMC method is inefficient for computing the integration in (1). Therefore the BH sampling method <ref type="bibr" target="#b35">[36]</ref> is introduced in the tracking problem to provide a better performance in such highdimensional solution spaces. The BH sampling method consists of two main steps, similar to the conventional MCMC method, namely, the proposal and acceptance steps.</p><p>• Proposal Step: For the proposal step, three different proposal densities are used, namely, Q 1 , Q 2 , and Q 3 , as illustrated in Figure <ref type="figure">8</ref>. The proposal density Q 1 is used once at the start of each frame to connect the previous frame to the current one. In Q 1 , the center of an object is assumed to be near the centroid formed by all local patches. And the scale of the object should be determined; thus, the bounding box contains all local patches compactly. With these assumptions, the proposal density Q 1 is designed, whose proposal variance changes</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</p><p>adaptively according to the states of local patches. Then the adaptive proposal is</p><formula xml:id="formula_23">Q 1 (x (1) t ; xt-1 ) = G(x t-1 , σ 2 x ) + γ x δ x , Q 1 (y (1) t ; ŷt-1 ) = G(ŷ t-1 , σ 2 y ) + γ y δ y , Q 1 (s (1) t ŝt-1 ) = G(ŝ t-1 , σ 2 s ) + γ s δ s ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_24">Q 1 (x<label>(1)</label></formula><p>t ; xt-1 ) and Q 1 (y</p><p>t ; ŷt-1 ) indicate that the first sample of the object center (x</p><formula xml:id="formula_26">(1) t , y<label>(1)</label></formula><p>t ) at the current frame is proposed based on the MAP state of the object center (x t-1 , ŷt-1 ) at the previous frame. In <ref type="bibr" target="#b12">(13)</ref>, δ x , δ y , and δ s denote the adapting constant set to 0.3, 0.3, and 0.01, respectively, and γ x , γ y , and γ s represent the adapting parameters defined by</p><formula xml:id="formula_27">γ x = ⎧ ⎨ ⎩ γ x + 1 if xt-1 1 m m i=1 xi t-1 γ x -1 if xt-1 1 m m i=1 xi t-1 γ x otherwise . , γ y = ⎧ ⎨ ⎩ γ y + 1 if ŷt-1 1 m m i=1 ŷi t-1 γ y -1 if ŷt-1 1 m m i=1 ŷi t-1 γ y otherwise . , γ s = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ γ s + 1 if ŝt-1 √ B 2 w +B 2 h B0 γ s -1 if ŝt-1 √ B 2 w +B 2 h B0 γ s otherwise . ,<label>(14)</label></formula><p>where B w and B h respectively denote the width and height of the bounding box B defined by ( <ref type="formula" target="#formula_17">9</ref>) and B 0 denotes the initial diagonal size of the bounding box of the target. In <ref type="bibr" target="#b13">(14)</ref> </p><formula xml:id="formula_28">Q 2 (x (l+1) t ; x (l) t ) = G(x (l) t , σ 2 x ), Q 2 (l+1) t ; y (l) t ) = G(y (l) t , σ 2 y ), Q 2 (s (l+1) t ; s (l) t ) = G(s (l) t , σ 2 s ),<label>(15)</label></formula><p>where G denotes the Gaussian distribution with mean x</p><formula xml:id="formula_29">(l) t ,y<label>(l)</label></formula><p>t , and s (l)</p><p>t and; variance σ<ref type="foot" target="#foot_2">2</ref> x ,σ 2 y , and σ 2 s to propose the new center and scale, respectively.</p><p>The center position of each local patch is determined using the proposal density Q 3 :</p><formula xml:id="formula_30">Q 3 (X i(l+1) t ; R i t ) = (x (l+1) t , y<label>(l+1) t</label></formula><p>)+R i t , for i = 1, . . . , m,</p><formula xml:id="formula_31">X i(l+1) t<label>(16) where</label></formula><p>is a new sample of the center position for the i-th local patch, (x</p><formula xml:id="formula_32">(l+1) t , y (l+1) t</formula><p>) is a new sample of the object center obtained by <ref type="bibr" target="#b14">(15)</ref>, and R i t is the parameter estimated by <ref type="bibr" target="#b6">(7)</ref>.</p><p>• Acceptance Step: Most performance in the BH sampling method comes from the novel acceptance step. The primary difference between the conventional acceptance ratio in Section 3.1 and that of the BH sampling method is that the acceptance ratio of the BH sampling method Algorithm 1 ABHMC-FS</p><formula xml:id="formula_33">Input: Xt-1 = (xt-1, yt-1, st-1, X 1 t-1 , • • • , X i t-1 , • • • , X m t-1 ) Output: Xt = (xt, ŷt, ŝt, X1 t , • • • , Xi t , • • • , Xm t )</formula><p>1: Initialize patches using <ref type="bibr" target="#b9">(10)</ref> in an initial frame. 2: Propose x (1)   t ,y</p><p>(1) t and s</p><p>(1) t using Q1 in (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3: for</head><formula xml:id="formula_34">l = 1 to N -1 do 4: Propose x (l+1) t , y<label>(l+1) t</label></formula><p>, and s (l+1) t using Q2 in (15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Determine X i(l+1) t for all i using Q3 in (16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Obtain S(X (l+1) t</p><p>) with the process described in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Calculate the likelihood score using (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Accept X (l+1) t with probability (17). 9: end for 10: Estimate the MAP state Xt using (2). 11: Select patches to be modified using <ref type="bibr" target="#b10">(11)</ref> and <ref type="bibr" target="#b11">(12)</ref>. 12: Choose features of the patches using the method described in Section 5.3. 13: Modify the patches using the criterion 1 and 2 introduced in Section 5.4. 14: Obtain updated parameters M i t+1 and R i t+1 using ( <ref type="formula" target="#formula_12">5</ref>) and <ref type="bibr" target="#b6">(7)</ref>.</p><p>is calculated by the likelihood ratio at the local mode of the state. Thus, the acceptance ratio is defined by</p><formula xml:id="formula_35">a = min ⎡ ⎣ 1, p Y t |O(X (l+1) t ) Q(X (l) t ; X (l+1) t ) p Y t |O(X (l) t ) Q X (l+1) t ; X (l) t ) ⎤ ⎦ ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_36">Q(X (l+1) t ; X (l)</formula><p>t ) represents the proposal density defined in (13)(15) <ref type="bibr" target="#b15">(16)</ref>, and p(Y t |O(X (l) t )) in ( <ref type="formula" target="#formula_16">8</ref>) returns the likelihood value at the state of the local mode. The state of the local mode is easily found by the modeseeking method such as the Lucas-Kanade image registration method <ref type="bibr" target="#b18">[19]</ref>, as shown in Figure <ref type="figure" target="#fig_8">9(a)</ref>.</p><p>The BH sampling method transforms the rough likelihood landscape of the original solution space into a simpler one using robust local optimization techniques in the sampling process, as depicted in Figure <ref type="figure" target="#fig_8">9</ref>(b). In a new transformed landscape, the minima of the original landscape are no longer of concern in the sampling process. Hence, a greater chance exists for reaching the global optimum with a smaller number of samples. In all experiments, 20 samples are sufficient to obtain the MAP estimate. Figure <ref type="figure" target="#fig_8">9</ref>(c) illustrates the advantage of our simplified landscape.</p><p>The proposed method robustly tracks a highly nonrigid target with the advanced appearance model using the topology between local patches, new onlineupdating scheme using the likelihood landscape analysis, and the efficient inference method using the Basin Hopping sampling. Algorithm 1 illustrates the whole process of our tracking method, including local patchbased dynamic appearance modeling and adaptive Basin Hopping Monte Carlo sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTAL RESULTS</head><p>For the experiments, 17 video sequences 2 were tested, namely, snowboard, diving, high-jump, transformer, and gymnastic sequences in <ref type="bibr" target="#b8">[9]</ref>; femaleskater, maleskater, indiandancer, and dancer sequences in <ref type="bibr" target="#b25">[26]</ref>; dinosaur and hand2 sequences in <ref type="bibr" target="#b22">[23]</ref>; motocross2 and skiing sequences in <ref type="bibr" target="#b23">[24]</ref>; coke, girl, tiger1, and tiger2, sequences in <ref type="bibr" target="#b2">[3]</ref>[37] <ref type="bibr" target="#b37">[38]</ref>. The proposed algorithms (ABHMC, ABHMC-F, ABHMC-FS) are compared with 8 different state-of-the-art tracking methods, namely, Mean-Shift tracker (MS) based on <ref type="bibr" target="#b12">[13]</ref>[39], Standard MCMC (MCMC) based on <ref type="bibr" target="#b30">[31]</ref>, Incremental learning for Visual Tracking (IVT) in <ref type="bibr" target="#b9">[10]</ref>, Fragment-based tracker (FRAGT) in <ref type="bibr" target="#b24">[25]</ref>, Block Histogram-based Tracker (BHT) in <ref type="bibr" target="#b25">[26]</ref>, Multiple Instance Learning tracker (MIL) in <ref type="bibr" target="#b2">[3]</ref>, Local Global Tracker (LGT) in <ref type="bibr" target="#b22">[23]</ref>, and Hough-based Tracker (HT) in <ref type="bibr" target="#b23">[24]</ref>.</p><p>ABHMC denotes our original method in <ref type="bibr" target="#b8">[9]</ref>. ABHMC-F denotes the improved version of the ABHMC with adaptive feature selection. ABHMC-FS is the final version, which utilizes rough segmentation results. All parameters of the proposed method are fixed for all experiments. λ p in (4), λ g in ( <ref type="formula">6</ref>), ω in (5) <ref type="bibr" target="#b6">(7)</ref>, λ s in (8), θ sm , θ st in Table <ref type="table" target="#tab_1">1</ref>, θ C1 , θ C1 in Section 5.4, and M are set 30, 1.0, 0.5, 5.0, 1.0, 0.25, 2.5, 5.0, and 50, respectively. Although the parameters were determined empirically, the proposed methods were not sensitive to all these parameters.</p><p>MCMC uses an HSV color histogram for the appearance model as in <ref type="bibr" target="#b13">[14]</ref>, while dividing an object into the upper and lower body. Proposal variances of the MCMC are set to 8 and 4 for the x and y directions, respectively. The parameters of other trackers are adjusted to produce the best tracking performances. Note we used the codes of the other methods provided by the authors in <ref type="bibr" target="#b2">[3]</ref>[10] <ref type="bibr">[23][24]</ref>[25] <ref type="bibr" target="#b25">[26]</ref>. We obtained the ground truths of the publicly available datasets from the corresponding authors, and constructed those of our datasets manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Efficiency of the Proposed Appearance Model</head><p>• Qualitative Performance: Figure <ref type="figure" target="#fig_9">10</ref> presents the qualitative performance of the dynamic appearance modeling scheme in diving seq. This sequence includes severe geometric changes of the target appearance as shown in Figure <ref type="figure" target="#fig_9">10(d)</ref>, which depicts the angle and distance changes of the head position with respect to the center position of the target object over time. Even under severe geometric changes of the object appearance, our method successfully tracked the object. The first row of Figure <ref type="figure" target="#fig_9">10</ref> illustrates our constructed appearance models where blue squares denote unmodified local patches and green Step  <ref type="figure" target="#fig_9">10</ref> shows the robustness of our on-line appearance model. In Figures <ref type="figure" target="#fig_9">10(a)</ref>, some local patches in the background region are removed from the next frame because their local modes had very rough landscapes and did not satisfy the two modification criteria in Section 5.4. Additionally, the proposed algorithms dealt with the occlusion of the object by deleting the occluded patches adaptively. As shown in Figures <ref type="figure" target="#fig_9">10(c</ref>), the algorithm reduced the number of local patches from 15 to 4.</p><p>• Quantitative Performance: To evaluate the performance of the dynamic appearance modeling scheme qualitatively, the number of modified and unmodified local patches in each frame were verified. As illustrated in Figure <ref type="figure" target="#fig_0">11</ref>(a), our appearance model actively moves, deletes, or adds patches based on the likelihood landscape analysis at each frame. This means that the topology between the local patches in the model evolves as time goes on. Figure <ref type="figure" target="#fig_0">11(b)</ref> shows that the proposed appearance model adaptively modifies the position and number of patches, particularly when geometric appearance of the object is drastically changing. The proposed methods successfully capture the movements of the head, legs, and arms without a specific model for the target object. We further evaluated the efficiency of the dynamic appearance modeling scheme using the likelihood landscape analysis. Table <ref type="table">2</ref> demonstrates how our dynamic appearance modeling scheme enhances the LLMs of the patches in the appearance model. As aforementioned in Section 5.3, the robustness of the proposed appearance model can be measured as S LLM . For example, the model is evaluated as a larger value S LLM if it is composed of good local patches with smoother and This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.  <ref type="bibr" target="#b9">[10]</ref> (a) Acceptance rate is defined by the number of accepted samples over the total number of samples in each frame. (b) To derive τ int , method used 500 samples. AIS denotes the Annealed Importance Sampling method in <ref type="bibr" target="#b39">[40]</ref>.</p><p>steeper LLMs. To obtain good local patches, our dynamic appearance modeling scheme adopts the online update step in Section 5.4 and the feature selection step in Section 5.3. After the online update and feature selection steps, the LLMs of the patches in the appearance model have become very smooth and steep, indicating that the model discriminates the object from the background well. Therefore, with this model, our trackers tracked the object robustly despite severe photometric and geometric appearance changes. Instead of color features, other features such as Gabor filters could be utilized in the C step. However, the Gabor filters did not provide good LLMs, rather making them very rough and gradual as demonstrated the C* step in Table <ref type="table">2</ref>. This means that the sampling method using Gabor filters need very long time to find the global optimum as comparison with the method using color features. Hence, with the limited number of samples, the sampling method using Gabor filters has difficulty in reaching the global optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Efficiency of the Proposed ABHMC Sampling</head><p>• Property of Sampling Strategy: To evaluate the performance of the ABHMC sampling more analytically and qualitatively, it was compared with the standard MCMCbased tracking algorithm <ref type="bibr" target="#b30">[31]</ref>. In this experiment, the test video only contained a rigid object for fair comparison. An equal number of samples, same appearance model, and same transition model for both methods were used. Note that one particularly good property of the ABHMC sampling is that it can easily jump over a deep basin by transforming a likelihood landscape into a simpler one. Thus, the depth of basins are lowered, and the ABHMC sampling can frequently accept the proposed samples. As shown in Figure <ref type="figure" target="#fig_11">12</ref>(a), our tracking algorithm had higher acceptance rates than the standard MCMC method. This means that the proposed methods easily escape from the local optima and obtains more diverse samples.</p><p>Autocorrelation time measures the degree of statistical independence between samples <ref type="bibr" target="#b40">[41]</ref>. This independence property is important in reducing the statistical error. If the samples are highly correlated, the statistical error does not decrease at the rate of the square root of the number of samples. Figure <ref type="figure" target="#fig_11">12(b)</ref> illustrates the integrated autocorrelation time τ int , where τ int of the ABHMC sampling was smaller than that of the MCMC sampling and the AIS (Annealed Importance Sampling) <ref type="bibr" target="#b39">[40]</ref>. This finding suggests that the ABHMC sampling method produces highly uncorrelated samples, which sufficiently minimizes the statistical error of the MAP estimate in (2).</p><p>• Efficiency of Sampling Strategy: The appearance model generally consists of 20 to 50 local patches, indicating a very large solution space. The proposed methods, however, use a very small number of samples, 20, in all experiments for tracking an object. This performance typically benefits from the ABHMC sampling. Figure <ref type="figure" target="#fig_12">13</ref> quantitatively demonstrates that the ABHMC sampling requires a smaller number of samples, compared with the MCMC sampling and the AIS method, to reach a similar tracking performance. For example, the ABHMC sampling needed only 5 samples to obtain the tracking accuracy of 0.7 in Gymnastics seq., whereas the MCMC needed more than 100 samples. Additionally, the figure shows that the ABHMC sampling maintains the best performance even with a drastically small number of samples. In all sequences, the ABHMC sampling produced the most accurate results, regardless of the number of samples. The main advantage of the ABHMC sampling is that it combines the stochastic method with the deterministic method. Hence, it has good properties from both the stochastic and deterministic methods. Using the deterministic method, the ABHMC sampling quickly finds several local minima and thus needs only a small number of samples to get the solution. The stochastic method prevents the method from getting trapped in a certain local minimum. On the other hand, the MCMC sampling and the AIS method require an enough number of samples to obtain a good solution because they only employ the stochastic process.</p><p>In the theoretical aspect, the simulated annealing method and its variants such as the AIS method suffers from the notorious "freezing" problem, as reported in <ref type="bibr" target="#b41">[42]</ref>. The freezing problem occurs because the escape rate from local minima diverges with decreasing temperature. The ABHMC sampling ameliorates this problem and simplifies the original likelihood landscape by replacing the likelihood of each conformation with the likelihood of a nearby local minima. This replacement eliminates high likelihood barriers in the stochastic search that are responsible for the freezing problem in simulated annealing, as reported in <ref type="bibr" target="#b42">[43]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Accuracy of the Proposed Tracking Methods</head><p>• Effect of Parameter Settings: Tracking accuracy could be dependent on several parameters. Thus the robustness of ABHMC-FS is evaluated to the variation of the parameters. As shown in Figures <ref type="figure" target="#fig_13">14(a</ref>)-(i), our method was not severely sensitive to all these parameters, although the tracking accuracy slightly decreased if the parameter values extremely increase. ABHMC-FS made our original ABHMC less sensitive to the parameter settings by employing the global appearance model. Figure <ref type="figure" target="#fig_13">14</ref>(a) illustrates the tracking accuracy as the maximum number of local patches increases. If this value is increased, the local patches can cover a larger portion of the object region. However, the risk that the local patches may cover background regions also increases. Figure <ref type="figure" target="#fig_13">14</ref>(b) shows the tracking accuracy as the parameter θ C1 increases. The parameter θ C1 is the threshold value of likelihood over f oreground likelihood over background , which is described in Section 5.4. If θ C1 is increased, the method reduces the chances of creating a false-positive of the local patches. However, the chances of creating a true-positive are also reduced. Figure <ref type="figure" target="#fig_13">14</ref>(c) describes the tracking accuracy as the parameter θ C2 increases. The parameter θ C2 is the threshold value of the distance between neighbor patches, which is also described in Section 5.4. If θ C2 increases, the ability to explore a missed object region is improved. However, the ability of exploiting the fine object region is decreased. Figures <ref type="figure" target="#fig_13">14(d</ref>)-(f) show the tracking accuracy as likelihood parameters, λ p , λ g , and λ s , increase. The parameters, λ p , λ g , and λ s adjust the weights of photometric, geometric, and segmentation factors in the likelihood function, respectively. The likelihood models have the different weights by assigning different values to λ p , λ g , and λ s in (3) and <ref type="bibr" target="#b7">(8)</ref>. Although these weights can be made adaptive to the tracking environment, we set them fixed to simplify our algorithm. This is feasible However, the method has difficulty in handling gradual appearance changes of the target. Figure <ref type="figure" target="#fig_13">14</ref>(i) describes the tracking accuracy as the appearance updating parameter ω increases. The parameter ω adjusts weights of old and new appearances of the target to construct the appearance model. If ω increases, the appearance model is constructed by reflecting the current appearance more rather than the old one. Hence, the tracking method accurately tracks the target although the appearance frequently changes. However, the constructed appearance model can be easily corrupted because the current appearance may contain erroneous tracking results.</p><p>• Comparison among the proposed methods: Using multiple features (ABHMC-F) and additional segmentation results (ABHMC-FS), we enhanced the performance of ABHMC, as shown in Table <ref type="table" target="#tab_4">3</ref>. ABHMC-FS always outperformed the ABHMC and ABHMC-F. Note that the gray test sequences, including dancer, femaleskater, indiandancer, and maleskater do not provide multiple (color) features. Hence, the results of ABHMC-F for these sequences are unavailable. As shown in Table <ref type="table" target="#tab_4">3</ref>, most important steps which contribute to the final tracking performance are the online updating step and the segmentation step. These results demonstrate that our original ABHMC proposed in <ref type="bibr" target="#b8">[9]</ref> is robust because it includes the online updating step and   <ref type="table" target="#tab_4">3</ref>) is real time. It approximately takes 0.1 seconds per frame. In spite of such computational overhead, the segmentation procedure in ABHMC-FS is very useful because it greatly improves the tracking accuracy.</p><p>• Comparison with other tracking methods: Table <ref type="table" target="#tab_5">4</ref> summarizes the tracking results of nine different test sequences that include objects whose geometric appearances are changing drastically over time. ABHMC-FS most accurately tracked the objects with the fixed parameters. The LGT and HT trackers also robustly tracked the targets and showed the second-best performance, where they are most recent tracking methods especially for highly non-rigid objects. However, these trackers were weak to severe illumination changes in snowboard and dinosaur sequences. BHT and FRAGT showed the good tracking performance. Both, however, were weak when the geometric appearance changes were more severe. Compared with these methods, ABHMC-FS produced accurate tracking results even though there were illumination changes and deformation of targets at the same time. With the online-feature selection process, ABHMC-FS constructed a good appearance model using the selected local patches, which was robust to both the illumination changes and the deformation of targets. With the segmentation process, ABHMC-FS employed global appearance information and prevented local patches from drifting into the background. As comparison with the adaptive color-based tracking algorithms like the MS and MCMC trackers, ABHMC-FS produced better tracking results since the non-rigid object severely changed its color appearance caused by deformation, while the MS and MCMC trackers suffered from drastic color changes. ABHMC-FS solved this problem by considering geometry appearance as well as color appearance of the object using the local patch-based appearance modeling. In addition, ABHMC-FS successfully tracked the targets in benchmark datasets such as coke, girl, tiger1, and tiger2. Note that ABHMC-FS showed small standard deviation, which means that the method produced stable tracking results. The main reason of small standard deviation is that ABHMC-FS partly utilizes the deterministic method, which produces zero standard deviation.</p><p>In Figures <ref type="figure" target="#fig_4">15(a</ref>) to 15(c), videos that include background clutter similar to the object were tested. In the case of other methods, trajectories were easily hijacked by the background clutter with colors similar to those of the object, when the object changes its geometric appearance. On the other hand, our ABHMC-FS robustly tracked the object despite the background clutter and geometric appearance changes. Figures <ref type="figure" target="#fig_4">15(d</ref>) and 15(e) demonstrate how the proposed method outperformed conventional tracking algorithms during drastic geometric appearance changes. The conventional tracking algorithms failed to track the object when the positions of the head and legs were reversed. Figure <ref type="figure" target="#fig_4">15(f)</ref> shows that the specific model of the object occasionally cannot capture the drastic geometric changes of the object. The proposed method also tracked thin parts of the object (e.g. arms or legs) and covered the greater parts of the object area, whereas other methods failed to track such objects accurately, as shown in Figures <ref type="figure" target="#fig_4">15(g</ref>  F, and ABHMC-FS). The improvement in tracking performance was significant, especially in snowboard seq., which includes severe illumination and scale changes. In the sequence, the ABHMC-F efficiently dealt with the illumination change using multiple features, and ABHMC-FS robustly handled the scale change with the segmentation results.</p><p>Figures <ref type="figure" target="#fig_0">16(a</ref>)-(l) show the tracking results of the proposed ABHMC-FS when only intensity values of gray sequences are available. In this case, the method neither uses color features nor chooses robust ones. The method, however, robustly tracked the object in these sequences as well, while our appearance model well described the geometric appearance of the object using local patches and their local modes. Figures <ref type="figure" target="#fig_0">17</ref> and<ref type="figure" target="#fig_0">18</ref> demonstrates that ABHMC-FS accurately tracks the targets in the recent challenging datasets including highly non-rigid objects and in the benchmark datasets including pose variations and occlusions of the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we have proposed an novel tracking algorithm evolving a local patch-based appearance model by the analysis of LLM (Landscape of the Local Mode). With the model, the algorithm robustly tracked the object whose geometric appearance is drastically changing over time, while efficiently finding the best state of the object with the BH (Basin-Hopping) sampling. By selecting robust features and using segmentation results, the tracking performance was further enhanced. The experimental results demonstrated that the proposed method outperformed conventional tracking algorithms in severe tracking environments. Future work aims to extend the proposed method to deal with severe occlusions and multi objects. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Example of tracking results in transformer seq. The proposed tracking algorithm successfully tracks a target even when the target's geometric appearance changes drastically. The white squares represent the affine transformed-local patches in the appearance model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>t ) Fig. 2 :</head><label>2</label><figDesc>Example of proposed local patch-based appearance model (a) The figure shows an example of the state, Xt. (b) The figure describes an example of the topology between local patches, defined by Rt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 :</head><label>3</label><figDesc>Process of segmentation in transformer seq. To construct a background patch, we firstly choose the nearest bounding line to each foreground patch. Then, we select a center position of the background patch outside this bounding line, which is in the perpendicular direction of the line, to place the patch 20 pixels away from the bounding box. The size of a background patch is equal to that of a foreground patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 :</head><label>4</label><figDesc>(l)t )] represents the segmented region obtained using the seeds centered on O(X (l) t ), M t indicates the whole model of the target, and λ s denotes the weighting parameter set to 5. The whole model M t is (a) Bounding box (b) Good points (c) Chosen patches Fig. Example of patch initialization in diving seq. (b) displays 50 points that have small K and (c) illustrates 15 initialized local patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 :</head><label>5</label><figDesc>Example of the likelihood landscape type The green curve indicates the likelihood landscape of the local patches. Red circles denote samples of a local patch. Blue circles represent local modes of these samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>6 : 7 :</head><label>67</label><figDesc>(a) Qualitative results of LLM Case Dsm Dst Description 1 20.0 1.85 The local patch has the dominant appearance. 2 0.08 0.19 There are severe background clutters around the patch. 3 0.62 3.20 There are many of similar textures around the patch. 4 5.64 0.12 There exists homogeneous regions around the patch. (b) Quantitative results of LLM Fig. Experimental results of the likelihood landscape analysis Red squares denote samples of a local patch. Blue squares represent the local modes of these samples. (a) Illumination changes (b) Feature selection results (c) Advantage of the feature selection Fig. Feature selection process at frame #81 in snowboard seq. (a) Region A is under severe the illumination changes. (b) Red, green, and blue squares denote local patches, which take hue, saturation, and value as a feature, respectively. As shown in the figure, the proposed method adaptively chooses a different feature for each local patch. (c) To describe region A, the hue and saturation (HS) features are better than the value (V) feature because they make the likelihood landscape smooth and steep.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 Fig. 8 :</head><label>38</label><figDesc>(a) Proposal Q 1 (b) Proposal Q 2 (c) Proposal Q Process of the proposal step in high-jump seq. (a) At the start of frame t, our method proposes a new center and scale of the object (green circle and dotted green rectangle) based on the positions of local patches (blue rectangles) at frame t-1 using Q 1 in (13). In the example, a new center is proposed in the right direction because the centroid of the local patches (blue circle) is located to the right of the object center (red circle). A new scale is proposed in the growing direction because the imaginary bounding box constructed by the local patches (dotted blue rectangle) is bigger than the current bounding box (red rectangle). (b) Within frame t, a new center and scale of the object (green circle and dotted green rectangle) are sampled by Q 2 in (15). (c) After proposing a new center and scale of the object, a new center of each local patch (white circle) is determined by Q 3 in (16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>2 and 5.3, bad patches can be identified as those with rough or gradual LLM. These bad patches are modified online. Two criteria for modification are provided, such that • Criterion 1: A modified local patch has to be similar to the foreground and dissimilar to the background. • Criterion 2: A modified local patch has to be far from other local patches. The first criterion prevents local patches from drifting away from an object and into a background. On the other hand, the second criterion allows local patches to cover as different parts of the object as possible. The first criterion is formulated by exp -λF 2 ( Xi t ,F M ) exp -λF 2 ( Xi t ,BM ) ≥ θ C1 , where Xi t denotes the modified i-th local patch, F 2 returns Bhattacharyya similarity between two HSV histograms, and λ indicates the weighting parameter. The foreground model F M is constructed by the average of HSV histograms of unmodified local patches and the background model BM is made by HSV histogram of a background local patch. The process of constructing the background local patch is illustrated in Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Process of the acceptance step in high-jump seq. (a) A state (red circle) is moved to the state of the local mode (blue circle) using a local optimizer. The states of the local modes represent the states of the local patches changed via affine transformation (blue rectangles). (b) After all states (red circles) proposed by Q in (13),<ref type="bibr" target="#b14">(15)</ref>, and (16) are moved to the states of local modes (blue and green circles), the original landscape (green curve) is transformed into a simpler one (red line). (c) In the simplified landscape, our sampler can easily reach the global optimum (blue circle) from the local optima (green circles) via the shorter path (dotted red arrows). On the other hand, the conventional sampler has difficulty reaching the global optimum because the longer path (dotted black arrows) contains the down direction.</figDesc><graphic coords="7,339.55,195.89,83.53,98.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>10 :</head><label>10</label><figDesc>Efficiency of local patch-based dynamic appearance modeling in diving seq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>11 :TABLE 2 :</head><label>112</label><figDesc>Number of stable, moved and newly added patches (b) Geometric appearance of the object when the number of moved patches peak in (a) Fig. Number of stable, moved, and newly added patches in gymnastics seq. (b) Among 27 local patches, 23 patches are moved at frame #32, 22 at frame #90, 22 at frame #153 and, 20 at frame #195, where green squares denote the moved patches and blue squares denote the stable ones. Likelihood landscape analysis of the proposed appearance model. A step: Local patch-based appearance modeling step, B step: Online updating step after A step, C step: Feature selecting step after B step. C* step: Other features (Gabor filters) were used for the step C. The numbers indicate S LLM in Section 5.3. Larger S LLM indicates better likelihood landscape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>12 :</head><label>12</label><figDesc>Property of the adaptive Basin Hopping Monte Carlo sampling in car4 seq. in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Efficiency of the adaptive Basin Hopping Monte Carlo sampling. Figures (a) and (b) illustrate the tracking accuracy as the number of samples increases. In the experiments, the tracking accuracy was obtained by Pascal score using the current number of samplesBest Pascal score * 100. AIS denotes the Annealed Importance Sampling method in<ref type="bibr" target="#b39">[40]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: Experiments on parameter settings of ABHMC-FS. In the experiments, the tracking accuracy was obtained by Pascal score using current parameters Best Pascal score * 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>)-(l). The test video used in Figures15(m)-(o) includes the illumination and scale changes of an object. For tracking an object that grows larger over time, the proposed method extended the range between the center of an object and each local patch, and added new patches. Moreover, by changing the features of the patches adaptively, the proposed method tracked the object successfully despite severe illumination changes. Figures15(p)-(r)show the comparison among the proposed methods (ABHMC, ABHMC-This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 : 16 :</head><label>1516</label><figDesc>Qualitative comparison with other methods using color sequences. In (a) to (o), the green, magenta, cyan, yellow and red rectangles denote the bounding boxes of MCMC, IVT, FRAGT, BHT, and ABHMC-FS, respectively. In (p) to (r), the pink, white, and red rectangles denote the bounding boxes of ABHMC, ABHMC-F, and ABHMC-FS, respectively. (a) Frame #40 (b) Frame #146 (c) Frame #216 (d) Frame #50 (e) Frame #79 (f) Frame #99 (g) Frame #17 (h) Frame #70 (i) Frame #133 (j) Frame #18 (k) Frame #39 (l) Frame #166 Fig. Tracking results of the proposed method using gray sequences. The red rectangles denote the bounding boxes of ABHMC-FS. White squares describe the local modes of patches in our appearance model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 17 : 18 :</head><label>1718</label><figDesc>Tracking results of the proposed ABHMC-FS using recent challenging sequences. The red rectangles denote the bounding boxes of ABHMC-FS. White squares describe the local modes of patches in our appearance model. (a) Frame #145 (b) Frame #289 (c) Frame #316 (d) Frame #455 (e) Frame #352 (f) Frame #340 Fig. Tracking results of the proposed method using benchmark sequences. The red rectangles denote the bounding boxes of ABHMC-FS. White squares describe the local modes of patches in our appearance model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where X</figDesc><table><row><cell cols="2">(l) t is is the new state, and G denotes the Gaussian function with mean X the previous state, X (l+1) t (l) t and variance σ 2 .</cell></row><row><cell cols="2">• Acceptance Step: The acceptance step determines</cell></row><row><cell>whether the new proposed state X</cell><cell>(l+1) t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>The</figDesc><table /><note><p><p><p><p><p><p><p><p><p>status of local modes.</p>Smoothness Status Dsm ≥ θsm</p>The landscape of local modes is smooth.</p>Dsm &lt; θsm</p>The landscape of local modes is rough.</p>Steepness Status Dst ≥ θst</p>The shape of local modes is steep.</p>Dst &lt; θst</p>The shape of local modes is gradual.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>\ Seq. diving</figDesc><table><row><cell></cell><cell></cell><cell>high-jump</cell><cell>gymnastics</cell><cell>transformer</cell><cell>Snowboard</cell></row><row><cell>A</cell><cell>31.98</cell><cell>21.54</cell><cell>98.65</cell><cell>11.23</cell><cell>9.71</cell></row><row><cell>B</cell><cell>44.74</cell><cell>87.34</cell><cell>101.43</cell><cell>12.28</cell><cell>47.11</cell></row><row><cell>C</cell><cell>76.94</cell><cell>150.675</cell><cell>202.43</cell><cell>35.34</cell><cell>85.03</cell></row><row><cell>C*</cell><cell>45.23</cell><cell>36.22</cell><cell>150.32</cell><cell>17.98</cell><cell>36.92</cell></row><row><cell cols="6">ones denote modified ones. The second row of Figures</cell></row><row><cell cols="6">10 represents the local modes of the patches as white</cell></row><row><cell cols="6">squares and the estimated center of the object as a red</cell></row><row><cell cols="2">point. Figure</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 : Quantitative analysis of individual component within the proposed methods.</head><label>3</label><figDesc>The numbers indicate tracking accuracy, which are evaluated by the Pascal score<ref type="bibr" target="#b43">[44]</ref>. The Pascal score is defined by the overlap ratio between the predicted bounding box Bp and ground truth bounding box Bgt: area(Bp B gt ) area(Bp B gt ) . A step: Base-line step (MCMC, Section 3.1), B step: Local patch-based appearance modeling step after A step (Section 4.1), C step: Online updating step after B step (ABHMC, Section 5.1, 5.2, and 5.4), D step: Feature selecting step after C step (ABHMC-F, Section 5.3), E step: Segmentation step after D step (ABHMC-FS, Section 4.2) ) illustrate the tracking accuracy as the LLM parameters, θ sm and θ st increase. The parameter θ sm and θ st are the threshold values to determine the smoothness and the steepness of LLM, respectively. If θ sm and θ st increase, local patches are more frequently updated because LLMs of local patches are considered as bad (rough and gradual) ones. Hence, our method can cover drastic appearance changes of the targets.</figDesc><table><row><cell>Seq. \ Step</cell><cell>A step</cell><cell>B step</cell><cell>C step</cell><cell>D step</cell><cell>E step</cell></row><row><cell>diving</cell><cell>0.32</cell><cell>0.37</cell><cell>0.47</cell><cell>0.58</cell><cell>0.64</cell></row><row><cell>high-jump</cell><cell>0.33</cell><cell>0.35</cell><cell>0.37</cell><cell>0.41</cell><cell>0.51</cell></row><row><cell>gymnastics</cell><cell>0.39</cell><cell>0.45</cell><cell>0.61</cell><cell>0.65</cell><cell>0.71</cell></row><row><cell>transformer</cell><cell>0.49</cell><cell>0.42</cell><cell>0.59</cell><cell>0.63</cell><cell>0.72</cell></row><row><cell>snowboard</cell><cell>0.16</cell><cell>0.12</cell><cell>0.14</cell><cell>0.43</cell><cell>0.58</cell></row><row><cell>dancer</cell><cell>0.45</cell><cell>0.47</cell><cell>0.55</cell><cell>N/A</cell><cell>0.58</cell></row><row><cell>femaleskater</cell><cell>0.51</cell><cell>0.49</cell><cell>0.50</cell><cell>N/A</cell><cell>0.58</cell></row><row><cell>indiandancer</cell><cell>0.41</cell><cell>0.44</cell><cell>0.52</cell><cell>N/A</cell><cell>0.59</cell></row><row><cell>maleskater</cell><cell>0.40</cell><cell>0.43</cell><cell>0.49</cell><cell>N/A</cell><cell>0.56</cell></row><row><cell>coke</cell><cell>0.08</cell><cell>0.17</cell><cell>0.21</cell><cell>N/A</cell><cell>0.35</cell></row><row><cell>dinosaur</cell><cell>0.23</cell><cell>0.26</cell><cell>0.33</cell><cell>0.37</cell><cell>0.63</cell></row><row><cell>girl</cell><cell>0.61</cell><cell>0.35</cell><cell>0.41</cell><cell>0.42</cell><cell>0.52</cell></row><row><cell>hand2</cell><cell>0.19</cell><cell>0.40</cell><cell>0.49</cell><cell>0.52</cell><cell>0.76</cell></row><row><cell>motocross2</cell><cell>0.47</cell><cell>0.53</cell><cell>0.60</cell><cell>0.61</cell><cell>0.72</cell></row><row><cell>skiing</cell><cell>0.03</cell><cell>0.21</cell><cell>0.30</cell><cell>0.30</cell><cell>0.55</cell></row><row><cell>tiger1</cell><cell>0.21</cell><cell>0.26</cell><cell>0.37</cell><cell>N/A</cell><cell>0.69</cell></row><row><cell>tiger2</cell><cell>0.17</cell><cell>0.20</cell><cell>0.28</cell><cell>N/A</cell><cell>0.62</cell></row><row><cell>Average</cell><cell>0.32</cell><cell>0.35</cell><cell>0.43</cell><cell>0.49</cell><cell>0.61</cell></row><row><cell>Improvement (%)</cell><cell>0</cell><cell>+5</cell><cell>+13</cell><cell>+10</cell><cell>+20</cell></row><row><cell>Time (sec/frame)</cell><cell>0.04</cell><cell>+0.01</cell><cell>+0.04</cell><cell>+0.01</cell><cell>+1.90</cell></row><row><cell cols="6">because the weights does not affect much the tracking</cell></row><row><cell cols="6">performance, as demonstrated in Figures 14(d)-(f). Fig-</cell></row><row><cell>ure 14(g) and 14(h</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>Quantitative comparison with other methods. The numbers indicate mean and standard deviation of tracking accuracy, which are evaluated by the Pascal score<ref type="bibr" target="#b43">[44]</ref>. These numbers were obtained by running each algorithm 5 times. The Pascal score is defined by the overlap ratio between the predicted bounding box Bp and ground truth bounding box Bgt: area(Bp B gt ) area(Bp B gt ) . The red mark represents the best results, whereas the blue mark represents the second-best results. All parameters of the proposed method (ABHMC-FS) are fixed for this experiment.</figDesc><table><row><cell>Seq. \ Method</cell><cell>MS</cell><cell>MCMC</cell><cell>IVT</cell><cell>FRAGT</cell><cell>BHT</cell><cell>MIL</cell><cell>LGT</cell><cell>HT</cell><cell>ABHMC-FS</cell></row><row><cell>diving</cell><cell>0.41</cell><cell>0.32 (0.09)</cell><cell>0.18 (0.08)</cell><cell>0.21 (0.07)</cell><cell>0.17 (0.08)</cell><cell>0.20 (0.11)</cell><cell>0.48 (0.11)</cell><cell>0.41 (0.10)</cell><cell>0.64 (0.08)</cell></row><row><cell>high-jump</cell><cell>0.35</cell><cell>0.33 (0.09)</cell><cell>0.07 (0.11)</cell><cell>0.07 (0.10)</cell><cell>0.11 (0.10)</cell><cell>0.06 (0.11)</cell><cell>0.40 (0.08)</cell><cell>0.38 (0.09)</cell><cell>0.51 (0.10)</cell></row><row><cell>gymnastics</cell><cell>0.45</cell><cell>0.39 (0.11)</cell><cell>0.36 (0.13)</cell><cell>0.43 (0.10)</cell><cell>0.56 (0.12)</cell><cell>0.33 (0.15)</cell><cell>0.44 (0.11)</cell><cell>0.47 (0.10)</cell><cell>0.71 (0.09)</cell></row><row><cell>transformer</cell><cell>0.45</cell><cell>0.49 (0.15)</cell><cell>0.33 (0.11)</cell><cell>0.46 (0.11)</cell><cell>0.50 (0.11)</cell><cell>0.37 (0.19)</cell><cell>0.24 (0.17)</cell><cell>0.51 (0.11)</cell><cell>0.72 (0.12)</cell></row><row><cell>snowboard</cell><cell>0.10</cell><cell>0.16 (0.25)</cell><cell>0.16 (0.23)</cell><cell>0.15 (0.17)</cell><cell>0.15 (0.15)</cell><cell>0.15 (0.25)</cell><cell>0.18 (0.19)</cell><cell>0.20 (0.27)</cell><cell>0.58 (0.21)</cell></row><row><cell>dancer</cell><cell>0.39</cell><cell>0.45 (0.15)</cell><cell>0.63 (0.13)</cell><cell>0.61 (0.13)</cell><cell>0.53 (0.13)</cell><cell>0.55 (0.13)</cell><cell>0.40 (0.13)</cell><cell>0.53 (0.15)</cell><cell>0.58 (0.13)</cell></row><row><cell>femaleskater</cell><cell>0.55</cell><cell>0.51 (0.13)</cell><cell>0.48 (0.12)</cell><cell>0.57 (0.11)</cell><cell>0.56 (0.11)</cell><cell>0.54 (0.10)</cell><cell>0.52 (0.12)</cell><cell>0.46 (0.10)</cell><cell>0.58 (0.11)</cell></row><row><cell>indiandancer</cell><cell>0.37</cell><cell>0.41 (0.12)</cell><cell>0.64 (0.13)</cell><cell>0.68 (0.10)</cell><cell>0.63 (0.13)</cell><cell>0.69 (0.10)</cell><cell>0.50 (0.15)</cell><cell>0.57 (0.13)</cell><cell>0.59 (0.12)</cell></row><row><cell>maleskater</cell><cell>0.39</cell><cell>0.40 (0.14)</cell><cell>0.13 (0.15)</cell><cell>0.55 (0.09)</cell><cell>0.56 (0.09)</cell><cell>0.24 (0.09)</cell><cell>0.40 (0.14)</cell><cell>0.28 (0.12)</cell><cell>0.56 (0.10)</cell></row><row><cell>coke</cell><cell>0.17</cell><cell>0.08 (0.17)</cell><cell>0.16 (0.14)</cell><cell>0.07 (0.11)</cell><cell>0.07 (0.12)</cell><cell>0.35 (0.11)</cell><cell>0.24 (0.10)</cell><cell>0.30 (0.09)</cell><cell>0.34 (0.09)</cell></row><row><cell>dinosaur</cell><cell>0.39</cell><cell>0.23 (0.20)</cell><cell>0.23 (0.13)</cell><cell>0.16 (0.15)</cell><cell>0.21 (0.14)</cell><cell>0.42 (0.15)</cell><cell>0.46 (0.15)</cell><cell>0.23 (0.15)</cell><cell>0.63 (0.13)</cell></row><row><cell>girl</cell><cell>0.50</cell><cell>0.61 (0.15)</cell><cell>0.03 (0.11)</cell><cell>0.63 (0.13)</cell><cell>0.51 (0.13)</cell><cell>0.56 (0.14)</cell><cell>0.06 (0.11)</cell><cell>0.52 (0.13)</cell><cell>0.52 (0.15)</cell></row><row><cell>hand2</cell><cell>0.15</cell><cell>0.19 (0.12)</cell><cell>0.13 (0.15)</cell><cell>0.20 (0.13)</cell><cell>0.30 (0.08)</cell><cell>0.11 (0.12)</cell><cell>0.65 (0.13)</cell><cell>0.60 (0.11)</cell><cell>0.76 (0.09)</cell></row><row><cell>motocross2</cell><cell>0.46</cell><cell>0.47 (0.15)</cell><cell>0.43 (0.11)</cell><cell>0.45 (0.12)</cell><cell>0.03 (0.10)</cell><cell>0.59 (0.09)</cell><cell>0.50 (0.09)</cell><cell>0.78 (0.10)</cell><cell>0.72 (0.11)</cell></row><row><cell>skiing</cell><cell>0.10</cell><cell>0.03 (0.07)</cell><cell>0.05 (0.06)</cell><cell>0.05 (0.06)</cell><cell>0.20 (0.05)</cell><cell>0.04 (0.10)</cell><cell>0.04 (0.07)</cell><cell>0.53 (0.06)</cell><cell>0.55 (0.05)</cell></row><row><cell>tiger1</cell><cell>0.25</cell><cell>0.21 (0.12)</cell><cell>0.10 (0.09)</cell><cell>0.17 (0.08)</cell><cell>0.22 (0.08)</cell><cell>0.64 (0.07)</cell><cell>0.12 (0.09)</cell><cell>0.40 (0.10)</cell><cell>0.69 (0.08)</cell></row><row><cell>tiger2</cell><cell>0.28</cell><cell>0.17 (0.11)</cell><cell>0.08 (0.08)</cell><cell>0.20 (0.10)</cell><cell>0.13 (0.10)</cell><cell>0.65 (0.10)</cell><cell>0.22 (0.09)</cell><cell>0.35 (0.09)</cell><cell>0.62 (0.09)</cell></row><row><cell cols="5">our ABHMC-FS is more robust because it includes the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">segmentation step as well. According to the experiment,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">to track the highly non-rigid object accurately, the local</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">patch-based appearance model should be modified via</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">online update like ABHMC. And the appearance model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">should include both the local and global appearance of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">the target like ABHMC-FS.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>also shows the computation time of each individual component in ABHMC-FS. The tracker approximately takes 2 seconds per frame using the Pentium 4 quad core 2.4GHz CPU. It is notable that our ABHMC without the segmentation procedure (before the E step in Table</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The Hessian matrix is defined byH = x [∇I ∂W ∂p ] T [∇I ∂W ∂p ]where W is the warp matrix, p is the warping parameter, and ∇I is the image gradient<ref type="bibr" target="#b18">[19]</ref>. To update the warping parameter during image alignment, the inverse Hessian matrix H -1 must be used. Therefore, numerical stability is important.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>The videos of tracking results are available at http://cv.snu.ac.kr/ paradiso/ABHMC.zip.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ensemble tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online selection of discriminative tracking features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1631" to="1643" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On-line boosting and vision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On-line density-based appearance modeling for object tracking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust online appearance models for visual tracking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F E</forename><surname>Maraghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1296" to="1311" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tracking-learningdetection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Tracking of a non-rigid object via patchbased dynamic appearance modeling and adaptive basin hopping monte carlo sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tracking non-stationary appearances and dynamic feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Superpixel tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time tracking of non-rigid objects using mean shift</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Color-based probabilistic tracking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vermaak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust real-time visual tracking using pixel-wise posteriors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bibby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial priors for part-based recognition using statistical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sparse object category model for efficient learning and exhaustive recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coupled object detection and tracking from static cameras and moving vehicles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1683" to="1698" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A rao-blackwellized partsconstellation tracker</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tracking people by learning their appearance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="81" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tracking multiple humans in crowded environment</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An adaptive coupledlayer visual model for robust visual tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hough-based tracking of non-rigid objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust fragments-based tracking using the integral histogram</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual tracking with histograms and articulating blocks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M S</forename><surname>Nejhum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial selection for attentional visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive fragments-based tracking of non-rigid objects using level sets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chockalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A nonparametric treatment for location/segmentation based visual tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Icondensation: Unifying low-level and high-level tracking in a stochastic framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MCMC-based particle filtering for tracking a variable number of interacting targets</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1805" to="1918" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using particles to track varying numbers of interacting people</title>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monte carlo sampling methods using markov chains and their applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hastings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The template update problem</title>
		<author>
			<persName><forename type="first">L</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="810" to="815" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generative image segmentation using random walks with restart</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multicanonical basin hopping: A new global optimization method for complex systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piwowar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5536" to="5542" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online multi-class lpboost</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mean-shift blob tracking through scale space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Annealed importance sampling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<idno>No. 9805</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Statistics, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Introduction to markov chain monte carlo simulations and their statistical analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys .Stat. Mech</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The scaling behaviour of stochastic minimization algorithms in a perfect funnel landscape</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hamacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wenzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="938" to="941" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stochastic optimization methods for structure prediction of biomolecular nanoscale systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Herges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Merlitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wenzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1161" to="1167" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">He is currently working toward the PhD degree in electrical engineering and computer science at Seoul National University. His research interests include visual tracking, event detection, and surveillance. He is a student member of the IEEE</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="308" />
			<date type="published" when="2006">2009. 2006 and 2008</date>
			<pubPlace>Seoul</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Control and Instrumentation Engineering from Seoul National University (SNU</orgName>
		</respStmt>
	</monogr>
	<note>Junseok Kwon received the BS degree in electrical engineering and the MS degree in electrical engineering and computer science from Seoul National University(SNU), Seoul, Korea. Korea in 1984 and 1986, respectively, and Ph. D.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
