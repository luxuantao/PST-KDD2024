<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vyasa: A High-Performance Vectorizing Compiler for Tensor Convolutions on the Xilinx AI Engine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-02">2 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Prasanth</forename><surname>Chatarasi</surname></persName>
							<email>cprasanth@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Tech Atlanta</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Neuendorffer</surname></persName>
							<email>stephenn@xilinx.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Xilinx Research Labs San Jose</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Bayliss</surname></persName>
							<email>samuelb@xilinx.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Xilinx Research Labs San Jose</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kees</forename><surname>Vissers</surname></persName>
							<email>keesv@xilinx.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Xilinx Research Labs San Jose</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
							<email>vsarkar@gatech.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Georgia Tech Atlanta</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vyasa: A High-Performance Vectorizing Compiler for Tensor Convolutions on the Xilinx AI Engine</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-02">2 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.01331v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Xilinx's AI Engine is a recent industry example of energy-efficient vector processing that includes novel support for 2D SIMD datapaths and shuffle interconnection network. The current approach to programming the AI Engine relies on a C/C++ API for vector intrinsics. While an advance over assemblylevel programming, it requires the programmer to specify a number of low-level operations based on detailed knowledge of the hardware. To address these challenges, we introduce Vyasa, a new programming system that extends the Halide DSL compiler to automatically generate code for the AI Engine. We evaluated Vyasa on 36 CONV2D and 6 CONV3D workloads, and achieved geometric means of 7.6 and 23.3 MACs/cycle for 32-bit and 16bit operands (which represent 95.9% and 72.8% of the peak performance respectively). For 4 of these workloads for which expert-written codes were available to us, Vyasa demonstrated a geometric mean performance improvement of 1.10× with 50× smaller code relative to the expert-written codes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>It is widely recognized that a major disruption is under way in computer hardware as processors strive to extend, and go beyond, the end-game of Moore's Law. Unlike previous generations of hardware evolution, these "extreme heterogeneity" systems will have a profound impact on future software. As part of these trends, there is a strong resurgence of interest in improving vector processing (SIMD) units due to the significant energy efficiency benefits of using SIMD parallelism. These benefits increase with widening SIMD vectors, reaching vector register lengths of 2048 bits in the scalable vector extension of the Armv8 architecture <ref type="bibr" target="#b0">[1]</ref>. Furthermore, there is an emphasis on specializing SIMD units to further improve energy efficiency benefits for specific domains such as Machine learning, Computer Vision, and 5G Wireless. An important specialization, which is referred to as "2D vector SIMD datapath" <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, is the ability of each vector lane to execute more than one scalar operation and to chain the results from one operation to another. Another specialization includes the removal of expensive data permutation units (e.g., shuffle units) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and instead introduce sophisticated, programmable interconnection networks (a.k.a shuffle networks) between the SIMD datapath and vector register file to support the required data permutation patterns <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[7]</ref>.</p><p>A recent industry example with these specializations is the Xilinx Versal AI Engine <ref type="bibr" target="#b8">[8]</ref>, a high-performance VLIW SIMD core which can deliver performance comparable to traditional FPGA solutions for Computer Vision, Deep Learning, and 5G wireless domains, but with 50% less power consumption and up to eight times more compute capacity per silicon area <ref type="bibr" target="#b8">[8]</ref>. AI Engine cores are tightly integrated with programmable logic in Xilinx Versal ACAP devices to form a seamless heterogeneous compute platform <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref> applicable to a wide variety of HPC applications. Furthermore, the Versal AI Engine series VC1902 has a total of 400 AI Engines that together delivers a peak performance of 6.4 TOPS, 25.6 TOPS and 102.4 TOPS for 32-bit, 16-bit, and 8-bit operands, respectively <ref type="bibr" target="#b10">[10]</ref>.</p><p>Tensor convolution is a widely used mathematical operation in these domains, and it is becoming increasingly important with the rise of its use in image processing workflows <ref type="bibr" target="#b11">[11]</ref>- <ref type="bibr" target="#b13">[13]</ref> and with the proliferation of deep learning models <ref type="bibr" target="#b14">[14]</ref>- <ref type="bibr" target="#b17">[17]</ref> in data centers, edge, and mobile devices. There has been a lot of prior work in optimizing tensor convolutions for a variety of target hardware devices such as CPUs <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b18">[18]</ref>, GPUs <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, FPGAs <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b20">[20]</ref>- <ref type="bibr" target="#b23">[23]</ref>, and Dataflow accelerators <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b24">[24]</ref>- <ref type="bibr" target="#b26">[26]</ref>. However, even for well understood applications like convolution, generating the best code for new high performance processor architectures from high-level descriptions can be challenging. This work demonstrates the ability to automatically optimize tensor convolutions for the AI Engine and to obtain close to the peak performance for various workloads while using a high-level programming model, rather than low-level C/C++ intrinsics. Challenges. Achieving peak performance on the AI Engine requires leveraging several architectural features to maximize vector datapath occupancy during program execution. Unlike standard SIMD architectures which operate on 1D vectors, the AI Engine architecture includes 2D vector operations for some datatypes which conceptually implement the fusion of several 1D vector operations. Also unlike other architectures, the AI Engine doesn't implement direct support for unaligned loads, scalar broadcasts, and data manipulation operations. Instead, the AI Engine architecture includes a novel shuffle network which selects desired elements of a vector register for a vector operation instead of explicitly shuffling and storing them into another vector register. In order to effectively leverage these features, the layout of data in memory must match the capabilities of the shuffle network.</p><p>Existing AI Engine compilers do not perform autovectorization, leaving it to expert programmers to explicitly write high-performance vector code using architectural intrinsic functions. Optimizing programs in this way can be timeconsuming even for experts. At the same time, there are a wide variety of tensor convolution operators in common use, for instance, deep neural networks may contain regular 2D convolutions, depth-wise convolutions, and point-wise convolutions. Even within the same network, the shape of tensor data can vary radically between the early and late layers in DNN models. We find that no single optimization strategy is an optimal choice for all these scenarios. Reducing the need for manual optimization and quickly adapting to new tensor operations through automatic optimization avoids these problems.</p><p>With all these challenges, the overall goal of our work is to automate the generation of high-performance vector code for tensor convolutions based on their variations and shapes, while exploiting the unique capabilities of the Xilinx AI Engine without requiring manual effort in development and tuning. Achieving this goal requires significant loop-level reuse analysis, code transformation, and data-layout transformation, along with optimized low-level code generation taking into account the shuffle network and memory optimizations such as vector register reuse (including partial reuse) <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>.</p><p>The main technical contributions of this paper are briefly described below:</p><p>• We introduce a new domain-specific intermediate representation called Triplet to symbolically capture the loop body of a tensor convolution, and to simplify analyses and transformations required to generate high-performance code for the AI Engine. • We propose a novel multi-step compiler approach which includes analyses and transformations to 1) exploit the 2D SIMD datapath by identifying multiple 1D logical vector operations that can be legally fused, 2) realize unaligned loads, scalar broadcasts, data manipulation using the shuffle network, 3) improve memory utilization by performing vector register reuse and also loop optimizations, and 4) generate code that is more amenable to enabling VLIW instruction scheduling for the AI Engine. • We created a new tool, Vyasa<ref type="foot" target="#foot_0">1</ref> , to implement our multistep compiler approach. Vyasa is built on the Halide framework <ref type="bibr" target="#b11">[11]</ref> and includes extensions needed for the AI Engine that are not supported by Halide. Given a tensor convolution specification in the Halide language and workload sizes, Vyasa generates high-performance C-code with vector intrinsics for the AI Engine. II. BACKGROUND In this section, we start with a brief overview of tensor convolutions, and then we briefly summarize the key architectural features of the Xilinx Versal AI Engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tensor Convolutions</head><p>A convolution is a mathematical operation which computes the amount of overlap of a function g as it is shifted over another function f , and it is symbolically represented as f • g. In this section, we restrict our attention to describing CONV2D, a popular convolution operator widely used in Deep learning <ref type="bibr" target="#b14">[14]</ref>- <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref> and Computer Vision <ref type="bibr" target="#b11">[11]</ref>- <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b31">[31]</ref>. In these domains, the function f and g are referred to as the "input" tensor (a.k.a image/activations) and "weight" tensor (a.k.a filters/kernels), respectively. The CONV2D deals with three four-dimensional tensors, i.e., Output (O), Weight (W), and Input (I), whose dimensions are described below.</p><formula xml:id="formula_0">Tensor Dim1 Dim2 Dim3 Dim4 Output (O) Width (X) Height (Y) Channels (K) Batch (N) Weight (W) Width (R) Height (S) Channels (C) Batch (K) Input (I) Width (X') Height (Y') Channels (C) Batch (N)</formula><p>The mathematical expression of the CONV2D operations is shown below, where f refers to stride factor.</p><formula xml:id="formula_1">O(x, y, k, n) = C c S s R r W (r, s, c, k) × I(x × f + r, y × f + s, c, n)</formula><p>The convolutions used in Computer Vision are special cases of the CONV2D operator, where each tensor has only the first two dimensions (width and height) and stride factor set to one. However, there exist a wide variety of filter sizes (ranging from 2 to 11) used in many different image processing operators, such as Gaussian smoothing and edge detection <ref type="bibr" target="#b31">[31]</ref>.</p><p>A wide variety of other specialized variations of the CONV2D operator are used in Convolutional Neural Networks such as point-wise, depth-wise separable, and spatially separable convolutions. These variations can be viewed as constraints on the regular CONV2D operator, and are shown below. Even though we briefly described the CONV2D operator and its variations, our approach is applicable to other convolution operators such as CONV1D and CONV3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Xilinx AI Engine</head><p>Driven by the performance and energy efficiency requirements of many computing applications, Xilinx introduced Versal Advanced Compute Acceleration Platform (ACAP) <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, a fully software-programmable, heterogeneous compute platform. The Versal platform consists of three types of programmable processors -Scalar Engines (CPUs), Adaptable Engines (Programmable Logic), and an array of Intelligent Engines (AI Engines) <ref type="bibr" target="#b10">[10]</ref>. In this work, we focus on AI Engines, which are specialized SIMD and VLIW high-performance processors for compute-intensive applications such as computer vision, machine learning workloads, and 5G wireless. AI Engines are highly energy efficient compared to FPGAs and can deliver up to 8X silicon compute density at 50% the power consumption of traditional FPGA solutions <ref type="bibr" target="#b8">[8]</ref>.</p><p>An AI Engine includes a 2D SIMD datapath for fixedpoint vector operations (our focus), a 1D SIMD datapath for floating-point vector operations, and a scalar unit for scalar operations. Each AI Engine also has access to 128KB scratchpad (a.k.a data/local) memory, a 16KB program memory, and a 256B vector register file (a total of 16 registers with each size being 128 bits). These high-performance AI Engines are programmed using the C/C++ programming language with optional pragmas. A simplified overview of the key architectural features of the AI Engine core is shown in fig. <ref type="figure" target="#fig_1">1</ref>, and these features are briefly described below. 1) Two-dimensional SIMD Datapath. The fixed point vector unit of the AI Engine is a two-dimensional SIMD datapath, and vector operations on the 2D SIMD datapath are described using lanes/rows and columns. The number of lanes corresponds to the number of output values generated from the vector operation. The number of columns is the number of operations that are done per output lane, with each of the results being reduced together. This technique of executing back to back dependent scalar operations along a vector lane is popularly known as operation chaining <ref type="bibr" target="#b1">[2]</ref>  2) Shuffle network. A key novelty of the AI Engine architecture is its shuffle network, a flexible interconnection network between the 2D SIMD datapath and vector register file to allow flexible data selection from the input vector registers for the multipliers of each lane and column of the SIMD datapath.</p><p>The ability to configure the shuffle network for each vector operation is exposed to programmers via the arguments of the vector intrinsic functions. Unlike the data manipulation units in traditional SIMD units, the data selection using the shuffle network over a vector register can only be used during a vector operation. The granularity of data selection using the shuffle network on the vector registers is 32b, and so the network allows full flexibility for making data selection, replication, and permutation on vectors of 32b data types. However, for data types of smaller sizes such as 16b and 8b data types, the shuffle network imposes further constraints on data selection.</p><p>Vector loads and stores in the AI Engine must be aligned to 128-bit data memory boundaries. The AI Engine does not implement unaligned loads or scalar broadcasts. Instead, these operations are typically realized/implemented using a combination of aligned loads and configuration of the shuffle network.</p><p>3) VLIW capabilities. The AI Engine has support for very long instruction word (VLIW) that can provide up to 6way instruction parallelism to hide long instruction latencies. The VLIW instruction includes two scalar operations, two vector load operations, one vector store operation, and one fixed/floating-point vector operation. The AI Engine compilers have support for automatic software pipelining <ref type="bibr" target="#b32">[32]</ref> of innermost loops to exploit instruction-level parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head><p>In this section, we introduce our approach to generating high-performance vector code for a given high-level specification of tensor convolution and its workload sizes that fit into a single AI Engine's data memory. These vector codes are intended to execute on a single AI Engine and will be integrated by a high-level compiler to run larger tensor convolutions across multiple AI Engines. Our approach is summarized in fig. <ref type="figure">2</ref> and is implemented in a tool called Vyasa. The tool is developed as an extension to the Halide framework <ref type="bibr" target="#b11">[11]</ref>. Our approach begins with an auto-Fig. <ref type="figure">2</ref>. Workflow of our approach (Vyasa) which is implemented as an extension to the Halide framework <ref type="bibr" target="#b11">[11]</ref>.</p><p>tuner taking the specification of a tensor convolution in the Halide language and also the corresponding workload sizes. Then, the auto-tuner iterates through each possible schedule in the space of loop transformations and data-layouts, and invokes our multi-step compiler approach to generate highperformance vector c-code corresponding to the schedule. Then, our approach evaluates the generated code using a cycleaccurate simulation of the AI Engine, and chooses the best one among all schedules to finally emit as the performant output code.</p><p>Our multi-step compiler approach starts from the specification of a tensor convolution, a schedule from the auto-tuner, and workload sizes. It consists of the following steps:</p><p>1) Transforming the loop body of the tensor convolution operation in the Halide IR (after lowering) into our symbolic triplet representation for convenience in doing analyses, transformations, and code generation, 2) Performing 'lazy stores' optimization by accumulating all partial (intermediate) results of an output before generating a store to reduce the memory traffic, 3) Exploiting vector register reuse, and realizing unaligned loads and scalar broadcast operations using the shuffle (interconnection) network, 4) Identifying suitable 1D logical vector operations (multiplications) that contribute to same output through accumulation/reduction and fusing them into operations matching the 2D SIMD datapath, 5) Interleaving load and store operations with vector operations to make it easy for the AI Engine compilers to perform VLIW instruction scheduling, 6) Generating C-code with vector intrinsics.  In general, tensor convolutions are specified/implemented as multi-dimensional perfectly nested loops, where each statement of the loop body has two aspects -1) A group of multiply-and-accumulate (MAC) operations over input and weight tensors, and 2) An update (reduction) operation to the output tensor Since each statement in the convolution loop body performs a reduction operation and the reduction is commutative, the order of each statements doesn't impact its correctness. Hence, a representation holding information for each statement about the two major aspects described above is sufficient to capture the body precisely. We call this representation a "triplet" since it holds information about the access patterns of the two operands of each multiplication and the update operand of each statement symbolically. We consider the convolution of a filter with size 4x3 on an input with size X' x Y' as a running example (shown in fig. <ref type="figure" target="#fig_3">3(a)</ref>) to illustrate each step of our compiler approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Translating into Triplet Representation</head><p>A sample schedule for the above convolution is shown in fig. <ref type="figure" target="#fig_3">3(b)</ref>, which refers to unrolling loops corresponding to filter dimensions (r.x, r.y) and vectorizing the loop-x with vector length as 16. A pictorial overview of the computation at the loop iterations x = 0, y = 0 is shown in fig. <ref type="figure" target="#fig_4">4</ref>.</p><p>After lowering the convolution specification using the schedule into the Halide IR, the first step in our approach is to translate the convolution loop body into our triplet representation. For instance, the triplet representation of the loop body in fig. <ref type="figure" target="#fig_3">3(c</ref>) is shown in Table <ref type="table" target="#tab_2">I</ref>, where each row in the table symbolically captures the access patterns of multiplication operands and update operands of a statement in the loop body. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lazy Stores Optimization</head><p>An approach to code generation based on the triplet representation involves generating a vector store for each row of the representation. But, this code generation can result in immediately writing multiplication results to the data memory causing more traffic. We introduce "lazy stores" optimization to delay writing the multiplication results of an output until there are no operations that can contribute to output. The optimization works by grouping all the rows of the triplet representation contributing to the same output. The benefits of the optimization can be observed in the presence of multiple statements in the loop body contributing to the same output. An example of such behavior is seen in Table <ref type="table" target="#tab_2">I</ref>, where all the statements contribute to the same output (O(x:x+15,y)), and all these statements can be grouped into a single group (Table <ref type="table" target="#tab_3">II</ref>). Now, the code generation involves generating a single vector store for each group, instead of generating for each row of the triplet representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Exploiting Vector Register Reuse &amp; Realizing Unaligned Loads and Scalar Broadcast</head><p>Our approach leverages the AI Engine architecture's unique shuffle network to realize unaligned vector loads and scalar broadcast operations which are common in vectorization of tensor convolutions. Our approach further uses the network to exploit the vector register reuse opportunities. 1) Realizing unaligned vector loads. Simple vectorization of tensor convolutions often result in unaligned vector loads. For example, if the vector load (I(x:x+15,y)) in fig. <ref type="figure" target="#fig_3">3</ref> is aligned to the boundary, then the subsequent vector loads such as I(x+1:x+16,y) are unaligned. Prior work on vectorization for SIMD architectures having no unaligned load/store support address this by generating two adjacent aligned loads covering the required load and using data manipulation/shuffle (register-to-register) instructions to realize an unaligned vector load <ref type="bibr" target="#b28">[28]</ref>. Since the AI Engine architecture doesn't support unaligned loads or shuffle instructions, an alternative solution is necessary. Our approach leverages the AI Engine architecture support for grouping vector registers into a larger vector register. Then, our approach constructs a larger aligned vector load which subsumes the required unaligned load and selects the data corresponding to the original unaligned vector load using the shuffle network. For instance, the unaligned vector load I(x+1:x+16,y) can be realized through a larger aligned vector load I(x:x+31,y) and appropriate data selection parameters during vector operations on the load.</p><p>2) Exploiting vector register reuse. Tensor convolutions often exhibit significant data reuse between vector loads. For instance, the two vector loads I(x:x+15,y) and I(x+1:x+16,y) have 15 data elements in common. Exploiting vector register reuse by reusing those common elements instead of fetching again from the data memory is important to reduce memory traffic and achieve better performance. Our approach groups individual vector loads having such reuse and constructs a larger aligned vector load that subsumes the individual vector loads having reuse. During the vector operations, the individual vector loads are realized through appropriate data selection on the larger vector using the shuffle network.</p><p>Our approach implements the above idea by constructing a reuse graph, an undirected graph where each node denotes a vector load in the triplet representation and an edge is constructed between two nodes if they have at least one common element between them, i.e., presence of a reuse. The reuse graph corresponding to the vector loads of the tensor I in table II is shown in fig. <ref type="figure" target="#fig_5">5</ref>, for instance, nodes I(x:x+15,y) and I(x+1:x+16,y) corresponds to two vector loads and the edge between them denotes the presence of common elements/reuse.</p><p>After constructing the reuse graph, our approach identifies connected components in the reuse graph, where each component represents a larger vector load that subsumes the individual vector loads in that component. For instance, the connected component I(x:x+31,y) in fig. <ref type="figure" target="#fig_5">5</ref> represents a larger vector load subsuming the vector loads I(x:x+15,y), I(x+1:x+16,y), I(x+2:x+17,y), and I(x+3:x+18,y). Since our approach hasn't yet fused the logical 1D vector operations to exploit all columns of the 2D SIMD datapath, computing  <ref type="table" target="#tab_3">II</ref>, and its connected components to construct larger vector loads.</p><p>the data selection parameters is deferred to a later step (section III-D). After replacing each individual vector load with its corresponding larger load, the running example results in having only three larger vector loads instead of twelve individual vector loads for the tensor I.</p><p>3) Realizing scalar broadcasts. Similar to unaligned vector loads, vectorization of tensor convolutions involve scalar operands and require the support for scalar to vector broadcast operation, for, e.g., the scalar operand W(0,0) in Table <ref type="table" target="#tab_3">II</ref>. A naive approach to realize the broadcast operation of a scalar operand is by loading an aligned vector covering the operand and then using the shuffle network to select the the operand for all the lanes. A downside of the above approach is that it may result in loading an entire vector while using only one value fetched from memory.</p><p>The scalar operands in the tensor convolutions typically exhibit significant spatial locality, e.g., the scalar operands such as W(0,0) and W(1,0) in Table II are contiguous in the data memory. Similar to our approach in exploiting vector register reuse, we construct another reuse graph to identify scalar operands that are adjacent in data memory and can be subsumed as part of a single vector load. For instance, the operands W(0,0), W(1,0), (2,0), W(3,0) can be realized over a vector load (say V2) of W(0:7, 0).</p><p>We represent the data selection of a set of values from a vector register using the shuffle network during a vector operation as SELECT(V, {s ij })) where V represents the vector register and s ij denotes the index of the required element in the register V for the i th lane and j th column multiplier in the 2D SIMD datapath. Our approach defers the computation of data selection parameters to the next step. The triplet representation after realizing the unaligned loads, scalar broadcasts, and exploiting vector register reuse is shown in table III.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. 2D Vector SIMD Datapath</head><p>A key distinguishing feature of the AI Engine relative to the traditional SIMD units is the presence of a twodimensional SIMD datapath which performs reduction across all columns of a SIMD lane. A single 1D logical vector operation can occupy a single column of 2D datapath, but the vector operations on the 2D SIMD datapath require using all the columns of the datapath and don't allow partial utilization. Hence, our approach identifies and logically groups (fusing) all suitable 1D logical vector operations that contribute to the same output through accumulation/reduction and use the same set of vector register operands. The identification is done by searching in the triplet representation for operations having the same update operand and the same set of vector registers as multiplication operands. Finally, our approach partitions the logical groups based on the number of columns available for the given operand type and also constraints imposed by the shuffle network on the data selection over vector register operands. If the data selection required for the operands of fused vector operations is incompatible with the constraints of the shuffle network, then our approach generates a compilation error and prunes that candidate code variant. There are four valid fusible logical 1D operations for each row of the filter in Table <ref type="table" target="#tab_3">II</ref>, our approach groups them into two fused vector operations whose overview is described in fig. <ref type="figure" target="#fig_7">6</ref>. Furthermore, the triplet representation after fusing the logical 1D vector operations is shown in Table <ref type="table" target="#tab_6">IV</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Code Generation</head><p>Our approach extends the code generation capabilities in the Halide <ref type="bibr" target="#b11">[11]</ref> by implementing a code generator for the triplet representation to generate explicitly vectorized code using AI Engine intrinsic functions. A naive approach to code generation can be implemented by first emitting all vector loads, followed by all vector MAC operation, and then finally all vector stores. However, this naive approach results in variables (loads) having large live ranges, possibly leading to register spills and preventing software pipelining. Furthermore, optimization of memory accesses can be challenging for the downstream compilers only given the generated intrinsic code. Hence, our approach reorders memory accesses and interleaves them with vector MAC operations during the code generation process to reduce the live range of each variable. This process is relatively easy given the information about memory access patterns in Halide and helps the downstream compilers to improve packing of stores, loads, and vector MACs into VLIW instructions. A snippet of the final code generated by our approach with interleaving of loads, vector operations, and stores over the running example is shown in fig. <ref type="figure">7</ref>.  <ref type="figure" target="#fig_5">VSTORE(V2, O, x:x+15,y</ref>); } Fig. <ref type="figure">7</ref>. A snippet of the generated 16-bit vector code for the running example in fig. <ref type="figure" target="#fig_3">3</ref>. VLOAD/VMUL/VMAC/VSTORE refers to vector load, vector multiplication, vector multiply-and-accumulate, and vector store. SELECT symbolically represents the data selection over a vector register for the i th row and j th column of 2D datapath multipliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Auto-tuner</head><p>Steps 1-5 in our multi-step compiler approach generates the vectorized code for a given specification of tensor convolution, a schedule from the auto-tuner, and workload sizes. The auto-tuning capabilities of the Halide framework support only multi-staged pipelines <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, but our focus is only on a single stage for the convolution. Hence, we implemented custom auto-tuner in our approach exploring all possible schedules to find the best schedule for a given convolution specification and the workload sizes. The search space of schedules include loop nest and data-layout optimizations. Search space. The space of loop transformations include loop interchange, loop unroll and jamming, and the choice of loop for vectorization. The space of data-layout optimizations include dimension permutation and data tiling. Exploration. Our approach applies the following pruning strategies: 1) unrolling of reduction loops to avoid memory traffic in writing and reading intermediate (partial) results, and 2) applying bounds on the unroll and jam factors to avoid code size explosion (AI Engine has only 16KB program memory) and also to avoid longer compilation times. Our auto-tuner evaluates each point in the pruned search space by generating the vectorized C-code, compiling with the AI Engine compiler, and executing it using a cycle-accurate architecture simulator. With performance as the primary optimization goal, our approach obtained a geometric mean performance improvement of 1.10× fewer cycles than the expert-written and tuned codes available for four workloads, showing that automatic exploration can find useful design points which are not obvious to humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluated our approach over a total of 36 workloads involving a wide variety of operators and variations of CONV2D and CONV3D over two operand precisions (32-bit and 16-bit) on a single AI Engine. Each workload represents a unique combination of a convolution operation, tensor shapes, and operand precision. The configuration is shown in Table <ref type="table" target="#tab_6">V</ref> and includes a 128KB local memory pre-loaded with all the data required for the evaluation of each workload. The configuration also includes a vector register file of size 256B (a total of 16 registers with each size as 128 bits) in between the SIMD datapath and the local memory. We used the AI Engine's cycle-accurate simulator to evaluate the functionality and performance of our generated codes. We define the performance (MACs/Cycle) of an implementation of a tensor convolution as the total number of MAC operations in the convolution divided by the total number of execution cycles taken by the implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CONV2D in Computer Vision</head><p>In the following experiments, we compare two experimental variants: 1) Code written by an expert (for 3×3 and 5×5 filters) available as part of the Xilinx's AI Engine compiler infrastructure, 2) Code generated by our approach leveraging the auto-tuner. Both codes are designed to produce a 256×16 tile of a larger image. We observe from fig. <ref type="figure">8</ref> that our approach achieved a geometric mean performance improvement of 1.10× from the Halide codes compared with the available expert-written codes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MACs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expert-Written</head><p>Our approach with auto-tuner AI Engine Peak Fig. <ref type="figure">8</ref>. Comparison of our approach with auto-tuner against the available expert-written codes for CONV2D operation with 3×3 and 5×5 filters.</p><p>The auto-tuner of our approach was able to find better schedules than used in the expert-written codes (roof-line graphs for the workloads is shown in fig. <ref type="figure" target="#fig_9">9</ref>), including nonunit unroll and jam factors along the image height (loop-y) dimension for better reuse. These non-unit factors also enabled more opportunities in the loop body for the downstream compilers to perform better software pipelining. Furthermore, since workload sizes are also expressed in the Halide codes, our approach annotated the loops of generated codes with pragmas about the loop sizes to help the downstream compilers, especially helping the automatic software pipelining to accurately estimate the pre-amble and post-amble set up overheads and generate better VLIW code. Such overheads can be significant, particularly for tiled inner loops executed many times. In case of the 3x3 and 5x5 filters with 16-bit operands, the total number of fusible logical 1D vector multiplications corresponding to each row of the filters is an odd number. Hence, our approach padded the filters with an additional column to generate even number of fusible 1D operations and map onto the two columns present in the 2D SIMD datapath for 16-bit types. But, expert-written codes fused logical 1D vector multiplications corresponding to different rows of the filters, there by avoiding the padding. This was accomplished by carefully merging the required input image data from different rows into a single vector register. Our approach currently doesn't exploit this optimization strategy and would require additional analysis to enable it. However, we see that the code generated using our approach is still able to perform better than the expert-written code by leveraging loop unroll and jam transformations. Our approach with auto-tuner for 32-bit types (AI Engine Peak : 8 MACs/cycles) Our approach with auto-tuner for 16-bit types (AI Engine Peak : 32 MACs/cycles) Fig. <ref type="figure" target="#fig_1">10</ref>. Performance of our approach generated codes for CONV2D workloads of Computer Vision over filter sizes from 2 to 11.</p><p>In addition to the 3x3 and 5x5 filters, we have evaluated other filter sizes commonly used in Computer Vision applications. Table VI presents those workload sizes, total MAC operations involved in each workload, and optimal schedules reported by the auto-tuner. We padded each non-even sized 16-bit filter with an additional column for evaluation, but we used the MACs obtained by the filter without padding while computing the performance (MACs/cycle). As can be observed from fig. <ref type="figure" target="#fig_1">10</ref>, our approach achieved a geometric mean performance of 7.67 and 25.92 MACs/cycle for 32-bit and 16-bit types respectively for the workloads in Table <ref type="table" target="#tab_9">VI</ref>. The auto-tuner chose the loop-x for vectorization for all the workloads, because it has more reuse opportunities and has larger number of iterations compared to the loop-y. The optimal unroll and jam factors are not the same for all the workloads and also vary for different precisions of the same filter size. Even though increasing unroll and jam factors improve the reuse opportunities, but it often resulted in register spills after a threshold and also interfered with software pipelining of inner loops. Furthermore, larger unroll and jam factors along the loop-x resulted in larger connected components of the reuse graph and required larger vector register than the maximum possible (e.g., 1024b for 32-bit operands) in the hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CONV2D in Deep Learning</head><p>We considered a wide variety of CONV2D operations in the deep learning domain such as regular (REG) CONV2D over various filter sizes, point-wise (PW), spatially separable (SS), depth-wise separable (DS), and fully-connected (FC) operations. Table VII presents those workload sizes (with unit batch size, i.e., N = 1), total MAC operations involved in each workload, and optimal schedules reported by the auto-tuner. Since the memory footprint of typical CONV2D operations don't fit into the local memory, we chose the similar output and input tensor memory footprint used in The auto-tuner chose either loop-x or loop-k for vectorizing the workloads, because the number of iterations of remaining loops are smaller than the vector length. The autotuner identified the vectorization along the loop-x to be beneficial for the REG-5x5, REG-7x7 workloads, because there exists more opportunities for vector register reuse (convolutional reuse) along the loop-x with the larger kernels sizes. But, for the workloads such as PW (REG-1x1), FC that have either less or no convolutional reuse along the loop-x, the vectorization was performed on the loop-k.</p><p>In these workloads, there exists an even number of fusible logical 1D vector multiplications corresponding to the filter channels, hence our approach didn't require any padding to the filter tensors (unlike in Table <ref type="table" target="#tab_9">VI</ref>), except for the depth-wise CONV2D workload which has only one channel. However, the data-layouts of these workload tensors need to be modified to support the fusion of 1D logical vector multiplications along the channels. An example data-layout for the input and weights of the 16-bit REG-3x3 workload for the fusion along channels is shown in fig. <ref type="figure" target="#fig_10">12</ref>, where the data-layout scheme for the input  <ref type="table" target="#tab_11">VII</ref>), to enable the fusion of 1D logical vector multiplications along the channels, there by avoiding the padding required for weights.</p><p>tensor (C/2)Y'X'(2) refers to first laying out a block of two channels followed by width, height, and remaining channels.</p><p>Along with the advantages of avoiding padding, data-layouts can be used for exploring better schedules as well. Such datalayout schemes over the workload tensors should respect two constraints: 1) The required number of data elements of each operand of the fused vector multiplication should fit into the maximum vector register size (e.g., 32 unique 16-bit input data elements for the vector multiplication in fig. <ref type="figure" target="#fig_10">12</ref> can fit into a 1024b vector register which is the maximum), and 2) The required data selection parameters over the vector register should respect the shuffle network constraints. Our auto-tuner was able to automatically explore a variety of such valid datalayout schemes in our evaluation. Although the resulting datalayouts can be implemented by the architecture, they can be rather complex and non-intuitive (e.g., (K/16)SR(C/2)(16)(2) in Table <ref type="table" target="#tab_11">VII</ref>). Manually identifying such a data layout and writing the corresponding instrisic-based code is extremely challenging and error-prone, even for experts, thereby demonstrating the benefits of our automatic approach.</p><p>In Table <ref type="table" target="#tab_11">VII</ref>, we see that the arithmetic intensity of the FC workload for the 16-bit is to the left-side of the inflection point of the roof-line graph of the AI engine, indicating memorybound execution. This is expected, since the FC workload has little opportunity for data reuse within a single convolution operation. The workload peak performance based on its arithmetic intensity is 21.22 MACs/cycle, and our approach achieved 15.77 MACs/cycle or 75% of the workload peak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CONV3D</head><p>In this evaluation, we focused on the simpler CONV3D workloads to further demonstrate the applicability of our approach. The output sizes in these workloads are the same as in the CONV2D workloads in Table VII, i.e., 168x2x16, and the weight tensor sizes are 3x3x3, 5x5x5, and 7x7x7 which are popular in the 3D CNN models <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>. Since the number of fusible 1D logical vector multiplications corresponding to any dimension of the weight tensor in these workloads are odd, we have padded the weights with an additional column for each row. With this padding, our approach achieved a geometric mean performance of 7.55 and 21.60 MACs/cycle for 32-bit and 16-bit types, respectively shown in fig. <ref type="figure" target="#fig_3">13</ref>. Our approach with auto-tuner for 32-bit types (AI Engine Peak: 8 MACs/cycles) Our approach with auto-tuner for 16-bit types (AI Engine Peak: 32 MACs/cycles) Fig. <ref type="figure" target="#fig_3">13</ref>. Performance of our approach generated codes for CONV3D workloads with weight sizes as 3x3x3, 5x5x5, 7x7x7.</p><p>Overall, our evaluation over all the workloads shows geometric means of 7.6 and 23.3 MACs/cycle for 32-bit and 16bit operands (which represent 95.9% and 72.8% of the peak performance respectively). This difference in efficiency is not surprising, since it is more challenging to utilize two columns in the SIMD data path in the case of 16-bit operands, compared to a single column in the case of 32-bit operands. However, the absolute performance in the 16-bit case is still significantly higher than the 32-bit case, despite a lower efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>High-level and domain-specific compiler frameworks <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b37">[37]</ref>- <ref type="bibr" target="#b43">[43]</ref> have been shown to improve the productivity of application programmers, while generating high-performance code for a variety of architectures including CPUs, GPUs, FPGAs, Spatial accelerators, and distributed systems. Notably, the Halide framework <ref type="bibr" target="#b11">[11]</ref> for image processing pipelines has gained popular attention in the academic and industrial world. Recently, Vocke et al. <ref type="bibr" target="#b44">[44]</ref> extended the Halide framework to support specialized Digital Signal Processors (DSPs), mainly focusing on SIMD instruction sets and heterogeneous scratchpad memories of the Intel Imaging Processing Units (IPUs). Furthermore, Halide has the support for the Hexagon Vector eXtensions (HVX) on the Qualcomm Hexagon DSP processors. However, none of the above prior work focused on targeting the 2D SIMD datapaths and the shuffle interconnection networks, which are unique to the AI Engine.</p><p>To the best of our knowledge, the only prior work on autovectorizing for a 2D SIMD datapath is the work by Dasika et al. <ref type="bibr" target="#b2">[3]</ref>, where the authors have proposed a greedy compiler approach implemented as an extension to Trimaran <ref type="bibr" target="#b45">[45]</ref> compiler, to identify a sequence of back to back vector operations for execution on their PEPSC's architecture chained FPUs. But, our approach identifies a group of such back to back dependent (i.e., fusible 1D logical) vector operations by searching in the triplet representation, a simplified and symbolic view of the convolution loop body.</p><p>Exploiting vector register reuse (including partial reuse) on SIMD units is a vital optimization to achieve highperformance, and prior work exploited the reuse by shuffling the vector registers using the data manipulation/shuffle units <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref>. But, our approach constructs a larger vector load covering the loads having reuse and uses the AI Engine's unique shuffle network to select the desired elements. Furthermore, our approach uses the shuffle network to address the unaligned vector loads and scalar broadcasts without requiring any additional hardware support.</p><p>The vector codes generated by our approach are viewed as high-performance primitives that are intended to execute on a single AI Engine. These primitives are composed and integrated by a high-level compiler to run larger tensor convolutions across multiple AI Engines. Some of the prior works that have followed the similar strategy of automating the library/primitive development for the performance-critical kernels are SPIRAL <ref type="bibr" target="#b48">[48]</ref> for the domain of linear transforms, ATLAS <ref type="bibr" target="#b49">[49]</ref> for the basic linear algebra subroutines (BLAS), and FFTW <ref type="bibr" target="#b50">[50]</ref> for the discrete Fourier transforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS &amp; FUTURE WORK</head><p>In this work, we introduced Vyasa, a high-level programming system built on the Halide framework, to generate highperformance vector codes for the tensor convolutions onto the Xilinx Versal AI Engine. Our proposed multi-step compiler approach leverages the AI Engine's unique capabilities of the 2D SIMD datapath and the shuffle interconnection networks to achieve close to the peak performance for various workloads. Manually identifying best schedules and writing the corresponding intrinsic-based code is extremely challenging and error-prone, even for experts, thereby demonstrating the benefits of our automatic approach. Our results show geometric means of 7.6 and 23.3 MACs/cycle for 32-bit and 16bit operands (which represent 95.9% and 72.8% of the peak performance respectively). For four of these workloads for which expert-written implementations were available to us, Vyasa achieved a geometric mean performance improvement of 1.10× from Halide code that is around 50× smaller than the expert-written C/C++ code. In the future, we plan to extend our system to other computationally expensive linear algebra kernels. Also, we plan to integrate the generated highperformance codes into a high-level compiler to run larger tensor convolutions across multiple AI Engines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Operator</head><label></label><figDesc>Constraints on CONV2D Point-wise (PW)Filter width = Filter height = 1Fully-connected (FC)Filter width = Input width Filter height = Input height Spatially separable (SS)Filter width = 1 or Filter height = 1 Depth-wise separable (DS) Input channels = Filter channels = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A pictorial overview of the key architectural features of the Xilinx AI Engine, i.e., 2D vector SIMD datapath and shuffle network.</figDesc><graphic url="image-1.png" coords="3,127.29,597.82,98.00,74.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Buffer&lt;int16&gt; I(X',Y'); Buffer&lt;int16&gt; W(4,3); Var x, y; RDom r(4, 3); Func O; //output //(a) Description of the convolution computation O(x,y) += W(r.x, r.y) * I(x+r.x, y+r.y); //(b) A sample schedule: Unrolling reduction loops //Vectorizing loop corresponding to image width O.update().unroll(r.x, 4).unroll(r.y,3) .vectorize(x, 16); //(c) Intermediate code after lowering for y: for x: (vectorized) O(x:x+15,y) += W(0,0) * I(x:x+15,y); O(x:x+15,y) += W(1,0) * I(x+1:x+16,y); O(x:x+15,y) += W(2,0) * I(x+2:x+17,y); O(x:x+15,y) += W(3,0) * I(x+3:x+18,y); ......</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Algorithmic description of the convolution of a 4x3 filter over an input 2D image in the Halide language<ref type="bibr" target="#b11">[11]</ref>. A(a:b,c) is a short hand vector notation for denoting a contiguous slice from A(a,c) to A(b,c) in one direction.</figDesc><graphic url="image-8.png" coords="4,48.96,139.89,251.05,204.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A pictorial overview of the convolution of 4x3 filter based on the schedule described in fig. 3(b) at the loop iterations x = 0 and y = 0.</figDesc><graphic url="image-9.png" coords="4,311.98,522.51,251.04,121.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Reuse graph corresponding to the vector loads of the tensor I in TableII, and its connected components to construct larger vector loads.</figDesc><graphic url="image-10.png" coords="6,102.05,50.54,144.90,92.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>:x+15, y) SELECT(V2, { }) SELECT(V1, { }) SELECT(V2, { }) SELECT(V1, { }) SELECT(V2, { }) SELECT(V1, { }) SELECT(V2, { })SELECT(V1, { }) ....</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. An overview of the two fused vector operations (a and b) over the vector registers V1, V2 for input and weights, respectively of the running example shown in Table II at x=0 and y=0. The shuffle network of the AI Engine helps each multiplier of the 16 lanes and 2 columns of the 2D SIMD datapath to choose required elements from the vector registers.</figDesc><graphic url="image-11.png" coords="6,311.98,346.39,251.05,239.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>/</head><label></label><figDesc>/Generated code for(int y=0; y &lt; Y; y++) for(int x=0; x &lt; X; x+=16) { V1 = VLOAD(I,x:x+31,y); V2 = VLOAD(W,0:7); V3 = VMUL(V2, SELECT(V2, {j}), V1, SELECT(V1, {i+j})); V3 = VMAC(V3, V2, SELECT(V2,{j+2}), V1, SELECT(V1,{i+j+2}})); V4 = VLOAD(I,x:x+31,y+1); V3 = VMAC(V3, V2, SELECT(V4, {j+4}), V1, SELECT(V1, {i+j})); V3 = VMAC(V3, V2, SELECT(V4,{j+6}), V1, SELECT(V1,{i+j+2}})); ....</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Roof-line graphs of four workloads considered in fig. 8, where each data point is a schedule explored by the auto-tuner.</figDesc><graphic url="image-12.png" coords="8,48.96,461.22,251.06,135.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Data-layouts of input and weight tensors of the 16-bit REG-3x3 workload (TableVII), to enable the fusion of 1D logical vector multiplications along the channels, there by avoiding the padding required for weights.</figDesc><graphic url="image-13.png" coords="9,311.98,50.54,251.06,111.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and can improve energy efficiency by not writing intermediate values back to the register file. Furthermore, the number of columns is dependent on the operand precision. Operations on 32-bit types are organized as 8 lanes with 1 column, without internal reduction. Operations on 16-bit types are organized as either 16 lanes with 2 columns or 8 lanes with 4 columns. Operations on 8-bit types are organized as 16 lanes with 8 columns. As a result, the 2D datapath can perform either 8 MACs on 32-bit inputs, 32 MACs on 16-bit input, or 128 MACs on 8-bit input per cycle.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I TRIPLET</head><label>I</label><figDesc>REPRESENTATION OF THE LOOP BODY IN FIG. 3(C)</figDesc><table><row><cell>Update Operation</cell><cell cols="2">MAC Operations</cell></row><row><cell>Operand</cell><cell>Operand1</cell><cell>Operand2</cell></row><row><cell>O(x:x+15, y)</cell><cell>W(0, 0)</cell><cell>I(x:x+15, y)</cell></row><row><cell>O(x:x+15, y)</cell><cell>W(1, 0)</cell><cell>I(x+1:x+16, y)</cell></row><row><cell>O(x:x+15, y)</cell><cell>W(2, 0)</cell><cell>I(x+2:x+17, y)</cell></row><row><cell>O(x:x+15, y)</cell><cell>W(3, 0)</cell><cell>I(x+3:x+18, y)</cell></row><row><cell>..</cell><cell>..</cell><cell>..</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II TRIPLET</head><label>II</label><figDesc>REPRESENTATION AFTER THE LAZY STORES OPTIMIZATION</figDesc><table><row><cell>Update Operation</cell><cell cols="2">MAC Operations</cell></row><row><cell>Operand</cell><cell>Operand1</cell><cell>Operand2</cell></row><row><cell></cell><cell>W(0, 0)</cell><cell>I(x:x+15, y)</cell></row><row><cell>O(x:x+15, y)</cell><cell>W(1, 0) W(2, 0)</cell><cell>I(x+1:x+16, y) I(x+2:x+17, y)</cell></row><row><cell></cell><cell>W(3, 0)</cell><cell>I(x+3:x+18, y)</cell></row><row><cell></cell><cell>..</cell><cell>..</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III TRIPLET</head><label>III</label><figDesc>REPRESENTATION AFTER ADDRESSING UNALIGNED LOADS, SCALAR BROADCAST, AND EXPLOITING VECTOR REGISTER REUSE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V THE</head><label>V</label><figDesc>AI ENGINE CONFIGURATION USED IN OUR EVALUATION.</figDesc><table><row><cell>Parameter</cell><cell>32-bit</cell><cell>16-bit</cell></row><row><cell>2D SIMD data path</cell><cell>8 x 1</cell><cell>16 x 2</cell></row><row><cell>Peak compute</cell><cell cols="2">8 MACs/cycle 32 MACs/cycle</cell></row><row><cell>Scratchpad memory</cell><cell cols="2">128 KB @ 96B/cycle</cell></row><row><cell>Scratchpad memory ports</cell><cell cols="2">32B 2 read and 1 write</cell></row><row><cell>Vector register file</cell><cell>256 B</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table VI .</head><label>VI</label><figDesc>As can be observed from fig.11, our approach achieved a geometric mean performance of 7.67 and 22.53 MACs/cycle for 32-bit and 16-bit types respectively for the workloads in TableVII.</figDesc><table><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell>28.37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>26.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MACs/Cycle</cell><cell>15 20 25</cell><cell>22.62</cell><cell></cell><cell></cell><cell>24.94</cell><cell>22.47</cell><cell>22.34</cell><cell>19.69</cell><cell>15.77</cell><cell>22.53</cell></row><row><cell></cell><cell>10</cell><cell>7.19</cell><cell>7.45</cell><cell>7.83</cell><cell>7.75</cell><cell>7.89</cell><cell>7.88</cell><cell>7.46</cell><cell>7.94</cell><cell>7.67</cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>REG-3x3</cell><cell>REG-5x5</cell><cell>REG-7x7</cell><cell>PW-1x1</cell><cell>SS-1x3</cell><cell>SS-3x1</cell><cell>DS-3x3</cell><cell>FC-1x1</cell><cell>Geo. Mean</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Our approach with auto-tuner for 32-bit types (AI Engine Peak: 8 MACs/cycles) Our approach with auto-tuner for 16-bit types (AI Engine Peak: 32 MACs/cycles)</head><label></label><figDesc>Fig. 11. Performance of our approach generated codes for CONV2D workloads (shown in Table VII) of Deep Learning.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII CONV2D</head><label>VII</label><figDesc>WORKLOADS OF DEEP LEARNING USED IN OUR EVALUATION (VARIABLE NAMES DESCRIBED IN SECTION II) AND OPTIMAL SCHEDULES.</figDesc><table><row><cell></cell><cell>CONV type</cell><cell>Output (O) size (XxYxK)</cell><cell>Filter (F) size (RxSxCxK)</cell><cell>Input (I) size (X'xY'xC)</cell><cell>#MACs</cell><cell>Precision</cell><cell>O</cell><cell cols="4">Optimal schedules from the auto-tuner Data layouts Vector loop SW loop W I</cell><cell cols="2">Unroll and Jam factors x y k</cell><cell>Loop order</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3x3x8x16</cell><cell>144x4x8</cell><cell>294912</cell><cell>32-bit 16-bit</cell><cell>XYK KYX</cell><cell>(K/8)(C/8)SR(8)(8) K(C/2)SR(2)</cell><cell>(C/8)Y'X'(8) (C/2)Y'X'(2)</cell><cell>k x</cell><cell>x x</cell><cell>1 2 1 1</cell><cell>1 1</cell><cell>kyx yxk</cell></row><row><cell></cell><cell>(REG)</cell><cell></cell><cell>5x5x8x16</cell><cell>144x6x8</cell><cell>819200</cell><cell>32-bit 16-bit</cell><cell>KYX KYX</cell><cell>KCSR K(C/2)SR(2)</cell><cell>CY'X' (C/2)Y'X'(2)</cell><cell>x x</cell><cell>x x</cell><cell>1 1 1 2</cell><cell>1 1</cell><cell>kyx kyx</cell></row><row><cell></cell><cell></cell><cell></cell><cell>7x7x8x16</cell><cell>144x8x8</cell><cell>1605632</cell><cell>32-bit 16-bit</cell><cell>KYX KYX</cell><cell>KCSR K(C/2)SR(2)</cell><cell>CY'X' (C/2)Y'X'(2)</cell><cell>x x</cell><cell>x x</cell><cell>1 2 1 2</cell><cell>1 1</cell><cell>kyx kyx</cell></row><row><cell></cell><cell>(PW)</cell><cell>128x2x16</cell><cell>1x1x8x16</cell><cell>144x2x8</cell><cell>32768</cell><cell>32-bit 16-bit</cell><cell cols="2">XYK YXK (K/16)SR(C/2)(16)(2) (K/8)(C/8)SR(8)(8)</cell><cell>(C/8)Y'X'(8) Y'X'C</cell><cell>k k</cell><cell>x k</cell><cell>1 2 1 2</cell><cell>1 1</cell><cell>kyx xyk</cell></row><row><cell></cell><cell>(SS)</cell><cell></cell><cell>1x3x8x16 3x1x8x16</cell><cell>144x4x8 144x2x8</cell><cell>98304 98304</cell><cell>32-bit 16-bit 32-bit 16-bit</cell><cell cols="2">XYK KYX XYK YXK (K/16)SR(C/2)(16)(2) (K/8)(C/8)SR(8)(8) K(C/2)SR(2) (K/8)(C/8)SR(8)(8)</cell><cell>(C/8)Y'X'(8) (C/2)Y'X'(2) (C/8)Y'X'(8) Y'X'C</cell><cell>k x k k</cell><cell>p x x k</cell><cell>1 2 1 2 1 1 1 2</cell><cell>1 1 1 1</cell><cell>kyx kyx kyx xyk</cell></row><row><cell></cell><cell>(DS)</cell><cell></cell><cell>3x3x16x16</cell><cell>144x4x16</cell><cell>36864</cell><cell>32-bit 16-bit</cell><cell>KYX KYX</cell><cell>KCSR KCSR</cell><cell>CY'X' CY'X'</cell><cell>x x</cell><cell>x x</cell><cell>1 2 1 2</cell><cell>1 1</cell><cell>kyx kyx</cell></row><row><cell></cell><cell>(FC)</cell><cell>4096x1x1</cell><cell>1x1x8x4096</cell><cell>16x1x8</cell><cell>32768</cell><cell>32-bit 16-bit</cell><cell cols="2">XYK YXK (K/16)SR(C/2)(16)(2) (K/8)(C/8)SR(8)(8)</cell><cell>(C/8)Y'X'(8) Y'X'C</cell><cell>k k</cell><cell>k k</cell><cell>1 1 1 1</cell><cell>1 1</cell><cell>kyx xyk</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MACs/Cycle</cell><cell>15 25 30 20</cell><cell>19.65</cell><cell>20.49</cell><cell></cell><cell>25.02</cell><cell>21.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>7.69</cell><cell>7.22</cell><cell>7.76</cell><cell></cell><cell>7.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>3x3x3</cell><cell>5x5x5</cell><cell cols="2">7x7x7</cell><cell>Geo. Mean</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Vyasa means "compiler" in the Sanskrit language, and also refers to the sage who first compiled the Mahabharata.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Since the AI Engine architecture was developed for real-time processing applications which require deterministic performance, the simulator results are reliably correlated with actual performance of the AI Engine hardware.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The ARM Scalable Vector Extension</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boettcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eapen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eyole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gabrielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horsnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Magklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Premillieu</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2017.35</idno>
		<ptr target="https://doi.org/10.1109/MM.2017.35" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="26" to="39" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SODA: A High-Performance DSP Architecture for Software-Defined Radio</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Flautner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="123" />
			<date type="published" when="2007-01">Jan 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PEPSC: A Power-Efficient Processor for Scientific Computing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Parallel Architectures and Compilation Techniques</title>
				<imprint>
			<date type="published" when="2011-10">Oct 2011</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AnySP: Anytime Anywhere Anyway Signal Processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Woh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Flautner</surname></persName>
		</author>
		<idno type="DOI">10.1145/1555815.1555773</idno>
		<ptr target="https://doi.org/10.1145/1555815.1555773" />
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="128" to="139" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic SIMD Vectorization of Fast Fourier Transforms for the Larrabee and AVX Instruction Sets</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Mcfarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Arbatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1995896.1995938</idno>
		<ptr target="https://doi.org/10.1145/1995896.1995938" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Supercomputing, ser. ICS &apos;11</title>
				<meeting>the International Conference on Supercomputing, ser. ICS &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating SIMD Vectorized Permutations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint European Conferences on Theory and Practice of Software 17th International Conference on Compiler Construction, ser. CC&apos;08/ETAPS&apos;08</title>
				<meeting>the Joint European Conferences on Theory and Practice of Software 17th International Conference on Compiler Construction, ser. CC&apos;08/ETAPS&apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Verlag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Customizing Wide-SIMD Architectures for H.264</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Systems, Architectures, Modeling and Simulation, ser. SAMOS&apos;09</title>
				<meeting>the 9th International Conference on Systems, Architectures, Modeling and Simulation, ser. SAMOS&apos;09</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<idno>v1.0.2</idno>
		<ptr target="https://www.xilinx.com/support/documentation/whitepapers/wp506-ai-engine.pdf" />
	</analytic>
	<monogr>
		<title level="j">Xilinx AI Engines and Their Applications</title>
		<imprint>
			<date type="published" when="2018">10 2018</date>
		</imprint>
	</monogr>
	<note>Xilinx</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xilinx adaptive compute acceleration platform: VersalTM architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gaitonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="84" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<idno>v1.0.1</idno>
		<ptr target="https://www.xilinx.com/support/documentation/whitepapers/wp505-versal-acap.pdf" />
		<title level="m">First Adaptive Compute Acceleration Platform (ACAP), Xilinx</title>
				<imprint>
			<biblScope unit="page" from="9" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PolyMage: Automatic Optimization for Image Processing Pipelines</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mullapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="429" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Programming heterogeneous systems from an image processing DSL</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, ser. OSDI&apos;18. USA: USENIX Association</title>
				<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, ser. OSDI&apos;18. USA: USENIX Association</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="579" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">cuDNN: Efficient Primitives for Deep Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno>abs/1410.0759</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimizing FPGA-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing loop operation and dataflow in FPGA acceleration of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vrudhula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays (FPGA)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">mRNA: Enabling Efficient Mapping Space Exploration on a Reconfigurable Neural Accelerator</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<meeting>2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Caffeine: Towards uniformed representation and acceleration for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>TCAD)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MARVEL: A Decoupled Model-driven Approach for Efficiently Mapping Convolutions on Spatial DNN Accelerators</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Haridas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.07752" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2002">2002.07752. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Intel nGraph: An Intermediate Representation, Compiler, and Executor for Deep Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhiwandiwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bobba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brookhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Convey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kanawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Korovaiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Lishka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Vectorization and Register Reuse in High Performance Computing</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Stock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>The Ohio State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">StVEC: A Vector Instruction Extension for High Performance Stencil Computation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sedaghati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Teodorescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Parallel Architectures and Compilation Techniques</title>
				<imprint>
			<date type="published" when="2011-10">Oct 2011</date>
			<biblScope unit="page" from="276" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Computer Vision</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stockman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Software Pipelining: An Effective Scheduling Technique for VLIW Machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.1145/53990.54022</idno>
		<ptr target="https://doi.org/10.1145/53990.54022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation, ser. PLDI &apos;88</title>
				<meeting>the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation, ser. PLDI &apos;88<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatically Scheduling Halide Image Processing Pipelines</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mullapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<idno type="DOI">10.1145/2897824.2925952</idno>
		<ptr target="https://doi.org/10.1145/2897824.2925952" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to Optimize Halide with Tree Search and Random Programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306346.3322967</idno>
		<ptr target="https://doi.org/10.1145/3306346.3322967" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A survey on Deep Learning Advances on Different 3D Data Representations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E R</forename><surname>Shabayek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cherenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An End-to-end 3D Convolutional Neural Network for Action Detection and Segmentation in Videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PolyMage: Automatic Optimization for Image Processing Pipelines</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mullapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<idno type="DOI">10.1145/2694344.2694364</idno>
		<ptr target="https://doi.org/10.1145/2694344.2694364" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;15</title>
				<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="429" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Del Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</title>
				<meeting>the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Next 700 Accelerated Layers: From Mathematical Expressions of Network Computation Graphs to Accelerated GPU Kernels, Automatically</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3355606</idno>
		<ptr target="https://doi.org/10.1145/3355606" />
	</analytic>
	<monogr>
		<title level="j">TACO</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, ser. OSDI&apos;18. USA: USENIX Association</title>
				<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, ser. OSDI&apos;18. USA: USENIX Association</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="579" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A DSL Compiler for Accelerating Image Processing Pipelines on FPGAs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<idno type="DOI">10.1145/2967938.2967969</idno>
		<ptr target="https://doi.org/10.1145/2967938.2967969" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Parallel Architectures and Compilation, ser. PACT &apos;16</title>
				<meeting>the 2016 International Conference on Parallel Architectures and Compilation, ser. PACT &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="327" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Diesel: DSL for Linear Algebra and Neural Net Computations on GPUs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Elango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sandanagobalane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Grover</surname></persName>
		</author>
		<idno type="DOI">10.1145/3211346.3211354</idno>
		<ptr target="https://doi.org/10.1145/3211346.3211354" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
				<meeting>the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">T2S-Tensor: Productively Generating High-Performance Spatial Hardware for Dense Tensor Computations</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="DOI">10.1109/FCCM.2019.00033</idno>
		<ptr target="https://doi.org/10.1109/FCCM.2019.00033" />
	</analytic>
	<monogr>
		<title level="m">27th IEEE Annual International Symposium on Field-Programmable Custom Computing Machines, FCCM 2019</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-01">April 28 -May 1, 2019, 2019</date>
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extending Halide to Improve Software Development for Imaging DSPs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Corporaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jordans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Corvino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nas</surname></persName>
		</author>
		<idno type="DOI">10.1145/3106343</idno>
		<ptr target="https://doi.org/10.1145/3106343" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An infrastructure for research in ILP</title>
		<author>
			<persName><surname>Trimaran</surname></persName>
		</author>
		<ptr target="http://www.trimaran.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Partial Elements Reuse of Vector Register in SIMD Mathematical Functions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advancements in Computing Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Optimization Technology in SIMD Mathematical Functions Based on Vector Register Reuse</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jinchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shaozhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 14th International Conference on High Performance Computing and Communication 2012 IEEE 9th International Conference on Embedded Software and Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1102" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Domain-specific library generation for parallel software and hardware platforms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Voronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Milder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Telgarsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hao Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>D'alberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>De Mesmay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M F</forename><surname>Hoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><surname>Püschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Symposium on Parallel and Distributed Processing</title>
				<imprint>
			<date type="published" when="2008-04">April 2008</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automated empirical optimizations of software and the ATLAS project</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petitet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PARALLEL COM-PUTING</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The Design and Implementation of FFTW3</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="231" />
			<date type="published" when="2005-02">Feb 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
