<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Learning Contrastive Representations for Learning with Noisy Labels</title>
				<funder ref="#_R9GJdnQ">
					<orgName type="full">Alzheimer&apos;s Association</orgName>
				</funder>
				<funder ref="#_zP8f2mx #_GvgfsAa">
					<orgName type="full">NSERC</orgName>
				</funder>
				<funder ref="#_StBJFYV #_AhcHkrn">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-03">3 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Western Ontario</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Liu</surname></persName>
							<email>shengliu@nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">NYU Center for Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>She</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">Ian</forename><surname>Mcleod</surname></persName>
							<email>aimcleod@uwo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Western Ontario</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boyu</forename><surname>Wang</surname></persName>
							<email>bwang@csd.uwo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Western Ontario</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On Learning Contrastive Representations for Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-03">3 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.01785v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks are able to memorize noisy labels easily with a softmax cross entropy (CE) loss. Previous studies attempted to address this issue focus on incorporating a noise-robust loss function to the CE loss. However, the memorization issue is alleviated but still remains due to the non-robust CE loss. To address this issue, we focus on learning robust contrastive representations of data on which the classifier is hard to memorize the label noise under the CE loss. We propose a novel contrastive regularization function to learn such representations over noisy data where label noise does not dominate the representation learning. By theoretically investigating the representations induced by the proposed regularization function, we reveal that the learned representations keep information related to true labels and discard information related to corrupted labels. Moreover, our theoretical results also indicate that the learned representations are robust to the label noise. The effectiveness of this method is demonstrated with experiments on benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The successes of deep neural networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref> largely rely on availability of correctly labeled large-scale datasets that are prohibitively expensive and time-consuming to collect <ref type="bibr" target="#b45">[46]</ref>. Approaches to addressing this issue includes: acquiring labels from crowdsourcing-like platforms or nonexpert labelers or other unreliable sources <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b54">55]</ref> but while theses methods can reduce the labeling cost, label noise is inevitable. Due to the over-parameterization of deep networks <ref type="bibr" target="#b18">[19]</ref>, examples with noisy labels can ultimately be memorized with a cross entropy loss <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, which is known as the memorization effect <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b52">53]</ref>, leading to poor performance <ref type="bibr" target="#b52">[53]</ref>. Therefore, it is important to develop methods that are robust to the label noise. Cross entropy (CE) loss is widely used as a loss function for image classification tasks due to its strong performance on clean training data <ref type="bibr" target="#b36">[37]</ref> but it is not robust to label noise. When labels in training data are corrupted, the performance drops <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Given the memorization effect of deep networks, training on noisy data with the CE loss results in the representations of the data clustered in terms of their noisy labels instead of the ground truth. Thus, the final layer of the deep networks cannot find a good decision boundary from these noisy representations.</p><p>To overcome the memorization effect, noise-robust loss functions have been actively studied in the literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55]</ref>. They aim to design noise-robust loss functions in a way such that they achieve small loss on clean data and large loss on wrongly labeled data. However, it has been empirically shown that being robust alone is not sufficient for a good performance as it also suffers from the underfitting problem <ref type="bibr" target="#b28">[29]</ref>. To address this issue, these noise-robust loss functions have to be explicitly or implicitly jointly used with the CE loss, which brings a trade-off between nonrobust loss and robust loss. As a result, the memorization effect is alleviated but still remains due to the non-robust CE loss.</p><p>In this paper, we tackle this problem from a different perspective. Specifically, we investigate contrastive learning and the effect of the clustering structure for learning with noisy labels. Owing to the power of contrastive representation learning methods <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>, learning contrastive representations has been extensively applied on various tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref>. The key component of contrastive learning is positive contrastive pair (x 1 , x 2 ). Training a contrastive objective encourages the representations of x 1 , x 2 to be closer. In supervised classification tasks, correct positive contrastive pairs are formed by examples from the same class. When label noise exists, defining contrastive pairs in terms of their noisy labels results in adverse effects. Encouraging representations from different classes to be closer makes it even more difficult to separate images of different classes. Similar to our attempt to learn contrastive representations from noisy data, previous work has focused on reducing the adverse effects by re-defining contrastive pairs according to their pseudo labels <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. However, pseudo labels can be unreliable, and then wrong contrastive pairs are inevitable and can dominate the representation learning.</p><p>To address this issue, we propose a new contrastive regularization function that does not suffer from the adverse effects. We theoretically investigate benefits of representations induced by the proposed contrastive regularization function from two aspects. First, the representations of images keep information related to true labels and discard information related to corrupted labels. Second, we theoretically show that the classifier is hard to memorize corrupted labels given the learned representations, which demonstrates that our representations are robust to label noise. Intuitively, learning such contrastive representations of data helps combat the label noise. If data points are clustered tightly in terms of their true labels, then it makes the classifier hard to draw a decision boundary to separate the data in terms of their corrupted labels. We illustrate this intuition in Figure <ref type="figure" target="#fig_0">1</ref>. Our main contributions are as follows.</p><p>? We theoretically analyze the representations induced by the contrastive regularization function, showing that the representations keep information related to true labels and discard information related to corrupted labels. Moreover, we formally show that representations with insufficient corrupted label-related information are robust to label noise.</p><p>? We propose a novel algorithm over data with noisy labels to learn contrastive representations, and provide gradient analysis to show that correct contrastive pairs can dominate the representation learning.</p><p>? We empirically show that our method can be applied with existing label correction techniques and noiserobust loss functions to further boost the performance. We conduct extensive experiments to demonstrate the efficacy of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Theoretical Analysis</head><p>In this section, we first introduce some notations and we then investigate the benefits of representations learned by the contrastive regularization function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preliminaries</head><p>We use uppercases X, Y, . . . to represent random variables, calligraphic letters X , Y, . . . to represent sample spaces, and lowercases x, y, . . . to represent their realizations. Let X be input random variable and Y be its true label. We use ? to denote the wrongly-labeled random variable that is not equal to Y . The entropy of the random variable Y is denoted by H(Y ) and the mutual information of X and Y is I(X, Y ).</p><p>Contrastive learning aims to learn representations of data that only the data from the same class have similar representations. In this paper, we propose to learn the representations by introducing the following contrastive regularization function over all examples {(x i , y i )} from X ? Y and y i is the ground truth.</p><p>L ctr (x i , x j ) = -qi , zj + qj , zi 1{y i = y j }, <ref type="bibr" target="#b0">(1)</ref> where qk = q k q k 2 and zk = z k z k 2 . Following SimSiam <ref type="bibr" target="#b8">[9]</ref>, we define q = h(f (x)), z = stopgrad(f (x)), f is an encoder network consisting of a backbone network and a projection MLP, and h is a prediction MLP. Minimizing Eq. (1) on {(x i , y i ), (x j , y j )} pulls representations of x i and x j closer if y i = y j . The designs of the stop-gradient operation and h applied on representations are mainly to avoid trivial constant solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Benefits of Representations Induced by Contrastive Regularization</head><p>We first relate the solutions that minimize Eq. ( <ref type="formula">1</ref>) to a mutual information I(Z; X + ) = p(z, x + ) log p(z|x + ) p(z) dx + dz, where z = f (x) and x + is from the same class as x.</p><p>Theorem 1. Representations Z learned by minimizing Eq. (1) maximizes the mutual information I(Z; X + ).</p><p>Theorem 1 reveals the equivalence between the contrastive learning and mutual information maximization. Intuitively, Eq. (1) encourages to pull representations from the same class together and push those from different classes apart. The estimate of z conditioned on x + is more accurate than random guessing because the representation z of x is similar to the representation of x + . Thus the pointwise mutual information log p(z|x + ) p(z) increases by minimizing Eq. ( <ref type="formula">1</ref>).</p><p>We denote Z = arg max Z ? I(Z ? , X + ) by the representation that maximizes the mutual information, where Z ? is a representation of X parameterized by the neural network f with parameters ?. To understand what Z is learned from inputs and to show that Z is noise-robust, we introduce the notion of ( , ?)-distribution:</p><formula xml:id="formula_0">Definition 1 (( , ?)-distribution). A distribution D(X, Y, ? ) is called ( , ?)-Distribution if there exists ? &gt; 0 such that I(X; Y |X + ) ? ,<label>(2)</label></formula><p>and I(X;</p><formula xml:id="formula_1">? |X + ) &gt; ?.<label>(3)</label></formula><p>Eq. ( <ref type="formula" target="#formula_0">2</ref>) characterizes the connection between images and their true labels. If we already know an image X + , then there is the limited extra information related to the true label by additionally knowing X. We use a small number to restrict this additional information gain. Eq. ( <ref type="formula" target="#formula_1">3</ref>) characterizes the connection between those images and their corrupted labels. By knowing an additional image X + , the information X contains about its corrupted label ? is still larger than ?. The above condition ? &gt; 0 states that images from the same class are much more similar with respect to the true label than the corrupted label. As it is mentioned in <ref type="bibr" target="#b37">[38]</ref>, if there is a perfect prediction of Y given X + , then = 0.</p><p>We illustrate the intuitions behind Definition 1 in Figure <ref type="figure" target="#fig_7">2</ref>. We use the Grad-CAM <ref type="bibr" target="#b34">[35]</ref> to highlight the important regions in the images for predictions. The highlighted regions captured by the model are most related to labels. For images with the same clean labels, their information related to true labels are similar. For example, when Cat 1 and Cat 2 in Figure <ref type="figure" target="#fig_7">2</ref> are labeled as "cat", cat faces are captured as the true label-related information and they all look alike. For images with corrupted labels, their information related to corrupted labels are quite different. When Cat 1 and Cat 2 in Figure <ref type="figure" target="#fig_7">2</ref> are labeled as "dog", the windows bars captured as the corrupted label-related information for Cat 1 is different from the floor and wall for Cat 2.</p><p>With the notion of ( , ?)-distribution, the following theorem help us understand the benefits of representations Z in depth.</p><p>Theorem 2. Given a distribution D(X, Y, ? ) that is ( , ?)-Distribution, we have Given images X and their labels Y , the mutual information I(X; Y ) is fixed. The theorem states that the learned representations Z keep as much true label-related information as possible and discard much corrupted label-related information. Since the corrupted label-related information is discarded from the representations Z , memorizing the corrupted labels based on Z is diminished. Lemma 1 establishes the lower bound on the expected error on wronglylabeled data.</p><formula xml:id="formula_2">I(X; Y ) -? I(Z ; Y ) ? I(X; Y ),<label>(4)</label></formula><formula xml:id="formula_3">I(Z ; ? ) ? I(X; ? ) -? + . (<label>5</label></formula><formula xml:id="formula_4">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1. Consider a pair of random variables (X, ? ).</head><p>Let ? be outputs of any classifier based on inputs Z ? , and ? = 1{ ? = ? }, where 1{A} be the indicator function of event A. Then, we have</p><formula xml:id="formula_5">E[?] ? H( ? ) -I(Z ? ; ? ) -H(?) log | ?| -1</formula><p>.</p><p>Lemma 1 provides a necessary condition on the success of learning with noisy labels based on representation learning and sheds new light on this problem by highlighting the role of minimizing I(Z ? ; ? ). To see this, note that small I(Z ? ; ? ) implies robustness to label noise since E[?] is the expected error over the corrupted labels. On the other hand, when minimizing Eq. ( <ref type="formula">1</ref>), small I(Z ; ? ) can be achieved as indicated by the upper bound in Eq. ( <ref type="formula" target="#formula_19">12</ref>). In the meanwhile, the lower bound on I(Z ; Y ) in Eq. ( <ref type="formula" target="#formula_18">11</ref>) also shows that Z can retain the discriminative information of the data to avoid a trivial solution to I(Z ? ; ? ) minimization (i.e., Z ? is a constant representation).</p><p>While Lemma 1 combined with Theorem 2 indicates that Z is robust to label noise, the following Lemma shows that Z can also avoid underfitting. Specifically, it implies that that a good classifier achieved under the clean distribution can also be achieved based on our representations Z .</p><formula xml:id="formula_6">Lemma 2. Let R(X) = inf g E X,Y [L(g(X), Y )] be the minimum risk over the joint distribution X ? Y , where L(p, y) = Y i=1 y (i) log p (i) is a CE loss and g is a func- tion mapping from input space to label space. Let R(Z ) = inf g E Z ,Y [L(g (Z ), Y )]</formula><p>be the minimum risk over the joint distribution Z ? Y and g maps from representation space to label space. Then,</p><formula xml:id="formula_7">R(Z ) ? R(X) + .</formula><p>To show the robustness and performance of the contrastive (CTR) representation Z , we empirically compare it to the representation learned by the CE loss. We first use clean labels to train neural networks with different loss functions. Then we initialize the parameters of the final linear classifier and fine-tune the linear layer with noisy labels. We denote the memorization by the fraction of corrupted examples whose predictions are equal to their labels. Figure <ref type="figure" target="#fig_1">3</ref> illustrates the improved performance and robustness in terms of test accuracy and reduced memorization with the CTR representation.</p><p>Conventionally, the memorization of label noise increases as the training progresses <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44]</ref>. We remark that previous memorization is observed and proved in overparameterized models, where the ratio of the number parameters and the sample size is around 220. In their settings, the fraction of examples memorized by the model will increase. However, the memorization in our setting is measured on a linear classifier on top of frozen data representations, where the ratio of the number parameters and the sample size is around 0.1, which is under-parameterized. This explains why Figure <ref type="figure" target="#fig_1">3</ref> shows that the memorization decreases as the training progresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithm</head><p>In practice, as we are only given a noisy data set, we do not know if a label is clean or not. Consequently, simply minimizing Eq. ( <ref type="formula">1</ref>) can lead to deteriorated performances.</p><p>To see this, note that Eq. ( <ref type="formula">1</ref>) is activated only when 1{y i = y j } = 1. Thus, two representations from different classes will be pulled together when there are noisy labels.</p><p>Since deep networks first fit examples with clean labels and the probabilistic outputs of these examples are higher than examples with corrupted labels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>, one straightforward approach to tackle this issue is to replace the indicator function with a more reliable criterion 1{p i p j ? ? }: L ctr (x i , x j ) = -qi , zj + qj , zi 1{p i p j ? ? }, <ref type="bibr" target="#b5">(6)</ref> where p i is the probabilistic output produced by linear classifier on the representation of image x i and ? is a confidence threshold. However, minimizing Eq. ( <ref type="formula">6</ref>) only helps representation learning during the early stage. After that period, examples with corrupted labels will dominate the learning procedure since the magnitudes of gradient from correct contrastive pairs overwhelm that from wrong contrastive pairs. In particular, given two clean examples x i , x j with y i = y j and a wrongly labeled example x m with ?m = y i = y j , during the early stage, representations q i qj ? 1 and q i qm ? 0. After the early stage, deep networks starts to fit wrongly labeled data. At this moment, the wrong contrastive pairs (x i , x m ) and (x j , x m ) are wrongly pulled together and they impair the representation learning instead of the correct pair (x i , x j ):</p><formula xml:id="formula_8">?L ctr (x i , x m ) ?q i 2 2 = c i (1 -q i qm ?1 )<label>(7)</label></formula><formula xml:id="formula_9">c i (1 -q i qj ?0 ) = ?L ctr (x i , x j ) ?q i 2 2</formula><p>, where c i = 1/ q i 2 for simplicity. The proof is shown in supplementary materials.</p><p>To address this issue, we propose the following regularization function to avoid the negative effects from wrong contrastive pairs:</p><formula xml:id="formula_10">L ctr (x i , x j ) = log 1 -qi , zj + log 1 -qj , zi 1{p i p j ? ? } (8)</formula><p>Eq. ( <ref type="formula">8</ref>) still aims to learn similar representations for data with the same true labels. Since the maximum of Eq. ( <ref type="formula">8</ref>) is the same as that of Eq. ( <ref type="formula">1</ref>), our theoretical results about Z still hold. Moreover, the gradient analysis of Eq. ( <ref type="formula">8</ref>) is given by</p><formula xml:id="formula_11">? L ctr (x i , x j ) ?q i 2 2 = c i (1 + q i qj ),<label>(9)</label></formula><p>which indicates that the gradient in L2 norm increases if qi and qj approach to each other. In other words, the gradient from the correct pair (x i , x j ) is larger than the gradient from the wrong pair</p><formula xml:id="formula_12">(x i , x m ) (1 + q i qj &gt; 1 + q i qm ? 1)</formula><p>during the learning procedure. Compared to the gradient given by Eq. ( <ref type="formula" target="#formula_8">7</ref>), our proposed regularization function does not suffer from the gradient domination by wrong pairs. Meanwhile, the model does not overfit clean examples even though the gradients of Eq. ( <ref type="formula">8</ref>) from correct pairs are larger than wrong pairs. As Eq. ( <ref type="formula" target="#formula_8">7</ref>) describes the gradient with respect to the representation, its magnitude can be viewed as the strength of pulling clean examples from the same class closer, which is not directly related to overfitting to clean examples. Moreover, we use a separate linear layer on top of the representations as the classifier, thus as long as the gradients of the classification loss with respect to the parameters in the linear layer are not large on the clean examples, the model would not overfit to them. Finally, the overall objective function is given by</p><formula xml:id="formula_13">L = L ce + ? L ctr ,<label>(10)</label></formula><p>where L ctr serves as a contrastive regularization (CTRR) on representations and ? controls the strength of the regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We evaluate our method on two artificially corrupted datasets CIFAR-10 <ref type="bibr" target="#b14">[15]</ref> and CIFAR-100 <ref type="bibr" target="#b14">[15]</ref>, and two real-world datasets ANIMAL-10N <ref type="bibr" target="#b35">[36]</ref> and Cloth-ing1M <ref type="bibr" target="#b46">[47]</ref>. CIFAR-10 and CIFAR1-00 contain 50, 000 training images and 10, 000 test images with 10 and 100 classes, respectively. ANIMAL-10N has 10 animal classes and 50, 000 training images with confusing appearances and 5000 test images. Its estimated noise level is around 8%. Clothing1M has a million training images and 10, 000 test images with 14 classes. Its estimated noise level is around 40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise Generation.</head><p>For CIFAR-10, we consider two different types of synthetic noise with various noise levels. For symmetric noise, each label has the same probability of flipping to any other classes, and we randomly choose r training data with their labels to be flipped for r ? {20%, 40%, 60%, 80%, 90%}. For asymmetric noise, following <ref type="bibr" target="#b5">[6]</ref>, we flip labels between TRUCK?AUTOMOBILE, BIRD?AIRPLANE, DEER?HORSE, and CAT?DOG. we randomly choose 40% training data with their labels to be flipped according to the asymmetric labeling rule. For CIFAR-100, we also consider two different types of synthetic noise with various noise levels. The generation for symmetric label noise is the same as that for CIFAR-10 with the noise level r ? {20%, 40%, 60%, 80%}. To generate asymmetric label noise, we randomly sample 40% data and flip their labels to the next classes.</p><p>Baseline methods. To evaluate our method, we mainly compare our robust loss function to other robust loss function methods: 1) CE loss. 2) Forward correction <ref type="bibr" target="#b32">[33]</ref>, which corrects loss values by a estimated noise transition matrix. 3) GCE <ref type="bibr" target="#b54">[55]</ref>, which takes advantages of both MAE loss and CE loss and designs a robust loss function. 4) Co-teaching <ref type="bibr" target="#b16">[17]</ref>, which maintains two networks and uses small-loss examples to update. 5) LIMIT <ref type="bibr" target="#b17">[18]</ref>, which introduces noise to gradients to avoid memorization. 6) SLN <ref type="bibr" target="#b5">[6]</ref>, which adds Gaussian noise to noisy labels to combat label noise. 7) SL <ref type="bibr" target="#b41">[42]</ref>, which uses CE loss and a reverse cross entropy loss (RCE) as a robust loss function. 8) APL (NCE+RCE) <ref type="bibr" target="#b28">[29]</ref>, which combines two mutually boosted robust loss functions for training.</p><p>Implementation details. We use a PreAct Resnet18 as the encoder for CIFAR datasets, and Resnet18 as the encoder for the two real-world datasets. The project MLP and the prediction MLP are the same for all encoders. Following SimSiam <ref type="bibr" target="#b8">[9]</ref>, the projection MLP consists of 3 layers which have 2048 hidden dimensions and output 2048-dimensional embeddings. The prediction MLP consists of 2 layers which have 512 hidden dimensions and output 2048-dimensional embeddings. Following <ref type="bibr" target="#b6">[7]</ref>, we apply strong augmentations to learn data representations, where the strong augmentation includes Gaussian blur, color distortion, random flipping and random cropping. We use weak augmentations to optimize the cross-entropy loss, which includes random flipping and random cropping. More implementation details can be found in supplementary materials.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CIFAR Results</head><p>Table <ref type="table" target="#tab_1">1</ref> and Table <ref type="table" target="#tab_2">2</ref> show the results on CIFAR-10 and CIFAR-100 with various label noise settings. We use PreAct Resnet18 <ref type="bibr" target="#b18">[19]</ref> for all methods and report the best test accuracy for them based on three runs. Our method achieves the best performance on all tested noise settings. The improvement is more substantial when the noise level is higher. Especially when noise levels reach to 80% or even 90%, our method significantly outperforms other methods. For example, on CIFAR-10 with r = 90%, CTRR maintains a high accuracy of 81.65%, whereas the second best one is 49.65%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ANIMAL-10N &amp; Clothing1M Results</head><p>Table <ref type="table" target="#tab_3">3</ref> shows the results on the real-world datasets ANIMAL-10N and Clothing1M. All methods use the same model and the best results are reported over three runs. In  Our method is superior to other baselines on the two realworld datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effects of hyperparameters</head><p>The hyperparameter ? controls the strength of the regularization to representations of data. A weak regularization is not able to address the memorization issue, while a strong regularization makes the neural network mainly focus on optimizing the regularization term and ignoring optimizing the linear classifier. Figure <ref type="figure" target="#fig_2">4</ref> (left) shows the test accuracy with different ?. The results are in line with the expectation that too strong or too weak regularizations leads to poor performance.</p><p>The ? is the confidence threshold for choosing two examples from the same classes. When the score for the two examples exceeds the threshold, the two examples are considered as the correct pair. Many wrong pairs are selected if ? is set too low. Figure <ref type="figure" target="#fig_2">4</ref> (right) shows the test accuracy with different ? . When we are always confident about any pairs (? =0), the model performance is reduced significantly (? 20%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effects of regularization functions</head><p>To study the effect of the proposed regularization function, we compare the performance of Eq. ( <ref type="formula">8</ref>) to Eq. ( <ref type="formula">6</ref>). Empirical results are consistent with the previous gradient analysis and they are shown in Table <ref type="table" target="#tab_4">4</ref>. Our proposed regularization function Eq. ( <ref type="formula">8</ref>) outperforms Eq. ( <ref type="formula">6</ref>) by a large margin across all noise levels. Thus, learning data representations with Eq. ( <ref type="formula">8</ref>) can avoid wrong pairs dominating the representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Other contrasitve learning frameworks</head><p>Since the InfoMax principle <ref type="bibr" target="#b39">[40]</ref> of contrastive learning and the gradient analysis can apply to other contrastive learning frameworks, we apply CTRR to other contrastive learning frameworks. Table <ref type="table" target="#tab_5">5</ref> indicates the performances of our method with different contrastive learning frameworks, and it shows that our method is not limited to the SimSiam framework. Since BYOL leverages an additional exponential moving average model to learn representations, compared with SimSiam, the performance of our method with the BYOL framework performs better. CTRR works slightly worse under SimCLR than the other two frameworks. For the implementation, we simply replace the inner product of positive representations in SimCLR with our regularization function Eq. ( <ref type="formula">8</ref>) and the SimCLR objective function from negative pairs the same. We hypothesize that keeping the objective function from negative pairs the same could cause negative effects because of the label noise. A study on how negative pairs from SimCLR affects representation learning is beyond the scope of this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Combination with other methods</head><p>Furthermore, CTRR is orthogonal to label correction techniques <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b53">54]</ref>. In other words, our method can be integrated with these techniques to further boost learning performances. Specifically, we use the basic label correction strategy following <ref type="bibr" target="#b5">[6]</ref> that labels are replaced by weighted averaged of both model predictions and original labels, where weights are scaled sample losses. In Table <ref type="table" target="#tab_6">6</ref>, we show that the performance is improved after enabling a simple label correction technique.</p><p>Note that GCE <ref type="bibr" target="#b54">[55]</ref> is a partial noise-robust loss function implicitly combined with CE and MAE. It is of interest to re-validate the loss function GCE along with our proposed regularization function. We show the performance of a combination of our method and GCE <ref type="bibr" target="#b54">[55]</ref> in Table <ref type="table" target="#tab_7">7</ref>. With representations induced by our proposed method, there is a significant improvement on GCE, which demonstrates the effectiveness of the learned representations. Meanwhile, the success of this combination implies that our proposed method is beneficial to other partial noise-robust loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>In this section, we briefly review existing approaches for learning with label noise.</p><p>Noise-robust loss functions are designed to achieve a small error on clean data instead of corrupted data while training on the noisy training data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>. Mean absolute error (MAE) is robust to label noise <ref type="bibr" target="#b12">[13]</ref> but it is not able to solve complicated classification tasks. The determinant based mutual information loss L DMI is proved to be robust to label noise <ref type="bibr" target="#b48">[49]</ref> but it only works on the instanceindependent label noise. The generalized cross entropy (GCE) <ref type="bibr" target="#b54">[55]</ref> takes advantages of MAE and implicitly combined it with CE. The symmetric cross entropy (SL) <ref type="bibr" target="#b41">[42]</ref> designs a noise-robust reverse cross entropy loss and explicitly combines it with CE. However, they have not com-pletely addressed the issue as CE is prone to memorizing corrupted labels. The LIMIT <ref type="bibr" target="#b17">[18]</ref> proposes to add noise to gradients to address the memorization issue. SLN <ref type="bibr" target="#b5">[6]</ref> proposes to combat label noise by adding noise to labels of data. However, they may suffer from underfitting problems.</p><p>There are many different contrasitve regularization functions and architectures proposed to learn representations such as SimCLR <ref type="bibr" target="#b6">[7]</ref>, MoCo <ref type="bibr" target="#b7">[8]</ref>, BYOL <ref type="bibr" target="#b15">[16]</ref>, SimSiam <ref type="bibr" target="#b8">[9]</ref> and SupCon <ref type="bibr" target="#b19">[20]</ref>, where SupCon is to learn supervised representations with clean labels while others focus on learning self-supervised representations without labels. We aim to learn representations with noisy labels. We mainly follow the SimSiam framework, but our method is not limited to the SimSiam framework. Recently, some methods existing methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> leverage contrasitve representation learning to address noisy label problems. Compared to their methods, we theoretically analyze the benefits of learning such contrastive representations and we focus on addressing a fundamental issue of how to avoid wrong contrastive pairs dominating the representation learning.</p><p>There are many other methods for learning with noisy labels. Sample selection methods such as Co-teaching <ref type="bibr" target="#b16">[17]</ref>, Co-teaching+ <ref type="bibr" target="#b51">[52]</ref>, SELFIE <ref type="bibr" target="#b35">[36]</ref>, and JoCoR <ref type="bibr" target="#b42">[43]</ref> are selecting small loss examples to update models where they treat small loss examples as clean ones. Loss correction methods such as Forward/Backward method <ref type="bibr" target="#b32">[33]</ref> modify the sample loss based on a noise transition matrix. Some works propose to improve the estimation of the noise transition matrix such as T-Revision <ref type="bibr" target="#b44">[45]</ref> and Dual T <ref type="bibr" target="#b49">[50]</ref>. Label correction methods such as ELR <ref type="bibr" target="#b26">[27]</ref>, M-DYR-H <ref type="bibr" target="#b1">[2]</ref> and PENCIL <ref type="bibr" target="#b50">[51]</ref> replace noisy labels with pseudo-labels using different strategies. Methods like DivideMix <ref type="bibr" target="#b22">[23]</ref> combine the sample selection, label correction and semi-supervised techniques and empirically demonstrate their success to combat noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We present a simple but effective CTRR to address the memorization issue. Our theoretical analysis indicates that CTRR induces noise-robust representations without suffering from the underfitting problem. From algorithmic perspectives, we propose a novel regularization function to avoid adverse effects from wrong pairs. The empirical results also demonstrate the effectiveness of CTRR. On the one hand, we show the potential combinations of existing methods to improve the model performance. On the other hand, we evaluate our method under different contrastive learning frameworks. Both of them reveal the flexibility of our method and the importance of correctly regularizing data representations. We believe that CTRR can be jointly used with other existing methods to better solve machine learning tasks where there exists label noise.</p><p>A.3. Hyperparameters CIFAR. Our method has two hyperparameters ? and ? . For each noise setting for CIFAR-10, we select the best hyperparameters: ? from {50, 130} and ? from {0.4, 0.8}. For each noise setting for CIFAR-100, we select the best hyperparameters: ? from {50, 90} and ? from {0.05, 0.7}. The batch size is set as 256, and the learning rate is 0.02 using SGD with a momentum of 0.9 and a weight dacay of 0.0005.</p><p>ANIMAL-10N &amp; Clothing1M. For ANIMAL-10N, we set ? = 50, ? = 0.8 and batch size is 256. The learning rate is set as 0.04 with the same SGD optimizer as the CIFAR experiment. For Clothing1M, we set ? = 90, ? = 0.4 and batch size is 256. The learning rate is set as 0.06 with the same SGD optimizer as above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proofs of Theoretical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Proof for Theorem 1</head><p>Theorem. Representations Z learned by minimizing Eq. ( <ref type="formula">1</ref>) maximizes the mutual information I(Z; X + ).</p><p>Proof. We first decompose the mutual information I(Z; X + ):</p><formula xml:id="formula_14">I(Z; X + ) =E Z,X + log p(Z|X + ) p(Z) =E X + E Z|X + [log p(Z|X + )] -E Z,X + [p(Z)] = -E X + H(Z|X + ) + H(Z).</formula><p>The first term E X + H(Z|X + ) measures the uncertainty of Z|X + , which is minimized when Z can be completely determined by X + . The second term H(Z) measures the uncertainty of Z itself and it is minimized when outcomes of Z are equally likely.</p><p>We next show that Z can be completely determined by X + when minimum of Eq. ( <ref type="formula">1</ref>) is achieved and uncertainty of Z itself is maintained by an assumption about the framework. By the Cauchy-Schwarz inequality,</p><formula xml:id="formula_15">E X,X + L ctr (X, X + ) ?E X,X + q 2 z+ 2 + q+ 2 z 2 ] = -2.</formula><p>The equality is attained when q = z+ and q+ = z for all x, x + from the same class. For any three images x 1 , x 2 , x 3 from the same class, we have:</p><formula xml:id="formula_16">f (x 1 ) = g(x 3 ), f (x 2 ) = g(x 3 ),</formula><p>where g = h(f (?)). We can find f (x 1 ) = f (x 2 ) for any images x 1 , x 2 from the same class. The result can be easily extended to the general case:</p><formula xml:id="formula_17">f (X 1 ) = f (X 2 ) for any (X 1 , Y 1 ) ? P (X, Y ), (X 2 , Y 2 ) ? P (X, Y ) with Y 1 = Y 2 .</formula><p>Thus Z can be determined by X + with the equation Z = f (X + ), which minimizes E X + H(Z|X + ) . When p(Z = c y |Y = y) = 1 |Y| , the entropy H(Z) is maximized. With extensive empirical results in Simsiam <ref type="bibr" target="#b8">[9]</ref>, we assume the collapsed solutions are perfectly avoided by using the SimSiam framework. By this assumption, c j = c k for any j = k. The model learns different clusters c y for different y and representations with different labels have different clusters. Therefore, for a balanced dataset, the outcomes of Z are equally likely and it maximizes the second term H(Z). In summary, the learned representations by Eq. ( <ref type="formula">1</ref>) maximizes the mutual information I(Z; X + ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Proof for Theorem 2</head><p>Theorem. Given a distribution D(X, Y, ? ) that is ( , ?)-Distribution, we have</p><formula xml:id="formula_18">I(X; Y ) -? I(Z ; Y ) ? I(X; Y ),<label>(11)</label></formula><formula xml:id="formula_19">I(Z ; ? ) ? I(X; ? ) -? + .<label>(12)</label></formula><p>Proof. The Theorem builds upon the Theorem 5 from <ref type="bibr" target="#b38">[39]</ref>. We first provide the proof for the first inequality, which can also be obtained from <ref type="bibr" target="#b38">[39]</ref>. Then we provide the proof for the second inequality.</p><p>For the first inequality, by adopting Data Processing Inequality in the Markov Chain Y ? X ? Z, we have I(X; Y ) ? I(Z; Y ) for any Z ? Z. Then, we have I(X; Y ) ? I(Z ; Y ). Since Z = arg max Z ? I(Z ? ; X + ), and I(Z ? ; X + ) is maximized at I(X; X + ), then I(Z ; X + ) = I(X; X + ) and I(Z ; X + |Y ) = I(X; X + |Y ). Meanwhile, use the result I(Z ; X + ; Y ) = I(X; X + ; Y ), which is given by</p><formula xml:id="formula_20">I(Z ; X + ; Y ) = I(Z ; X + ) -I(Z ; X + |Y ) = I(X; X + ) -I(X; X + |Y ) = I(X; X + ; Y ), we have I(Z ; Y ) =I(X; X + ; Y ) + I(Z ; Y |X + ) =I(X; Y ) -I(X; Y |X + ) + I(Z ; Y |X + ).<label>(13)</label></formula><p>Thus, by Eq. ( <ref type="formula" target="#formula_20">13</ref>) and the Definition 1, we get</p><formula xml:id="formula_21">I(Z ; Y ) ? I(X; Y ) -I(X; Y |X + ) ? I(X; Y ) -<label>(14)</label></formula><p>Now we present the second inequality I(Z ; ? ) ? I(X; ? ) -? + .</p><p>Similarly, by Eq. ( <ref type="formula" target="#formula_20">13</ref>), we have</p><formula xml:id="formula_22">I(Z ; ? ) = I(X; ? ) -I(X; ? |X + ) + I(Z ; ? |X + ) (15) ? I(X; ? ) -? + I(Z ; ? |X + ) (16) ? I(X; ? ) -? + I(Z ; Y |X + )<label>(17)</label></formula><p>? I(X; ? ) -? +</p><p>, where the first and the third inequalities are by the definition 1; the second inequality is by the Data Processing Inequality in the Markov Chain ? ? Y ? X ? Z. The first equality can also be decomposed into another form: </p><formula xml:id="formula_24">H( ? ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gradients of Contrastive regularization Functions</head><p>For the contrastive regularization function</p><formula xml:id="formula_25">L ctr (x i , x j ) = - q i q i 2 ? z j z j 2 + q j q j 2 ? z i z i 2 ,</formula><p>we only consider the case 1{p i p j ? ? } = 1 because L ctr (x i , x j ) is not calculated in the algorithm when 1{p i p j ? ? } = 0. We assume that h is an identity function and x i , x j are from the same class for simplicity. Let a = q i 2 , b = q i , x = Taking a, b, c and x back to the equation, we get the result ?L ctr (x i , x j ) ?q i = -1 q i 2 q j q j 2 -( q i q j q i 2 q j 2 ) q i q i 2 .</p><p>Note that z i = Stopgrad(q i ) because of the identity map h. Let c i = 1/ q i 2 2 and then we have ?L ctr (x i , x j ) ?q i</p><formula xml:id="formula_26">2 2 = c i (1 -(q i qj ) 2 ).</formula><p>Similarly, for the contrastive regularization function L ctr (x i , x j ) = log 1 -q i q i 2 , z j z j 2</p><p>+ log 1 -q j q j 2 , z i z i 2 , ? L ctr (x i , x j ) ?q i = 1 1 -q i qj ?L ctr (x i , x j ) ?q i =c i (1 + q i qj ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the proposed method with noisy labels. Black curves are the best classifiers that are learned during training. Left: Deep networks without contrastive regularization. Right: Deep networks with contrastive regularization. Two classes are better separated by deep networks that points with the same class are pulled into a tight cluster and clusters are pushed away from each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Results of memorization of label noise and performance on test data on CIFAR-10 with 80% symmetric label noise (SYM) and 40% asymmetric label noise (ASYM). The memorization is defined by the fraction of wrongly labeled examples whose predictions are equal to their labels.</figDesc><graphic url="image-13.png" coords="4,50.11,85.70,123.74,86.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Analysis of ? and ? on CIFAR-10 with 60% symmetric label noise</figDesc><graphic url="image-18.png" coords="7,50.11,271.55,123.75,71.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustration of our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>CTRR Pseudocode in a PyTorch-like style # Training # f: backbone + projection mlp # h: prediction mlp # g: backbone + softmax linear classifier for x, y in loader: bsz = x.size(0) x1, x2 = strong_aug(x), strong_aug(x) # strong random augmentation x3 = weak_aug(x) # weak random augmentation z1, z2 = f(x1), f(x2) q1, q2 = h(z1), h(z2) p = g(x3) # compute representations c1 = torch.matmul(q1, z2.t()) # B X B c2 = torch.matmul(q2, z1.t()) # B X B # compute contrastive loss for each pair m1 = torch.zeros(bsz, bsz).fill_diagonal_(1) # identity matrix m2 = torch.ones(bsz, bsz).fill_diagonal_(0) # 1-identity matrix # -&lt;i,i&gt; + log(1-&lt;i,j&gt;) c1 = -c1 * m1 + ((1-c1).log()) * m2 c2 = -c2 * m1 + ((1-c2).log()) * m2 c = torch.cat([c1, c2], dim=0) # 2B X B # compute probability threshold probs_thred = torch.matmul(p, p.t()).fill_diagonal_(1).detach() # B X B mask = (probs_thred &gt;= tau).float() probs_thred = probs_thred * mask # normalize the threshold weight = probs_thred / probs_thred.sum(1, keepdim=True) weight = weight.repeat((2, 1)) # 2B X B loss_ctr = (contrast_logits * weight).sum(dim=1).mean(0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>B. 3 . 1 .</head><label>31</label><figDesc>Proof for Lemma 1Lemma. Consider a pair of random variables (X, ? ). Let ? be outputs of any classifier based on inputs Z ? , and ? = 1{ ? = ? }, where 1{A} be the indicator function of event A. Then, we haveE[?] ? H( ? ) -I(Z ? ; ? ) -H(?) log | ?| -Proof. If we are given any two of {? = 1}, ? , ? , the other one is known. By the properties of conditional entropy, H( ? , ?| ? , Z ? ) can be decomposed into the two equivalent forms.H( ? , ?| ? , Z ? ) = H( ? |?, ? , Z ? ) + H(?| ? , Z ? ) = H( ? |?, ? , Z ? ) 0 +H( ? | ? , Z ? ) (19)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>?| ? , Z ? ) =H( ? |?, ? , Z ? ) + H(?| ? , Z ? ) =p(? = 1)H( ? |? = 1, ? , Z ? ) + p(? = 0) H( ? |? = 0, ? , Z ? ) 0 +H(?| ? , Z ? ) =p(? = 1)H( ? |? = 1, ? , Z ? ) + H(?| ? , Z ? )(20)Relating Eq. (19) to Eq. (20), we haveE[?] = H( ? | ? , Z ? ) -H(?| ? , Z ? ) H( ? |? = 1, ? , Z ? ) ? H( ? | ? , Z ? ) -H(?| ? , Z ? ) log (|Y| -1) ? H( ? | ? , Z ? ) -H(?) log (|Y| -1) = H( ? ) -I( ? ; Z ? , ? ) -H(?) log (|Y| -1) = H( ? ) -I( ? ; Z ? ) -H(?) log (|Y| -1).The first inequality is byH( ? |? = 1, ? , Z ? ) ? log (|Y| -1), where ? can take at most |Y| -1 values. For the second inequality,H(?| ? , Z ? ) = H(?) -I(?; ? , Z ? ) ? H(?).For the last equality,I( ? ; Z ? , ? ) =H(Z ? , ? ) -H(Z ? , ? | ? ) =H(Z ? ) + H( ? |Z ? ) -H(Z ? | ? ) -H( ? |Z ? , ? ) =I(Z ? , ? ) + I( ? ; ? |Z ? ) =I(Z ? , ? ),whereI( ? ; ? |Z ? ) = 0 given the Markov Chain ? ? Y ? X ? Z ? ? : I( ? ; ? |Z ? ) =H( ? |Z ? ) -H( ? |Z ? , ? ) = H( ? |Z ? ) -H( ? |Z ? ) = 0. B.4. Proof for Lemma 2 Lemma. Let R(X) = inf g E X,Y [L(g(X), Y )] be the minimum risk over the joint distribution X ? Y , where L(p, y) = Y i=1 y (i) log p (i)is a CE loss and g is a function mapping from input space to label space. Let R(Z ) = inf g E Z ,Y [L(g (Z ), Y )] be the minimum risk over the joint distribution Z ? Y and g maps from representation space to label space. Then,R(Z ) ? R(X) + .Proof. The lemma is given by the variational form of the conditional entropyH(Y |Z ) = inf g E Z ,Y [L(g (Z ), Y )]<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref>.According to a property of mutual information,I(A; B) = H(A) -H(A|B),we have R(Z ) = H(Y ) -I(Z ; Y ). By the results of Theorem 2, R(Z ) ?H(Y ) -I(X; Y ) + =H(Y |X) = inf g E X,Y [L(g(X), Y )].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>zj zj 2</head><label>2</label><figDesc>and c = b a . According to the equation a 2 = b b, we differentiate both side of the equation and get 2a da = 2b db.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>An example of Grad-CAM<ref type="bibr" target="#b34">[35]</ref> results of Resnet34 trained on noisy dataset with 40% symmetric label noise and clean dataset, separately. When there is label noise, information related to corrupted labels captured by the model varies from image to image (e.g. window bars in Cat 1 v.s. floor and wall in Cat 2). When there is no label noise, information related to true labels are similar for images from the same class (e.g. cat face in Cat 1 v.s. cat face in Cat 2).</figDesc><table><row><cell>Original</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Noisy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Labels</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clean</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Labels</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cat 1</cell><cell>Cat 2</cell><cell>Dog 1</cell><cell>Dog 2</cell></row><row><cell>Figure 2.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>?0.21 93.05 ?0.32 92.16 ?0.31 87.34 ?0.84 83.66 ?0.52 81.65 ?2.46 89.00 ?0.56 Test accuracy on CIFAR-10 with different noise types and noise levels. All method use the same model PreAct Resnet18 [19] and their best results are reported over three runs. ?0.41 70.09 ?0.45 65.32 ?0.20 54.20 ?0.34 43.69 ?0.28 54.47 ?0.37</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sym.</cell><cell></cell><cell></cell><cell>Asym.</cell></row><row><cell></cell><cell>0%</cell><cell>20%</cell><cell></cell><cell>40%</cell><cell>60%</cell><cell></cell><cell>80%</cell><cell>90%</cell><cell>40%</cell></row><row><cell>CE</cell><cell>93.97 ?0.22</cell><cell>88.51 ?0.17</cell><cell cols="2">82.73 ?0.16</cell><cell cols="2">76.26 ?0.29</cell><cell>59.25 ?1.01</cell><cell>39.43 ?1.17</cell><cell>83.23 ?0.59</cell></row><row><cell>Forward</cell><cell>93.47 ?0.19</cell><cell>88.87 ?0.21</cell><cell cols="2">83.28 ?0.37</cell><cell cols="2">75.15 ?0.73</cell><cell>58.58 ?1.05</cell><cell>38.49 ?1.02</cell><cell>82.93 ?0.74</cell></row><row><cell>GCE</cell><cell>92.38 ?0.32</cell><cell>91.22 ?0.25</cell><cell cols="2">89.26 ?0.34</cell><cell cols="2">85.76 ?0.58</cell><cell>70.57 ?0.83</cell><cell>31.25 ?1.04</cell><cell>82.23 ?0.61</cell></row><row><cell cols="2">Co-teaching 93.37 ?0.12</cell><cell>92.05 ?0.15</cell><cell cols="2">87.73 ?0.17</cell><cell cols="2">85.10 ?0.49</cell><cell>44.16 ?0.71</cell><cell>30.39 ?1.08</cell><cell>77.78 ?0.59</cell></row><row><cell>LIMIT</cell><cell>93.47 ?0.56</cell><cell>89.63 ?0.42</cell><cell cols="2">85.39 ?0.63</cell><cell cols="2">78.05 ?0.85</cell><cell>58.71 ?0.83</cell><cell>40.46 ?0.97</cell><cell>83.56 ?0.70</cell></row><row><cell>SLN</cell><cell>93.21 ?0.21</cell><cell>88.77 ?0.23</cell><cell cols="2">87.03 ?0.70</cell><cell cols="2">80.57 ?0.50</cell><cell>63.99 ?0.79</cell><cell>36.64 ?1.77</cell><cell>81.02 ?0.25</cell></row><row><cell>SL</cell><cell>94.21 ?0.13</cell><cell>92.45 ?0.08</cell><cell cols="2">89.22 ?0.08</cell><cell cols="2">84.63 ?0.21</cell><cell>72.59 ?0.23</cell><cell>51.13 ?0.27</cell><cell>83.58 ?0.60</cell></row><row><cell>APL</cell><cell>93.97 ?0.25</cell><cell>92.51 ?0.39</cell><cell cols="2">89.34 ?0.33</cell><cell cols="2">85.01 ?0.17</cell><cell>70.52 ?2.36</cell><cell>49.38 ?2.86</cell><cell>84.06 ?0.20</cell></row><row><cell cols="2">CTRR 94.29 Method</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100 Sym.</cell><cell></cell><cell></cell><cell>Asym.</cell></row><row><cell></cell><cell>0%</cell><cell>20%</cell><cell></cell><cell cols="2">40%</cell><cell></cell><cell>60%</cell><cell>80%</cell></row><row><cell>CE</cell><cell>73.21 ?0.14</cell><cell cols="2">60.57 ?0.53</cell><cell cols="2">52.48 ?0.34</cell><cell cols="2">43.20 ?0.21</cell><cell>22.96 ?0.84</cell><cell>44.45 ?0.37</cell></row><row><cell>Forward</cell><cell>73.01 ?0.33</cell><cell cols="2">58.72 ?0.54</cell><cell cols="2">50.10 ?0.84</cell><cell cols="2">39.35 ?0.82</cell><cell>17.15 ?1.81</cell><cell>-</cell></row><row><cell>GCE</cell><cell>72.27 ?0.27</cell><cell cols="2">68.31 ?0.34</cell><cell cols="2">62.25 ?0.48</cell><cell cols="2">53.86 ?0.95</cell><cell>19.31 ?1.14</cell><cell>46.50 ?0.71</cell></row><row><cell cols="2">Co-teaching 73.39 ?0.27</cell><cell cols="2">65.71 ?0.20</cell><cell cols="2">57.64 ?0.71</cell><cell cols="2">31.59 ?0.88</cell><cell>15.28 ?1.94</cell><cell>-</cell></row><row><cell>LIMIT</cell><cell>65.53 ?0.91</cell><cell cols="2">58.02 ?1.93</cell><cell cols="2">49.71 ?1.81</cell><cell cols="2">37.05 ?1.39</cell><cell>20.01 ?0.11</cell><cell>-</cell></row><row><cell>SLN</cell><cell>63.13 ?0.21</cell><cell cols="2">55.35 ?1.26</cell><cell cols="2">51.39 ?0.48</cell><cell cols="2">35.53 ?0.58</cell><cell>11.96 ?2.03</cell><cell>-</cell></row><row><cell>SL</cell><cell>72.44 ?0.44</cell><cell cols="2">66.46 ?0.26</cell><cell cols="2">61.44 ?0.23</cell><cell cols="2">54.17 ?1.32</cell><cell>34.22 ?1.06</cell><cell>46.12 ?0.47</cell></row><row><cell>APL</cell><cell>73.88 ?0.99</cell><cell cols="2">68.09 ?0.15</cell><cell cols="2">63.46 ?0.17</cell><cell cols="2">53.63 ?0.45</cell><cell>20.00 ?2.02</cell><cell>52.80 ?0.52</cell></row><row><cell>CTRR</cell><cell>74.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Test accuracy on CIFAR-100 with different noise levels. All method use the same model PreAct Resnet18<ref type="bibr" target="#b18">[19]</ref> and their best results are reported over three runs.</figDesc><table><row><cell>Method</cell><cell cols="2">ANIMAL-10N Clothing1M</cell></row><row><cell>CE</cell><cell>83.18 ?0.15</cell><cell>70.88 ?0.45</cell></row><row><cell>Forward</cell><cell>83.67 ?0.31</cell><cell>71.23 ?0.39</cell></row><row><cell>GCE</cell><cell>84.42 ?0.39</cell><cell>71.34 ?0.12</cell></row><row><cell>Co-teaching</cell><cell>85.73 ?0.27</cell><cell>71.68 ?0.21</cell></row><row><cell>SLN</cell><cell>83.17 ?0.08</cell><cell>71.17 ?0.12</cell></row><row><cell>SL</cell><cell>83.92 ?0.28</cell><cell>72.03 ?0.13</cell></row><row><cell>APL</cell><cell>84.25 ?0.11</cell><cell>72.18 ?0.21</cell></row><row><cell>CTRR</cell><cell>86.71 ?0.15</cell><cell>72.71 ?0.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>. Test accuracy on the real-world datasets ANIMAL-10N</cell></row><row><cell>and Clothing1M. For ANIMAL-10N, all methods use a random</cell></row><row><cell>initialized Resnet18 [19]. For Clothing1M, all methods use a pre-</cell></row><row><cell>trained Resnet18. The results are obtained based on three different</cell></row><row><cell>runs.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>?0.21 93.05 ?0.32 92.16 ?0.31 87.34 ?0.84 83.66 ?0.52 81.65 ?2.46 The performance of the model with respect to different regularization functions. ?0.32 92.16 ?0.31 87.34 ?0.84 83.66 ?0.52 81.65 ?2.46 CTRR (SimCLR) 92.50 ?0.35 90.12 ?0.43 87.41 ?0.83 84.96 ?0.44 79.57 ?1.32 CTRR (BYOL) 93.31 ?0.16 92.12 ?0.16 88.71 ?0.52 86.99 ?0.59 84.31 ?0.66</figDesc><table><row><cell>Regularization Functions</cell><cell>0%</cell><cell></cell><cell>20%</cell><cell>40%</cell><cell cols="2">CIFAR-10</cell><cell>60%</cell><cell>80%</cell><cell>90%</cell></row><row><cell>L ctr (6)</cell><cell cols="2">93.58 ?0.11</cell><cell>86.05 ?0.33</cell><cell cols="2">82.34 ?0.25</cell><cell cols="2">74.35 ?0.54</cell><cell>54.83 ?1.00</cell><cell>40.96 ?0.99</cell></row><row><cell cols="2">L ctr (8) 94.29 Contrastive Frameworks</cell><cell cols="2">20%</cell><cell>40%</cell><cell cols="3">CIFAR-10 60%</cell><cell>80%</cell><cell>90%</cell></row><row><cell cols="2">CTRR (SimSiam)</cell><cell>93.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Extending our method to other contrasitve learning frameworks</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>?0.11 92.76 ?0.67 89.23 ?0.18 85.40 ?0.93 / indicates the label correction technique is enabled/disabled. CTRR+GCE 93.94 ?0.09 93.06 ?0.29 92.79 ?0.06 90.25 ?0.40</figDesc><table><row><cell>Label Correction</cell><cell></cell><cell></cell><cell cols="3">CIFAR-10</cell></row><row><cell>Technique</cell><cell>20%</cell><cell cols="2">40%</cell><cell></cell><cell>60%</cell><cell>80%</cell></row><row><cell></cell><cell>93.05 ?0.32</cell><cell cols="2">92.16 ?0.31</cell><cell cols="2">87.34 ?0.84</cell><cell>83.66 ?0.52</cell></row><row><cell cols="2">93.32 Method 20%</cell><cell>40%</cell><cell cols="2">CIFAR-10</cell><cell>60%</cell><cell>80%</cell></row><row><cell>GCE</cell><cell>91.22 ?0.25</cell><cell cols="2">89.26 ?0.34</cell><cell cols="2">85.76 ?0.58</cell><cell>70.57 ?0.83</cell></row><row><cell>CTRR</cell><cell>93.05 ?0.32</cell><cell cols="2">92.16 ?0.31</cell><cell cols="2">87.34 ?0.84</cell><cell>83.66 ?0.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>The performance of the model with respect to GCE, CTRR and CTRR+GCE.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement li Yi was supported by <rs type="funder">NSERC</rs> <rs type="programName">Discovery Grants Program</rs>. <rs type="person">Sheng Liu</rs> was partially supported by <rs type="funder">NSF</rs> grant <rs type="grantNumber">DMS 2009752</rs>, <rs type="funder">NSF</rs> <rs type="grantNumber">NRT-HDR Award 1922658</rs> and <rs type="funder">Alzheimer's Association</rs> grant <rs type="grantNumber">AARG-NTF-21-848627</rs>. <rs type="person">Boyu Wang</rs> was supported by <rs type="funder">NSERC</rs> <rs type="programName">Discovery Grants Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zP8f2mx">
					<orgName type="program" subtype="full">Discovery Grants Program</orgName>
				</org>
				<org type="funding" xml:id="_StBJFYV">
					<idno type="grant-number">DMS 2009752</idno>
				</org>
				<org type="funding" xml:id="_AhcHkrn">
					<idno type="grant-number">NRT-HDR Award 1922658</idno>
				</org>
				<org type="funding" xml:id="_R9GJdnQ">
					<idno type="grant-number">AARG-NTF-21-848627</idno>
				</org>
				<org type="funding" xml:id="_GvgfsAa">
					<orgName type="program" subtype="full">Discovery Grants Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Experiment Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Implementation Details</head><p>For CIFAR datasets, we use the model PreAct Resnet18 <ref type="bibr" target="#b18">[19]</ref>. For ANIMAL-10N, we use a random initialized model Resnet18 <ref type="bibr" target="#b18">[19]</ref>. For Clothing1M, we use an ImageNet pretrained model Resnet18 <ref type="bibr" target="#b18">[19]</ref>. We illustrate our framework in Figure <ref type="figure">5</ref>. The projection MLP is 3-layer MLP and the prediction MLP is 2-layer MLP as proposed in Simsiam <ref type="bibr" target="#b8">[9]</ref>. We use weak augmentations A w : X ? X including random resized crop and random horizontal flip for optimizing the cross entropy loss L ce . Following Sim-Siam <ref type="bibr" target="#b8">[9]</ref> [7], we use a strong augmentation A s : X ? X applied on images twice for optimizing the contrastive regularization term L ctr . Specifically, {z i } = f A s ({x i }) and {q i } = h f A s ({x i } for every example x i , where one strong augmented image is for calculating z and another is for calculating q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Algorithm</head><p>According to our gradient analysis on two different clean images x i , x j with y i = y j and a noisy image x m with y m = y i , apply the regularization function Eq. ( <ref type="formula">8</ref>) can avoid representation learning dominated by the wrong contrastive pair (x i , x m ). The analysis does not cover the same image with two different augmentations. When applying the strong augmentation twice, each image x has two different augmentations x , x . The contrastive pair (x , x ) will also dominate the representation learning given the property of Eq. ( <ref type="formula">8</ref>). However, focusing on learning similar representations of (x , x ) does not help to form a cluster structure in representation space. As mentioned in <ref type="bibr" target="#b40">[41]</ref>, learning this self-supervised representations causes representations of data distributed uniformly on the unit hypersphere. Hence, we want the gradient from the pair (x , x ) to be smaller when their representations approach to each other. We use the original contrastive regularization to regularize the pair (x , x ). The pseudocode of the proposed method is given in Algorithm 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust bi-tempered logistic loss based on bregman divergences</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noise against noise: stochastic label noise helps combat inherent label noise</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2020. 2, 5</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2008">2020. 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021">2021. 2, 5, 8, 12</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A framework using contrastive learning for classification with noisy labels</title>
		<author>
			<persName><forename type="first">Madalina</forename><surname>Ciortan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Peel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2008">2021. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A minimax approach to supervised learning</title>
		<author>
			<persName><forename type="first">Farzan</forename><surname>Farnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Can cross entropy loss be robust to label noise?</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senlin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrastive learning improves model robustness under label noise</title>
		<author>
			<persName><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2021. 2, 8</date>
			<biblScope unit="page" from="2703" to="2708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>?vila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving generalization by controlling labelnoise information in neural network weights</title>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Reing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2020. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CURL: contrastive unsupervised representations for reinforcement learning</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning invariant representations and risks for semi-supervised domain adaptation</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yezhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="1104" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from noisy data with robust representation learning</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2021. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mopro: Webly supervised learning with momentum prototypes</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks</title>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2020. 1, 4, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Active contrastive learning of audio-visual video representations</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2020. 1, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What do neural networks learn when trained with random labels?</title>
		<author>
			<persName><forename type="first">Hartmut</forename><surname>Maennel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><forename type="middle">M</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Baldock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Keysers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Noise tolerance under risk minimization</title>
		<author>
			<persName><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coresets for robust training of deep neural networks against noisy labels</title>
		<author>
			<persName><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SELFIE: refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yooju</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08199</idno>
		<title level="m">Learning from noisy labels with deep neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An information theoretic framework for multi-view learning</title>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Annual Conference on Learning Theory</title>
		<editor>
			<persName><forename type="first">Rocco</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-supervised learning from a multi-view perspective</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>PMLR, 2020. 12</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2008">2019. 2019. 1, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust early-learning: Hindering the memorization of noisy labels</title>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detco: Unsupervised contrastive learning for object detection</title>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">L dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dual T: reducing estimation error for transition matrix in label-noise learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In 5th International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning with feature-dependent label noise: A progressive approach</title>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2018. 1, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
