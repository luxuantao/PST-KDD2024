<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Linning</forename><surname>Peng</surname></persName>
							<email>pengln@seu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Junqing</forename><surname>Zhang</surname></persName>
							<email>junqing.zhang@liverpool.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
							<email>mingliu@bjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Aiqun</forename><surname>Hu</surname></persName>
							<email>aqhu@seu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Deep Learning Based RF Fingerprint Identification Using Differential Constellation Trace Figure</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Electronics</orgName>
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<postCode>L69 3GJ</postCode>
									<settlement>Liverpool</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Lab of Transportation Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<addrLine>No. 3 Shangyuancun</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C49206C65B8BFD76BF9AD37B9DD80D1F</idno>
					<idno type="DOI">10.1109/TVT.2019.2950670</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Physical layer security</term>
					<term>radio frequency fingerprint</term>
					<term>differential constellation trace figure</term>
					<term>convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel deep learning-based radio frequency fingerprint (RFF) identification method for internet of things (IoT) terminal authentications. Differential constellation trace figure (DCTF), a two-dimensional (2D) representation of differential relationship of signal time series, is utilized to extract RFF features without requiring any synchronization. A convolutional neural network (CNN) is then designed to identify different devices using DCTF features. Compared to the existing CNN-based RFF identification methods, the proposed DCTF-CNN possesses the merits of high identification accuracy, zero prior information and low complexity. Experimental results have demonstrated that the proposed DCTF-CNN can achieve an identification accuracy as high as 99.1% and 93.8% under SNR levels of 30 dB and 15 dB, respectively, when classifying 54 target ZigBee devices, which significantly outperforms the existing RFF identification methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the authenticator who will extract a particular transmitter RFF feature, e.g., carrier frequency offset (CFO), In-phase (I) and Quadrature (Q) offset, non-linear variation. The CFO is caused by the oscillator variations. The I/Q offset is generated from I/Q channel gain and DC offset. The transmitter amplifier nonlinearity creates unique non-linear behaviour of waveforms near maximal power. The authenticator will maintain a table of the device index and its RFF features in the database. During the classification stage, the authenticator will classify the index/label of the device according to the previously stored database when it receives a new transmission from a device to be classified. Many machine learning algorithms have been used, including k-nearest neighbor (KNN), support vector machine (SVM), random forest (RndF), multiple discriminant analysis (MDA), etc <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. However, the identification accuracies of these algorithms are limited especially when there are a large number of targets.</p><p>There are recent research efforts employing deep learning to improve the identification accuracy. Deep learning has been used widely for wireless channel estimations <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, human motion behavior detections <ref type="bibr" target="#b7">[8]</ref>, speech language identifications <ref type="bibr" target="#b8">[9]</ref> and RFF device identifications <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Time domain I/Q complex baseband samples are directly used as the CNN input in most of convolutional neural network (CNN)based RFF device identification. As the data samples are sequential input, we call this sample-based CNN. Merchant et al. achieved an accuracy of 91% over seven target devices in experiments <ref type="bibr" target="#b10">[11]</ref>. However, in order to obtain stable I/Q samples for CNN training and verification, prior knowledge is required for carrier frequency and time synchronization <ref type="bibr" target="#b10">[11]</ref>. In addition, due to the complexity consideration, the length of the I/Q sequence in existing sample-based CNN schemes are usually quite short <ref type="bibr" target="#b10">[11]</ref>, which results in a limited identification accuracy. Ding et al. designed a sophisticated bispectrum-based CNN method for RFF identification and achieved an accuracy of 87% over five target devices <ref type="bibr" target="#b9">[10]</ref>. However, it is worth noting that the spectrum-based RFF features are sensitive to the noise and environment <ref type="bibr" target="#b11">[12]</ref>. The bispectrum estimation also requires high computational complexity. Hilbert-Huang transform (HHT) is introduced for RFF feature extractions in <ref type="bibr" target="#b12">[13]</ref>. Similar to the bispectrum, different Hilbert spectrums are employed for identification. An algorithm employing the Fisher's discriminant ratio (FDR) is used to select elements of the Hilbert spectrum for classifications.</p><p>A differential constellation trace figure (DCTF)-based feature extraction method is introduced in <ref type="bibr" target="#b13">[14]</ref>, which converts the time domain I/Q samples to a 2D image containing RFF Motivated by the success of CNN at complex classification tasks in image recognition, this paper applies CNN to classify DCTFs obtained from different devices. Being different from most of RFF identification schemes using sample-based CNN, the proposed scheme directly classifies different targets using the "fingerprint" liked figures, which is termed as figure-based CNN. The main contributions of this paper are as follows:</p><p>• We propose and design a novel DCTF and CNN-based scheme for RFF identification. The generated DCTF is a "fingerprint" liked figure and thus image recognition CNNs can be used for identification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DCTF-CNN-BASED RFF IDENTIFICATION</head><p>The system block diagram is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We first sample the waveforms from RF devices. A DCTF generation method is then employed to generate the "fingerprint" figure for each device. A deep learning CNN is designed to train and classify different RF modules based on their unique DCTFs.</p><p>DCTF is beneficial because it would include most of the RFF. The transmitted RF signal can be written as:</p><formula xml:id="formula_0">X(t) = β I (x I (t) + α I ) + jβ Q (x Q (t) + α Q ) • e -j2πf t c t ,<label>(1)</label></formula><p>where x I (t) and x Q (t) are the signals at the I and Q branches, respectively, f t c is the carrier frequency at transmitter, α I and α Q are the DC offset at I/Q channels, β I and β Q are the I/Q gain imbalances. We assume that the channel and receiver is ideal, the received signal could be downconverted as:</p><formula xml:id="formula_1">Y (t) = y I (t) + jy Q (t) = X(t) • e j2πf r c t ,<label>(2)</label></formula><p>where y I (t) and y Q (t) are the received baseband I and Q signal components, respectively; f r c is the carrier frequency at the receiver. Because of the manufacturing imperfections, f t c and f r c will deviate with a slight CFO, ψ = f r c -f t c . The differential process is carried out with no frequency and time synchronization required, which can be given as</p><formula xml:id="formula_2">D(t) = d I (t) + jd Q (t) = y I (t) + jy Q (t + ε) • y I (t + λ) + jy Q (t + λ + ε) * ,<label>(3)</label></formula><p>where d I (t) and d Q (t) are the I/Q channel signals, λ is the differential time interval, ε is the introduced I/Q phase mismatch to enlarge the fingerprint feature, and (•) * denotes the conjugation operation. Upon substitution of ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>) into (3), the differentiated signal can be further written as</p><formula xml:id="formula_3">D(t) = β I (x I (t) + α I ) + jβ Q (x Q (t + ε) + α Q ) • e j2πψt • β I (x I (t + λ) + α I ) -jβ Q (x Q (t + λ + ε) + α Q ) • e -j2πψ(t+λ) = β 2 I (x I (t) + α I ) • (x I (t + λ) + α I ) + β 2 Q (x Q (t + ε) + α Q ) • (x Q (t + λ + ε) + α Q ) + j(β I β Q (x Q (t + ε) + α Q ) • (x I (t + λ) + α I ) -β I β Q (x I (t) + α I ) • (x Q (t + λ + ε) + α Q )) • e -j2πλψ .<label>(4)</label></formula><p>The CFO ψ turns into a fixed parameter e -j2πλψ in (4), which causes a fixed I/Q sample rotation. For the simplicity of analysis, we assume that ε is 0. We analyse the distribution of samples in D(t) with the following two simple scenarios:</p><formula xml:id="formula_4">(i) x I (t) = x I (t + λ), x Q (t) = x Q (t + λ); (ii) x I (t) = -x I (t + λ), x Q (t) = -x Q (t + λ).<label>(5)</label></formula><p>In the case (i), D(t) can be simplified as</p><formula xml:id="formula_5">(i) D(t) = (β 2 I x 2 I (t) + β 2 Q x 2 Q (t)) + β 2 I α I (α I + 2x I (t)) + β 2 Q α Q (α Q + 2x Q (t)) • e -j2πλψ ,<label>(6)</label></formula><p>In the case (ii), D(t) can be simplified as</p><formula xml:id="formula_6">(ii) D(t) = -(β 2 I x 2 I (t) + β 2 Q x 2 Q (t)) + β 2 I α 2 I + β 2 Q α 2 Q + j2β I β Q (x Q (t)α I -x I (t)α Q ) • e -j2πλψ .<label>(7)</label></formula><p>In ( <ref type="formula" target="#formula_5">6</ref>) and ( <ref type="formula" target="#formula_6">7</ref>), I/Q DC offset α I and α Q are much smaller than the signal components x I (t) and x Q (t). Therefore, the I channel signal (</p><formula xml:id="formula_7">β 2 I x 2 I (t) + β 2 Q x 2 Q (t)</formula><p>) could present the transmitted signal power with the I/Q gain imbalance impact, Fig. <ref type="figure">2</ref>. Generated DCTFs from real devices with different parameters which could be mostly affected by amplifier non-linearity. In addition, we can also find that in <ref type="bibr" target="#b6">(7)</ref>, D(t) exists residual Q channel signal caused from I/Q DC offset α I and α Q . The differentiated signal D(t) owns different gathering centers around the maximal values at I channel with a fixed e -j2πλψ rotation.</p><p>It is intuitive that a method should be designed to present these features. Constellation map is straightforward to evaluate the signal qualities but synchronization will be required. An alternate constellation trace figure is employed to present the differential result D(t). Furthermore, it is sensible to investigate the trace distribution density in the figure, which is the essential characteristic due to the RFF. A measurement matrix Φ is built to count the distribution density.</p><p>The measurement matrix Φ could be generated with a size of M × N pixel grids ranging from -A to +A. The values of elements Φ m,n are initialized as zero. For each D(t), we get the index (m, n) of Φ by following process:</p><formula xml:id="formula_8">m = d I (t) + A 2A M , n = d Q (t) + A 2A N ,<label>(8)</label></formula><p>where [•] is the round operation. Then we add the element of Φ m,n by 1 when D(t) falls to the index (m, n). Finally, when all of the samples are counted and added in the specific index of measurement matrix, Φ could represent the distribution density of the entire waveform. The elements of Φ are normalized by re-scaling the pixel values from 0 to 255, which can be seen as a M × N image.</p><p>In practical systems, wireless signal is affected by channel fading, which causes different received signal powers. Therefore, the received signal is initially power-normalized. After differential process in (3), the ranging value A in (8) should be larger than 3 in order to avoid overflow in low SNR scenarios. Finally, we can obtain DCTF from measurement matrix Φ generated in aforementioned process. Some DCTF examples are shown in Fig. <ref type="figure">2</ref>.</p><p>As DCTF is a 2D image, it inspires us to employ the popular deep learning CNN. Being different from the existing CNN designed to identify the I/Q samples, the neural network in our method aims to recognize the underlying RFF patterns in a 2D image. The neural network is trained with DCTFs from different devices in different conditions, e.g., SNR, λ, ε, image sizes (M × N ). In the classification stage, the scheme will classify the target device by finding out its label based on the network parameters provided by the CNN training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL DESIGN A. Experiment Setup</head><p>Extensive experiments were carried out to evaluate the performance of DCTF-CNN. L = 54 TI CC2530 system-onchip ZigBee modules are employed as the target devices to be classified. A universal software radio peripheral (USRP) software defined radio (SDR) hardware platform is built to collect ZigBee RF waveforms. A PC installed with MATLAB R2018b and NVIDIA GTX1060 graphic card is setup for signal processing and carrying out CNN training and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ZigBee Burst Collection</head><p>Similar to the most of the deep learning-based RF fingerprint identification work, we performed ten measurements for each ZigBee device at different locations with line-ofsight (LOS) transmission. The carrier frequency of the ZigBee device was set as 2505 MHz with offset quadrature phase shift keying (OQPSK) modulation. It took five minutes for the devices to warm up and reach a steady working condition.</p><p>A USRP receiver was employed to capture RF waveforms from different ZigBee devices at 2505 MHz. The receiver sampling rate was 10 Msample/s, which owns ten times oversampling compared to ZigBee 1M chip rate. Each measurement was composed of 9 frame segments and each segment can generate a DCTF. For each frame segment, the USRP receiver captured approximately 40K samples.</p><p>We use a spectrum estimation method to evaluate the SNR of the received signal. Due to the different locations and transmitter variations, the estimated SNR of the captured signals was 20∼25 dB. In order to save experiment time, we added different levels of additive white Gaussian noise (AWGN), varying from 0∼30 dB, to the received waveforms in order to emulate various signal qualities.</p><p>In order to optimize the DCTF-CNN parameters and evaluate the performance, we divided the overall ten measurement data into two groups, each with five measurements. The first group was used to find the optimal parameters of DCTF and CNN, including λ, ε, image size, batch size and initial learning rate; the second group was used for testing performance with different SNRs. For each group, we added specified AWGN via ten simulations and used the waveforms from two measurements for training. The training and validation sets were randomly split with 85% and 15% respectively. The rest three measurements served as separate test sets. In each measurement, we can obtain nine frames for DCTF generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CNN Design</head><p>Inspired by the famous LeNet-5 network which has been successfully used to solve the canonical MNIST digits recognition <ref type="bibr" target="#b14">[15]</ref>, we chose a neural network with three 2D convolutional layers followed by a fully connected layer. The kernel size was set to [3×3] to capture the local details of RFF in the DCTF. Channel numbers were chosen to 16, 32 and 64 to accommodate more higher level features. The rectified linear unit (ReLU) was employed as the activation function to reduce the likelihood of vanishing gradient and accelerate the training. Two [2×2] max pooling layers were applied after the first two convolutional layers to down-sample the output features and limit the size of the neural network. A fully connected layer of L outputs together with the Softmax activation was used in the last stage to perform the classification among L target devices. Cross entropy for L mutually exclusive classes was adopted as the loss function of classification. Gradient descent algorithm was used in the network training process. For an input image size of [65×65], the parameters of each layer are as follows.</p><p>• The first convolutional layer: There are (3 Therefore, the total trainable parameters of this network is 160+4640+18464+L×16385 = 23,264 + L×16,385, with weights and biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance with Different DCTF Generation Parameters</head><p>When receivers have zero prior information of the target signal, the differential interval λ and introduced I/Q phase mismatch distortion ε are arbitrarily chosen because the signal symbol rate and modulation type are not known at the receiver. This section investigated effects of the differential interval and I/Q phase mismatch distortion. The SNR was 30 dB and the DCTF image size was 65 × 65.</p><p>Some examples of the obtained DCTFs with different generation parameters were shown in Fig. <ref type="figure">2</ref>. The DCTFs varied greatly with λ and ε. The diversity among different devices was also distinguishable under specific parameter setup.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> depicts the accuracy of DCTF-CNN with different DCTF generation parameters. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, although the accuracy dramatically decreases when extremely short differential interval λ is selected, the overall accuracy is higher than 97% when λ is larger than 3. In addition, Fig. <ref type="figure" target="#fig_2">3</ref> shows that the overall accuracy is always higher than 97% when ε is selected as 0, 1, 2 and 3. In particular, the introduced I/Q phase mismatch distortion ε of 1 had the best performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance with Different DCTF Image Qualities</head><p>DCTF image quality is affected by the DCTF size, M × N , and the number of used samples for generating DCTF. It is obvious that the larger size of the DCTF image, the lower chance of the different samples falling into the same pixel, which consequently requires more data samples to maintain the details of the figure. On the other hand, lower size of DCTF will blur features among devices. In addition, the larger number of used samples for generating DCTF, the better quality of the image, at a cost of higher complexity. This section evaluated the effect of the image quality on the identification accuracy, when SNR = 30 dB, λ = 10 and ε = 1.</p><p>The accuracy with different DCTF image qualities is depicted in Fig. <ref type="figure" target="#fig_3">4</ref>. An image size of 65 × 65 pixels offers the best performance for our designed CNN. When DCTF image size was smaller than 17 × 17, the RFF feature got blurred. It is also evident that the smaller DCTF image size, the less network complexity. Table <ref type="table" target="#tab_2">I</ref> demonstrates the CNN training time cost with different DCTF sizes, which shows that the overall complexity of DCTF-CNN is very low. The maximal training time was only 498 seconds even with 257 × 257 DCTF image size. It is worth noting that the smaller size of DCTF image, the lower complexity of network computation. Therefore, the DCTF image size with 65 × 65 and 33 × 33 are preferable setup choices that achieve a satisfactory tradeoff between performance and complexity.</p><p>When DCTF image size is fixed, Fig. <ref type="figure" target="#fig_3">4</ref> shows that the more samples used for generating DCTF, the better identification accuracy, thanks to the better DCTF image quality. However, we find that with the DCTF size of 65 × 65, the accuracy difference between 20K and 40K samples was negligible. The DCTF-CNN accuracy can be higher than 97% when more than 10K samples were used. CNN efficiency and accuracy are affected by batch sizes, maximal epoches and initial learning rates. In particular, batch size is one of the most important hyperparameters to tune in deep learning training <ref type="bibr" target="#b15">[16]</ref>. In our 54 ZigBee devices identification problem, 8262 training DCTFs with the size of 65 × 65 were used. We evaluated the performance with different batch sizes from 32 to 1024 and different maximal epoches from 5 to 40. We also evaluated the performance with different initial learning rates at 0.01 and 0.1. Extensive simulations were taken with 20 loops averaging. The obtained results are shown in Fig. <ref type="figure">5</ref>.</p><p>Fig. <ref type="figure">5</ref> demonstrates that the network training can be faster and more accurate when initial learning rate is reduced from 0.1 to 0.01. In addition, the system performance is better when the batch size is between 128 and 256. Larger batch size requires more hardware resources to load DCTFs and more epoches to get the converged result. However, it is observed that DCTF-CNN accuracies dramatically deteriorated when batch size was larger than 256 for both initial learning rates at and 0.1. On the other hand, small batch sizes also cause performance degradations when initial learning rate is 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance with Different SNR</head><p>Configured with the optimized parameters, we evaluated the performance of the DCTF-CNN under different SNR levels in the range of {0, 5, 10, 15, 20, 25, 30} dB. The λ was set to 10 and ε was set to 1. The DCTF size was 65 × 65, 40K samples were used to generate a DCTF.  <ref type="bibr" target="#b10">[11]</ref>, bispectrumbased CNN <ref type="bibr" target="#b9">[10]</ref> and Hilbert-Huang Transform (HHT)-based method <ref type="bibr" target="#b12">[13]</ref>. The same data set was used for all of the evaluations. The FFT length for bispectrum generation is 128. The HHT size is 512, the maximum number of intrinsic mode functions (IMF) extracted is 5. The DCTF-CNN confusion matrixes at 10 dB and 15 dB SNRs for the 54 ZigBee devices identification problem are depicted in Fig. <ref type="figure">7</ref>. When SNR is higher than 20 dB, the accuracy of the proposed DCTF-CNN is higher than 98% and can even reach 99.1% at 30 dB.</p><p>The DCTF with a k-means clustering algorithm can achieve an accuracy of 60.1%. The best performance was around 96% even when the system was enhanced by multiple features <ref type="bibr" target="#b13">[14]</ref>. Thanks to the feature representation and classification capability of CNN, the proposed DCTF-CNN acquired a significant performance enhancement, especially in high SNR scenarios. Our DCTF-CNN scheme can achieve an identification accuracy of 99.1% among 54 target devices in contrast to the 91.4% accuracy with I/Q samples-based CNN, 81.4% accuracy with bispectrum-based CNN and 41.7% accuracy with HHT-based CNN. In low SNR scenarios, the DCTF-CNN had similar performance compared to I/Q samples-based CNN, but also with higher accuracy than spectrum-based CNN methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this correspondence, we proposed a novel RFF identification method that combined the DCTF-based feature extraction and the CNN-based classification. Different from existing sample-based CNN RFF identification methods that exploit complex (I/Q channel) signal samples, we directly classified target devices without requirement of any synchronization and compensation. In addition, the complexity of DCTF-CNN is very low due to the small input figure size. We carried out extensive experiments with a testbed including 54 ZigBee devices. We initially investigated parameter optimizations in our DCTF-CNN system. With the help of optimal DCTF generation parameters and CNN training parameters, the identification accuracy is as high as 99.1% and 93.8% at the SNR levels of 30 dB and 15 dB, respectively, which significantly outperforms the existing deep learning-based RFF identification methods. The future work includes study of target movements in DCTF-CNN identification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. DCTF-CNN-based RFF identification</figDesc><graphic coords="2,52.09,56.07,244.81,117.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Therefore, for each group, we have 54 * 9 * 10 * 2 * 0.85 = 8262 training DCTFs, 54 * 9 * 10 * 2 * 0.15 = 1458 validation DCTFs and 54 * 9 * 10 * 3 = 14580 testing DCTFs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. DCTF-CNN accuracy with different intervals λ and I/Q phase mismatch distortions ε</figDesc><graphic coords="4,315.11,56.07,244.78,123.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. DCTF-CNN accuracy with different DCTF image sizes (40000 samples) and number of used samples (65 × 65 image size)</figDesc><graphic coords="5,52.09,56.07,244.76,118.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Identification accuracy under different SNRs</figDesc><graphic coords="5,343.91,56.07,187.20,169.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I CNN</head><label>I</label><figDesc>TRAINING COST WITH DIFFERENT DCTF SIZES</figDesc><table><row><cell>Size</cell><cell>Epochs</cell><cell>Time (s)</cell><cell>Size</cell><cell>Epochs</cell><cell>Time (s)</cell></row><row><cell>257 × 257</cell><cell>23</cell><cell>498</cell><cell>33 × 33</cell><cell>33</cell><cell>28</cell></row><row><cell>129 × 129</cell><cell>42</cell><cell>202</cell><cell>17 × 17</cell><cell>23</cell><cell>19</cell></row><row><cell>65 × 65</cell><cell>31</cell><cell>52</cell><cell>9 × 9</cell><cell>30</cell><cell>23</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61501022, 61571110, 61601114, 61602113 and Purple Mountain Laboratories for Network and Communication Security. The work of J. Zhang was supported by Royal Society Research Grants under grant ID RGS/R1/191241. L. Peng and A. Hu are with the School of Cyber Science and Engineering, Southeast University, No. 2 Sipailou, Nanjing, China and Purple Mountain Laboratories, Nanjing, China</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Device fingerprinting in wireless networks: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surveys Tuts</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="104" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On physical-layer identification of wireless devices</title>
		<author>
			<persName><forename type="first">B</forename><surname>Danev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zanetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Capkun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Authorized and rogue device discrimination using dimensionally reduced RF-DNA fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Reising</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1180" to="1192" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving ZigBee device network authentication using ensemble decision tree classifiers with radio frequency distinct native attribute fingerprinting</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Rel</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="233" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Physical-layer fingerprinting of LoRa devices using supervised and zero-shot learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Robyns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lamotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Quax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Singele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preneel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Security Privacy in Wireless and Mobile Networks (WiSec)</title>
		<meeting>ACM Conf. Security Privacy in Wireless and Mobile Networks (WiSec)<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for super-resolution channel estimation and doa estimation based massive mimo system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Tech</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="8549" to="8560" />
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning based nlos identification with commodity wlan devices</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Tech</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3295" to="3303" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time human motion behavior detection via cnn using mmwave radar</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Short utterance based speech language identification in intelligent vehicles with time-scale modifications and deep bottleneck features</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Tech</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="128" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Specific emitter identification via convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2591" to="2594" />
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for RF device fingerprinting in cognitive communication networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Revay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stantchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nousain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
			<date type="published" when="2018-02">feb 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wireless physicallayer identification: Modeling and validation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2091" to="2106" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Specific emitter identification via hilbert-huang transform in single-hop and relaying scenarios</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1192" to="1205" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Design of a hybrid RF fingerprint extraction and device classification scheme</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="349" to="360" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Corinna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2019-09-05">5 Sep., 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Don&apos;t decay the learning rate, increase the batch size</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th International Conference on Learning Representations (ICLR)</title>
		<meeting>6th International Conference on Learning Representations (ICLR)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
