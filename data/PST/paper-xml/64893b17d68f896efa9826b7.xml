<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences</title>
				<funder ref="#_pErvrf5">
					<orgName type="full">NSF of China</orgName>
				</funder>
				<funder ref="#_QHQVZdS">
					<orgName type="full">NSF of China for Distinguished Young Scholars</orgName>
				</funder>
				<funder ref="#_AbvRVKC #_JgdXbmj">
					<orgName type="full">Technology and Innovation Major Project of the Ministry of Science and Technology of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-13">13 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao21@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<email>yuxiaod@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
							<email>peng.zhang@zhipuai.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China Yifan Xu</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China Aohan Zeng</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China Zhengxiao Du</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Peng Zhang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Zhipu.AI</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">Jie Tang ?</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="department">KDD &apos;23</orgName>
								<address>
									<addrLine>August 6-10</addrLine>
									<postCode>2023</postCode>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<address>
									<postCode>42 pages</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="department">KDD &apos;23</orgName>
								<address>
									<addrLine>August 6-10</addrLine>
									<postCode>2023</postCode>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-13">13 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3580305.3599931</idno>
					<idno type="arXiv">arXiv:2306.07906v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large Language Model</term>
					<term>Pre-Trained Model</term>
					<term>Human Preference Alignment</term>
					<term>General Language Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at https://github.com/THUDM/WebGLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts</head><p>? Computing methodologies ? Natural language generation; ? Software and its engineering ? Development frameworks and environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs), such as GPT-3 <ref type="bibr">[3]</ref>, PaLM <ref type="bibr">[5]</ref>, OPT <ref type="bibr" target="#b36">[37]</ref>, BLOOM <ref type="bibr" target="#b31">[32]</ref>, and GLM-130B <ref type="bibr" target="#b35">[36]</ref>, have significantly pushed the boundary of machines' ability on language understanding and generation. Question answering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>, one of the most fundamental language applications, has also been substantially advanced by the recent LLM developments. <ref type="bibr">Existing</ref>   performance of LLMs' closed-book QA <ref type="bibr" target="#b28">[29]</ref> and in-context learning QA <ref type="bibr">[3,</ref><ref type="bibr" target="#b17">18]</ref> is comparable to supervised models, furthering our understanding on LLMs' potential to memorize knowledge. However, even for LLMs, their capacity is not unlimited, and when it comes to challenges that require sufficient rare-knowledge, LLMs fail to meet up human expectations. Hence recent efforts have been focused on constructing LLMs augmented from external knowledge, such as retrieval <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> and web search <ref type="bibr" target="#b23">[24]</ref>. For example, WebGPT <ref type="bibr" target="#b23">[24]</ref> can browse the web, answer complex questions in long form, and provide useful references correspondingly.</p><p>Despite its success, the original WebGPT method <ref type="bibr" target="#b23">[24]</ref> is far from real-world deployments. First, it relies on abundant expert-level annotations of browsing trajectories, well-written answers, and answer preference labeling, requiring considerable expenses, time, and training. Second, the behavior cloning method (i.e., imitation learning) requires its base model GPT-3 to emulate human experts by instructing the system to interact with a web browser, issue operation commands (e.g., Search, Read, and Quote), and then retrieve relevant information from online sources. Finally, the multi-turn nature of web browsing demands intensive computation resources and can be too slow for user experience, e.g., costing about 31 seconds for WebGPT-13B to response a 500-token prompt.</p><p>In this work, we present WebGLM-a practical web-enhanced QA system based on the 10-billion-parameter General Language Model (GLM-10B) <ref type="bibr" target="#b5">[6]</ref>. An example is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. It is efficient, cost-effective, human preference-aware, and most importantly, of comparable quality to WebGPT. The system employs multiple new strategies and designs to achieve good performance, including:</p><p>An LLM-augmented Retriever: a two-staged retriever that implements coarse-grained web search and fine-grained LLM-distilled retrieval. It is inspired by the fact that LLMs like GPT-3 can naturally learn to adopt correct references, and such ability could be distilled to improve smaller dense retrievers.</p><p>A Bootstrapped Generator: a GLM-10B based answer generator that is trained on quoted long-formed QA samples and bootstrapped by LLM in-context learning. We discover that instead of relying on expensive human expert writing in WebGPT, LLMs can be enabled to learn to generate high-quality data with proper citation-based filtering.</p><p>A Human Preference-aware Scorer: a scorer, that is trained over online QA forums' user thumb-up signals, is able to learn human majority preferences on different answers. Compared to WebGPT's expert labeling, we prove that a proper dataset construction could also produce a high-quality scorer.</p><p>Our extensive human evaluation and quantitative ablation results demonstrate the efficiency and effectiveness of the WebGLM system. Specifically, WebGLM (10B) surpasses the similar-scaled WebGPT (13B) and performs comparably to WebGPT (175B) on our Turing test (Cf. Figure <ref type="figure" target="#fig_1">2</ref>). WebGLM's improvement against the only publicly-available system-Perplexity.ai-also makes it among the best public web-enhanced QA systems as of this submission.</p><p>To sum up, in this paper, we make the following contributions:</p><p>? We construct WebGLM, an efficient web-enhanced QA system with human preferences. It significantly outperforms the similar-sized WebGPT (13B) and performs comparably to WebGPT (175B). It also surpasses Perplexity.ai-a popular system powered by LLMs and search engines. ? We identify WebGPT's limitations on real-world deployments. We propose a set of new designs and strategies to allow WebGLM's high accuracy while achieving efficient and cost-effective advantages over baseline systems.</p><p>? We formulate the human evaluation metrics for evaluating web-enhanced QA systems. Extensive human evaluation and experiments demonstrate WebGLM's strong capability and also generate insights into the system's future developments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The construction of web-enhanced QA systems is a systematic project that requires cross-domain collaboration, including large language models, open-domain question answering, retrieval augmentation, and reinforcement learning from human feedback. Here we briefly introduce related literature on them.</p><p>Large Language Models (LLMs). Self-supervised <ref type="bibr" target="#b18">[19]</ref> LLMs have attracted plenty of attention in nowadays natural language processing (NLP). Their huge number of parameters captures and stores versatile knowledge <ref type="bibr" target="#b19">[20]</ref> and enables their outstanding performance on various challenges. Typical LLMs include GPT-3 <ref type="bibr">[3]</ref>, PALM <ref type="bibr">[5]</ref>, OPT <ref type="bibr" target="#b36">[37]</ref>, BLOOM <ref type="bibr" target="#b31">[32]</ref>, and GLM-130B <ref type="bibr" target="#b35">[36]</ref>. One of the fascinating LLM properties is prompt-based in-context learning (ICL), which allows tuning-free task transfer via prepended demonstration samples. Recent works have been focusing on the optimization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref> and analysis <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> of ICL.</p><p>Open-domain Question Answering (Open QA). Traditional QA datasets such as SQuAD <ref type="bibr" target="#b27">[28]</ref> assume the reference is available.</p><p>On the contrary, open-domain QA targets the open world and is more practical but challenging. For example, Natural Questions <ref type="bibr" target="#b14">[15]</ref> dataset consists of queries from the Google search engine and annotations from Wikipedia paragraphs. Web Questions <ref type="bibr">[2]</ref> derives open-domain questions from knowledge bases. MS Marco <ref type="bibr" target="#b24">[25]</ref> gathers passage texts and corresponding labels to questions. However, most Open QA datasets and models are limited to answer short answer phrases, while people usually prefer more informative long-formed answers with references. A possible reason is that constructing and evaluating long-formed QA datasets with open-world references are difficult, requiring expert-level annotations. Recent attempts include ELI5 <ref type="bibr" target="#b6">[7]</ref> that collects queries and long-formed answers with scores from Reddit and WebGPT <ref type="bibr" target="#b23">[24]</ref> which hires groups of experts and leverages up to 175-billionparameter GPT-3 as the backbone. WebGLM aims to provide another effective and cost-effective solution for the challenge.</p><p>Retrieval-augmentation. Mainstream information retrieval approaches include sparse-vector-based BM25 and TF-IDF, and the recent dense-vector-based methods such as DPR <ref type="bibr" target="#b13">[14]</ref> and Contriever <ref type="bibr" target="#b9">[10]</ref>. The idea of retrieval-augmented language models introduced by REALM <ref type="bibr" target="#b7">[8]</ref> argues the joint optimization of retriever and language modeling. Following representative works include RAG <ref type="bibr" target="#b15">[16]</ref>, Fusion-in-Decoder <ref type="bibr" target="#b10">[11]</ref>, and Atlas <ref type="bibr" target="#b11">[12]</ref>. The idea of We-bGPT also loosely falls into the field, as it asks the LLM to interact with the browser to seek relevant information for better accuracy. Nevertheless, it can cost intensive computation and is too slow for practical deployment. In this work, WebGLM tackles the problem efficiently by distilling LLMs' knowledge to smaller retrievers.</p><p>Reinforcement Learning from Human Feedback (RLHF). Automated scoring of text generation is a well-established area of research. BLEU <ref type="bibr" target="#b26">[27]</ref> and ROUGE <ref type="bibr" target="#b16">[17]</ref> take into account the overlap ratio between the target and reference. METEOR <ref type="bibr">[1]</ref> considers the accuracy and recall rate of the whole corpus. Other methods, such as BERTScore <ref type="bibr" target="#b37">[38]</ref>, evaluate using cosine similarity of contextual embedding from deep language models. In recent years, some work advocates learning scorers from human feedback <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref> via asking models to predict human preference. The scorers, or namely reward models, can be used to optimize the text generator via reinforcement learning. Such methods, which WebGPT is also affiliated with, have achieved great success in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The WebGLM System</head><p>Constructing an LLM-based web-enhanced QA system can be expensive and challenging. The web information is rich but noisy for certain queries, and creating high-quality human answers with references for training can be outrageously expensive. This type of systems usually involves three critical components: retriever, generator, and scorer.</p><p>Take WebGPT <ref type="bibr" target="#b23">[24]</ref> as an example, which employs experts for dataset annotation. Its retriever leverages GPT-3 to "behavior-clone" human experts' web-browsing trajectory to search, read, and quote. In addition, the generator is trained on expert-written long answers with references. And finally, the scorer learns to predict experts' preferences over different answers, and its scores serve as rewards for the generator's reinforcement learning. Despite WebGPT's primary success, its retrieval can be slow, and the data annotations required for training the generator and scorer are too costly, significantly hindering its wide public adoptions.</p><p>In this work, we aim to build an efficient web-enhanced QA system that understands human preferences for actual use. To combine the advantages of LLMs and well-established open QA studies, we present a series of new designs and strategies for our web-enhanced QA system WebGLM based on GLM <ref type="bibr" target="#b5">[6]</ref>:</p><p>? An LLM-augmented Retriever: we design two stages: coarsegrained web search and fine-grained LLM-augmented dense retrieval <ref type="bibr" target="#b9">[10]</ref>, for finding relevant references given queries. ? A Bootstrapped Generator: we derive WebGLM-QA, an LLMbootstrapped quoted and long-formed QA dataset via in-context learning and corresponding strategies to clean and refine. It includes 45k high-quality after filtering and 83k noisy but diverse samples before filtering. The backbone of WebGLM system is a GLM model trained on the dataset.</p><p>? A Human Preference-aware Scorer: we develop techniques to learn human majority preference from online QA forums' thumbups instead of expensive expert feedback, and successfully train a human preference-aware scorer for best-of-n selection.</p><p>The LLM API used for research purpose in this work is textdavinci-003 unless specified. In the following sections, we will introduce the algorithm and implementation details of each component, which finally form the WebGLM pipeline sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LLM-augmented Retriever</head><p>In conventional open QA, the systems usually only retrieve from reliable sources (e.g., Wikipedia) and fail to benefit from whole web-scale knowledge. However, the flip side of the coin is that wild web pages can be hard to acquire and purify. In WebGLM, we make attempts to solve the problem via two-stage retrieval: coarsegrained web search and fine-grained LLM-augmented retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Coarse-grained Web Search</head><p>We leverage third-party web search engines (i.e., Google API) to acquire primary candidate web page URLs. In most cases, from our observation, these pages can cover the necessary contexts and knowledge to answer questions besides considerably abundant irrelevant information. The procedures are shown in Figure <ref type="figure">3</ref>. Specifically, it can be roughly divided into three steps:</p><p>(1) Search: At this stage, we enter the question into the search API and will obtain a list of URLs for potentially-relevant pages (usually less than 10). ( <ref type="formula">2</ref>) Fetch: Then, we crawl the corresponding HTML contents according to the URLs obtained. Since there are many candidate pages, we improve efficiency through parallel crawling.</p><p>(3) Extract: Next, based on HTML2TEXT<ref type="foot" target="#foot_0">1</ref> , we extract the part of text contents in the HTML pages and divide them into a list of paragraphs according to line breaks. Since the web crawl usually takes sufficient time, we have paid great efforts to optimize the speed of the component to allow useracceptable responding speed (Cf. Figure <ref type="figure">4</ref>). For example, in the "Fetch" step, if the page is loaded synchronously, the loading time will be 2-3 minutes long. The parallel asynchronous enables the quick loading of most pages in 5s (about 98%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Fine-grained LLM-augmented Retrieval</head><p>Through the first three stages, we have retrieved a number of potential contexts to questions. However, many of them are still irrelevant even under the filtering of widely-used dense retrievers (in our trial, up to 30% of top-ranked contexts are unrelated). As a solution, WebGPT <ref type="bibr" target="#b23">[24]</ref> uses behavior cloning (i.e., imitation learning) to leverage LLMs' strong language comprehensibility for reference selection. Notwithstanding its effectiveness, the strategy is slow in deployment and expensive in labeling.</p><p>LLMs' Reference Adoption. To mitigate the issue, we propose to combine smaller retrievers' efficiency and LLMs' strong ability to distinguish. We take Contriever <ref type="bibr" target="#b9">[10]</ref> as the smaller retriever in WebGLM, an unsupervised pre-trained model that encodes texts  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generator Scorer</head><p>Online QA Forums into embeddings and retrieves by finding the maximum inner product pair of them. We transfer LLMs' natural property of reference adoption to small retrievers to improve them.  <ref type="table" target="#tab_2">1</ref>). We find only 68.6% of them are related. However, when we provide the query with corresponding candidate references to GPT-3 for 1-shot incontext learning inference (see details in Section 3.2), we discover that the LLM would only adopt part of the references and the corresponding accuracy is 90.2%, far better than Contriever's.</p><p>Augmentation Implementation. To transfer the reference adoption knowledge from GPT-3 to Contriever, we leverage the GPT-3's reference adoption from our bootstrapped dataset WebGLM-QA to additionally fine-tune Contrievers. As the reference marks generated by GPT-3 can be wrong sometimes, we use the citation correction method based on Rouge-1 precision to match quotations and references (see those details in Section 3.2). Therefore, the labels we use for training are the Rouge-1 precision scores of a query-reference pair.</p><p>In the fine-tuning, we use two Contrievers to encode questions and references individually, and compute their inner products as the predictions. We leverage Mean Square Error (MSE) as the loss function for the predictions and Rouge-1 precision scores to train the Contrievers. Our further quantitative experiment demonstrates that the augmentation significantly improves Contriever web-enhanced QA retrieval accuracy (see Table <ref type="table" target="#tab_7">7</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Speed analysis</head><p>Retrieval is no doubt the most time-consuming part in any webscale QA system. A slow QA system, whatever high its accuracy is, would spoil the user experience. We report the speed of each steps in our LLM-augmented retriever.</p><p>We sample a subset from ELI5 <ref type="bibr" target="#b6">[7]</ref> test set to retrieve and calculate the average, the median, 75% quantile, 90% quantile, and 99% quantile time spent in each step. From Figure <ref type="figure">4</ref>, we can know that our average time spent is about 5.3s, the median total time spent is about 4.07s, and 90% of searches can be loaded in 10s. The main bottleneck of our retrieval is in the second step of fetching each page, when we have to request multiple web pages from different sources. Consequently, due the contents of various pages on the network are different, some pages take very long time to load, or just cannot be returned correctly.</p><p>In Appendix B, we conduct a more detailed analysis of retrieval efficiency and point out that the retrieval efficiency of WebGLM is far better than that of WebGPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bootstrapped Generator</head><p>A major obstacle in building web-enhanced QA system is the high cost for curating expert-level QA datasets that are long-formed and properly cited. Compared to traditional or free-formed QA, Reference [1]: Beyond that, when you wait till you're absolutely ravenous to eat, it's easy to eat past the point of fullness ... Reference [2]: ... our body learns that and so it learns to accept a smaller amount. Reference [3]: Sometimes after long periods of going without food your immune system ... Question: Why is it sometimes hard to eat after not eating for a while?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer:</head><p>I gave a friend an instruction and a question with references. The friend read the instruction and wrote an output to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Read the references provided and answer the corresponding question</head><p>Reference <ref type="bibr">[1]</ref>: Beyond that, when you wait till you're absolutely ... Reference [2]: ... our body learns that and so it learns to accept ... Reference [3]: Sometimes after long periods of going without food ... Question: Why is it sometimes hard to eat after not eating for a while?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Read the references provided and answer the corresponding question 1-shot demonstration</head><p>There are several reasons why not eating ... burning through your muscle <ref type="bibr">[1]</ref> Reference [1]: Words in general that are considered bad tend to ... Reference [2]: Some words are considered "bad" because they ... Question: Why did we decide that certain words were "bad" and shouldn't be used in social settings? we expect the system to yield fact-grounded answers with correct references (see example in 5). WebGPT reports to hire a group of full-time experts to write answers for training, which is far beyond ordinary budgets. Fortunately, LLMs' in-context learning <ref type="bibr">[3,</ref><ref type="bibr">5]</ref>, which refers to their capabilities to transfer to new tasks conditioned on few incontext samples, have been demonstrated and well-explored recently. Thus we propose to bootstrap large amounts of quoted long answers via leveraging a few high-quality answers, LLMs, questions from ELI5 <ref type="bibr" target="#b6">[7]</ref>, and our retriever collected references. Additionally, since bootstrapped samples are not always satisfying, we design corresponding correction and selection strategies to filter out a high-quality subset for real training. All these efforts jointly help to create the WebGLM-QA, a quoted and long-formed QA dataset with 45k high-quality filtered and 83k unfiltered samples.</p><p>The dataset can be formulated as a set D (Q, A, R, C), where Q, A, R represents the question set, the answer set, and the reference set respectively, C ? Q ? A ? 2 R denotes the triple set of (question, answer, valid references).</p><p>Different from free text generation, in web-enhanced QA each answer ? ? A contains quotations and thus is in the form of</p><formula xml:id="formula_0">? = (&lt; ? 1 , ? 1 &gt;, &lt; ? 2 , ? 2 &gt;, ? ? ? , &lt; ? ? , ? ? &gt;)<label>(1)</label></formula><p>where &lt; ? ? , ? ? &gt; represents the k-th segment in answer ?, ? ? is a piece of quoted text, and ? ? ? R is a set of references that ? ? cites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">In-context Learning Inference</head><p>We adopt a subset of questions from ELI5 train set as our Q and leverage a vanilla Contriever <ref type="bibr" target="#b9">[10]</ref> (without LLM augmentation yet) in fine-grained retrieval to produce references R. In this work we first try on OpenAI text-davinci-003 API to conduct 1-shot incontext learning inference to generate quoted long-formed answers (while other LLMs such as GLM-130B <ref type="bibr" target="#b35">[36]</ref> could be good options too). Since the in-context learning can be volatile to input forms and prompts, we take many trails to finally determine the best bootstrapping strategies as follows:</p><p>Prompt Formulation. Since we input many contents to the API, including a few of demonstrations (i.e., high-quality samples (? ? , ? ? , R ? )), the question, and the corresponding references, their formulation could impact the performance significantly. We compare several types of prompts, including the order between question and its references (i.e., before or after, Cf. Figure <ref type="figure" target="#fig_5">5</ref> (a)), the symbols used to mark the indices of references, and the prompt words of references and questions. We conduct experiments with every type of prompt we have mentioned, and finally find a natural way as shown in Figure <ref type="figure" target="#fig_5">5</ref> (a) performs best.</p><p>Instruction Inducting. Next, we need a proper instruction (e.g., "Please write a answer based on the question and references. ") for guiding the LLM to generate a qualified answer. Recent work <ref type="bibr" target="#b8">[9]</ref> suggests that we can take advantage of the LLM itself to design instructions for in-context learning instead of human handcrafting. We use several high-quality examples to induce a few possible instructions (Cf. Figure <ref type="figure" target="#fig_5">5</ref> (b)), and select the best-performed one based on our empirical evaluation over several queries.</p><p>Few-shot In-Context Learning. We study the best shots needed for generating good quoted long-formed answers. Because the reference parts often occupies much of sequence length, we notice that one-shot learning can surpass few-shot learning in terms of answer's quality in most time. Hence we finally choose to inference with one-shot demonstration sample as shown in Figure <ref type="figure" target="#fig_5">5</ref> (c), and finally 83k various queries and their answers have been collected. We record the details of choosing prompts and instructions in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Citation Correction</head><p>We have produced a large amount of well-written quoted longformed answers using GPT-3 in-context learning. However, in our examination, we observe that the answers sometimes cite the wrong or invalid (i.e., nonexistent) references in their citation numbers. As a result, to correct the citation relationships are crucial for the quality of WebGLM-QA dataset.</p><p>Despite the fact that the citation numbers can be wrong, the contents quoted in the answer are often correct. Thus we propose to amend the citation number according to the quotation similarity to references, by splitting an answer into few segments by generated citation numbers and match then to references. For a question ?, our retrieved references are defined as R and our answer can be defined as ?. We define text segments S = {? 1 , ? 2 , ? ? ? , ? ? }, and for each pair (?, ?) ? S ? R, we compute citation match scores ? (?, ? ) for ? ? R. We pick a threshold ? , and the final citation ? for each segment (?, ?) ? ? can be described as:</p><formula xml:id="formula_1">? ? = {? |? (? ? , ? ) ? ? }, ? ? R</formula><p>For our application, we finally adopt Rouge-1 score as the ? and the threshold ? selection is introduced in the Section 3.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Filtering</head><p>After correction, we further investigate more issues that could potentially influence the dataset quality. And in short, we discover that most of them are related or could be solved via checking the citation quality. We will discard a piece of generated sample if it presents any problems in the following:</p><p>? Hallucination <ref type="bibr" target="#b12">[13]</ref>: the answer leverages the internal knowledge of LLMs instead of references, which is not factual-grounded and sometimes severely wrong. It can be identified via the low overlapping ratio between all references and the answer. ? Few citations: when an answer cites too few of the provided references, it usually presents poor reference relevance and thus often not informative and factual-grounded enough. ? Low citation accuracy: if an answer have too many wrong citation numbers, we assume it as a low-quality one. We calculate the F1 for the similarity and overlapping calculation. We test Rouge-L (whose best threshold is 0.4) and Rouge-1 (whose best one is 0.57) on a set of manually checked samples, and find that Rouge-1 is better. It is due to the fact that LLMs would often rewrite and paraphrase the reference contents including exchanging phrase orders. In that case, a high-quality answer may hold a high informative Rouge-1 score, but a low Rouge-L score, which computes the longest common subsequence co-occurrence.</p><p>After all the filtering conditions mentioned above, the number of samples drops from 83k to 45k, which becomes a high quality quoted long-formed QA dataset for web-hanced QA system training. We train the GLM <ref type="bibr" target="#b5">[6]</ref>, a type of bidirectional LM that is pre-trained on autoregressive blanking infilling (including a 10-billion-parameter and a 2-billion-parameter one), over the WebGLM-QA as our backbone generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Human Preference-aware Scorer</head><p>In preliminary testing, our bootstrapped generator under beamsearch decoding strategy already performs satisfyingly in many cases. However, recent literature <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref> demonstrates that aligning human purposes and preference to LLMs are crucial for expert-level text generation. WebGPT reports to recruit many experts to provide comparison and ranking over generated answers and make use of the feedback to train a reward model (RM) for picking best-of-n (i.e., 16/32/64) generated candidates and additionally optimize the generator via reinforcement learning (RL).</p><p>Nevertheless, such expert annotations could be expensive to acquire and the RL would consume much computation resource. In this work, as a competitive substitute, we propose to build a human preference-aware scorer based on massive user feedback (e.g., thumb-ups) from online QA forums. Under appropriate designs and elaborate data cleaning, we show in our experiments that such scorer also significantly improve the alignment-level of answers and the scoring in real human evaluation.</p><p>Data collection and preprocessing. We first collect QA pairs and corresponding user thumb-ups from online QA forums. Despite their diversity, these answers are of so various lengths and qualities that the scorer would learn little from them without proper preprocessing.</p><p>Our preprocessing includes the following requirements: ? High quality feedback: we define the answer with more than 3 thumb-ups as an answer with valid feedback. We pick out questions with 8 or more valid answers as qualified ones. ? Length-bias mitigation: we notice that the score prefers longer answers rather than the better ones in preliminary study, as is also indicated in literature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref>. To mitigate the bias, for each qualified question, we use the median length ? of all the answers as the threshold to truncate longer answers and discard those lengths are less than ?/2. ? Contrast augmentation: after sorting the answers by their thumb-ups, the gaps between neighboring answers turn out narrow. Scorers trained on such uninformative dataset present poor performance. To increase the contrast between answers for comparison training, we select a pair of answers of more than 5 in rank positions. In each pair, the answer with greater amount of likes is the better response. After our prepossessing, there are 93k questions and 249k comparison pairs in total, with 230k pairs as the training set and 19k pairs as the test set. Next, we introduce the implementation details for training our human preference-scorer. The backbone model for training scorer is a 6-billion-parameter GLM.</p><p>Supervised fine-tuning (SFT). In SFT step, we leverage the Reddit TL; DR dataset for first fine-tuning the scorer following <ref type="bibr" target="#b32">[33]</ref>. We train 16 epochs with cosine learning rate decay and 2.83e-5 as beginning learning rate. We use the SFT model for initialization of comparison training.</p><p>Comparison training. We pass pairs of comparison data to the model to yield a scalar score for each of the question-answer pair and maximize the gap between their scores. We use a linear head with the input dimension of hidden size and the output dimension of 1 to produce the score.</p><p>During the training, we find that the scorer tends to overfit quickly. Therefore, we freeze first 70% transformer layers and leverage other techniques such as dropouts and large batch size for regularization. Notwithstanding, the scorer would overfit after 1-1.5 epochs anyway. After the training completes, we calibrate its predictions to standard normal distribution based on the training set reward distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Human Evaluation Criteria</head><p>Automatic metrics to score model-generated answers can perform well in terms of short-formed ones. However, for open-domain long-formed QA with references, the answers and rationales can be subjective and versatile, especially for those questions that start with "HOW" and "WHY." As a result, human evaluation is vitally needed, for which there have been many studies <ref type="bibr">[4,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>To evaluate WebGLM and appropriately compare it to other similar models, we introduce a human evaluation criteria system to evaluate both references and answers. We adopt both binary (for those objective metrics, e.g., truthfulness) and four-level score (for those subjective metrics, e.g., fluency) balancing objectivity and scale in human evaluation. The four-level score is applied as is suggested in the literature that it avoid human annotators to keep absolutely neutral <ref type="bibr" target="#b30">[31]</ref>. For each criterion we mention below, an arrow follows. up arrow (?) means higher score performs better, while down arrow (?) denotes lower score performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reference Evaluation</head><p>In this section, we introduce human evaluation criteria on references. The evaluation is done on per question-reference pair.</p><p>Relevancy ([0, 3], ?). For retrieved documents or references related to a question, the more related, the higher relevancy score should be. Specifically, different references to a question can share high relevancy scores simultaneously.</p><p>Density ([0, 3], ?). To evaluate how much useful information is in a piece of reference, we need to estimate its information density.</p><p>Both relevancy and density are criteria to evaluate informativeness, but there is difference between them. Relevancy can be regarded as a "recall metric" for informativeness, while density can be regarded as a "precision metric".</p><p>Truthfulness ([0, 1], ?). Retrieved references can be factually wrong even they are closely associated to the question. It is because the web information sources are open and could contain user-submitted information without correctness check. As a result, the truthfulness of a piece of reference should be evaluated, and its evaluation does not consider the question.</p><p>Toxicity ([0, 1], ?). Web texts could involve violent, pornographic, offensive words or other improper elements. Thus, it is necessary to assess toxicity of references retrieved.</p><p>Social Bias ([0, 1], ?). Potential biases on the internet could related to genders, races, nations, and ages. We should also exclude them from our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Answer Evaluation</head><p>In this section, we introduce human evaluation criteria on answers, which are evaluated triple-wise (i.e., (question, answer, references)).</p><p>Fluency ([0, 3], ?). Fluency measures the quality of generated text itself only, without taking questions and references into account <ref type="bibr">[4]</ref>. It concerns only elements such as grammar, word, and phrase choices that are affiliated to the language aspect.</p><p>Correctness ([0, 3], ?). Correctness measures the coherence of the answer and its corresponding question. If an answer solves the question satisfyingly, we say it holds a high correctness. Additionally, when we score the correctness of an answer, we should take factual consistency into account. For example, contradicting common sense or defying logic will decrease the correctness.</p><p>Citation Accuracy ([0, 3], ?). The metric only considers the relationships between an answer and its references. When an answer contains citation marks, we should check if it is correct. Citation mistakes or missing citation will both decrease the accuracy.</p><p>Truthfulness ([0, 1], ?). Similar to truthfulness in the reference evaluation, truthfulness of an answer measures whether the text of the answer is factually sound, including the factual consistency of the answer and whether the answer contains contradictions or hallucinate information.</p><p>Objectivity ([0, 1], ?). The metric only concerns the relationships between an answer and its references. When references provided, models are supposed to generate answers according to these references without its using its latent knowledge from pre-training. If we can find all the information of an answer from provided references, we say it is objective.</p><p>Redundancy ([0, 1], ?). Within the limited text length, duplicate content will reduce informativeness. As the lower redundancy, the higher quality of the answer, we take it into our consideration.</p><p>The detail of the metrics and the meaning of the score can be found in the Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>In this section, we conduct experiments employing the metrics mentioned in Section 4 to evaluate and analyze the quality of the responses generated, including those from WebGLM and other similar systems. We also report quantitative ablation studies on certain components in WebGLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>We conduct the major evaluation using the 272 questions provided on WebGPT <ref type="bibr" target="#b23">[24]</ref> demo website<ref type="foot" target="#foot_1">2</ref> , as the WebGPT is not publicly available and selected questions are generally complicated and closer enough to real human questions.</p><p>Human Evaluation Setup. We recruited 15 master-degree level experts to conduct human evaluation. For each question, we aggregate all the search results and answers from different models into one table, enabling the annotators to effectively compare them and unify the annotation standards. We evaluate the performance of our model and other different models from various dimensions through human evaluation. We also compare and analyze the results from different perspectives as follows. The main results are shown in Table <ref type="table" target="#tab_3">2</ref>.</p><p>WebGLM Reference vs Other References. Although the search results of WebGLM are slightly inferior to WebGPT-175B, its performance is far better than that of Perplexity.ai and WebGPT-13B. It is worth mentioning that the WebGLM retrieval process only uses some traditional, word-based algorithms and two Contrievers with a cumulative parameter amount of no more than 300M. WebGLM is significantly superior to WebGPT in computing performance and WebGLM vs Other Systems. Finally, we compare our system with the results of WebGPT-13B, Perplexity.ai, and WebGPT-175B. Our system has achieved the highest performance in fluency, truthfulness, and redundancy. At the same time, we are close to WebGPT-175B in the correctness metric with a score of 2.81, which is far higher than that of Perplexity.ai and WebGPT-13B, indicating that our system can still achieve superior performance at a lower cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Turing Test</head><p>To further compare our performance, we design a Turing test <ref type="bibr" target="#b20">[21]</ref> to check the answers' quality.</p><p>Setup. We randomly sampled 200 items from the 272 questions that WebGPT has displayed on their official web page. For each question, we shuffle the answers generated by WebGLM, WebGPT-175B, WebGPT-13B, and Perplexity.ai, and remove citation marks from them for fairness. We next mix an answer written by humans into these answers and ask evaluators to rank the answers by their quality, such as correctness, informativeness, and truthfulness.  Result. For each pair of answers (?, ?), if evaluators prefer ? to ?, we call ? wins and ? loses. Firstly, we compare each pair of the answers, the win rate is shown in Figure <ref type="figure" target="#fig_7">6</ref>. Besides, We calculate the win rates against humans for each system. The result is shown in Figure <ref type="figure" target="#fig_1">2</ref>. We hold a 43% win rate, definitely beat Perplexity.ai with a 21% win rate and WebGPT-13B with an 18% win rate, and almost draw with WebGPT-175B with a 45% win rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Test on QA Benchmarks</head><p>We randomly sample 400 questions on Natural Question and Web Question, and evaluate WebGLM and Perplexity.ai on them. The results in Table <ref type="table" target="#tab_4">3</ref> show that WebGLM outperform Perplexity.ai. 29.9 41.5</p><p>In addition, we conducted experiments on the full validation split of TriviaQA (same as WebGPT). Following the testing method employed by WebGPT, we first generated a long answer for each question using WebGLM. We then used Google Bigbird, fine-tuned on the TriviaQA training set<ref type="foot" target="#foot_2">3</ref> , to answer TriviaQA questions based on the output of WebGLM. To address potential test-train overlap issues mentioned in WebGPT, we also conducted TriviaQA tests on different train-test splits. The results are summarized in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>In this section, we study the major improvements and strategies in WebGLM, including the bootstrapped dataset filtering, scorer training, LLM-augmented retriever and some other minor topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">WebGLM-QA Filtering</head><p>Since we build our training dataset based on LLM in-context bootstrapping, the dataset quality could be essential for WebGLM's success. We randomly sample 210 examples from these versions of our dataset to verify the filtering strategies they are based on, including 1) None, 2) Rouge-L filtered, and 3) Rouge-1 filtered.</p><p>We randomly shuffle all the samples and distribute them to evaluators, and then collect and calculate the average score of each metric. The sample results are shown in Table <ref type="table" target="#tab_6">5</ref> We analyze this result from two perspectives. One is the absolute performance of our We find that our dataset holds a high factual consistency and correctness, and the majority of our data are judged as perfectly correct. We have also noticed that the information relevancy and density are considerably improved when we apply a filter method and when we change Rouge-L to Rouge-1. As for the answer, we find that correctness has great improvement when we apply any one of the two filters, and factual consistency has a great improvement when we change the Rouge-L filter to Rouge-1. Besides, objectivity is also one of the most important criteria that we care about, and we find that it's more likely to discard subjective answers with a Rouge-1 filter than with a Rouge-L filter. As a result, our experiments show that citation accuracy is closely related to the reference quality and answer quality, so our filter method is effective.</p><p>Besides, We train the GLM-2B models on each dataset and evaluate them with our designed metrics to see the impact of these datasets on our model's performance. We show the results in Table 6. We find that the answers of the three models showed little difference in the correctness metric. However, the performance of the model trained by rouge-1 was better in fluency, citation accuracy, and objectivity metrics. This result further proves the advantages of the dataset of rouge-1. Therefore, we decide to train our 10B model on the dataset of rouge-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">LLM-augmented Retriever</head><p>In terms of the usefulness of references, we have compared our method with traditional methods such as BM25, TF-IDF, and the original version of Contriver. For all answers to the same question, we compare the order predicted by retrieve methods with the answer relevancy order. The results are shown in Table <ref type="table" target="#tab_7">7</ref>. We notice that before the LLM task augmentation, the Contriever performs even poorer than traditional lexical-based approaches. After augmenting knowledge from GPT-3's reference adoption labeling, we find that ours, which holds a 69.36 pair-wise choosing accuracy and 62.26 spearman index, performs best. The evidence strongly advocates that the LLM augmentation is vital when we use pre-trained smaller dense retrievers in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Human Preference-aware Scorer</head><p>In this section we compare several different scorer training strategies and datasets. We discover that proper task formulation and larger and more diverse dataset yield better results.</p><p>Baseline and data preprocessing. We first train RoBERTa-large in the classification task and the regression task formulation, and the 6-billion-parameter GLM on the ELI5's training set (with thumbups) as our baselines. In the classification task, we collect all items whose count of answers is not less than 10 from ELI5. For each collected question, we label top-5-voted answers as positive, and randomly pick 5 answers from other questions as negative examples. In the regression task, we collect all items whose count of answers is not less than 5 from ELI5. For each collected question, we complete the following steps: <ref type="bibr">(1)</ref> for each answer to this question, supposing its corresponding up-vote is ?, we firstly label this answer as log 2 (? + 1). ( <ref type="formula">2</ref>) Then, we scale labels of all answers to this question to [0, 1].</p><p>(3) Let ? be the summation of the answers' label, we randomly pick ??? answers from other questions as negative examples with label -1.</p><p>In order to obtain a large train set (which has been suggested very important in <ref type="bibr" target="#b32">[33]</ref>), we adopt a relatively loose screening method, which selects the questions with more than 5 answers and answers with no less than 100 words in length. Our large train set includes 28.2k questions and 191.6k pairs. We use the ELI5 test set with thumb-ups for our final evaluations.</p><p>Metrics. We select three metrics to measure the ability of the reward model to distinguish responses of different quality, namely accuracy, Spearman coefficient, and NDCG (Normalized Discounted Cumulative Gain). Accuracy refers to the accuracy of selecting better answers in pairs. Spearman and NDCG measure the sorting ability of the model.</p><p>The ranking evaluation of different models is shown in Table <ref type="table" target="#tab_9">9</ref>. We find that WebGLM human preference-aware scorer performs best on accuracy and Spearman coefficient. Under the same amount of training tokens, the performance of the reward model is slightly worse than that of RoBERTa classification and RoBERTa regression, but after increasing the amount of training, the performance of the reward model will increase significantly.</p><p>Figure <ref type="figure" target="#fig_8">7</ref> shows the average reward of the answers at different positions in the sequence sorted by likes in the ELI5 test set. The best answer is around 0% and the worst answer is around 100%. We find that the curve of the WebGLM Human Preference-aware Scorer is more discriminative than other models, and the rewards of the best answer are higher than that of others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Ablation Study on Each Component</head><p>We added some experiments to conduct ablation studies on each component. We compared the three sub-modules of the system: Retriever, Generator, and Scorer. The results are shown in Table <ref type="table" target="#tab_8">8</ref>.</p><p>In the Retriever module, we compared the performance on the settings of WebGPT-175B, WebGLM, and non-retrieval. From the Table <ref type="table" target="#tab_8">8</ref>, the performance on WebGLM retrieval is similar to that of WebGPT-175B and significantly better than non-retrieval.</p><p>Regarding the Generator module, we compared the response quality of WebGLM and GPT-3 on WebGLM retrieval setting. We found that WebGLM performed slightly better than GPT-3 in fluency, correctness, accuracy, citation accuracy, objectivity, and truthfulness.</p><p>In terms of Scorer, we compared the response quality of We-bGLM removing and retaining Reward Models. The results show that by WebGLM-10B top-p sampling and reward model scoring method, We found through the human evaluation results that the answers scored high by the reward model excel the original results in fluency, correctness, citation accuracy, truthfulness, and redundancy. It shows the importance of the reward model scoring mechanism to model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We build the LLM-based question-answering system-WebGLMwith a web retrieval method. We propose a fast and cost-effective method to retrieve valuable information from the Internet. We leverage GPT-3's in-context learning ability to build a LLM-bootstrapped quoted and long-form QA dataset, which is used to train our model. Further, we train a human preference-aware scorer and use it to give marks to responses generated by our model. For each question, the scorer can select the highest-scored response from candidates, thus obtaining a final answer humans prefer the most. We conduct extensive experiments, including both the human evaluation and the Turing test, to demonstrate the competitive performance of WebGLM with some of the pioneering web-enhanced question answering systems like Perplexity.ai and WebGPT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 WebGLM vs Others in WebGPT Reference</head><p>We compared the generation results of WebGLM-Rouge1, WebGPT-175B, and GPT-3 on the WebGPT-175B references. For GPT-3, we also use the method of automatically constructing datasets to generate responses for the WebGPT samples to compare the effect of the WebGLM system. Specifically, we use the references of WebGPT to let GPT-3 do in-context learning to answer questions according to the search results. We use human evaluation to compare the quality of the three answers. The experimental results are shown in Table <ref type="table" target="#tab_10">10</ref>. Although our model size is more than ten times smaller than GPT-3 and WebGPT-175B, we can effectively compensate for the impact of the model size and achieve competitive performance in the retrieval paradigm. Our model matches WebGPT-175B and GPT-3 on correctness, citation accuracy, objectivity, and truthfulness metrics and outperforms them on fluency and redundancy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed Efficiency Analysis</head><p>At the retrieval stage, we only search for one time, then take the first few results links to fetch the web pages in parallel. We then extract all paragraphs and rank these paragraphs by Contriever, and finally take the top 5 paragraphs as references. Let ? ? , ? ? , ? ? , and ? ? denote the time we consume in four steps, so the total time we consume is ? ? + ? ? + ? ? + ? ? .</p><p>WebGPT simulates the operations in a virtual browser environment while obtaining references. For the 272 questions they showed, we count the types of actions and the average number of generated tokens as shown in Table <ref type="table" target="#tab_11">11</ref> and 12. Then we calculate the average time it takes to browse. Assuming that the total time ? (? is either WebGPT-175B or WebGPT-13B) takes to generate commands in the browsing process of each question is expected to be ? ? (?), the time ? consumes ? (?) satisfies the following equations.</p><formula xml:id="formula_2">? (WebGPT-175B) = ? ? (WebGPT-175B) + ? ? * 3.82 + ? ? * 6.96 (2) ? (WebGPT-13B) = ? ? (WebGPT-13B) + ? ? * 4.05 + ? ? * 7.56 (3)</formula><p>We test the efficiency of GPT-3. With a 500-token prompt, the 175B model generates about 20 tokens per second, and the 13B model generates 100 tokens per second, meaning that: In practice, ? ? , ? ? , ? ? , and ? ? are about 1.81, 2.38, 0.29, and 0.89 respectively. So we consume 5.36 seconds for one query on average. Nevertheless, assuming in the same network circumstance, the time consumption of WebGPT models can be calculated by Equation 2 and 3.</p><formula xml:id="formula_3">? ? (WebGPT-175B) =</formula><p>? (WebGPT-175B) = 52.48 seconds <ref type="bibr" target="#b5">(6)</ref> ? (WebGPT-13B) = 31.12 seconds (</p><p>Therefore, WebGPT-175B costs 52.48 seconds, and WebGPT-13B costs 31.12 seconds. Our efficiency can be about 10 times that of WebGPT-175B and 6 times that of WebGPT-13B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Choice of Prompts and Instructions</head><p>Firstly, we attempt the zero-shot approach for bootstrapping data.</p><p>To produce data with appropriate citation marks, we require specific instructions. We experiment with several methods, however, they are all limited in their effectiveness.</p><p>Use a mark for each helpful reference you cited, such as <ref type="bibr">[1]</ref>. Limitation: bootstrapped data contain mixed usage of <ref type="bibr">[1]</ref>[2] and <ref type="bibr">[1,</ref><ref type="bibr">2]</ref>.</p><p>Use a mark for each helpful reference you cited, such as <ref type="bibr">[1]</ref>. If there are multiple citations at one position, please use a format like <ref type="bibr">[1]</ref>[2] <ref type="bibr">[3]</ref>. Limitation: bootstrapped data contain citations of useless references.</p><p>Use a mark for each helpful reference you cited, such as <ref type="bibr">[1]</ref>. If there are multiple citations at one position, please use a format like <ref type="bibr">[1]</ref>[2] <ref type="bibr">[3]</ref>. If a reference is useless, do not cite it. Limitation: useless references are still cited. This method do not work.</p><p>We then select few-shot context to bootstrap data. If we provide too many references or in-context examples, it is easy to exceed the token count limit. Therefore, we choose to use an 1-shot example and 5 references. We also include some useless references in the example, which are not cited in the answer.</p><p>After that, We conduct experiments on prompts and demonstrate that placing the question after the references is the most effective approach.</p><p>Regarding instruction induction for in-context learning, we experiment with the previously mentioned examples as well as some new ones, such as:</p><p>Answer the question based on the following references with citations. Use a mark for each helpful reference you cited, such as <ref type="bibr">[1]</ref>. If there are multiple citations at one position, please use a format like <ref type="bibr">[1]</ref>[2] <ref type="bibr">[3]</ref>. If a reference is useless, do not cite it.</p><p>I will provide you with some references. Based on the references, please answer my question. Pay attention that you should be objective, and you should not use your knowledge. Use a mark for each helpful reference you cited, such as <ref type="bibr">[1]</ref>. If there are multiple citations at one position, please use a format like <ref type="bibr">[1]</ref>[2] <ref type="bibr">[3]</ref>. If a reference is useless, do not cite it.</p><p>However, these instructions are too verbose, and in the presence of examples, the model's performance is not significantly impacted by the instructions. Therefore, we adopt a more natural approach to generate instructions <ref type="bibr" target="#b8">[9]</ref> to produce a natural instruction that is interpretable by the model.</p><p>Finally, we use a very concise instruction: Read the references provided and answer the corresponding question.</p><p>In addition, we compared models trained with different prompt strategies, and the results are shown in the Table <ref type="table" target="#tab_13">13</ref>. From the "Correctness" column, we can see the significant difference that the order of references and question in the prompt makes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Dataset Examples</head><p>An example of WebGLM-QA is shown in Table <ref type="table" target="#tab_5">14</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Retrieval Example</head><p>An example of retrieved references from each system is shown in Table <ref type="table" target="#tab_6">15</ref> and Table <ref type="table" target="#tab_14">16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Answer Examples</head><p>Some examples consisting of only answers are shown in Table <ref type="table" target="#tab_15">17</ref> and Table <ref type="table" target="#tab_17">18</ref>. We remove citation marks for all the answers to evaluate answers only. In this example, WebGLM's answer is consistent with the question, while the answers of two WebGPT models are beside the point in the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Reward Model Example</head><p>WebGLM performs better after implementing the reward model. An example is shown in Table <ref type="table" target="#tab_9">19</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Criteria Details</head><p>The criteria details of human evaluation are shown in Table <ref type="table" target="#tab_3">20</ref> and Table <ref type="table" target="#tab_3">21</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Demo</head><p>We provide a web demo as shown in Figure <ref type="figure" target="#fig_10">8</ref>. Meanwhile, some real examples are shown in Figure <ref type="figure">9</ref> and the later figures.</p><p>Table <ref type="table" target="#tab_5">14</ref>: Dataset Example. The fifth reference is not cited because it is unrelated to the question(dirty data). In the training procedure, we need these unrelated references to train our model to pick related information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Why were the cities that are capital cities chosen as capitals? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>What is the difference between sigma bonds and pi bonds?</p><p>References from WebGPT-175B <ref type="bibr">[1]</ref> Pi bonds are formed by the sidewise positive (same phase) overlap of atomic orbitals along a direction perpendicular to the internuclear axis. During the formation of ? bonds, the axes of the atomic orbitals are parallel to each other whereas the overlapping is perpendicular to the internuclear axis. This type of covalent bonding is illustrated below. [Image: Pi Bonds] Pi Bonds are generally weaker than sigma bonds, owing to the significantly lower degree of overlapping. Generally, double bonds consist of one sigma and one pi bond, whereas a typical triple bond is made up of two ? bonds and one ? bond. It is important to note that a combination of sigma and pi bonds is always stronger than a single sigma bond.</p><p>[2] In chemistry, sigma bonds (? bonds) are the strongest type of covalent chemical bond. They are formed by head-on overlapping between atomic orbitals. Sigma bonding is most simply defined for diatomic molecules using the language and tools of symmetry groups. In this formal approach, a ?-bond is symmetrical with respect to rotation about the bond axis. By this definition, common forms of sigma bonds are ? + ?, ? ? + ? ? , ? + ? ? ????? 2 + ?? 2 (where z is defined as the axis of the bond or the internuclear axis). Quantum theory also indicates that molecular orbitals (MO) of identical symmetry actually mix or hybridize. As a practical consequence of this mixing of diatomic molecules, the wavefunctions ? + ????? ? + ? ? molecular orbitals become blended. The extent of this mixing (or hybridization or blending) depends on the relative energies of the MOs of like symmetry.</p><p>[3] A sigma bond is stronger than a pi bond. The reason is that the overlapping of atomic orbitals can take place to a greater extent during the formation of a sigma bond, whereas overlapping of orbitals occurs to a smaller extent during the formation of a pi bond. A pi bond between two atoms is formed only in addition to a sigma bond. The reason is that the atoms constituting a single bond prefer to form a strong sigma bond rather than a weak pi bond. Thus, a pi bond is always present in molecules with multiple bonds, i.e., double or triple bonds. In other words, a single bond cannot be a pi bond. There can be free rotation of atoms around the sigma bonds. Free rotation of atoms around pi bonds is not possible because it involves breaking the pi bonds. WebGLM displaying black on OLED screens does use less power than displaying white, because the LEDs that make up each pixel are off when displaying black. However, this is not the case for LCD monitors, where white is actually slightly more energy efficient than black. This is because LCD monitors rely on an array of thin-tube fluorescent bulbs that provide a constant source of light to create a white screen, and to make it black, LCDs rely on a diffuser to block this light. Therefore, displaying black does not necessarily use less energy, and may not even show truer black than white.</p><p>WebGPT-175B Most computer and mobile device displays do not use black pixels to save energy. Instead, they use an array of thin-tube fluorescent bulbs to provide a constant source of light to create a white screen. WebGPT-13B It depends on the resting state of the LCD as to whether they require energy to stop light or to allow light to pass through, " Heap explains. "This is why screen tests show that some CCFL (cold cathode fluorescent lamp) LCD screens save energy and some use a bit more. All of the scientific test data we have come across shows a slight saving on black LCD screens, which suggests that the rest state on many LCD screens does not allow light through. " Heap also points out that a large number of Blackle users come from India and South America, where CRTs are still commonly sold. Most LCD devices use a backlight which is always on. When an LCD is black, it is doing full light output, and then also blocking all of that light to create the appearence of black. If a monitor is displaying all-black, most will reduce power to the backlight accordingly, primarily to produce a deeper black, and of course also to save power.</p><p>Perplexity AI While it is true that black pixels use less energy than white pixels on some displays, such as OLEDs, this is not always the case. On LCDs, for example, black pixels still require power to be lit up. Additionally, the amount of energy used by a pixel depends on its color intensity, so even if a pixel is black it may still use some energy.</p><p>Human Writen It's not true for all types of screens. For LCD screens, energy cost is not definitely related to brightness. The principle of the LCD screen is that there is a white LED panel at the back, and the front uses the LCD panel to cover the pixels one by one. Turn down the backlight brightness to save power. The screen is adjusted to internal black, but the LCD panel is shielded to black, which will not save power. The power saving has nothing to do with the background color. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perplexity AI</head><p>The phenomenon of time seeming to pass faster when watching something for a second time is likely due to the recency effect, which amplifies the importance of recent stimuli or observations. Additionally, paying more attention to time passing can make it seem like it's going by faster, as can familiarity with the content. Finally, life-threatening situations, eye movements, tiredness, hypnosis, age and emotions can all warp our perception of time.</p><p>Human Writen When humans repeatedly look at the same thing, the brain will quickly recall the previous memory when it sees the same content. After people recall quickly in the brain, they can quickly grasp its contents, and feel the time has become faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 19: Comparison Example of GLM-10B-RM and GLM-10B-Rouge1</head><p>Question Why is it that soap can wash almost anything off of your hands but doesn't wash its fragrance off of you?</p><p>References <ref type="bibr">[1]</ref> Soap is able to clean hands and dishes because of some pretty nifty chemistry. Soap molecules have on one end what's known as a polar salt, which is hydrophilic, or attracted to water. The other end of the molecule is a nonpolar chain of fatty acids or hydrocarbons, which is hydrophobic-meaning that it's repelled by water but attracted to grease and other oily substances. When you wash your hands, the soap forms something like a molecular bridge between the water and the dirty, germ-laden oils on your hands, attaching to both the oils and the water and lifting the grime off and away. Soaps can also link up with the fatty membranes on the outside of bacteria and certain viruses, lifting the infectious agents off and even breaking them apart. Once the oily dirt and germs are off your hands, the soap molecules thoroughly surround them and form tiny clusters, known as micelles, that keep them from attaching to anything else while they wash down the drain.</p><p>[2] First, the hydrophilic ends of the soap molecule attach to the water, then the hydrophobic ends attach to the oils, pathogens, or other debris on the hand. After the soap has bonded with the germs on your hand the water can then wash it down the drain. This works because the soap molecule can attach to the germs more strongly than the germs can hold onto your skin.</p><p>[3] Therefore, when you wash with soap and water the soap molecules grab on to the fat molecules and pulls them off the skin into the rinse water. When your rinse the skin the fat and germs are removed from the skin. Therefore, the main reason why soap works to cleanse and sanitise is it literally washes everything off the skin.</p><p>WebGLM-10B-RM Soap is able to cleanse and sanitise hands and bodies because of its unique chemistry. Soap molecules have on one end a polar salt, which is attracted to water, and on the other end a nonpolar chain of fatty acids or hydrocarbons, which is hydrophobic and attracted to grease and other oily substances. When you wash your hands, the soap forms a molecular bridge between the water and the dirty, germ-laden oils on your hands, attaching to both the oils and the water and lifting the grime off and away. <ref type="bibr">[1]</ref> Soap molecules can also link up with the fatty membranes on the outside of bacteria and certain viruses, lifting the infectious agents off and even breaking them apart. Once the oily dirt and germs are off your hands, the soap molecules thoroughly surround them and form tiny clusters, known as micelles, that keep them from attaching to anything else while they wash down the drain. <ref type="bibr">[2]</ref> Therefore, the main reason why soap works to cleanse and sanitise is it literally washes everything off the skin. <ref type="bibr">[3]</ref> However, while soap can wash almost anything off of your hands, it cannot wash its fragrance off of you because fragrances are not dirt and are not removed by rinsing.</p><p>WebGLM-10B-Rouge1 The main reason why soap works to cleanse and sanitise is because of its molecules, which have on one end a polar salt, which is hydrophilic, or attracted to water, and on the other end a nonpolar chain of fatty acids or hydrocarbons, which are hydrophobic, meaning they are repelled by water but attracted to grease and other oily substances <ref type="bibr">[1]</ref>. Soap molecules form a molecular bridge between the water and the dirty, germ-laden oils on your hands, attaching to both the oils and the Water and lifting the grime off and away <ref type="bibr">[1][2]</ref>. Then, the soap molecules surround the oily dirt and germs and form tiny clusters, known as micelles, that keep them from attaching to anything else while they wash down the drain <ref type="bibr">[1]</ref>. However, the fragrance of soap is not removed this way because it is not bound to the fat molecules that are removed from the skin when you wash with soap and water <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 20: Criteria Details For References</head><p>Relevancy Evaluating how a reference text is related to the question and how much information of the question can be found in the reference. 0 The reference is almost not related to the question. 1 A few information or answers related to question can be found in the reference. 2 Most of the answers to the question can be found in the reference, but a small amount of reasoning or external information is still needed. 3 The complete answer to the question can be found in the reference Density Evaluating the proportion of information related to question in a reference text. 0 Reference content is almost irrelevant to questions and answers.</p><p>1 Reference has most content irrelevant to the problem, but there is still a small amount of content related to the problem. 2 Reference has a few irrelevant contents. 3 Reference is closely related to the problem, and almost the whole reference focuses on the problem.</p><p>Truthfulness Evaluating whether the text itself violates common sense, logic or contradictions. There is no need to pay attention to whether it is relevant to the question. 0 The text contains statements that are contrary to the facts, and the authenticity is low.</p><p>1 The text has no statements contrary to the facts and is highly authentic. Keep Blank If the evaluator cannot determine the authenticity through his or her own knowledge, leave it blank.</p><p>Toxicity ? Evaluating whether It involves violence, pornography, indecent words and other similar elements.</p><p>0 The above element does not exist.</p><p>1 One or more of the above elements exist.</p><p>Social Bias ? Evaluating whether there are prejudices against specific social groups, such as racial discrimination and gender discrimination. 0 The above element does not exist. 1 One or more of the above elements exist.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A screenshot of WebGLM's response to an example question with web references.</figDesc><graphic url="image-1.png" coords="1,320.36,303.78,235.44,153.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The win rates of popular web-enhanced QA systems against human references. WebGLM (10B) performs comparably to WebGPT (175B), approaching human-level QA ability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fine</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>[3]. Another reason is ... called gluconeogenesis[2]. Also, leptin levels can rapidly decline in ? (a) Prompt Formulation (b) Instruction Inducting (c) Few-shot In-context Learning Words considered bad relate to negative ways of talking about ...[1]  ... certain words.[2]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: We construct WebGLM-QA for generator training via LLM in-context bootstrapping. It includes three stages: 1) prompt formulation, 2) instruction inducting, and 3) few-shot in-context learning. In this way, we avoid the outrageous cost in time and money for hiring experts but still create a high-quality quoted long-formed QA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Win rates between systems. Numbers denote the rate that the answers from corresponding source from the first column are better than ones from corresponding source from the first row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Average score of answers in ELI5 test set. It is sorted by likes in the ELI5 test set. The best answer is around 0% and the worst answer is around 100%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>580.08 tokens/query 20 tokens/second = 29 seconds (4) ? ? (WebGPT-13B) = 580.08 tokens/query 100 tokens/second = 5.8 seconds (5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: WebGLM web demo page</figDesc><graphic url="image-8.png" coords="14,317.96,419.08,240.24,163.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :</head><label>910111213141516171819</label><figDesc>Figure 9: Real Example: How to balance career and hobbies?</figDesc><graphic url="image-9.png" coords="23,53.80,107.59,504.39,580.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 20 :Figure 21 :Figure 22 :Figure 23 :Figure 24 :</head><label>2021222324</label><figDesc>Figure 20: Real Example: Tell me about the movie Black Panther 2</figDesc><graphic url="image-20.png" coords="34,53.80,107.60,504.40,599.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 25 :Figure 26 :Figure 27 :Figure 28 :</head><label>25262728</label><figDesc>Figure 25: Real Example: What is copilot?</figDesc><graphic url="image-25.png" coords="39,53.80,107.60,504.40,517.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>studies suggest that the</figDesc><table><row><cell>Win Rate Against Human (%)</cell><cell>0 10 20 30 40 50 60</cell><cell>WebGLM (10B)</cell><cell>WebGPT (175B)</cell><cell>WebGPT (13B)</cell><cell>Perplexity.ai Human-level</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Evaluation on LLM's reference adoption.</figDesc><table><row><cell>Specifically, we find LLMs can</cell><cell></cell><cell></cell></row><row><cell>naturally distinguish and only</cell><cell></cell><cell></cell></row><row><cell>adopt useful references in in-</cell><cell></cell><cell></cell></row><row><cell>context learning (ICL). We create a 200-query dataset, where each</cell><cell>Method</cell><cell>Acc.</cell></row><row><cell>query is accompanied with 5 top-</cell><cell>Contriever</cell><cell>68.6%</cell></row><row><cell>ranked candidate references from</cell><cell cols="2">LLM ICL adoption 90.2%</cell></row><row><cell>Contriever. We manually annotate</cell><cell></cell><cell></cell></row><row><cell cols="2">the relevance of each piece of reference (Cf. Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Main results based on human evaluation metrics. Human evaluation results of generations on questions provided on the WebGPT demo website. For reference evaluation, Rel., Den., Tru., Tox?., and Soc. Bias? are the abbreviations corresponding to Relevancy, Density, Truthfulness, Toxicity, and Social Bias. For answer evaluation, Flu., Cor., Cit. Acc., Obj., Tru., Red.? correspond to Fluency, Correctness, Citation Accuracy, Objectivity, Truthfulness, and Redundancy.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">Reference Evaluation</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Answer Evaluation</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Rel.</cell><cell>Den.</cell><cell>Tru.</cell><cell>Tox.?</cell><cell>Soc. Bias?</cell><cell>Flu.</cell><cell>Cor.</cell><cell>Cit. Acc.</cell><cell>Obj.</cell><cell>Tru.</cell><cell>Red.?</cell></row><row><cell>WebGPT (175B)</cell><cell>2.512</cell><cell>2.660</cell><cell>0.996</cell><cell>0.015</cell><cell>0.006</cell><cell>2.457</cell><cell>2.889</cell><cell>2.837</cell><cell>0.990</cell><cell>0.975</cell><cell>0.087</cell></row><row><cell>Perplexity.ai</cell><cell>1.652</cell><cell>1.636</cell><cell>0.955</cell><cell>0.005</cell><cell>0.001</cell><cell>2.718</cell><cell>2.321</cell><cell>2.512</cell><cell>0.726</cell><cell>0.975</cell><cell>0.032</cell></row><row><cell>WebGPT (13B)</cell><cell>1.782</cell><cell>1.766</cell><cell>0.998</cell><cell>0.008</cell><cell>0.016</cell><cell>2.692</cell><cell>2.102</cell><cell>2.769</cell><cell>0.974</cell><cell>0.872</cell><cell>0.051</cell></row><row><cell>WebGLM (10B)</cell><cell>1.980</cell><cell>2.226</cell><cell>0.983</cell><cell>0.002</cell><cell>0.002</cell><cell>2.829</cell><cell>2.810</cell><cell>2.757</cell><cell>0.943</cell><cell>0.998</cell><cell>0.021</cell></row><row><cell cols="6">time consumption. Its performance is far superior to that of the 13B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">model and close to that of the 175B model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Natural Questions Web Questions</cell></row><row><cell>WebGLM</cell><cell>60.8</cell><cell>63.5</cell></row><row><cell>Perplexity.ai (sample)</cell><cell>57.3</cell><cell>57.5</cell></row><row><cell>GPT3-175B</cell><cell></cell><cell></cell></row></table><note><p>Open QA Performance on NaturalQuestions and We-bQuestions. Perplexity.ai is evaluated on sampled subsets because the website prohibits crawling.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>WebGLM, WebGPT and other comparison methods on TriviaQA. The setting follows WebGPT<ref type="bibr" target="#b23">[24]</ref> Appendix G.</figDesc><table><row><cell>Method</cell><cell>Total</cell><cell>Question overlap</cell><cell>No question overlap</cell><cell>Answer overlap</cell><cell>Answer overlap only</cell><cell>No overlap</cell></row><row><cell>Bigbird + WebGLM (Ours)</cell><cell>70.80%</cell><cell>86.40%</cell><cell>67.10%</cell><cell>78.70%</cell><cell>73.60%</cell><cell>49.30%</cell></row><row><cell>GPT-3 175B</cell><cell>58.70%</cell><cell>75.90%</cell><cell>52.90%</cell><cell>67.30%</cell><cell>61.60%</cell><cell>39.00%</cell></row><row><cell>GPT-3 175B + WebGPT 175B BC</cell><cell>69.50%</cell><cell>86.30%</cell><cell>65.30%</cell><cell>78.40%</cell><cell>73.20%</cell><cell>52.40%</cell></row><row><cell>UnitedQA-E</cell><cell>68.90%</cell><cell>89.30%</cell><cell>62.70%</cell><cell>78.60%</cell><cell>70.60%</cell><cell>44.30%</cell></row><row><cell>UnitedQA (hybrid model)</cell><cell>70.50%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on different dataset filtering strategies in creating the bootstrapped generator.</figDesc><table><row><cell cols="2">Filtering</cell><cell></cell><cell cols="3">Reference Evaluation</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Answer Evaluation</cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell>Rel.</cell><cell>Den.</cell><cell>Tru.</cell><cell>Tox.?</cell><cell>Soc. Bias?</cell><cell>Flu.</cell><cell>Cor.</cell><cell>Cit. Acc.</cell><cell>Tru.</cell><cell>Obj.</cell><cell>Red.?</cell></row><row><cell cols="2">None</cell><cell>1.711</cell><cell>1.619</cell><cell>0.991</cell><cell>0.011</cell><cell>0.011</cell><cell>2.872</cell><cell>2.636</cell><cell>2.370</cell><cell>2.810</cell><cell>0.805</cell><cell>0.134</cell></row><row><cell cols="2">Rouge-L</cell><cell>1.833</cell><cell>1.728</cell><cell>0.994</cell><cell>0.022</cell><cell>0.010</cell><cell>2.731</cell><cell>2.680</cell><cell>2.573</cell><cell>2.896</cell><cell>0.841</cell><cell>0.181</cell></row><row><cell cols="2">Rouge-1</cell><cell>1.832</cell><cell>1.751</cell><cell>0.993</cell><cell>0.010</cell><cell>0.012</cell><cell>2.826</cell><cell>2.694</cell><cell>2.688</cell><cell>2.919</cell><cell>0.890</cell><cell>0.120</cell></row><row><cell cols="7">Table 6: Ablation study on different dataset filtering strate-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">gies, based on GLM-2B's post-training evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Flu.</cell><cell cols="3">Cor. Cit. Acc. Obj.</cell><cell cols="2">Tru. Red.?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell cols="2">2.610 2.738</cell><cell>2.655</cell><cell cols="3">0.961 0.961 0.063</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Rouge-L 2.604 2.742</cell><cell>2.727</cell><cell cols="3">0.952 0.975 0.034</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Rouge-1 2.852 2.738</cell><cell>2.743</cell><cell cols="3">0.976 0.970 0.044</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">final version of the dataset. The other is comparing the performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">of our different versions of datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance of LLM-augmented Retriever (Ours). "N-NDCG" refers to Normalized NDCG. We collect 22000 examples from WebGLM-QA, and for each question, we calculate Rouge-1 precision score ? of corresponding answer ? and each of the reference ? , and then label the referenceanswer pair (?, ?) as ?. Finally, we gain a training dataset containing 20000 examples and a test dataset containing 2000 examples.</figDesc><table><row><cell>Metric(%)</cell><cell>TF-IDF</cell><cell>BM25</cell><cell>Contriever</cell><cell>Ours</cell></row><row><cell>Accuracy</cell><cell>46.85</cell><cell>40.33</cell><cell>18.54</cell><cell>69.36</cell></row><row><cell>Spearman</cell><cell>9.92</cell><cell>-20.94</cell><cell>-1.58</cell><cell>62.26</cell></row><row><cell>NDCG</cell><cell>82.54</cell><cell>76.28</cell><cell>81.16</cell><cell>91.99</cell></row><row><cell>N-NDCG</cell><cell>46.05</cell><cell>26.77</cell><cell>41.75</cell><cell>75.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on different sub-modules (Scorer, Retriever, and Generator) in WebGLM.</figDesc><table><row><cell>Method</cell><cell>Flu.</cell><cell>Cor.</cell><cell>Cit. Acc.</cell><cell>Obj.</cell><cell>Tru.</cell><cell>Red.?</cell></row><row><cell></cell><cell cols="2">Scorer Ablation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Scorer</cell><cell>2.797</cell><cell>2.757</cell><cell>2.723</cell><cell>0.961</cell><cell>0.970</cell><cell>0.039</cell></row><row><cell>Human Preference-aware Scorer (Ours)</cell><cell>2.829</cell><cell>2.810</cell><cell>2.757</cell><cell>0.943</cell><cell>0.998</cell><cell>0.021</cell></row><row><cell></cell><cell cols="3">Retriever Ablation (w.o. RM)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Retriever</cell><cell>2.364</cell><cell>1.982</cell><cell>-</cell><cell>-</cell><cell>0.645</cell><cell>0.091</cell></row><row><cell>WebGPT Retriever</cell><cell>2.750</cell><cell>2.884</cell><cell>2.808</cell><cell>0.981</cell><cell>0.980</cell><cell>0.038</cell></row><row><cell>Contriever</cell><cell>2.761</cell><cell>2.732</cell><cell>2.721</cell><cell>0.963</cell><cell>0.930</cell><cell>0.043</cell></row><row><cell>LLM-augmented Retriever (Ours)</cell><cell>2.797</cell><cell>2.757</cell><cell>2.723</cell><cell>0.961</cell><cell>0.970</cell><cell>0.039</cell></row><row><cell></cell><cell cols="3">Generator Ablation (w.o. RM)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-3 (text-davinci-003, zero-shot)</cell><cell>2.751</cell><cell>2.752</cell><cell>2.607</cell><cell>0.927</cell><cell>0.966</cell><cell>0.034</cell></row><row><cell>Bootstrapped Generator (Ours)</cell><cell>2.797</cell><cell>2.757</cell><cell>2.723</cell><cell>0.961</cell><cell>0.970</cell><cell>0.039</cell></row><row><cell>WebGLM (Ours)</cell><cell>2.829</cell><cell>2.810</cell><cell>2.757</cell><cell>0.943</cell><cell>0.998</cell><cell>0.021</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Different scorers' performance on ELI5 test set.</figDesc><table><row><cell></cell><cell cols="3">Accuracy Spearman N-NDCG</cell></row><row><cell>Classification (RoBERTa)</cell><cell>0.552</cell><cell>0.129</cell><cell>0.319</cell></row><row><cell>Regression (RoBERTa)</cell><cell>0.569</cell><cell>0.164</cell><cell>0.352</cell></row><row><cell>RM (ELI5)</cell><cell>0.568</cell><cell>0.197</cell><cell>0.406</cell></row><row><cell>RM (WebGLM)</cell><cell>0.596</cell><cell>0.241</cell><cell>0.367</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ablation study on different Generators based on WebGPT references</figDesc><table><row><cell>Generator</cell><cell>Flu.</cell><cell cols="2">Cor. Cit. Acc.</cell><cell>bj.</cell><cell>Tru.</cell><cell>Red.</cell></row><row><cell>GPT-3 In-Context</cell><cell cols="2">2.801 2.883</cell><cell>2.726</cell><cell cols="3">0.966 0.975 0.024</cell></row><row><cell>WebGPT-175B</cell><cell cols="2">2.457 2.889</cell><cell>2.837</cell><cell cols="3">0.990 0.975 0.087</cell></row><row><cell cols="3">WebGLM-10B-Rouge1 2.750 2.884</cell><cell>2.808</cell><cell cols="3">0.981 0.980 0.038</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Efficiency statistics for browsing stage in WebGPT-175B. Average count per query, tokens per action, and tokens per query (the product of the first two terms) are displayed in this table.</figDesc><table><row><cell>action</cell><cell cols="3">count/query tokens/action tokens/query</cell></row><row><cell>search</cell><cell>3.82</cell><cell>9.80</cell><cell>37.46</cell></row><row><cell>click_link</cell><cell>6.96</cell><cell>5.00</cell><cell>34.82</cell></row><row><cell>quote</cell><cell>3.49</cell><cell>124.49</cell><cell>434.80</cell></row><row><cell>back</cell><cell>5.35</cell><cell>1.00</cell><cell>5.35</cell></row><row><cell>scroll_down</cell><cell>11.41</cell><cell>4.00</cell><cell>45.63</cell></row><row><cell>scroll_up</cell><cell>1.62</cell><cell>4.00</cell><cell>6.49</cell></row><row><cell>top</cell><cell>0.49</cell><cell>1.00</cell><cell>0.49</cell></row><row><cell>end</cell><cell>0.43</cell><cell>3.00</cell><cell>1.29</cell></row><row><cell>find_in_page</cell><cell>0.13</cell><cell>5.11</cell><cell>0.68</cell></row><row><cell>invalid</cell><cell>0.12</cell><cell>111.09</cell><cell>13.07</cell></row><row><cell>tokens</cell><cell></cell><cell>580.08</cell><cell></cell></row><row><cell>generating speed</cell><cell></cell><cell>20 tokens/second</cell><cell></cell></row><row><cell>action time</cell><cell></cell><cell>29s</cell><cell></cell></row><row><cell>total time</cell><cell></cell><cell>52s</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Efficiency statistics for browsing stage in WebGPT-13B. Average count per query, tokens per action, and tokens per query (the product of the first two terms) are displayed in this table.</figDesc><table><row><cell>action</cell><cell cols="3">count/query tokens/action tokens/query</cell></row><row><cell>search</cell><cell>4.05</cell><cell>9.65</cell><cell>39.08</cell></row><row><cell>click_link</cell><cell>7.56</cell><cell>5.00</cell><cell>37.81</cell></row><row><cell>quote</cell><cell>3.44</cell><cell>125.85</cell><cell>433.08</cell></row><row><cell>back</cell><cell>5.90</cell><cell>1.00</cell><cell>5.90</cell></row><row><cell>scroll_down</cell><cell>10.30</cell><cell>4.00</cell><cell>41.21</cell></row><row><cell>scroll_up</cell><cell>2.01</cell><cell>4.00</cell><cell>8.04</cell></row><row><cell>top</cell><cell>0.32</cell><cell>1.00</cell><cell>0.32</cell></row><row><cell>end</cell><cell>0.44</cell><cell>3.00</cell><cell>1.33</cell></row><row><cell>find_in_page</cell><cell>0.21</cell><cell>5.04</cell><cell>1.06</cell></row><row><cell>invalid</cell><cell>0.10</cell><cell>136.58</cell><cell>13.06</cell></row><row><cell>tokens</cell><cell></cell><cell>580.89</cell><cell></cell></row><row><cell>generating speed</cell><cell></cell><cell>100 tokens/second</cell><cell></cell></row><row><cell>action time</cell><cell></cell><cell>5.8s</cell><cell></cell></row><row><cell>total time</cell><cell></cell><cell>31s</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>The performance with training data bootstrapped by difference prompt strategies.</figDesc><table><row><cell>Prompt</cell><cell>Flu.</cell><cell cols="3">Cor. Cit. Acc. Obj. Tru. Red.</cell></row><row><cell>WebGLM Prompt</cell><cell cols="2">2.797 2.757</cell><cell>2.723</cell><cell>0.961 0.970 0.039</cell></row><row><cell cols="3">Question before Reference 2.633 2.518</cell><cell>2.700</cell><cell>0.933 0.970 0.058</cell></row><row><cell>3-Reference</cell><cell cols="2">2.658 2.412</cell><cell>2.819</cell><cell>0.933 0.930 0.065</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16 :</head><label>16</label><figDesc>Retrieved Reference Example (WebGPTs)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 17 :</head><label>17</label><figDesc>Answer Example (1)QuestionWhen electronics show black, the pixels are not off. Wouldn't that use less energy, and show truer black?</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>To make it black, LCDs rely on a diffuser to block this light. As a result, LCDs use more energy than CRTs to display a black screen. Measuring a 17-inch (43-centimeter) LCD monitor, Schindler found that white required 22.6 watts, while black came in a tad higher at 23.2 watts. With a 20-inch (50.8-centimeter) LCD, black required 6 percent more energy than white. In contrast, AMOLED displays don't have a backlight at all. Instead, each pixel is like a tiny red, blue, or green colored light. If the screen is instructed to display black, it doesn't need to block any light, it simply doesn't light up any of the little colored sub-pixels. As a result, black pixels save you a lot of power because those pixels can be turned completely off.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 18 :</head><label>18</label><figDesc>Answer Example (2) Question Why does it seem like when I watch something the second time around, it goes by faster than the first time I watched it? WebGLM It seems like when you watch something the second time around, it goes by faster than the first time you watched it because you are focusing on the little details instead, which might make it feel like time is going by faster. Another reason could be that when you know what's coming, your mind breaks it up and you have intervals/milestones, and know when it's going to end, which helps you digest it easier. However, if you go back a third time, it may be ruined because all the things you could discover have been ruined by the second rerun. WebGPT-175B There are several different ideas about why this happens. One theory is that when you are cognitively engaged, meaning you are thinking about the task at hand and are busy doing it, you don't have the opportunity to notice the passage of time. When you're not engaged, like when you are bored, you have the time to think about the passage of time and so it seems like it goes by slower. Another theory is that the chemicals in your brain release dopamine when you are having fun, and it impacts how your brain perceives time. When you are having fun and your brain is releasing dopamine, time seems to pass faster. WebGPT-13B There are two main reasons time seems to pass faster the second time around. First, information our brains absorbs and processes increases with each viewing, so the more information there is, the slower time goes. Second, as we age, the rate at which we process visual information slows down, and this is what makes time 'speed up' as we grow older. The complexity of the information we examine and the networks of neurons in our brain increase as we age, and electrical signals must traverse greater distances, causing signal processing to take more time. This is what leads to time passing more rapidly.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/aaronsw/html2text</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://openaipublic.blob.core.windows.net/webgpt-answer-viewer/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://huggingface.co/google/bigbird-base-trivia-itc</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>This work is supported by <rs type="funder">Technology and Innovation Major Project of the Ministry of Science and Technology of China</rs> under Grant <rs type="grantNumber">2022ZD0118600</rs> and <rs type="grantNumber">2022ZD0118601</rs>, <rs type="funder">NSF of China for Distinguished Young Scholars</rs> (No. <rs type="grantNumber">61825602</rs>), <rs type="funder">NSF of China</rs> (No. <rs type="grantNumber">62276148</rs>), and a research fund from Zhipu.AI.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AbvRVKC">
					<idno type="grant-number">2022ZD0118600</idno>
				</org>
				<org type="funding" xml:id="_JgdXbmj">
					<idno type="grant-number">2022ZD0118601</idno>
				</org>
				<org type="funding" xml:id="_QHQVZdS">
					<idno type="grant-number">61825602</idno>
				</org>
				<org type="funding" xml:id="_pErvrf5">
					<idno type="grant-number">62276148</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>What is the difference between sigma bonds and pi bonds?</p><p>References from WebGLM <ref type="bibr">[1]</ref> Sigma bonds are often formed by the combination of s orbitals in different atoms whereas pi bonds are formed from the combination of p and similar orbitals in different atoms.</p><p>[2] The main differences between sigma and pi bonds has to do with their formation and strength. Sigma bonds are formed by direct head to head overlap between orbitals while pi bonds are formed by side to side overlap, usually between p orbitals. This difference in formation leads to a difference in strength. Sigma bonds are stronger than pi bonds because the direct head to head overlap provides a bigger (and therefore stronger) overlap than the side to side overlap of pi bonds. Additionally, sigma bonds form single bonds and can exist without a pi bond present; however, a sigma bond must already be formed for a pi bond to form.</p><p>[3] Sigma and pi bonds are two types of covalent bonds formed by the overlapping of atomic orbitals. Sigma bonds form by the direct head to head overlap of atomic orbitals and can occur between s-s, p-p and s-p orbitals. Pi bonds form by the side to side overlap of p orbitals.</p><p>[4] The sigma bond is stronger than the Pi bond, according to quantum mechanics, since the orbital paths of Pi bonds are parallel, resulting in less overlap between the p-orbitals. Two atomic orbitals come into contact with each other through two areas of overlap, forming Pi bonds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 21: Criteria Details For Answers</head><p>Fluency Evaluating whether grammar, spelling, word usage, etc. conform to people's grammar habits (please do not consider any semantic factors). 0 There are major errors in grammar and spelling, which make the text difficult to read. 1 There are small errors in grammar and spelling, which will slightly affect understanding. 2 There are a few grammatical, spelling or case errors that do not affect understanding. 3 Fluent language, correct grammar, no mistakes, easy to read. Correctness Evaluating whether the question is correctly answered.</p><p>0 No answer, or the answer is irrelevant or wrong. 1 A few answers are given, but they are particularly incomplete or fragmented. The question is basically not answered.</p><p>2 Basically answer the questions, but there are a few mistakes or omissions.</p><p>3 Answer the question perfectly.</p><p>Citation Accuracy Evaluating whether the reference marks in the answer are accurate. 0 The reference marks are basically wrong or there is no reference label. 1 There are a large number of missing and wrong marks.</p><p>2 There are a few missing and wrong marks.</p><p>3 The reference marks are completely accurate.</p><p>Objectivity Evaluating whether all the answers come from references. 0 There is external knowledge in the answer which does not come from references.</p><p>1 All answers can be based on the reference.</p><p>Truthfulness Evaluating whether the text itself violates common sense, logic or contradictions. There is no need to pay attention to whether it is relevant to the question. 0 The text contains statements that are contrary to the facts, and the authenticity is low.</p><p>1 The text has no statements contrary to the facts and is highly authentic. Keep Blank If the evaluator cannot determine the authenticity through his or her own knowledge, leave it blank.</p><p>Redundancy ? Evaluating whether there is redundancy in the answer, such as repeating the same sentence or the same fact repeatedly. 0 There is no redundancy. 1 There is redundancy.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14799</idno>
		<title level="m">Evaluation of text generation: A survey</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Palm: Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GLM: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ELI5: Long Form Question Answering</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3558" to="3567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Chang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Instruction induction: From few examples to natural language task descriptions</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Honovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10782</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised Dense Information Retrieval with Contrastive Learning</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?douard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Few-shot learning with retrieval augmented language models</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03299</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Survey of hallucination in natural language generation</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Frieske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etsuko</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dense Passage Retrieval for Open-Domain Question Answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning Inside Out</title>
		<meeting>Deep Learning Inside Out</meeting>
		<imprint>
			<date type="published" when="2022">2022. DeeLIO 2022</date>
			<biblScope unit="page" from="100" to="114" />
		</imprint>
	</monogr>
	<note>What Makes Good In-Context Examples for GPT-3?</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="857" to="876" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">GPT understands, too</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Chatterbots, tinymuds, and the turing test: Entering the loebner prize competition</title>
		<author>
			<persName><surname>Michael L Mauldin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noisy Channel Language Model Prompting for Few-Shot Text Classification</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5316" to="5330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.12837</idno>
		<title level="m">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09332</idno>
		<title level="m">Webgpt: Browser-assisted question-answering with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">choice</title>
		<imprint>
			<biblScope unit="volume">2640</biblScope>
			<biblScope unit="page">660</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How Much Knowledge Can You Pack Into the Parameters of a Language Model</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning To Retrieve Prompts for In-Context Learning</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<meeting>the 2022 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2655" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey of evaluation metrics used for NLG systems</title>
		<author>
			<persName><forename type="first">Akash</forename><surname>Ananya B Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Kumar Mohankumar</surname></persName>
		</author>
		<author>
			<persName><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ili?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagn?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gall?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">Bloom: A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Hongjin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.01975</idno>
		<title level="m">Selective annotation makes language models better few-shot learners</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An Explanation of In-context Learning as Implicit Bayesian Inference</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BERTScore: Evaluating Text Generation with BERT</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">In some cases, state officials wanted the capital located in the geographic center of their states. Jefferson City and Pierre were chosen as capitals of Missouri and South Dakota respectively for this reason. In other instances, a capital site may be picked because of its location relative to other cities. Florida chose the city of Tallahassee to be its capital in 1824 because it was situated halfway between St. Augustine and Pensacola, the state&apos;s two largest cities at the time. Some capital locations were picked because they were centers of trade and transportation</title>
		<imprint/>
	</monogr>
	<note>Reasons differ from state to state as to why the capital city was chosen. This is why Albany, located near the Hudson River, was picked as New York&apos;s capital</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ottawa and Washington became capital cities. Sometimes, the location of a new capital city was chosen to terminate actual or potential squabbling between various entities</title>
		<imprint>
			<pubPlace>Bern, Bras?lia, Canberra, Madrid; Canberra, Ottawa, Washington, Wellington and Managua</pubPlace>
		</imprint>
	</monogr>
	<note>The selection or founding of a &quot;neutral&quot; capital city, one unencumbered by regional or political identities, was meant to represent the unity of a new state when Ankara. such as in the cases of Bras?lia</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">However a country decides on its capital, that city becomes an important symbol of nationhood</title>
		<imprint/>
	</monogr>
	<note>While being a home to its inhabitants and a place for tourists to visit. it is also the city chosen to represent the entire country to the world</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">State capitals are important because they house the governments of their states, with a meeting place for the state legislature. Many meeting buildings are called capitols, which are distinguished from capitals in that capitol refers to buildings while capital always references a city. Cities are chosen to be the capital for differing reasons, sometimes because it was located in the center of the state, like Jefferson City, or was a center of commerce, like Albany. Today, many capitals are not the largest city in their states and can be overshadowed by bigger cities</title>
		<imprint/>
	</monogr>
	<note>but retain important historical value</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Capital cities: How are they chosen and what do they represent?</title>
	</analytic>
	<monogr>
		<title level="j">BBC News</title>
		<imprint>
			<date type="published" when="2017-12-06">6 December 2017. March 2022. 26 June 2022</date>
		</imprint>
	</monogr>
	<note>Not Cited) 8</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Answer The reasons why cities are chosen as capitals vary from state to state. In some cases, state officials wanted the capital located in the geographic center of their state[1][4], while in other instances, a capital site may be picked because of its location relative to other cities[1]. Some capital cities were picked because they were centers of trade and transportation[1], or to represent the unity of a new state[1][2][4]. In some cases, the location of a new capital city was chosen to terminate actual or potential squabbling between various entities[1][2]. Cities are chosen to be the capital because they house the governments of their states, with a meeting place for the state legislature[1][4]. Today, many capitals are not the largest city in their states and can be overshadowed by bigger cities</title>
		<imprint/>
	</monogr>
	<note>but retain important historical value[4]. The capital city also represents the entire country to the world[3</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">References from Perplexity AI [1] Sigma bond is a chemical bond formed by the linear or co-axial overlapping of the atomic orbitals of two atoms. A pi bond is a type of covalent bond</title>
		<imprint/>
	</monogr>
	<note>that exists between atoms where the electrons are on top and bottom of the axis connecting the nuclei of the joined atoms</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The sigma bond is stronger than the Pi bond, according to quantum mechanics, since the orbital paths of Pi bonds are parallel, resulting in less overlap between</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A ? bond has cylindrical symmetry; a ? bond has a nodal plane that includes the bond axis</title>
		<imprint/>
	</monogr>
	<note>A ? bond comes from the end-to-end overlap of the</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">What is the difference between Sigma Bond and pi Bond? ? Sigma bonds are formed by head to head overlapping of orbitals, whereas pi bonds are formed by the lateral overlapping. ? Sigma bonds are stronger than pi bonds. ? Sigma bonds can be formed between both s and p orbitals whereas pi bonds are mostly formed between p and d orbitals. ? The single covalent bonds between atoms are sigma bonds. When there are multiple bonds between atoms, pi bonds can be seen. ? pi bonds result in unsaturated molecules</title>
		<imprint/>
	</monogr>
	<note>Sigma bonds allow free rotation of atoms whereas pi bonds restrict the free rotation</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">is a bond formed by the overlap of orbitals in an end-to-end fashion, with the electron density concentrated between the nuclei of the bonding atoms. A pi bond (? bond) is a bond formed by the overlap of orbitals in a side-by-side fashion with the electron density</title>
		<imprint/>
	</monogr>
	<note>? bond. concentrated above and below the plane of the nuclei of the bonding atoms</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">the plane of electron density), where electron density is greatest. The end-to-end axial overlap of electrons attracts the nuclei to form a strong bond. In a pi bond, the p-orbitals overlap side-to-side above and below the nuclei (equatorial plane), so this attraction is more of a &quot;sideways</title>
		<imprint/>
	</monogr>
	<note>In a sigma bond, the electrons are in orbitals between the nuclei of the bonding atoms. in. pull as opposed to a direct line of pulling. Additionally, the election density is spread out over a greater area. Thus, sigma bonds are stronger than pi bonds</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
