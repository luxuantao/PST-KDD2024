<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting the Transferability of Adversarial Samples via Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weibin</forename><surname>Wu</surname></persName>
							<email>wbwu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxin</forename><surname>Su</surname></persName>
							<email>yxsu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xixian</forename><surname>Chen</surname></persName>
							<email>xixianchen@tencent.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shenglin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
							<email>king@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
							<email>lyu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<email>yuwingtai@tencent.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting the Transferability of Adversarial Samples via Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The widespread deployment of deep models necessitates the assessment of model vulnerability in practice, especially for safety-and security-sensitive domains such as autonomous driving and medical diagnosis. Transfer-based attacks against image classifiers thus elicit mounting interest, where attackers are required to craft adversarial images based on local proxy models without the feedback information from remote target ones. However, under such a challenging but practical setup, the synthesized adversarial samples often achieve limited success due to overfitting to the local model employed. In this work, we propose a novel mechanism to alleviate the overfitting issue. It computes model attention over extracted features to regularize the search of adversarial examples, which prioritizes the corruption of critical features that are likely to be adopted by diverse architectures. Consequently, it can promote the transferability of resultant adversarial instances. Extensive experiments on ImageNet classifiers confirm the effectiveness of our strategy and its superiority to state-of-the-art benchmarks in both white-box and black-box settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) have emerged as a cutting-edge solution to a broad spectrum of real-world applications, such as object detection, speech recognition, and machine translation <ref type="bibr" target="#b26">[27]</ref>. Despite the impressive performance of these deep learning techniques, they are surprisingly vulnerable to the so-called adversarial samples <ref type="bibr" target="#b35">[36]</ref>. For example, by imposing human-imperceptible noises on legitimate images purposefully, the resultant adversarial input can incur erroneous predictions from state-of-the-art image classifiers. It raises growing concerns over the reliability of these high-performance black boxes and hinders the deployment of these models in practice, especially in safetyand security-sensitive domains such as autonomous driving and medical diagnosis <ref type="bibr" target="#b2">[3]</ref>.</p><p>Attacks thus play an important part in evaluating a model and revealing its blind spots before deployment, and one of the most fundamental and recognized tasks is to generate adversarial images against DNN image classifiers <ref type="bibr" target="#b2">[3]</ref>. To simulate the threat a DNN image classifier may face, there are generally two kinds of threat models considered in the literature <ref type="bibr" target="#b19">[20]</ref>. One is white-box settings, where attackers have full access to the victim model, such as the model architectures and parameters. The other one is black-box settings, where attackers only possess query access to the target model, namely, offering input images and obtaining output predictions.</p><p>Corresponding to the threat models that they are tailored for, there exist two sorts of attacks: white-box attacks and black-box ones <ref type="bibr" target="#b19">[20]</ref>. White-box attacks can exploit the exact gradient information of the victim model to craft malicious instances <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5]</ref>, while black-box attacks can be further divided into two categories according to the mechanism attackers adopt <ref type="bibr" target="#b7">[8]</ref>. One is query-based, and the other one is transfer-based. Query-based black-box attacks usually require excessive queries before a successful trial <ref type="bibr" target="#b15">[16]</ref>. On the contrary, without the feedback information from the target model, transfer-based black-box attacks devise adversarial samples with off-the-shelf local models (i.e., source models) and directly harness the resultant example to fool the remote target model (i.e., victim models) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Among these two sorts of black-box attacks, the transfer-based one has attracted ever-increasing attention recently <ref type="bibr" target="#b7">[8]</ref>. In general, only costly query access to deployed models is available in practice. Therefore, whitebox attacks hardly reflect the possible threat to a model, while query-based attacks have less practical applicability than the transfer-based counterparts due to the prohibitive query cost they may incur <ref type="bibr" target="#b7">[8]</ref>.</p><p>Thanks to the observed cross-model transferability of adversarial samples, a popular practice is to freely employ any white-box attack strategy as transfer-based black-box attacks <ref type="bibr" target="#b20">[21]</ref>. Unfortunately, the malicious images synthesized by such a scheme are prone to overfit to the exclusive blind spots of the source model <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7]</ref>. Specifically, although the crafted adversarial samples can attack Inception V3 VGG <ref type="bibr" target="#b15">16</ref> ResNet V2</p><p>Figure <ref type="figure">1</ref>: The attention heatmaps of three representative models (VGG 16 <ref type="bibr" target="#b32">[33]</ref>, ResNet V2 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, and Inception V3 <ref type="bibr" target="#b34">[35]</ref>) for a cat prediction. The visualization is generated with the technique of <ref type="bibr" target="#b29">[30]</ref> as detailed in Section 4.2. Redder regions possess higher importance to the model decision.</p><p>the source model with near 100% success rates, they suffer from limited success against the target model.</p><p>In this work, we aim to promote such transfer-based attacks, which requires improving the transferability of adversarial samples crafted with white-box attack strategies. We expect that the crux is to guide the search of adversarial images towards the common vulnerable directions of both the source and the target models. Therefore, it inspires us to seek for the common characteristics of diverse models and exploit such information to ameliorate the overfitting issue.</p><p>We discover that before different models arrive at a correct decision, they should first extract various features and then weigh these features appropriately, namely, allocating suitable attention over extracted features <ref type="foot" target="#foot_0">1</ref> . Although some models may adopt exclusive feature extractors, the most critical features that diverse architectures employ tend to overlap largely in our numerous observations. For instance, as demonstrated in Figure <ref type="figure">1</ref>, when different models recognize a cat image, albeit one of the models (Inception V3) also looks for features extracted from the cat neck, all of them tend to pay attention to the face-related features spontaneously.</p><p>The similarity among different models in the employed features inspires us to exploit the model's attention to guide the search of adversarial perturbations. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the proposed strategy. In short, we first adopt back-propagated gradients to approximate the importance of different features to model decisions (i.e., attention extraction). Then we require the adversarial manipulation to contaminate attention-weighed feature outputs. As a result, the synthesized malicious noise can focus on undermining the most vital image features that the local source model employs (i.e., critical feature destruction). Since different models strongly rely on such features, we can alleviate overfitting to a specific source model and boost the transferability of resultant adversarial samples.</p><p>In summary, we would like to highlight the following   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>According to the knowledge of attackers, there are generally two categories of threat models in the literature <ref type="bibr" target="#b2">[3]</ref>. One is white-box settings where attackers acquire full access to the victim model, for example, the model architecture and parameters. The other one is black-box settings where adversaries only obtain query access, namely, image input uploading and prediction output downloading. Under both scenarios, attackers aim to synthesize adversarial samples to mislead learning algorithms by perturbing legitimate images in a human-unnoticeable manner. Corresponding to the setting that they are tailored for, attacks are coined white-box attacks and black-box ones <ref type="bibr" target="#b2">[3]</ref>.</p><p>The white-box attack enjoys great popularity among early work on attacking DNNs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref>. Different from the process of model training, they feature an optimization in input space to elevate training loss. Fast gradient sign method (FGSM) alters clean seed images by taking one step along with the sign of the gradient of the model loss function <ref type="bibr" target="#b8">[9]</ref>. Its successor, basic iterative method (BIM), iteratively applies FGSM perturbations of smaller magnitude to improve attack success rates <ref type="bibr" target="#b17">[18]</ref>. Projected gradient descent (PGD) extends BIM with random start to diversify the synthesized adversarial instances <ref type="bibr" target="#b21">[22]</ref>. Carlini and Wagner attacks (C&amp;W) devise a novel attack object to absorb the perturbation budget constraint <ref type="bibr" target="#b4">[5]</ref>, which also admits the employment of sophisticated optimizers like Adam <ref type="bibr" target="#b16">[17]</ref> during the search for deceptive noises. Jacobian-based Saliency Map Attack (JSMA) <ref type="bibr" target="#b24">[25]</ref> is tailored for seeking the adversarial noise with the minimal l 0 norm. Therefore, it proposes to prioritize the modification of the most important image pixels to model decisions.</p><p>However, white-box attacks hardly reflect the threat to models in practice since only query access is allowed in most realistic cases. Therefore, black-box attacks have attracted increasing attention recently. There are roughly two sorts of black-box attacks according to the mechanism they adopt. One is query-based <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref>, and the other one is transfer-based <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Query-based black-box attacks can settle the susceptible direction of the victim model as per the response of the target model to given inputs <ref type="bibr" target="#b9">[10]</ref>. Alternatively, attackers can approximate the loss gradient of the target model through training a local replica <ref type="bibr" target="#b23">[24]</ref> or finite difference techniques <ref type="bibr" target="#b1">[2]</ref>. However, such attacks usually require excessive queries before a successful trial and thus have limited applicability in practice <ref type="bibr" target="#b7">[8]</ref>.</p><p>Transfer-based black-box attacks are motivated by the transferability of adversarial samples across different models. Concretely, attackers first launch attacks on off-theshelf local models to which they have white-box access. Then the deceptive samples are directly transferred to fool the remote victim model. Therefore, attackers can apply any white-box attack algorithm in this task, such as FGSM and BIM. Unfortunately, such a straightforward strategy frequently suffers from overfitting to specific weaknesses of local source models and manifesting limited success. We show that by introducing regularizers into the optimization process of adversarial samples, we can significantly improve the performance of such transfer-based black-box attacks.</p><p>There also exist two sorts of methods to promote adversarial transferability. Ensemble-based mechanisms of-ten require the deduced distortion to remain harmful for an ensemble of models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32]</ref> or images <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref>. More related to our work is the regularization-based approach: transferable adversarial perturbation (TAP) introduced by <ref type="bibr" target="#b40">[41]</ref>. TAP injects two regularization terms into the vanilla training loss function of the model to guide the search of adversarial manipulations, which alleviates the issue of vanishing gradient and reduces the variations of resultant adversarial samples. We reveal that different models share similar attention when making correct predictions. Therefore, we can exploit this property to boost the transferability of malicious images.</p><p>There is a huge body of parallel proposals to enhance the robustness of deep models. Unfortunately, defenders appear to lag far behind in the arms race against adversaries due to the prevailing reactive defense methodology <ref type="bibr" target="#b2">[3]</ref>. Failed attempts include pre-processing the input images to diminish malicious noises <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1]</ref>, defensive distillation to mask gradients <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b5">6]</ref>, and feature squeezing to detect adversarial samples <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref>. Adversarial training arguably remains the most effective and promising defense to date, where defenders proactively craft deceptive images for their model and augment the clean training data with such instances to train the model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref>. Moreover, exploiting the malicious examples tailored for diverse hold-out models can further strengthen defense and confer robustness to transferbased black-box attacks <ref type="bibr" target="#b36">[37]</ref>. Therefore, we also employ state-of-the-art adversarial trained models to investigate the performance of our strategy against defended models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>We represent a DNN image classifier as a function f (x), which is usually a hierarchical composition of layers of neurons. It outputs the probability vector for a given image x, where f (x)[i] denotes the probability of the image x belonging to class i. We signify the hidden representation of x in layer k as A k (x) = f k (x), which consists of multiple feature maps. We will omit the input x when it is clear from the context. Therefore, A c k is the c th feature map in layer k, and A c k [m, n] is the output of the neuron with the spatial position [m, n] therein.</p><p>Given a model f , an adversarial counterpart x ′ of the clean image x with ground truth label t should satisfy the following two conditions:</p><formula xml:id="formula_0">argmax f (x ′ ) = t,<label>(1)</label></formula><p>and</p><formula xml:id="formula_1">||x ′ − x|| p ≤ ǫ.<label>(2)</label></formula><p>The first condition formulates the attack object, namely, misleading the target model to a wrong prediction with the malicious instance x ′ . The second condition ensures that the induced distortion to the original image x is imperceptible, since ǫ is usually a fairly small number. We adopt l ∞ norm in this work, as it is the most widely advocated in the community <ref type="bibr" target="#b8">[9]</ref>. We also note that our method is generally applicable to other norm choices. Let l(f (x), t) signify the loss function to guide the training of model f . Attackers can harness the training loss function as a surrogate for the attack object in Eq. ( <ref type="formula" target="#formula_0">1</ref>) and formulate the generation of an adversarial image x ′ as the optimization problem below:</p><formula xml:id="formula_2">maximize l(f (x ′ ), t), subject to ||x ′ − x|| p ≤ ǫ.</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Under the setup of transfer-based black-box attacks, attackers can only exploit off-the-shelf local models to manufacture deceptive samples. However, the solution to the above optimization problem of Eq. ( <ref type="formula">3</ref>) usually exhibits limited transferability due to overfitting to the source model.</p><p>To overcome the pitfall, we propose to augment the vanilla training loss function with an attention-based regularization term in Eq. ( <ref type="formula">3</ref>). It encourages the search toward harmful directions common to different deep architectures when updating the deceptive perturbations.</p><p>As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, we will first approximate model attention over extracted features with corresponding back-propagated gradients (Section 4.1). Then we formulate the destruction of attention-weighed combinations of feature maps as a regularization term to Eq. (3) (Section 4.3). Finally, we explain the algorithm we employ to solve the reformulated optimization problem for adversarial sample generation (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Attention Extraction</head><p>We suppose that transfer-based attackers can benefit from explicitly attacking hidden feature detectors within DNN image classifiers. Different from traditional image classification approaches that count on hand-designed features, deep learning-based image classifiers are renowned for their competence to automatically extract discriminative features from images <ref type="bibr" target="#b14">[15]</ref>. We can thus separate a DNN image classifier into two parts: a hierarchical feature extraction module and a softmax classifier. The learned feature extractors of a DNN image classifier are often so generic that they can adapt to different domains and tasks <ref type="bibr" target="#b30">[31]</ref>. Inspired by the fact, we expect that lots of feature descriptors are shared among different architectures for the same task, for example, the edge detector for face recognition. Therefore, if the synthesized adversarial noise can not only fool the final prediction of a target model, but also severely contaminate the extracted intermediate features, it is more likely to transfer across different models.</p><p>However, polluting the intermediate features under a restricted perturbation budget may still suffer from overfitting to a specific model, since there are some feature filters exclusive to the source model. To address the issue, we ask the deceptive noise to focus on undermining critical features for the model prediction. We assume that although different models may seek for distinct feature evidence to arrive at the final decision, the most crucial features one model pays attention to are frequently shared among various architectures. For example, for a cat image, it is very likely that different models all need to exploit the face-related features when making a correct prediction (Figure <ref type="figure">1</ref>).</p><p>Consequently, we need to derive the importance of diverse features to model decisions, namely, the model's attention. We regard one whole feature map as basis feature detectors. Therefore, we approximate the importance of feature map A c k (i.e., the c-th feature map in layer k) to class t with spatially pooled gradients:</p><formula xml:id="formula_3">α c k [t] = 1 Z m n ∂f (x)[t] ∂A c k [m, n] . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>Here Z is a normalizing constant such that</p><formula xml:id="formula_5">α c k [i] ∈ [−1, 1]. We name α k [t]</formula><p>the attention weight of the model to various features extracted in layer k regarding class t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attention Visualization</head><p>Built upon the deduced attention weights, we propose to visualize the attention maps of various models with the technique of <ref type="bibr" target="#b29">[30]</ref>. Such visualization aims to explore what the model attention looks like and examine whether distinct models showcase similar attentions for the same correctly classified image. Therefore, it serves as a proof of concept for our idea.</p><p>Specifically, we first scale different feature maps with corresponding model attention weights α c k [t]. Then we perform channel-wise summation of all feature maps in the same layer. After that, we proceed with a ReLU operation to derive the attention map for the label prediction t:</p><formula xml:id="formula_6">H t k = ReLU( c α c k [t] • A c k ).<label>(5)</label></formula><p>We apply the ReLU operation here to remove negative pixels in the attention map so that we can focus on supportive features, which have a positive influence on the class of interest. Negative pixels probably stands for features from other classes. We note that H t k is of the same spatial resolution as the feature maps in layer k. Since the size of the feature maps varies across different layers and models, we finally bilinearly interpolate the attention map to the same resolution as the input image for better comparison.</p><p>For the same cat image, Figure <ref type="figure">1</ref> displays the attention heatmaps of various ImageNet classifiers regarding the cat prediction. We note that all these models correctly classify the cat image. It corroborates our assumption that diverse models exhibit similar attention when making a correct prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Critical Feature Destruction</head><p>After obtaining the model attention, we can now ask the adversarial samples to not only mislead the final decision of the target model, but also destroy the vital intermediate features. We combine both goals as a novel surrogate attack object function for Eq. ( <ref type="formula" target="#formula_0">1</ref>):</p><formula xml:id="formula_7">maximize J(x, x ′ , t, f ), where J(x, x ′ , t, f ) = l(f (x ′ ), t)+ λ k ||H t k (x ′ ) − H t k (x)|| 2 . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>Here the first term in J is the vanilla training loss (i.e., the cross-entropy loss), and we maximize it to achieve the first goal. The second term measures the distance between attention-weighed combinations of original feature outputs and the corrupted counterparts. It corresponds to preferring great alterations to features with large attention weight and thus accounts for the second goal. λ is a tunable weight to control the regularization effect of the second term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Optimization Algorithm</head><p>After substituting the proposed attack object function (Eq. ( <ref type="formula" target="#formula_7">6</ref>)) for that in Eq. (3), we can now reformulate the manufacture of transferable adversarial samples as the following optimization problem:</p><formula xml:id="formula_9">maximize J(x, x ′ , t, f ), where J(x, x ′ , t, f ) = l(f (x ′ ), t)+ λ k ||H t k (x ′ ) − H t k (x)|| 2 , subject to ||x ′ − x|| p ≤ ǫ.<label>(7)</label></formula><p>Therefore, we can freely apply different backbone optimization algorithms to acquire a solution. For fair comparisons, the optimization strategy we apply in this paper is the same as the white-box benchmark (BIM), which is an iterative refinement of FGSM. Concretely speaking, BIM extends FGSM into an iterative procedure with a smaller step size ǫ ′ in each run:</p><formula xml:id="formula_10">x ′ k+1 = clip x,ǫ {x ′ k + ǫ ′ sign( ∂l(f (x ′ k ), t) ∂x )},<label>(8)</label></formula><p>where x ′ 0 = x, and clip x,ǫ {x ′ } conducts pixel-wise clipping for the resultant image x ′ . Accordingly, it guarantees that x ′ stays within the l ∞ ǫ-neighborhood of the seed image x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Attention-guided Transfer Attack (ATA)</head><p>Require: A classifier f , attack object function J (Eq. ( <ref type="formula" target="#formula_7">6</ref>)), a clean image x, and its ground-truth label t Require: The perturbation budget ǫ, iteration number K</p><formula xml:id="formula_11">Ensure: ||x ′ − x|| ∞ ≤ ǫ 1: ǫ ′ = ǫ K 2: x ′ 0 = x 3: for k = 0 to K − 1 do 4: x ′ k+1 = clip x,ǫ {x ′ k + ǫ ′ sign( ∂J(x, x ′ k , t, f ) ∂x )} 5: end for 6: return x ′ = x ′ K</formula><p>Algorithm 1 summarizes our algorithm to craft transferable adversarial samples. In short, it features an introduction of attention-based regularization term to the optimization procedure of BIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first elucidate the experimental setup in Section 5.1. Then we report the results of our attack against diverse top-performance models and make comparisons with numerous state-of-the-art benchmark approaches in Section 5.2. Subsequently, we investigate the effect of hyper-parameters on our attack success rates in Section 5.3. Finally, we verify the complementing effect of our strategy on compatible algorithms in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>We focus on attacking image classifiers trained on Im-ageNet <ref type="bibr" target="#b28">[29]</ref>, which is the most broadly recognized benchmark task for transfer-based black-box attacks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>. We follow the protocol of the baseline method <ref type="bibr" target="#b40">[41]</ref> to curate experimental datasets and target models for fair comparisons.</p><p>Dataset. We need two sorts of datasets to develop and assess our attacks, respectively. The development dataset is the ILSVRC 2012 validation dataset <ref type="bibr" target="#b28">[29]</ref>, where we finetune our hyper-parameters. The test data adopted to assess our technique is the ImageNet-compatible dataset released by the NeurIPS 2017 adversarial competition <ref type="bibr" target="#b19">[20]</ref>. This test set contains 1000 images that are not included in the original ImageNet dataset. Therefore, it satisfies the requirement of evaluating the generalization capability of attack algorithms in practice.</p><p>Target model. We examine our technique with both undefended and defended models. As for undefended models, we employ numerous top-performance models with diverse architectures, including ResNet V2 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, Inception V3 <ref type="bibr" target="#b34">[35]</ref>, Inception V4 <ref type="bibr" target="#b33">[34]</ref>, and Inception-ResNet V2 <ref type="bibr" target="#b33">[34]</ref>  We also attack the corresponding ensemble model (referred to as Ensemble), whose prediction is the average probability output of all the above models. When it comes to the defended models, we adopt multiple state-of-the-art adversarially trained models as remote targets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19]</ref>, since adversarial training is arguably the most promising and effective defense to date <ref type="bibr" target="#b21">[22]</ref>. These adversarially trained models include adversarially trained Inception V3 (Adv-Inc-v3), adversarially trained Inception-ResNet V2 (Adv-IncRes-v2), adversarially trained Inception V3 with deceptive samples from an ensemble of three models (Ens3-Adv-Inc-v3) and four models (Ens4-Adv-Inc-v3), respectively 3 .</p><p>Baseline. We compare the performance of our attack with three kinds of benchmark techniques. As a naive baseline attack, we attach Gaussian noise under the same norm constraint to clean images, which is denoted as the Random Noise attack. More importantly, we compare our strategies with diverse state-of-the-art white-box attacks, includ- 3 These models are all publicly available at https://github. com/tensorflow/models/tree/master/research/adv_ imagenet_models.</p><p>ing FGSM <ref type="bibr" target="#b8">[9]</ref>, BIM <ref type="bibr" target="#b17">[18]</ref>, C&amp;W <ref type="bibr" target="#b4">[5]</ref>, and JSMA <ref type="bibr" target="#b24">[25]</ref>, to showcase the effectiveness of our algorithm in alleviating the overfitting issue and improving the transferability of white-box attacks. Since the original C&amp;W implementation cannot strictly meet the l ∞ budget, we employ the modified l ∞ version of C&amp;W as introduced by <ref type="bibr" target="#b40">[41]</ref>, which can explicitly satisfy the l ∞ norm constraint. Similar to our strategy, TAP <ref type="bibr" target="#b40">[41]</ref> boosts adversarial transferability through two regularization terms and is the state-of-the-art approach under this category. Therefore, we also include TAP in the competing benchmarks.</p><p>Metric. We compare different attacks via the top-1 accuracy of target models. Accordingly, lower accuracy of victim models on the synthesized adversarial samples represents better attack performance.</p><p>Parameter. We only include the last convolutional layer of the source model in our regularization term based on our preliminary experiments. For fair comparisons, we adopt default parameters as recommended in benchmark approaches and Foolbox <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b27">28]</ref>. The random noise is sampled from a clipped normal distribution with mean 0 and variance 1.  Following <ref type="bibr" target="#b40">[41]</ref>, we fix the perturbation budget ǫ to 16 for all methods. We conduct grid search on the development dataset to settle the best hyper-parameter for our algorithm. In all our experiments, the attack iteration number K is set to 5. The regularization weight λ roughly balances the contribution of each term in the loss function J (Eq. ( <ref type="formula" target="#formula_7">6</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Transferability of Attacks</head><p>Here we study the performance of our attack against both undefended and defended models. Specifically, we first fix a source model and run our algorithm on the model to produce adversarial samples. The resultant samples are then directly fed to the source model and other different models to simulate the white-box and black-box setups, respectively.</p><p>We first attack undefended models, and Table <ref type="table" target="#tab_1">1</ref> reports the results. We make the following observations. First, all these models possess impressive clean accuracy and appear resistant to random noise. Models with higher capacity usually exhibit better performance. Second, under white-box setups, BIM is the winning attack. Our algorithm achieves matching results to BIM and significantly outperforms the others. Third, under black-box settings, our attack significantly boosts the transferability of BIM. For example, when employing Inception V3 as the source model, our attack witnesses an average gain of 40.4% on attack success rates compared to BIM. Moreover, we defeat all the other benchmark methods with a significant margin, except for two cases, where we only lag a little behind TAP. We note that TAP employs two regularization terms, one for maximizing internal feature distances and the other for smoothing resultant perturbations. Contrarily, by applying only one regularization term to maximize attention-weighed internal feature distances, our method outperforms TAP in almost all cases. We next attack models defended by adversarial training. For fair comparisons with the baseline approach <ref type="bibr" target="#b40">[41]</ref>, we stick to employing undefended models as local source models. Therefore, we explore a more challenging black-box scenario where the source and target models possess more distinct property. We present the results in Table <ref type="table" target="#tab_3">2</ref>. We draw the following conclusions. First, we consistently improve the transferability of BIM to a great extent. For example, we increase the attack success rate of BIM by 29.3% on average, when applying Inception V3 as the source model. Second, our ATA remarkably outperforms all the other benchmarks except for two cases, where we only slightly lag behind TAP.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> displays one generated adversarial image against Inception V3 with our attack. We note that the deduced manipulations to the clean image are hardly visible. It confirms that our attack is stealthy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of Hyper-parameters on Attack Success Rates</head><p>The regularization weight λ is the dominant hyperparameter in our algorithm, and here we explore its effect on our attack success rates. Specifically, we vary λ while keeping the other parameters fixed to synthesize adversarial samples. Similar to previous experiments, we report the top-1 accuracy of target models on the resultant malicious  examples to measure the attack success rates.</p><p>Figure <ref type="figure">4</ref> illustrates the effect of λ on attack success rates against all undefended and defended models, where the source model is Inception V3. We vary λ from 1 × 10 −4 to 1 with a step size of 1 in log scale. We observe similar trends when employing other source models and thus omit such results. We note that there is generally a tradeoff between the two terms in J (Eq. ( <ref type="formula" target="#formula_7">6</ref>)). Because under a restricted perturbation budget, it is crucial to balance the contribution from each term to alleviate overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Complementing Effect of the Proposed Strategy</head><p>In principle, our strategy is compatible with other transfer-based black-box attacks. Therefore, we can conveniently integrate the proposed technique with such algorithms. We select two sorts of cutting-edge transfer-based attacks to corroborate the complementing effect introduced by our strategy. One is the ensemble-based translationinvariant attack (TI) developed by <ref type="bibr" target="#b7">[8]</ref>, and the other one is the regularization-based transferable adversarial perturbation (TAP) proposed by <ref type="bibr" target="#b40">[41]</ref>. With the integrated attacks, we conduct experiments similar to Section 5.2. Specifically, the combination of TI and ATA will only modify the update rule of Algorithm 1 as:</p><formula xml:id="formula_12">x ′ k+1 = clip x,ǫ {x ′ k +ǫ ′ sign(W * ∂J(x, x ′ k , t, f ) ∂x )}, (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where W is a pre-defined kernel, and * signifies convolution operation. The integration of TAP and ATA only adds the following term into the attack object function J (Eq. ( <ref type="formula" target="#formula_7">6</ref>)):</p><formula xml:id="formula_14">η||S * (x ′ − x)|| 1 ,<label>(10)</label></formula><p>where S is a pre-defined convolution filter. We abandon the other term in TAP for simplicity because we do not have the issue of vanishing gradients. Table <ref type="table" target="#tab_4">3</ref> shows the results with Inception V4 as the source model. In black-box settings, our strategy promotes the average attack success rate of TAP and TI by 6.8% and 4.6%, respectively. In white-box settings, our strategy can also further improve their attack success rates. Therefore, it corroborates the complementing effect of our technique to existing efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we introduce an attention-guided transfer attack to synthesize adversarial samples against black-box DNNs without any feedback information from the target model. The proposed strategy exploits the attention of the source model to regularize the search direction for adversarial samples. Consequently, it can focus on undermining critical features that different models count on and manifest remarkable transferability. We conduct extensive experiments to validate the effectiveness of our approach and confirm its superiority to state-of-the-art baselines. Therefore, our attack can more faithfully expose the vulnerability of deep models and serve as a strong benchmark when examining defenses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The proposed procedure of model attention extraction and its application to guide the search of deceptive samples towards critical feature destruction. See Section 4 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A clean source image and the corresponding adversarial image crafted with the proposed ATA. The target model is Inception V3. Although the perturbation is imperceptible to humans, it can successfully fool topperformance models.</figDesc><graphic url="image-12.png" coords="7,350.25,81.96,74.24,74.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 4 :</head><label>44</label><figDesc>Figure 4: The effect of hyper-parameter on attack success Figure 4: The effect of hyper-parameter λ on our attack success rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>2 . Accuracy of undefended models under attacks. The first column shows the source model employed, while the first row states the remote target models.</figDesc><table><row><cell></cell><cell>Attack</cell><cell cols="5">ResNet V2 Inception V3 Inception V4 Inception-ResNet V2 Ensemble</cell></row><row><cell></cell><cell>No Perturbation</cell><cell>89.6%</cell><cell>96.4%</cell><cell>97.6%</cell><cell>100%</cell><cell>99.8%</cell></row><row><cell></cell><cell>Random Noise</cell><cell>84.5%</cell><cell>91.7%</cell><cell>94.6%</cell><cell>97.8%</cell><cell>98.1%</cell></row><row><cell></cell><cell>FGSM</cell><cell>14.6%</cell><cell>56.3%</cell><cell>64.8%</cell><cell>66.8%</cell><cell>63.1%</cell></row><row><cell></cell><cell>BIM</cell><cell>4.4%</cell><cell>53.2%</cell><cell>62.0%</cell><cell>63.8%</cell><cell>54.3%</cell></row><row><cell>ResNet V2</cell><cell>C&amp;W</cell><cell>37.7%</cell><cell>94.5%</cell><cell>96.4%</cell><cell>98.5%</cell><cell>98.5%</cell></row><row><cell></cell><cell>JSMA</cell><cell>27.2%</cell><cell>59.3%</cell><cell>65.2%</cell><cell>62.1%</cell><cell>64.4%</cell></row><row><cell></cell><cell>TAP</cell><cell>9.5%</cell><cell>51.2%</cell><cell>60.1%</cell><cell>55.5%</cell><cell>50.3%</cell></row><row><cell></cell><cell>ATA</cell><cell>8.7%</cell><cell>52.9%</cell><cell>58.3%</cell><cell>55.1%</cell><cell>49.4%</cell></row><row><cell></cell><cell>FGSM</cell><cell>65.7%</cell><cell>27.2%</cell><cell>70.2%</cell><cell>72.9%</cell><cell>76.2%</cell></row><row><cell></cell><cell>BIM</cell><cell>76.8%</cell><cell>0.01%</cell><cell>67.7%</cell><cell>70.2%</cell><cell>73.6%</cell></row><row><cell>Inception V3</cell><cell>C&amp;W</cell><cell>86.9%</cell><cell>24.5%</cell><cell>93.5%</cell><cell>96.2%</cell><cell>96.0%</cell></row><row><cell></cell><cell>JSMA</cell><cell>66.4%</cell><cell>22.4%</cell><cell>57.2%</cell><cell>60.3%</cell><cell>68.9%</cell></row><row><cell></cell><cell>TAP</cell><cell>48.2%</cell><cell>0.1%</cell><cell>24.5%</cell><cell>26.3%</cell><cell>34.2%</cell></row><row><cell></cell><cell>ATA</cell><cell>47.2%</cell><cell>0.1%</cell><cell>22.1%</cell><cell>25.7%</cell><cell>31.9%</cell></row><row><cell></cell><cell>FGSM</cell><cell>68.3%</cell><cell>67.1%</cell><cell>50.3%</cell><cell>72.8%</cell><cell>76.4%</cell></row><row><cell></cell><cell>BIM</cell><cell>62.1%</cell><cell>40.9%</cell><cell>0.9%</cell><cell>69.1%</cell><cell>55.5%</cell></row><row><cell>Inception V4</cell><cell>C&amp;W</cell><cell>86.7%</cell><cell>91.7%</cell><cell>49.5%</cell><cell>93.2%</cell><cell>92.9%</cell></row><row><cell></cell><cell>JSMA</cell><cell>70.7%</cell><cell>68.9%</cell><cell>30.0%</cell><cell>65.2%</cell><cell>68.9%</cell></row><row><cell></cell><cell>TAP</cell><cell>58.4%</cell><cell>27.3%</cell><cell>1.8%</cell><cell>24.2%</cell><cell>51.7%</cell></row><row><cell></cell><cell>ATA</cell><cell>59.9%</cell><cell>24.8%</cell><cell>0.9%</cell><cell>22.1%</cell><cell>50.3%</cell></row><row><cell></cell><cell>FGSM</cell><cell>71.7%</cell><cell>69.0%</cell><cell>76.5%</cell><cell>57.2%</cell><cell>78.7%</cell></row><row><cell></cell><cell>BIM</cell><cell>60.4%</cell><cell>41.5%</cell><cell>51.5%</cell><cell>1.2%</cell><cell>54.5%</cell></row><row><cell>Inception-ResNet V2</cell><cell>C&amp;W</cell><cell>85.6%</cell><cell>91.7%</cell><cell>92.4%</cell><cell>49.0%</cell><cell>93.5%</cell></row><row><cell></cell><cell>JSMA</cell><cell>55.4%</cell><cell>62.7%</cell><cell>66.8%</cell><cell>50.3%</cell><cell>64.9%</cell></row><row><cell></cell><cell>TAP</cell><cell>53.3%</cell><cell>25.9%</cell><cell>33.2%</cell><cell>4.8%</cell><cell>48.2%</cell></row><row><cell></cell><cell>ATA</cell><cell>49.8%</cell><cell>22.1%</cell><cell>30.1%</cell><cell>1.2%</cell><cell>45.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of adversarially trained models under attacks. The first column shows the source model employed, while the first row states the remote target models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of models under attacks that combine the proposed ATA and compatible algorithms.</figDesc><table><row><cell>Attack</cell><cell cols="5">ResNet Inception Inception Inception-Ensemble V2 V3 V4 ResNet V2</cell><cell cols="2">Adv-Inc-v3 IncRes-v2 Adv-</cell><cell cols="2">Ens3-Adv-Ens4-Adv-Inc-v3 Inc-v3</cell></row><row><cell>TAP</cell><cell>58.4%</cell><cell>27.3%</cell><cell>1.8%</cell><cell>24.2%</cell><cell>51.7%</cell><cell>65.3%</cell><cell>90.4%</cell><cell>83.2%</cell><cell>87.3%</cell></row><row><cell cols="2">TAP+ATA 53.6%</cell><cell>22.7%</cell><cell>0.8%</cell><cell>19.8%</cell><cell>48.1%</cell><cell>57.9%</cell><cell>85.3%</cell><cell>73.2%</cell><cell>72.9%</cell></row><row><cell>TI</cell><cell>57.1%</cell><cell>30.9%</cell><cell>2.1%</cell><cell>26.9%</cell><cell>58.3%</cell><cell>62.7%</cell><cell>91.4%</cell><cell>81.9%</cell><cell>83.5%</cell></row><row><cell>TI+ATA</cell><cell>56.2%</cell><cell>24.9%</cell><cell>0.7%</cell><cell>24.2%</cell><cell>50.1%</cell><cell>57.9%</cell><cell>88.2%</cell><cell>76.9%</cell><cell>77.6%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this work, we consistently employ the term "feature" to refer to the hidden representations of images extracted by middle layers of DNNs, rather than the raw image pixels.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">These pre-trained models are all publicly available at https:// github.com/Cadene/pretrained-models.pytorch.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank anonymous reviewers for their valuable comments. The work described in this paper was supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (CUHK 14210717 of the General Research Fund and CUHK 2300174 of the Collaborative Research Fund, No. C5026-18GF).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Practical black-box attacks on deep neural networks using efficient query mechanisms</title>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="158" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06705</idno>
		<title level="m">On evaluating adversarial robustness</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2006">2017. 1, 3, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">EAD: Elastic-net attacks to deep neural networks via adversarial examples</title>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9185" to="9193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evading defenses to transferable adversarial examples by translation-invariant attacks</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2008">2019. 1, 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2006">2015. 1, 3, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple black-box adversarial attacks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial example defense: Ensembles of weak defenses are not strong</title>
		<author>
			<persName><forename type="first">Warren</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Workshop on Offensive</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning multiple layers of representation</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Black-box adversarial attacks with limited queries and information</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2142" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial attacks and defences competition</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The NIPS&apos;17 Competition: Building Intelligent Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and blackbox attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1765" to="1773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Asia Conference on Computer and Communications Security (ASIA CCS)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE European Symposium on Security and Privacy (Eu-roS&amp;P)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on deep learning: Algorithms, techniques, and applications</title>
		<author>
			<persName><forename type="first">Samira</forename><surname>Pouyanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saad</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiman</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Presa</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei-Ling</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Ching</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Computing Surveys (CSUR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Foolbox: A python toolbox to benchmark the robustness of machine learning models</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Dung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName><surname>Caad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01268</idno>
		<title level="m">Generating transferable adversarial examples</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirtyfirst AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep Validation: Toward detecting real-world corner cases for deep neural networks</title>
		<author>
			<persName><forename type="first">Weibin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanqiang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Dependable Systems and Networks (DSN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="125" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature Squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Network and Distributed System Security Symposium (NDSS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transferable adversarial perturbations</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
