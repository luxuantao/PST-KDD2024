<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 ADAGCN: ADABOOSTING GRAPH CONVOLUTIONAL NETWORKS INTO DEEP MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 ADAGCN: ADABOOSTING GRAPH CONVOLUTIONAL NETWORKS INTO DEEP MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The design of deep graph models still remains to be investigated and the crucial part is how to explore and exploit the knowledge from different hops of neighbors in an efficient way. In this paper, we propose a novel RNN-like deep graph neural network architecture by incorporating AdaBoost into the computation of network; and the proposed graph convolutional network called AdaGCN (Adaboosting Graph Convolutional Network) has the ability to efficiently extract knowledge from high-order neighbors of current nodes and then integrates knowledge from different hops of neighbors into the network in an Adaboost way. Different from other graph neural networks that directly stack many graph convolution layers, AdaGCN shares the same base neural network architecture among all "layers" and is recursively optimized, which is similar to a RNN. Besides, We also theoretically established the connection between AdaGCN and existing graph convolutional methods, presenting the benefits of our proposal. Finally, extensive experiments demonstrate the consistent state-of-the-art prediction performance on graphs across different label rates and the computational advantage of our approach AdaGCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, research related to learning on graph structural data has gained considerable attention in machine learning community. Graph neural networks <ref type="bibr" target="#b12">(Gori et al., 2005;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017;</ref><ref type="bibr" target="#b33">Veli?kovi? et al., 2018)</ref>, particularly graph convolutional networks <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b8">Defferrard et al., 2016;</ref><ref type="bibr" target="#b4">Bruna et al., 2014)</ref> have demonstrated their remarkable ability on node classification <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2017)</ref>, link prediction <ref type="bibr" target="#b38">(Zhu et al., 2016)</ref> and clustering tasks <ref type="bibr" target="#b9">(Fortunato, 2010)</ref>. Despite their enormous success, almost all of these models have shallow model architectures with only two or three layers. The shallow design of GCN appears counterintuitive as deep versions of these models, in principle, have access to more information, but perform worse. Oversmoothing <ref type="bibr" target="#b22">(Li et al., 2018)</ref> has been proposed to explain why deep GCN fails, showing that by repeatedly applying Laplacian smoothing, GCN may mix the node features from different clusters and makes them indistinguishable. This also indicates that by stacking too many graph convolutional layers, the embedding of each node in GCN is inclined to converge to certain value <ref type="bibr" target="#b22">(Li et al., 2018)</ref>, making it harder for classification. These shallow model architectures restricted by oversmoothing issue limit their ability to extract the knowledge from high-order neighbors, i.e., features from remote hops of neighbors for current nodes. Therefore, it is crucial to design deep graph models such that high-order information can be aggregated in an effective way for better predictions.</p><p>There are some works <ref type="bibr">(Xu et al., 2018b;</ref><ref type="bibr" target="#b23">Liao et al., 2019;</ref><ref type="bibr" target="#b20">Klicpera et al., 2018;</ref><ref type="bibr" target="#b21">Li et al., 2019)</ref> that tried to address this issue partially, and the discussion can refer to Appendix A.1. By contrast, we argue that a key direction of constructing deep graph models lies in the efficient exploration and effective combination of information from different orders of neighbors. Due to the apparent sequential relationship between different orders of neighbors, it is a natural choice to incorporate boosting algorithm into the design of deep graph models. As an important realization of boosting theory, AdaBoost <ref type="bibr" target="#b10">(Freund et al., 1999)</ref> is extremely easy to implement and keeps competitive in terms of both practical performance and computational cost <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>. Moreover, boosting theory has been used to analyze the success of ResNets in computer vision <ref type="bibr" target="#b16">(Huang et al., 2018)</ref> and <ref type="bibr">AdaGAN (Tolstikhin et al., 2017)</ref> has already successfully incorporated boosting algorithm into the training of GAN <ref type="bibr" target="#b11">(Goodfellow et al., 2014)</ref>.</p><p>In this work, we focus on incorporating AdaBoost into the design of deep graph convolutional networks in a non-trivial way. Firstly, in pursuit of the introduction of AdaBoost framework, we refine the type of graph convolutions and thus obtain a novel RNN-like GCN architecture called AdaGCN. Our approach can efficiently extract knowledge from different orders of neighbors and then combine these information in an AdaBoost manner with iterative updating of the node weights. Also, we compare our AdaGCN with existing methods from the perspective of both architectural difference and feature representation power to show the benefits of our method. Finally, we conduct extensive experiments to demonstrate the consistent state-of-the-art performance of our approach across different label rates and computational advantage over other alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OUR APPROACH: ADAGCN</head><formula xml:id="formula_0">2.1 ESTABLISHMENT OF ADAGCN Consider an undirected graph G = (V, E) with N nodes v i ? V, edges (v i , v j ) ? E. A ? R N ?N</formula><p>is the adjacency matrix with corresponding degree matrix D ii = j A ij . In the vanilla GCN model <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2017)</ref> for semi-supervised node classification, the graph embedding of nodes with two convolutional layers is formulated as:</p><formula xml:id="formula_1">Z = ? ReLU( ?XW (0) )W (1)</formula><p>(1)</p><p>where Z ? R N ?K is the final embedding matrix (output logits) of nodes before softmax and K is the number of classes. X ? R N ?C denotes the feature matrix where C is the input dimension. ? = D-1 2 ? D-1 2 where ? = A + I and D is the degree matrix of ?. In addition, W (0) ? R C?H is the input-to-hidden weight matrix for a hidden layer with H feature maps and W (1) ? R H?K is the hidden-to-output weight matrix.</p><p>Our key motivation of constructing deep graph models is to efficiently explore information of highorder neighbors and then combine these messages from different orders of neighbors in an AdaBoost way. Nevertheless, if we naively extract information from high-order neighbors based on GCN, we are faced with stacking l layers' parameter matrix W (i) , i = 0, ..., l -1, which is definitely costly in computation. Besides, Multi-Scale Deep Graph Convolutional Networks <ref type="bibr" target="#b25">(Luan et al., 2019)</ref> also theoretically demonstrated that the output can only contain the stationary information of graph structure and loses all the local information in nodes for being smoothed if we simply deepen GCN. Intuitively, the desirable representation of node features does not necessarily need too many nonlinear transformation f applied on them. This is simply due to the fact that the feature of each node is normally one-dimensional sparse vector rather than multi-dimensional data structures, e.g., images, that intuitively need deep convolution network to extract high-level representation for vision tasks. This insight has been empirically demonstrated in many recent works <ref type="bibr" target="#b34">(Wu et al., 2019;</ref><ref type="bibr" target="#b20">Klicpera et al., 2018;</ref><ref type="bibr">Xu et al., 2018a)</ref>, showing that a two-layer fully-connected neural networks is a better choice in the implementation. Similarly, our AdaGCN also follows this direction by choosing an appropriate f in each layer rather than directly deepen GCN layers. Thus, we propose to remove ReLU to avoid the expensive joint optimization of multiple parameter matrices. Similarly, Simplified Graph Convolution (SGC) <ref type="bibr" target="#b34">(Wu et al., 2019</ref>) also adopted this practice, arguing that nonlinearity between GCN layers is not crucial and the majority of the benefits arises from local weighting of neighboring features. Then the simplified graph convolution is:</p><formula xml:id="formula_2">Z = ?l XW (0) W (1) ? ? ? W (l-1) = ?l X W ,<label>(2)</label></formula><p>where we collapse W (0) W (1) ? ? ? W (l-1) as W and ?l denotes ? to the l-th power. In particular, one crucial impact of ReLU in GCN is to accelerate the convergence of matrix multiplication since the ReLU is a contraction mapping intuitively. Thus, the removal of ReLU operation could also alleviate the oversmoothing issue, i.e. slowering the convergence of node embedding to indistinguishable ones <ref type="bibr" target="#b22">(Li et al., 2018)</ref>. Additionally, without ReLU this simplified graph convolution is also able to avoid the aforementioned joint optimization over multiple parameter matrices, resulting in computational benefits. Nevertheless, we find that this type of stacked linear transformation from graph convolution has insufficient power in representing information of high-order neighbors, which is revealed in our experiment described in Appendix A.2. Therefore, we propose to utilize an appropriate nonlinear function f ? , e.g., a two-layer fully-connected neural network, to replace the ? sharing the same neural network architecture f ? . w l and ? l denote node weights and parameters computed after the l-th base classifier, respectively. linear transformation W in Eq. 2 and enhance the representation ability of each base classifier in AdaGCN as follows:</p><formula xml:id="formula_3">Z (l) = f ? ( ?l X),<label>(3)</label></formula><p>where Z (l) represents the final embedding matrix (output logits before Softmax) after the l-th base classifier in AdaGCN . This formulation also implies that the l-th base classifier in AdaGCN is extracting knowledge from features of current nodes and their l-th hop of neighbors. Due to the fact that the function of l-th base classifier in AdaGCN is similar to that of the l-th layer in other traditional GCN-based methods that directly stack many graph convolutional layers, we regard the whole part of l-th base classifier as the l-th layers in AdaGCN. As for the realization of Multi-class AdaBoost, we apply SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss function) algorithm <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>, a natural and clean multi-class extension of the two-class AdaBoost adaptively combining weak classifiers.</p><p>As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, we apply base classifier f (l)</p><p>? to extract knowledge from current node feature and l-th hop of neighbors by minimizing current weighted loss. Then we directly compute the weighted error rate err (l) and corresponding weight ? (l) of current base classifier f (l) ? as follows:</p><formula xml:id="formula_4">err (l) = n i=1 w i I c i = f (l) ? (x i ) / n i=1 w i ? (l) = log 1 -err (l) err (l) + log(K -1),<label>(4)</label></formula><p>where w i denotes the weight of i-th node and c i represents the category of current i-th node. To attain a positive ? (l) , we only need (1 -err (l) ) &gt; 1/K, i.e., the accuracy of each weak classifier should be better than random guess <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>. This can be met easily to guarantee the weights to be updated in the right direction. Then we adjust nodes' weights by increasing weights on incorrectly classified ones:</p><formula xml:id="formula_5">w i ? w i ? exp ? (l) ? I c i = f (l) ? (x i ) , i = 1, . . . , n<label>(5)</label></formula><p>After re-normalizing the weights, we then compute ?l+1 X = ? ? ( ?l X) to sequentially extract knowledge from l+1-th hop of neighbors in the following base classifier f (l+1) ?</p><p>. One crucial point of AdaGCN is that different from traditional AdaBoost, we only define one f ? , e.g. a two-layer fully connected neural network, which in practice is recursively optimized in each base classifier just similar to a recurrent neural network. This also indicates that the parameters from last base classifier are leveraged as the initialization of next base classifier, which coincides with our intuition that l + 1-th hop of neighbors are directly connected from l-th hop of neighbors. The efficacy of this kind of layer-wise training has been similarly verified in <ref type="bibr" target="#b2">(Belilovsky et al., 2018)</ref> recently. Further, we combine the predictions from different orders of neighbors in an Adaboost way to obtain the final prediction C(A, X):</p><formula xml:id="formula_6">C(A, X) = arg max k L l=0 ? (l) f (l) ? ( ?l X)<label>(6)</label></formula><p>Finally, we obtain the concise form of AdaGCN in the following:</p><formula xml:id="formula_7">?l X = ? ? ( ?l-1 X) Z (l) = f (l) ? ( ?l X) Z = AdaBoost(Z (l) ) (7)</formula><p>Note that f ? is non-linear, rather than linear in SGC <ref type="bibr" target="#b34">(Wu et al., 2019)</ref>, to guarantee the representation power. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the architecture of AdaGCN is a variant of RNN with synchronous sequence input and output. We provide a detailed description of the our algorithm in Section 3. Architectural Difference. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref> and 2, there is an apparent difference among the architectures of GCN (Kipf &amp; Welling, 2017), SGC <ref type="bibr" target="#b34">(Wu et al., 2019)</ref>, JK <ref type="bibr">(Xu et al., 2018b)</ref> and AdaGCN. Compared with these existing graph convolutional approaches that sequentially convey intermediate result Z (l) to compute final prediction, our AdaGCN transmits weights of nodes w i , aggregated features of different hops of neighbors ?l X. More importantly, in AdaGCN the embedding Z (l) is independent of the flow of computation in the network and the sparse adjacent matrix ? is also not directly involved in the computation of individual network because we compute ?(l+1) X in advance and then feed it instead of ? into the classifier f (l+1) ? , thus yielding significant computation reduction, which will be discussed further in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">COMPARISON WITH EXISTING METHODS</head><p>Connection with PPNP and APPNP. We also established a strong connection between AdaGCN and previous state-of-the-art PPNP and APPNP <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref> method that leverages personalized pagerank to reconstruct graph convolutions in order to use information from a large and adjustable neighborhood. The analysis can be summarized in the following Proposition 1. Proof can refer to Appendix A.3.</p><p>Proposition 1. Suppose that ? is the teleport factor. Let matrix sequence {Z (l) } be from the output of each layer l in AdaGCN, then PPNP is equivalent to the Exponential Moving Average (EMA) with exponentially decreasing factor ? on {Z (l) } in a sharing parameters version, and its approximate version APPNP can be viewed as the approximated form of EMA with a limited number of terms.</p><p>Proposition 1 illustrates that AdaGCN can be viewed as an adaptive form of APPNP, formulated as:</p><formula xml:id="formula_8">Z = L l=0 ? (l) f (l) ? ( ?l X)<label>(8)</label></formula><p>Specifically, the first discrepancy between AdaGCN and APPNP lies in the adaptive coefficient ? (l) in AdaGCN determined by the error of l-th base classifier f (l)</p><p>? rather than fixed exponentially decreased weights in APPNP. In addition, AdaGCN employs classifier f (l) ? with different parameters to learn the embedding of different orders of neighbors, while APPNP shares these parameters in its form. We verified this benefit of our approach in our experiments shown in Section 4.2. Definition 1. General layer-wise Neighborhood Mixing: A graph convolution network has the ability to represent the layer-wise neighborhood mixing if for any b 0 , b 1 , ..., b L , there exists an injective mapping f with a setting of its parameters, such that the output of this graph convolution network can express the following formula:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with MixHop</head><formula xml:id="formula_9">f L l=0 b l ? ?l X (9)</formula><p>Proposition 2. AdaGCNs defined by our proposed approach (Eq. equation <ref type="formula">7</ref>) are capable of representing general layer-wise neighborhood mixing, i.e., can meet the Definition 1.</p><p>Albeit the similarity, AdaGCN distinguishes from MixHop in many aspects. Firstly, MixHop concatenates all outputs from each order of neighbors while we combines these predictions in an Adaboost way, which has theoretical generalization guarantee based on boosting theory. Meantime, MixHop allows full linear mixing of different orders of neighboring features, while AdaGCN utilizes different non-linear transformation f</p><formula xml:id="formula_10">(l)</formula><p>? among all layers, enjoying stronger expressive power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM</head><p>In practice, we employ SAMME.R <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref>, the soft version of SAMME, in AdaGCN. SAMME.R (R for Real) algorithm <ref type="bibr" target="#b14">(Hastie et al., 2009)</ref> leverages real-valued confidence-rated predictions, i.e., weighted probability estimates, rather than predicted hard labels in SAMME, in the prediction combination, which has demonstrated a better generalization and faster convergence than SAMME. We elaborate the final version of AdaGCN in Algorithm 1. We provide the analysis on the choice of model depth L in Appendix A.7, and then we elaborate the computational advantage of AdaGCN in the following. Obtain the weighted probability estimates p (l) ( X(l) ) for f</p><formula xml:id="formula_11">(l) ? : p (l) k ( X(l) ) = Softmax(f (l) ? (c = k| X(l) )), k = 1, . . . , K 5: Compute the individual prediction h (l) k (x) for the current graph convolutional classifier f (l) ? : h (l) k ( X(l) ) ? (K -1) log p (l) k ( X(l) ) - 1 K k log p (l) k ( X(l) )</formula><p>where k = 1, . . . , K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Adjust the node weights w i for each node x i with label y i on training set:</p><formula xml:id="formula_12">w i ? w i ? exp - K -1 K y i log p (l) (x i ) , i = 1, . . . , n 7:</formula><p>Re-normalize all weights w i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Update l+1-hop neighbor feature matrix X(l+1) : X(l+1) = ? X(l) 9: end for 10: Combine all predictions h (l) k ( X(l) ) for l = 0, ..., L. Analysis of Computational Advantage. Due to the similarity of graph convolution in <ref type="bibr">Mix-Hop (Abu-El-Haija et al., 2019)</ref>, AdaGCN also requires no additional memory or computational complexity compared with previous GCN models. Meanwhile, our approach enjoys huge computational advantage compared with GCN-based models, e.g., PPNP and APPNP, stemming from excluding the additional computation involved in sparse tensors, such as the sparse tensor multiplication between ? and other dense tensors, in the forward and backward propagation of the neural network. Specifically, there are only L times sparse tensor operations for an AdaGCN model with L layers, i.e., ?l X = ? ? ( ?l-1 X) for each layer l. This operation in each layer yields a dense tensor B l = ?l X for the l-th layer, which is then fed into the computation in a two-layer fully-connected network, i.e., f 1) . Due to the fact that dense tensor B l has been computed in advance, there is no other computation related to sparse tensors in the multiple forward and backward propagation procedures while training the neural network. By contrast, this multiple computation involved in sparse tensors in the GCN-based models, e.g., GCN: ? ReLU( ?XW (0) )W (1) , is highly expensive. AdaGCN avoids these additional sparse tensor operations in the neural network and then attains huge computational efficiency. We demonstrate this viewpoint in the Section 4.3. We select five commonly used graphs: CiteSeer, Cora-ML <ref type="bibr">(Bojchevski &amp; G?nnemann, 2018;</ref><ref type="bibr" target="#b28">McCallum et al., 2000)</ref>, PubMed <ref type="bibr" target="#b29">(Sen et al., 2008)</ref>, MS-Academic <ref type="bibr" target="#b30">(Shchur et al., 2018)</ref> and Reddit. Dateset statistics are summarized in Table <ref type="table" target="#tab_0">1</ref>. Recent graph neural networks suffer from overfitting to a single splitting of training, validation and test datasets <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref>. To address this problem, inspired by <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref>, we test all approaches on multiple random splits and initialization to conduct a rigorous study. Detailed dataset splittings are provided in Appendix A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C(A, X) = arg max</head><formula xml:id="formula_13">(l) ? (B l ) = ReLU(B l W (0) )W (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Basic Setting of Baselines and AdaGCN. We compare AdaGCN with <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref> and Simple Graph Convolution (SGC) <ref type="bibr" target="#b34">(Wu et al., 2019)</ref> in Figure <ref type="figure" target="#fig_5">3</ref>. In Table <ref type="table">2</ref>, we employ the same baselines as <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref>: V.GCN (vanilla GCN) (Kipf &amp; Welling, 2017) and GCN with our early stopping, N-GCN (network of GCN) <ref type="bibr">(Abu-El-Haija et al., 2018a)</ref>, GAT (Graph Attention Networks) <ref type="bibr" target="#b33">(Veli?kovi? et al., 2018)</ref>, BT.FP (bootstrapped feature propagation) <ref type="bibr" target="#b5">(Buchnik &amp; Cohen, 2018)</ref> and JK (jumping knowledge networks with concatenation) <ref type="bibr">(Xu et al., 2018b)</ref>. In the computation part, we additionally compare AdaGCN with FastGCN <ref type="bibr" target="#b7">(Chen et al., 2018)</ref> and GraphSAGE <ref type="bibr" target="#b13">(Hamilton et al., 2017)</ref>. We refer to the result of baselines from <ref type="bibr" target="#b20">(Klicpera et al., 2018)</ref> / and the implementation of AdaGCN is adapted from APPNP. For AdaGCN, after the line search on hyper-parameters, we set h = 5000 hidden units for the first four datasets except Ms-academic with h = 3000, and 15, 12, 20 and 5 layers respectively due to the different graph structures. In addition, we set dropout rate to 0 for Citeseer and Cora-ML datasets and 0.2 for the other datasets and 5 ? 10 -3 L 2 regularization on the first linear layer. We set weight decay as 1 ? 10 -3 for Citeseer while 1 ? 10 -4 for others. More detailed model parameters and analysis about our early stopping mechanism can be referred from Appendix A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DESIGN OF DEEP GRAPH MODELS TO CIRCUMVENT OVERSMOOTHING EFFECT</head><p>It is well-known that GCN suffers from oversmoothing <ref type="bibr" target="#b22">(Li et al., 2018)</ref> with the stacking of more graph convolutions. However, combination of knowledge from each layer to design deep graph models is a reasonable method to circumvent oversmoothing issue. In our experiment, we aim to explore the prediction performance of GCN, GCN with residual connection (Kipf &amp; Welling, 2017), SGC and our AdaGCN with a growing number of layers.</p><p>From Figure <ref type="figure" target="#fig_5">3</ref>, it can be easily observed that oversmoothing leads to the rapid decreasing of accuracy for GCN (blue line) as the layer increases. In contrast, the speed of smoothing (green line) of SGC is much slower than GCN due to the lack of ReLU analyzed in Section 2.1. Similarly, GCN with residual connection (yellow line) partially mitigates the oversmoothing effect of original GCN but fails to take advantage of information from different orders of neighbors to improve the prediction performance constantly. Remarkably, AdaGCN (red line) is able to consistently enhance the performance with the increasing of layers across the three datasets. This implies that AdaGCN can efficiently incorporate knowledge from different orders of neighbors and circumvent oversmoothing of original GCN in the process of constructing deep graph models. In addition, the fluctuation of performance for AdaGCN is much lower than GCN especially when the number of layer is large.  Besides, we explore the computational cost of ReLU and sparse adjacency tensor with respect to the number of layers in the right part of Figure <ref type="figure">4</ref>. We focus on comparing AdaGCN with SGC and GCN as other GCN-based methods, such as GraphSAGE and APPNP, behave similarly with GCN. Particularly, we can easily observe that both SGC (blue line) and GCN (red line) show a linear increasing tendency and GCN yields a larger slope arises from ReLU and more parameters. It also shows that the computational cost involved sparse matrices in neural networks plays a dominant role in all the cost especially when the layer is large enough. In contrast, our AdaGCN (pink line) displays an almost constant trend as the layer increases because it avoids the additional computation related to sparse matrices in a network, enjoying huge computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PREDICTION PERFORMANCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSIONS AND CONCLUSION</head><p>One potential concern is that AdaBoost <ref type="bibr" target="#b14">(Hastie et al., 2009;</ref><ref type="bibr" target="#b10">Freund et al., 1999</ref>) is established on i.i.d. hypothesis while graphs have inherent data-dependent property. Fortunately, the statistical convergence and consistency of boosting <ref type="bibr" target="#b26">(Lugosi &amp; Vayatis, 2001;</ref><ref type="bibr" target="#b27">Mannor et al., 2003)</ref> can still be preserved when the samples are weakly dependent <ref type="bibr" target="#b24">(Lozano et al., 2013)</ref>. More discussion can refer to Appendix A.5. In this paper, we propose a novel RNN-like deep graph neural network architecture called AdaGCNs. With the delicate architecture design, our approach AdaGCN can effectively explore and exploit knowledge from different orders of neighbors in an Adaboost way.</p><p>Our work paves a way towards better combining different-order neighbors to design deep graph models rather than only stacking on specific type of graph convolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 :</head><label>1</label><figDesc>Figure 1: The RNN-like architecture of AdaGCN with each base classifier f (l)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of the graph model architectures. f a in JK network denotes one aggregation layer with aggregation function such as concatenation or max pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>MixHop (Abu-El-Haija et al., 2019)  applied the similar way of graph convolution by repeatedly mixing feature representations of neighbors at various distance. Proposition 2 proves that both AdaGCN and MixHop are able to represent feature differences among neighbors while previous GCNs-based methods cannot. Proof can refer to Appendix A.4. Recap the definition of general layer-wise Neighborhood Mixing<ref type="bibr" target="#b1">(Abu-El-Haija et al., 2019)</ref> as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 (</head><label>1</label><figDesc>AdaGCN based on SAMME.R AlgorithmInput: Features Matrix X, normalized adjacent matrix ?, a two-layer fully connected network f ? , number of layers L and number of classes K. Output: Final combined prediction C(A, X).1: Initialize the node weights w i = 1/n, i = 1, 2, ..., n on training set, neighbors feature matrix X(0) = X and classifier f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>k ( X(l) )11: return Final combined prediction C(A, X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Comparison of test accuracy of different models as the layer increases. We regard the l-th base classifier as the l-th layer in AdaGCN as both of them are leveraged to exploit the information from l-th order of neighbors for current nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dateset statistics</figDesc><table><row><cell>Experimental Setup.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dateset</cell><cell>Nodes</cell><cell cols="4">Edges Classes Features Label Rate</cell></row><row><cell>CiteSeer</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>3.6%</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>5.2%</cell></row><row><cell>PubMed</cell><cell>19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell><cell>0.3%</cell></row><row><cell cols="2">MS Academic 18,333</cell><cell>81,894</cell><cell>15</cell><cell>6,805</cell><cell>1.6%</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 11,606,919</cell><cell>41</cell><cell>602</cell><cell>65.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Average accuracy across different label rates with 20 splittings of datasets under 100 runs.with APPNP, showing more efficiency on graphs with few labeled nodes. Inspired by the Layer Effect on graphs<ref type="bibr" target="#b31">(Sun et al., 2019)</ref>, we argue that the increase of layers in AdaGCN can result in more benefits on the efficient propagation of label signals especially on graphs with limited labeled nodes.</figDesc><table><row><cell cols="2">We conduct a rigorous study of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">AdaGCN on four datasets under mul-</cell><cell>Model</cell><cell>Citeseer</cell><cell>Cora-ML</cell><cell>Pubmed</cell><cell>MS Academic</cell></row><row><cell cols="2">tiple splittings of dataset. The results from Table 2 suggest the state-of-</cell><cell>V.GCN GCN N-GCN</cell><cell cols="3">73.51?0.48 82.30?0.34 77.65?0.40 75.40?0.30 83.41?0.39 78.68?0.38 74.25?0.40 82.25?0.30 77.43?0.42</cell><cell>91.65?0.09 92.10?0.08 92.86?0.11</cell></row><row><cell cols="2">the-art performance of our approach and the improvement compared with</cell><cell>GAT JK BT.FP</cell><cell cols="3">75.39?0.27 84.37?0.24 77.76?0.44 73.03?0.47 82.69?0.35 77.88?0.38 73.55?0.57 80.84?0.97 72.94?1.00</cell><cell>91.22?0.07 91.71?0.10 91.61?0.24</cell></row><row><cell cols="2">APPNP validates the benefit of adap-</cell><cell>PPNP</cell><cell cols="2">75.83?0.27 85.29?0.25</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell cols="2">tive form for our AdaGCN. More rig-orously, p values under paired t test</cell><cell cols="4">APPNP PPNP (ours) APPNP (ours) 75.41?0.35 84.28?0.28 79.41?0.34 75.73?0.30 85.09?0.25 79.73?0.31 75.53?0.32 84.39?0.28 OOM</cell><cell>93.27?0.08 OOM 92.98?0.07</cell></row><row><cell cols="2">demonstrate the significance of im-provement for our method.</cell><cell>AdaGCN P value</cell><cell cols="3">76.68?0.20 85.97?0.20 79.95?0.21 1.8?10 -15 2.2?10 -16 1.1?10 -5</cell><cell>93.17?0.07 2.1?10 -9</cell></row><row><cell cols="2">In the realistic setting, graphs usually</cell><cell cols="5">Table 2: Average accuracy under 100 runs with uncertainties</cell></row><row><cell cols="2">have different labeled nodes and thus</cell><cell cols="5">showing the 95 % confidence level calculated by bootstrap-</cell></row><row><cell cols="2">it is necessary to investigate the ro-</cell><cell cols="5">ping. OOM denotes "out of memory". "(ours)" denotes the</cell></row><row><cell cols="2">bust performance of methods on dif-</cell><cell cols="5">results based on our implementation, which are slight lower</cell></row><row><cell cols="2">ferent number of labeled nodes. Here</cell><cell cols="5">than numbers above from original literature (Klicpera et al.,</cell></row><row><cell cols="2">we utilize label rates to measure the</cell><cell cols="5">2018). P values of paired t test between APPNP (ours) and</cell></row><row><cell cols="2">different numbers of labeled nodes</cell><cell cols="4">AdaGCN are provided in the last row.</cell></row><row><cell cols="7">and then sample corresponding labeled nodes per class on graphs respectively. Table 3 presents</cell></row><row><cell cols="7">the consistent state-of-the-art performance of AdaGCN under different label rates. An interesting</cell></row><row><cell cols="7">manifestation from Table 3 is that AdaGCN yields more improvement on fewer label rates compared</cell></row><row><cell></cell><cell>Citeseer</cell><cell>Cora-ML</cell><cell cols="2">Pubmed</cell><cell>MS Academic</cell></row><row><cell>Label Rates</cell><cell>1.0% / 2.0%</cell><cell>2.0% / 4.0%</cell><cell cols="2">0.1% / 0.2%</cell><cell>0.6% / 1.2%</cell></row><row><cell>V.GCN</cell><cell cols="5">67.6?1.4/70.8?1.4 76.4?1.3/81.7?0.8 70.1?1.4/74.6?1.6 89.7?0.4/91.1?0.2</cell></row><row><cell>GCN</cell><cell cols="5">70.3?0.9/72.7?1.1 80.0?0.7/82.8?0.9 71.1?1.1/75.2?1.0 89.8?0.4/91.2?0.3</cell></row><row><cell>PPNP</cell><cell cols="2">72.5?0.9/74.7?0.7 80.1?0.7/83.0?0.6</cell><cell>OOM</cell><cell></cell><cell>OOM</cell></row><row><cell>APPNP</cell><cell cols="5">72.2?1.3/74.2?1.1 80.1?0.7/83.2?0.6 74.0?1.5/77.2?1.2 91.7?0.2/92.6?0.2</cell></row><row><cell>AdaGCN</cell><cell cols="5">74.2?0.3/75.5?0.3 83.7?0.3/85.3?0.2 77.1?0.5/79.3?0.3 92.1?0.1/92.7?0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>From the left part of Figure4, it exhibits that AdaGCN has the fastest speed of per-epoch training time in comparison with other methods except the comparative performance with FastGCN in Pubmed. In addition, there is a somewhat inconsistency in computation of Fast-GCN, with fastest speed in Pubmed but slower than GCN on Cora-ML and MS-Academic datasets. Furthermore, with multiple power iterations involved in sparse tensors, APPNP unfortunately has relatively expensive computation cost. It should be noted that this computational advantage of AdaGCN is more significant when it comes to large datasets, e.g., Reddit. Table4demonstrates AdaGCN has the potential to perform much faster on larger datasets.</figDesc><table><row><cell>: Average F1-scores and per-</cell></row><row><cell>epoch training time of typical methods</cell></row><row><cell>on Reddit dataset under 5 runs.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">N-gcn: Multi-scale graph convolution for semi-supervised node classification</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Mining and Learning with Graphs (MLG)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mixhop: Higher-order graph convolution architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy layerwise learning can scale to imagenet</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Eickenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2019. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2015. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bootstrapped graph diffusions: Exposing the power of nonlinearity</title>
		<author>
			<persName><forename type="first">Eliav</forename><surname>Buchnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edith</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boosting with the l 2 loss: regression and classification</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">462</biblScope>
			<biblScope unit="page" from="324" to="339" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2019. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Community detection in graphs</title>
		<author>
			<persName><forename type="first">Santo</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="issue">3-5</biblScope>
			<biblScope unit="page" from="75" to="174" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal-Japanese Society For Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1612</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-class adaboost</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saharon</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and its Interface</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="360" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep resnet blocks sequentially using boosting theory</title>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Process consistency for adaboost</title>
		<author>
			<persName><forename type="first">Wenxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Power up! robust graph convolutional network against evasion attacks based on graph powering</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sojoudi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10029</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2018. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2019. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<title level="m">Can gcns go as deep as cnns? ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convergence and consistency of regularized boosting with weakly dependent observations</title>
		<author>
			<persName><forename type="first">Aurelie</forename><forename type="middle">C</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="651" to="660" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Break the ceiling: Stronger multiscale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On the bayes-risk consistency of boosting methods</title>
		<author>
			<persName><forename type="first">G?bor</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vayatis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Greedy algorithms for classification-consistency, convergence rates, and adaptivity</title>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="713" to="742" />
			<date type="published" when="2003-10">Oct. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relational Representation Learning Workshop (R2L 2018)</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-stage self-supervised learning for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adagan: Boosting generative models</title>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Ilya O Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl-Johann</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5424" to="5433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2019. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? ICLR</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Boosting with early stopping: Convergence and consistency</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1538" to="1579" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Max-margin nonparametric latent feature models for link prediction</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07428</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
