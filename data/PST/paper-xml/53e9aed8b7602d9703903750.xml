<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On SIFTs and their Scales *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<email>hassner@openu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Open University of Israel</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Viki</forename><surname>Mayzels</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Open University of Israel</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On SIFTs and their Scales *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6C1EADD8780313636BFD1200AFFF41FA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scale invariant feature detectors often find stable scales in only a few image pixels. Consequently, methods for feature matching typically choose one of two extreme options: matching a sparse set of scale invariant features, or dense matching using arbitrary scales. In this paper we turn our attention to the overwhelming majority of pixels, those where stable scales are not found by standard techniques. We ask, is scale-selection necessary for these pixels, when dense, scale-invariant matching is required and if so, how can it be achieved? We make the following contributions: (i) We show that features computed over different scales, even in low-contrast areas, can be different; selecting a single scale, arbitrarily or otherwise, may lead to poor matches when the images have different scales. (ii) We show that representing each pixel as a set of SIFTs, extracted at multiple scales, allows for far better matches than singlescale descriptors, but at a computational price. Finally, (iii) we demonstrate that each such set may be accurately represented by a low-dimensional, linear subspace. A subspaceto-point mapping may further be used to produce a novel descriptor representation, the Scale-Less SIFT (SLS), as an alternative to single-scale descriptors. These claims are verified by quantitative and qualitative tests, demonstrating significant improvements over existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the past decade and a half, scale invariant feature detectors, such as the Harris-Laplace <ref type="bibr" target="#b19">[20]</ref> and robust descriptors such as the SIFT <ref type="bibr" target="#b17">[18]</ref>, have played pivotal roles in maturing Computer Vision systems. The key idea is that at each interest point, one (or few) scales are selected based on a scale-invariant function (e.g., the Laplacian of Gaussians). Presumably, local extrema of this function occur at the same scales for the same feature in different images allowing the features to be matched across images in different scales <ref type="bibr" target="#b21">[22]</ref>. A typical image, however, often has relatively few pixels for which such scales may be reliably selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left image Right image</head><p>Warp with DSIFT Warp with our SLS Consequently, matching of scale invariant features has so far been applied mostly to few pixels in each image.</p><p>When dense correspondences are required, traditional methods restrict themselves to using pixels or pixel patches, filtered or otherwise (see, e.g., <ref type="bibr" target="#b10">[11]</ref>). Alternatively, feature descriptors may be computed for all the pixels in the image (e.g., <ref type="bibr" target="#b25">[26]</ref>). These are designed to be robust to a range of geometric and photometric image transformation. One such example is the Dense-SIFT (DSIFT) descriptor <ref type="bibr" target="#b27">[28]</ref> which is extracted at a single scale for all the pixels in the image. Establishing correspondences between two images is then performed either locally or by using global optimization schemes such as the SIFT-Flow algorithm <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Such methods, however, all implicitly assume that features in the two images share the same, or sufficiently similar, scales.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, when this does not hold, correspondence estimation fails.</p><p>In this paper we focus on those pixels for which a method 1. We show that even in low contrast areas of the image, where scale-selection is difficult, descriptors may change their values from one scale to the next. Consequently, selecting an arbitrary single scale may lead to false matches when two images have different scales.</p><p>2. We propose representing each pixel by a set of SIFT descriptors extracted at multiple scales and matched from one image to the next using set-to-set similarities. The computational cost of matching more descriptors is balanced by a substantial boost in accuracy.</p><p>3. We demonstrate that each such set of SIFTs resides on a low-dimensional subspace. We further show that the subspace-to-point mapping of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, provides a means of representing these subspaces as a novel feature descriptor, the Scale-Less-SIFT (SLS).</p><p>These set-based, multi-scale SIFT representations are tested on dense correspondence estimation problems with images separated by wide scale differences and changing viewing conditions and shown to significantly outperform existing methods both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous work</head><p>Objects and scenes appear in images in different scales. In order to correctly describe features when these scales are unknown, one must consider multiple scales for each feature point. Since the early 90s automatic scale selection techniques have been proposed which seek for each feature point a stable, characteristic scale. They thus augment earlier scale-space methods by choosing one scale for each feature for the purpose of both reducing the computational burden of higher level visual systems, as well as improving their performance by focusing on more relevant information (See <ref type="bibr" target="#b13">[14]</ref> for more on these early approaches).</p><p>Lindeberg <ref type="bibr" target="#b14">[15]</ref> suggested seeking for each feature its "interesting scales"; that is, scales which reflect a characteristic size of a feature. He proposed selecting these scales by choosing the extrema in the Laplacian of Gaussian (LoG) function computed over the image scales. Pixels of local extrema may additionally be rejected if their LoG value is lower than a predefined threshold. This is applied in order to ensure that unstable, low-contrast points are not selected. An efficient approximation to the LoG function is based on differences of Gaussian (DoG) filters (e.g., <ref type="bibr" target="#b17">[18]</ref>). For a given image, three sets of sub-octave, DoG filters are produced. The resulting 3D structure (x, y and scale) is then scanned, searching for pixels with higher or lower values than their 26 space-scale neighbors. Coordinate localization is then performed in order to obtain more accurate pixel locations as well as, again, reject unstable detections located in low contrast areas or near edges.</p><p>Scale selection is sometimes performed along-side spatial localization. The Harris-Laplace detector <ref type="bibr" target="#b19">[20]</ref>, for example, uses a scale-adapted Harris corner detector to localize points spatially and LoG filter extrema to localize points in scale. These two steps are performed in an iterative procedure which searches for the joint peaks of these two values. Here too, points are rejected if they fail to produce responses stronger than a given threshold.</p><p>These methods, as well as similar techniques, all typically produce a small set of interest points located near corner structures in the image. Mikolajczyk <ref type="bibr" target="#b18">[19]</ref> reports that under a scale change factor of 4.4 the percent of pixels for which a scale is detected is as little as 38% for the DoG detector of which in only 10.6%, the detected scale was correct.</p><p>Several existing methods use few invariant features to seed a search for dense matches between different views of a widebaseline stereo system <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>. As far as we know, however, none of these methods is designed to provide dense correspondences across scale differences. A noteworthy exception is the work of <ref type="bibr" target="#b23">[24]</ref> which uses few scale-invariant features to locate an object in an image and then produces dense matches along with accurate segmentations. Their method, however, relies on a global alignment scheme to overcome the main scale differences before dense matching.</p><p>It is thus unclear how it performs when no such alignment is possible (e.g., several independent scene motions).</p><p>In <ref type="bibr" target="#b11">[12]</ref> scale invariant descriptors (SID) are proposed without requiring the estimation of image scale. A main advantage of SID is that they are applicable to a broader range of image structures, such as edges, for which scale selection is unreliable. Our experiments here show that SID are less capable of matching across different scenes than the SIFT descriptors underlying our representation. In <ref type="bibr" target="#b26">[27]</ref>, scale selection is avoided by computing multi-scale fractal features, developed for the purpose of texture classification.</p><p>Dense SIFT -no scale selection. When dense matching is required, a common approach is to forgo scale estimation, producing instead descriptors on a regular grid using constant, typically arbitrarily selected, scales. One such example is the efficient DAISY descriptors of <ref type="bibr" target="#b25">[26]</ref> or, more related to this work, Dense-SIFT (DSIFT) descriptors <ref type="bibr" target="#b27">[28]</ref>.</p><p>In object recognition tasks, regular sampling strategies for descriptor generation have been shown to outperform invariant features generated at stable coordinates and  scales <ref type="bibr" target="#b22">[23]</ref>. This is may be due to the benefits of having many descriptors over accurate scales for just a few.</p><p>Existing work on dense matching between two images has thus far largely ignored the issue of scale invariance. The SIFT-Flow system of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, for example, produces DSIFT descriptors at each pixel location. These descriptors are then matched between two images, taking advantage of the robustness of the SIFT representation, without attempting to provide additional scale invariance. Matching is performed using a modified optical flow formulation <ref type="bibr" target="#b7">[8]</ref>. Although the DSIFT descriptors used by the SIFT-Flow algorithm provide some scale invariance, this quickly degrades as the scale differences between the two images increase (Fig. <ref type="figure" target="#fig_2">2</ref>). An additional related method is the Generalized Patch-Match <ref type="bibr" target="#b1">[2]</ref>, designed for matching descriptors extracted at each pixel, here, with an emphasis on speed.</p><p>The methods described above provide means for matching descriptors produced on dense regular grids. In the absence of per-pixel scale-invariant descriptors, they are not designed to handle large scale differences. In this paper we extend these approaches by discussing the utility of multiple SIFT descriptors at each pixel, and their representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The behavior of SIFT across scales</head><p>We begin by considering how the values of multiple SIFT descriptors vary through scales. The scale space L(x, y, σ) of an image I(x, y) is defined by the convolution of I(x, y) with the variable-scale Gaussian G(x, y, σ) <ref type="bibr" target="#b12">[13]</ref>, where:</p><formula xml:id="formula_0">L(x, y, σ) = G(x, y, σ) I(x, y) G(x, y, σ) = 1 2πσ 2 e -(x 2 +y 2 )/2σ 2</formula><p>Typically (Section 2), a feature detector selects coordinates in space x, y and scale σ, from which a single SIFT de-scriptor h σ = h(x, y, σ) is then extracted <ref type="bibr" target="#b17">[18]</ref>. Although sometimes more than one scale is selected, they are usually treated independently of each other.</p><p>Here, we consider instead all the descriptors h σi = h(x, y, σ i ), where σ i is taken from a discrete set of scales {σ 1 , ..., σ k }. Our chief assumption is that corresponding pixels should exhibit a similar behavior throughout scales. In other words, the same pattern of SIFT descriptors h(x, y, σ i ) should be apparent when examining corresponding pixels. The challenge then becomes how to effectively capture this pattern of change across scales?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SIFT sets</head><p>Rather than selecting a single scale for each pixel we compute multiple descriptors at multiple scales and represent pixels as sets of SIFT descriptors. Formally, denote by p and p a pair of corresponding pixels in images I and I , respectively. For a set of scales σ 1 , . . . , σ k , the two pixels are represented by the sets H = [h σ1 , . . . , h σ k ] and H = h σ1 , . . . , h σ k .</p><p>To match the pixels, a set-to-set similarity measure is needed.</p><p>There are several such measures available, e.g., <ref type="bibr" target="#b28">[29]</ref>. As we show in Sec. 4, however, highly accurate matching results are obtained by considering the straightforward "min-dist" measure <ref type="bibr" target="#b28">[29]</ref>, defined as follows.</p><p>mindist(p, p ) = min i,j dist(h σi , h σj ).</p><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>Comparing two pixels represented as n SIFT descriptors, would require O(128 × n 2 ) operations, which may be prohibitive if the sets are large. Often, however, only a few scales are required to provide accurate representations (Sec. 4). This is explained by the following assumption.</p><p>Assumption 1 -Corresponding points are similar at multiple scales. Our underlying assumption is that there exist a set of scales σ 1 , . . . , σ k for image I and a set of scales σ 1 , . . . , σ k for image I , such that the descriptors produced at the two pixels are equal (or else sufficiently similar):</p><formula xml:id="formula_2">h σi = h σ i . Let H = [h σ1 , . . . , h σ k ] and H = h σ 1 , . . . , h σ k , then we can write H = H .</formula><p>This equality, however, holds only when all the scales σ 1 , . . . , σ k and σ 1 , . . . , σ k correspond exactly. In practice, we do not have these correspondences and instead sample the scales at fixed intervals for all images. Thus, the set of scales in one image may be interleaved with the other.</p><p>Because SIFT values change gradually with scale, only few scales need to be sampled to provide similar descriptors even in such cases. This is illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SIFT subspaces</head><p>An alternative, Geometric representation for sets of SIFT descriptors, is obtained by considering the linear subspace on which these SIFTs reside. Subspaces have often been used to represent varying information. Some such examples are listed in <ref type="bibr" target="#b3">[4]</ref>. Here, we show that low-dimensional linear subspaces are highly capable of capturing the scale-varying values of SIFT descriptors.</p><p>Assumption 2 -Descriptors computed at multiple scales of the same point span a linear subspace. The SIFT de-scriptor consists of gradient histograms. In many cases the local statistics of these gradients are equivalent at different scales. For example, in homogeneous, low-contrast regions or areas of stationary textures, the size of the local neighborhood does not change the distribution of gradients. In these cases we get h σi = h σj for σ i = σ j .</p><p>In other cases, the statistics do change with the scale, however, if we sample the scales densely enough these changes are gradual and monotonic (Fig. <ref type="figure" target="#fig_3">3</ref>). In such cases we get h σi = j w ij h σj , where w ij = 0 when h σi does not depend on h σj and w ij = scalar otherwise. In other words, each descriptor can be represented as a linear combination of several other descriptors at different scales. This occurs when the regions surrounding the patch are piecewise stationary. Enlarging the window size by small steps maintains similar statistics within each window.</p><p>The observations above suggest that the set of descriptors h σ1 , . . . , h σ k approximately lie on a linear subspace:</p><formula xml:id="formula_3">H = [h σ1 , . . . , h σ k ] = ĥ1 , . . . , ĥb W = ĤW (2)</formula><p>where ĥ1 , . . . , ĥb are basis vectors spanning the space of descriptors and W is a matrix of coefficients.</p><p>Combining the two assumptions. According to assumption 1, for two corresponding pixels, if we knew the set of corresponding scales we would have H = H . This implies that the two sets of descriptors share the same spanning basis, i.e., Ĥ = Ĥ . While we do not know the scales required to construct H and H , according to assumption 2 this is not crucial. As long as we sample the scale densely enough we can compute the bases Ĥ and Ĥ .</p><p>The distance between a pair of pixels, p and p , can be measured by the distance between the corresponding subspaces H p and H p , represented as matrices Ĥ and Ĥ with orthonormal columns. There are several possible definitions to the distance dist 2 (H p , H p ) between two linear subspaces <ref type="bibr" target="#b8">[9]</ref>. Here we use the Projection Frobenius Norm (Projection F-Norm), defined as:</p><formula xml:id="formula_4">dist 2 (H p , H p ) = || sin θ|| 2 2 (3)</formula><p>Where sin θ is the vector of sines of the principal angles between the two subspaces H p and H p . This may be computed by considering the cosines of the principal angles obtained from SV D( ĤT Ĥ ) in O(128 × d 2 ) operations, where d is the subspace dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Scale-Less SIFT (SLS) representation</head><p>It is often beneficial to have a point representation for each pixel, rather than a subspace. Such is the case when, for example, efficient indexing is required. We therefore employ the subspace-to-point mapping proposed by Basri et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> to produce the Scale-Less SIFT (SLS) descriptor for each such subspace.</p><p>Specifically, consider the subspace H p produced at pixel p, represented as a 128 × d matrix Ĥ with orthonormal columns. We produce the SLS representation by mapping this subspace to a point P by rearranging the elements of the projection matrix A = Ĥ ĤT using the following operator:</p><formula xml:id="formula_5">P SLS( Ĥp ) = a 11 √ 2 , a 12 , ..., a 1d , a 22 √ 2 , a 23 , ..., a dd √ 2 T (4)</formula><p>Where a ij is the element (i, j) in matrix A. A key property of this mapping is that the distance between two such mapped subspaces, P and P is monotonic with respect to the Projection F-Norm between the original subspaces H p and H p <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. That is, for a constant µ:</p><formula xml:id="formula_6">||P -P || 2 = µ dist 2 (H p , H p )<label>(5)</label></formula><p>Point P thus captures the behavior of SIFTs throughout scale space, at a quadratic cost in the dimension of the descriptors. Here, we employ the SLS descriptor, P , as a surrogate for the subspace H p without making further adjustments to the method used to compute correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our code is written in MATLAB, using the SIFT code of <ref type="bibr" target="#b27">[28]</ref> and the SID code of <ref type="bibr" target="#b11">[12]</ref>. Flow was estimated using the SIFT-Flow code <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, with either its original DSIFT, or alternatively using SID, and our own SLS descriptor. Our SLS results were produced using 8D, linear subspaces obtained by standard PCA on 20 scales at each pixel, linearly distributed in the range [0. <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Quantitative results on Middlebury data <ref type="bibr" target="#b0">[1]</ref>. We compare our SLS with both SID and DSIFT, on the Middlebury optical flow set. Since this data does not include scale changes, we rescale the left and right images by factors of 0.7 and 0.2, respectively. The quality of an estimated match was measured using both angular and endpoint errors (± SD) <ref type="bibr" target="#b0">[1]</ref>. Table <ref type="table">1</ref> shows that both multi-scale approaches outperform the single-scale DSIFT significantly. Furthermore, our SLS descriptors lead to lower errors when compared to the descriptors of <ref type="bibr" target="#b11">[12]</ref>.</p><p>Qualitative results. We present a visual comparison of the quality of the estimated flows, using each of the three alternatives: DSIFT, SID and our SLS descriptor. Our results present a Left-image (source) warped onto the Rightimage (target) according to the estimated flows. SLS results in Fig. <ref type="figure">4</ref> and 5 are further cropped to show areas of high confidence matches (see below).</p><p>We tested image pairs with scene motion (Fig. <ref type="figure">4</ref>) and images of different scenes with similar appearances (Fig. <ref type="figure">5</ref>). All these images include scale differences, often extreme. We know of no previous method which successfully presents dense matches on such challenging pairs. Our results show that the SLS enables accurate dense correspondences even under extreme changes in scale.</p><p>In Fig. <ref type="figure">4</ref> DSIFT typically manages to lock onto a single scale quite well, while missing other scale changes in the scene. The SLS descriptor better captures the scale-varying behavior at each pixel and so manages to better match pixels at different scales with only local misalignments. Cropping the result to its ROI. When matching views of significantly different scales, warping one image to the other introduces the problem of cropping the image to its region of interest (ROI). In <ref type="bibr" target="#b23">[24]</ref> this problem is avoided by assuming that the high resolution image is neatly cropped. Without this knowledge, the warped high resolution image would include noisy, "smeared" areas where it does not overlap the low resolution image (see Fig. <ref type="figure" target="#fig_8">6</ref>).</p><p>Here we automatically select the region of high confidence matches, as follows. Given images I and I , we compute the two dense flows, from I to I and then back, from I to I. In both cases we count for each pixel in the target image, the number of source image pixels which were mapped onto it. We threshold the pixels by these numbers and then apply morphological operators to remove small clusters of target pixels. Finally, the ROI of image I is selected as the bounding box of the remaining target pixels obtained by warping image I , and vice versa. This is demonstrated in Fig. <ref type="figure" target="#fig_8">6</ref>. No optimization was performed on this process and it is applied without modification to our images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Scale selection is largely motivated by a need to reduce computational cost as well as the assumption that few scales Table <ref type="table">1</ref>. Quantitative results on rescaled Middlebury data <ref type="bibr" target="#b0">[1]</ref>. Both angular and endpoint errors (± SD) show that multiple scales (SID <ref type="bibr" target="#b11">[12]</ref> &amp; SLS) are always advantageous over a single scale (DISFT <ref type="bibr" target="#b27">[28]</ref>) with SLS outperforming SID on most data-sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left</head><p>Right DSIFT <ref type="bibr" target="#b27">[28]</ref> SID <ref type="bibr" target="#b11">[12]</ref> SLS can be reliably matched <ref type="bibr" target="#b13">[14]</ref>. In this paper we show that images contain valuable information in multiple scales. Thus, scale selection may be detrimental to the quality of the results when dense correspondences are required. The alternative, extracting SIFT descriptors at multiple scales, significantly improves results but at a computational price. We examine how such multiple scales may be compared, representing them as sets or low-dimensional, linear subspaces. In both cases multiple SIFTs outperform single descriptors in pixel matching tests by wide margins. Finally, we present a point representation for these subspaces, the SLS descriptor, which we use as a stand-in for DSIFT in the SIFT-Flow method, improving correspondences on a wide range of challenging viewing conditions.</p><p>We focus here on the SIFT descriptor because of its popularity and its convenient property of changing gradually through scales. It remains to be seen how well the same approach carries over to other successful descriptors, in-  cluding the likes of DAISY <ref type="bibr" target="#b25">[26]</ref>, SURF <ref type="bibr" target="#b5">[6]</ref>, GLOH <ref type="bibr" target="#b20">[21]</ref>, and others. Extensions to full affine invariance also require study. Lastly, examining the impact of these representations on other Computer Vision problems, chiefly, Object Recognition, must be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Dense matches of different objects in different scales. Top: Left and Right input images. Bottom: Left image warped onto Right using the recovered flows: Using DSIFT (bottom left) and our SLS descriptor (bottom right), overlaid on the Right and manually cropped to demonstrate the alignment. DSIFT fails to capture the scale differences and produces an output in the same scale as the input. SLS captures scale changes at each pixel: the output produced by using SLS has the appearance of the Left image in the scale and position of the Right.</figDesc><graphic coords="1,429.78,278.95,94.49,70.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Effects of scale differences on DSIFT vs. our own SLS descriptor. Left images warped onto right image using correspondences obtained by the SIFT-Flow algorithm<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and the DSIFT descriptor, compared against the SLS descriptor (Sec. 3.3). The results in the bottom two rows should appear similar to the top-right image. DSIFT descriptors provide some scale invariance despite a single arbitrary scale selection (left column, middle row). The SLS descriptors provide scale invariance across far greater scale differences (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. SIFT behavior through scales. Top: Two images separated by a ×2 scale factor. SIFT descriptors are extracted at a low contrast area where no interest point was detected, at scales ranging from 10 to 35. Bottom: SIFT descriptor histograms. These demonstrate that (a) SIFTs from the Left image match those at higher scales in the Right, implying that setting the same scale to all pixels in both images may lead to poor matches. (b) Even in low contrast areas, SIFT values are not uniform. Finally, (c) the values of the SIFT descriptors gradually change through scales.</figDesc><graphic coords="4,58.01,337.07,224.87,144.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 1</head><label>1</label><figDesc>Fig.1 and 5present matches estimated between images of different scenes. A good result would have the appearance of the Left (source) images, in the scales and poses of the Right (target) images. As can be seen, the DSIFT and SID descriptors either leave the source in its original scale, unchanged, or else completely fail to produce coherent matches. Although some artifacts are visible in the SLS results (right column) the results present coherent scenes in the target image scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>.8 0.16±0.3 0.17±0.5 108.9±42.1 0.70±0.3 0.80±0.4 Grove2 7.06±5.7 0.66±4.4 0.15±0.3 59.07±40.9 1.50±5.0 0.77±0.4 Grove3 5.23±4.2 1.62±6.9 0.15±0.4 108.95±76.5 4.48±10.5 0.87±0.4 Hydrangea 4.24±4.5 0.32±0.6 0.22±0.8 33.80±32.2 1.59±2.8 0.91±1.1 RubberWhale 24.63±26.9 0.16±0.3 0.15±0.3 116.83±57.7 0.73±1.1 0.80±0.4 Urban2 6.24±7.6 0.37±2.7 0.32±1.3 54.8±54.0 1.33±3.8 1.51±5.4 Urban3 10.26±14.0 0.27±0.6 0.35±0.9 91.81±66.1 1.55±3.7 9.41±24.6 Venus 4.30±4.9 0.24±0.6 0.23±0.5 31.52±34.0 1.16±3.8 0.74±0.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Dense flow with scene motion. Image pairs presenting different scale changes in different parts of the scene, due to camera and scene motion. Correspondences from Left to Right images shown cropped to the area of high confidence matches.LeftRight DSIFT<ref type="bibr" target="#b27">[28]</ref> SID<ref type="bibr" target="#b11">[12]</ref> SLS</figDesc><graphic coords="6,156.31,421.96,93.19,62.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Automatic crop to the region of interest. Dense matches directly formed, without estimating Epipolar Geometry, between the first and last images of the Corridor sequence [10] (left column). Right: Notice the areas where no information is available in the Right image to correspond with parts of the Left image. These areas are automatically cropped to include only the area onto which pixels from the second image were warped.</figDesc><graphic coords="7,67.61,152.74,65.20,65.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>selecting well defined scales is not known. Making up most of the image, these are the pixels for which local image intensities do not vary sufficiently to provide strong extrema in the scale selection function. This work presents the following contributions:</figDesc><table /><note><p><p>978-</p>1-4673-1228-8/12/$31.00 ©2012 IEEE for</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Lihi Zelnik-Manor was supported in part by the Ollendorf foundation, the Israel Ministry of Science, and by the Israel Science Foundation under Grant 1179/11.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The generalized PatchMatch correspondence algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010-09">Sept. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Approximate nearest subspace search with applications to pattern recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A general framework for approximate nearest subspace search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximate nearest subspace search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="278" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speeded-up robust features (surf)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large displacement optical flow</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lucas/kanade meets horn/schunck: Combining local and global optic flow methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The geometry of algorithms with orthogonality constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="303" to="353" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of stereo matching costs on images with radiometric differences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1582" to="1599" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scale invariance without scale selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scale-space theory: A basic tool for analysing structures at different scales</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of App. stat</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="270" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature detection with automatic scale selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Principles for automatic scale selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook on Computer Vision and Applications</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="239" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sift flow: dense correspondence across different scenes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="28" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Detection of local features invariant to affine transformations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institut National Polytechnique de Grenoble</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scale &amp; affine invariant interest point detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Is sift scale invariant? Inverse Problems and Imaging (IPI)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="115" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sampling strategies for bag-of-features image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="490" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A probabilistic model for object recognition, segmentation, and non-rigid correspondence</title>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dense matching of multiple wide-baseline views</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Daisy: An efficient dense descriptor applied to wide-baseline stereo</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="815" to="830" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Locally invariant fractal features for statistical texture classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vlfeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. int. conf. on Multimedia</title>
		<meeting>int. conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1469" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">3D modeling and rendering from multiple wide-baseline images by match propagation. Signal processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="506" to="518" />
		</imprint>
	</monogr>
	<note>Image communication</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
