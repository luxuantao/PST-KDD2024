<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computer Science Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tossapon</forename><surname>Boongoen</surname></persName>
							<email>tossapon.boo@mfu.ac.th</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IQ-D Research Unit</orgName>
								<orgName type="department" key="dep2">School of Information Technology</orgName>
								<orgName type="institution">Mae Fah Luang University</orgName>
								<address>
									<addrLine>Muang District, Chiang Rai 57100</addrLine>
									<settlement>Tasud</settlement>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Natthakan</forename><surname>Iam-On</surname></persName>
							<email>natthakan@mfu.ac.th</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IQ-D Research Unit</orgName>
								<orgName type="department" key="dep2">School of Information Technology</orgName>
								<orgName type="institution">Mae Fah Luang University</orgName>
								<address>
									<addrLine>Muang District, Chiang Rai 57100</addrLine>
									<settlement>Tasud</settlement>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computer Science Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B42D5F92C02770E2CACA0F0C01B6DAE</idno>
					<idno type="DOI">10.1016/j.cosrev.2018.01.003</idno>
					<note type="submission">Received 10 April 2017 Received in revised form 26 December 2017 Accepted 29 January 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Data clustering Cluster ensemble Theoretical extension Domain specific application</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cluster ensembles have been shown to be better than any standard clustering algorithm at improving accuracy and robustness across different data collections. This meta-learning formalism also helps users to overcome the dilemma of selecting an appropriate technique and the corresponding parameters, given a set of data to be investigated. Almost two decades after the first publication of a kind, the method has proven effective for many problem domains, especially microarray data analysis and its downstreaming applications. Recently, it has been greatly extended both in terms of theoretical modelling and deployment to problem solving. The survey attempts to match this emerging attention with the provision of fundamental basis and theoretical details of state-of-the-art methods found in the present literature. It yields the ranges of ensemble generation strategies, summarization and representation of ensemble members, as well as the topic of consensus clustering. This review also includes different applications and extensions of cluster ensemble, with several research issues and challenges being highlighted.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Cluster analysis is usually employed in the initial stage of understanding a raw data, especially for new problems where prior https://doi.org/10.1016/j.cosrev.2018.01.003 1574-0137/© 2018 Elsevier Inc. All rights reserved. knowledge is minimal. Also, in the pre-processing stage of supervised learning, it is exploited to identify outliers and possible object classes for the following expert-directed labelling process. This is crucial when the complexity of modern-age information is generally overwhelming for a human investigation. The need to acquire knowledge or learn from the excessive amount of data is hence a major driving force for making clustering a highly active research subject. Data clustering is applied to a variety of problem domains such as biology <ref type="bibr" target="#b0">[1]</ref>, customer relationship management <ref type="bibr" target="#b1">[2]</ref>, information retrieval <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, image processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, marketing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, psychology <ref type="bibr" target="#b8">[9]</ref> and recommender systems <ref type="bibr" target="#b9">[10]</ref>. In addition, the recent development of clustering cancer gene expression data has attracted a lot of interests amongst computer scientists, biological and clinical researchers <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>Principally, the core of cluster analysis is the clustering process which divides data objects into groups or clusters such that objects in the same cluster are more similar to each other than to those belonging to different clusters <ref type="bibr" target="#b13">[14]</ref>. Objects under examination are normally described in terms of object-specific (e.g., attribute values) or relative measurements (e.g., pairwise dissimilarity). Unlike supervised learning to which classification is categorized, clustering is 'unsupervised' and does not require class information, which is typically achieved through a manual tagging of category labels on data objects, by a domain expert (or through the consensus of multiple experts). Given its potential, a large number of research studies focus on several aspects of cluster analysis: for instance, clustering algorithms and extensions for particular data type <ref type="bibr" target="#b14">[15]</ref>, dissimilarity (or distance) metric <ref type="bibr" target="#b15">[16]</ref>, optimal cluster numbers <ref type="bibr" target="#b16">[17]</ref>, relevance of data attributes per cluster or subspace clustering <ref type="bibr" target="#b17">[18]</ref>, evaluation of clustering results <ref type="bibr" target="#b18">[19]</ref>, and cluster ensembles <ref type="bibr" target="#b19">[20]</ref>.</p><p>Specific to this survey, the practice of cluster ensembles is motivated by the fact that the performance of most clustering techniques are highly data dependent. A particular clustering model may produce an acceptable result for one dataset, but possibly become ineffective for others. Generally, there are two major challenges inherent to clustering algorithms. First, different techniques discover different structures (e.g., cluster size and shape) from the same set of data objects <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. For example, k-means that is probably the best known technique is suitable for spherical-shape clusters, while single-linkage hierarchical clustering is effective to detect connected patterns. This is due to the fact that each individual algorithm is designed to optimize a specific criterion. Second, a single clustering algorithm with different parameter settings can also reveal various structures on the same dataset. A specific setting may be good for a few, but not all datasets. Users encounter these challenges, which consequently make the selection of a proper clustering technique very difficult.</p><p>A solution to this dilemma remains an ultimate goal. In order to accomplish this, researchers invented the methodology of combining different clusterings into a single consensus clustering. This process which is widely known as 'cluster ensembles' can provide more robust and stable solutions across different domains and datasets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. However, modelling a mechanism (usually referred to as a 'consensus function') that is effective for integrating multiple data partitions in a cluster ensemble is far from trivial. This task is difficult since there is no well defined correspondence between the different clustering results. The further challenges arising from the need to combine data partitions and generate a better clustering result without prior knowledge are of high interest amongst researchers.</p><p>The rest of this survey is organized as follows. To set the scene for concepts and discussion presented here, Section 2 introduces the basis of cluster ensembles, including formal definition, framework and different ensemble generation strategies. Then, four major approaches to find a consensus clustering are illustrated in Section 3. In addition, Section 4 provides applications and recent theoretical extensions of those cluster ensemble techniques, especially the use of ensemble information as a data transformation approach for classification task. The survey is concluded in Section 5 with future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The problem of cluster ensembles</head><p>This paper first presents the fundamental concepts of data clustering including a number of benchmark algorithms that have been employed for various problem domains. Each of these conventional techniques are designed on a particular assumption(s), which is normally realized via input parameters. Generally, there is no clustering algorithm, or the algorithm with distinct parameter settings, that performs well for every set of data. To overcome the difficulty with identifying a proper alternative, the methodology of cluster ensemble which is the focus of this review has been continuously developed in the past decade. The second part of this section includes details of general framework and an overview of cluster ensemble methods found in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data clustering and conventional techniques</head><p>Data clustering is one of the fundamental and effective tools for understanding the structure of a given dataset. It plays a crucial, foundational role in machine learning, data mining, information retrieval and pattern recognition. Clustering aims to categorize data into groups or clusters such that the data in the same cluster are more similar to each other than to those in different clusters. Similarity or proximity is measured using the attribute values that represent objects (data points) in the dataset <ref type="bibr" target="#b13">[14]</ref>. Clustering is branded an unsupervised learning approach as the measurement of similarity is conducted without knowledge of class assignment. This knowledge-free scenario brings about a series of difficult decisions, hence the corresponding research studies, with respect to selecting appropriate algorithm, similarity measure, criterion function, and initial parameter condition <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref>. Clustering is widely recognized as an ideal candidate for research and development <ref type="bibr" target="#b24">[25]</ref>, given its benefits and possible advances to be made in this field. There are a large number of clustering algorithms developed in the literature. Examples of well-known techniques are explained in this section.</p><p>k-means is perhaps, the best known clustering technique that partitions data points into clusters. Its name comes from representing each of k clusters by the mean of its members or so-called 'centroid'. k-means is an iterative algorithm that exploits a squareerror as a criterion function (i.e., the total distance between each data point and its cluster centre, <ref type="bibr" target="#b25">[26]</ref>). It begins with initializing centroids randomly and then allocates data points to clusters such that the square-error is minimized. This criterion function tends to work well with separated and compact clusters. Given a dataset X , the square-error e 2 of a clustering π = {C i , . . . , C k } with k clusters is defined as</p><formula xml:id="formula_0">e 2 (X , π) = k ∑ p=1 ∑ ∀x∈Cp ∥x -c p ∥ 2 ,<label>(1)</label></formula><p>where ∥.∥ denotes the Euclidean norm and c p is the centre of the pth cluster. A general description of the k-means algorithm is given as follows:</p><p>1. k data points are first randomly selected as initial cluster centres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Repeat:</head><p>(a) Assign each data point to its closest cluster centre. The Euclidean metric is commonly used to compute the distance between data points and centroids.</p><p>(b) The centroid of each cluster is updated as the mean of all current data points in that cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Until the termination criteria are met.</head><p>The examples of termination criteria are: (i) no changes are made to the cluster centres (i.e., no reassignment of any data point from one cluster to another), (ii) the maximum number of iterations is exceeded, and (iii) there is no improvements in the objective function such as decrease in the square-error. The kmeans algorithm is popular largely due to its efficiency, with time complexity of O(Nkr), where N is the number of data points, k is the number of clusters and r is the number of iterations. However, it is sensitive to the choices of initial cluster centres (i.e., different initial states can lead to different output partitions). One might have to run the algorithm multiple times with various initial partitions and chooses the resulting clustering that offers the minimum squareerror. Yet, k-means does not work well on noisy data and nonconvex cluster shapes.</p><p>k-modes is introduced by Huang <ref type="bibr" target="#b26">[27]</ref> as an extension of the conventional k-means technique for clustering categorical data. It iteratively refines k cluster representatives, each as a vector of attribute values that has the minimal distance to all the data points in a cluster (i.e., the cluster's most frequent attribute values). kmodes uses a simple similarity measure that is determined by the number of common categorical attributes shared by two data points.</p><p>Formally, let X be a set of N data points {x 1 , . . . , x N } described by D categorical attributes, i.e., x i = (x i1 , . . . , x iD ), i = 1 . . . N. The distance d(x i , c p ) between data point x i and centroid of the pth cluster is defined by</p><formula xml:id="formula_1">d(x i , c p ) = D ∑ j=1 δ(x ij , c pj ),<label>(2)</label></formula><p>where δ(y, z) =</p><formula xml:id="formula_2">{ 0 if y = z 1 otherwise (3)</formula><p>Similar to the k-means algorithm, k-modes is also affected by the initialization step and requires the number of clusters k to be specified in advance. However, it is still efficient with the computational complexity of O(Nkr), where N is the number of data points, k is the number of required clusters and r is the number of iterations.</p><p>k-prototypes: extends both k-means and k-modes to clustering mixed numeric and categorical data <ref type="bibr" target="#b15">[16]</ref>. The clustering method is similar to the k-means algorithm except that it uses the kmodes approach to update the categorical attribute values of cluster prototypes (i.e., centroids). It employs a heterogeneous distance function to compute the dissimilarity between data points and cluster prototypes. This cost function combines distance measure on both numerical and categorical attributes. While the Euclidean distance is used for numeric attributes, the categorical dissimilarity is derived from the number of mismatches between categorical values. In addition, this function requires different weights for the contribution of numeric versus categorical attributes to avoid favouring either type of attribute. Specifically, the distance between data point x i ∈ X (described by D n numeric attributes and D c categorical attributes) and cluster prototype c p is estimated by</p><formula xml:id="formula_3">d(x i , c p ) = Dn ∑ j=1 (x ij -c pj ) 2 + γ Dc ∑ g=1 δ(x ig , c pg ),<label>(4)</label></formula><p>where δ(y, z) = 0 if y = z and 1, otherwise. In addition, γ is a weight for categorical attributes. A large weight parameter γ means that the clustering process favours the categorical attributes, while a small γ indicates that numerical attributes are emphasized.</p><p>Agglomerative Hierarchical Clustering begins by considering each data point as a cluster (i.e., singleton cluster), and then gradually merges similar clusters until all the clusters are combined into one big group (i.e., the top node of a dendrogram). The resulting dendrogram can be cut at any level to obtain the desired data partitions <ref type="bibr" target="#b27">[28]</ref>. The main differences among agglomerative clustering methods are the definitions of distance between two clusters, which are used to determine how data points in the dataset should be grouped into clusters. The well-known agglomerative hierarchical techniques are:</p><p>• Single-Linkage (SL): defines the distance between two clusters to be the minimum distance between all pairs of data points, taken one from each cluster. Let C p and C q be clusters, the single-linkage distance between these two clusters D CpCq is defined by</p><formula xml:id="formula_4">D CpCq = min ∀x∈Cp,x ′ ∈Cq d(x, x ′ ),<label>(5)</label></formula><p>where d(x, x ′ ) is usually the Euclidean distance between data points x, x ′ ∈ X . • Complete-Linkage (CL): determines the dissimilarity between clusters via the largest distance between data points in the clusters under examination. Formally, the distance between two clusters D CpCq is defined as follows:</p><formula xml:id="formula_5">D CpCq = max ∀x∈Cp,x ′ ∈Cq d(x, x ′ )<label>(6)</label></formula><p>• Average-Linkage (AL): uses the average value of all pair-wise distance among data points in the two clusters as the cluster distance. In particular, the distance between clusters C p and C q is estimated by</p><formula xml:id="formula_6">D CpCq = 1 N p N q ∑ ∀x∈Cp ∑ ∀x ′ ∈Cq d(x, x ′ ),<label>(7)</label></formula><p>where N p and N q are the number of data points in cluster C p and C q , respectively.</p><p>These methods provide visualizations on how data points are grouped in different levels on a dendrogram, which can help users to analyse data more easily or select desired groups of data at some fixed level of proximity. However, the main drawback of such techniques is their complexity, O(N 2 ) to O(N 3 ). Therefore, they are impractical for large datasets.</p><p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) relies on the density of data points in the neighbourhood of each data point <ref type="bibr" target="#b28">[29]</ref>. This density with respect to a definite data point is determined by the number of the other objects within a hypersphere area around it. The key idea is that, for each object of a cluster, its neighbourhood of a given radius (ϵ) has to contain at least a minimum number of instances (MinPts).</p><p>Several definitions used in DBSCAN are defined, based on the input parameters ϵ and MinPts, as follows:</p><p>• An ϵ-neighbourhood of x i ∈ X is a set of data objects that have distance from x i less than or equal to ϵ (i.e., ϵ indicates the neighbourhood radius).</p><p>• A core object is a data point with a neighbourhood consisting of more than MinPts data points.</p><p>• An object x j ∈ X is directly density-reachable from a core object x i if x j is within ϵ-neighbourhood of x i . • An object x j is density-reachable from a core object x i if there exists a finite sequence of core objects between x i and x j such that each connecting core belongs to an neighbourhood of its predecessor.</p><p>• Two data points x i and x j are density-connected if they are density-reachable from a common core.</p><p>To find a cluster, DBSCAN checks the ϵ-neighbourhood of each data point. If the ϵ-neighbourhood of x i ∈ X contains more than MinPts members, a new cluster with x i as a core object is created. Then it iteratively collects directly density-reachable objects from these core objects, which may involve merging a few densityreachable clusters. This process terminates when no new object can be added to any cluster. DBSCAN can find arbitrary shapes of clusters, identify outliers, and determine the number of clusters automatically. However, the main disadvantage is that the quality of the resulting clusters is sensitive to the user-defined parameters, ϵ and MinPts, which are difficult to determine. The computational</p><formula xml:id="formula_7">complexity of DBSCAN is O(N log N) if a spatial index is used. Otherwise, it is O(N 2 )</formula><p>, where N is the number of data points. SOM (Self-Organizing Map) is a very useful and well-known tool <ref type="bibr" target="#b29">[30]</ref> for a range of applications, including dimensional reduction, sampling, classification and data clustering <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. SOM makes extensive use of the neural network technology, with the basic idea of mapping the data patterns onto a multi-dimensional grid of neurons or units. That grid forms the output space, as opposed to the input space where the original data patterns are. The underlying mapping attempts to preserve the 'topological' relations, i.e., those patterns that are close in the input grid are to be mapped to units that are close in the output grid. In other words, the information regarding neighbourhoods of the patterns under examination are preserved through the mapping process. Many SOM variants have been introduced in the literature, see Furukawa <ref type="bibr" target="#b32">[33]</ref> and Tokunaga and Furukawa <ref type="bibr" target="#b33">[34]</ref> for examples of the recent development.</p><p>Specific to data clustering, the basic SOM algorithm <ref type="bibr" target="#b31">[32]</ref> can be described as follows, where X denotes a dataset of N data points:</p><p>1. Initialize a set of cluster centre, i.e., C = {c 1 , . . . , c k } 2. For each data point x i ∈ X : (a) Assign x i to a cluster C j that provides the minimum Euclidean distance between x i and its cluster centre c j . (b) Update the cluster centre c j , which is the weight vector of SOM's output units, using the following equation.</p><formula xml:id="formula_8">c j = c j + h[x i -c j ],<label>(8)</label></formula><p>where h ∈ [0, 1] is the degree of neighbourhood. In addition to updating the centre c j of the cluster that x i belongs to, all the cluster centres that are in the neighbourhood of c j on the grid map are also updated. This neighbourhood-based propagation is controlled by h, which can be specified using the neighbourhood functions such as the bell-shaped (Gaussian-like) and the square (or bubble). This process is repeated for all</p><formula xml:id="formula_9">x i ∈ X .</formula><p>Despite its innovative concept and reported success, the major disadvantage of SOM is the long processing time, especially with a large dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Basis of cluster ensembles</head><p>Although, a large number of clustering algorithms have been introduced for a variety of application areas <ref type="bibr" target="#b13">[14]</ref>, the No Free Lunch theorem <ref type="bibr" target="#b34">[35]</ref> suggests <ref type="foot" target="#foot_0">1</ref> there is no single clustering algorithm that performs best for all datasets <ref type="bibr" target="#b35">[36]</ref>, i.e., no algorithm is able to discover all types of cluster shapes and structures presented in data <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. Each algorithm has its own strengths and weaknesses. For a particular dataset, different algorithms, or even the same algorithm with different parameters usually provide distinct solutions. Therefore, it is extremely difficult for users to decide which algorithm would be the proper alternative for a given set of data. As identified in the previous section, the use of those conventional clustering algorithms such as k-means, k-modes and k-prototypes can be complicated due to their settings of k, distance metric or initial centroids. Likewise, agglomerative hierarchical clustering methods also encounter the problem of selecting and appropriate k, while this is naturally solved within the process of DBSCAN. Nonetheless, the latter suffers greatly from configuring several parameters.</p><p>Recently, cluster ensembles have emerged as an effective solution that is able to overcome these limitations, and improve the robustness as well as the quality of clustering results. The main objective of cluster ensembles is to combine different decisions of various clusterings in such a way to achieve the accuracy superior to those of individual clustering. Examples of well-known ensemble methods are: (i) the feature-based approach that treats the problem of cluster ensembles as the clustering of categorical data, i.e., cluster labels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>, (ii) the direct approach that finds the final partition through relabelling the results of base clustering <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, (iii) graph-based algorithms that employ a graph partitioning methodology <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref>, and (iv) the pairwise-similarity approach that makes use of co-occurrence relationships between all pairs of data points <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref>. The following subsections will introduce three fundamental concepts of problem definition, ensemble generation and consensus function, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Problem formulation</head><p>Let X = {x 1 , . . . , x N } be a set of N data points, where each x i ∈ X is represented by a vector of D attribute values, i.e., x i = (x i,1 , . . . , x i,D ). Also, let Π = {π 1 , . . . , π M } be a cluster ensemble with M base clusterings, each of which is referred to as an 'ensemble member'. Each base clustering returns a set of clusters</p><formula xml:id="formula_10">π g = {C g 1 , C g 2 , . . . , C g kg }, such that ⋃ kg t=1 C g t = X ,</formula><p>where k g is the number of clusters in the gth clustering. For each x i ∈ X , C g (x i ) denotes the cluster label in the gth base clustering to which data</p><formula xml:id="formula_11">point x i belongs, i.e., C g (x i ) = 't' (or 'C g t ') if x i ∈ C g t . The problem is to find a new partition π * = C * 1 , . . . , C * K ,</formula><p>where K denotes the number of clusters in the final clustering result, of a dataset X that summarizes the information from the cluster ensemble Π.</p><p>The general framework of cluster ensembles is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Essentially, solutions achieved from different base clusterings are aggregated to form a final partition. This meta-level method involves two major tasks of: (i) generating a cluster ensemble, and (ii) producing the final partition (normally referred to as a consensus function).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Ensemble generation strategies</head><p>It has been shown that ensembles are most effective when constructed from a set of predictors whose errors are dissimilar <ref type="bibr" target="#b51">[52]</ref>. To a great extent, diversity amongst ensemble members is introduced to enhance the result of an ensemble <ref type="bibr" target="#b52">[53]</ref>. Specific to data clustering, the results obtained with any single algorithm over many iterations are usually very similar. In such circumstance when all ensemble members agree on how a dataset should be partitioned, aggregating the base clustering results will show no improvement over any of the constituent members. Several heuristics have been proposed to introduce artificial instabilities in clustering algorithms, hence the diversity within a cluster ensemble. The following ensemble generation methods yield different clusterings of the same data, by exploiting different cluster models and different data partitions. Homogeneous ensembles: Base clusterings are created using repeated runs of a single clustering algorithm, with several sets of parameter. In particular, the k-means technique has often been employed with a random initialization of cluster centres <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54]</ref>. An ensemble of k-means is computational efficient as its time complexity is O(kNM), where k, N and M denote the number of clusters, the number of data points and the number of base clusterings, respectively. Other non-deterministic clustering techniques (results of multiple runs are dissimilar) such as PAM <ref type="bibr" target="#b54">[55]</ref> can be used to form a homogeneous ensemble.</p><p>However, as compared with k-means, the ensembles of PAM are less efficient with time complexity of O(Mk(Nk) 2 ) and</p><formula xml:id="formula_12">O(M(ks 2 + k(N -k)))</formula><p>, respectively. Note that s denotes the sample size (s &lt; N). Unlike the aforementioned alternatives of base clustering, hierarchical clustering techniques (e.g., single-linkage (SL), complete-linkage (CL) and average-linkage (AL)) are deterministic with the identical result being obtained from multiple runs for any given number of clusters, k. Hence, such methods cannot generate diversity within a homogeneous ensemble.</p><p>This method is widely practised to create base clusterings each with distinct parameters and hence output. Selecting such parameters is problematic for a basic clustering approach, but with the ensemble method, a variation of setting can be randomly chosen for generate diverse versions of clustering model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different-k:</head><p>The output of clustering algorithms is dependent on the initial choice of the number of clusters k. To acquire an ensemble diversity, base clusterings are created using a specific value of k or randomly selected k from a pre-specified interval. Intuitively, k should be greater than the expected number of clusters and the common rule-of-thumb is k = √ N <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref>. This generation scheme allows a large number of clustering algorithms, both partitioning and hierarchical, to be used as base clustering. However, k-means is still often employed for the efficiency reason. It is noteworthy that the time complexity of creating a cluster ensemble with a hierarchical clustering technique being used as base clusterings is O(N 3 M).</p><p>Random subspacing/sampling: A cluster ensemble can also be achieved by applying manifold subsets of initial data to base clusterings. It is assumed that each clustering algorithm can provide different levels of performance for different partitions of a dataset <ref type="bibr" target="#b42">[43]</ref>. In practice, data partitions can be obtained by projecting data onto different subspaces <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b56">57]</ref>, choosing different subsets of features <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b57">58]</ref>, or using data sampling techniques <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>Let a matrix X ∈ R N×D represents a dataset of N data points each of which is specified by D attributes/features. An artificial diversity within an ensemble Π can be achieved by generating base clustering results from different perturbed variations of X . To this extent, a random projection method <ref type="bibr" target="#b60">[61]</ref> is objectively used in <ref type="bibr" target="#b46">[47]</ref> and <ref type="bibr" target="#b56">[57]</ref> to create such a transformed data matrix X ′ ∈ R N×D ′ from the original X , where D ′ &lt; D.</p><p>It is also possible to create different data subspaces each of which contains a randomly selected subset of original attributes <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62]</ref>. Each data subspace X ′ is generated by firstly defining D ′ :</p><formula xml:id="formula_13">D ′ = D ′ min + ⌊α(D ′ max -D ′ min )⌋,<label>(9)</label></formula><p>where α ∈ [0, 1] is a uniform random variable, D ′ min and D ′ max are the lower and upper bounds of the generated subspace, respectively. Following Yu et al. <ref type="bibr" target="#b57">[58]</ref>, D ′ min and D ′ max are set to 0.75D and 0.85D. An attribute is selected one by one from the pool of D attributes, until the collection of D ′ is obtained. In particular, the index of each randomly selected attribute is determined as follows: h = ⌊1 + βD⌋, <ref type="bibr" target="#b9">(10)</ref> where h denotes the hth attribute in the pool of D attributes and β ∈ [0, 1) is a uniform random variable.</p><p>In addition to using data subspaces, an ensemble can also be created by applying a selected clustering algorithm(s) to a set of data perturbations. In the studies of Dudoit and Fridyand <ref type="bibr" target="#b58">[59]</ref> and Fischer and Buhmann <ref type="bibr" target="#b40">[41]</ref>, perturbed datasets (of the same size as the original data) are obtained using the bootstrapping (or bagging) resampling scheme <ref type="bibr" target="#b62">[63]</ref>, whereby data points are sampled with replacement from the original dataset. Despite its effectiveness, especially for classifier ensembles, bootstrapping produces datasets with duplicated data points, which artificially distort the actual data compactness. An alternative to overcome this shortcoming is a subsampling technique, whereby a subset of data points is sampled without replacement from the original dataset. Specific to the strategy employed by Kim and Lee <ref type="bibr" target="#b63">[64]</ref> and Monti et al. <ref type="bibr" target="#b50">[51]</ref>, each base clustering is obtained with a data subset that contains randomly selected 80% of original data points.</p><p>Heterogeneous ensembles: As an alternative, heterogeneous ensembles may be exploited, where diversity is induced by allowing each base clustering to be generated using different clustering algorithms <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>. The key idea is that each clustering technique has its own benefits and drawbacks, and is suitable for different types of dataset. Multiple algorithms can provide different decisions on data partitions and complement each other. Thus, combining different clustering results based on multiple clustering techniques can assure better data clusterings. This approach is adapted by many ensemble algorithms, for example, the clustering aggregation proposed by Gionis et al. <ref type="bibr" target="#b41">[42]</ref> applies single linkage, average linkage, complete linkage, Ward's clustering and k-means to generate the ensembles.</p><p>Mixed heuristics: In addition to using one of the aforementioned methods, any combination of them can be applied as well.  </p><formula xml:id="formula_14">Π = {π 1 , π 2 }, π 1 = {C 1 1 , C 1 2 , C 1 3 } and π 2 = {C 2 1 , C 2 2 }.</formula><p>This can be found in <ref type="bibr" target="#b44">[45]</ref>, where several clusters (i.e., clustering algorithms) are used with multiple subspaces of data. Similarly, Monti et al. <ref type="bibr" target="#b50">[51]</ref> apply hierarchical clustering with averagelinkage and the self organizing map (SOM) with different sets of sampled data, while Domeniconi and Al-Razgan <ref type="bibr" target="#b42">[43]</ref> generate the ensembles using their subspace clustering algorithm (LAC) with different input conditions. In addition, Fred and Jain <ref type="bibr" target="#b66">[67]</ref> employ all strategies to construct their ensembles by applying three algorithms (k-means, single-linkage and spectral algorithm), with various initial settings, to multiple subsampled data. In the experiments of Nguyen and Caruana <ref type="bibr" target="#b38">[39]</ref>, the ensembles are produced using weighting k-means and k-means with different random restarts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Consensus functions</head><p>Having obtained the cluster ensemble, a variety of consensus functions have been developed and made available for deriving the ultimate data partition. Each consensus function utilizes a specific form of information matrix, which summarizes the base clustering results. From the cluster ensemble shown in Fig. <ref type="figure" target="#fig_2">2(a)</ref>, three general types of such ensemble-information matrix can be constructed. Firstly, the label-assignment matrix (Fig. <ref type="figure" target="#fig_2">2(b</ref>)), of size N × M, represents cluster labels that are assigned to each data point by different base clusterings. Secondly, the pairwise similarity matrix (Fig. <ref type="figure" target="#fig_2">2(c</ref>)), of size N × N, summarizes co-occurrence statistics amongst data points. Furthermore, the binary clusterassociation (BA) matrix (see Fig. <ref type="figure" target="#fig_2">2</ref>(d) for an example) provides a cluster-specific view of the original label-assignment matrix. The association degree that a data point belonging to a specific cluster is either 1 or 0.</p><p>Given this background, a large number of different consensus functions found in the literature can be described and classified to four major categorizations: direct, feature-based, pairwisesimilarity based and graph-based approaches, respectively. Examples of cluster ensemble methods belonging to these families will be provided in the next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cluster ensemble methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Direct approach</head><p>The first family of cluster ensemble methods is characterized by the use of a combination strategy such as 'voting', which has proven effective for classifier ensembles <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>. However, such practice is not directly applicable to the cluster ensemble problem where a priori class information is not available. The cluster labels in different data partition (i.e., base clustering) π g , g = 1 . . . M are arbitrary. As a result, a mechanism that finds 'label correspondence' and re-labels each partition in accordance with a reference partition, is necessary for developing such a voting model. Most methods in this category require the number of clusters in each base partition to be K , i.e., k g = K , g = 1 . . . M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple Voting:</head><p>Based on the analysis of Topchy et al. <ref type="bibr" target="#b69">[70]</ref>, the underlying re-labelling process is equivalent to the problem of maximum weight bipartite matching. This starts with the creation of a contingency matrix Ω ∈ R K ×K from the reference π r and tobe re-labelled π g partitions, where K is the number of clusters in each partition. Each entry Ω(l, l ′ ) that denotes the co-occurrence statistics between labels l ∈ π r and l ′ ∈ π g , is defined by</p><formula xml:id="formula_15">Ω(l, l ′ ) = ∑ ∀x i ∈X ω(x i ),<label>(11)</label></formula><p>where ω(</p><formula xml:id="formula_16">x i ) = 1 if (C r (x i ) = l) ∧ (C g (x i ) = l ′ ), otherwise ω(x i ) = 0.</formula><p>Having obtained Ω, the label correspondence is solved by maximizing</p><formula xml:id="formula_17">K ∑ l=1 K ∑ l ′ =1 Ω(l, l ′ )Θ(l, l ′ ),<label>(12)</label></formula><p>where Θ ∈ R K ×K is another matrix representing correspondences amongst labels of partitions π r and π g . An entry Θ(l, l ′ ) = 1 if label l ∈ π r corresponds to label l ′ ∈ π g , 0 otherwise. Note also that</p><formula xml:id="formula_18">K ∑ l=1 Θ(l, l ′ ) = K ∑ l ′ =1 Θ(l, l ′ ) = 1<label>(13)</label></formula><p>The solution to this optimization problem can be found using the Hungarian algorithm <ref type="bibr" target="#b70">[71]</ref>. Fig. <ref type="figure" target="#fig_3">3</ref>(a) presents an example of the contingency matrix Ω which is created from two sets of labels given to four data points X = {x 1 , . . . , x 4 } within partitions π r = {1, 1, 1, 2} and π g = {1, 2, 2, 1}. As shown in Fig. <ref type="figure" target="#fig_3">3(b</ref>), the Ω matrix can also be presented as a weighted bipartite graph, in which the maximum matchings are identified as bold edges. This suggests relabelling the label '1' of partition π g as '2', and the label '2' as '1'.</p><p>Particularly to the study of Topchy et al. <ref type="bibr" target="#b69">[70]</ref>, the reference partition π r is randomly selected from the pool of M partitions in an ensemble Π, i.e., π r ∈ Π. Then, each of the M -1 remaining partitions is re-labelled with respect to the chosen π r , by following the aforementioned steps. Hence, a globally consistent label set is employed by all partitions. Then, a plurality voting can be employed to determine the consensus label of each data point</p><p>x i ∈ X . The consensus methods of Dudoit and Fridyand <ref type="bibr" target="#b58">[59]</ref> and Fischer and Buhmann <ref type="bibr" target="#b40">[41]</ref> also implement a similar voting model. However, the reference partition π r ̸ ∈ Π is obtained from the original data (X ), while partitions in an ensemble (π 1 . . . π M ∈ Π) are acquired on subsets of X . That is, each partition π g , g = 1 . . . M is re-labelled in accordance with π r that is not part of the ensemble itself.</p><p>Incremental Voting: The incremental (or cumulative) voting model is initially developed in the studies of Ayad and Kamel <ref type="bibr" target="#b71">[72]</ref>, Dimitriadou et al. <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74]</ref>, Frossyniotis et al. <ref type="bibr" target="#b74">[75]</ref>, and later generalized by Ayad and Kamel <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref>. Unlike the simple voting previously discussed, data partitions are added to the underlying ensemble one by one, with the voting statistics being repeatedly updated. Let P g ∈ R N×K be the matrix representing the gth partition, i.e., π g ∈ Π.</p><formula xml:id="formula_19">Each P g (x i , C g t ) is 1 if data point x i ∈ X belongs to cluster C g t ∈ π g , 0 otherwise. Also let V g ∈ R N×K be</formula><p>the matrix presenting the result of combining the first g partitions (π 1 , . . . , π g ), and V g (x i , L j ) ∈ {0, 1, . . . , g} is the accumulative frequency (or the number of partitions) that the label L j is assigned</p><formula xml:id="formula_20">to data point x i . Note that, initially, V 1 = P 1 .</formula><p>At the (g + 1)th step (1 &lt; (g + 1) ≤ M) where the (g + 1)th partition is added to cluster ensemble, the relabelling algorithm such as Hungarian is exploited to find the correspondence between cluster labels (or columns) of the matrices V g and P g+1 , with the first set being the reference partition. This begins with the creation of the contingency matrix, Ω ∈ R K ×K , where each Ω(l, l ′ ) is estimated as</p><formula xml:id="formula_21">Ω(l, l ′ ) = ∑ ∀x i ∈X ω(x i ),<label>(14)</label></formula><p>where ω(</p><formula xml:id="formula_22">x i ) = 1 if (V g (x i , l) ≥ 1) ∧ (P g (x i , l ′ ) = 1), otherwise ω(x i ) = 0.</formula><p>The optimal label (column) correspondence can be found as the maximum matching in the weighted bipartite graph created from Ω.</p><p>After that, the matrix V g+1 that represents the result of merging g + 1 partitions is generated such that an entry V g+1 (x i , l) is calculated as</p><formula xml:id="formula_23">V g+1 (x i , l) = V g (x i , l) + P g+1 (x i , l ′ ),<label>(15)</label></formula><p>where the label or column l ′ of P g+1 corresponds to the column l of V g .</p><p>To decide the final label C * (x i ) of each data point x i ∈ X from the incremental combination of M data partitions, the V M matrix is exploited as follows:</p><formula xml:id="formula_24">C * (x i ) = argmax l V M (x i , l)<label>(16)</label></formula><p>Label Correspondence Search (LCS): Instead of relying on the combination strategy inherited from the task of classifier ensembles, the method introduced by Boulis &amp; Ostendorf <ref type="bibr" target="#b36">[37]</ref> searches for 'label correspondence', which has been specifically modelled as an optimization problem. Let U g t ∈ R N×1 be a vector that represents the posteriors of cluster C g t for N data points. With respect to data</p><formula xml:id="formula_25">point x i ∈ X , the ith entry U g t (x i ) = p(C g t |x i ). Specifically to a crisp partition, U g t (x i ) = 1 if x i ∈ C g t , 0 otherwise. The agreement G(C g t , C g ′ t ′ ) between clusters C g t and C g ′ t ′ in the partitions π g , π g ′ ∈ Π</formula><p>, respectively, can be defined as</p><formula xml:id="formula_26">G(C g t , C g ′ t ′ ) = ( U g t ) T • U g ′ t ′<label>(17)</label></formula><p>This allows the correspondence between clusters to be identified. Hence, LCS makes use of such measure to formulate the following 'goodness' function,</p><formula xml:id="formula_27">F Λ . It is objectively maximized to generate meta-clusters C * m , m = 1 . . . K from clusters in a given ensemble Π. F Λ = K ∑ m=1 M ∑ g=1 K ∑ t=1 Λ(C g t , C * m ) × S(C g t , C * m ),<label>(18)</label></formula><p>subjected to</p><formula xml:id="formula_28">K ∑ m=1 Λ(C g t , C * m ) = 1, ∀g, t<label>(19)</label></formula><p>here Λ(C</p><formula xml:id="formula_29">g t , C * m ) = 1 if the cluster C g t ∈ π g is assigned to the meta-cluster C * m and 0 otherwise. Also, S(C g t , C * m ) denotes the score of assigning the cluster C g t to the meta-cluster C * m , provided that C g t ∈ C * m if Λ(C g t , C * m ) ̸ = 0. S(C g t , C * m ) = 1 |C * m | ∑ ∀cl∈C * m ,cl̸ =C g t G(C g t , cl)<label>(20)</label></formula><p>Optionally, the objective function F Λ can be maximized with respect to the additional constraint of:</p><formula xml:id="formula_30">K ∑ t=1 Λ(C g t , C * m ) = 1, ∀m, g<label>(21)</label></formula><p>According to <ref type="bibr" target="#b36">[37]</ref>, two models can be derived by omitting the constraint given in Eq. ( <ref type="formula" target="#formula_30">21</ref>) or including it in the optimization process. This results in the 'Unconstrained' and 'Constrained' methods, respectively. The outcome of these models is the matrix F ∈ R N×K where an entry F (x i , C * m ) denotes the association between data</p><formula xml:id="formula_31">point x i , i = 1 . . . N and meta-cluster C * m , m = 1 . . . K . Each column F m , m = 1 . . . K represents the centroid of the meta-cluster C * m .</formula><p>In particular, F m is defined as</p><formula xml:id="formula_32">F m = 1 |C * m | ∑ ∀ C g t ∈C * m ( U g t ) T (<label>22</label></formula><formula xml:id="formula_33">)</formula><p>The final label</p><formula xml:id="formula_34">C * (x i ) of data point x i ∈ X is α given that F (x i , C * α ) = max m=1 ... K F (x i , C * m )<label>(23)</label></formula><p>Another method, named 'Singular Value Decomposition Combination', to create the output matrix F has also been proposed in <ref type="bibr" target="#b36">[37]</ref>. Let p g (t|i) be the posterior of cluster</p><formula xml:id="formula_35">C g t in the partition π g ∈ Π for data point x i ∈ X . Each entry F (x i , C * m ), ∀ i = 1 . . . N, ∀ m = 1 . . . K is estimated by F (x i , C * m ) = M ∑ g=1 λ(f g (C * m ), C * m ) × p g (h(f g (C * m ))|i),<label>(24)</label></formula><p>where f g (C * m ) denotes the function that aligns the meta-cluster C * m and clusters in the partition π g . This is estimated using the SVD (Singular Value Decomposition) technique which finds the most correlated pair of clusters. In addition, λ weighting function provides a soft alignment of clusters in question, while the function</p><formula xml:id="formula_36">h(l) = l -K ⌊l/K ⌋.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature-based approach</head><p>Several techniques that are categorized into this group are based similarly on the categorical/nominal data presented in the label-assignment matrix (see example in Fig. <ref type="figure" target="#fig_2">2(b)</ref>). Unlike the direct approach previously discussed, feature-based methods cluster data points using the nominal information that is originally obtained from an ensemble, without searching for correspondence amongst labels or re-labelling. Details of such models are given below:</p><p>Iterative Voting Consensus (IVC): This feature-based method was recently introduced by Nguyen and Caruana <ref type="bibr" target="#b38">[39]</ref>. It aims to obtain the consensus partition π * of data points X from the labelassignment or categorical data matrix that is induced by a cluster ensemble Π = {π 1 , . . . , π M }. IVC makes use of the set of label vectors Y = (y 1 , . . . , y N ), with N denoting the number of data points and y i , i = 1 . . . N being specified as</p><formula xml:id="formula_37">y i = (y i1 = C 1 (x i ), . . . , y iM = C M (x i )),<label>(25)</label></formula><p>where C g (x i ) represents a label of specific cluster in clustering π g , g = 1 . . . M, to which data point x i ∈ X belongs. Note that y i corresponds to the ith row in the label-assignment matrix.</p><p>In each iteration, IVC first estimates the centre of each cluster in π * . Note that each cluster C * p , p = 1 . . . K in the target clustering π * has a cluster centre C * p = {mode(X p , π 1 ), . . . , mode(X p , π M )}, where X p ⊂ X is the set of data points belonging to the cluster C * p and mode(X p , π g ) denotes the majority labels (in the clustering π g ) of members in X p . Having obtained these centres, IVC then reassigns each data point to its closest cluster centre. This is possible using the Hamming distance between M-dimensional vectors that represent data points and cluster centres. The iterative process continues until there is no change with the target clustering π * . It is noteworthy that a consensus function similar to IVC has also been developed in the study of Luo et al. <ref type="bibr" target="#b77">[78]</ref>.</p><p>Mixture Model: Similar to IVC, this method of Topchy et al. <ref type="bibr" target="#b23">[24]</ref> also generates the final clustering π * from the label matrix Y . In particular, it is based on a finite mixture model for the probability of the cluster labels</p><formula xml:id="formula_38">y i = (C 1 (x i ), . . . , C M (x i )) of data point x i ∈ X , which is acquired from the ensemble Π = {π 1 , . . . , π M }.</formula><p>Label vectors y i representing data points x i are specified as random variables generated from a probability distribution. This can be described as a mixture of multivariate component densities.</p><formula xml:id="formula_39">P(y i |Θ) = K ∑ t=1 ϕ t P t (y i |θ t ),<label>(26)</label></formula><p>where Θ = {ϕ 1 , . . . , ϕ K , θ 1 , . . . , θ K } is the collection of components. The tth components are identified with respect to the cluster C * t , t = 1 . . . K in the final clustering π * . In this model, each data point y i , i = 1 . . . N is presumed to be created by: first, drawing a component in according to the mixing coefficient ϕ t , then, sampling a data point from the distribution P t (y i |θ t ).</p><p>The mixture model is formulated as a maximum likelihood estimation problem, which aims to find the best fitting mixture density for a given data Y . This is obtained by maximizing the following likelihood function with respect to the unknown Θ.</p><formula xml:id="formula_40">Θ * = argmax Θ log L(Θ|Y )<label>(27)</label></formula><p>By assuming that all data points y 1 , . . . , y N are independent and identically distributed, the previous function can be simplified to</p><formula xml:id="formula_41">log L(Θ|Y ) = log N ∏ i=1 P(y i |Θ) = N ∑ i=1 log K ∑ t=1 ϕ t P t (y i |θ t )<label>(28)</label></formula><p>Following that, for each data point, the corresponding density distribution P t (y i |θ t ) is defined as follows, where a conditional independence assumption is made for the components of y i .</p><formula xml:id="formula_42">P t (y i |θ t ) = M ∏ g=1 P g t (y ig |θ g t ),<label>(29)</label></formula><p>given that</p><formula xml:id="formula_43">P g t (y ig |θ g t ) = kg ∏ l=1 (ϑ gt (l)) δ(y ig ,l) ,<label>(30)</label></formula><p>where k g is the number of clusters in the clustering π g ∈ Π, ϑ gt (l), l = 1 . . . k g are probabilities of the outcomes with ∑ kg l=1 ϑ gt (l) = 1, ∀g = 1 . . . M, ∀t = 1 . . . K , and δ(y ig , l) is defined as</p><formula xml:id="formula_44">δ(y ig , l) = { 1 if y ig = l 0 otherwise (31)</formula><p>The EM algorithm is adopted to optimize the likelihood function given in Eq. ( <ref type="formula" target="#formula_41">28</ref>). For such purpose, the existences of hidden data Z and the likelihood of complete data (Y , Z ) are hypothesized. It is possible to identify which of K mixture components has been exploited to generate data point y i if the corresponding</p><formula xml:id="formula_45">z i = (z i1 , . . . , z iK ) is known. Specifically, z it = 1 if y i belongs to the tth component (i.e., y i ∈ C * t , t = 1 . . . K ), otherwise z it = 0. As a result,</formula><p>the likelihood function of Eq. ( <ref type="formula" target="#formula_41">28</ref>) is modified as</p><formula xml:id="formula_46">log L(Θ|Y , Z ) = N ∑ i=1 K ∑ t=1 z it log ϕ t P t (y i |θ t )<label>(32)</label></formula><p>The resulting EM process begins with an initial guess of the</p><formula xml:id="formula_47">model parameters Θ ′ = {ϕ ′ 1 , . . . , ϕ ′ K , θ ′ 1 , . . . , θ ′ K }.</formula><p>Then the following steps are repeated until the convergence criterion is satisfied.</p><p>As suggested by Topchy et al. <ref type="bibr" target="#b23">[24]</ref>, the stability of the assignment of data points Y (or equivalently X ) can be employed as a convergence criterion in practice.</p><formula xml:id="formula_48">1. Compute expected values E[z it ], ∀i = 1 . . . N, t = 1 . . . K : E[z it ] = ϕ ′ t ∏ M g=1 ∏ kg l=1 (ϑ ′ gt (l)) δ(y ig ,l) ∑ K s=1 ϕ ′ s ∏ M g=1 ∏ kg l=1 (ϑ ′ gs (l)) δ(y ig ,l)<label>(33)</label></formula><p>2. Re-estimate the parameters:</p><formula xml:id="formula_49">ϕ t = ∑ N i=1 E[z it ] ∑ N i=1 ∑ K t=1 E[z it ]<label>(34)</label></formula><formula xml:id="formula_50">ϑ gt (l) = ∑ N i=1 δ(y ig , l)E[z it ] ∑ N i=1 ∑ kg l=1 δ(y ig , l)E[z it ]<label>(35)</label></formula><p>Having obtained the final (or converged) Z , the consensus cluster label of each data point x i (or y i , i = 1 . . . N) can be defined as</p><formula xml:id="formula_51">C * (x i ) = ξ provided that E[z iξ ] = max t=1 ... K E[z it ]<label>(36)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering Aggregation (AGG):</head><p>The problem of clustering aggregation <ref type="bibr" target="#b41">[42]</ref> is to find a clustering that minimizes the 'disagreements' with ensemble members. Formally, a measure of disagreement between two clustering π a , π b ∈ Π with respect to two specific data points x i , x j ∈ X can be defined as follows:</p><formula xml:id="formula_52">d x i ,x j (π a , π b ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 if ( C a (x i ) = C a (x j ) ∧ C b (x i ) ̸ = C b (x j ) ) ∨ ( C a (x i ) ̸ = C a (x j ) ∧ C b (x i ) = C b (x j ) ) 0 otherwise ,<label>(37)</label></formula><p>where C g (x i ) denotes the label that is assigned to data point x i ∈ X in the clustering π g ∈ Π. Note that such information is summarized by the label-assignment matrix, see Fig. Given the set of data points X = {x 1 , . . . , x N }, the distance/proximity between two clusterings π a , π b ∈ Π is specified by</p><formula xml:id="formula_53">d X (π a , π b ) = ∑ ∀(x i ,x j )∈X ×X d x i ,x j (π a , π b )<label>(38)</label></formula><p>With the cluster ensemble Π = {π 1 , . . . , π M } of data points X , the aim of clustering aggregation is to search for a median partition π * that minimizes the following objective function:</p><formula xml:id="formula_54">D(π * ) = M ∑ g=1 d X (π g , π * )<label>(39)</label></formula><p>According to Gionis et al. <ref type="bibr" target="#b41">[42]</ref>, this problem can be generalized to 'correlation clustering' <ref type="bibr" target="#b78">[79]</ref> that sets to minimize the cost function of:</p><formula xml:id="formula_55">d(π * ) = ∑ ∀(x i , x j ), C * (x i ) = C * (x j ) DA(x i , x j ) + ∑ ∀(x i , x j ), C * (x i ) ̸ = C * (x j ) 1 -DA(x i , x j ), (<label>40</label></formula><formula xml:id="formula_56">)</formula><p>where DA ∈ R N×N is the matrix of distance amongst N data points and each DA(x i , x j ) can be estimated by</p><formula xml:id="formula_57">DA(x i , x j ) = ∑ M g=1 β g (x i , x j ) M ,<label>(41)</label></formula><formula xml:id="formula_58">here β g (x i , x j ) = 1 if C g (x i ) ̸ = C g (x j</formula><p>), and 0 otherwise.</p><p>Based on the aforementioned basis, a number of algorithms have been proposed to find the partition π * by applying conventional clustering techniques to the discovered DA matrix. In particular, three algorithms of AGG F , AGG LSR and AGG LSF , apparently the most effective (see further details in <ref type="bibr" target="#b41">[42]</ref>), are included in this review.</p><p>At the outset, the AGG F algorithm makes use of the Furthest-First traversal (FF) method of Hochbaum and Shmoys <ref type="bibr" target="#b79">[80]</ref>. AGG F begins with a single cluster that contains all data points. It then searches for the pair of data points (x i , x j ∈ X ) which are furthest apart; in other words, DA(x i , x j ) = max ∀(xp,xq)∈X×X DA(x p , x q ). These data points become new clusters' centres and the remaining data points are assigned to the closest cluster (i.e., the closest cluster centre). This process is iterated such that, at each step, a new cluster centre which is the furthest from the existing centres is selected. The data points are re-assigned to the centre that incurs the least cost. At the end of each step, the cost of a new solution is calculated using Eq. ( <ref type="formula" target="#formula_55">40</ref>). If it is lower than that of the prior step, the aforementioned procedure continues. Otherwise, the algorithm terminates and outputs the previous solution.</p><p>Another algorithm called 'Local Search' is also introduced for the clustering aggregation problem. It begins with an initial partition of data points, which can be obtained randomly or from the result of another model such as AGG F . The resulting methods are referred to as AGG LSR and AGG LSF with the former and the latter setting, respectively. They similarly determine the cluster, one of the existing clusters or a new singleton cluster, that each data point should belong to with the minimum cost. This procedure is repeated until an additional alteration cannot further decrease the cost.</p><p>In particular to a data point x i ∈ X and the data partition</p><formula xml:id="formula_59">π * = {C * 1 , . . . , C * K }, the cost d(x i , C * p ) of assigning x i to C * p ∈ π *</formula><p>is defined as</p><formula xml:id="formula_60">d(x i , C * p ) = ∑ ∀x j ∈C * p DA(x i , x j ) + ∑ ∀x l ̸ ∈C * p 1 -DA(x i , x l )<label>(42)</label></formula><p>Similarly, the cost of assigning x i to a new singleton cluster</p><formula xml:id="formula_61">C single ̸ ∈ π * can be given by d(x i , C single ) = ∑ ∀x j ∈X 1 -DA(x i , x j )<label>(43)</label></formula><p>Quadratic Mutual Information (QMI): The cluster ensemble method of Topchy et al. <ref type="bibr" target="#b23">[24]</ref> searches for a 'median' partition that is the most similar to those data partitions generated by ensemble members. This is achieved by maximizing the measure of 'Quadratic mutual information (QMI)' which determines the quality of the final clustering result. In particular, QMI and CU (Category Utility) that is employed by the conceptual clustering (COBWEB) algorithm <ref type="bibr" target="#b80">[81]</ref> give the same consensus clustering criterion (see proofs and further details in the study of Topchy et al. <ref type="bibr" target="#b23">[24]</ref>). Also, it has been demonstrated by <ref type="bibr">Mirkin [82]</ref> that the maximization of CU is equivalent to minimization of the square-error criterion of k-means if the number of clusters in target partition is fixed. In particular, the label-assignment matrix (Fig. <ref type="figure" target="#fig_2">2(b)</ref>) acquired from the cluster ensemble under examination is firstly converted into its equivalent Binary Cluster-Association (BA) matrix (Fig. <ref type="figure" target="#fig_2">2(d)</ref>) counterpart. It is then transformed to another numerical variation (TMB) to which k-means can be effectively applied. The value of each TMB(x i , cl), ∀x i ∈ X , ∀cl ∈ π g , g = 1 . . . M can be defined by</p><formula xml:id="formula_62">TMB(x i , cl) = BA(x i , cl) -p(cl), (<label>44</label></formula><formula xml:id="formula_63">)</formula><p>where p(cl) is simply estimated as follows, given that N is the number of data points.</p><formula xml:id="formula_64">p(cl) = ∑ N j=1 BA(x j , cl) N (<label>45</label></formula><formula xml:id="formula_65">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refined K-Means (RKM):</head><p>The RKM method of Bradley and Fayyad <ref type="bibr" target="#b82">[83]</ref> provides a general intuition of combining multiple clustering results. Given an ensemble Π of M members, each base clustering π g ∈ Π of K clusters is obtained by applying k-means to the dataset X (or perhaps, a subset of X ). Let C g t represents the centroid of the cluster C g t , t = 1 . . . K in π g . RKM considers the set of centroids CM that is obtained from the underlying ensemble (CM = {C  <ref type="bibr" target="#b45">(46)</ref> and Φ(FM p , CM) is defined as</p><formula xml:id="formula_66">BM = argmin FMp Φ(FM p , CM),</formula><formula xml:id="formula_67">Φ(FM p , CM) = ∑ ∀cl∈FMp ∑ ∀cl ′ ∈CM d(cl, cl ′ ),<label>(47)</label></formula><p>where d(cl, cl ′ ) is the Euclidean distance between centroids cl ∈ FM p and cl ′ ∈ CM. Having obtained BM, k-means is applied to X to generate the final clustering π * , using the best refined centroids of BM to initialize the clustering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pairwise-similarity based approach</head><p>This specific category of cluster ensemble methods is based principally on the pairwise similarity amongst data points <ref type="bibr" target="#b21">[22]</ref>. A number of different consensus functions have been applied to such similarity matrix to generate the final clustering result.</p><p>Agglomerative Hierarchical Clustering Models: Given a set of data points X = {x 1 , . . . , x N }, it first generates a cluster ensemble Π = {π 1 , . . . , π M } by applying M base clusterings to the dataset X . Following that, an N × N similarity matrix is constructed for each base clustering, denoted as S g , g = 1 . . . M. Each entry in this matrix represents the relationship between two data points. If they are assigned to the same cluster, the entry will be 1, 0 otherwise. More precisely, the similarity between two data points x i , x j ∈ X from the gth ensemble member can be computed as</p><formula xml:id="formula_68">S g (x i , x j ) = { 1 if C g (x i ) = C g (x j ) 0 otherwise (48)</formula><p>Following that, M similarity matrices of S 1 , . . . , S M are merged to form a 'co-association (CO)' matrix <ref type="bibr" target="#b21">[22]</ref>, which is also called consensus matrix <ref type="bibr" target="#b50">[51]</ref>, similarity matrix <ref type="bibr" target="#b44">[45]</ref> and agreement matrix <ref type="bibr" target="#b83">[84]</ref> -see Fig. <ref type="figure" target="#fig_2">2</ref>(c) for an example. Each entry in the CO matrix represents the similarity between any two data points, which is a ratio of a number of ensemble members in which they are assigned to the same cluster to the total number of ensemble members.</p><p>Formally, each entry of such a matrix CO(x i , x j ), x i , x j ∈ X is defined as</p><formula xml:id="formula_69">CO(x i , x j ) = 1 M M ∑ g=1 S g (x i , x j )<label>(49)</label></formula><p>Since the CO matrix is a similarity matrix, any similarity-based clustering algorithm (as a consensus function) can be applied to this matrix to yield the final partition π * <ref type="bibr" target="#b21">[22]</ref>. Amongst several existing similarity-based techniques, the most well-known is agglomerative hierarchical clustering algorithm. Specifically, the SL (single-linkage) agglomerative hierarchical clustering is similarly used as a consensus function by EAC-SL <ref type="bibr" target="#b21">[22]</ref>, MULTI-K <ref type="bibr" target="#b11">[12]</ref> and FKNNCE <ref type="bibr" target="#b84">[85]</ref> cluster ensemble methods. In addition, the AL (average-linkage) clustering has also been exploited to create the final data partition by EAC-AL <ref type="bibr" target="#b21">[22]</ref> and CC HC <ref type="bibr" target="#b50">[51]</ref> methods.</p><p>Hierarchical Clustering on Normalized Edges (HCNE): Instead of applying a conventional hierarchical clustering to the CO matrix directly, the HCNE method of Li et al. <ref type="bibr" target="#b85">[86]</ref> formulates a new hierarchical clustering procedure based on the concept of 'normalized edges'. At the outset, an undirected graph G = (V , E) is created such that each vertex v i ∈ V corresponds to data point x i , ∈ X , whilst an unweighted edge e ij ∈ E connecting vertices v i , v j ∈ V exists only when CO(x i , x j ) &gt; θ. θ is a user-defined parameter in the range of [0, 1]. The measure Edge(C p , C q ) is specified as the number of distinct edges between data points in clusters C p and C q .</p><p>Given that x i , x j ∈ X ,</p><formula xml:id="formula_70">Edge(C p , C q ) = ∑ ∀ x i ∈Cp ∑ ∀ x j ∈Cq I(e ij ),<label>(50)</label></formula><p>where</p><formula xml:id="formula_71">I(e ij ) = 1 if e ij ∈ E, 0 otherwise.</formula><p>By following Guha et al. <ref type="bibr" target="#b86">[87]</ref>, this initial measure is modified to 'normalized edges (NE)' such that it is robust to clusters' sizes and shapes.</p><formula xml:id="formula_72">NE(C p , C q ) = Edge(C p , C q ) (n p + n q ) 1+f (θ ) -n 1+f (θ ) p -n 1+f (θ ) q , (<label>51</label></formula><formula xml:id="formula_73">)</formula><p>where n p denotes the number of data points in cluster C p , while n are the expected number of edges within clusters C p and C q , respectively. Also,</p><formula xml:id="formula_74">f (θ ) = 1 -θ 1 + θ<label>(52)</label></formula><p>Having obtained such means to estimate the similarity between clusters, a conventional hierarchical clustering process is used with the two most similar clusters being merged in each iteration.</p><p>Fuzzy Ensemble Clustering (FEC): Unlike many cluster ensemble methods that focus on combining the results of crisp clustering, the FEC model of Avogadri and Valentini <ref type="bibr" target="#b61">[62]</ref> has been introduced for aggregating soft data partitions each of which is obtained by applying a fuzzy clustering algorithm (such as fuzzy cmeans; <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b88">89]</ref>). For a partition π g ∈ Π where π g = {C g 1 , . . . , C g kg },</p><formula xml:id="formula_75">U g p (x i ) ∈ [0, 1] is the membership degree of data point x i ∈ X belonging to cluster C g p ∈ π g , provided that ∑ kg p=1 U g p (x i ) = 1.</formula><p>Following that, the CO alike matrix, CO ′ , is created from soft data partitions π g , g = 1 . . . M, such that CO ′ (x i , x j ) is estimated by</p><formula xml:id="formula_76">CO ′ (x i , x j ) = 1 M M ∑ g=1 kg ∑ p=1 τ (U g p (x i ), U g p (x j )), (<label>53</label></formula><formula xml:id="formula_77">)</formula><p>where τ is a fuzzy t-norm operator. In particular to the study of Avogadri and Valentini <ref type="bibr" target="#b61">[62]</ref> an algebraic product is selected as</p><formula xml:id="formula_78">t-norm, i.e., τ (U g p (x i ), U g p (x j )) = U g p (x i ) × U g p (x j ).</formula><p>To generate the soft final partition π * = {C * 1 , . . . , C * K }, the fuzzy c-means technique is applied to clustering rows of the CO ′ matrix.</p><p>The membership degree that data point x i belongs to cluster C * q is denoted as U * q (x i ). Given this, a crisp clustering result can be achieved by determining the most appropriate cluster C * q ∈ π * to which each data point should belong. Formally,</p><formula xml:id="formula_79">x i ∈ C * q if U * q (x i ) = max s=1 ... K U * s (x i ) (54)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Graph-based approach</head><p>This family of algorithms makes use of the graph representation to solve the cluster ensemble problem <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b57">58]</ref>. In this approach, a weighted graph is first constructed from the clustering ensemble. Then, the graph is partitioned in to K parts to produce the final clustering using any graph partitioning techniques.</p><p>Graph-based Consensus Clustering (GCC): This method of Yu et al. <ref type="bibr" target="#b57">[58]</ref> transforms the CO matrix into a graph G = (V , W ), where</p><p>V and W are the sets of vertices and weighted edges, respectively.</p><p>Each vertex v i ∈ V corresponds to a specific data point x i ∈ X , while the weight of edge w ij ∈ W connecting vertices v i , v j ∈ V equals to the value of entry CO(x i , x j ). The resulting graph is undirected such that w ij = w ji , ∀v i , v j ∈ V . In order to obtain the final clustering π * , the GCC approach applies the normalized cut algorithm <ref type="bibr" target="#b89">[90]</ref> to the graph G.</p><p>Cluster-based Similarity Partitioning Algorithm (CSPA): Similar to GCC, the CSPA method <ref type="bibr" target="#b44">[45]</ref> also creates a similarity graph G = (V , W ) from the CO matrix. Afterwards, a multi-level kway graph partitioning called METIS <ref type="bibr" target="#b90">[91]</ref> is used to partition the graph G into K clusters of roughly equal size. METIS handles multiconstraint graph partitioning in three phases: (i) coarsening phase, the size of the graph is successively decreased; (ii) initial partitioning phase, a k-way partition of the smaller graph is computed; and (iii) uncoarsening phase, the partitioning is successively refined as it is projected to the larger graphs. More details of METIS can also be found in the reports of Karypis and Kumar <ref type="bibr" target="#b91">[92]</ref> and Karypis and Kumar <ref type="bibr" target="#b92">[93]</ref>.</p><p>Shared Nearest Neighbours-Based Combiner (SNNC): Another graph-based method that also makes use of the CO matrix is developed by Ayad and Kamel <ref type="bibr" target="#b45">[46]</ref>. It first modifies a given similarity matrix such that only entries of a value above the pre-specified threshold µ are maintained. In other words, for any x i , x j ∈ X , CO(x i , x j ) remains unchanged if CO(x i , x j ) &gt; µ, 0 otherwise. Following that, data point x j belongs to a set of nearest neighbours</p><formula xml:id="formula_80">N x i ⊂ X of data point x i if CO(x i , x j ) &gt; 0.</formula><p>A weighted graph G = (V , W ) is then created where V is a set of weighted vertices and W is a set of weighted edges. In particular, the weight of edge w ij connecting vertices v i , v j ∈ V (corresponding to data points x i and x j ) can be estimated by</p><formula xml:id="formula_81">w ij = 2 × |N x i ∩ N x j | |N x i | + |N x j | ,<label>(55)</label></formula><p>where |A| denotes the size of set A. In addition, each vertex v i ∈ V that represents data point x i , i = 1 . . . N is given the following weight:</p><formula xml:id="formula_82">v i = N x i N (<label>56</label></formula><formula xml:id="formula_83">)</formula><p>Similar to the CSPA method, SNNC also exploits METIS to generate the final data partition π * . Note that, to reflect the majority of voting amongst ensemble members, it is suggested by Ayad and Kamel <ref type="bibr" target="#b45">[46]</ref> that the threshold µ should be around 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-Graph Partitioning Algorithm (HGPA):</head><p>Based on the binary cluster-association (BA) matrix (see an example in Fig. <ref type="figure" target="#fig_2">2(d))</ref>, HGPA <ref type="bibr" target="#b44">[45]</ref> constructs a hyper-graph, where vertices represent data points and the same-weighted hyper-edges represent clusters in the ensemble. Then, HMETIS <ref type="bibr" target="#b93">[94]</ref> is applied to partition the underlying hyper-graph into K parts with roughly of the same size.</p><p>Meta-Clustering Algorithm (MCLA): This graph-based method <ref type="bibr" target="#b44">[45]</ref> generates a graph that represents the relationships among clusters in the ensemble. In this meta-level graph, each vertex corresponds to each cluster in the ensemble and each edge's weight between any two cluster vertices is computed using the binary Jaccard measure (i.e., the ratio of the intersection to the union of the sets of objects belonging to the two clusters). METIS is also employed to partition the meta-level graph into K meta-clusters. Effectively, each data point has a specific association degree to each meta-cluster. This can be estimated from the number of original clusters, to which the data point belongs, in the underlying meta-cluster. The final clustering π * is produced by assigning each data point to the meta-cluster with which it is most frequently associated (i.e., with the highest association degree).</p><p>Hybrid Bipartite Graph Formulation (HBGF): HBGF <ref type="bibr" target="#b43">[44]</ref> is introduced with the purpose to improve the previous models of CSPA and MCLA that considers only either the associations between data points or those amongst clusters. In particular, a bipartite graph G = (V , W ) is used by the HBGF method, where V = V X ∪ V C is the set of vertices corresponding to data points (V X ) and clusters (V C ). The weight of edge w ij ∈ W between vertices v i , v j ∈ V X or that of edge w pq connecting v p , v q ∈ V C is zero. On the other hand, the weight of edge w ip connecting vertices v i ∈ V X and v p ∈ V C can be obtained from the BA matrix.</p><formula xml:id="formula_84">w ip = BA(x i , C p ), x i ∈ X , C p ∈ Π (<label>57</label></formula><formula xml:id="formula_85">)</formula><p>This graph is undirected such that w ip is equivalent to w pi . The spectral graph partitioning algorithm of Ng et al. <ref type="bibr" target="#b94">[95]</ref> and METIS are exploited to obtain the final clustering from this graph. Weighted Similarity Partitioning Algorithm (WSPA): This cluster ensemble technique is developed as the by-product of a new soft-subspace clustering model <ref type="bibr" target="#b42">[43]</ref>. It creates a BA-alike information matrix, WDM, from which the final clustering can be effectively determined. Unlike the conventional BA in which each entry is determined by the underlying label assignment, an entry WDM(x i , cl) is estimated from the distance between data point x i ∈ X and centre of the cluster cl ∈ Π. For each base clustering π g ∈ Π where π g = {C g 1 , . . . , C g kg }, the value of WDM(x i , cl), ∀cl ∈ π g can be defined by</p><formula xml:id="formula_86">WDM(x i , cl) = D i -d(x i , cl) + 1 k g D i + k g - ∑ ∀cl ′ ∈πg d(x i , cl ′ ) ,<label>(58)</label></formula><p>where k g denotes the number of clusters in the base clustering π g ∈ Π and d(x i , cl) is the distance between data point x i and cl, that is centre (or centroid) of the cluster cl. In addition, D i can be specified as</p><formula xml:id="formula_87">D i = max ∀cl∈πg d(x i , cl)<label>(59)</label></formula><p>According to Domeniconi and Al-Razgan <ref type="bibr" target="#b42">[43]</ref>, the distance d(x i , cl) can be defined as follows, where D is the number of attributes, w cl,s ∈ [0, 1] is the weight of the sth attribute that is specific to the cluster cl ∈ π g , x i,s denotes value of the sth attribute of data x i , and cl s denotes the sth attribute value of the cluster centre cl.</p><formula xml:id="formula_88">d(x i , cl) =    √ D ∑ s=1 w cl,s (x i,s -cl s ) 2<label>(60)</label></formula><p>For any cl ∈ π g ,</p><formula xml:id="formula_89">D ∑ s=1 w cl,s = 1 (<label>61</label></formula><formula xml:id="formula_90">)</formula><p>The set of cluster-specific weights is systematically obtained from a so-called 'soft subspace clustering' technique such as LAC (Locally Adaptive Clustering; <ref type="bibr" target="#b95">[96]</ref>). This method extends the conventional k-means by iteratively revising cluster-specific attribute weights that allow more compact clusters to be obtained. Let X = {x 1 , . . . , x N } be a set of data points and each object</p><formula xml:id="formula_91">x i = (x i,1 , . . . , x i,D ), i = 1 . . . N is characterized by a set of attribute F = {f 1 , . . . , f D }. LAC searches for the partition π = {C 1 , . . . , C k } of X into k clusters that minimizes the following objective function. J(U, Z , W ) = k ∑ l=1 D ∑ s=1 [ w l,s O l,s + h w l,s log w l,s ] ,<label>(62)</label></formula><p>where</p><formula xml:id="formula_92">k ∑ l=1 u i,l = 1 (<label>63</label></formula><formula xml:id="formula_93">)</formula><p>and U ∈ R N×K is a matrix in which each entry u i,l represents a membership degree that data point x i ∈ X has with cluster C l ∈ π (u i,l ∈ {0, 1} and u i,l ∈ [0, 1] for crisp and soft clustering, respectively). In addition, Z = {z 1 , . . . , z k } denotes a vectors representing the centroids of k clusters, |C l | is the cardinality of the cluster C l , with O l,s being defined by the following.</p><formula xml:id="formula_94">O l,s = 1 |C l | ∑ ∀x i ∈C l (x i,s -z l,s ) 2 ,<label>(64)</label></formula><p>while h ≥ 0 is the constant that controls the relative differences between dimension weights. In each iteration of the k-means alike process, W is updated by</p><formula xml:id="formula_95">w l,s = exp ( -O l,s h ) ∑ D t=1 exp ( -O l,t h )<label>(65)</label></formula><p>In the study of Domeniconi and Al-Razgan <ref type="bibr" target="#b42">[43]</ref>, the resulting WDM matrix is used to design the graph-based ensemble methods of WSPA (Weighted Similarity Partitioning Algorithm) and WBPA (Weighted Bipartite Partitioning Algorithm). Particularly to WSPA, it creates an N ×N pairwise-similarity matrix S g from a given WDM, for each clustering π g , g = 1 . . . M where π g = {C g 1 , . . . , C g kg }. Let P g i be a vector of entries in the WDM matrix that corresponds to data point x i ∈ X and clusters in π g .</p><formula xml:id="formula_96">P g i = ( WDM(x i , C g 1 ), . . . , WDM(x i , C g kg ) )<label>(66)</label></formula><p>Each entry S g (x i , x j ) that represents the similarity between data points x i , x j ∈ X can be estimated by the following cosine measure:</p><formula xml:id="formula_97">S g (x i , x j ) = P g i P g j ∥P g i ∥∥P g j ∥ ,<label>(67)</label></formula><p>where</p><formula xml:id="formula_98">∥P g i ∥ is estimated by √ WDM(x i , C g 1 ) 2 + • • • + WDM(x i , C g kg ) 2<label>(68)</label></formula><p>For an ensemble of M clustering, the overall similarity measures is presented by the matrix S, which can be specified as</p><formula xml:id="formula_99">S = 1 M M ∑ g=1 S g (<label>69</label></formula><formula xml:id="formula_100">)</formula><p>Similar to CSPA, this similarity matrix, S, is transformed to a weighted graph, which is later partitioned into K clusters using METIS <ref type="bibr" target="#b91">[92]</ref>.</p><p>Weighted Bipartite Partitioning Algorithm (WBPA): Unlike the WSPA method, WBPA transforms the underlying WDM matrix to a bipartite graph, which is partitioned into clusters using spectral graph partitioning (SPEC; <ref type="bibr" target="#b94">[95]</ref>) or METIS. Following the representation scheme used by HBGF, the bipartite graph G = (V , W ) consists of the set of vertices V = V X ∪V C corresponding to data points (V X ) and clusters (V C ), and the set of weighted edges W . The weight of edge w ij ∈ W between vertices v i , v j ∈ V X or w pq between v p , v q ∈ V C is zero, whilst the weight of edge w ip connecting vertices v i ∈ V X and v p ∈ V C can be obtained directly from the WDM matrix, i.e., w ip = WDM(x i , C p ), x i ∈ X , C p ∈ Π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connected-Triple Similarity (CTS) Algorithm:</head><p>To enhance the performance of CSPA <ref type="bibr" target="#b44">[45]</ref> that makes use of the conventional CO matrix, the link-based algorithms of Connected-Triple Similarity (CTS) and SimRank-based Similarity (SRS) have been introduced to refine the evaluation of similarity measures among data samples <ref type="bibr" target="#b53">[54]</ref>. Despite its simplicity, the CO matrix fails drastically to handle a large number of 'unknown' relations, each of which is presented with '0'. This information matrix can expose only a small proportion of pairwise similarity between data points, which may be better discovered by bringing in additional information regarding relations between clusters in an ensemble. As a result, S CTS and S SRS matrices are established with substantially less unknown entries, as compared to the CO counterpart.</p><p>Specifically to the CTS method, the similarity between clusters in ensemble Π are assessed from the weighted graph G = (V , W ), where V is the set of vertices each representing a cluster and W is a set of weighted edges between clusters. Formally, the weight assigned to the edge w pq ∈ W , that connects clusters C p , C q ∈ V , is estimated by</p><formula xml:id="formula_101">w pq = |L p ∩ L q | |L p ∪ L q | ,<label>(70)</label></formula><p>where L p ⊂ X denotes the set of samples belonging to cluster C p ∈ V .</p><p>Given this network formalism, the Weighted Connected-Triples (WCT) measure is employed to disclose the similarity between any pair of clusters. It extends the Connected-Triple method of Reuther and Walter <ref type="bibr" target="#b96">[97]</ref> that has been originally developed to identify ambiguous author names within publication databases. In particular, the similarity of any C p , C q ∈ V can be estimated by counting the number of Connected-Triples (i.e. triples) they are part of. Formally, a triple, Triple = (V Triple , W Triple ), is a subgraph of G containing three vertices V Triple = {C p , C q , C o } ⊂ V and two nonzero edges W Triple = {w po , w qo } ⊂ W , with w pq = 0. This simple counting is sufficient for any indivisible object, e.g. sample, but becomes inappropriate for clusters, i.e., a set of samples. As a result, the WCT measure of clusters C p , C q ∈ V with respect to each triple C o ∈ V , is estimated as WCT o pq = min(w po , w qo ), <ref type="bibr" target="#b70">(71)</ref> where w po , w qo ∈ W are weights of the edges connecting clusters C p and C o , and clusters C q and C o , respectively. The count of all triples (1 . . . α) between clusters C p and C q can be calculated as follows: <ref type="bibr" target="#b71">(72)</ref> Then, the similarity between clusters C p and C q can be estimated by</p><formula xml:id="formula_102">WCT pq = α ∑ o=1 WCT o pq</formula><formula xml:id="formula_103">Sim WCT (C p , C q ) = WCT pq WCT max × DC ,<label>(73)</label></formula><p>where WCT max is the maximum WCT st value of any two clusters C s , C t ∈ V and DC ∈ [0, 1] is a constant decay factor (i.e. confidence level of accepting two non-identical clusters as being similar).</p><p>Following that, the S CTS matrix is generated as follows. For each ensemble member π g , g = 1 . . . M, the similarity between samples x i , x j ∈ X is estimated as</p><formula xml:id="formula_104">S g (x i , x j ) = { 1 if C g (x i ) = C g (x j ) Sim WCT (C g (x i ), C g (x j )) otherwise (74)</formula><p>Each entry in the S CTS matrix can be computed by</p><formula xml:id="formula_105">S CTS (x i , x j ) = 1 M M ∑ g=1 S g (x i , x j )<label>(75)</label></formula><p>Similar to CSPA, the similarity matrix, S CTS , is transformed to the weighted graph, from which the final clustering result π * is generated using METIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SimRank-based Similarity (SRS) Algorithm:</head><p>Besides considering a cluster ensemble as a network of clusters only (as for the CTS algorithm), the SRS method <ref type="bibr" target="#b53">[54]</ref> utilizes a bipartite graph representation and SimRank measure <ref type="bibr" target="#b97">[98]</ref> to reveal hidden relations.</p><p>Given a cluster ensemble Π, the bipartite graph G = (V , W ) can be constructed, where V = V X ∪ V C is a set of vertices representing both data samples (V X ) and clusters (V C ) in the ensemble, and W denotes a set of edges between samples and the clusters to which they are assigned. In particular, the weight of edge w ip connecting sample x i ∈ X and cluster C p ∈ Π is 1 if x i ∈ C p , 0 otherwise. Let S SRS ∈ R N×N and S ′ SRS ∈ R P×P be the pairwise similarity matrices amongst N samples and P clusters, respectively. An entry S SRS (x i , x j ) that represents the similarity between samples x i , x j ∈ X equals to 1 if x i = x j , otherwise</p><formula xml:id="formula_106">S SRS (x i , x j ) = DC |N x i ||N x j | ∑ ∀Cp∈Nx i ∑ ∀Cq∈Nx j S ′ SRS (C p , C q ),<label>(76)</label></formula><p>where DC ∈ [0, 1] is a constant decay factor and N x i ⊂ V C denotes the set of cluster vertices connecting to the sample vertex</p><formula xml:id="formula_107">x i ∈ V X , i.e. w ip = 1, ∀C p ∈ N x i .</formula><p>Likewise, any entry S ′</p><formula xml:id="formula_108">SRS (C p , C q ) is 1 if C p = C q , otherwise S ′ SRS (C p , C q ) = DC |N Cp ||N Cq | ∑ ∀x∈N Cp ∑ ∀x ′ ∈N Cq S SRS (x, x ′ ),<label>(77)</label></formula><p>In fact, both S SRS and S ′ SRS matrices can be correctly achieved through the iterative refinement process. In particular to the S SRS matrix,</p><formula xml:id="formula_109">lim r→∞ S SRSr (x i , x j ) = S SRS (x i , x j ) (78)</formula><p>Let S SRSr (x i , x j ) be a similarity degree between x i , x j ∈ X at the rth iteration, the estimation of the similarity score at the next iteration r + 1 is defined as</p><formula xml:id="formula_110">S SRS r+1 (x i , x j ) = DC |N x i ||N x j | ∑ Cp∈Nx i ∑ Cq∈Nx j S ′ SRSr (C p , C q ) (79)</formula><p>Note that, initially, S SRS 0 (x i , x j ) = 1 if x i = x j and 0 otherwise. This updating procedure is applicable to the case of S ′ SRS matrix, where S ′ SRS 0 (C p , C q ) = 1 if C p = C q , else 0. Once the similarity matrix S SRS is obtained, it is transformed to the weighted graph that is similar to those used by CSPA and CTS techniques. Again, METIS is exploited to partition this graph into the final clustering result.</p><p>Link-based Cluster Ensembles (LCE) Algorithm: To improve the efficiency of previous link-based methods (CTS and SRS) to cluster ensemble problem, LCE <ref type="bibr" target="#b19">[20]</ref> focuses on refining the BA matrix that is less expensive to build than the pairwise similarity alternative. It extends the HBGF method that is based on information presented in the conventional BA matrix, where each entry BA(x i , C p ) ∈ {0, 1} represents a 'crisp' association degree between sample x i ∈ X and cluster C p ∈ Π. Similar to the case of CO matrix, a large number of entries in the BA are 'unknown', each presented with '0'. These hidden or unknown associations can be estimated upon the similarity amongst clusters, discovered from a link network of clusters.</p><p>In particular, the refined cluster-association (RA) matrix is put forward as the enhanced variation of the original BA. Its aim is to approximate value of unknown associations ('0') from known ones ('1'), whose association degrees are preserved within the RA. In other words,</p><formula xml:id="formula_111">BA(x i , C p ) = 1 → RA(x i , C p ) = 1 (80)</formula><p>For each clustering π g , g = 1 . . . M and their corresponding clusters C g 1 , . . . , C g kg (where k g is the number of clusters in the clustering π g ), the association degree RA(x i , cl) ∈ [0, 1] that sample x i ∈ X has with each cluster cl ∈ {C g 1 , . . . , C g kt } is estimated as follows:</p><formula xml:id="formula_112">RA(x i , cl) = { 1 if cl = C g (x i ) Sim(cl, C g (x i )) otherwise (<label>81</label></formula><formula xml:id="formula_113">)</formula><p>where C g (x i ) is a cluster label (corresponding to a particular cluster of the clustering π g ) to which the sample x i belongs. In addition, Sim(C p , C q ) ∈ [0, 1] denotes the similarity between clusters C p , C q ∈ Π, which can be discovered using the Weighted Connected-Triples algorithm (see Eqs. ( <ref type="formula" target="#formula_101">70</ref>)-( <ref type="formula" target="#formula_103">73</ref>), for details).</p><p>Having obtained the RA matrix, a graph-based partitioning method is exploited to obtain the final clustering. Similar to HBGF, this consensus function requires the underlying matrix to be initially transformed into a weighted bipartite graph G = (V , W ), where V = V X ∪ V C is the set of vertices corresponding to samples (V X ) and clusters (V C ). The weight of edge w ij ∈ W between v i , v j ∈ V X or that of edge w pq between v p , v j ∈ V C is zero. On the other hand, the weight of edge w ip connecting vertices v i ∈ V X and v p ∈ V C can be obtained from the RA matrix, i.e. w ip = RA(x i , C p ), x i ∈ X , C p ∈ Π. The spectral graph partitioning algorithm <ref type="bibr" target="#b94">[95]</ref> is finally applied to G to acquire π * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Recent extensions and applications</head><p>Soon after 2010, a large number of research studies have published new concepts and findings related to several issues of cluster ensemble. Some introduce theoretical improvement and extensions to the previous approaches to ensemble generation, representation and consensus clustering. Others focus on the application side, where existing methods are exploited for real problems and different data-mining tasks. The section is to provide details and a useful insight of these exciting developments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Theoretical improvement and extensions</head><p>The following three subsections explore the literature for new developments of conceptual components within cluster ensemble, which commonly aim to promote the quality of final clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Ensemble generation</head><p>It is known that the goodness of the ensemble decision is highly subjected to both diversity within the ensemble and accuracy of those ensemble members. Also, Fern and Lin <ref type="bibr" target="#b98">[99]</ref> have recommended to form a smaller but better-performing cluster ensemble with a subset of members, than using all primary alternatives. In addition to the collection of general approaches to ensemble generation discussed in Section 2, this part provides details of more up-to-date methods to reach the aforementioned goal.</p><p>• Validity index based generation: Similar to the study of Fern and Lin <ref type="bibr" target="#b98">[99]</ref>, Alizadeh et al. <ref type="bibr" target="#b99">[100]</ref> introduce an ensemble clustering framework, which makes use of a subset of initial members in the ensemble, instead of employing all like before. As such, the quality metric of Normalized Mutual Information or NMI is exploited for the determination of these target clusterings. Of course, setting an appropriate NMI threshold is data dependent and requires domain knowledge. About the same time, Zhang et al. <ref type="bibr" target="#b100">[101]</ref> make use of Adjusted Rand Index (ARI) to control the formation of cluster ensemble. In particular, this classical validity metric is generalized to new measures of ARImp and ARImm. The former compares the similarity between base clusterings and the consensus matrix that summarizes the entire ensemble, while the other computes the similarity between any pair of primary partitions. The NMI metric is also exploited by Parvin and Minaei-Bidgoli <ref type="bibr" target="#b101">[102]</ref> to determine a good subset of base clusterings, which are initially generated using the weighted locally adaptive clustering (WLAC) algorithm. Following that, a new asymmetric criterion named Alizadeh-Parvin-Moshki-Minaei (APMM) has been brought forward as the alternative to NMI to control the process of ensemble selection <ref type="bibr" target="#b102">[103]</ref>. Likewise, the measure of cluster stability and dataset simplicity are coupled to assess the quality of subsets of base partitions <ref type="bibr" target="#b103">[104]</ref>.</p><p>In addition to the aforementioned, the comparative study of Naldi et al. <ref type="bibr" target="#b104">[105]</ref> reports the use of different relative clustering validity indexes to select ensemble members. A major finding reveals that each index can be more suitable for a specific data conformation. As such, a combination of distinct relative indexes is proposed based on the intuition that the majority of indices may compensate the poor performance caused by some within the group. Another approach called Cluster Ensemble Selection (CES) is recently proposed to identify good clusterings that should be parts of the desirable ensemble <ref type="bibr" target="#b105">[106]</ref>. A collection of pre-generated clusterings are represented as a multiplex network, in which slices are formed based on clustering dissimilarity indices. Provided this, a community detection algorithm is deployed to deliver communities in the aforementioned slices. Then, for each community, select the best clustering with respect to quality and diversity indexes. These are finally combined to form the target ensemble.</p><p>• Heuristic based generation: One of the recent extensions model the ensemble generation based on a concept called The Wisdom of Crowds <ref type="bibr" target="#b106">[107]</ref>. It is a phenomenon founded in social science that suggests criteria applicable to group behaviour. Intuitively, with these criteria being satisfied, the group decisions may often be better than those of individual members. As a result, Wisdom of Crowds Cluster Ensemble (WOCCE) is introduced with the capability to analyse conditions necessary for an ensemble to exhibit its collective wisdom. These include decentralization condition for generating base clusterings, independence condition among base algorithms, and diversity condition within the ensemble. Besides, Jia et al. <ref type="bibr" target="#b107">[108]</ref> also implement a new selection strategy based on the rule of nearest neighbours. Their method called SELective Spectral Clustering Ensemble (SELSCE) promotes the diversity through random scaling parameter, Nyström approximation and random initialization of k-means. Before the application of neighbour-based heuristic, the set of primary decisions are filtered using a measure integrating diversity and quality.</p><p>• Hierarchical clustering based selection: Since the relationship between diversity and quality is uncertain, Akbari et al. <ref type="bibr" target="#b108">[109]</ref> has proposed the Hierarchical Cluster Ensemble Selection (HCES) method and the diversity measure to determine the effect of diversity and quality on final results. In particular, HCES employs single-linkage, average-linkage, and complete-linkage agglomerative techniques to select members hierarchically. It is reported that the proposed diversity metric leads to more diverse members than that of the pairwise diversity counterpart. This claim is supported with empirical studies on two benchmark ensemble methods of CSPA and HGPA.</p><p>• Soft clustering based generation: Parvin and Minaei-Bidgoli <ref type="bibr" target="#b109">[110]</ref> is one among several researchers that approach the ensemble generation through fuzzy clustering. With Fuzzy Weighted Locally Adaptive Clustering (FWLAC) algorithm, it is possible to produce a diverse and accurate ensemble using the weighting scheme for differentiating informative and uninformative features. Specific to the problem of tumour clustering, Yu et al. <ref type="bibr" target="#b110">[111]</ref> propose the random double clustering based fuzzy cluster ensemble framework (RDCFCE). It first creates a set of representative features using a randomly selected clustering algorithm in the ensemble. Then, data points are assigned to appropriate clusters based on the grouping results. These assignments are turned into a fuzzy consensus matrix, from which the final decision is obtained using the normalized cut algorithm.</p><p>As the core part of granular computing, the rough set theory that deals with dealing with uncertain or vague information, has also been applied for the problem of cluster ensemble <ref type="bibr" target="#b111">[112]</ref>. A work published last year by Hu et al. <ref type="bibr" target="#b112">[113]</ref> demonstrates such an idea, where a hierarchical cluster ensemble model based on knowledge granulation is proposed with a novel rough distance to measure the dissimilarity between base partitions.</p><p>• Model-initialization based generation: For an homogeneous ensemble like that of k-means members, model initialization plays a crucial part in producing diversity. Wu et al. <ref type="bibr" target="#b113">[114]</ref> has suggested a number of desired conditions for K-means-based consensus clustering (KCC), including the criteria for initialization. Another interesting concept of co-initialization <ref type="bibr" target="#b114">[115]</ref> has been investigated, with the results suggesting that the quality of clusterings can often be improved when a set of diverse clustering techniques provides initializations for each other.</p><p>• Re-sampling: The initial work of Fern and Lin <ref type="bibr" target="#b98">[99]</ref> has been generalized by the SELective Spectral Clustering Ensemble (SELSCE) method <ref type="bibr" target="#b115">[116]</ref>. Primary components are first created using spectral clustering (SC), with Nyström approximation to perturb the results of SC. Then, these base decisions are manipulated through the bagging process, which is usually applied in supervised learning. At last, the components are ranked by aggregating multiple NMI or ARI values, which have been obtained from random comparisons between individual components and the consensus matrix. Similarly inspired by bagging and boosting algorithms in classification, other studies by Parvin et al. <ref type="bibr" target="#b116">[117]</ref> and Minaei-Bidgoli et al. <ref type="bibr" target="#b117">[118]</ref> examines the non-weighing and weighing-based sampling approaches to ensemble generation. And recently, this line of research has continued to cover a new framework called Weighted-Object Ensemble Clustering (WOEC) with the co-association matrix being employed to represent the ensemble information <ref type="bibr" target="#b118">[119]</ref>.</p><p>In addition, a novel cluster generation method based on random sampling or RS-NN is introduced, where the nearest neighbour strategy is adopted to fill the category information of the missing samples <ref type="bibr" target="#b119">[120]</ref>. According to the evaluation against a typical random projection method (Random Feature Subset or FS) and another random sampling method (Random Sampling based on Nearest Centroid or RS-NC), it is found that RS-NN is able to produce base clusterings with a good balance between quality and diversity, thus achieving significant improvement over the counterparts. Note that FS usually generates more diverse partitions, while RS-NC delivers high-quality partitions. Yang and Jiang <ref type="bibr" target="#b120">[121]</ref> propose a novel hybrid sampling-based clustering ensemble by combining the strengths of boosting and bagging. The base partitions are iteratively created via a hybrid process exhibiting characteristics of both boosting and bagging.</p><p>• Re-using feature selection/transformation techniques: It is also possible to view clustering solutions as features, such that existing feature selection algorithms can be employed to selection a subset of primary features (or solutions). With this in mind, Yu et al. <ref type="bibr" target="#b121">[122]</ref> propose a hybrid clustering solution selection strategy (HCSS) to aggregate different feature selection techniques for identifying the suitable subset of ensemble members. Similar to this work, the use of data transformation operators has also been investigated <ref type="bibr" target="#b122">[123]</ref>. In particular, two new data transformation operators are developed to create new datasets in the ensemble. These are known as probabilistic based data sampling operator and probabilistic based attribute sampling operator. Following that, three new random transformation models are proposed, including the random combination of transformation operators in the data dimension, in the attribute dimension, and in both dimensions, respectively.</p><p>• Multiple distance functions and pruning: Yu et al. <ref type="bibr" target="#b123">[124]</ref> introduced a cluster ensemble framework named as AP2CE, which is claimed to be noise immune. This is feasible with the use of affinity propagation algorithm (AP) and multiple distance functions. In that, a set of new data matrices is produced with respect to the subspaces consists of representative attributes obtained by AP. In order to enhance the quality of ensemble, diversity is increased through removing the redundant base partitions <ref type="bibr" target="#b124">[125]</ref>. The significance of attribute founded in rough set theory is adopted as a heuristic to select the subset of ensemble members.</p><p>• Multiple data modalities: Specific to biomedical data analysis, a new method called Complementary ensemble clustering (CEC) is presented as an weighted extension of coassociation or CO matrix <ref type="bibr" target="#b125">[126]</ref>. In that, base partitions are obtained from separate clusterings of different data modalities, e.g., text and images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Representation and summarization of multiple clusterings</head><p>In addition to the generation aspect, there have been several studies devoting to the topic of representing and summarizing base partitions. These include:</p><p>• String representation: A collection of new cluster ensemble methods design the problem of combining primary partitions as an optimization process (see the next section on consensus functions for more details). As such, the information among ensemble members is formulated in 0-1 bit strings. With this terminology, Alizadeth et al. <ref type="bibr" target="#b126">[127]</ref> introduce a constrained nonlinear objective function called fuzzy string objective function (FSOF), to search for a median partition. This is achieved by maximizing the agreement between the ensemble members and minimizes the disagreement at the same time.</p><p>• Tree representation: In the attempt to improve the problem with time and memory complexity of CO based methods, Wang <ref type="bibr" target="#b127">[128]</ref> proposes a hierarchical structure called a coassociation tree or CA-tree, which is built using the base cluster labels. At a given threshold, the corresponding cut of this tree creates a preliminary partition of the data into disjoint groups or pre-clusters. Then, the CO matrix is derived from the representatives of these groups.</p><p>• Graph representation: Du et al. <ref type="bibr" target="#b128">[129]</ref> argued that existing approaches to represent cluster ensemble is either by multiple co-association pairwise relations or cluster based features. Given this background, a unified framework is put forward to integrate the two representation schemes by means of weighted graph regularized nonnegative matrix factorization. It is also reported that such a combination outperforms both of the individuals, with respect to clustering accuracy and stability. Another work by Huang et al. <ref type="bibr" target="#b129">[130]</ref> propose a new approach named as ensemble clustering using factor graph (ECFG). In that, the concept of superobject is founded as a compact and adaptive representation for the ensemble data. Based on probabilistic modelling, the problem with approximated data is formulated as a binary linear programming (BLP) problem. In order to solve this optimization, an efficient solver based on factor graph is established.</p><p>• Data fragments: Instead of applying the ensemble clustering to the entire data points, it is feasible to separately analyse data fragments that represent subsets of the original data <ref type="bibr" target="#b130">[131]</ref>. Of course, this help scaling up the model to large datasets. The concept of clustering aggregation or AGG is reused to generate the final results from base partitions of these fragments. It is reported with empirical results that the proposed approach is more efficient than the existing AGG methods (Agglomerative, Furthest, and LocalSearch), without sacrificing the accuracy. See the study of Chung and Dai <ref type="bibr" target="#b131">[132]</ref> for a recent extension.</p><p>• Relation and link-based representation: Wang et al. <ref type="bibr" target="#b132">[133]</ref> invent the framework for coupled clustering ensembles (CCE) to overcome the problem of explicating the dependency between base partitions and between data points. In fact, it integrates the two coupling relationships, which are presented as the intra-coupling within one base clustering and the inter-coupling between different base clusterings. Besides this invention, the following two works extend the concept of link-based formation introduced by LCE approach <ref type="bibr" target="#b133">[134]</ref>. The first introduces a new method termed WETU that is capable of refining the data cluster association matrix with a link-based similarity measure. Unlike LCE, the matrix is acquired from the similarity of clusters among all base clusterings, not from any specific one. As such, WETU can provide more discriminative information than the original counterpart. The other proposes the use of crowd agreement estimation and multi-granularity link analysis to improve the quality of cluster ensemble <ref type="bibr" target="#b134">[135]</ref>. At the outset, base partitions are weighted using the normalized crowd agreement index (NCAI). Following that, the relationship between clusters is explored with the application of source aware connected triple (SACT) similarity, which encodes information regarding common neighbours and the source reliability. Based on these, two novel consensus clusterings are provided, weighted evidence accumulation clustering (WEAC) and graph partitioning with multi-granularity link analysis (GP-MGLA), respectively.</p><p>• Extensions to CO matrix: Several attempts have been made to extend the representation and application of the so-called CO matrix. Some of these can be summarized in this heading. Duarte et al. <ref type="bibr" target="#b135">[136]</ref> make use of the information represented in CO matrix to determine degrees of confidence associated with data points. These confidence values dictate the likelihood of data points being included in the consensus clustering process. Another work by Lourenco et al. <ref type="bibr" target="#b136">[137]</ref> identifies the fact that a differentiation among the base partitions can lead to improved quality of the consensus clustering. In particular, the framework of CO matrix is modified to implement a weighting mechanism that represents the importance of different ensemble members. With a similar intuition, Ren et al. <ref type="bibr" target="#b137">[138]</ref> also present the Weighted-Object Ensemble Clustering (WOEC) method, which embed the weights associated to data points in the conventional CO matrix. These weights are initially obtained through the application of boosting during the clustering process. After that, another approach to representing weights in the CO matrix is introduced via the notion of competence, which reflects the quality of different algorithms used to create base members <ref type="bibr" target="#b138">[139]</ref>. The efficiency of this method is demonstrated with Monte-Carlo modelling.</p><p>A path-based approach is developed to refine the CO matrix such that distinct contributions of individual data points and base partitions to the ensemble can be represented <ref type="bibr" target="#b139">[140]</ref>. The path-based similarity allows more global information of the cluster structure to be incorporated into the matrix, from which the final clustering is generated using spectral clustering. Built upon the paradigm of CO matrix, Louren et al. <ref type="bibr" target="#b140">[141]</ref> devise the new probabilistic approach to assign data points to clusters. This is achieved via minimizing a Bregman divergence between the observed co-association frequencies and the corresponding co-occurrence probabilities expressed as functions of the unknown assignments. Unlike the works described so far, another attempt by Liu et al. <ref type="bibr" target="#b141">[142]</ref> focuses on the efficiency aspect of CO matrix based solution. As a result, an efficient Spectral Ensemble Clustering method is proposed, where it is theoretical equivalent to weighted k-means process, thus vastly reducing the algorithmic complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Consensus clustering</head><p>According to the four general categories of cluster ensemble approaches, this part will provide details of consensus functions recently developed in the literature.</p><p>• Direct approach: In contrary to the conventional voting method that is compatible with hard base clusterings, Wang et al. <ref type="bibr" target="#b142">[143]</ref> propose an alternative to aggregate soft partitions. It is called Soft-Voting Clustering Ensemble (SVCE), which provides better flexibility and generalization than the hard counterpart. Similarly, Sevillano et al. <ref type="bibr" target="#b143">[144]</ref> introduces a new consensus function to consolidate the outcomes of multiple fuzzy clusterings into a single fuzzy partition. This is achieved through the application of positional and confidence voting techniques. Another work by Zhang et al. <ref type="bibr" target="#b144">[145]</ref> focuses on the re-labelling process within the twolayer clustering framework.</p><p>Several recent extensions represent the problem of consensus clustering as an optimization or search for the median partition. For that, a framework to learn a low-rank matrix via optimization is examined <ref type="bibr" target="#b145">[146]</ref>, with a block coordinate descent algorithm being employed to solve the problem. Li et al. <ref type="bibr" target="#b146">[147]</ref> make use of simulated annealing method named BV-RSA to solve the problem of ensemble clustering. In addition, intracluster criteria such as Minimum-Sum-of-Squares-Clustering (MSSC) is also exploited to formulate the objective function <ref type="bibr" target="#b147">[148]</ref>. Later, Chatterjee and Mukhopadhyay <ref type="bibr" target="#b148">[149]</ref> models this as a multiobjective optimization problem and a multiobjective evolutionary algorithm (MOE-CEA). The final clustering is generated from input partitions by optimizing two criteria simultaneously. One is to maximize the similarity of the resultant clustering with all the input clusterings. The other minimizes the standard deviation among the similarity scores. This can help to prevent the evolving solution to be very similar with one of the input clusterings. Besides, Gullo et al. <ref type="bibr" target="#b149">[150]</ref> extends the concept of Projective Clustering Ensemble (PCE), where a single-objective formulation is effective to allow both sample-based and feature-based cluster representations to be jointly considered.</p><p>Franek and Jiang <ref type="bibr" target="#b150">[151]</ref> reduce the complexity of cluster ensemble to the well-known Euclidean median problem. This is solved by the Weiszfeld algorithm and an inverse transformation that maps the Euclidean median back into the clustering domain. Besides, Bhatnagar et al. <ref type="bibr" target="#b151">[152]</ref> claim to obtain robust clustering using discriminant analysis. It kicks off with re-labelling input partitions using the Hungarian algorithm, followed by applying discriminant analysis to construct of a label matrix. At last, clustering scheme is refined to deliver robust and stable outcome. Along this line of research, some studies complement the aforementioned with the reduction of search space. One of these attempts to find the best subspace to derive the consensus partition <ref type="bibr" target="#b152">[153]</ref>. In addition, Vega-Pons and Avesani <ref type="bibr" target="#b153">[154]</ref> introduce a new pruning technique that allows a dramatic reduction of the search space.</p><p>• Feature based approach: At first, Lock and Dunson <ref type="bibr" target="#b154">[155]</ref> propose the BCC (Bayesian consensus clustering) as an integrative statistical model, which combines input clusterings of the objects from different data sources. This is applied to the problem of identifying subtypes of breast cancer tumour samples, based on public data from The Cancer Genome Atlas. Following that, the Gaussian mixture model based cluster structure ensemble framework (GMMSE) is presented as a novel probabilistic approach <ref type="bibr" target="#b155">[156]</ref>. In particular, GMMSE employs a number of Gaussian mixture models to capture cluster structures embedded in the data. Through the process of Expectation Maximization (EM), components of the Gaussian mixture models are estimated and then viewed as new data samples. These are used to create the matrix representing the relations among components. In that, the Bhattacharyya distance function is used to calculate the similarity between two components corresponding to their respective Gaussian distributions. Lastly, GMMSE builds a graph to represent new data samples and the aforementioned matrix, and looks for the most representative cluster structure.</p><p>In addition to the existing works belonging to this category, the theory of belief functions is introduced to the problem of cluster ensemble <ref type="bibr" target="#b156">[157]</ref>. A number of belief functions can be defined on the lattice of interval partitions of samples to represent degree of confidence. Provided this, the consensus belief function is obtained using a suitable combination rule. Likewise, Wu et al. <ref type="bibr" target="#b157">[158]</ref> introduce a new approach that utilizes Dempster-Shafer (DS) evidence theory and Gaussian Mixture Modelling (GMM) technique to combine the base partitions. Another group of new consensus methods concentrates on developing a new proximity metric that can be effective at ensemble-level for summarizing similarities among samples. See the studies of Zheng et al. <ref type="bibr" target="#b158">[159]</ref> and Aidos and Fred <ref type="bibr" target="#b159">[160]</ref> for examples.</p><p>• Pairwise similarity based approach: One of the most exciting works that adopt concepts invented in other fields is the model termed Cluster Forests or CF <ref type="bibr" target="#b160">[161]</ref>. Implied by the name, it is inspired by the success of Random Forests (RF) in the context of classification. CF aims to obtain good local partitions through randomly probing a high-dimensional data. Based on a cluster quality measure kappa, CF gradually obtains improved local clustering in a manner that resembles RF tree growth. Another work on Dual-Similarity Clustering Ensemble (DSCE) initially establishes core clusters based on similarity among objects, then clusters may be merged in accordance with their member-based similarity. Besides these, there have been a vast amount of applications of CO matrix or pairwise similarity scheme (see the next section for more details).</p><p>• Graph based approach: In the work of Xiao et al. <ref type="bibr" target="#b161">[162]</ref>, an ensemble is created using multiple trials of CHAMELEON, with a CO matrix being employed to summarize ensemble information. Then, the matrix is modelled as a similarity graph, to which METIS is applied to acquire p sub-graphs, where p ≫ k. After that, these are combined along the process of hierarchical clustering, to get the final clustering. A similar model has also been introduced by Mimaroglu and Erdil <ref type="bibr" target="#b162">[163]</ref>, with a specific advantage of obtaining the number of clusters in the final partition automatically. Based on the terminology and application for image segmentation, Abdala et al. <ref type="bibr" target="#b163">[164]</ref> has adapted a random walker (RW) algorithm to work with cluster ensembles. It first generates a graph representation of the ensemble, from which the similarity between objects can be inferred using the RW technique.</p><p>• Other new approaches: Apart from the extensions belonging to the four conventional families, there are still several new approaches worth mentioning here. One of these is presented as the Gravitational ensemble clustering (GEC) method, which is designed to aggregate the results obtained from weak algorithms like k-means <ref type="bibr" target="#b164">[165]</ref>. Moreover, Du et al. <ref type="bibr" target="#b165">[166]</ref> propose a self-supervised learning framework for the problem of cluster ensemble. In particular, base partitions are treated as pseudoclass labels, each of which a classifier can be learned. With this, the relationships between these input partitions can be exhibited by adding priors to the parameters of the corresponding classifiers. Yet another concept called enhanced splitting merging awareness tactics (E-SMART) is employed specifically to determine the appropriate number of clusters, which remains a major problem to many state-of-the-art consensus clustering methods <ref type="bibr" target="#b166">[167]</ref>.</p><p>In 2016, Teng et al. <ref type="bibr" target="#b167">[168]</ref> publish the work on a cluster ensemble framework based on the group method of data handling (CE-GMDH), which consists of three components of an initial solution, a transfer function and an external criterion, respectively. Provided this, a number of models can be formulated using different types of transfer functions and external criteria. Examples of the transfer function include least squares and semidefinite programming.</p><p>In the context of image segmentation, Ammour and Alajlan <ref type="bibr" target="#b168">[169]</ref> make use of an hybrid cluster ensemble, in which ensemble members are created using fuzzy c-means and fuzzy local information c-means algorithms with different parameter settings. The consensus clustering is performed by the ordered weighted averaging (OWA) method that is normally exploited for group-based decision making. A similar idea of engaging aggregation operators to combine multiple hierarchical clusterings has also been recently reported by Rashedi et al. <ref type="bibr" target="#b169">[170]</ref>. In particular, desired properties of different aggregators for hierarchical clustering have been elaborated and assessed. This study is motivated by the initial finding that weighted combination of hierarchical clusterings perform better than other combination methods, e.g., averaging <ref type="bibr" target="#b170">[171]</ref>. Again, with the goal of combining base hierarchical clusterings, Rashedi et al. <ref type="bibr" target="#b171">[172]</ref> make use of Renyi and Jensen-Shannon Divergences as the measures to shape the aggregation of data matrices, each representing an input hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Applications of cluster ensembles</head><p>In addition to the extensions elaborated earlier, this section looks into different applications of cluster ensemble, with respect to two viewpoints. The first part explores the applications to specific problem domains such as time series analysis. The second emphasizes the use of cluster ensemble for other data-mining tasks, e.g., transfer learning and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Specific problem domains</head><p>Since 2010, various applications of cluster ensembles have been implemented and deployed in different domains, including a number of interesting areas such as transportation and cybersecurity. These can be categorized as follows.</p><p>• Transportation: Since the security issues of High Speed Train (HST) have recently been the centre of attention, a cluster ensemble method called CECH algorithm <ref type="bibr" target="#b161">[162]</ref> is used for the diagnosis of running gear faults. The study is based on vibration data collected by sensors that reflect the operation condition. In addition, Fiori et al. <ref type="bibr" target="#b172">[173]</ref> also apply the concept of consensus clustering to disclose the transportation network knowledge. The method called DeCoClu (Density Consensus Clustering) is introduced to analyse GPS data to infer geographical locations of stops and other information captured by the vehicles during their work.</p><p>• Time series analysis: The analysis of time series becomes another important area of research with numerous applications. Particularly to manufacturing, ability to recognize slide alterations is needed as indicative of a malfunction. It is known that manual monitoring can be tedious, yet require experts' undistracted attention. Hence, an automated alternative called control chart pattern recognition (CCPR) model has been proposed with the use of consensus clustering <ref type="bibr" target="#b173">[174]</ref>. In addition, Ramasso et al. <ref type="bibr" target="#b174">[175]</ref> introduce a cluster ensemble approach for unsupervised pattern recognition in acoustic emission (AE) time-series issued from composite materials. It is able to emphasize sudden growths of damages in composites under solicitations. Furthermore, a HMM-based partitioning ensemble is proposed for temporal data clustering <ref type="bibr" target="#b175">[176]</ref>. The resulting model provides several benefits such as: (i) the model initialization problem can be solved through the ensemble setting; (ii) the appropriate number of clusters is automatically determined; and (iii) no parameter re-estimation is required for a pair of clusters to be merged, which helps the HMM agglomerative clustering to be much more efficient.</p><p>• Image processing and computer vision: The task of image segmentation is the initial and one of the most critical stage in image analysis. There exist various segmentation techniques each of which naturally requires an optimal setting of parameters. In practice, this is achieved by an application of supervised parameter learning to derive the desired setting. On the other hand, a new research direction leads to the combination of different segmentations into a final consensus solution. To reach the goal, the methodology of cluster ensemble is exploited to aggregate the results of different segmentation algorithms and parameter settings <ref type="bibr" target="#b176">[177]</ref>. Similar study by <ref type="bibr">Kim et al. [178]</ref> has investigated the use of hierarchical segmentation ensemble clustering for the partitioning of images into foreground and background regions. In addition to the aforementioned attempts, Wang et al. <ref type="bibr" target="#b178">[179]</ref> also introduce a cluster ensemble-based image segmentation algorithm, which overcomes several problems of traditional methods. In particular, the ensemble framework is exploited to fuse the segmentation results from different types of visual features. As a result, it can deliver a better final result and achieve a much more stable performance for broad image categories.</p><p>Along this line of research, Akbarizadeh and Rahmani <ref type="bibr" target="#b179">[180]</ref> report the study that integrates spectral clustering and Gabor feature clustering, which can lead to improved segmentation results. Specific to the task of segmenting a satellite image, a hybridization of fuzzy-based cluster ensemble and a supervised learning technique like support vector machine (SVM) is developed to improve the accuracy <ref type="bibr" target="#b180">[181]</ref>. Firstly, multiple partitions are generated using a fuzzy clustering technique. These solutions are then separately improved by a classifier-directed process, and finally combined to form the final data partition. Another interesting application of cluster ensemble to shape decomposition is reported by Lewin et al. <ref type="bibr" target="#b181">[182]</ref>. Moreover, some researchers develop and utilize visual words (i.e., vectorquantized local descriptors) for category-level object and activity recognition. These vocabularies are frequently built by using a local feature such as SIFT and a single clustering algorithm. It is possible to lift the quality of visual recognition by aggregating heterogeneous codebooks via consensus clustering <ref type="bibr" target="#b182">[183]</ref>. This idea has been investigated with the problems of identifying objects and scenes in very challenging datasets.</p><p>• Biometrics: In the research of Lourenco et al. <ref type="bibr" target="#b183">[184]</ref>, Electrocardiography (ECG) that has typically been employed for patient monitoring, is investigated as a biometrics trait. The EAC framework of consensus clustering adopted for the analysis of ECG signals in the context of ECG-based biometrics.</p><p>• Voice processing: With a rapid increase in the volume of recorded speech, e.g., television and audio broadcasting, meeting recordings and voice mails; a growing need for automatically processing of such repository has arisen. However, attempts to content organization, navigation, browsing, and search have been constrained by the data size. One approach to tackle this is speaker segmentation and speaker clustering, where cluster ensembles have proven effective <ref type="bibr" target="#b184">[185]</ref>.</p><p>• Chemoinformatics: Research related to chemoinformatics aims to obtain chemical knowledge through representation and organization of chemical data. It is commonly employed for drug discovery and design, especially the process of High-Throughput Screening (HTS) that screens available compounds for useful information. A consensus clustering method is exploited to reduce cost and time for this screening <ref type="bibr" target="#b185">[186]</ref>. As such, it leads to the selection of a representative subset of all the compounds, with the chance of producing redundant information being minimized. Following the previous attempt, Saeed et al. <ref type="bibr" target="#b186">[187]</ref> has introduced the information theory and voting based algorithm (Adaptive Cumulative Voting-based Aggregation Algorithm A-CVAA) for the analysis of chemical structures. This is assessed MDL Drug Data Report (MDDR) and Maximum Unbiased Validation (MUV) datasets, based on the ability to separate active from inactive molecules in each cluster <ref type="bibr" target="#b187">[188]</ref>.</p><p>• Ontology: An automatic ontology alignment tool performs the matching between concepts belonging to two ontologies. In that, it provides a similarity measure for each pair of the aligned concepts. Despite the development for this issue, none of the existing alternatives is absolutely accurate, with different tools generating distinct similarity values for a specific alignment. Instead of throwing away the results of some methods that seem less appropriate, Chowdhury and Dou <ref type="bibr" target="#b188">[189]</ref> propose an ensemble moel of ontology alignment that aggregates multiple alignment outcomes.</p><p>• Text mining: Document clustering is used in the context of text mining to set groups of similar documents. A specific model called Gravitational Ensemble Clustering (GEC) is introduced for this task <ref type="bibr" target="#b189">[190]</ref>. With a similar objective, Costa and Ortale <ref type="bibr" target="#b190">[191]</ref> also exploits a cluster ensemble method for the partitioning of XML corpus. This allows the inherently difficult problem of catching structural content relationships among XML documents into a number of simpler subproblems, whose results will be combined to form the final solution. Another application of consensus clustering to the field of text mining is to improving the quality of subtopic retrieval <ref type="bibr" target="#b191">[192]</ref>.</p><p>• Emotion recognition: A method that is capable of automatically detecting a person's emotion state is in great demand for human-machine interaction and other fields like psychology and psychiatry. For this purpose Aidos et al. <ref type="bibr" target="#b192">[193]</ref> put forward a voting-based approach of cluster ensemble to analyse a dataset containing EEG signals from subjects who performed a stress-inducing task. In particular, the study focuses on six different feature spaces obtained from band power features and phase-locking factors.</p><p>• Remote sensing: For an attempt to develop a weatherwise classification system, Mahrooghy et al. <ref type="bibr" target="#b193">[194]</ref> introduces High resolution Satellite Precipitation Estimation (SPE), which is based on the Precipitation Estimation from Remotely Sensed Imagery using an Artificial Neural Network Cloud Classification (PERSIANN-CCS) framework. This model consists of four steps: (i) segmentation of infrared cloud images into patches; (ii) extracting features from cloud patches; (iii) clustering cloud patches using the consensus clustering method of LCE; and (4) deriving interpretation through dynamic application of brightness temperature and rain-rate relationships, respectively. Besides this work, another investigation has recently proposed a sampling based approximate spectral clustering ensemble (SASCE) for unsupervised land cover identification using large remote sensing images <ref type="bibr" target="#b194">[195]</ref>. To be efficient with large datasets, a simple voting approach is implemented for the generation of final clustering. For agricultural and environmental monitoring, cluster analysis of high spatial resolution remote-sensing images exhibits a crucial role in land-cover identification. To this end, Tasdemir et al. <ref type="bibr" target="#b195">[196]</ref> has developed an approximate spectral-clustering ensemble (ASCE2) to fuse data partitions acquired by image clustering with different similarity representations.</p><p>• Geospatial data analysis: Despite the fact that geospatial clustering emerges as one of the important topics in spatial analysis, existing techniques still analyse only at data level without taking into account domain knowledge as well as users' goals. Regarding the limitation, Gu et al. <ref type="bibr" target="#b196">[197]</ref> has invented an ontology-based geospatial cluster ensemble method to generate good clustering results.</p><p>• Bioinformatics: For successful diagnosis and treatment of cancer, discovering cancer types accurately becomes essential. The difficulty arises as gene expression profiles normally possess a large number of genes, with many are noisy. In order to overcome this, two new consensus clustering frameworks, named as triple spectral clustering-based consensus clustering (SC3) and double spectral clusteringbased consensus clustering (SC2Ncut), are proposed <ref type="bibr" target="#b197">[198]</ref>. Apart from the analysis of microarray data, the task of detecting protein complexes from protein-protein interaction (PPI) networks is challenging in the field of bioinformatics. In spite of a vast number of computational methods developed for this course, almost all concentrate on a single aspect of the PPI network, hence the limited collection of features for cluster analysis. To overcome such as deficit, a Bayesian Nonnegative Matrix Factorization (NMF)-based cluster ensemble method is used to aggregate clustering results, which are derived from features of different PPI aspects <ref type="bibr" target="#b198">[199]</ref>. Another work by Wang et al. <ref type="bibr" target="#b199">[200]</ref> has demonstrated the use of consensus clustering for determining the subtype for a breast cancer patient, through the integration of multiple modalities of data. These range from genotypes to multiple levels of phenotypes.</p><p>Besides, Lock and Dunson <ref type="bibr" target="#b154">[155]</ref> report the work on an integrative ensemble model that fuses separate clusterings of the objects for each data source. It makes use of a Bayesian framework for simultaneous estimation of both the consensus clustering and the source-specific clusterings. This is evaluated with the task of identifying subtype of breast cancer tumour samples using publicly available data from the Cancer Genome Atlas.</p><p>A semi-supervised consensus clustering algorithm has also been implemented for electrocardiography (ECG) pathology classification <ref type="bibr" target="#b200">[201]</ref>. Yang et al. <ref type="bibr" target="#b201">[202]</ref> presents a specific use of cluster ensemble in the context of microbial community responses to human habitats. This is a significant task, as establishing baselines of human microbiome is essential in understanding its role in human disease and health. The study investigates a microbial similarity network that integrates 1920 metagenomic samples from three body habitats.</p><p>• Environment and natural resources: Provided the global concern of water scarcity, a large number of hydrology researchers have worked on forecasting of water quantity and quality, as well as regionalization of river basins. As such, the need to enhance the quality of prediction of yield in river basins arises. In response, Ahuja <ref type="bibr" target="#b202">[203]</ref> publishes research findings based on the data of Godavari basin, which is regionalized using a cluster ensemble method. The method of consensus clustering is also implemented for characterizing flow patterns in soils <ref type="bibr" target="#b203">[204]</ref>. It is known that the quality of both surface water and groundwater is directly subjected to flow paths in the vadose zone. This leads to studies that aim to visualize flow patterns in soils. In general, it requires image classification of stained and non-stained parts and the calculation of the dye coverage, which can be interpreted against depth to determine flow types.</p><p>• Cybersecurity: Along the advance development of Internet technology and applications, the subject of cybersecurity has gained an enormous impact on both information integrity and privacy. One of the emerging threats encountered around the globe is malware that has been induced by the extensive use of mobile communication. With the aim to detect this harmful program, Ye et al. <ref type="bibr" target="#b204">[205]</ref> propose the Automatic Malware Categorization System (AMCS) that automatically groups malware samples into sets that share some common characteristics using a cluster ensemble. The underlying analysis is based on features related to instruction frequency and function-based instruction sequences. Following that, an ensemble clustering system called DUET is introduced for the same task <ref type="bibr" target="#b205">[206]</ref>, providing a learning platform to combine static instruction features and dynamic behaviour features.</p><p>In fact, determining class boundaries of overlapping malware families is a difficult goal to accomplish. As a response, Hou et al. <ref type="bibr" target="#b206">[207]</ref> create an intelligent malware detection system that can resolve this using cluster-oriented ensemble classifiers. It is evaluated with Windows Application Programming Interface (API) calls extracted from the file samples. In comparison with malware incidents, phishing website fraud is a relatively new threat. However, shared properties exist: (i) as driven by economic benefits, both malware samples and phishing websites are created at a rate of thousands per day; and (ii) phishing websites represented by the term frequencies possess similar characteristics with malware samples represented by the instruction frequencies. An example of using cluster ensemble for phishing website detection is given in the work of Zhuang et al. <ref type="bibr" target="#b207">[208]</ref>, where domain knowledge in the form of website-level constraints can be naturally incorporated into the ensemble framework.</p><p>• Network analysis: Revealing the modules in complex networks is significant to the understanding of systems. An ensemble clustering method is employed to incorporate node groupings of various sizes, with sequential removal of weak links between nodes that are rarely grouped together <ref type="bibr" target="#b208">[209]</ref>. It has been successfully applied to several cases, e.g., hierarchical random networks and the American college football network, each with known modular structures. Moreover, Lancichinetti and Fortunato <ref type="bibr" target="#b209">[210]</ref> make use of consensus clustering to study the community structure of complex networks. This can help to reveal organization of the discovered communities and hidden relationships among their constituents.</p><p>• Business process management: With respect to the study of Zhao et al. <ref type="bibr" target="#b210">[211]</ref>, resource allocation has been regarded as a multi-criteria decision problem, which can be solved by a clustering ensemble approach. This is obtained through the analysis of resource characteristics and task preference patterns from the previous process executions. As such, the right resources may well be recommended, thus improving resource utility.</p><p>• Cloud computing: As size and complexity of cloud infrastructure increase, scalability has become troublesome for process monitoring and management. This is the case as all virtual machines (VMs) are separately treated, thus producing huge amounts of data to handle. The problem can be tackled by leveraging the similarity between VMs with respect to resource usage patterns <ref type="bibr" target="#b211">[212]</ref>. For that, a cluster ensemble framework is created to group similar VMs, without knowledge of the software active in these sessions.</p><p>• Smart living: A methodology of cluster ensemble has also been investigated for activity monitoring systems, which incorporate sensor-based technology within the smart living scheme <ref type="bibr" target="#b212">[213]</ref>. In particular, activities are designed as groups or clusters built on different subsets of extracted features. To classify a new incident, it is assigned to the cluster with the smallest proximity measure from the one under examination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Application to other data mining tasks</head><p>Despite the fact that cluster ensemble has been established for unsupervised learning, the method has recently been exploited for other tasks related to data analysis. These include the followings.</p><p>• Transfer learning: Conventional supervised learning usually assumes that both training and test data are from a common distribution. Thus a challenge arises in transfer learning, where training and test distributions may be mismatched. The problem is even worse when the test data is actually from a different domain and without labels. In order to resolve this, Acharya et al. <ref type="bibr" target="#b213">[214]</ref> introduce an optimization framework, which takes as input one or more classifiers learned on the source domain as well as the results of a cluster ensemble operating solely on the target domain, and yields a consensus labelling of the data in the target domain.</p><p>• Detecting ambiguity in data: Another novel application scheme of cluster ensemble is to identify uncertain or ambiguous regions in the data under examination <ref type="bibr" target="#b214">[215]</ref>. Following the detection, two approaches have been suggested for the treatment of such uncertainty. Firstly, the simplest way is to ignore ambiguous patterns prior to the consensus clustering, thus preserving the non-ambiguous data as good prototypes for any further modelling. The other alternative is to use the ensemble solution obtained by the first to train a supervised model that is later applied to reallocate the ambiguous clusters.</p><p>• Dimensionality reduction: With large amounts of data being generated in various domains such as bioinformatics and social networks, dimensionality reduction remains a challenging task for data-mining researchers. The concept of cluster ensemble is recently exploited for this problem with the use of genetic algorithm <ref type="bibr" target="#b215">[216]</ref>. Based on the validation with conventional classification methods and benchmark data collections, its performance is promising with the accuracy on par with the latest approaches proposed in the literature.</p><p>• Semi-supervised learning: For the analysis of gene expression data, Wang and Pan <ref type="bibr" target="#b216">[217]</ref> introduce semisupervised consensus clustering (SSCC) that integrate the LCE model <ref type="bibr" target="#b19">[20]</ref> with semi-supervised clustering process. The clustering quality can be improved when prior knowledge (in terms of must-link and cannot-link constraints) is provided in addition to a typical proximity metric. This study follows the line of research initially brought about by Yu et al. <ref type="bibr" target="#b217">[218]</ref> and Yang et al. <ref type="bibr" target="#b218">[219]</ref>. As for the former, a new cluster ensemble method named knowledge based cluster ensemble (KCE) is proposed where prior knowledge of data is included into the cluster ensemble framework. Specific to this, pairwise constraints among data points are encoded as confidence factors between base clusterings. Later, these will be concluded in the form of consensus matrix from which the final result is generated. In the latter, an improved Cop-Kmeans (ICop-Kmeans) algorithm has been put forward to tackle the violation of pairwise constraints usually encountered with the original Cop-Kmeans model. Likewise, Zhang et al. <ref type="bibr" target="#b219">[220]</ref> and Yu et al. <ref type="bibr" target="#b220">[221]</ref> contribute to this subject by proposing the semi-supervised clustering ensemble model based on collaborative training (SCET) and the incremental semi-supervised clustering ensemble framework (ISSCE), respectively.</p><p>• Data classification: In spite of the difference between unsupervised and supervised learning, the use of cluster analysis in classification tasks has shown to be effective to raise the classification accuracy <ref type="bibr" target="#b221">[222]</ref>. This is pretty much with the fact that data clusters can provide supplementary constraints that may yield the generalization capability of a classifier. In the work of Nguyen et al. <ref type="bibr" target="#b222">[223]</ref>, the use of clustering information in addition to the original data attributes has been reported to improve the accuracy of intrusion detection problem. Two other works of Sang-Woon <ref type="bibr" target="#b223">[224]</ref> and Nasierding et al. <ref type="bibr" target="#b224">[225]</ref> have combined cluster labels and conventional supervised algorithms for face recognition and image annotation, respectively.</p><p>Following these, Iam-On and Boongoen <ref type="bibr" target="#b225">[226]</ref> present an investigation of employing the information of cluster ensemble for classification modelling. In particular, the ensemble-information matrix created by link-based ensemble clustering or LCE <ref type="bibr" target="#b19">[20]</ref> is evaluated as the transformed data for classifier development. In that, the refined samplecluster association matrix can be considered as the representation of samples in the transformed space, which is discovered from multiple clusterings in the setting of original features. Having accomplished this, the initial data dimensions are reduced to a set of cluster labels, with which each sample associates to a certain degree. Given the common conclusion that a combination of multiple classifiers is able to increase classification accuracy, a new classifier combination scheme is proposed based on the Decision Templates Combiner <ref type="bibr" target="#b221">[222]</ref>. It represents the classifiers decision as a vector in an intermediate feature space, then creates decision templates using cluster ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Challenges and conclusion</head><p>This survey has presented classical and recently developed approaches to cluster ensemble. It kicks off with the formal terminology by which the problem is defined. Four basic categories of consensus clustering methods are then discussed in depth with illustrative examples. After that, it provides details of extensions to three main components of a cluster ensemble framework: ensemble generation, representation and summarization, and consensus function, respectively. Given the superior capability to deliver accurate data partitions, many cluster ensemble techniques have been exploited for a wide range of applications and domain problems. In addition, the use of this meta-learning approach for other data-mining tasks such as classification has been studied. The attention received by this subject has consistently increased over the years, especially after 2010 that is the focus of this survey. Based on the statistics shown in Fig. <ref type="figure" target="#fig_6">4</ref>, the numbers of Google scholar search results for ''cluster ensemble'' or ''consensus clustering'' are 1240, 1660 and 2800 for the periods of 2011-12, 2013-14 and 2015-16, respectively. It is clearly illustrated that these counts are much higher than those belonging to the intervals before 2010.</p><p>The aforementioned observation is greatly due to the maturity of basic practice to cluster ensemble and a flourish of its applications. From the early period with most of the works relating only to bioinformatics, especially microarray data analysis, the application landscape has largely expanded over the past few years. It covers several new challenges to the modern age such as cybersecurity and time-series data analysis. The followings summarize potential challenges of cluster ensemble in the near future.</p><p>• Heterogeneous data analysis: Despite the long history of development, most of cluster ensemble methods have been directed to numerical data analysis. Only a handful of publications report findings with other types of data. Specific to the work of Iam-On et al. <ref type="bibr" target="#b226">[227]</ref>, the link-based method or LCE is adopted for the clustering of biological samples. Each of these can be expressed by both continuous variables extracted from microarray data, and nominal variables obtained from clinical or pathological data of the samples under examination. This so-called integrative approach to biological data analysis has shown to improve the accuracy of prognostic outcome, as compared to those obtained by using one of the aforementioned factors alone. However, given the fact that the aforementioned model is based simply on k-prototype algorithm, its performance is highly subjected to parameter setting (i.e., weights given to continuous and nominal variables).</p><p>A gap of improvement in terms of clustering quality and model robustness exists especially for implementing new inventions of mixed-type data clustering in the ensemble context. For instance, Blomstedt et al. <ref type="bibr" target="#b227">[228]</ref> recently introduce a model-based algorithm for clustering attributes of mixed type, which is based a Bayesian predictive framework. Provided that clustering solutions represent random data partitions, the posterior probability for a partition can be determined using conjugate analysis. Another approach applies unsupervised feature learning (UFL) to mixed-type data in order to acquire a sparse representation. As a result, it becomes easier for clustering algorithms to disclose data partitions <ref type="bibr" target="#b228">[229]</ref>. While conventional UFL techniques are designed for homogeneous data, the aforementioned works with the mixed-type data using fuzzy adaptive resonance theory (ART). In the biomedical domain, Abidin and Westhead <ref type="bibr" target="#b229">[230]</ref> also point out the need for accurate cluster analysis of mixed type data. This commonly appears as a mixture of binary or nominal data (e.g. presence of mutations, binding and epigenetic marks) and continuous data (e.g. gene expression and metabolite levels). As such, a generic clustering method is proposed and evaluated with genetic regulation and the clustering of cancer samples.</p><p>• Big data analysis: Common applications on office and social based platforms have facilitated the vast amount of data being generated daily. Analysing this so-called big data has been a major trend and challenge within the community of data mining. To better appreciate this, see the comparative study of Fahad et al. <ref type="bibr" target="#b230">[231]</ref>, where several classical clustering techniques are assessed against big datasets. In particular to an ensemble model, it may face the problem of scaling up, despite the quality it produces. In response, Su et al. <ref type="bibr" target="#b231">[232]</ref> introduce a novel cluster ensemble approach for fuzzy clustering, especially for big data. It first builds fuzzy base clusters with respect to each data feature. Then, it makes use of a fuzzy hierarchical graph to represent relationships between those base clusters. Based on this representation scheme, the final result is generated using hierarchical clustering as the consensus function. This work follows an initial attempt to mitigate the practice of cluster ensemble to large data <ref type="bibr" target="#b232">[233]</ref>. In that, ECCA (Ensemble of Combined Clustering Algorithms) is invented as a framework of ensemble of algorithms with fixed uniform grids. The final collective solution is based on pairwise classification of the elements of the grid structure. Another study attempts to deal with the curse of dimensionality in big data, especially for cluster ensemble <ref type="bibr" target="#b233">[234]</ref>. In particular, a new fuzzy c-means (FCM) algorithm with random projection has been created as the basis of novel consensus clustering, which scales linearly with data size. This is achieved through calculating spectral embedding of data with cluster-centre based representation.</p><p>Analysing the big data has become a major challenge, especially to those web-based organizations such as Google and Facebook. They commonly develop a customized variation of non-relational systems not only to overcome the limitations of efficient storage and retrieval, but also pave the way for data analytics <ref type="bibr" target="#b234">[235]</ref>. Some of the new approaches to analysing big data gain a great deal of attention amongst commercial and academic researchers, e.g., Google's MapReduce framework, Hadoop and Hive. According to the report of Dean and Ghemawat <ref type="bibr" target="#b235">[236]</ref>, MapReduce has been the most popular solution for parallel and analysis of large amount of data. Within the community of data mining, implementations of several techniques using MapReduce have been presented in the past few years. For instance, Liu et al. <ref type="bibr" target="#b236">[237]</ref> introduce a MapReduce based parallel back-propagation neural network (MR-BPNN). As for data clustering, a MapReduce-based artificial bee colony (MR-ABC) is developed for a clustering method similar to kmeans <ref type="bibr" target="#b237">[238]</ref>. This ABC implementation helps to optimize the assignment of the large data objects to clusters. However, for cluster ensemble, such an implementation has rarely been reported in the literature. In fact, one recent publication kicks off this research direction, with the introduction of a new parallel k-means clustering based on MapReduce framework for aspect based summary generation <ref type="bibr" target="#b238">[239]</ref>. Of course, an opportunity to coupling existing cluster ensemble methods with MapReduce or other big-data platforms is obvious. This may further boost its application that is in line with the new challenges encountered by big data scientists.</p><p>• Repository of tools: Ever since its introduction in the early 2000s, the scope of end users of cluster ensemble or consensus clustering is rather limited. As compared to conventional clustering algorithms like k-means or DBSCAN that are available in several well-known data mining tools (e.g., Weka<ref type="foot" target="#foot_1">2</ref> and RapidMiner<ref type="foot" target="#foot_2">3</ref> ), implementations of those ensemble models appear to be harder to obtain. Most of them are provided as a supplementary to the publication, which can disappear over time. Yet, this is typically not user friendly as it has been customized in a specific programming</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The basic process of cluster ensembles. It first applies multiple base clusterings to a dataset X to obtain diverse clustering decisions (π 1 . . . π M ). Then, these solutions are combined to establish the final clustering result (π * ) using a consensus function.</figDesc><graphic coords="5,152.54,54.73,300.07,127.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of (a) cluster ensemble, (b) label-assignment matrix, (c) pairwise similarity matrix and (d) binary cluster-association (BA) matrix. Note that X = {x 1 , . .. , x 5 }, Π = {π 1 , π 2 }, π 1 = {C 1 1 , C 1 2 , C 1 3 } and π 2 = {C 2 1 , C 2 2 }.</figDesc><graphic coords="6,57.77,164.57,90.87,75.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example of (a) contingency matrix Ω and (b) the corresponding weighted bipartite graph.</figDesc><graphic coords="6,337.23,54.73,180.07,89.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2(b) for example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A comparison of Google scholar search results of ''cluster ensemble'' or ''consensus clustering'', over different time intervals from 2001 to 2016.</figDesc><graphic coords="21,152.54,54.73,300.04,175.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Following that, the best set BM of refined centroids is selected from {FM 1 , . . . , FM M }, using the distortion measure Φ:</figDesc><table /><note><p>1 1 , . . . , C 1 K , . . . , C M 1 , . . . , C M K }) as features for the next clustering stage, i.e., clustering clusters. Particularly, each collection FM p , p = 1 . . . M of refined centroids is created by applying k-means to CM using {C p 1 , . . . , C p K } ⊂ CM as the initial K centroids. Note that FM p consists of K refined centroids {F p 1 , . . . , F p K }.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The No Free Lunch theorem seems to apply here because the problem of clustering can be reduced to an optimization problem -we are seeking to find the optimal set of clusters for a given dataset via an algorithm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.cs.waikato.ac.nz/ml/weka/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://rapidminer.com/.environment. As a result, it is also significant to make this family of methods known to a wider public, perhaps as an extension to the well-established tools. This may help broaden the application domain to cover interesting problems in the new era of data intensive industry and society.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is funded by ST/P005594/1 -Newton STFC-NARIT: Using astronomy surveys to train Thai researchers in Big Data analysis.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cluster analysis for gene expression data: A survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1370" to="1386" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data mining application in customer relationship management of credit card business</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on Computer software and applications</title>
		<meeting>international conference on Computer software and applications</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="39" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conceptual clustering in information retrieval</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Deogun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="427" to="436" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Information retrieval by semantic analysis and visualisation of the concept space of D-Lib magazine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tripathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>D-Lib Mag</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cluster analysis using self-organising maps and image processing techniques</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Andrade Netto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Syst. Man Cybern</title>
		<meeting>IEEE Int. Conf. Syst. Man Cybern</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="367" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Color image edge detection using cluster analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="834" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using cluster analysis to improve marketing experiments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Heeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Market. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="340" to="347" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The sequence of factor analysis and cluster analysis: Differences in segmentation and dimensionality through the use of raw and factor scores, Tourism Anal</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Sheppard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cluster analysis in family psychology research</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorman-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Family Psychol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A recommender system using GA K-means clustering in an online shopping market</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1200" to="1209" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Functional network analysis reveals extended gliomagenesis pathway maps and three novel MYC-interacting genes in human gliomas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bredel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bredel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Harsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sikic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="8679" to="8689" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate classification of microarray subtypes using ensemble k-means clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ashlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><surname>Multi-K</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">260</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Repeated observation of breast tumor subtypes in independent gene expression data sets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sorlie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johnsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pesich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="8418" to="8423" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data clustering: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Survey</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A k-mean clustering algorithm for mixed numeric and categorical data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="503" to="527" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Claustering large data sets with mixed numeric and categorical values</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Pacific Asia Knowledge Discovery and Data Mining Conference</title>
		<meeting>the First Pacific Asia Knowledge Discovery and Data Mining Conference</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="21" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A prediction-based resampling method for estimating the number of clusters in a dataset</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dudoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridyand</surname></persName>
		</author>
		<idno>RESEARCH0036</idno>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nearest-neighbour guided evaluation of data reliability and its applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boongoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. B</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1622" to="1633" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LCE: A link-based cluster ensemble method for improved gene expression data analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Iam-On</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boongoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1513" to="1519" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings using evidence accumulation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="835" to="850" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminatively regularized least-squares classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clustering ensembles: Models of consensus and weak partitions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1866" to="1881" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="4" to="37" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A bfast clustering algorithm to cluster very large categorical data sets in data mining</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery</title>
		<meeting>the SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<title level="m">Introduction to Data Mining</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>first ed.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-organized formation of topologically correct feature maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybernet</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the use of self-organizing maps for clustering and visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Flexer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="373" to="384" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Clustering of the self-origanising map</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vesanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alhoniemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="586" to="600" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Som of soms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="463" to="478" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modular network som</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tokunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">No free lunch theorems for search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<idno>SFI-TR-95-02-010</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Santa Fe Institute</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using diversity in cluster ensembles</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Hadjitodorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Systems, Man and Cybernetics</title>
		<meeting>the IEEE International Conference on Systems, Man and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1214" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combining multiple clustering systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Principles and Practice of Knowledge Discovery in Databases</title>
		<meeting>European Conference on Principles and Practice of Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Finding median partitions using informationtheoretical-based genetic algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cristofor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simovici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Univ. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="153" to="172" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Consensus clusterings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining</title>
		<meeting>IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="607" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A mixture model for clustering ensembles</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIAM International Conference on Data Mining</title>
		<meeting>SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="379" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bagging for path-based clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1411" to="1415" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Clustering aggregation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tsaparas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weighted cluster ensembles: Methods and analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Razgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Solving cluster ensemble problems by bipartite graph partitioning</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cluster ensembles: A knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Finding natural clusters using multiclusterer combiner based on shared nearest neighbors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Multiple Classifier Systems</title>
		<meeting>International Workshop on Multiple Classifier Systems</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="166" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Random projection for high dimensional data clustering: A cluster ensemble approach</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Finding consistent clusters in data partitions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Fred</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiple Classifier Systems</title>
		<imprint>
			<biblScope unit="page" from="309" to="318" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Data clustering using evidence accumulation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="276" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust data clustering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="128" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Mesirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="91" to="118" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evaluation of stability of k-means cluster ensembles with respect to random initialization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1798" to="1808" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Refining pairwise similarity matrix for cluster ensemble problem with cluster relations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Iam-On</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boongoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eleventh International Conference on Discovery Science</title>
		<meeting>Eleventh International Conference on Discovery Science</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="222" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Finding Groups in Data: An Introduction to Cluster Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Wiely Publishers</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Moderate diversity for better cluster ensembles</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Hadjitodorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Todorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Fusion</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="264" to="275" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Combining multiple weak clusterings</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining</title>
		<meeting>IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph-based consensus clustering for class discovery from gene expression data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2888" to="2896" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bagging to improve the accuracy of a clustering procedure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dudoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridyand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1090" to="1099" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A comparison of resampling methods for clustering ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Minaei-Bidgoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning: Models, Technologies and Applications</title>
		<meeting>the International Conference on Machine Learning: Models, Technologies and Applications</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="939" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Random projection in dimensionality reduction: Applications to image and text data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fuzzy ensemble clustering based on random projections for DNA microarray data analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Avogadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="173" to="183" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Ensemble clustering method based on the resampling similarity measure for gene expression data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Methods in Medical Research</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cluster ensemble and its applications in gene expression analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asia-Pacific Bioinformatics Conference</title>
		<meeting>Asia-Pacific Bioinformatics Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="297" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multiobjective data clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="424" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning pairwise similarity for data clustering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="925" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An empirical comparison of voting classification algortihms: Bagging, boosting, and variants</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="105" to="139" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Application of majority voting to pattern recognition: An analysis of its behavior and performance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="553" to="568" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Analysis of consensus partition in cluster ensemble</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H C</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Fred</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining</title>
		<meeting>IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. SIAM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Cluster-based cumulative ensembles</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Multiple Classifier Systems</title>
		<meeting>International Workshop on Multiple Classifier Systems</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="236" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Voting-merging: An ensemble method for clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dimitriadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weingessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Neural Networks</title>
		<meeting>International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A combination scheme for fuzzy clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dimitriadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weingessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognit. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="901" to="912" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A multi-clustering fusion algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Frossyniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pertselakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stafylopatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Hellenic Conference on AI</title>
		<meeting>Hellenic Conference on AI</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cumulative voting consensus method for partitions with a variable number of clusters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="160" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On voting-based consensus of cluster ensembles</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1943" to="1953" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings via k-modes algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Advanced Data Mining and Applications</title>
		<meeting>International Conference on Advanced Data Mining and Applications</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="308" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Correlation clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="89" to="113" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A best possible heuristic for the k-center problem</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Shmoys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="180" to="184" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Knowledge acquisition via incremental conceptual clustering</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="139" to="172" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Reinterpreting the category utility function</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mirkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Refining initial points for k-means clustering</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Consensus clustering and functional interpretation of gene-expression data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vinciotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Orengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kellam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Clustering ensemble based on the fuzzy KNN algorithm</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing</title>
		<meeting>International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1001" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Clustering ensembles based on normalized edges</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining</title>
		<meeting>Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="664" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">ROCK: A robust clustering algorithm for categorical attributes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="345" to="366" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bezdek</surname></persName>
		</author>
		<title level="m">Pattern Recognition with Fuzzy Objective Function Algorithms</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Recent convergence results for the fuzzy c-means clustering algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bezdek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hathaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Classification</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="237" to="247" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Multilevel k-way partitioning scheme for irregular graphs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="96" to="129" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A parallel algorithm for multilevel graph-partitioning and sparse matrix ordering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="71" to="95" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Multilevel hypergraph partitioning: Applications in vlsi domain</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. VLSI Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="849" to="856" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Locally adaptive metrics for clustering high-dimensional data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Razgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papadopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="63" to="97" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Survey on test collections and techniques for personal name matching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Reuther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Metadata Semant. Ontol</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="99" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">SimRank: A measure of structural-context similarity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="538" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Cluster ensemble selection</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Anal. Data Mining</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="128" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A framework for cluster ensemble based on a max metric as cluster evaluator</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IAENG Int. J. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Generalized adjusted rand indices for cluster ensembles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2214" to="2226" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">A clustering ensemble framework based on elite selection of weighted clusters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Parvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Minaei-Bidgoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Data Anal. Classif</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="181" to="208" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Cluster ensemble selection based on a new cluster stability measure</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Minaei-Bidgoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="389" to="408" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">To improve the quality of cluster ensembles by selecting a subset of base clusters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Minaei-Bidgoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Experiment. Theoret. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="127" to="150" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Cluster ensemble selection based on relative validity indexes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A multiplex-network based approach for clustering ensemble selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rastin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kanawati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
		<meeting>IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1332" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Wisdom of crowds cluster ensemble</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yousefnezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Bidgoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="485" to="503" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Similarity-based spectral clustering ensemble selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Fuzzy Systems and Knowledge Discovery</title>
		<meeting>International Conference on Fuzzy Systems and Knowledge Discovery</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1071" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Hierarchical cluster ensemble selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Dahlana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ibrahima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="146" to="156" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A clustering ensemble framework based on selection of fuzzy weighted clusters in a locally adaptive clustering algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Parvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Minaei-Bidgoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="87" to="112" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Adaptive fuzzy consensus clustering framework for clustering analysis of cancer data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="887" to="901" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Partially ordered rough ensemble clustering for multi granular representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lingras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Haider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="103" to="116" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Hierarchical cluster ensemble model based on knowledge granulation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">K-means-based consensus clustering: A unified view</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="155" to="169" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Improving cluster analysis by co-initializations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="71" to="77" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Bagging-based spectral clustering ensemble selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1456" to="1467" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Data weighing mechanisms for clustering ensembles</title>
		<author>
			<persName><forename type="first">H</forename><surname>Parvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Minaei-Bidgoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alinejad-Rokny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electr. Eng</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1433" to="1450" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Minaei-Bidgoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alinejad-Rokny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Effects of resampling method and adaptation on clustering ensemble efficacy</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="27" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Demeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Weigted object ensemble clustering: Methods and analysis</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="661" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Exploring the diversity in cluster ensemble generation: Random sampling and random projection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="4844" to="4866" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Hybrid sampling-based clustering ensemble with global and local constitutions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="952" to="965" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Hybrid clustering solution selection strategy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3362" to="3375" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Hybrid cluster ensemble framework based on the random combination of data transformation operators</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1826" to="1837" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Adaptive noise immune cluster ensemble using affinity propagation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3176" to="3189" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Rough set based cluster ensemble selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Information Fusion</title>
		<meeting>International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="438" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Complementary ensemble clustering of biomedical data</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Fodeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krauthammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="436" to="443" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Optimizing fuzzy cluster ensemble in string representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alizadeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Minaei-Bidgoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognit. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Ca-tree: A hierarchical structure for efficient and scalable coassociation-based cluster ensembles</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. B</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1083" to="4419" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Cluster ensembles via weighted graph regularized nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Advanced Data Mining and Applications</title>
		<meeting>International Conference on Advanced Data Mining and Applications</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="215" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Ensemble clustering using factor graph</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Efficient clustering aggregation based on data fragments</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. B</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="913" to="926" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">A fragment-based iterative consensus clustering algorithm with a robust similarity</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inform. Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="591" to="609" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Coupled clustering ensemble: Incorporating coupling relationships both between base clusterings and objects</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Engineering</title>
		<meeting>IEEE International Conference on Data Engineering</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="374" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A link-based approach to the cluster ensemble problem</title>
		<author>
			<persName><forename type="first">N</forename><surname>Iam-On</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boongoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2396" to="2409" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings via crowd agreement estimation and multi-granularity link analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="240" to="250" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Adaptive evidence accumulation clustering using the confidence of the objects&apos; assignments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Duarte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PAKDD</title>
		<meeting>PAKDD</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="70" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Consensus clustering with robust evidence accumulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lourenco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on EMM-CVPR</title>
		<meeting>International Conference on EMM-CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="307" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Weighted-object ensemble clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining</title>
		<meeting>IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Weighted ensemble of algorithms for complex data clustering</title>
		<author>
			<persName><forename type="first">V</forename><surname>Berikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A clustering ensemble: Two-level-refined co-association matrix with path-based transformation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2699" to="2709" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Probabilistic consensus clustering using evidence accumulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Louren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rebagliati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="331" to="357" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="715" to="724" />
		</imprint>
	</monogr>
	<note>Spectral ensemble clustering</note>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Soft-voting clustering ensemble</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Multiple Classifier Systems</title>
		<meeting>International Workshop on Multiple Classifier Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="307" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Positional and confidence voting-based consensus functions for fuzzy cluster ensembles</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sevillano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Socoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Cascaded cluster ensembles</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Cybern</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="335" to="343" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Learning a robust consensus matrix for clustering ensemble via kullback-leibler divergence minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4112" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Bv-rsa: A rapid simulated annealing model for ensemble clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Service Systems and Service Management</title>
		<meeting>International Conference on Service Systems and Service Management</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Coordination of cluster ensembles via exact methods</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Christou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="279" to="293" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Clustering ensemble: A multiobjective genetic algorithm based approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Intelligence: Modeling Techniques and Applications</title>
		<meeting>International Conference on Computational Intelligence: Modeling Techniques and Applications</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="443" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Metacluster-based projective clustering ensembles</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tagarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="181" to="216" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Ensemble clustering by means of clustering embedding in vector spaces</title>
		<author>
			<persName><forename type="first">L</forename><surname>Franek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="833" to="842" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Discriminant analysis-based cluster ensemble</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Data Mining Modell. Manage</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="83" to="107" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Subspace similarity-based algorithm for combine multiple clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Internet Computing for Engineering and Science</title>
		<meeting>International Conference on Internet Computing for Engineering and Science</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">On pruning the search space for clustering ensemble problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vega-Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Avesani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="481" to="489" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Bayesian consensus clustering</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Lock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2610" to="2616" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Probabilistic cluster structure ensemble</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="16" to="34" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Ensemble clustering in the belief functions framework</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Approx. Reason</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="92" to="109" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">A new ensemble clustering method based on dempstershafer evidence theory and gaussian mixture modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">A framework for hierarchical ensemble clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Consensus of clusterings based on high-order dissimilarities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aidos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Partitional Clustering Algorithms</publisher>
			<biblScope unit="page" from="313" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Cluster forests</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Statist. Data Anal</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="178" to="192" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Semi-supervised hierarchical clustering ensemble and its application</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="362" to="1376" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings using similarity graph</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mimaroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="694" to="703" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Abdala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wattuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<title level="m">Proceedings of International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1433" to="1436" />
		</imprint>
	</monogr>
	<note>Ensemble clustering via random walker consensus strategy</note>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Nezamabadi-pour, Gravitational ensemble clustering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">A self-supervised framework for clustering ensemble</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Web-Age Information Management</title>
		<meeting>International Conference on Web-Age Information Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Coce-smart: Consensus clustering based on enhanced splitting-merging awareness tactics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abu-Jamous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2011" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Cluster ensemble framework based on the group method of data handling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="35" to="46" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">A dynamic weights owa fusion for ensemble clustering, Signal Image Video Process</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ammour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alajlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="727" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Optimized aggregation function in hierarchical clustering combination</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Comparing weighted combination of hierarchical clustering based on cophenetic measure</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Vahidipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">An information theoretic approach to hierarchical clustering combination</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="487" to="497" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Decoclu: Density consensus clustering approach for public transport data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mignone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rospo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">328</biblScope>
			<biblScope unit="page" from="378" to="388" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">A robust unsupervised consensus control chart pattern recognition framework</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haghtalab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xanthopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Madani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="6767" to="6776" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Unsupervised consensus clustering of acoustic emission time-series for robust damage sequence estimation in composites</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ramasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Placet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Boubakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="3297" to="3307" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Hmm-based hybrid meta-clustering ensemble for temporal data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yanga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="299" to="310" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Image segmentation fusion using general ensemble clustering methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Franek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Abdala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vega-Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="373" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Image segmentation using consensus from hierarchical segmentation ensembles</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3272" to="3276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Cluster ensemble-based image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adv. Robot. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">297</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">A new ensemble clustering method for polsar image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Akbarizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Information and Knowledge Technology</title>
		<meeting>International Conference on Information and Knowledge Technology</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Svmefc: Svm ensemble fuzzy clustering for satellite image segmentation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Maulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Plewczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sensing Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="52" to="55" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">A clustering-based ensemble technique for shape decomposition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lewin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clausing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition</title>
		<meeting>Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Heterogeneous visual codebook integration via consensus clustering for visual categorization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Lopez-Sastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Renes-Olalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gil-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado-Bascon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafuente-Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1358" to="1368" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Ecg analysis using consensus clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lourenco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carreiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Signal Processing</title>
		<meeting>European Conference on Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="511" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Speaker diarization exploiting the eigengap criterion and cluster ensembles</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bassiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Moschou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Language Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2134" to="2144" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Voting-based consensus clustering for combining multiple clusterings of chemical structures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cheminform</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Information theory and voting based consensus clustering for combining multiple clusterings of chemical structures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mole. Inform</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="591" to="598" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Weighted voting-based consensus clustering for chemical structure databases</title>
		<author>
			<persName><forename type="first">F</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shamsir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Salim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Aided Mol. Des</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="675" to="684" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Improving the accuracy of ontology alignment through ensemble fuzzy clustering</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Distributed Objects, Middleware and Applications</title>
		<meeting>International Symposium on Distributed Objects, Middleware and Applications</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="826" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Nezamabadi-pour, Document clustering using gravitational ensemble clustering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Artificial Intelligence and Signal Processing</title>
		<meeting>International Symposium on Artificial Intelligence and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="240" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Developments in partitioning xml documents by content and structure based on combining multiple clusterings</title>
		<author>
			<persName><forename type="first">G</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ortale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Tools with Artificial Intelligence</title>
		<meeting>IEEE International Conference on Tools with Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="477" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Consensus clustering based on a new probabilistic rand index with application to subtopic retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2315" to="2326" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Evidence accumulation approach applied to eeg analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aidos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carreiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="479" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">On the use of a cluster ensemble cloud classification technique in satellite precipitation estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahrooghy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Younan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Anantharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aanstoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yarahmadian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1356" to="1363" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Sampling based approximate spectral clustering ensemble for unsupervised land cover identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Moazzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yalcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tasdemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Geoscience and Remote Sensing</title>
		<meeting>IEEE International Symposium on Geoscience and Remote Sensing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2405" to="2408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">An approximate spectral clustering ensemble for high spatial resolution remote-sensing images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tasdemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moazzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yildirim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="1996">2015. 1996-2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Use of ontology and cluster ensembles for geospatial clustering analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Canadian Conference on Artificial Intelligence</title>
		<meeting>Canadian Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="119" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Sc3: Triple spectral clustering-based consensus clustering framework for class discovery from cancer gene expression profiles</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinform</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1751" to="1765" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Protein complex detection via weighted ensemble clustering based on bayesian nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">62158</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Breast cancer patient stratification using a molecular regularized consensus clustering method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="304" to="312" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Semi-supervised consensus clustering for ecg pathology classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aidos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lourenco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fred</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="150" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Microbial community pattern detection in human body habitats via ensemble clustering framework</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Regionalization of river basins using cluster ensemble</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Water Resour. Protect</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="560" to="566" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Characterising flow patterns in soils by feature extraction and multiple consensus clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bognera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Widemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecol. Inform</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="44" to="52" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Automatic malware categorization using cluster ensemble</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Duet: integration of dynamic and static analyses for malware clustering with cluster ensembles</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Conference on Computer Security Applications</title>
		<meeting>Annual Conference on Computer Security Applications</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Cluster-oriented ensemble classifiers for intelligent malware detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Demihovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Semantic Computing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Ensemble clustering for internet security applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. C</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1784" to="1796" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Multiscale ensemble clustering for finding modules in complex networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">U</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">26119</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Consensus clustering in complex networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lancichinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">336</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">An entropy-based clustering ensemble method to support resource allocation in business process management</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inform. Syst</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="305" to="330" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Exploiting ensemble techniques for automatic virtual machine clustering in cloud systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Canali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lancellotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automat. Softw. Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="319" to="344" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Clustering-based ensemble learning for activity recognition in smart homes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="12285" to="12304" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Transfer learning with cluster ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Acharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Unsupervised and Transfer Learning</title>
		<meeting>Workshop on Unsupervised and Transfer Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">On ambiguity detection and postprocessing schemes using cluster ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Albalate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suchindranath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Soenmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suendermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Agents and Artificial Intelligence</title>
		<meeting>International Conference on Agents and Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Consensus clustering for dimensionality reduction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhavani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Contemporary Computing</title>
		<meeting>International Conference on Contemporary Computing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="148" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Pan, Semi-supervised consensus clustering for gene expression data analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC BioData Mining</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Knowledge based cluster ensemble for cancer discovery from biomolecular data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. NanoBiosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="76" to="85" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Consensus clustering based on constrained selforganizing map and improved cop-kmeans ensemble in intelligent decision support systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Semi-supervised clustering ensemble based on collaborative training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Rough Sets and Knowledge Technology</title>
		<meeting>International Conference on Rough Sets and Knowledge Technology</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="450" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Incremental semi-supervised clustering ensemble for high dimensional data clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="701" to="714" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">A new classifier combination scheme using clustering ensemble</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Duval-Poo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sosa-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guerra-Gandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vega-Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz-Shulcloper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Iberoamerican Congress on Pattern Recognition</title>
		<meeting>Iberoamerican Congress on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">An efficient fuzzy clustering-based approach for intrusion detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Darmont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining</title>
		<meeting>IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="607" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">A pre-clustering technique for optimizing subclass discriminant analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sang-Woon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="462" to="468" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Clustering based multi-label classification for image annotation and retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nasierding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Kouzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on System, Man and Cybernetics</title>
		<meeting>IEEE International Conference on System, Man and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="4514" to="4519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Revisiting link-based cluster ensembles for microarray data classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Iam-On</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boongoen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Systems, Man and Cybernetics</title>
		<meeting>IEEE Systems, Man and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="4543" to="4548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">New cluster ensemble approach to integrative biological data analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Iam-On</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boongoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Data Mining Bioinform</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="150" to="168" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">A bayesian predictive model for clustering data of mixed discrete and continuous type</title>
		<author>
			<persName><forename type="first">P</forename><surname>Blomstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Granlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="489" to="498" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Clustering data of mixed categorical and numerical type with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1605" to="1613" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Flexible model-based clustering of mixed binary and continuous data: Application to genetic regulation and cancer</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N Z</forename><surname>Abidin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Westhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res. gkw</title>
		<imprint>
			<biblScope unit="volume">1270</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">A survey of clustering algorithms for big data: Taxonomy and empirical analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fahad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alshatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Foufou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Top. Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="267" to="279" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">A hierarchical fuzzy cluster ensemble approach and its application to big data clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2409" to="2421" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Ensemble of clustering algorithms for large datasets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Pestunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Berikovb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Kulikovaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rylova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optoelectron. Instrum. Data Process</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="45" to="252" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Fuzzy c-means and cluster ensemble with random projection for big data clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Probl. Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Channeling the data deluge</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Swedlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zanetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Best</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="463" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Mapreduce: A flexible data processing tool</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Mapreduce based parallel neural networks in enabling large scale machine learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell. Neurosci</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">A mapreduce-based artificial bee colony for large-scale data clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banharnsakun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="78" to="84" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Ensemble based parallel k means using map reduce for aspect based summarization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Priya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Umamaheswari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Informatics and Analytics</title>
		<meeting>the International Conference on Informatics and Analytics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
