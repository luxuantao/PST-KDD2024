<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing</title>
				<funder>
					<orgName type="full">GSRC</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel University Research O ce</orgName>
				</funder>
				<funder>
					<orgName type="full">Oracle</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel Science and Technology Center on Cloud Computing</orgName>
				</funder>
				<funder>
					<orgName type="full">Samsung</orgName>
				</funder>
				<funder>
					<orgName type="full">AMD</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
							<email>vseshadr@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
							<email>michael.a.kozuch@intel.com</email>
						</author>
						<author>
							<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>B.3.2 [Design Styles]: Cache memories Design</term>
					<term>Experimentation</term>
					<term>Measurement</term>
					<term>Performance Caching</term>
					<term>Memory</term>
					<term>Pollution</term>
					<term>Thrashing</term>
					<term>Insertion policy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>O -chip main memory has long been a bottleneck for system performance. With increasing memory pressure due to multiple onchip cores, e ective cache utilization is important. In a system with limited cache space, we would ideally like to prevent 1) cache pollution, i.e., blocks with low reuse evicting blocks with high reuse from the cache, and 2) cache thrashing, i.e., blocks with high reuse evicting each other from the cache.</p><p>In this paper, we propose a new, simple mechanism to predict the reuse behavior of missed cache blocks in a manner that mitigates both pollution and thrashing. Our mechanism tracks the addresses of recently evicted blocks in a structure called the Evicted-Address Filter (EAF). Missed blocks whose addresses are present in the EAF are predicted to have high reuse and all other blocks are predicted to have low reuse. The key observation behind this prediction scheme is that if a block with high reuse is prematurely evicted from the cache, it will be accessed soon after eviction. We show that an EAFimplementation using a Bloom lter, which is cleared periodically, naturally mitigates the thrashing problem by ensuring that only a portion of a thrashing working set is retained in the cache, while incurring low storage cost and implementation complexity.</p><p>We compare our EAF-based mechanism to ve state-of-the-art mechanisms that address cache pollution or thrashing, and show that it provides signi cant performance improvements for a wide variety of workloads and system con gurations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>run simultaneously on the same chip, increasing the pressure on the memory subsystem. In most modern processor designs, such concurrently running applications share the on-chip last-level cache <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>. As a result, e ective use of the available cache space is critical for high system performance.</p><p>Ideally, to ensure high performance, the cache should be lled only with blocks that have high temporal reuse -blocks that are likely to be accessed multiple times within a short time interval. However, as identi ed by prior work (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>), two problems degrade cache performance signi cantly. First, cache blocks with little or no reuse can evict blocks with high reuse from the cache. This problem is referred to as cache pollution. Second, when there are a large number of blocks with high reuse, they start evicting each other from the cache due to lack of space. This problem is referred to as cache thrashing. Both pollution and thrashing increase the miss rate of the cache and consequently reduce system performance. Prior work proposed to modify the cache insertion policy to mitigate the negative e ects of pollution and thrashing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>To prevent cache pollution, prior approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref> predict the reuse behavior of missed cache blocks and insert blocks predicted to have low reuse with a low priority -e.g., at the leastrecently-used (LRU) position for the LRU replacement policy. This ensures that such low-reuse blocks get evicted from the cache quickly, thereby preventing them from polluting the cache. To predict the reuse behavior of missed cache blocks, these mechanisms group blocks based on the program counter that accessed them <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b54">55]</ref> or the memory region to which the blocks belong <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b54">55]</ref>. The mechanisms subsequently learn the reuse behavior of each group and use that to predict the reuse behavior of an individual cache block. As such, they do not distinguish between the reuse behavior of blocks within a group.</p><p>To prevent thrashing, recent prior work <ref type="bibr" target="#b34">[35]</ref> proposed the use of a thrash-resistant bimodal insertion policy (BIP). BIP inserts a majority of missed blocks with low priority (at the LRU position) and a small fraction of blocks with high priority (at the most-recentlyused (MRU) position). By doing so, a fraction of the working set can be retained in the cache, increasing the hit rate when the working set is larger than the cache size. When multiple threads share the cache, prior approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> learn the thrashing behavior of individual threads using a technique called set-dueling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, and use BIP for those threads that are determined to su er from thrashing. As such, these approaches do not distinguish between the reuse behavior of cache blocks within a thread.</p><p>The Problem: In this work, we observe that previous proposals are not e ective in preventing both pollution and thrashing at the same time. On one hand, proposals that address cache pollution do not distinguish between the reuse behavior of blocks that belong to the same group. As a result, these mechanisms can lead to many mispredictions when the reuse behavior of an individual block does not correlate with the reuse behavior of the group to which it belongs. Moreover, these approaches do not have any inherent mechanism to detect and prevent thrashing. If a large number of blocks are predicted to have high reuse, they will evict each other from the cache. On the other hand, since proposals that address thrashing use thread-level behavior to detect thrashing, they cannot distinguish between high-reuse blocks and low-reuse blocks within a thread. As a result, they either insert low-reuse blocks of a nonthrashing thread with high priority and hence, pollute the cache, or they repeatedly insert high-reuse blocks of a thrashing thread with the bimodal insertion policy, potentially leading to cache underutilization. Since approaches to prevent pollution and approaches to prevent thrashing both modify the cache insertion policy using di erent mechanisms, it is di cult to combine them to address both problems concurrently. Our goal in this work is to design a mechanism that seamlessly reduces both pollution and thrashing to improve system performance.</p><p>Our Approach: To prevent cache pollution, one would like to predict the reuse behavior of a missed cache block and prevent low-reuse blocks from polluting the cache by choosing an appropriate cache insertion policy. As such, we take an approach similar to prior work, but unlike prior work which predicts the reuse behavior of a group of blocks, we predict the reuse behavior of each missed block based on its own past behavior. Unfortunately, keeping track of the behavior of all blocks in the system would incur large storage overhead and lookup latency. We eliminate this high cost by taking advantage of the following observation: If a block with high reuse is prematurely evicted from the cache, it will likely be accessed soon after eviction. On the other hand, a block with low reuse will not be accessed for a long time after eviction. This observation indicates that it is su cient to keep track of a small set of recently evicted blocks to predict the reuse behavior of missed blocks -blocks evicted a long time ago are unlikely to have high reuse. To implement this prediction scheme, our mechanism keeps track of addresses of recently evicted blocks in a hardware structure called the Evicted-Address Filter (EAF). If a missed block's address is present in the EAF, the block is predicted to have high reuse. Otherwise, the block is predicted to have low reuse.</p><p>Cache thrashing happens when the working set is larger than the cache. In the context of EAF, there are two cases of thrashing. First, the working set can be larger than the aggregate size of the blocks tracked by the cache and the EAF together. We show that this case can be handled by using the thrash-resistant bimodal insertion policy <ref type="bibr" target="#b34">[35]</ref> for low-reuse blocks, which ensures that a fraction of the working set is retained in the cache. Second, the working set can be larger than the cache but smaller than the aggregate size of the blocks tracked by the cache and the EAF together. In this case, thrashing can be mitigated by restricting the number of blocks predicted to have high reuse to a value smaller than the number of blocks in the cache. We nd that implementing EAF using a Bloom lter enables this e ect by forcing the EAF to be periodically cleared when it becomes full. Doing so results in the EAF predicting only a portion of the working set to have high reuse, thereby mitigating thrashing. We describe the required changes that enable the EAF to mitigate thrashing in detail in Section 2.3, and evaluate them quantitatively in Section 7.4. Thus, our mechanism can reduce the negative impact of both cache pollution and thrashing using a single structure, the Evicted-Address Filter.</p><p>Summary of Operation: Our mechanism augments a conventional cache with an Evicted-Address Filter (EAF) that keeps track of addresses of recently evicted blocks. When a block is evicted from the cache, the block's address is inserted into the EAF. On a cache miss, the cache tests whether the missed block address is present in the EAF. If yes, the block is predicted to have high reuse and inserted with a high priority into the cache. Otherwise, the block is predicted to have low reuse and inserted with the bimodal insertion policy. When the EAF becomes full, it is completely cleared. We show that EAF naturally lends itself to a low-cost and low-complexity implementation using a Bloom lter (Section 3.2).</p><p>Using EAF to predict the reuse behavior of missed cache blocks has three bene ts. First, the hardware implementation of EAF using a Bloom lter has low overhead and complexity. Second, the EAF is completely outside the cache. As a result, our mechanism does not require any modi cations to the existing cache structure, and consequently, integrating EAF in existing processors incurs low design and veri cation cost. Third, EAF operates only on a cache miss and does not modify the cache hit operation. Therefore, it can be favorably combined with other techniques that improve performance by monitoring blocks while they are in the cache (e.g., better cache replacement policies).</p><p>We compare our EAF-augmented cache (or just EAF-cache) with ve state-of-the-art cache management approaches that aim to prevent pollution or thrashing: 1) Thread-aware dynamic insertion policy <ref type="bibr" target="#b15">[16]</ref> (TA-DIP), 2) Thread-aware dynamic re-reference interval prediction policy <ref type="bibr" target="#b16">[17]</ref> (TA-DRRIP), 3) Signature-based Hit Prediction using Instruction pointers (SHIP) <ref type="bibr" target="#b54">[55]</ref>, 4) Run-time cache bypassing <ref type="bibr" target="#b18">[19]</ref> (RTB), and 5) Miss classi cation table <ref type="bibr" target="#b8">[9]</ref> (MCT). Our evaluations show that EAF-cache signi cantly outperforms all prior approaches for a wide variety of workloads and on a number of system con gurations.</p><p>We make the following contributions:</p><p>? We show that keeping track of a small set of recently evicted blocks can enable a new and low-cost per-block reuse prediction mechanism.</p><p>? We provide a low-overhead, practical implementation of a new cache insertion policy based on Evicted-Address Filters (EAF), which mitigates the negative impact of both cache pollution and thrashing.</p><p>? We compare the EAF-cache with ve state-of-the-art cache management mechanisms using a wide variety of workloads and show that EAF-cache provides better overall system performance than all of them (21% compared to the baseline LRU replacement policy and 8% compared to the best previous mechanism, SHIP <ref type="bibr" target="#b54">[55]</ref>, for a 4-core system). We also show that EAF-cache is orthogonal to improvements in cache replacement policy, by evaluating it in conjunction with two di erent replacement policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE EVICTED-ADDRESS FILTER</head><p>As mentioned before, our goal in this work is to devise a mechanism to prevent both cache pollution and thrashing. Unlike prior approaches, instead of predicting the reuse behavior of a missed cache block indirectly using program counter, memory region or application behavior, our approach predicts the reuse behavior of a missed block based on its own past behavior. This can be achieved by remembering the history of reuse behavior of every block based on its past accesses. However, keeping track of the reuse behavior of every cache block in the system is impractical due to the associated high storage overhead.</p><p>In this work, we make an observation that leads to a simple perblock reuse prediction mechanism to address cache pollution. We rst describe this observation and our basic mechanism. We show that a na?ve implementation of our basic mechanism 1) has high storage and power overhead, and 2) does not address thrashing. We then describe an implementation of our mechanism using a Bloom lter <ref type="bibr" target="#b5">[6]</ref>, which addresses both of the above issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Addressing Cache Pollution</head><p>Observation: If a cache block with high reuse is prematurely evicted from the cache, then it will likely be accessed soon after eviction. On the other hand, a cache block with little or no reuse will likely not be accessed for a long time after eviction.</p><p>The above observation indicates that to distinguish blocks with high reuse from those with low reuse, it is su cient to keep track of a set of recently evicted blocks. Based on this, our mechanism augments a cache with a structure that tracks the addresses of recently evicted blocks in a FIFO manner. We call this structure the Evicted-Address Filter (EAF). On a cache miss, if the missed block address is present in the EAF, it is likely that the block was prematurely evicted from the cache. Therefore, the cache predicts the block to have high reuse. On the other hand, if the missed block address is not present in the EAF, then either the block is accessed for the rst time or it was evicted from the cache a long time ago. In both cases, the cache predicts the block to have low reuse.</p><p>Depending on the replacement policy used by the cache, when a missed block's address is present in the EAF, the block (predicted to have high reuse) is inserted with a high priority, which keeps it in the cache for a long period -e.g., at the most-recently-used (MRU) position for the conventional LRU replacement policy. In this case, the corresponding address is also removed from the EAF, as it is no longer a recently evicted block address. This also allows the EAF to track more blocks, and thus make potentially better reuse predictions. When a missed block address is not present in the EAF, the block (predicted to have low reuse) is inserted with a low priority such that it is less likely to disturb other blocks in the cache -e.g., at the LRU position.</p><p>The size of the EAF -i.e., the number of recently evicted block addresses it can track -determines the boundary between blocks that are predicted to have high reuse and those that are predicted to have low reuse. Intuitively, blocks that will be reused in the cache should be predicted to have high reuse and all other blocks should be predicted to have low reuse. For this purpose, we set the size of the EAF to be the same as the number of blocks in the cache. Doing so ensures that if any block that can be reused gets prematurely evicted, it will be present in the EAF. This also ensures any block with a large reuse distance will likely not be present in the EAF. <ref type="bibr">Section 7.4</ref> analyzes the e ect of varying the size of the EAF.</p><p>In summary, the EAF keeps track of as many recently evicted block addresses as the number of blocks in the cache. When a block gets evicted from the cache, the cache inserts its address into the EAF. On a cache miss, the cache tests if the missed block address is present in the EAF. If yes, it removes the address from the EAF and inserts the block into the cache set with a high priority. Otherwise, it inserts the block with a low priority. When the EAF becomes full, the cache removes the least-recently-evicted block address from the EAF (in a FIFO manner).</p><p>Although the EAF-based mechanism described above, which we refer to as the plain-EAF, can distinguish high-reuse blocks from low-reuse blocks, it su ers from two problems: 1) a na?ve implementation of the plain-EAF (a set-associative tag store similar to the main tag store) has high storage and power overhead, and 2) the plain-EAF does not address thrashing.</p><p>Observation: Implementing EAF using a Bloom lter addresses the rst problem, as a Bloom lter has low storage and power overhead. In addition, we observe that a Bloomlter-based EAF implementation also mitigates thrashing.</p><p>We defer a detailed description of the Bloom lter and how it enables a low-overhead implementation of EAF to Section 3. We rst provide a deeper understanding of why the plain-EAF su ers from the thrashing problem (Section 2.2) and how a Bloom-lterbased EAF implementation mitigates thrashing (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Problem with Large Working Sets</head><p>Consider an application that accesses a working set larger than the cache size in a cyclic manner. In the context of EAF, there are two possible cases.</p><p>Case 1: The working set is larger than the aggregate size of blocks tracked by the cache and EAF together. In this case, no missed block's address will be present in the EAF. Therefore, all missed blocks will be predicted to have low reuse and inserted into the cache with a low priority. This leads to cache under-utilization as no block is inserted with a high priority, even though there is reuse in the working set. <ref type="foot" target="#foot_0">1</ref> In this case, we would like to always retain a fraction of the working set in the cache as doing so will lead to cache hits at least for that fraction.</p><p>Case 2: The working set is smaller than the aggregate number of blocks tracked by both the cache and EAF together. In this case, every missed block's address will be present in the EAF. As a result, every missed block will be predicted to have high reuse and inserted into the cache with a high priority. This will lead to blocks of the application evicting each other from the cache. However, similar to the previous case, we would like to always retain a fraction of the working set in the cache to prevent this problem.</p><p>The above discussion indicates that simply using the plain-EAF for predicting the reuse behavior of missed blocks would degrade cache performance when the working set is larger than the cache because using the plain-EAF causes cache under-utilization (Case 1) and does not address thrashing (Case 2).</p><p>To address the cache under-utilization problem (Case 1), we insert blocks predicted to have low reuse with the bimodal insertion policy (BIP) <ref type="bibr" target="#b34">[35]</ref>, as opposed to always inserting them with a low priority. BIP inserts a small fraction of blocks with a high priority and the remaining blocks with a low priority, retaining a fraction of the large working set in the cache. As a result, using BIP for low-reuse blocks allows EAF to adapt to working sets larger than the cache size and the EAF size combined. In the following section, we describe our solution to address cache thrashing (Case 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Addressing Cache Thrashing</head><p>The reason why the plain-EAF su ers from thrashing is that it does not have any mechanism to prevent a large number of highreuse blocks from getting into the cache with a high priority. To mitigate thrashing, the plain-EAF should be augmented with a mechanism that can restrict the number of blocks predicted to have high reuse. Such a mechanism would ensure that not all blocks of a large working set are inserted into the cache with a high priority, and thereby prevent them from evicting each other from the cache. We show that implementing EAF using a Bloom lter achieves this goal.</p><p>EAF using a Bloom Filter: Note that the EAF can be viewed as a set of recently evicted block addresses. Therefore, to reduce its storage cost, we implement it using a Bloom lter, which is a compact representation of a set, in this case, a set of addresses. However, a Bloom lter does not perfectly match the requirements of the plain-EAF. As summarized in Section 2.1, the plain-EAF requires three operations: insertion, testing and removal of an address. Although the Bloom lter supports the insertion and testing operations, it does not support removal of an individual address (Section 3.1). Therefore, to make the plain-EAF implementable using a Bloom lter, we slightly modify the plain-EAF design to eliminate the need to remove an individual address from the EAF. As described in Section 2.1, there are two cases when an address is removed from the EAF. First, when a missed block address is present in the EAF, the cache removes it from the EAF. In this case, we propose to simply not remove the address from the EAF. Second, when the EAF becomes full, the least-recently-evicted address is removed from the EAF, in a FIFO manner. In this case, we propose to clear the EAF completely, i.e. remove all addresses in the EAF, as the Bloom lter supports such a clear operation (Section 3.1). With these two changes, the EAF design becomes amenable for low-cost implementation using a Bloom lter.</p><p>Serendipitously, we also found that the changes we make to the plain-EAF design to enable its implementation with a Bloom lter has the bene t of mitigating thrashing. To see why this happens, let us examine the e ect of the two changes we make to the plain-EAF. First, not removing a cache block address from the EAF ensures that the EAF becomes full even when a thrashing working set ts into the cache and the EAF together. Second, once the EAF becomes full, its gets cleared. These two changes together enable the periodic clearing of the EAF. Such periodic clearing results in only a fraction of the blocks of a thrashing working set to get predicted as high-reuse blocks, retaining only that fraction in the cache. This improves the cache hit-rate for such a working set, and thereby mitigates thrashing.</p><p>In summary, we propose three changes to the plain-EAF to handle working sets larger than the cache size: 1) insert low-reuse blocks with the bimodal insertion policy, 2) not remove a missed block address from the EAF, when it is present in the EAF, and 3) clear the EAF when it becomes full. We evaluate and analyze the e ect of each of these three changes in Section 7.4. Hereafter, we use "EAF" to refer to an EAF with all the above proposed changes. In Section 7.7, we evaluate other possible changes to the plain-EAF design that also mitigate thrashing. We nd that those designs perform similarly to the EAF design we proposed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Putting it All Together</head><p>Figure <ref type="figure">1</ref> summarizes our EAF-based cache management mechanism. There are three operations involving the EAF. First, when a block is evicted from the cache, the cache inserts the block's address into the EAF . Second, on a cache miss, the cache tests whether the missed block address is present in the EAF . If so, the cache inserts the missed block with a high priority, otherwise it inserts the block with the bimodal policy. Third, when the EAF becomes full -i.e., when the number of addresses in the EAF is the same as the number of blocks in the cache -the cache clears the EAF . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EAF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Handling LRU-friendly Applications</head><p>When a block with high reuse is accessed for the rst time, it will not be present in the EAF. In this case, the EAF will falsely predict that the block has low reuse. Hence, the block will likely get inserted with a low priority (due to BIP) and get evicted from the cache quickly. As a result, for an LRU-friendly application -i.e., one whose most recently accessed blocks have high reuse -EAF-cache incurs one additional miss for a majority of blocks by not inserting them with high priority on their rst access. In most multicore workloads with LRU-friendly applications, we nd that this does not impair performance as the cache is already lled with useful blocks. However, we observe that, for some workloads with all LRU-friendly applications, EAF-cache performs considerably worse than prior approaches because of the additional mispredictions.</p><p>To address this problem, we use set-dueling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> to determine if the system as a whole bene ts from an always-high-priority insertion policy. The cache chooses two groups of 32 random sets. A missed block that belongs to a set of the rst group is always inserted with the priority chosen based on the EAF prediction. A missed block that belongs to a set of the second group is always inserted with high priority. A saturating counter is used to keep track of which group incurs fewer misses. Blocks that do not belong to either of the groups are inserted with the policy that leads to fewer misses. We call this enhancement Dueling-EAF (D-EAF) and evaluate its performance in Section 7. Our results indicate that D-EAF mitigates the performance loss incurred by EAF-cache for workloads with all LRU-friendly applications and does not signi cantly a ect performance of other workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRACTICAL IMPLEMENTATION</head><p>As described in Section 2.3, the EAF is designed to be implementable using a Bloom lter <ref type="bibr" target="#b5">[6]</ref>. In this section, we describe the problems with a na?ve implementation of the EAF that forced us to consider alternate implementations. We follow this with a detailed description of the hardware implementation of a Bloom lter, which has much lower hardware overhead and complexity compared to the na?ve implementation. Finally, we describe the implementation of EAF using a Bloom lter.</p><p>A na?ve implementation of the EAF is to organize it as a setassociative tag store similar to the main tag store. Each set of the EAF can keep track of addresses of the most recently evicted blocks of the corresponding cache set. However, such an implementation is not desirable for two reasons. First, it incurs a large storage overhead as it requires a separate tag store of the same size as the main tag store. Second, testing whether an address is present in the EAF requires an associative look-up. Although the test operation is not on the critical path, associative look-ups consume additional power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bloom Filter</head><p>A Bloom lter is a probabilistic data structure used as a compact representation of a set. It allows three simple operations: 1) insert an element into the set, 2) test if an element is present in the set, and 3) remove all the elements from the set. The test operation can have false positives -i.e., the Bloom lter can falsely declare that an element is present in the set even though that element might never have been inserted. However, the false positive rate can be controlled by appropriately choosing the Bloom lter parameters <ref type="bibr" target="#b5">[6]</ref>.</p><p>The hardware implementation of a Bloom lter consists of a bitarray with m bits and a set of k hash functions. Each hash function maps elements (e.g., addresses) to an integer between 0 and m -1. values of all the k hash functions on the element, and sets the corresponding bits in the bit-array -e.g., in the gure, inserting the element x sets the bits at locations 1 and 8. To test if an element is present in the set, the Bloom lter computes the values of all the k hash functions on the element, and checks if all the corresponding bits in the bit-array are set. If so, it declares that the element is present in the set -e.g., test(x) in the gure. If any of the corresponding bits is not set, the lter declares that the element is not present in the set -e.g., test(w) in the gure. Since the hash functions can map di erent elements to the same bit(s), the bits corresponding to one element could have been set as a result of inserting other elements into the set -e.g., in the gure, the bits corresponding to the element z are set as a result of inserting elements x and y. This is what leads to a false positive for the test operation. Finally, to remove all the elements from the set, the Bloom lter simply resets the entire bit-array. Since multiple elements can map to the same bit, resetting a bit in the Bloom lter can potentially remove multiple elements from it. This is the reason why an individual element cannot be removed from a Bloom lter. Since a Bloom lter does not directly store elements, it generally requires much smaller storage than a na?ve implementation of a set. In addition, all operations on a Bloom lter require only indexed lookups into the bit-array, which are more energy e cient than associative lookups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EAF Using a Bloom Filter and a Counter</head><p>As described in Section 2.3, the EAF is designed so that the three operations associated with it, namely, insert, test, and clear, match the operations supported by the Bloom lter. In addition to the Bloom lter required to implement the EAF, our mechanism also requires a counter to keep track of the number of addresses currently inserted into the Bloom lter. This is required to determine when the EAF becomes full, so that the cache can clear it.</p><p>Figure <ref type="figure">3</ref> shows the operation of an EAF implemented using a Bloom lter and a counter. Initially, both the Bloom lter and the counter are reset. When a cache block gets evicted, the cache inserts the block's address into the Bloom lter and increments the counter. On a cache miss, the cache tests if the missed block address is present in the Bloom lter and chooses the insertion policy based on the result. When the counter reaches the number of blocks in the cache, the cache resets both the counter and the Bloom lter.</p><p>For our Bloom lter implementation, we use the H3 class of hash functions <ref type="bibr" target="#b40">[41]</ref>. These functions require only XOR operations on the input address and, hence, are simple to implement in hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hardware Implementation Cost</head><p>Implementing EAF on top of a conventional cache requires three additional hardware structures: 1) a bit-array for the Bloom lter, Reset counter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counter</head><p>Figure <ref type="figure">3</ref>: EAF implementation using a Bloom lter and a counter. The gure shows the three events that trigger operations on the EAF.</p><p>2) a counter that can count up to the number of blocks in the cache, and 3) peripheral logic structure to determine the insertion priority of each missed cache block. The primary source of storage overhead is the bit-array required to implement the Bloom lter.</p><p>The number of bits required for the bit-array scales linearly with the maximum number of addresses expected to be stored in the Bloom lter, which in our case, is same as the number of blocks in the cache (C) (as described in Section 2.1). We set the number of bits in the bit-array to be ?C, where ? is the average number of bits required per address stored in the Bloom lter. We will discuss the trade-o s of this parameter shortly. For a system where cache block size is B and the size of each cache tag entry is T , the storage overhead of EAF compared to the cache size is given by: Bloom lter size Cache size</p><formula xml:id="formula_0">= ?C (T + B)C = ? T + B<label>(1)</label></formula><p>The false positive probability (p) of the Bloom lter is given by (as shown in <ref type="bibr" target="#b11">[12]</ref>):</p><formula xml:id="formula_1">p = 2 -?ln2<label>(2)</label></formula><p>Since the false positive rate of the Bloom lter directly a ects the accuracy of EAF predictions, the parameter ? presents a crucial trade-o between the storage overhead of EAF and its performance improvement. From Equations ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>), it can be seen that as ? increases, the storage overhead also increases, but the false positive rate decreases. We nd that, for most systems we evaluate, setting ? to 8 provides the performance bene ts of an EAF with no false positives (Section 7.4). For a 2-MB last-level cache with 64-byte block size, this amounts to a modest 1.47% storage overhead. Our evaluations show that, for a 4-core system, EAF-cache provides 21% performance improvement compared to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ADVANTAGES &amp; DISADVANTAGES</head><p>As we will show in Section 7, EAF-cache, which addresses both pollution and thrashing, provides the best performance compared to ve prior approaches. In addition, EAF-cache has three advantages. Ease of hardware implementation: As Figure <ref type="figure">3</ref> shows, EAF-cache can be implemented using a Bloom lter and a counter. Bloom lters have low hardware overhead and complexity. Several previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> have proposed the use of Bloom lters in hardware for various applications. Therefore, our EAF implementation using a Bloom lter incurs low design and implementation cost. No modi cations to cache design: Since both the Bloom lter and the counter are outside the cache, our mechanism does not require any modi cations to the cache. This leads to low design and verication e ort for integrating EAF into existing processor designs. No modi cations to cache hit operation: Finally, EAF operates only on a cache miss. The cache hit operation remains unchanged. Therefore, EAF can be combined with mechanisms that improve cache performance by monitoring only cache hits -e.g., the cache replacement policy. As an example, we show that EAF-cache provides better performance with an improved cache replacement policy (Section 7.3). One shortcoming of EAF-cache is its storage overhead compared to certain other prior approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> to address cache thrashing or pollution individually (see Table <ref type="table" target="#tab_1">1</ref>). However, as we show in Section 7, EAF-cache signi cantly outperforms these techniques, thereby justifying its additional storage overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">QUALITATIVE COMPARISON TO PRIOR WORK</head><p>We qualitatively compare our proposed mechanism, the EAFcache, with ve state-of-the-art high-performance mechanisms to address cache pollution or thrashing. The fundamental di erence between the EAF-cache and prior approaches is that unlike the EAFcache, previous mechanisms do not concurrently address the negative impact of both pollution and thrashing. As a result, they do not obtain the best performance for all workloads. Table <ref type="table" target="#tab_1">1</ref> lists the previous mechanisms that we compare with the EAF-cache. The table indicates whether each mechanism addresses pollution and thrashing, along with its implementation complexity. We now describe each mechanism individually.</p><p>Thread-Aware Dynamic Insertion Policy (TA-DIP) <ref type="bibr" target="#b15">[16]</ref> addresses the thrashing problem by determining thrashing at a thread granularity. It does so by using set-dueling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> to determine if a thread incurs fewer misses with the conventional LRU policy or the bimodal insertion policy (BIP). All blocks of a thread are inserted with the policy that has fewer misses. As a result, TA-DIP cannot distinguish between high-reuse blocks and low-reuse blocks within a thread. When following the LRU policy, this can lead to lowreuse blocks polluting the cache. When following BIP, this can lead to high-reuse blocks getting repeatedly inserted with the bimodal insertion policy, causing a low cache hit rate. As a result, TA-DIP does not provide the best performance when blocks within a thread have di erent reuse behavior.</p><p>Thread-Aware Dynamic Re-Reference Interval Prediction (TA-DRRIP) <ref type="bibr" target="#b16">[17]</ref> improves upon TA-DIP by using a better replacement policy than LRU, RRIP. Unlike the LRU policy, which inserts all incoming blocks with the highest priority (MRU position), RRIP inserts all incoming blocks with a lower priority. This allows RRIP to reduce the performance degradation caused by low-reuse blocks, when compared with LRU. However, as identi ed by later work <ref type="bibr" target="#b54">[55]</ref>, TA-DRRIP does not completely address the pollution problem as it monitors the reuse behavior of a block after inserting the block into the cache. In addition, like TA-DIP, since TA-DRRIP operates at a thread (rather than a block) granularity, it su ers from the shortcomings of TA-DIP described before.</p><p>Signature-based HIt Prediction (SHIP) <ref type="bibr" target="#b54">[55]</ref> addresses the pollution problem by using program counter information to distinguish blocks with low reuse from blocks with high reuse. The key idea is to group blocks based on the program counter that loaded them into the cache and learn the reuse behavior of each group using a table of counters. On a cache miss, SHIP indexes the table with the program counter that generated the miss and uses the counter value to predict the reuse behavior of the missed cache block. SHIP suffers from two shortcomings: 1) blocks loaded by the same program counter often may not have the same reuse behavior. In such cases, SHIP leads to many mispredictions, 2) when the number of blocks predicted to have high reuse exceeds the size of the cache, SHIP cannot address the resulting cache thrashing problem. <ref type="foot" target="#foot_1">2</ref>Run-time Cache Bypassing (RTB) <ref type="bibr" target="#b18">[19]</ref> addresses cache pollution by predicting reuse behavior of a missed block based on the memory region to which it belongs. RTB learns the reuse behavior of 1KB memory regions using a table of reuse counters. On a cache miss, RTB compares the reuse behavior of the missed block with the reuse behavior of the to-be-evicted block. If the missed block has higher reuse, it is inserted into the cache. Otherwise, it bypasses the cache. As RTB's operation is similar to that of SHIP, it su ers from similar shortcomings as SHIP. <ref type="foot" target="#foot_2">3</ref>Miss Classi cation Table (MCT) <ref type="bibr" target="#b8">[9]</ref> also addresses the cache pollution problem. MCT keeps track of one most recently evicted block address for each set in the cache. If a subsequent miss address matches this evicted block address, the miss is classi ed as a con ict miss. Otherwise, the miss is classi ed as a capacity miss. MCT inserts only con ict-missed blocks into the cache, anticipating that they will be reused. All other blocks are inserted into a separate bu er. MCT has two shortcomings. First, for a highly associative last-level cache, keeping track of only one most recently evicted block per set leads to many con ict misses getting falsely predicted as capacity misses, especially in multi-core systems with shared caches. Second, na?vely extending MCT to keep track of more evicted blocks can lead to the number of predicted-con ictmiss blocks to exceed the number of blocks in the cache. In such a scenario, MCT cannot address the resulting thrashing problem.</p><p>In contrast to previous approaches, our proposed mechanism, the EAF-Cache, is designed to address both cache pollution and thrashing with a single structure, the Evicted-Address Filter. Instead of indirectly predicting the reuse behavior of a cache block using the  behavior of the application, program counter or memory region as a proxy, the EAF-cache predicts the reuse behavior of each missed cache block based on the block's own past behavior. As described in Section 2.3, the EAF-cache also mitigates thrashing by retaining only a fraction of blocks of a thrashing working set in the cache. As a result, EAF-cache adapts to workloads with varying working set characteristics, and as we will show in our quantitative results, it signi cantly outperforms prior approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">METHODOLOGY</head><p>We use an in-house event-driven x86 simulator for our evaluations. Instruction traces were collected by running the benchmarks on top of the Wind River Simics full system simulator <ref type="bibr" target="#b3">[4]</ref>, which are then fed to our simulator's core model. Our framework faithfully models all memory-related processor-stalls. All systems use a 3-level cache hierarchy similar to some modern architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. The L1 and L2 caches are private to individual cores and the L3 cache is shared across all the cores. We do not enforce inclusion in any level of the hierarchy. All caches use a 64B block size. Writebacks do not update the replacement policy state.</p><p>Other major simulation parameters are provided in Table <ref type="table" target="#tab_2">2</ref>. The cache sizes we use for our primary evaluations (1MB for 2-core and 2MB for 4-core) were chosen to account for the small working set sizes of most of our benchmarks. Section 7.5 presents results with larger cache sizes (up to 16MB). For all mechanisms, except baseline LRU and DIP, the last-level cache uses the re-reference interval prediction replacement policy <ref type="bibr" target="#b16">[17]</ref>. For our EAF proposals, we assume that the operations on the EAF can be overlapped with the long latency of memory access (note that all EAF operations happen only on a last-level cache miss).</p><p>For evaluations, we use benchmarks from the SPEC CPU2000 and CPU2006 suites, three TPC-H queries, one TPC-C server and an Apache web server. All results are collected by running a representative portion of each benchmark for 500 million instructions <ref type="bibr" target="#b48">[49]</ref>. We classify benchmarks into nine categories based on their cache sensitivity (low, medium, or high) and memory intensity (low, medium, or high). For measuring cache sensitivity, we run the benchmarks with a 1MB last-level cache and a 256KB last-level cache and use the performance degradation as a metric. We de ne a benchmark's memory intensity as its misses per 1000 instructions (MPKI) on a 256KB L2 cache. We do not consider benchmarks with L2-MPKI less than one for our studies as they do not exert significant pressure on the last-level cache. Table <ref type="table" target="#tab_3">3</ref> shows the memory intensity and cache sensitivity of di erent benchmarks used in our evaluations along with the criteria used for classifying them.</p><p>We evaluate multi-programmed workloads running on 2-core and 4-core CMPs.</p><p>We generate nine categories of multiprogrammed workloads with di erent levels of aggregate intensity (low, medium, or high) and aggregate sensitivity (low, medium, or high). 4 We evaluate the server benchmarks separately with ten 2core and ve 4-core workload combinations. In all, we present results for 208 2-core and 135 4-core workloads. 5  We evaluate system performance using the weighted speedup metric <ref type="bibr" target="#b49">[50]</ref>, which is shown to correlate with the system throughput <ref type="bibr" target="#b10">[11]</ref>. We also evaluate three other metrics, namely, instruction throughput, harmonic speedup <ref type="bibr" target="#b27">[28]</ref>, and maximum slowdown <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, in Section 7.6 for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted Speedup</head><formula xml:id="formula_2">= i IPC shared i IPC alone i 7. RESULTS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Single-core Results</head><p>Figure <ref type="figure" target="#fig_2">4</ref> compares the performance, instructions per cycle (IPC), for di erent benchmarks with di erent mechanisms. 6 The percentages on top of the bars show the reduction in the last-level cache misses per kilo instruction of D-EAF compared to LRU. We draw two conclusions from the gure.</p><p>First, D-EAF, with its ability to address both pollution and thrashing, provides performance comparable to the better of the previous 4 We compute aggregate intensity/sensitivity of a workload as the sum of individual benchmark intensities/sensitivities (low = 0, medium = 1, high = 2). 5 We provide details of individual workloads in our tech report <ref type="bibr" target="#b46">[47]</ref>. 6 For clarity, results for DIP and RTB are excluded from Figures <ref type="figure" target="#fig_2">4</ref> and<ref type="figure">5</ref> (left) as their performance improvements are lower compared to the other mechanisms. We also don't present single-core results for benchmarks where all mechanisms perform within 1% of the baseline LRU. Our tech report provides the full results <ref type="bibr" target="#b46">[47]</ref>.  best mechanisms to address pollution (SHIP) or thrashing (DRRIP) for most benchmarks. On average, D-EAF provides the best performance across all benchmarks (7% IPC improvement over LRU).</p><p>In fact, except for benchmarks swim, facerec and parser, D-EAF always reduces the miss rate compared to the LRU policy. As the gure shows, MCT considerably degrades performance for several benchmarks. This is because MCT keeps track of only one most recently evicted block address per set to identify con ict misses. This leads to many con ict-miss blocks to get falsely predicted as capacity misses (as described in Section 5). As a result, such blocks are inserted with low priority, leading to poor performance. This e ect becomes worse in a multi-core system, where there is interference between concurrently running applications.</p><p>Second, as expected, for LRU-friendly benchmarks like swim and mgrid, EAF degrades performance considerably. This is because, when a block is accessed for the rst time, EAF predicts it to have low reuse. However, the most recently accessed blocks in these benchmarks have high reuse. Therefore, EAF incurs one additional miss for most blocks. D-EAF identi es this problem and inserts all blocks of such applications with high priority, thereby mitigating the performance degradation of EAF. For example, in case of swim, D-EAF reduces the MPKI by 18% compared to EAF. We conclude that D-EAF, with the ability to dynamically adapt to di erent benchmark characteristics, provides the best average performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Multi-core Results</head><p>In multi-core systems, the shared last-level cache is subject to varying access patterns from the concurrently-running applications. In such a scenario, although pollution or thrashing caused by an application may not a ect its own performance, they can adversely a ect the performance of the concurrently-running applications. Therefore, the cache management mechanism should e ciently handle all the access patterns to provide high system performance. The EAF-cache with its ability to concurrently handle both cache pollution and thrashing provides better cache utilization than prior approaches that were designed to handle only one of the two problems. As a result, we nd that the EAF-cache signi cantly outperforms prior approaches for multi-programmed workloads (on both 2-core and 4-core systems).</p><p>Performance by workload categories: Figure <ref type="figure">7</ref> shows the absolute weighted speedup for our 4-core system grouped based on different workload categories (as described in Section 6). 7 The percentage on top of the bars show the improvement in weighted speedup of D-EAF compared to the LRU policy. For SPEC workloads, as expected, overall system performance decreases as the intensity of 7 We do not include the results for 2-core workloads due to space constraints. The observed trends for them are similar to those for 4-core workloads. We include these results in our tech report <ref type="bibr" target="#b46">[47]</ref>.</p><p>the workloads increases from low to high. 8 Regardless, both EAF and D-EAF outperform other prior approaches for all workload categories (21% over LRU and 8% over the best previous mechanism, SHIP). The gure also shows that within each intensity category, the performance improvement of our mechanisms increases with increasing cache sensitivity of the workloads. This is because the negative impact of cache under-utilization increases as workloads become more sensitive to cache space. Although not explicitly shown in the gure, the performance improvement of D-EAF over the best previous mechanism (SHIP) also increases as the cache sensitivity of the workloads increases.</p><p>For server workloads, D-EAF improves weighted speedup by 17% compared to the LRU policy. These workloads are known to have scans <ref type="bibr" target="#b16">[17]</ref> (accesses to a large number of blocks with no reuse) which pollute the cache, making pollution the major problem. This is the reason why SHIP and RTB, which were designed to address only cache pollution, are able to perform comparably to D-EAF for these workloads. In contrast, TA-DIP, which is designed to address only cache thrashing, o ers no improvement over LRU.</p><p>We conclude that our EAF-based approach is better than other prior approaches for both SPEC and server workloads and its benets improve as workloads become more sensitive to cache space.</p><p>Weighted Speedup  Overall performance analysis: Figures <ref type="figure">5</ref> and<ref type="figure" target="#fig_4">6</ref> plot the performance improvements of the di erent approaches compared to the baseline LRU policy for all 2-core and 4-core workloads, respectively. The workloads are sorted based on the performance improvement of EAF. The goal of these gures is not to indicate how di erent mechanisms perform for all workloads. Rather, the gures show that both EAF and D-EAF (thick lines) signi cantly outperform all prior approaches for most workloads. 8 Our server benchmarks have little scope for categorization. Therefore, we present average results for all server workloads. Two other observations can be made from these gures. First, there is no consistent trend in the performance improvement of the prior approaches. Di erent approaches improve performance for di erent workloads. This indicates that addressing only cache pollution or only cache thrashing is not su cient to provide high performance for all workloads. Unlike these prior approaches, EAF and D-EAF, with their ability to address both problems concurrently, provide the best performance for almost all workloads. Second, there is no visible performance gap between EAF and D-EAF for the majority of the workloads except for a small set of LRU-friendly 2-core workloads (indicated in the far left of Figure <ref type="figure">5</ref>), where D-EAF performs better than EAF. This is because, for most workloads (even with LRU-friendly applications), the cache (with only EAF) is already lled with high-reuse blocks. As a result, the additional misses caused by EAF for LRU-friendly applications do not significantly a ect performance. But, as evidenced by our results for single-core and the small fraction of LRU-friendly 2-core workloads, D-EAF is more robust in terms of performance than EAF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Interaction with the Replacement Policy</head><p>As EAF-cache modi es only the cache insertion policy, it can be used with any cache replacement policy. Figure <ref type="figure" target="#fig_5">8</ref> shows the bene ts of augmenting our EAF mechanism to 1) a cache following the LRU replacement policy, and 2) a cache following the RRIP <ref type="bibr" target="#b16">[17]</ref> policy for all 4-core workloads. As the gure shows, EAF-cache consistently improves performance in both cases for almost all workloads (11% on average for LRU and 12% for RRIP). We conclude that the bene ts of EAF-cache are orthogonal to the bene ts of using an improved cache replacement policy. The size of the EAF -i.e., the number of evicted addresses it keeps track of -determines boundary between blocks that are considered to be recently evicted and those that are not. On hand, having a small EAF increases the likelihood of a high-reuse block getting incorrectly predicted to have low reuse. On the other hand, a large EAF increases the likelihood of a low-reuse block getting incorrectly predicted to have high reuse. As such, we expect performance to rst increase as the size of the EAF increases and then decrease beyond some point. Figure <ref type="figure" target="#fig_7">9</ref> presents the results of our experiments studying this trade-o . As indicated in the gure, the performance improvement of EAF peaks when the size of the EAF is approximately the same as the number of blocks in the cache. This concurs with our intuition (as mentioned in Section 2.1) -sizing the EAF to be around the same size as the number of blocks in the cache ensures that a block that can potentially be reused in the cache will likely be predicted to have high reuse while a block with a larger reuse distance will likely be predicted to have low reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalized</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size of EAF / Number of Blocks in Cache % Weighted Speedup</head><p>Improvement over LRU  A large value of ? leads to a low false positive rate but incurs high storage overhead. On the other hand, a small value of ? incurs small storage overhead but leads to a high false positive rate. As a false positive can result in an actually low-reuse block to be predicted to have high reuse, the choice of ? is critical for performance. Figure <ref type="figure" target="#fig_8">10</ref> shows the effect of varying ? on performance, when the EAF size is the same as the number of blocks in the cache. The gure also shows the corresponding storage overhead of the Bloom lter with respect to the cache size. As ? increases, performance improves because false positive rate decreases. However, almost all of the potential performance improvement of having a perfect EAF (EAF with no false positives) is achieved with ? = 8 (1.47% storage overhead).</p><p>E ect of EAF policies for cache thrashing: In Section 2.3, we proposed three changes to the plain-EAF, to address cache thrashing: 1) using the bimodal insertion policy (BIP) for low-reuse blocks (B), 2) not removing a block address when it is present in the EAF (N), and 3) clearing the EAF when it becomes full (C). Figure <ref type="figure" target="#fig_9">11</ref> shows the performance improvement of adding all combinations of these policies to the plain-EAF (described in Section 2.1). Two observations are in order.</p><p>First, the not remove (+N) and the clear (+C) policies individually improve performance, especially signi cant in the 4-core system.  However, applying both policies together (+NC) provides signicant performance improvement for all systems. This is because, when the working set is larger than the cache size but smaller than the aggregate size of the blocks tracked by the cache and EAF together, the not remove (+N) policy is required to ensure that the EAF gets full, and the clear (+C) policy is required to clear the EAF when it becomes full. Second, the best performance improvement is achieved for all systems when all the three policies are applied to the EAF (+NCB), as doing so improves the hit-rate for both categories of large working sets (as discussed in Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% Weighted Speedup</head><p>Improvement over LRU </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Sensitivity to System Parameters</head><p>Varying Cache Size: Figure <ref type="figure" target="#fig_10">12</ref> plots the performance improvements of SHIP (best previous mechanism), EAF and D-EAF compared to LRU for di erent cache sizes for multi-core systems. The performance improvement of di erent mechanisms decreases with increasing cache size. This is expected because with increasing cache size, cache under-utilization becomes less of a problem. However, the EAF-cache provides signi cant performance improvements even for 8MB/16MB caches, and better performance than SHIP for all cache sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% Weighted Speedup</head><p>Improvement over LRU Varying Memory Latency: Figure <ref type="figure" target="#fig_11">13</ref> shows the e ect of varying the memory latency. For ease of analysis, we use a xed latency for all memory requests in these experiments. As expected, system throughput decreases as the memory latency increases. However, the performance improvement of EAF-cache increases with increasing memory latency, making it a more attractive mechanism for future systems, which are likely to have higher access latencies due to contention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted Speedup</head><p>Memory access latency (processor cycles)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Multi-core Systems: Other Metrics</head><p>Table <ref type="table" target="#tab_9">4</ref> shows the percentage improvement of the EAF-cache over the baseline LRU policy and the best previous mechanism, SHIP, on four metrics: weighted speedup <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">50]</ref>, instruction throughput, harmonic speedup <ref type="bibr" target="#b27">[28]</ref> and maximum slowdown <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. For both 2-core and 4-core systems, EAF-cache signi cantly improves weighted speedup, instruction throughput and harmonic speedup, and also considerably reduces the average maximum slowdown. We conclude that EAF-cache improves both system performance and fairness compared to prior approaches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Alternate EAF Designs</head><p>In this section, we describe two di erent approaches to mitigate cache pollution and thrashing by making slight modi cations to the proposed EAF design: 1) Segmented-EAF, and 2) Decoupled-Clear-EAF. We describe these two designs below and compare their performance to our original EAF-cache design.</p><p>Segmented-EAF: The plain-EAF (described in Section 2.1) and the EAF the three changes described in Section 2.3) are two ends of a spectrum. While the former removes just one block address when the EAF becomes full, the latter clears the entire EAF when it becomes full. One can think of an approach which clears a fraction of the EAF (say 1  2 ) when it becomes full. This can be accomplished by segmenting the EAF into multiple Bloom lters (organized in a FIFO manner) and clearing only the oldest one when the EAF becomes full. We call this variant of the EAF Segmented-EAF. A Segmented-EAF trades o the ability to mitigate thrashing (number of addresses removed from the EAF) with the ability to mitigate pollution (amount of information lost due to clearing).</p><p>Decoupled-Clear-EAF: In EAF-cache, the number of blocks that enter the cache with high priority is reduced by clearing the EAF when it becomes full. However, this approach requires the size of the EAF to be around the same as the number of blocks in the cache, so as to limit the number of blocks predicted to have highreuse between two EAF-clear events. This restricts the EAF's visibility of recently evicted blocks. However, having a larger visibility can potentially allow EAF to better identify the reuse behavior of more cache blocks. The Decoupled-Clear-EAF addresses this visi- bility problem by decoupling the size of the EAF from determining when the EAF is cleared. In this approach, the EAF keeps track of more addresses than the number of blocks in the cache (say 2?).</p><p>An additional counter keeps track of the number of blocks inserted with high priority into the cache. The EAF is cleared on two events: 1) when it becomes full, and 2) when the number of high-priority blocks reaches the number of blocks in the cache. Table <ref type="table" target="#tab_10">5</ref> compares the performance improvement (over baseline LRU) of the above designs with our proposed EAF design. Our results indicate that there is not much performance gap (less than 2%) across all three designs. We conclude that all the three heuristics perform similarly for our workloads. We leave an in-depth analysis of these design points and other potential designs as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RELATED WORK</head><p>The primary contribution of this paper is the EAF-cache, a lowcomplexity cache insertion policy that mitigates the negative impact of both cache pollution and thrashing, by determining the reuse behavior of a missed block based on the block's own past behavior. We have already provided qualitative and quantitative comparisons to the most closely related work on addressing pollution or thrashing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b54">55]</ref>, showing that EAF-cache outperforms all these approaches. In this section, we present other related work.</p><p>A number of cache insertion policies have been proposed to prevent L1-cache pollution <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b52">53]</ref>. Similar to some approaches we have discussed in our paper <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b54">55]</ref>, these mechanisms use the instruction pointer or the memory region to predict the reuse behavior of missed blocks. As we showed, our EAF-based approach provides better performance than these prior approaches as it can address both pollution and thrashing simultaneously.</p><p>Prior work has proposed compiler-based techniques (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref>) to mitigate the negative impact of pollution and thrashing by either modifying the data layout or by providing hints to the hardware -e.g., non-temporal load/store in x86 <ref type="bibr" target="#b13">[14]</ref>. However, the scope of such techniques is limited to cases where reuse and/or locality of accesses can be successfully modeled by the programmer/compiler, which often corresponds to array accesses within loop nests. Our EAF-cache proposal can potentially improve performance for a much broader set of memory accesses, and hence complements these compiler-based techniques.</p><p>Much prior research (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46]</ref>) has focused on improving the cache replacement policy. Researchers have also proposed mechanisms to improve cache utilization (e.g., <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57]</ref>) by addressing the set imbalance problem (i.e., certain cache sets su er many con ict misses while others are under-utilized). The EAF-cache can be combined with any of these mechanisms to further improve performance.</p><p>A number of page replacement policies have been proposed to improve virtual memory performance (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>). As these mechanisms were designed for software-based DRAM bu er management, they usually employ complex algorithms and assume large amounts of storage. As a result, applying them to hardware caches incurs higher storage overhead and implementation complexity than the EAF-cache.</p><p>Jouppi proposed victim caches <ref type="bibr" target="#b20">[21]</ref> to improve the performance of direct mapped caches by reducing the latency of con ict misses.</p><p>A victim cache stores recently evicted cache blocks (including data) in a fully associative bu er. As a result, it has to be a small structure. In contrast, our proposed EAF mechanism is a reuse predictor which keeps track of only addresses of a larger number of recently evicted blocks. As such, the two techniques can be employed together.</p><p>Cache partitioning is one technique that has been e ectively used to improve shared-cache performance <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56]</ref> or provide QoS in multi-core systems with shared caches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref>. The mechanisms to improve performance use partitioning to provide more cache space to applications that bene t from the additional space. For QoS, the proposed approaches ensure that applications are guaranteed some minimum amount of cache space. Since blocks with low reuse contribute neither to system performance nor to fairness, these mechanisms can be employed in conjunction with the EAFcache, which can lter out low-reuse blocks to further improve performance or fairness. Evaluation of this is part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSION</head><p>E cient cache utilization is critical for high system performance. Cache pollution and thrashing reduce cache utilization and consequently, degrade cache performance. We show that prior works do not concurrently address cache pollution and thrashing.</p><p>We presented the EAF-cache, a mechanism that mitigates the negative impact of both cache pollution and thrashing. EAF-cache handles pollution by keeping track of addresses of recently evicted blocks in a structure called the Evicted-Address Filter (EAF) to distinguish high-reuse blocks from low-reuse blocks. Implementing the EAF using a Bloom lter naturally mitigates the thrashing problem, while incurring low storage and power overhead. Extensive evaluations using a wide variety of workloads and system con gurations show that the EAF-cache provides the best performance compared to ve di erent prior approaches.</p><p>We conclude that EAF-cache is an attractive mechanism that can handle both cache pollution and thrashing, to provide high performance at low complexity. The concept of EAF provides a substrate that can enable other cache optimizations, which we are currently exploring.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 Figure 2 :</head><label>22</label><figDesc>Figure 2: Hardware implementation of a Bloom lter using a bit vector of size 16 and two hash functions. The insert operations are performed before the test operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: System performance: Single-core system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 : 4 -</head><label>74</label><figDesc>Figure 7: 4-core system performance for di erent workload categories. The value on top of each group indicates % improvement in weighted speedup provided by D-EAF compared to LRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 5: System performance: 2-core</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: EAF-cache with di erent replacement policies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Sensitivity to EAF size overhead of the Bloom lter: As described in Section 3.3, for a xed EAF size, the parameter ? associated with the Bloom lter presents a trade-o between the false positive rate and the storage overhead of the Bloom lter.A large value of ? leads to a low false positive rate but incurs high storage overhead. On the other hand, a small value of ? incurs small storage overhead but leads to a high false positive rate. As a false positive can result in an actually low-reuse block to be predicted to have high reuse, the choice of ? is critical for performance. Figure10shows the effect of varying ? on performance, when the EAF size is the same as the number of blocks in the cache. The gure also shows the corresponding storage overhead of the Bloom lter with respect to the cache size. As ? increases, performance improves because false positive rate decreases. However, almost all of the potential performance improvement of having a perfect EAF (EAF with no false positives) is achieved with ? = 8 (1.47% storage overhead).E ect of EAF policies for cache thrashing: In Section 2.3, we proposed three changes to the plain-EAF, to address cache thrashing: 1) using the bimodal insertion policy (BIP) for low-reuse blocks (B), 2) not removing a block address when it is present in the EAF (N), and 3) clearing the EAF when it becomes full (C). Figure11shows the performance improvement of adding all combinations of these policies to the plain-EAF (described in Section 2.1). Two observations are in order.First, the not remove (+N) and the clear (+C) policies individually improve performance, especially signi cant in the 4-core system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Sensitivity to Bloom lter storage overhead</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: E ect of changes to plain-EAF (P-EAF)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: EAF-cache with di erent cache sizes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: EAF-cache with di erent memory latencies. The value on top of each group indicates % weighted speedup improvement of D-EAF compared to LRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Storage overhead and implementation complexity of di erent mechanisms (for a 1MB 16-way set-associative cache with 64B block size). *In our evaluations, we use an in nite size table for both SHIP and RTB to eliminate interference caused by aliasing.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Main con guration parameters used for our simulations</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Core</cell><cell></cell><cell cols="4">4 Ghz processor, in-order x86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">L1-D Cache</cell><cell cols="11">32KB, 2-way associative, LRU replacement policy, single cycle latency, 64B block size</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Private L2 Cache</cell><cell cols="11">256KB, 8-way associative, LRU replacement policy, latency = 8 cycles, 64B block size</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">1-core 1 MB, 16-way associative, latency = 21 cycles, 64B block size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">L3 Cache</cell><cell cols="10">2-core Shared, 1 MB, 16-way associative, latency = 21 cycles, 64B block size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">4-core Shared, 2 MB, 16-way associative, latency = 28 cycles, 64B block size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Main memory</cell><cell cols="12">DDR2 parameters, Row hits = 168 cycles, Row con icts = 408 cycles, 4 Banks, 8 KB row bu ers,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Name</cell><cell cols="2">L2-MPKI</cell><cell cols="2">Sens.</cell><cell>Name</cell><cell cols="2">L2-MPKI</cell><cell>Sens.</cell><cell></cell><cell>Name</cell><cell cols="2">L2-MPKI</cell><cell>Sens.</cell><cell></cell><cell>Name</cell><cell cols="2">L2-MPKI</cell><cell>Sens.</cell><cell></cell></row><row><cell>ammp</cell><cell>5.76</cell><cell>M</cell><cell>36%</cell><cell>H</cell><cell>fma3d</cell><cell>1.14</cell><cell>L</cell><cell>5%</cell><cell>M</cell><cell>lucas</cell><cell>3.11</cell><cell>L</cell><cell>0%</cell><cell>L</cell><cell>vpr</cell><cell>6.13</cell><cell>M</cell><cell>46%</cell><cell>H</cell></row><row><cell>applu</cell><cell>1.92</cell><cell>L</cell><cell>2%</cell><cell>L</cell><cell>galgel</cell><cell>7.94</cell><cell>M</cell><cell>17%</cell><cell>M</cell><cell>mcf</cell><cell>49.58</cell><cell>H</cell><cell>26%</cell><cell>H</cell><cell>wupwise</cell><cell>1.33</cell><cell>L</cell><cell>1%</cell><cell>L</cell></row><row><cell>art</cell><cell>40.56</cell><cell>H</cell><cell>52%</cell><cell>H</cell><cell>gcc</cell><cell>4.08</cell><cell>L</cell><cell>3%</cell><cell>M</cell><cell>mgrid</cell><cell>3.14</cell><cell>L</cell><cell>5%</cell><cell>M</cell><cell>xalancbmk</cell><cell>10.89</cell><cell>H</cell><cell>16%</cell><cell>M</cell></row><row><cell>astar</cell><cell>25.49</cell><cell>H</cell><cell>6%</cell><cell>M</cell><cell>GemsFDTD</cell><cell>16.57</cell><cell>H</cell><cell>1%</cell><cell>L</cell><cell>milc</cell><cell>12.33</cell><cell>H</cell><cell>0%</cell><cell>L</cell><cell>zeusmp</cell><cell>5.77</cell><cell>L</cell><cell>1%</cell><cell>L</cell></row><row><cell>bwaves</cell><cell>15.03</cell><cell>H</cell><cell>0%</cell><cell>L</cell><cell>gobmk</cell><cell>1.92</cell><cell>L</cell><cell>2%</cell><cell>L</cell><cell>omnetpp</cell><cell>12.73</cell><cell>H</cell><cell>10%</cell><cell>M</cell><cell cols="4">Server Benchmarks</cell><cell></cell></row><row><cell>bzip2</cell><cell>7.01</cell><cell>M</cell><cell>32%</cell><cell>H</cell><cell>h264ref</cell><cell>1.52</cell><cell>L</cell><cell>5%</cell><cell>M</cell><cell>parser</cell><cell>2.0</cell><cell>L</cell><cell>18%</cell><cell>H</cell><cell>apache20</cell><cell>5.8</cell><cell>L</cell><cell>9%</cell><cell>M</cell></row><row><cell>cactusADM</cell><cell>4.4</cell><cell>L</cell><cell>8%</cell><cell>M</cell><cell>hmmer</cell><cell>2.63</cell><cell>L</cell><cell>2%</cell><cell>L</cell><cell>soplex</cell><cell>25.31</cell><cell>H</cell><cell>18%</cell><cell>H</cell><cell>tpcc64</cell><cell>11.48</cell><cell>H</cell><cell>31%</cell><cell>H</cell></row><row><cell>dealII</cell><cell>1.51</cell><cell>L</cell><cell>9%</cell><cell>M</cell><cell>lbm</cell><cell>24.64</cell><cell>H</cell><cell>1%</cell><cell>L</cell><cell>sphinx3</cell><cell>14.86</cell><cell>H</cell><cell>9%</cell><cell>M</cell><cell>tpch2</cell><cell>17.02</cell><cell>H</cell><cell>31%</cell><cell>H</cell></row><row><cell>equake</cell><cell>9.22</cell><cell>M</cell><cell>6%</cell><cell>M</cell><cell>leslie3d</cell><cell>14.02</cell><cell>H</cell><cell>7%</cell><cell>M</cell><cell>swim</cell><cell>17.7</cell><cell>H</cell><cell>46%</cell><cell>H</cell><cell>tpch6</cell><cell>3.93</cell><cell>L</cell><cell>23%</cell><cell>H</cell></row><row><cell>facerec</cell><cell>4.61</cell><cell>L</cell><cell>18%</cell><cell>H</cell><cell>libquantum</cell><cell>14.31</cell><cell>H</cell><cell>1%</cell><cell>L</cell><cell>twolf</cell><cell>10.21</cell><cell>M</cell><cell>56%</cell><cell>H</cell><cell>tpch17</cell><cell>13.97</cell><cell>H</cell><cell>26%</cell><cell>H</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Benchmark classi cation based on intensity (L2-MPKI) and sensitivity (Sens.). (L -low, M -Medium, H -High).</figDesc><table /><note><p>Intensity: L2-MPKI &lt; 5 (Low); &gt; 10 (High); Rest (Medium). Sensitivity: Perf. Degradation &lt; 5% (Low); &gt; 18% (High); Rest (Medium)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Improvement of EAF on di erent metrics</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Comparison of EAF with other design alternatives</figDesc><table><row><cell cols="4">Design 1-core 2-core 4-core</cell></row><row><cell>EAF</cell><cell>7%</cell><cell>15%</cell><cell>21%</cell></row><row><cell>Segmented-EAF</cell><cell>6%</cell><cell>13%</cell><cell>21%</cell></row><row><cell>Decoupled-Clear-EAF</cell><cell>6%</cell><cell>14%</cell><cell>21%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We assume that the cache is already lled with some other data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We also implemented single-usage block prediction<ref type="bibr" target="#b33">[34]</ref> (SU), which also uses the program counter to identify reuse behavior of missed blocks. Both SU and SHIP provide similar performance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The SHIP paper<ref type="bibr" target="#b54">[55]</ref> also evaluates mechanisms that use memory regions and other signatures to group blocks. The paper identi es that program counter-based grouping provides the best results. We reach a similar conclusion based on our experimental results.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>Many thanks to <rs type="person">Chris Wilkerson</rs>, <rs type="person">Phillip Gibbons</rs>, and <rs type="person">Aamer Jaleel</rs> for their feedback during various stages of this project. We thank the anonymous reviewers for their valuable feedback and suggestions. We acknowledge members of the SAFARI and LBA groups for their feedback and for the stimulating research environment they provide. We acknowledge the generous support of <rs type="funder">AMD</rs>, <rs type="funder">Intel</rs>, <rs type="funder">Oracle</rs>, and <rs type="funder">Samsung</rs>. This research was partially supported by grants from <rs type="funder">NSF</rs>, <rs type="funder">GSRC</rs>, <rs type="funder">Intel University Research O ce</rs>, and <rs type="funder">Intel Science and Technology Center on Cloud Computing</rs>. We thank <rs type="person">Lavanya Subramanian</rs>, <rs type="person">David Andersen</rs>, <rs type="person">Kayvon Fatahalian</rs> and <rs type="person">Michael Papamichael</rs> for their feedback on this paper's writing.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://goo.gl/iQBfK" />
		<title level="m">AMD Phenom II key architectural features</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intel next generation microarchitecture</title>
		<ptr target="http://goo.gl/3eskx" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://goo.gl/KZSnc" />
		<title level="m">Oracle SPARC T4</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Wind River Simics</title>
		<ptr target="www.windriver.com/producs/simics" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CAR: Clock with adaptive replacement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Space/time trade-o s in hash coding with allowable errors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970-07">July 1970</date>
			<publisher>ACM Communications</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cooperative cache partitioning for chip multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new design of Bloom lter for packet inspection speedup</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GLOBECOM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hardware identi cation of cache con ict misses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving e ective bandwidth through compiler enhancement of global cache reuse</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JPDC</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">System-level performance metrics for multiprogram workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Small subset queries and Bloom lters using ternary associative memories, with applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fully associative software managed cache design</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 architectures software developer&apos;s manuals</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CQoS: A framework for enabling QoS in shared caches of CMP platforms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for managing shared caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sebot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LIRS: An e cient low inter-reference recency set replacement policy to improve bu er cache performance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Run-time cache bypassing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Merten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2Q: A low overhead high performance bu er management replacement algorithm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch bu ers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">IBM&apos;s next-generation server processor</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sinharoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Floyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IEEE Micro</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cache replacement based on reuse-distance prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Keramidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCD</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fair cache sharing and partitioning in a chip multiprocessor architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ATLAS: A scalable and high-performance scheduling algorithm for multiple memory controllers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harchol-Balter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Thread cluster memory scheduling: Exploiting di erences in memory access behavior</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cache bursts: A new approach for eliminating dead blocks and increasing cache e ciency</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Balancing thoughput and fairness in SMT processors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gummaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ISPASS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The design of a Bloom lter hardware accelerator for ultra low power systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISLPED</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ARC: A self-tuning, low overhead replacement cache</title>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FAST</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual private caches</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The LRU-K page replacement algorithm for database disk bu ering</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bloom ltering cache misses for accurate data speculation and prefetching</title>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Peir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting single-usage for e ective memory management</title>
		<author>
			<persName><forename type="first">T</forename><surname>Piquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rochecouste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACSAC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for high performance caching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A case for MLP-aware cache replacement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Utility-based cache partitioning: A low-overhead, high-performance, runtime mechanism to partition shared caches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>In MICRO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The V-way cache: Demand based associativity via global replacement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Emulating optimal replacement with a shepherd cache</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramaswamy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Virtualizing transactional memory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rajwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">E cient hardware hashing functions for high performance computers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bahcekapili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Compiler optimizations for eliminating cache con ict misses</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Tseng</surname></persName>
		</author>
		<idno>UMIACS-TR-97-59</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>College Park</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reducing con icts in direct-mapped caches with a temporality-based design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adaptive line placement with the set balancing cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Fraguela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Reducing capacity and con ict misses using set saturation levels</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Fraguela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HiPC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The ZCache: Decoupling ways and associativity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The Evicted-Address Filter: A uni ed mechanism to address both cache pollution and thrashing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<idno>SAFARI 2012-002</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A case for two-way skewed-associative caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Symbiotic jobscheduling for a simultaneous multithreaded processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Optimal partitioning of cache memory</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<date type="published" when="1992-09">Sep. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A new memory monitoring scheme for memory-aware scheduling and partitioning</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A modi ed approach to data cache management</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Farrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Pleszkun</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Using the compiler to improve cache replacement decisions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Weems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">SHIP: Signature-based hit predictor for high performance caching</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">PIPP: Promotion/insertion pseudo-partitioning of multi-core shared caches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">STEM: Spatiotemporal management of capacity for intra-core last level caches</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Seth</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
