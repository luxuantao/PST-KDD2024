<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ansor: Generating High-Performance Tensor Programs for Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-15">15 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<title level="a" type="main">Ansor: Generating High-Performance Tensor Programs for Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-15">15 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.06762v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-performance tensor programs are crucial to guarantee efficient execution of deep learning models. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously difficult. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering efforts in developing platform-specific optimization code or fall short in finding high-performance programs due to restricted search space and ineffective exploration strategy.</p><p>We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores much more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing stateof-the-art approaches. Besides, Ansor utilizes a scheduler to simultaneously optimize multiple subgraphs in a set of deep neural networks. Our evaluation shows that Ansor improves the execution performance of deep neural networks on the Intel CPU, ARM CPU, and NVIDIA GPU by up to 3.8×, 2.6×, and 1.7×, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Low latency execution of deep neural networks (DNN) plays a critical role in autonomous driving <ref type="bibr" target="#b12">[13]</ref>, augmented reality <ref type="bibr" target="#b2">[3]</ref>, language translation <ref type="bibr" target="#b13">[14]</ref>, and other applications of AI. DNNs can be expressed as a directed acyclic computational graph (DAG), in which nodes represent the operators (e.g., convolution and matrix multiplication) and directed edges represent the dependencies between operators. Existing deep learning frameworks (e.g., Tensorflow <ref type="bibr" target="#b0">[1]</ref>, PyTorch <ref type="bibr" target="#b34">[36]</ref>, MXNet <ref type="bibr" target="#b8">[9]</ref>) map the operators in DNNs to vendor-provided kernel libraries (e.g., CuDNN <ref type="bibr" target="#b11">[12]</ref>, MKL-DNN <ref type="bibr" target="#b22">[24]</ref>) to achieve high performance. However, these kernel libraries require significant engineering efforts in manual tuning towards each hardware platform and operator. The significant manual efforts required to produce efficient operator implementations for each target accelerator limit the development and innovation of new operators <ref type="bibr" target="#b5">[6]</ref> and specialized accelerators <ref type="bibr" target="#b30">[32]</ref>.</p><p>Given the importance of DNNs' performance, researchers and industry practitioners have turned to search-based compilation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b51">53]</ref> for automated generation of tensor programs, i.e. low-level implementations of tensor operators. For an operator or a (sub-)graph of multiple operators, users define the computation in a high-level declarative language ( §2), and the compiler then searches for programs tailored towards different hardware platforms.</p><p>To find performant tensor programs, it is necessary for a search-based approach to explore a large enough search space to cover all the useful tensor program optimizations. However, existing approaches fail to capture many effective optimization combinations, because they rely on either predefined manually-written templates (e.g., TVM <ref type="bibr" target="#b10">[11]</ref>, FlexTensor <ref type="bibr" target="#b51">[53]</ref>) or aggressive pruning by inaccurately evaluating incomplete programs (e.g. Halide auto-scheduler <ref type="bibr" target="#b1">[2]</ref>), which prevents them from covering a large enough search space ( §2). The rules they use to construct the search space are also limited.</p><p>In this paper, we want to explore a novel search strategy for generating high-performance tensor programs. It can automatically generates a large search space with comprehensive coverage of optimizations and gives every tensor program in the space a chance to be chosen. It thus enables us to find high-performance programs that existing approaches miss.</p><p>Realizing this goal faces multiple challenges. First, it requires us to automatically construct a large search space to cover as many tensor programs as possible for a given computation definition. Second, we need to search efficiently without comparing incomplete programs in the large search space that can be orders of magnitude larger than what existing templates can cover. Finally, when optimizing an entire DNN with many subgraphs, we should recognize and prioritize the subgraphs that are critical to the end-to-end performance.</p><p>To this end, we design and implement Ansor, a framework for automated tensor program generation. Ansor utilizes a hierarchical representation to cover a large search space. This representation decouples high-level structures and low-level details, enabling flexible enumeration of high-level structures and efficient sampling of low-level details. The space is constructed automatically for a given computation definition. Ansor then samples complete programs from the search space and fine-tunes these programs with evolutionary search and a learned cost model. To optimize the performance of DNNs with multiple subgraphs, Ansor dynamically prioritizes subgraphs of the DNNs that are more likely to improve the endto-end performance.</p><p>We evaluate Ansor on both standard deep learning benchmarks and emerging new workloads against manual libraries and state-of-the-art search frameworks. Experiment results show that Ansor improves the execution performance of DNNs on the Intel CPU, ARM CPU, and NVIDIA GPU by up to 3.8×, 2.6×, and 1.7×, respectively. For most computation definitions, the best program found by Ansor is outside the search space of existing search-based approaches. The results also show that, compared with existing searchbased approaches, Ansor has a more efficient search algorithm, generating higher-performance programs in a shorter time, despite its larger search space. Besides, Ansor enables automatic extension to new operators without requiring manual templates.</p><p>In summary, this paper makes the following contributions:</p><p>• A mechanism to generate a large search space with a hierarchical representation, from which diverse tensor programs can be sampled. • An evolutionary strategy with a learned cost model to fine-tune the performance of tensor programs. • A scheduling algorithm based on gradient descent to prioritize important subgraphs when optimizing the endto-end performance of one or more DNNs. • An implementation and comprehensive evaluation of the Ansor system demonstrating that the above techniques outperform state-of-the-art systems on a variety of deep learning models and hardware platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The deep learning ecosystem is embracing a rapidly growing diversity of hardware platforms including CPUs, GPUs, FP-GAs, and ASICs. In order to deploy DNNs on these platforms, high-performance tensor programs are needed for the operators used in DNNs. The required operator set typically contains a mixture of standard operators (e.g., matmul, conv2d) and novel operators invented by machine learning researchers (e.g., capsule conv2d <ref type="bibr" target="#b19">[21]</ref>, dilated conv2d <ref type="bibr" target="#b50">[52]</ref>).</p><p>To deliver portable performance of these operators on a wide range of hardware platforms in a productive way, multi-</p><formula xml:id="formula_0">C = compute((N, M), lambda i, j: sum(A[i, k]*B[k, j], [k]))</formula><p>Matrix Multiplication 𝐶 ", % = ∑ 𝐴 ", ) 𝐵 ), % ) Figure <ref type="figure">1</ref>: The computation definition of matrix multiplication. ple compiler techniques have been introduced (e.g., TVM <ref type="bibr" target="#b9">[10]</ref>, Halide <ref type="bibr" target="#b36">[38]</ref>, Tensor Comprehensions <ref type="bibr" target="#b43">[45]</ref>). Users define the computation in a form similar to mathematical expressions using a high-level declarative language, and the compiler generates optimized tensor programs according to the definition. Figure <ref type="figure">1</ref> shows the computation definition of matrix multiplication in the TVM tensor expression language. Users mainly need to define the shapes of the tensors and how each element in the output tensor is computed.</p><p>However, automatically generating high-performance tensor programs from a high-level definition is extremely difficult. Depending on the architecture of the targeted platform, the compiler needs to search in an extremely large and complicated search space containing combinatorial choices of optimizations (e.g., tile structure, tile size, vectorization, parallelization). Finding high-performance programs requires the search strategy to cover a large enough space and explore it efficiently. We describe two recent and effective approaches in this section and other related work in §8.</p><p>Template-guided search. In template-guided search, the search space is defined by manual templates. As shown in the Figure <ref type="figure">2</ref> (a), the compiler (e.g., TVM) requires the user to manually write a template for a computation definition. The template defines the structure of the tensor programs with some tunable parameters (e.g., tile size, and unrolling factor). The compiler then searches for the best values of these parameters for a specific input shape configuration and a specific hardware target. This approach has achieved good performance on common deep learning operators. However, developing templates requires substantial efforts. For example, the code repository of TVM already contains more than 15K lines of code for these templates. This number continues to grow as new operators and new hardware platforms emerge. Besides, constructing a quality template requires expertise in both tensor operators and hardware. It takes non-trivial research efforts <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b51">53]</ref> to develop quality templates. Despite the huge efforts in developing templates, existing manual templates only cover limited program structures because manually enumerating all optimization choices for all operators is prohibitive.</p><p>Sequential construction based search. This approach defines the search space by decomposing the program construction into a fixed sequence of decisions. The compiler then uses an algorithm such as beam search <ref type="bibr" target="#b29">[31]</ref> to search for good decisions (e.g., Halide auto-scheduler <ref type="bibr" target="#b1">[2]</ref>). In this approach, the compiler constructs a tensor program by sequentially unfolding all nodes in the computational graph. For each node, the compiler makes a few decisions on how to transform it into low-level tensor programs. When all nodes are unfolded, a complete tensor program is constructed. This approach ... uses a set of general unfolding rules for every node, so it can search automatically without requiring manual templates.</p><p>Because the number of possible choices of each decision is large, to make the sequential process feasible, this approach keeps only top-k candidate programs after every decision. The compiler estimates and compares the performance of candidate programs with a learned cost model to select the top-k candidates; while other candidates are pruned. During the search, the candidate programs are incomplete because only part of the computational graph is unfolded or only some of the decisions are made. Figure <ref type="figure">2</ref> (b) shows this process. However, estimating the final performance of incomplete programs is difficult in several respects: (1) the cost model trained on complete programs cannot accurately predict the final performance of incomplete programs. The cost model can only be trained on complete programs because we need to compile programs and measure their execution time to get the labels for training. Directly using this model to compare the final performance of incomplete programs will result in poor accuracy. As a case study, we train a cost model on 20,000 random complete programs from our search space and use the model to predict the final performance of incomplete programs. The incomplete programs are obtained by masking fractions of the 20,000 complete programs. We use two ranking metrics for evaluation: the accuracy of pairwise comparison and the recall@k score of top-k programs <ref type="foot" target="#foot_0">1</ref> . As shown in Figure <ref type="figure">3</ref>, the two curves start from 50% and 0% respec-tively, meaning that random guess with zero information gives 50% pairwise comparison accuracy and 0% top-k recall. The two curves go up quickly as the programs become complete, which means the cost model performs very well for complete programs but fails to accurately predict the final performance of incomplete programs. <ref type="bibr" target="#b1">(2)</ref> The fixed order of sequential decisions limits the design of the search space. For example, some optimization needs to add new nodes to the computational graph (e.g., adding cache nodes, using rfactor <ref type="bibr" target="#b40">[42]</ref>). The number of decisions for different programs becomes different. It is thus hard to align the incomplete programs for a fair comparison. (3) Sequential construction based search is not scalable. Enlarging the search space needs to add more sequential construction steps, but this will result in a worse accumulated error.</p><p>Ansor's hierarchical approach As shown in Figure <ref type="figure">2</ref>(c), Ansor is backed by a hierarchical search space that decouples high-level structures and low-level details. Ansor constructs the space automatically, eliminating the manual efforts in developing templates. Ansor then samples complete programs from the space and performs fine-tuning on complete programs, which avoids the inaccurate estimation of incomplete programs. Figure <ref type="figure">2</ref> shows the key difference between Ansor's approach and existing approaches. Next, we give an overview of all components of Ansor in §3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design Overview</head><p>Ansor is an automated tensor program generation framework. Figure <ref type="figure">4</ref> shows the overall architecture of Ansor. The input of Ansor is a set of to be optimized DNNs. Ansor has three major components: (1) a program sampler that constructs a large search space and samples diverse programs from it; (2) a performance tuner that fine-tunes the performance of sampled programs; (3) a task scheduler that allocates time resources for optimizing multiple subgraphs in the DNNs.</p><p>Program sampler. One key challenge Ansor has to address is to generate a large search space for a given computa- Performance tuner. The performance of randomly sampled programs is not necessarily good. The next challenge is to fine-tune them. Ansor employs evolutionary search and a learned cost model to perform fine-tuning iteratively ( §5). At each iteration, Ansor uses re-sampled new programs as well as good programs from previous iterations as the initial population to start evolutionary search. Evolutionary search fine-tunes programs by mutation and crossover which perform out-of-order rewrite and break the limitation of sequential construction. Querying the learned cost model is orders of magnitude faster than actual measurement, so we can evaluate thousands of programs in seconds via the cost model.</p><p>Task scheduler. Using program sampling and performance fine-tuning allows Ansor to find high-performance tensor programs for a computational graph. Intuitively, treating a whole DNN as a single computational graph and generating a full tensor program for it could potentially achieve the optimal performance. This, however, is inefficient because it has to deal with the unnecessary exponential explosion of the search space. Typically, the compiler partitions the large computational graph of a DNN into several small subgraphs <ref type="bibr" target="#b9">[10]</ref>. This partition has a negligible effect on the performance thanks to the layer-by-layer construction nature of DNNs. This brings the final challenge of Ansor: how to allocate time resources when generating programs for multiple subgraphs. The task scheduler ( §6) in Ansor uses a scheduling algorithm based on gradient descent to allocate resources to the subgraphs that are more likely to improve the end-to-end DNN performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Program Sampling</head><p>The search space an algorithm explores determines the best programs it can find. The considered search spaces in existing approaches are limited by the following factors: (1) Manual enumeration (e.g., TVM <ref type="bibr" target="#b10">[11]</ref>). It is impractical to manually enumerate all possible choices by templates, so existing manual templates only cover a limited search space heuristically.</p><p>(2) Aggressive early pruning (e.g., Halide auto-scheduler <ref type="bibr" target="#b1">[2]</ref>). Aggressive early pruning based on evaluating incomplete programs prevents the search algorithm from exploring certain regions in the space.</p><p>In this section, we introduce techniques to push the boundary of the considered search space by addressing the above limitations. To solve (1), we automatically expand the search space by recursively applying a set of flexible derivation rules. To avoid (2), we randomly sample complete programs in the search space. Since random sampling gives an equal chance to every point to be sampled, our search algorithm can potentially explore every program in the considered space. We do not rely on random sampling to find the optimal program, because every sampled program is later fined-tuned ( §5).</p><p>To sample programs that can cover a large search space, we define a hierarchical search space with two levels: sketch and annotation. We define the high-level structures of programs as sketches and leave billions of low-level choices (e.g., tile size, parallel, unroll annotations) as annotations. At the top level, we generate sketches by recursively applying a few derivation rules. At the bottom level, we randomly annotate these sketches to get complete programs. This representation summarizes a few basic structures from billions of low-level choices, enabling the flexible enumeration of high-level structures and efficient sampling of low-level details.</p><p>While Ansor covers both CPU and GPU, we explain the sampling process for CPUs in the rest of this section as an example. The sampling rules for GPUs are mostly the same with minor modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sketch Generation</head><p>The first column in Figure <ref type="figure" target="#fig_2">5</ref> shows two examples of the input of our problem. The input has three equivalent forms: the mathematical expression, the corresponding naive program got by directly expanding the loop indices, and the corresponding computational graph (directed acyclic graph, or DAG). where 0 ≤ 𝑖 &lt; 8, 0 ≤ 𝑗 &lt; 4, 0 ≤ 𝑘 &lt; 512, 0 ≤ 𝑙 &lt; 400</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* The corresponding naïve program:</head><p>for i in range( <ref type="formula">8</ref>): for l in range(400):</p><formula xml:id="formula_1">B[i, l] = max(A[i, l], 0.0) for i in range(8): for k in range(512): C[i, k] = B[i, k] if k &lt; 400 else 0 for i in range(8):</formula><p>for j in range( <ref type="formula">4</ref> To generate sketches for a DAG with multiple nodes, we visit all the nodes in a topological order and build the structure iteratively. For computation nodes that are compute-intensive and have a lot of data reuse opportunities (e.g., conv2d, matmul), we build basic tile and fusion structures for them as the sketch. For simple element-wise nodes (e.g., ReLU, elementwise add), we can safely inline them. Note that new nodes (e.g., caching nodes, layout transform nodes) may also be introduced to the DAG during the sketch generation.</p><p>We propose a derivation-based enumeration approach to generate all possible sketches by recursively applying several basic rules. This process takes a DAG as an input and returns a list of sketches. We define the State σ = (S, i), where S is the current partially generated sketch for the DAG, and i is the index of the current working node. The nodes in a DAG are sorted in a topological order from output to input. The derivation begins from the initial naive program and the last node, or the initial state σ = (naive program, index o f the last node). Then we try to apply all derivation rules to the states re-cursively. For each rule, if the current state satisfies the application condition, we apply the rule to σ = (S, i) and get σ = (S , i ) where i ≤ i. This way the index i (working node) decreases monotonically. A state becomes a terminal state when i = 0. During enumeration, multiple rules can be applied to one state to generate multiple succeeding states. One rule can also generate multiple possible succeeding states. So we maintain a queue to store all intermediate states. The process ends when the queue is empty. All σ.S in terminal states form a sketch list at the end of the sketch generation.</p><p>Table <ref type="table" target="#tab_1">1</ref> lists derivation rules we used for the CPU. We first provide the definition of the used predications and then describe the functionality of each rule. We statically analyze the computation definitions to get the values for these predications. IsStrictInliable(S, i) indicates if the node i in S is a simple element-wise operator that can always be inlined (e.g. element-wise add, ReLU). HasDataReuse(S, i) indicates if the node i in S is a compute-intensive operator and has plentiful data reuse opportunity (e.g., mat-mul, conv2d). HasFusibleConsumer(S, i) indicates if the node i in S has only one consumer j and node j can be fused into node i (e.g., matmul + bias_add, conv2d + relu). HasMoreReductionParallel(S, i) indicates if the node i in S has little parallelism in space dimensions but has ample parallelism opportunity in reduction dimensions. (e.g. computing 2-norm of a matrix, matmul C 2×2 = A 2×512 • B 512×2 ). Next we introduce the functionality of each derivation rule.</p><p>Rule 1 just simply skips a node if it is not strictly inlinable. Rule 2 always inlines strictly inlinable nodes. Since the conditions of rule 1 and rule 2 are mutually exclusive, a state with i &gt; 1 can always satisfy one of them and continue to derive.</p><p>Rules 3, 4, and 5 deal with the multi-level tiling and fusion for nodes that have data reuse. Rule 3 performs multi-level tiling for data reusable nodes. For CPU, we use a "SSRSRS" tile structure, where "S" stands for one tile level of space loops and "R" stands for one tile level of reduction loops. For example, in the matmul</p><formula xml:id="formula_2">C(i, j) = ∑ k A[i, k] × B[k, j</formula><p>], i and j are space loops and k is a reduction loop. The "SSRSRS" tile structure for matmul expands the original 3-level loop (i, j, k) into a 10-level loop (i 0 , j 0 , i 1 , j 1 , k 0 , i 2 , j 2 , k 1 , i 3 , j 3 ). Although we do not permute the loop order, this multi-level tiling can also cover reorder cases. For example, the above 10-level loop can be specialized to just a simple reorder (k 0 , j 2 , i 3 ) by setting the length of other loops to one. Rule 4 performs multi-level tiling and also fuse the fusible consumers. For example, we fuse the element-wise nodes (e.g., ReLU, bias add) into the tiled nodes (e.g., conv2d, matmul). Rule 5 adds a caching node if the current data-reusable node does not have a fusible consumer. For example, the final output node in a DAG does not have any consumer, so it directly writes results into main memory by default and this is inefficient due to the high latency of memory accesses. By adding a cache node, we introduce a new fusible consumer into the DAG, then rule 4 can be applied to fuse this newly added cache node into the final output node. With the cache node fused, now the final output node writes its results into a cache block, and the cache block will be written to the main memory at once when all data in the block is computed.</p><p>Rule 6 can use rfactor <ref type="bibr" target="#b40">[42]</ref> to factorize a reduction loop into a space loop to bring more parallelism.</p><p>Figure <ref type="figure" target="#fig_2">5</ref> shows three examples of the generated sketches. The sketches are different from the manual templates in TVM. Because the manual templates specify both high-level structures and low-level details while sketches only define highlevel structures. For the example input 1, the sorted order of the four nodes in the DAG is (A, B,C, D). To derive the sketches for the DAG, we start from output node D(i = 4) and apply rules to the nodes one by one. Specifically, the derivation for generated sketch 1 is:</p><formula xml:id="formula_3">Input 1 →σ(S 0 , i = 4) Rule 1 −−−→ σ(S 1 , i = 3) Rule 4 −−−→ σ(S 2 , i = 2) Rule 1 −−−→ σ(S 3 , i = 1) Rule 1 −−−→ Sketch 1</formula><p>For the example input 2, the sorted order of the five nodes is (A, B,C, D, E). Similarly, we start from the output node E(i = 5) and apply rules recursively. The generated sketch 2 is derived by:</p><formula xml:id="formula_4">Input 2 →σ(S 0 , i = 5) Rule 5 −−−→ σ(S 1 , i = 5) Rule 4 −−−→ σ(S 2 , i = 4) Rule 1 −−−→ σ(S 3 , i = 3) Rule 1 −−−→ σ(S 4 , i = 2) Rule 2 −−−→ σ(S 5 , i = 1) Rule 1 −−−→ Sketch 2</formula><p>Similarly, the generated sketch 3 is derived by:</p><formula xml:id="formula_5">Input 2 →σ(S 0 , i = 5) Rule 6 −−−→ σ(S 1 , i = 4) Rule 1 −−−→ σ(S 2 , i = 3) Rule 1 −−−→ σ(S 3 , i = 2) Rule 2 −−−→ σ(S 4 , i = 1) Rule 1 −−−→ Sketch 3</formula><p>While the presented rules are practical enough to cover the structures for most operators, there are always exceptions. For example, some special algorithms (e.g., Winograd convolution <ref type="bibr" target="#b25">[27]</ref>) and accelerator intrinsics (e.g., TensorCore <ref type="bibr" target="#b32">[34]</ref>) require special tile structures to be effective. Although the template-guided search approach (in TVM) can craft a new template for every new case, it needs a great amount of design effort. It is not straightforward for the sequential construction based search with a fixed predefined order to expand its construction sequence either. On the other hand, the derivation-based sketch generation in Ansor is flexible enough to generate the required structures for emerging algorithms and hardware, as we allow users to register new derivation rules and integrate them seamlessly with existing rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Random Annotation</head><p>The sketches generated by the previous subsection are incomplete programs because they only have tile structures without specific tile sizes and loop annotations, such as parallel, unroll, and vectorization. In this subsection, we annotate sketches to be complete programs for fine-tuning and evaluation.</p><p>Given a list of generated sketches, we randomly pick one sketch, randomly fill out tile sizes, parallelize some outer loops, vectorize some inner loops, and unroll a few inner loops. We also randomly change the computation location of some nodes in the program to make a slight tweak to the tile structure. If some special algorithms (e.g., Winograd convolution <ref type="bibr" target="#b25">[27]</ref>) require special annotation policy to be effective, we allow users to give simple hints in the computation definition to adjust the annotation policy. Finally, since it is valid to arbitrarily change the layout of constant tensors, we rewrite the layouts of the constant tensors according to the multi-level tile structure to make them most cache-friendly and eliminate layout transformation overheads between nodes. This optimization is effective because the weight tensors of convolution or dense layers are constants for inference applications.</p><p>Examples of random sampling are shown in Figure <ref type="figure" target="#fig_2">5</ref>. The sampled program might have a different number of loops than the sketch because the loops with length one are simplified. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance Fine-tuning</head><p>The programs sampled by the program sampler have good coverage of the search space, but their qualities are not guaranteed. This is because the optimization choices, such as tile structure and loop annotations, are all randomly sampled. In this section, we introduce the performance tuner that fine-tunes the performance of the sampled programs via evolutionary search and a learned cost model. The fine-tuning is performed iteratively. At each iteration, we first use evolutionary search to find a small batch of promising programs according to a learned cost model. We then measure these programs on hardware to get the actual execution time cost. Finally, the profiling data got from measurement is used to re-train the cost model to make it more accurate.</p><p>The evolutionary search uses randomly sampled programs as well as high-quality programs from the previous measurement as the initial population and applies mutation and crossover to generate the next generation. The learned cost model is used to predict the fitness of each program, which is the throughput of one program in our case. We run evolution for a fixed number of generations and pick the best programs found during the search. We leverage a learned cost model because the cost model can give relatively accurate estimations of the fitness of programs while being orders of magnitudes faster than the actual measurement. It allows us to compare tens of thousands of programs in the search space in seconds, and pick the promising ones to do actual measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evolutionary Search</head><p>Evolutionary search <ref type="bibr" target="#b47">[49]</ref> is a generic meta-heuristic algorithm inspired by biological evolution. By iteratively mutating highquality programs, we can generate new programs with potentially higher quality. The evolution starts from the sampled initial generation. To generate the next generation, we first select some programs from the current generation according to certain probabilities. The probability of selecting a program is proportional to its fitness predicted by the learned cost model ( §5.2), meaning that the program with a higher performance score has a higher probability to be selected. For the selected programs, we randomly apply one of the evolution operations to generate a new program. Basically, for every decision we made during sampling ( §4.2), we design corresponding evolution operations to rewrite and fine-tune it. We highlight some of them as follows.</p><p>Tile size mutation. This operation scans the program and randomly selects a tiled loop. For this tiled loop, it divides a tile size of one tile level by a random factor and multiplies this factor to another level. Since this operation keeps the product of tile sizes equal to the original loop length, the mutated program is always valid.</p><p>Parallel, vectorization mutation. This operation scans the program and randomly selects a loop annotated with parallel or vectorization. For this loop, this operation changes the granularity by either fusing its adjacent loop levels or splitting it by a factor.</p><p>Node-based crossover. Crossover is an operation to generate new offspring by combining the genes from two or more parents. The genes of a program in Ansor are its rewriting steps. Every program generated by Ansor is rewritten from its initial naive implementation. Ansor preserves a complete rewriting history for each program during sketch generation and random annotation. We can treat rewriting steps as the genes of a program because they describe how this program is formed from the initial naive one. Based on this, we can generate a new program by combining the rewriting steps of two existing programs. However, arbitrarily combining rewriting steps from two programs might break the dependencies in steps and create an invalid program. For example, a rewriting step annotates vectorization to the innermost loop, and the innermost loop may be generated by a previous tiling step. As a result, the granularity of crossover operation in Ansor is based on nodes in the DAG, because the rewriting steps for different nodes usually have less dependency. Ansor uses the cost model to estimate the performance scores of each node in the two programs and merges the rewriting steps of nodes with higher scores. When there are dependencies between nodes, Ansor try to analyze and adjust the steps with simple heuristics. Ansor further verifies the merged programs to guarantee the functional correctness. The verification is simple because Ansor only uses a small set of loop transformation rewriting steps, and the underlying code generator can check the correctness by dependency analysis.</p><p>The evolutionary search leverages mutation and crossover to generate a new set of candidates repeatedly for several rounds and outputs a small set of programs with the highest scores. These programs will be compiled and measured on the target hardware to obtain the real running time cost. The collected measurement data is then used to update the cost model. In this way, the accuracy of the learned cost model is gradually improved to match the target hardware. Consequently, the evolutionary search gradually generates higher-quality programs for the target hardware platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learned Cost Model</head><p>A cost model is necessary for estimating the performance of programs during the search. We adopt a learned cost model similar to related works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> with newly designed program features. A learned cost model has great portability because a single model design can be reused for different hardware targets by feeding in different training data.</p><p>Since our target programs are mainly data parallel tensor programs, which are made by multiple interleaved loop nests with several assignment statements as the innermost statements, we train the cost model to predict the score of one innermost non-loop statement in a loop nest. For a full program, we make predictions for each innermost non-loop statement and add the predictions up as the score. We build the feature vector for an innermost non-loop statement by extracting features in the context of a full program. The extracted features include arithmetic features and memory access features. A detailed list of extracted features is in the Appendix B.</p><p>We use weighted squared error as the loss function. Because we mostly care about identifying the well-performing programs from the search space, we put more weight on the programs that run faster. Specifically, the loss function of the model f on a program P with throughput y is loss( f , P, y) = w p (∑ s∈S(P) f (s) − y) 2 = y(∑ s∈S(P) f (s) − y) 2  where S(P) is the set of innermost non-loop statements in P. We directly use the throughput y as weight. We train a gradient boosting decision tree <ref type="bibr" target="#b7">[8]</ref> as the underlying model f . A single model is trained for all tensor programs coming from all DAGs, and we normalize the throughput of all programs come from the same DAG to be in the range of [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Task Scheduler</head><p>A DNN can be partitioned into many independent subgraphs (e.g., conv2d + relu). For some subgraphs, spending time in tuning them does not improve the end-to-end DNN performance significantly. This is due to two reasons: either (1) the subgraph is not a performance bottleneck, or (2) tuning brings only minimal improvement in the subgraph's performance.</p><p>To avoid wasting time on tuning unimportant subgraphs, Ansor dynamically allocates different amounts of time resources to different subgraphs. Take ResNet-50 for example, it has 29 unique subgraphs among all 50 convolution layers. Most of these subgraphs are convolution layers with different shapes configurations (input size, kernel size, stride, etc). We need to generate different programs for different convolution layers because the best tensor program depends on these shape configurations. We define a task as a process performed to generate high-performance programs for a subgraph. It means that optimizing a single DNN requires finishing dozens of tasks (e.g., 29 tasks for ResNet-50). In reality, we may even want to optimize a set of DNNs, which leads to even more tasks. A subgraph can also appear multiple times in a DNN or across different DNNs.</p><p>Ansor's task scheduler allocates time resources to tasks in an iterative manner. At each iteration, Ansor selects a task, generates a batch of promising programs for the subgraph, and measures the program on hardware. We define such an iteration as one unit of time resources. When we allocate one unit of time resources to a task, the task obtains an opportunity to generate and measure new programs, which also means the chance to find better programs. We next present the formulation of the scheduling problem and our solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Problem Formulation</head><p>When tuning a DNN or a set of DNNs, a user can have various types of goals, for example, reducing a DNN's latency, meeting latency requirements for a set of DNNs, minimizing tuning time when tuning no longer improves DNN performance significantly. We thus provide users a set of objective functions to express their goals. Users can also provide their own objective functions.</p><p>Suppose there are n tasks in total. Let t ∈ Z n be the allocation vector, where t i is the number of time units spent on task i. Let the minimum subgraph latency task i achieves be a function of the allocation vector g i (t). Let the end-to-end cost of the DNNs be a function of the latency of the subgraphs f (g 1 (t), g 2 (t), ..., g 3 (t)). Our objective is to minimize the end-to-end cost: minimize f (g 1 (t), g 2 (t), ..., g 3 (t)) To minimize the end-to-end latency of a single DNN, we can define f (g 1 , g 2 , ..., g n ) = ∑ n i=1 w i × g i , where w i is the number of appearances of task i in the DNN. This formulation is straightforward because f is an approximation of the end-to-end DNN latency.</p><p>When tuning a set of DNNs, there are several options. Table 2 shows a number of example objective functions for tuning multiple DNNs. Let m be the number of DNNs, S( j) is the set of tasks that belong to DNN j. f 1 adds up the latency of every DNN, which means to optimize the cost of a pipeline that sequentially runs all DNNs once. In f 2 , we define L j as the latency requirement of DNN j, meaning that we do not want to spend time on a DNN if its latency has already met the requirement. In f 3 , we define B j as the reference latency of a DNN j. As a result, our goal is to maximize the geometric mean of speedup against the given reference latency. Finally in f 4 , we define a function ES(g i ,t) that returns an</p><formula xml:id="formula_6">f 1 = ∑ m j=1 ∑ i∈S( j) w i × g i (t) f 2 = ∑ m j=1 max(∑ i∈S( j) w i × g i (t), L j ) f 3 = −(∏ m j=1 B j ∑ i∈S( j) w i ×g i (t) ) 1 m f 4 = ∑ m j=1 ∑ i∈S( j) w i × max(g i (t), ES(g i ,t</formula><p>)) Table <ref type="table">2</ref>: Examples of objective functions for multiple neural networks early stopping value by looking at the history of latency of task i. It can achieve the effect of per task early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Optimizing with Gradient Descent</head><p>We propose a scheduling algorithm based on gradient descent to efficiently optimize the objective function. Given the current allocation t, the idea is to approximate the gradient of the objective function ∂ f ∂t i in order to choose the task i such</p><formula xml:id="formula_7">that i = argmax i | ∂ f ∂t i |.</formula><p>We approximate the gradient by doing optimistic guess and considering the similarity between tasks. The derivation can be found in the Appendix A. We approximate the gradient by</p><formula xml:id="formula_8">∂ f ∂t i ≈ ∂ f ∂g i (α g i (t i ) − g i (t i − ∆t) ∆t + (1 − α)(min(− g i (t i ) t i , β C i max k∈N(i) V k − g i (t i ))))</formula><p>where ∆t is a small backward window size, g i (t i ) and g i (t i − ∆t) are known from the history allocations. N(i) is the set of similar tasks of i, C i is the number of floating point operations in task i and V k is the number of floating point operation per second we can achieve in task k. The parameter α and β controls the weight to trust some predictions.</p><p>To run the algorithm, Ansor starts from t = 0 and warms up with a round of round-robin to get an initial allocation vector t = (1, 1, ..., 1). After the warm-up, at each iteration, we compute the gradient of each task and pick argmax i | ∂ f ∂t i |. Then we allocate the resource unit to task i and update the allocation vector t i = t i + 1. The optimization process continues until we run out of the time budget. To encourage exploration, we adopt a ε-greedy strategy <ref type="bibr" target="#b41">[43]</ref>, which preserves a probability of ε to randomly select a task.</p><p>Take the case of optimizing for a single DNN's end-to-end latency for example, Ansor prioritizes a subgraph that has a high initial latency because our optimistic guess says we can reduce its latency quickly. Later, if Ansor spends many iterations on it without observing a decrease in its latency, Ansor leaves the subgraph because its</p><formula xml:id="formula_9">| ∂ f ∂t i | decreases.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>The core of Ansor is implemented in C++ with about 10K lines of code (3K for the search policy, 7K for other infrastructures). Ansor generates programs in its own intermediate representation (IR). These programs are then lowered to TVM IR for code generation targeting various hardware platforms. Ansor only utilizes TVM as a deterministic code generator. We evaluate Ansor on three levels: single operator, subgraph, and entire neural network. For each level of evaluation, we compare Ansor against the state-of-the-art search frameworks and hardware-specific manual libraries.</p><p>The generated tensor programs were benchmarked on three hardware platforms: an Intel CPU (20-core Platinum 8269CY@3.1 GHz), an NVIDIA GPU (V100 ), and an ARM CPU (4-core Cortex-A53@1.4GHz on the Raspberry Pi 3b+). We use float32 as the data type for all evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Single Operator Benchmark</head><p>We first evaluate Ansor on a set of common deep learning operators, including 1D, 2D and 3D convolution (C1D, C2D, C3D respectively), matrix multiplication (GMM), group convolution (GRP), dilated convolution (DIL) <ref type="bibr" target="#b50">[52]</ref>, depth-wise convolution (DEP) <ref type="bibr" target="#b20">[22]</ref>, transposed 2D convolution (T2D) <ref type="bibr" target="#b35">[37]</ref>, capsule 2D convolution (CAP) <ref type="bibr" target="#b19">[21]</ref> and matrix 2-norm (NRM). For each operator, we select 4 different shape configurations from common DNNs and evaluate them with two batch sizes (1 and 16). In total, there are 10 operators ×4 shape configurations ×2 batch size (= 80) test cases. We run these test cases on the Intel CPU.</p><p>We include PyTorch <ref type="bibr" target="#b34">[36]</ref>, Halide auto-scheduler <ref type="bibr" target="#b1">[2]</ref>, Flex-Tensor <ref type="bibr" target="#b51">[53]</ref> and AutoTVM <ref type="bibr" target="#b10">[11]</ref> as baseline frameworks.</p><p>PyTorch is backed by the vendor-provided kernel library MKL-DNN <ref type="bibr" target="#b22">[24]</ref>. Halide auto-scheduler is a sequential construction based search framework for Halide. AutoTVM and FlexTensor are template-guided search frameworks based on TVM. Since Halide auto-scheduler does not have a pre-trained cost model for AVX-512, we disabled AVX-512 for searchbased frameworks, while the MKL-DNN in PyTorch utilizes AVX-512 by default.</p><p>For each test case in this evaluation, we let search frameworks (i.e., Halide auto-scheduler, FlexTensor, AutoTVM, and Ansor) run search or auto-tuning with at most 1, 000 measurement trials. This means each framework can measure at most 80 × 1000 programs for auto-tuning in this evaluation. Using the same number of measurement trials makes it a fair comparison without involving implementation details. For a single operator, 1, 000 measurement trials are typically enough for the search to converge in these frameworks.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> shows the normalized performance. For each operator, we compute the geometric mean of the throughputs on four shapes and normalize the geometric means of all frameworks relative to the best one. As shown in the figure, Ansor performs the best on 19 out of 20 test cases. Ansor outperforms existing search frameworks by 1.1 − 32.7×. The performance improvements of Ansor come from both its large search space and effective exploration strategy. For most operators, we found the best program generated by Ansor is out-   side the search space of existing search frameworks because Ansor is able to explore more optimization combinations. For example, the significant speedup on NRM is because Ansor can parallelize reduction loop, while other frameworks do not. The large speedup on T2D is because Ansor can use correct tile structures and unrolling strategies to let the code generator simplify the multiplication of zeros in strided transposed convolution. In contrast, other frameworks fail to capture many effective optimizations in their search space, making them unable to find the programs that Ansor does. For example, the unfolding rules in Halide does not split the reduction loop in GMM and does not split reduction loops in C2D when padding is computed outside of reduction loops. The manual templates in AutoTVM have limited tile structures, as they cannot cover the structure of "Generated Sketch 1" in Figure <ref type="figure" target="#fig_2">5</ref>. The manual template in FlexTensor does not change the computation location of padding and has a fixed unrolling policy. Finally, for the only case (GMM with batch size 16) that Ansor performs worse than PyTorch, it is due to the disabling of AVX-512. Ansor can match PyTorch after utilizing AVX-512. Ablation study. We run four variants of Ansor on a convolution operator and report the performance curve. We pick the last convolution operator in ResNet-50 with batch size=16 as the test case, because it has a large enough interesting search space to evaluate the search algorithms. Other operators share a similar pattern. In Figure <ref type="figure" target="#fig_5">7</ref>, each curve is the Conv Layer @ C Conv Layer @ G TBG @ C TBG @ G 0.0 0.2 0.4 0.6 0.8 1.0 Batch size = 1 PyTorch Halide FlexTensor AutoTVM Ansor (ours)</p><p>Conv Layer @ C Conv Layer @ G TBG @ C TBG @ G 0.0 0.2 0.4 0.6 0.8 1.0 Batch size = 16 Normalized Performance Figure <ref type="figure">8</ref>: Subgraph performance benchmark on a 20-core Intel-Platinum-8269CY and an NVIDIA V100. "@C" denotes CPU results and "@G" denotes GPU results. The y-axis is the throughput normalized to the best throughput for each subgraph. median of 5 runs. "Ansor (ours)" uses all our introduced techniques. "Beam Search" means we prune incomplete programs with the cost model during the sampling process and do not use fine-tuning. "No fine-tuning" is based on "Ansor (ours)" but disables fine-tuning and only relies on random sampling. "Limited space" is also based on "Ansor (ours)" but limits the search space to make it similar to the space in existing manual templates. As demonstrated by Figure <ref type="figure" target="#fig_5">7</ref>, dropping either the large search space or efficient fine-tuning decreases the final performance significantly. The aggressive early pruning in "Beam search" throws away incomplete programs with good final performance due to inaccurate estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Subgraph Benchmark</head><p>We perform subgraph benchmark on two common subgraphs in DNNs. The "ConvLayer" is a subgraph consisting of 2D convolution, batch normalization <ref type="bibr" target="#b23">[25]</ref> and ReLU activation, which is a common pattern in convolutional neural networks. The "TBG" is a subgraph consisting of two matrix transposes and one batch matrix multiplication, which is a common pattern in the multi-head attention <ref type="bibr" target="#b44">[46]</ref> in language models. Similar to single operator benchmark, we select four different shape configurations and two batch sizes and run auto-tuning with up to 1, 000 measurement trails per test case. We use the same set of baseline frameworks and run the benchmark on the Intel CPU and the NVIDIA GPU. We do not report the performance of Halide auto-scheduler on GPU because its GPU support is still in an experimental stage.</p><p>Figure <ref type="figure">8</ref> shows that Ansor outperforms manual libraries and other search frameworks by 1.1 − 1.8×. Ansor can generate high-performance programs consistently for these subgraphs on both platforms. In comparison, other frameworks perform poorly on certain cases. For example, the template in FlexTensor is mainly designed for a single operator, so it lacks the ability to perform GPU kernel fusion in some cases. Relatively, FlexTensor performs worse on "ConvLayer@G" than on "TBG@G"" because it cannot fuse batch normalization and ReLU into the convolution operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">End-to-End Network Benchmark</head><p>We benchmark the end-to-end inference execution time of several DNNs, which include ResNet-50 <ref type="bibr" target="#b18">[20]</ref> and MobileNet-V2 <ref type="bibr" target="#b38">[40]</ref> for image classification, 3D-ResNet-18 <ref type="bibr">[19]</ref> for action recognition, DCGAN <ref type="bibr" target="#b35">[37]</ref> generator for image generation, and BERT <ref type="bibr" target="#b13">[14]</ref> for language understanding. We benchmark these DNNs on three hardware platforms. For the server-class Intel CPU and NVIDIA GPU, we report the results for batch size 1 and batch size 16. For the ARM CPU in the edge device, real-time feedback is typically desired, so we only report the results for batch size 1.</p><p>We include PyTorch, TensorFlow, TensorRT (TensorFlow integration) <ref type="bibr" target="#b33">[35]</ref>, TensorFlow Lite and AutoTVM as baseline frameworks. We do not include Halide auto-scheduler or FlexTensor because they lack the support of widely-used deep learning model formats (e.g., the formats in PyTorch and TensorFlow) and high-level graph optimizations. As a result, we expect that the end-to-end execution time they can achieve will be the sum of the latency of all subgraphs in a DNN. In contract, AutoTVM can optimize a whole DNN with its manual templates and various graph-level optimizations (e.g., graph-level layout search <ref type="bibr" target="#b27">[29]</ref>, graph-level constant folding <ref type="bibr" target="#b37">[39]</ref>). We let both AutoTVM and Ansor run auto-tuning with up to 1000 × n measurement trials on each DNN, where n is the number of subgraphs in the DNN. They have similar search overhead, so it roughly takes the same amount of time for them to do the same number of measurements. We set the objective of the task scheduler as minimizing the total latency of one DNN and generate programs for these test cases one by one. On the other hand, PyTorch, TensorFlow, TensorRT, and TensorFlow Lite are all backed by static kernel libraries (MKL-DNN on Intel CPU, CuDNN on NVIDIA GPU, and Eigen on ARM CPU) and do not need auto-tuning. We enabled AVX-512 for all frameworks on the CPU in this network benchmark.</p><p>Figure <ref type="figure">9</ref> shows the results on the Intel CPU, NVIDIA GPU and ARM CPU 2 . Compared with search-based AutoTVM, Ansor matches or outperforms it on all cases with 1.0 − 9.4× speedup. Compared with the best alternative, Ansor improves the execution performance of DNNs on the Intel CPU, ARM CPU, and NVIDIA GPU by up to 3.8×, 2.6×, and 1.7×, respectively. Overall, Ansor performs the best or equally the best on 24 out of 25 cases. The only exception is BERT with batch size 16 on the GPU. This is because BERT consists of many matrix multiplications with large input sizes. It is hard for compilation-based approaches to beat manually-written assembly code on large matrix multiplications <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b43">45]</ref>, as the code has been hand-optimized for decades.  Ablation study. We run variants of Ansor on two test cases in Figure <ref type="figure" target="#fig_6">10</ref>. In the left figure, we run four variants of Ansor to generate programs for a single mobilenet-V2. In the right figure, we run these variants for both mobilenet-V2 and ResNet-50. We set the objective function of the task scheduler to be the geometric mean of speedup against AutoTVM. As shown in Figure <ref type="figure" target="#fig_6">10</ref>, "No task scheduler" means we use a round-robin strategy to allocate equal time resources to all subgraphs. "Limited space" is based on "Ansor (ours)" but limits the search space. "No fine-tuning" is also based on "Ansor (ours)" but disables fine-tuning and relies on random sampling only. As can be seen in Figure <ref type="figure" target="#fig_6">10</ref>, "Limited space" performs the worst in terms of the final achieved performance, proving that the best programs are not included in the limited space. The final achieved performance can be improved by enlarging the search space, as depicted in "No fine-tuning". However, in the right figure, randomly assigning tile sizes and annotations still cannot beat AutoTVM in the given time budget. After enabling fine-tuning, "No task scheduler" outperforms Au-toTVM in both cases. Finally, "Ansor (ours)" employs the task scheduler to prioritize performance bottlenecks (e.g., subgraphs contain 3x3 convolution), so it performs the best in both search efficiency and the final achieved performance.</p><p>Search time. Ansor searches efficiently and can outperform or match AutoTVM with less search time. AutoTVM does not have a task scheduler so it generates programs for all subgraphs sequentially with a predefined budget of measurement trials. To get the reference results in Figure <ref type="figure" target="#fig_6">10</ref>, it requires around 30, 000 measurement trials for mobilenet-V2 and 50, 000 measurement trials for mobilenet-V2 and ResNet-50. However, Ansor can match its performance on these two cases with 10× less measurement trials, thanks to the task scheduler, efficient fine-tuning and comprehensive coverage of effective optimizations. As a reference, depending on the target platforms and the complexity of the subgraph, it takes about one to two seconds to compile one program and measure it with other search overhead amortized. Therefore, it takes several hours to generate programs for a DNN. This is acceptable for inference applications, because we only need to run program generation for the DNNs once before deployment. Comparing Ansor against AutoTVM on other network benchmark cases, we observe similar significant reductions in the wall clock search time up to 10×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Automatic tensor programs generation based on scheduling languages. Halide <ref type="bibr" target="#b36">[38]</ref> introduces a scheduling language that can describe loop optimization primitives. This language is suitable for both manual optimization and automatic search. Halide has three versions of auto-scheduler based on different techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b31">33]</ref>. The latest one with beam search and learned cost model performs the best among them, which is also used in our evaluation. TVM <ref type="bibr" target="#b9">[10]</ref> utilizes a similar scheduling language and includes a template-guided search framework AutoTVM <ref type="bibr" target="#b10">[11]</ref>. Similar to the motivation of this paper, FlexTensor <ref type="bibr" target="#b51">[53]</ref> attempts to reduce human efforts in writing templates. It proposes more general templates target-ing a set of operators so that the required number of templates could be reduced. Its templates are still manually designed, so it fails to cover certain operators and is lacking the support for some important optimizations (e.g., operator fusion).</p><p>Polyhedral compilation models. Polyhedral compilation model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b46">48]</ref> formulates the optimization of programs as an integer linear programming (ILP) problem. It optimizes a program with affine loop transformation that minimizes the data reuse distance between dependent statements. Tiramisu <ref type="bibr" target="#b4">[5]</ref> and TensorComprehensions <ref type="bibr" target="#b43">[45]</ref> are two polyhedral compilers that also target deep learning domain. Tiramisu provides a scheduling language similar to Halide language, and it needs manual scheduling. TensorComprehensions can search for GPU code automatically, but it is not yet meant to be used for compute-bounded problems <ref type="bibr" target="#b9">[10]</ref>. It cannot outperform TVM on operators like conv2d and matmul <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">44]</ref>. This is because of the lack of certain optimizations and the inaccurate implicit cost model in the polyhedral formulation.</p><p>Graph level optimization for deep learning. Graph level optimizations treat an operator in the computational graph as a basic unit and perform optimization at graph level without changing the internal implementations of operators. The common optimizations at graph level include layout optimizations <ref type="bibr" target="#b27">[29]</ref>, operator fusion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">35]</ref>, constant folding <ref type="bibr" target="#b37">[39]</ref>, auto-batching <ref type="bibr" target="#b28">[30]</ref>, automatic generation of graph substitution <ref type="bibr" target="#b24">[26]</ref> and so forth. The graph-level optimizations are typically orthogonal to operator-level optimizations. It can also benefit from high-performance implementations of operators. For example, general operator fusion relies on the code generation ability of Ansor. We leave the joint optimization of Ansor and more graph level optimization as future work.</p><p>Search-based compilation and auto-tuning. Search based compilation and auto-tuning have already shown its effectiveness in domains other than deep learning. Stock <ref type="bibr" target="#b39">[41]</ref> is a super-optimizer based on random search. Stock searches for loop-free hardware instruction sequences, while Ansor generates tensor programs with nests of loops. OpenTuner <ref type="bibr" target="#b3">[4]</ref> is a general framework for program auto-tuning based on multiarmed bandit approaches. OpenTuner relies on user-specified search space, while Ansor constructs the search space automatically. Traditional high-performance libraries such as ATLAS <ref type="bibr" target="#b49">[51]</ref> and FFTW <ref type="bibr" target="#b15">[16]</ref> also utilizes auto-tuning. More recent works NeuroVectorizer <ref type="bibr" target="#b16">[17]</ref> and AutoPhase <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">23]</ref> use deep reinforcement learning to automatically vectorize programs and optimize the compiler phase ordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We proposed Ansor, an automated search framework that generates high-performance tensor programs for deep neural networks. By efficiently exploring a large search space, Ansor finds high-performance programs that are outside the search space of existing approaches. Ansor outperforms existing manual libraries and search-based frameworks on a diverse set of neural networks and hardware platforms by up to 3.8×. All of Ansor's source code will be publicly available. forests with deep reinforcement learning. In Third Conference on Machine Learning and Systems (ML-Sys), 2020.</p><p>[19] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.</p><p>Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6546-6555, 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Gradient Approximation for the Task Scheduler</head><p>Now we show how to approximate the gradient for the objective function f . First, do the approximation g i (t) ≈ g i (t i ). This means we assume the best cost of task i depends only on the resource units spent on it. This may not be true because all tasks share a cost model. Different resource allocations lead to different collections of training data, which then leads to different cost models. Here we make this approximation to continue derivation:</p><formula xml:id="formula_10">∂ f ∂t i = ∂ f ∂g i ∂g i ∂t i ≈ ∂ f ∂g i (α g i (t i ) − g i (t i − ∆t) ∆t + (1 − α) g i (t i + ∆t) − g i (t i ) ∆t ) ≈ ∂ f ∂g i (α g i (t i ) − g i (t i − ∆t) ∆t + (1 − α)(g i (t i + 1) − g i (t i )))</formula><p>In this expression, ∆t is a small backward window size, g i (t i ) and g i (t i − ∆t) are known from the history allocations. But g i (t i + 1) is unknown because we have not allocated t i + 1 units of resource to this task. So we have to predict this value. The parameter α controls the weight to trust the prediction. We predict g i (t i + 1) in two ways. First, we have an optimistic guess that if we spend extra t i , we can decrease the latency of task i to 0. This means g i (t i + 1) ≈ g i (t i ) − g i (t i ) t i . Second, if subgraphs are structurally similar, their latency is also similar per floating point operation. Considering both factors, we have the following approximation:</p><formula xml:id="formula_11">g i (t i + 1) ≈ min(g i (t i ) − g i (t i ) t i , β C i max k∈N(i) V k )</formula><p>where N(i) is the set of similar tasks of i, C i is the number of floating point operations in task i and V k is the number of</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Search strategy comparison. The pseudo-code shows tensor programs with loop nests. The orange question marks denote low-level parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Generated sketch 3 4 *</head><label>34</label><figDesc>parallel i in range(8): for k in range(512): C[i, k] = ... for j in range(4): unroll k_o in range(32): vectorized k_i in range(16): E.rf[...] += C[...] * D[...] parallel i in range(8): for j in range(4): unroll k_i in range(16): E[...] += E.rf[...] Sampled program The mathmetical expression: 𝐵 𝑖, 𝑙 = max (𝐴 𝑖, 𝑙 , 0.0) 𝐶[𝑖, 𝑘] = &gt; 𝐵[𝑖, 𝑘], 𝑘 &lt; 400 0 , 𝑘 ≥ 400 𝐸 𝑖, 𝑗 = &amp; 𝐶[𝑖, 𝑘] , × 𝐷[𝑘, 𝑗]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of generated sketches and sampled programs. This figure shows two example inputs, three generated sketches and four sampled programs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Single operator performance benchmark on a 20core Intel-Platinum-8269CY. The y-axis is the throughput normalized to the best throughput for each operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Ablation study of four variants of Ansor on a convolution operator. The y-axis is the throughput relative to the throughput of the best program.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Network performance auto-tuning curve. The yaxis is the speedup relative to AutoTVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Derivation rules used to generate sketches. The condition runs on the current state σ = (S, i). The application derives the next state σ = (S , i ) from the current state σ. Note that some function (e.g., AddR f actor, FuseConsumer) can return multiple possible values of S . In this case we collect all possible S , and return multiple next states σ for a single input state σ.</figDesc><table><row><cell cols="2">No Rule Name</cell><cell>Condition</cell><cell>Application</cell></row><row><cell>1</cell><cell>Skip</cell><cell>¬IsStrictInlinable(S, i)</cell><cell>S = S; i = i − 1</cell></row><row><cell>2</cell><cell>Always Inline</cell><cell>IsStrictInlinable(S, i)</cell><cell>S = Inline(S, i); i = i − 1</cell></row><row><cell>3</cell><cell>Multi-level Tiling</cell><cell>HasDataReuse(S, i)</cell><cell>S = MultiLevelTiling(S, i); i = i − 1</cell></row><row><cell>4</cell><cell cols="2">Multi-level Tiling with Fusion HasDataReuse(S, i) ∧ HasFusibleConsumer(S, i)</cell><cell>S = FuseConsumer(MultiLevelTiling(S, i), i); i = i − 1</cell></row><row><cell>5</cell><cell>Add Cache Stage</cell><cell cols="2">HasDataReuse(S, i) ∧ ¬HasFusibleConsumer(S, i) S = AddCacheW rite(S, i); i = i</cell></row><row><cell>6</cell><cell>Reduction Factorization</cell><cell>HasMoreReductionParallel(S, i)</cell><cell>S = AddR f actor(S, i); i = i − 1</cell></row><row><cell>...</cell><cell>User Defined Rule</cell><cell>...</cell><cell>...</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>3D-ResNet and DCGAN are not yet supported by TensorFlow Lite on the ARM CPU.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PyTorch</cell><cell>TensorFlow</cell><cell>AutoTVM</cell><cell>Ansor (ours)</cell></row><row><cell cols="2">Normalized Performance</cell><cell cols="3">0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>ResNet-50 Mobilenet V2 3D-ResNet DCGAN Batch size = 1 ResNet-50 Mobilenet V2 3D-ResNet DCGAN Batch size = 16</cell><cell>BERT BERT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Intel CPU</cell></row><row><cell></cell><cell></cell><cell cols="4">PyTorch</cell><cell>TensorFlow</cell><cell>TensorRT-TF</cell><cell>AutoTVM</cell><cell>Ansor (ours)</cell></row><row><cell cols="3">Normalized Performance</cell><cell cols="2">0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>ResNet-50 Mobilenet V2 3D-ResNet DCGAN Batch size = 1 ResNet-50 Mobilenet V2 3D-ResNet DCGAN Batch size = 16</cell><cell>BERT BERT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) NVIDIA GPU</cell></row><row><cell cols="2">Normalized Performance</cell><cell cols="3">0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>ResNet-50 Mobilenet V2 3D-ResNet DCGAN Batch size = 1 TensorFlow Lite AutoTVM Ansor (ours) BERT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) ARM CPU</cell></row><row><cell cols="6">Figure 9: Network inference performance benchmark on three</cell></row><row><cell cols="6">hardware platforms. The y-axis is the throughput relative to</cell></row><row><cell cols="6">the best throughput for each network.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ansor (ours) No task scheduler</cell><cell>No fine-tuning Limited space</cell><cell>AutoTVM</cell></row><row><cell>Relative Speedup</cell><cell cols="3">0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6</cell><cell cols="2">0 3000 6000 9000 12000 # Measurement trials 0 6000 12000 18000 24000 0.0 1.4 0.6 0.8 1.0 1.2 Mobilenet V2 0.2 0.4 Mobilenet V2 + ResNet-50</cell></row></table><note>2 </note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">recall@k of top-k = |G∩P| k , where G is the set of top-k program according to the ground truth and P is the set of top-k programs predicted by the model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sampled program 1 parallel i.2 in range(16): for j.2 in range(128): for k.1 in range(512): for i.3 in range(32): vectorize j.3 in range(4): C[...] += A[...] * B[...] parallel i.4 in range(512): for j.4 in range(512): D[...] = max(C[...], 0.0) Sampled program 2 for i.0 in range(TILE_I0): for j.0 in range(TILE_J0): for i.1 in range(TILE_I1): for j.1 in range(TILE_J1): for k.0 in range(TILE_K0): for i.2 in range(TILE_I2): for j.2 in range(TILE_J2): for k.1 in range(TILE_I1): for i.3 in range(TILE_I3): for j.3 in range(TILE_J3): C[...] += A[...] * B[...] for i.4 in range(TILE_I2 * TILE_I3): for j.4 in range(TILE_J2 * TILE_J3): D[...] = max(C[...], 0.0) Generated sketch 1 for i in range(8): for k in range(512): C[i, k] = max(A[i, k], 0.0) if k &lt; 400 else 0 for i in range(8): for j in range(4): for k_o in range(TILE_K0): for k_i in range(TILE_KI): E.rf[...] += C[...] * D[...] for i in range(8): for j in range(4): for k_i in range(TILE_KI): E[...] += E.rf[...]</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for i in range(512): for j in range(512): for k in range(512): C[i, j] += A[i, k] * B[k, j] for i in range(512): for j in range(512): D[i, j] = max(C[i, j], 0.0</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>floating point operation per second we can achieve in task k. The parameter β controls the weight to trust the prediction based on similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B The List of Extracted Features</head><p>We extract the following features for one innermost non-loop statement in the context of a full tensor program. The features include categorical features and numerical features. We use one-hot encoding to encode category features. The length of a feature vector including all the listed features for one statement is 164. We use the same set of features for both CPU and GPU. . We compute the arithmetic intensity for each loop level and draw a curve with linear interpolation. Then we sample 10 points from this curve.</p><p>• Buffer Access Feature For each buffer this statement accesses, we extract features for it. While different statement can access different numbers of buffers, we perform feature extraction for at most five buffers. We pad zeros if a statement accesses less then five buffers and remove small buffers if a statement accesses more than five buffers.</p><p>-Access type. The type of access (read, write, read + write).</p><p>-Bytes. The total number of bytes accessed by this statement.</p><p>-Unique bytes. The total number of unique bytes accessed by this statement. -Lines. The total number of cache lines accessed by this statement.</p><p>-Unique lines. The total number of unique cache lines accessed by this statement.</p><p>-Reuse type. The type of data reuse (LoopMulti-pleRead, SerialMultipleRead, NoReuse).</p><p>-Reuse distance. The distance between data reuse in terms of number of for loop iterations and total accessed bytes.</p><p>-Reuse counter. The number of the happening of data reuse.</p><p>-Stride. The stride of access.</p><p>-Accessed bytes divided by reuse. We compute the following values: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to optimize halide with tree search and random programs</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karima</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayvon</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Augmented reality meets deep learning for car instance segmentation in urban scenes</title>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Abu Alhaija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British machine vision conference</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Opentuner: An extensible framework for program autotuning</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Bosboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Una-May O'</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Parallel architectures and compilation</title>
				<meeting>the 23rd international conference on Parallel architectures and compilation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="303" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tiramisu: A polyhedral compiler for expressing fast and portable code</title>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malek</forename><surname>Ben Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Del Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdurrahman</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<imprint>
			<biblScope unit="page" from="193" to="205" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine learning systems are stuck in a rut</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Hot Topics in Operating Systems</title>
				<meeting>the Workshop on Hot Topics in Operating Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A practical automatic polyhedral parallelizer and locality optimizer</title>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Hartono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jagannathan</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuswamy</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
				<meeting>the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3389" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Analyzing bandit-based adaptive operator selection mechanisms</title>
		<author>
			<persName><forename type="first">Álvaro</forename><surname>Fialho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">Da</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Schoenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fftw: An adaptive software architecture for the fft</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">G</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<title level="s">ICASSP&apos;98 (Cat. No. 98CH36181</title>
		<meeting>the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1381" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neurovectorizer: end-to-end vectorization with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Willke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization</title>
				<meeting>the 18th ACM/IEEE International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="242" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Krste Asanovic, and Ion Stoica</title>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wawrzynek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autophase: Juggling hls phase orderings in random</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Matrix capsules with em routing</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Autophase: Compiler phase-ordering for hls with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Qijing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wawrzynek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="308" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Intel R math kernel library for deep learning networks</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Taso: optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
				<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4013" to="4021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Differentiable programming for image processing and deep learning in halide</title>
		<author>
			<persName><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimizing {CNN} model inference on cpus</title>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 {USENIX} Annual Technical Conference ({USENIX}{ATC} 19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1025" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep learning with dynamic computation graphs</title>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Looks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Herreshoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delesley</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02181</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Speech understanding systems: Report of a steering committee</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franklin</forename><forename type="middle">S</forename><surname>Medress</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><forename type="middle">W</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><surname>Forgie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Klatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">P</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Neuburg</surname></persName>
		</author>
		<author>
			<persName><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><surname>Ritea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="316" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A hardware-software blueprint for flexible deep learning specialization</title>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="8" to="16" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatically scheduling halide image processing pipelines</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Teja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mullapudi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayvon</forename><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Nvidia. Nvidia tensor cores</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<title level="m">Nvidia tensorrt: Programmable inference accelerator</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Relay: A high-level ir for deep learning</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marisa</forename><surname>Kirisame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08368</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stochastic superoptimization</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Schkufza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Parallel associative reductions in halide</title>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<imprint>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="2017">2017. 2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Triton: an intermediate language and compiler for tiled neural network computations</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
				<meeting>the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<title level="m">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Presburger formulas and polyhedral compilation</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Polyhedral parallel code generation for cuda</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Juega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">Ignacio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Tenllado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francky</forename><surname>Catthoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms: A critical review and its future prospects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pradnya</surname></persName>
		</author>
		<author>
			<persName><surname>Vikhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International conference on global trends in signal processing, information computing and communication (ICGTSPICC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A unified optimization approach for cnn model inference on integrated gpus</title>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Conference on Parallel Processing</title>
				<meeting>the 48th International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automatically tuned linear algebra software</title>
		<author>
			<persName><forename type="first">Clinton</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;98: Proceedings of the 1998 ACM/IEEE conference on Supercomputing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="38" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="859" to="873" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
