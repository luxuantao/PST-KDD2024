<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-04-19">19 Apr 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Zico Kolter</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
						</author>
						<title level="a" type="main">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-04-19">19 Apr 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1803.01271v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning practitioners commonly regard recurrent architectures as the default starting point for sequence modeling tasks. The sequence modeling chapter in the canonical textbook on deep learning is titled "Sequence Modeling: Recurrent and Recursive Nets" <ref type="bibr" target="#b22">(Goodfellow et al., 2016)</ref>, capturing the common association of sequence modeling and recurrent architectures. A well-regarded recent online course on "Sequence Models" focuses exclusively on recurrent architectures <ref type="bibr" target="#b54">(Ng, 2018)</ref>.</p><p>On the other hand, recent research indicates that certain convolutional architectures can reach state-of-the-art accuracy in audio synthesis, word-level language modeling, and machine translation <ref type="bibr">(van den Oord et al., 2016;</ref><ref type="bibr" target="#b37">Kalchbrenner et al., 2016;</ref><ref type="bibr">Dauphin et al., 2017;</ref><ref type="bibr" target="#b20">Gehring et al., 2017a;</ref><ref type="bibr">b)</ref>. This raises the question of whether these successes of convolutional sequence modeling are confined to specific application domains or whether a broader reconsideration of the association between sequence processing and recurrent networks is in order.</p><p>We address this question by conducting a systematic empirical evaluation of convolutional and recurrent architectures on a broad range of sequence modeling tasks. We specifically target a comprehensive set of tasks that have been repeatedly used to compare the effectiveness of different recurrent network architectures. These tasks include polyphonic music modeling, word-and character-level language modeling, as well as synthetic stress tests that had been deliberately designed and frequently used to benchmark RNNs. Our evaluation is thus set up to compare convolutional and recurrent approaches to sequence modeling on the recurrent networks' "home turf".</p><p>To represent convolutional networks, we describe a generic temporal convolutional network (TCN) architecture that is applied across all tasks. This architecture is informed by recent research, but is deliberately kept simple, combining some of the best practices of modern convolutional architectures. It is compared to canonical recurrent architectures such as LSTMs and GRUs.</p><p>The results suggest that TCNs convincingly outperform baseline recurrent architectures across a broad range of sequence modeling tasks. This is particularly notable because the tasks include diverse benchmarks that have commonly been used to evaluate recurrent network designs <ref type="bibr" target="#b11">(Chung et al., 2014;</ref><ref type="bibr" target="#b57">Pascanu et al., 2014;</ref><ref type="bibr" target="#b35">Jozefowicz et al., 2015;</ref><ref type="bibr" target="#b74">Zhang et al., 2016)</ref>. This indicates that the recent successes of convolutional architectures in applications such as audio processing are not confined to these domains.</p><p>To further understand these results, we analyze more deeply the memory retention characteristics of recurrent networks. We show that despite the theoretical ability of recurrent architectures to capture infinitely long history, TCNs exhibit substantially longer memory, and are thus more suitable for domains where a long history is required.</p><p>To our knowledge, the presented study is the most extensive systematic comparison of convolutional and recurrent architectures on sequence modeling tasks. The results suggest that the common association between sequence modeling and recurrent networks should be reconsidered. The TCN architecture appears not only more accurate than canonical recurrent networks such as LSTMs and GRUs, but also simpler and clearer. It may therefore be a more appropriate starting point in the application of deep networks to sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Convolutional networks <ref type="bibr" target="#b45">(LeCun et al., 1989)</ref> have been applied to sequences for decades <ref type="bibr" target="#b61">(Sejnowski &amp; Rosenberg, 1987;</ref><ref type="bibr" target="#b30">Hinton, 1989)</ref>. They were used prominently for speech recognition in the 80s and 90s <ref type="bibr" target="#b67">(Waibel et al., 1989;</ref><ref type="bibr" target="#b5">Bottou et al., 1990)</ref>. ConvNets were subsequently applied to NLP tasks such as part-of-speech tagging and semantic role labelling <ref type="bibr" target="#b13">(Collobert &amp; Weston, 2008;</ref><ref type="bibr" target="#b14">Collobert et al., 2011;</ref><ref type="bibr" target="#b17">dos Santos &amp; Zadrozny, 2014)</ref>. More recently, convolutional networks were applied to sentence classification <ref type="bibr" target="#b36">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b38">Kim, 2014)</ref> and document classification <ref type="bibr" target="#b75">(Zhang et al., 2015;</ref><ref type="bibr" target="#b15">Conneau et al., 2017;</ref><ref type="bibr" target="#b33">Johnson &amp; Zhang, 2015;</ref><ref type="bibr">2017)</ref>. Particularly inspiring for our work are the recent applications of convolutional architectures to machine translation <ref type="bibr" target="#b37">(Kalchbrenner et al., 2016;</ref><ref type="bibr" target="#b20">Gehring et al., 2017a;</ref><ref type="bibr">b)</ref>, audio synthesis (van den <ref type="bibr">Oord et al., 2016)</ref>, and language modeling <ref type="bibr">(Dauphin et al., 2017)</ref>.</p><p>Recurrent networks are dedicated sequence models that maintain a vector of hidden activations that are propagated through time <ref type="bibr" target="#b19">(Elman, 1990;</ref><ref type="bibr" target="#b68">Werbos, 1990;</ref><ref type="bibr" target="#b24">Graves, 2012)</ref>. This family of architectures has gained tremendous popularity due to prominent applications to language modeling <ref type="bibr" target="#b64">(Sutskever et al., 2011;</ref><ref type="bibr" target="#b25">Graves, 2013;</ref><ref type="bibr" target="#b29">Hermans &amp; Schrauwen, 2013)</ref> and machine translation <ref type="bibr" target="#b65">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b3">Bahdanau et al., 2015)</ref>. The intuitive appeal of recurrent modeling is that the hidden state can act as a representation of everything that has been seen so far in the sequence. Basic RNN architectures are notoriously difficult to train <ref type="bibr" target="#b4">(Bengio et al., 1994;</ref><ref type="bibr" target="#b56">Pascanu et al., 2013)</ref> and more elaborate architectures are commonly used instead, such as the LSTM <ref type="bibr" target="#b31">(Hochreiter &amp; Schmidhuber, 1997)</ref> and the GRU <ref type="bibr" target="#b10">(Cho et al., 2014)</ref>. Many other architectural innovations and training techniques for recurrent networks have been introduced and continue to be actively explored <ref type="bibr" target="#b18">(El Hihi &amp; Bengio, 1995;</ref><ref type="bibr" target="#b60">Schuster &amp; Paliwal, 1997;</ref><ref type="bibr" target="#b22">Gers et al., 2002;</ref><ref type="bibr" target="#b40">Koutnik et al., 2014;</ref><ref type="bibr" target="#b43">Le et al., 2015;</ref><ref type="bibr" target="#b2">Ba et al., 2016;</ref><ref type="bibr" target="#b70">Wu et al., 2016;</ref><ref type="bibr" target="#b42">Krueger et al., 2017;</ref><ref type="bibr" target="#b50">Merity et al., 2017;</ref><ref type="bibr" target="#b8">Campos et al., 2018)</ref>.</p><p>Multiple empirical studies have been conducted to evaluate the effectiveness of different recurrent architectures. These studies have been motivated in part by the many degrees of freedom in the design of such architectures. <ref type="bibr" target="#b11">Chung et al. (2014)</ref> compared different types of recurrent units (LSTM vs. GRU) on the task of polyphonic music modeling. <ref type="bibr" target="#b57">Pascanu et al. (2014)</ref> explored different ways to construct deep RNNs and evaluated the performance of different architectures on polyphonic music modeling, character-level language modeling, and word-level language modeling. <ref type="bibr" target="#b35">Jozefowicz et al. (2015)</ref> searched through more than ten thousand different RNN architectures and evaluated their performance on various tasks. They concluded that if there were "architectures much better than the LSTM", then they were "not trivial to find". <ref type="bibr" target="#b26">Greff et al. (2017)</ref> benchmarked the performance of eight LSTM variants on speech recognition, handwriting recognition, and polyphonic music modeling. They also found that "none of the variants can improve upon the standard LSTM architecture significantly". <ref type="bibr" target="#b74">Zhang et al. (2016)</ref> systematically analyzed the connecting architectures of RNNs and evaluated different architectures on characterlevel language modeling and on synthetic stress tests. <ref type="bibr" target="#b49">Melis et al. (2018)</ref> benchmarked LSTM-based architectures on word-level and character-level language modeling, and concluded that "LSTMs outperform the more recent models".</p><p>Other recent works have aimed to combine aspects of RNN and CNN architectures. This includes the Convolutional LSTM <ref type="bibr" target="#b62">(Shi et al., 2015)</ref>, which replaces the fully-connected layers in an LSTM with convolutional layers to allow for additional structure in the recurrent layers; the Quasi-RNN model <ref type="bibr" target="#b7">(Bradbury et al., 2017)</ref> that interleaves convolutional layers with simple recurrent layers; and the dilated RNN <ref type="bibr" target="#b9">(Chang et al., 2017)</ref>, which adds dilations to recurrent architectures. While these combinations show promise in combining the desirable aspects of both types of architectures, our study here focuses on a comparison of generic convolutional and recurrent architectures.</p><p>While there have been multiple thorough evaluations of RNN architectures on representative sequence modeling tasks, we are not aware of a similarly thorough comparison of convolutional and recurrent approaches to sequence modeling. <ref type="bibr" target="#b72">(Yin et al. (2017)</ref> have reported a comparison of convolutional and recurrent networks for sentence-level and document-level classification tasks. In contrast, sequence modeling calls for architectures that can synthesize whole sequences, element by element.) Such comparison is particularly intriguing in light of the aforementioned recent success of convolutional architectures in this domain. Our work aims to compare generic convolutional and recurrent architectures on typical sequence modeling tasks that are commonly used to benchmark RNN variants themselves <ref type="bibr" target="#b29">(Hermans &amp; Schrauwen, 2013;</ref><ref type="bibr" target="#b43">Le et al., 2015;</ref><ref type="bibr" target="#b35">Jozefowicz et al., 2015;</ref><ref type="bibr" target="#b74">Zhang et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Convolutional Networks</head><p>We begin by describing a generic architecture for convolutional sequence prediction. Our aim is to distill the best practices in convolutional network design into a simple architecture that can serve as a convenient but powerful starting point. We refer to the presented architecture as a temporal convolutional network (TCN), emphasizing that we adopt this term not as a label for a truly new architecture, but as a simple descriptive term for a family of architectures. (Note that the term has been used before <ref type="bibr" target="#b44">(Lea et al., 2017)</ref>.) The distinguishing characteristics of TCNs are: 1) the convolutions in the architecture are causal, meaning that there is no information "leakage" from future to past; 2) the architecture can take a sequence of any length and map it to an output sequence of the same length, just as with an RNN. Beyond this, we emphasize how to build very long effective history sizes (i.e., the ability for the networks to look very far into the past to make a prediction) using a combination of very deep networks (augmented with residual layers) and dilated convolutions.</p><p>Our architecture is informed by recent convolutional architectures for sequential data <ref type="bibr">(van den Oord et al., 2016;</ref><ref type="bibr" target="#b37">Kalchbrenner et al., 2016;</ref><ref type="bibr">Dauphin et al., 2017;</ref><ref type="bibr" target="#b20">Gehring et al., 2017a;</ref><ref type="bibr">b)</ref>, but is distinct from all of them and was designed from first principles to combine simplicity, autoregressive prediction, and very long memory. For example, the TCN is much simpler than WaveNet (van den <ref type="bibr">Oord et al., 2016)</ref> (no skip connections across layers, conditioning, context stacking, or gated activations).</p><p>Compared to the language modeling architecture of <ref type="bibr">Dauphin et al. (2017)</ref>, TCNs do not use gating mechanisms and have much longer memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sequence Modeling</head><p>Before defining the network structure, we highlight the nature of the sequence modeling task. Suppose that we are given an input sequence x 0 , . . . , x T , and wish to predict some corresponding outputs y 0 , . . . , y T at each time. The key constraint is that to predict the output y t for some time t, we are constrained to only use those inputs that have been previously observed: x 0 , . . . , x t . Formally, a sequence modeling network is any function f : X T +1 → Y T +1 that produces the mapping ŷ0 , . . . , ŷT = f (x 0 , . . . , x T )</p><p>(1)</p><p>if it satisfies the causal constraint that y t depends only on x 0 , . . . , x t and not on any "future" inputs x t+1 , . . . , x T . The goal of learning in the sequence modeling setting is to find a network f that minimizes some expected loss between the actual outputs and the predictions, L(y 0 , . . . , y T , f (x 0 , . . . , x T )), where the sequences and outputs are drawn according to some distribution.</p><p>This formalism encompasses many settings such as autoregressive prediction (where we try to predict some signal given its past) by setting the target output to be simply the input shifted by one time step. It does not, however, directly capture domains such as machine translation, or sequenceto-sequence prediction in general, since in these cases the entire input sequence (including "future" states) can be used to predict each output (though the techniques can naturally be extended to work in such settings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Causal Convolutions</head><p>As mentioned above, the TCN is based upon two principles: the fact that the network produces an output of the same length as the input, and the fact that there can be no leakage from the future into the past. To accomplish the first point, the TCN uses a 1D fully-convolutional network (FCN) architecture <ref type="bibr" target="#b47">(Long et al., 2015)</ref>, where each hidden layer is the same length as the input layer, and zero padding of length (kernel size − 1) is added to keep subsequent layers the same length as previous ones. To achieve the second point, the TCN uses causal convolutions, convolutions where an output at time t is convolved only with elements from time t and earlier in the previous layer.</p><p>To put it simply: TCN = 1D FCN + causal convolutions.</p><p>Note that this is essentially the same architecture as the time delay neural network proposed nearly 30 years ago by <ref type="bibr" target="#b67">Waibel et al. (1989)</ref>, with the sole tweak of zero padding to ensure equal sizes of all layers.</p><p>A major disadvantage of this basic design is that in order to achieve a long effective history size, we need an extremely deep network or very large filters, neither of which were particularly feasible when the methods were first introduced. Thus, in the following sections, we describe how techniques from modern convolutional architectures can be integrated into a TCN to allow for both very deep networks and very long effective history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dilated Convolutions</head><p>A simple causal convolution is only able to look back at a history with size linear in the depth of the network. This makes it challenging to apply the aforementioned causal convolution on sequence tasks, especially those requiring longer history. Our solution here, following the work of <ref type="bibr">van den Oord et al. (2016)</ref>, is to employ dilated convolutions that enable an exponentially large receptive field <ref type="bibr" target="#b73">(Yu &amp; Koltun, 2016)</ref>. More formally, for a 1-D sequence input x ∈ R n and a filter f : {0, . . . , k − 1} → R, the dilated convolution operation F on element s of the sequence is defined as where d is the dilation factor, k is the filter size, and s − d • i accounts for the direction of the past. Dilation is thus equivalent to introducing a fixed step between every two adjacent filter taps. When d = 1, a dilated convolution reduces to a regular convolution. Using larger dilation enables an output at the top level to represent a wider range of inputs, thus effectively expanding the receptive field of a ConvNet.</p><formula xml:id="formula_0">F (s) = (x * d f )(s) = k−1 i=0 f (i) • x s−d•i (2) x 0 x 1 x T 1 x T . . . ŷ1 ŷ0 ŷT ŷT 1 . . . Input Hidden Hidden Output d = 1 d = 2 d = 4 x 2 x T 2 ŷT 2 ŷ2 Residual block (k, d) 1x1 Conv (optional) WeightNorm Dilated Causal Conv ReLU Dropout WeightNorm Dilated Causal Conv ReLU Dropout + ẑ(i) = (ẑ (i) 1 , . . . , ẑ(i) T ) ẑ(i 1) = (ẑ (i 1) 1 , . . . , ẑ(i 1) T ) x 0 x 1 x T . . . x T 1 + + ẑ(1) T 1 ẑ(1) T Residual block (k=3, d=1) Convolutional Filter Identity Map (or 1x1 Conv) (a) (b) (c)</formula><p>This gives us two ways to increase the receptive field of the TCN: choosing larger filter sizes k and increasing the dilation factor d, where the effective history of one such layer is</p><formula xml:id="formula_1">(k − 1)d.</formula><p>As is common when using dilated convolutions, we increase d exponentially with the depth of the network (i.e., d = O(2 i ) at level i of the network). This ensures that there is some filter that hits each input within the effective history, while also allowing for an extremely large effective history using deep networks. We provide an illustration in Figure <ref type="figure" target="#fig_0">1</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Residual Connections</head><p>A residual block <ref type="bibr" target="#b28">(He et al., 2016)</ref> contains a branch leading out to a series of transformations F, whose outputs are added to the input x of the block:</p><formula xml:id="formula_2">o = Activation(x + F(x))<label>(3)</label></formula><p>This effectively allows layers to learn modifications to the identity mapping rather than the entire transformation, which has repeatedly been shown to benefit very deep networks.</p><p>Since a TCN's receptive field depends on the network depth n as well as filter size k and dilation factor d, stabilization of deeper and larger TCNs becomes important. For example, in a case where the prediction could depend on a history of size 2 12 and a high-dimensional input sequence, a network of up to 12 layers could be needed. Each layer, more specifically, consists of multiple filters for feature extraction. In our design of the generic TCN model, we therefore employ a generic residual module in place of a convolutional layer.</p><p>The residual block for our baseline TCN is shown in <ref type="bibr">Figure 1(b)</ref>. Within a residual block, the TCN has two layers of dilated causal convolution and non-linearity, for which we used the rectified linear unit (ReLU) <ref type="bibr" target="#b53">(Nair &amp; Hinton, 2010)</ref>. For normalization, we applied weight normalization <ref type="bibr" target="#b59">(Salimans &amp; Kingma, 2016)</ref> to the convolutional filters.</p><p>In addition, a spatial dropout <ref type="bibr" target="#b63">(Srivastava et al., 2014)</ref> was added after each dilated convolution for regularization: at each training step, a whole channel is zeroed out.</p><p>However, whereas in standard ResNet the input is added directly to the output of the residual function, in TCN (and ConvNets in general) the input and output could have different widths. To account for discrepant input-output widths, we use an additional 1x1 convolution to ensure that elementwise addition ⊕ receives tensors of the same shape (see Figure <ref type="figure" target="#fig_0">1(b,c</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion</head><p>We conclude this section by listing several advantages and disadvantages of using TCNs for sequence modeling.</p><p>• Parallelism. Unlike in RNNs where the predictions for later timesteps must wait for their predecessors to complete, convolutions can be done in parallel since the same filter is used in each layer. Therefore, in both training and evaluation, a long input sequence can be processed as a whole in TCN, instead of sequentially as in RNN. There are also two notable disadvantages to using TCNs.</p><p>• Data storage during evaluation. In evaluation/testing, RNNs only need to maintain a hidden state and take in a current input x t in order to generate a prediction. In other words, a "summary" of the entire history is provided by the fixed-length set of vectors h t , and the actual observed sequence can be discarded. In contrast, TCNs need to take in the raw sequence up to the effective history length, thus possibly requiring more memory during evaluation. • Potential parameter change for a transfer of domain.</p><p>Different domains can have different requirements on the amount of history the model needs in order to predict. Therefore, when transferring a model from a domain where only little memory is needed (i.e., small k and d) to a domain where much longer memory is required (i.e., much larger k and d), TCN may perform poorly for not having a sufficiently large receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sequence Modeling Tasks</head><p>We evaluate TCNs and RNNs on tasks that have been commonly used to benchmark the performance of different RNN sequence modeling architectures <ref type="bibr" target="#b29">(Hermans &amp; Schrauwen, 2013;</ref><ref type="bibr" target="#b11">Chung et al., 2014;</ref><ref type="bibr" target="#b57">Pascanu et al., 2014;</ref><ref type="bibr" target="#b43">Le et al., 2015;</ref><ref type="bibr" target="#b35">Jozefowicz et al., 2015;</ref><ref type="bibr" target="#b74">Zhang et al., 2016)</ref>. The intention is to conduct the evaluation on the "home turf" of RNN sequence models. We use a comprehensive set of synthetic stress tests along with real-world datasets from multiple domains.</p><p>The adding problem. In this task, each input consists of a length-n sequence of depth 2, with all values randomly chosen in [0, 1], and the second dimension being all zeros except for two elements that are marked by 1. The objective is to sum the two random values whose second dimensions are marked by 1. Simply predicting the sum to be 1 should give an MSE of about 0.1767. First introduced by Hochreiter &amp; Schmidhuber (1997), the adding problem has been used repeatedly as a stress test for sequence models <ref type="bibr">(Martens &amp; Sutskever, 2011;</ref><ref type="bibr" target="#b56">Pascanu et al., 2013;</ref><ref type="bibr" target="#b43">Le et al., 2015;</ref><ref type="bibr" target="#b1">Arjovsky et al., 2016;</ref><ref type="bibr" target="#b74">Zhang et al., 2016)</ref>.</p><p>Sequential MNIST and P-MNIST. Sequential MNIST is frequently used to test a recurrent network's ability to retain information from the distant past <ref type="bibr" target="#b43">(Le et al., 2015;</ref><ref type="bibr" target="#b74">Zhang et al., 2016;</ref><ref type="bibr" target="#b69">Wisdom et al., 2016;</ref><ref type="bibr" target="#b16">Cooijmans et al., 2016;</ref><ref type="bibr" target="#b42">Krueger et al., 2017;</ref><ref type="bibr" target="#b32">Jing et al., 2017)</ref>. In this task, MNIST images <ref type="bibr" target="#b46">(LeCun et al., 1998)</ref> are presented to the model as a 784×1 sequence for digit classification. In the more challenging P-MNIST setting, the order of the sequence is permuted at random <ref type="bibr" target="#b43">(Le et al., 2015;</ref><ref type="bibr" target="#b1">Arjovsky et al., 2016;</ref><ref type="bibr" target="#b69">Wisdom et al., 2016;</ref><ref type="bibr" target="#b42">Krueger et al., 2017)</ref>.</p><p>Copy memory. In this task, each input sequence has length T + 20. The first 10 values are chosen randomly among the digits 1, . . . , 8, with the rest being all zeros, except for the last 11 entries that are filled with the digit '9' (the first '9' is a delimiter). The goal is to generate an output of the same length that is zero everywhere except the last 10 values after the delimiter, where the model is expected to repeat the 10 values it encountered at the start of the input. This task was used in prior works such as Zhang et al. ( <ref type="formula">2016</ref> JSB Chorales and Nottingham. JSB Chorales <ref type="bibr" target="#b0">(Allan &amp; Williams, 2005</ref>) is a polyphonic music dataset consisting of the entire corpus of 382 four-part harmonized chorales by J. S. Bach. Each input is a sequence of elements. Each element is an 88-bit binary code that corresponds to the 88 keys on a piano, with 1 indicating a key that is pressed at a given time. Nottingham is a polyphonic music dataset based on a collection of 1,200 British and American folk tunes, and is much larger than JSB Chorales. JSB Chorales and Nottingham have been used in numerous empirical investigations of recurrent sequence modeling <ref type="bibr" target="#b11">(Chung et al., 2014;</ref><ref type="bibr" target="#b57">Pascanu et al., 2014;</ref><ref type="bibr" target="#b35">Jozefowicz et al., 2015;</ref><ref type="bibr" target="#b26">Greff et al., 2017)</ref>. The performance on both tasks is measured in terms of negative log-likelihood (NLL).</p><p>PennTreebank. We used the PennTreebank (PTB) <ref type="bibr" target="#b47">(Marcus et al., 1993)</ref> for both character-level and word-level language modeling. When used as a character-level language corpus, PTB contains 5,059K characters for training, 396K for validation, and 446K for testing, with an alphabet size of 50. When used as a word-level language corpus, PTB contains 888K words for training, 70K for validation, and 79K for testing, with a vocabulary size of 10K. This is a highly studied but relatively small language modeling dataset <ref type="bibr" target="#b52">(Miyamoto &amp; Cho, 2016;</ref><ref type="bibr" target="#b42">Krueger et al., 2017;</ref><ref type="bibr" target="#b50">Merity et al., 2017)</ref>.</p><p>Wikitext-103. Wikitext-103 <ref type="bibr" target="#b50">(Merity et al., 2016)</ref> is almost LAMBADA. Introduced by <ref type="bibr" target="#b55">Paperno et al. (2016)</ref>, LAM-BADA is a dataset comprising 10K passages extracted from novels, with an average of 4.6 sentences as context, and 1 target sentence the last word of which is to be predicted. This dataset was built so that a person can easily guess the missing word when given the context sentences, but not when given only the target sentence without the context sentences. Most of the existing models fail on LAMBADA <ref type="bibr" target="#b55">(Paperno et al., 2016;</ref><ref type="bibr" target="#b23">Grave et al., 2017)</ref>. In general, better results on LAMBADA indicate that a model is better at capturing information from longer and broader context. The training data for LAMBADA is the full text of 2,662 novels with more than 200M words. The vocabulary size is about 93K.</p><p>text8. We also used the text8 dataset for character-level language modeling <ref type="bibr" target="#b51">(Mikolov et al., 2012)</ref>. text8 is about 20 times larger than PTB, with about 100M characters from Wikipedia (90M for training, 5M for validation, and 5M for testing). The corpus contains 27 unique alphabets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We compare the generic TCN architecture described in Section 3 to canonical recurrent architectures, namely LSTM, GRU, and vanilla RNN, with standard regularizations. All experiments reported in this section used exactly the same TCN architecture, just varying the depth of the network n and occasionally the kernel size k so that the receptive field covers enough context for predictions. We use an exponential dilation d = 2 i for layer i in the network, and the Adam optimizer <ref type="bibr" target="#b39">(Kingma &amp; Ba, 2015)</ref> with learning rate 0.002 for TCN, unless otherwise noted. We also empirically find that gradient clipping helped convergence, and we pick the maximum norm for clipping from [0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synopsis of Results</head><p>A synopsis of the results is shown in Table <ref type="table" target="#tab_1">1</ref>. Note that on several of these tasks, the generic, canonical recurrent architectures we study (e.g., LSTM, GRU) are not the stateof-the-art. (See the supplement for more details.) With this caveat, the results strongly suggest that the generic TCN architecture with minimal tuning outperforms canonical recurrent architectures across a broad variety of sequence modeling tasks that are commonly used to benchmark the performance of recurrent architectures themselves. We now analyze these results in more detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Synthetic Stress Tests</head><p>The adding problem. Convergence results for the adding problem, for problem sizes T = 200 and 600, are shown in Figure <ref type="figure">2</ref>. All models were chosen to have roughly 70K parameters. TCNs quickly converged to a virtually perfect solution (i.e., MSE near 0). GRUs also performed quite well, albeit slower to converge than TCNs. LSTMs and vanilla RNNs performed significantly worse.</p><p>Sequential MNIST and P-MNIST. Convergence results on sequential and permuted MNIST, run over 10 epochs, are shown in Figure <ref type="figure">3</ref>. All models were configured to have roughly 70K parameters. For both problems, TCNs substantially outperform the recurrent architectures, both in terms of convergence and in final accuracy on the task. For P-MNIST, TCNs outperform state-of-the-art results (95.9%) based on recurrent networks with Zoneout and Recurrent BatchNorm <ref type="bibr" target="#b16">(Cooijmans et al., 2016;</ref><ref type="bibr" target="#b42">Krueger et al., 2017)</ref>.</p><p>Copy memory. Convergence results on the copy memory task are shown in Figure <ref type="figure" target="#fig_3">4</ref>. TCNs quickly converge to correct answers, while LSTMs and GRUs simply converge to the same loss as predicting all zeros. In this case we also compare to the recently-proposed EURNN <ref type="bibr" target="#b32">(Jing et al., 2017)</ref>, which was highlighted to perform well on this task. While both TCN and EURNN perform well for sequence length T = 500, the TCN has a clear advantage for T = 1000 and longer (in terms of both loss and rate of convergence). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Polyphonic Music and Language Modeling</head><p>We now discuss the results on polyphonic music modeling, character-level language modeling, and word-level language modeling. These domains are dominated by recurrent architectures, with many specialized designs developed for these tasks <ref type="bibr" target="#b74">(Zhang et al., 2016;</ref><ref type="bibr" target="#b27">Ha et al., 2017;</ref><ref type="bibr" target="#b42">Krueger et al., 2017;</ref><ref type="bibr" target="#b23">Grave et al., 2017;</ref><ref type="bibr" target="#b26">Greff et al., 2017;</ref><ref type="bibr" target="#b50">Merity et al., 2017)</ref>. We mention some of these specialized architectures when useful, but our primary goal is to compare the generic TCN model to similarly generic recurrent architectures, before domain-specific tuning. The results are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Polyphonic music. On Nottingham and JSB Chorales, the TCN with virtually no tuning outperforms the recurrent models by a considerable margin, and even outperforms some enhanced recurrent architectures for this task such as HF-RNN <ref type="bibr" target="#b6">(Boulanger-Lewandowski et al., 2012)</ref> and Diagonal RNN (Subakan &amp; Smaragdis, 2017). Note however that other models such as the Deep Belief Net LSTM perform better still <ref type="bibr" target="#b66">(Vohra et al., 2015)</ref>; we believe this is likely due to the fact that the datasets are relatively small, and thus the right regularization method or generative modeling procedure can improve performance significantly. This is largely orthogonal to the RNN/TCN distinction, as a similar variant of TCN may well be possible.</p><p>Word-level language modeling. Language modeling remains one of the primary applications of recurrent networks and many recent works have focused on optimizing LSTMs for this task <ref type="bibr" target="#b42">(Krueger et al., 2017;</ref><ref type="bibr" target="#b50">Merity et al., 2017)</ref>.</p><p>Our implementation follows standard practice that ties the weights of encoder and decoder layers for both TCN and RNNs <ref type="bibr" target="#b58">(Press &amp; Wolf, 2016)</ref>, which significantly reduces the number of parameters in the model. For training, we use SGD and anneal the learning rate by a factor of 0.5 for both TCN and RNNs when validation accuracy plateaus.</p><p>On the smaller PTB corpus, an optimized LSTM architecture (with recurrent and embedding dropout, etc.) outperforms the TCN, while the TCN outperforms both GRU and vanilla RNN. However, on the much larger Wikitext-103 corpus and the LAMBADA dataset <ref type="bibr" target="#b55">(Paperno et al., 2016)</ref>, without any hyperparameter search, the TCN outperforms the LSTM results of <ref type="bibr" target="#b23">Grave et al. (2017)</ref>, achieving much lower perplexities.</p><p>Character-level language modeling. On character-level language modeling (PTB and text8, accuracy measured in bits per character), the generic TCN outperforms regularized LSTMs and GRUs as well as methods such as Normstabilized LSTMs <ref type="bibr" target="#b41">(Krueger &amp; Memisevic, 2015)</ref>. (Specialized architectures exist that outperform all of these, see the supplement.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Memory Size of TCN and RNNs</head><p>One of the theoretical advantages of recurrent architectures is their unlimited memory: the theoretical ability to retain information through sequences of unlimited length. We now examine specifically how long the different architectures can retain information in practice. We focus on 1) the copy memory task, which is a stress test designed to evaluate longterm, distant information propagation in recurrent networks, and 2) the LAMBADA task, which tests both local and non-local textual understanding.</p><p>The copy memory task is perfectly set up to examine a model's ability to retain information for different lengths of time. The requisite retention time can be controlled by varying the sequence length T . In contrast to Section 5.2, we now focus on the accuracy on the last 10 elements of the output sequence (which are the nontrivial elements that must be recalled). We used models of size 10K for both TCN and RNNs.</p><p>The results of this focused study are shown in Figure <ref type="figure">5</ref>. TCNs consistently converge to 100% accuracy for all sequence lengths, whereas LSTMs and GRUs of the same size quickly degenerate to random guessing as the sequence length T grows. The accuracy of the LSTM falls below 20% for T &lt; 50, while the GRU falls below 20% for T &lt; 200.</p><p>These results indicate that TCNs are able to maintain a much longer effective history than their recurrent counterparts.</p><p>This observation is backed up on real data by experiments on the large-scale LAMBADA dataset, which is specifically designed to test a model's ability to utilize broad context <ref type="bibr" target="#b55">(Paperno et al., 2016)</ref>. As shown in Table <ref type="table" target="#tab_1">1</ref>, TCN outperforms LSTMs and vanilla RNNs by a significant margin in perplexity on LAMBADA, with a substantially smaller network and virtually no tuning. (State-of-the-art results on this dataset are even better, but only with the help of additional memory mechanisms <ref type="bibr" target="#b23">(Grave et al., 2017)</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented an empirical evaluation of generic convolutional and recurrent architectures across a comprehensive suite of sequence modeling tasks. To this end, we have described a simple temporal convolutional network (TCN)</p><p>Figure <ref type="figure">5</ref>. Accuracy on the copy memory task for sequences of different lengths T . While TCN exhibits 100% accuracy for all sequence lengths, the LSTM and GRU degenerate to random guessing as T grows.</p><p>that combines best practices such as dilations and residual connections with the causal convolutions needed for autoregressive prediction. The experimental results indicate that TCN models substantially outperform generic recurrent architectures such as LSTMs and GRUs. We further studied long-range information propagation in convolutional and recurrent networks, and showed that the "infinite memory" advantage of RNNs is largely absent in practice. TCNs exhibit longer memory than recurrent architectures with the same capacity.</p><p>Numerous advanced schemes for regularizing and optimizing LSTMs have been proposed <ref type="bibr" target="#b58">(Press &amp; Wolf, 2016;</ref><ref type="bibr" target="#b42">Krueger et al., 2017;</ref><ref type="bibr" target="#b50">Merity et al., 2017;</ref><ref type="bibr" target="#b8">Campos et al., 2018)</ref>. These schemes have significantly advanced the accuracy achieved by LSTM-based architectures on some datasets. The TCN has not yet benefitted from this concerted community-wide investment into architectural and algorithmic elaborations. We see such investment as desirable and expect it to yield advances in TCN performance that are commensurate with the advances seen in recent years in LSTM performance. We will release the code for our project to encourage this exploration.</p><p>The preeminence enjoyed by recurrent networks in sequence modeling may be largely a vestige of history. Until recently, before the introduction of architectural elements such as dilated convolutions and residual connections, convolutional architectures were indeed weaker. Our results indicate that with these elements, a simple convolutional architecture is more effective across diverse sequence modeling tasks than recurrent architectures such as LSTMs. Due to the comparable clarity and simplicity of TCNs, we conclude that convolutional networks should be regarded as a natural starting point and a powerful toolkit for sequence modeling. As discussed in Section 5, the number of hidden units was chosen so that the model size is approximately at the same level as the recurrent models with which we are comparing.</p><p>In Table <ref type="table" target="#tab_4">2</ref>, a gradient clip of N/A means no gradient clipping was applied. In larger tasks (e.g., language modeling), we empirically found that gradient clipping (we randomly picked a threshold from [0.3, 1]) helps with regularizing TCN and accelerating convergence.</p><p>All weights were initialized from a Gaussian disitribution N (0, 0.01). In general, we found TCN to be relatively insensitive to hyperparameter changes, as long as the effective history (i.e., receptive field) size is sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Hyperparameters for LSTM/GRU</head><p>Table <ref type="table">3</ref> reports hyperparameter settings that were used for the LSTM. These values are picked from hyperparameter search for LSTMs that have up to 3 layers, and the optimizers are chosen from {SGD, Adam, RMSprop, Adagrad}.</p><p>For certain larger datasets, we adopted the settings used in prior work (e.g., <ref type="bibr" target="#b23">Grave et al. (2017)</ref> on Wikitext-103). GRU hyperparameters were chosen in a similar fashion, but typically with more hidden units than in LSTM to keep the total network size approximately the same (since a GRU cell is more compact).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. State-of-the-Art Results</head><p>As previously noted, the generic TCN and LSTM/GRU models we used can be outperformed by more specialized architectures on some tasks. State-of-the-art results are summarized in Table <ref type="table">4</ref>. The same TCN architecture is used across all tasks. Note that the size of the state-of-the-art model may be different from the size of the TCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of Filter Size and Residual Block</head><p>In this section we briefly study the effects of different components of a TCN layer. Overall, we believe dilation is required for modeling long-term dependencies, and so we mainly focus on two other factors here: the filter size k used by each layer, and the effect of residual blocks.</p><p>We perform a series of controlled experiments, with the results of the ablative analysis shown in Figure <ref type="figure" target="#fig_4">6</ref>. As before, we kept the model size and depth exactly the same for different models, so that the dilation factor is strictly controlled. The experiments were conducted on three different tasks: copy memory, permuted MNIST (P-MNIST), and Penn Treebank word-level language modeling. These experiments confirm that both factors (filter size and residual connections) contribute to sequence modeling performance.</p><p>Filter size k. In both the copy memory and the P-MNIST tasks, we observed faster convergence and better accuracy for larger filter sizes. In particular, looking at Figure <ref type="figure" target="#fig_4">6a</ref>, a TCN with filter size ≤ 3 only converges to the same level as random guessing. In contrast, on word-level language modeling, a smaller kernel with filter size of k = 3 works best. We believe this is because a smaller kernel (along with fixed dilation) tends to focus more on the local context, which is especially important for PTB language modeling (in fact, the very success of n-gram models suggests that only a relatively short memory is needed for modeling language).</p><p>Residual block. In all three scenarios that we compared here, we observed that the residual function stabilized training and brought faster convergence with better final results. Especially in language modeling, we found that residual connections contribute substantially to performance (See Figure <ref type="figure" target="#fig_4">6f</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Gating Mechanisms</head><p>One component that had been used in prior work on convolutional architectures for language modeling is the gated activation (van den <ref type="bibr">Oord et al., 2016;</ref><ref type="bibr">Dauphin et al., 2017)</ref>. We have chosen not to use gating in the generic TCN model. We now examine this choice more closely.  <ref type="formula">2017</ref>) compared the effects of gated linear units (GLU) and gated tanh units (GTU), and adopted GLU in their non-dilated gated ConvNet. Following the same choice, we now compare TCNs using ReLU and TCNs with gating (GLU), represented by an elementwise product between two convolutional layers, with one of them also passing through a sigmoid function σ(x). Note that the gates architecture uses approximately twice as many convolutional layers as the ReLU-TCN.</p><p>The results are shown in Table <ref type="table" target="#tab_5">5</ref>, where we kept the number of model parameters at about the same size. The GLU does further improve TCN accuracy on certain language modeling datasets like PTB, which agrees with prior work. However, we do not observe comparable benefits on other tasks, such as polyphonic music modeling or synthetic stress tests that require longer information retention. On the copy memory task with T = 1000, we found that TCN with gating converged to a worse result than TCN with ReLU (though still better than recurrent models).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Architectural elements in a TCN. (a) A dilated causal convolution with dilation factors d = 1, 2, 4 and filter size k = 3. The receptive field is able to cover all values from the input sequence. (b) TCN residual block. An 1x1 convolution is added when residual input and output have different dimensions. (c) An example of residual connection in a TCN. The blue lines are filters in the residual function, and the green lines are identity mappings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>); Arjovsky et al. (2016); Wisdom et al. (2016); Jing et al. (2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. Results on the adding problem for different sequence lengths T . TCNs outperform recurrent architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Result on the copy memory task for different sequence lengths T . TCNs outperform recurrent architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Controlled experiments that study the effect of different components of the TCN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Stable gradients. Unlike recurrent architectures, TCN has a backpropagation path different from the temporal direction of the sequence. TCN thus avoids the problem of exploding/vanishing gradients, which is a major issue for RNNs (and which led to the development of LSTM, GRU, HF-RNN(Martens &amp; Sutskever, 2011), etc.).• Low memory requirement for training. Especially in the case of a long input sequence, LSTMs and GRUs can easily use up a lot of memory to store the partial results for their multiple cell gates. However, in a TCN the filters are shared across a layer, with the backpropagation path depending only on network depth. Therefore in practice, we found gated RNNs likely to use up to a multiplicative factor more memory than TCNs. • Variable length inputs. Just like RNNs, which model inputs with variable lengths in a recurrent way, TCNs can also take in inputs of arbitrary lengths by sliding the 1D convolutional kernels. This means that TCNs can be adopted as drop-in replacements for RNNs for sequential data of arbitrary length.</figDesc><table><row><cell>• Flexible receptive field size. A TCN can change its re-</cell></row><row><cell>ceptive field size in multiple ways. For instance, stacking</cell></row><row><cell>more dilated (causal) convolutional layers, using larger</cell></row><row><cell>dilation factors, or increasing the filter size are all viable</cell></row><row><cell>options (with possibly different interpretations). TCNs</cell></row><row><cell>thus afford better control of the model's memory size,</cell></row><row><cell>and are easy to adapt to different domains.</cell></row><row><cell>•</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of TCNs and recurrent architectures on synthetic stress tests, polyphonic music modeling, character-level language modeling, and word-level language modeling. The generic TCN architecture outperforms canonical recurrent networks across a comprehensive suite of tasks and datasets. Current state-of-the-art results are listed in the supplement. h means that higher is better. means that lower is better.</figDesc><table><row><cell>Sequence Modeling Task</cell><cell>Model Size (≈)</cell><cell>LSTM</cell><cell cols="2">Models GRU RNN</cell><cell>TCN</cell></row><row><cell>Seq. MNIST (accuracy h )</cell><cell>70K</cell><cell>87.2</cell><cell>96.2</cell><cell>21.5</cell><cell>99.0</cell></row><row><cell>Permuted MNIST (accuracy)</cell><cell>70K</cell><cell>85.7</cell><cell>87.3</cell><cell>25.3</cell><cell>97.2</cell></row><row><cell>Adding problem T =600 (loss )</cell><cell>70K</cell><cell>0.164</cell><cell>5.3e-5</cell><cell>0.177</cell><cell>5.8e-5</cell></row><row><cell>Copy memory T =1000 (loss)</cell><cell>16K</cell><cell cols="4">0.0204 0.0197 0.0202 3.5e-5</cell></row><row><cell>Music JSB Chorales (loss)</cell><cell>300K</cell><cell>8.45</cell><cell>8.43</cell><cell>8.91</cell><cell>8.10</cell></row><row><cell>Music Nottingham (loss)</cell><cell>1M</cell><cell>3.29</cell><cell>3.46</cell><cell>4.05</cell><cell>3.07</cell></row><row><cell>Word-level PTB (perplexity )</cell><cell>13M</cell><cell>78.93</cell><cell>92.48</cell><cell cols="2">114.50 88.68</cell></row><row><cell>Word-level Wiki-103 (perplexity)</cell><cell>-</cell><cell>48.4</cell><cell>-</cell><cell>-</cell><cell>45.19</cell></row><row><cell>Word-level LAMBADA (perplexity)</cell><cell>-</cell><cell>4186</cell><cell>-</cell><cell>14725</cell><cell>1279</cell></row><row><cell>Char-level PTB (bpc )</cell><cell>3M</cell><cell>1.36</cell><cell>1.37</cell><cell>1.48</cell><cell>1.31</cell></row><row><cell>Char-level text8 (bpc)</cell><cell>5M</cell><cell>1.50</cell><cell>1.53</cell><cell>1.69</cell><cell>1.45</cell></row><row><cell cols="2">110 times as large as PTB, featuring a vocabulary size of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">about 268K. The dataset contains 28K Wikipedia articles</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(about 103 million words) for training, 60 articles (about</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">218K words) for validation, and 60 articles (246K words)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>for testing. This is a more representative and realistic dataset than PTB, with a much larger vocabulary that includes many rare words, and has been used in<ref type="bibr" target="#b50">Merity et al. (2016)</ref>;<ref type="bibr" target="#b23">Grave et al. (2017);</ref> Dauphin et al. (2017).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table2lists the hyperparameters we used when applying the generic TCN model on various tasks and datasets. The most important factor for picking parameters is to make sure that the TCN has a sufficiently large receptive field by choosing k and d that can cover the amount of context needed for the task.</figDesc><table><row><cell>An Empirical Evaluation of Generic Convolutional and Recurrent Networks</cell></row><row><cell>for Sequence Modeling</cell></row><row><cell>Supplementary Material</cell></row><row><cell>A. Hyperparameters Settings</cell></row><row><cell>A.1. Hyperparameters for TCN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>TCN parameter settings for experiments in Section 5.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">TCN SETTINGS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset/Task</cell><cell>Subtask</cell><cell cols="4">k n Hidden Dropout Grad Clip</cell><cell>Note</cell></row><row><cell></cell><cell>T = 200</cell><cell>6 7</cell><cell>27</cell><cell></cell><cell></cell><cell></cell></row><row><cell>The Adding Problem</cell><cell>T = 400</cell><cell>7 7</cell><cell>27</cell><cell>0.0</cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>T = 600</cell><cell>8 8</cell><cell>24</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Seq. MNIST</cell><cell>-</cell><cell>7 8 6 8</cell><cell>25 20</cell><cell>0.0</cell><cell>N/A</cell><cell></cell></row><row><cell>Permuted MNIST</cell><cell>-</cell><cell>7 8 6 8</cell><cell>25 20</cell><cell>0.0</cell><cell>N/A</cell><cell></cell></row><row><cell></cell><cell>T = 500</cell><cell>6 9</cell><cell>10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Copy Memory Task</cell><cell>T = 1000</cell><cell>8 8</cell><cell>10</cell><cell>0.05</cell><cell>1.0</cell><cell>RMSprop 5e-4</cell></row><row><cell></cell><cell>T = 2000</cell><cell>8 9</cell><cell>10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Music JSB Chorales</cell><cell>-</cell><cell>3 2</cell><cell>150</cell><cell>0.5</cell><cell>0.4</cell><cell></cell></row><row><cell>Music Nottingham</cell><cell>-</cell><cell>6 4</cell><cell>150</cell><cell>0.2</cell><cell>0.4</cell><cell></cell></row><row><cell></cell><cell>PTB</cell><cell>3 4</cell><cell>600</cell><cell>0.5</cell><cell></cell><cell>Embed. size 600</cell></row><row><cell>Word-level LM</cell><cell cols="2">Wiki-103 LAMBADA 4 5 3 5</cell><cell>1000 500</cell><cell>0.4</cell><cell>0.4</cell><cell>Embed. size 400 Embed. size 500</cell></row><row><cell>Char-level LM</cell><cell>PTB text8</cell><cell>3 3 2 5</cell><cell>450 520</cell><cell>0.1</cell><cell>0.15</cell><cell>Embed. size 100</cell></row><row><cell>Dauphin et al. (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>An evaluation of gating in TCN. A plain TCN is compared to a TCN that uses gated activations.</figDesc><table><row><cell>Task</cell><cell>TCN</cell><cell>TCN + Gating</cell></row><row><cell>Sequential MNIST (acc.)</cell><cell>99.0</cell><cell>99.0</cell></row><row><cell>Permuted MNIST (acc.)</cell><cell>97.2</cell><cell>96.9</cell></row><row><cell>Adding Problem T = 600 (loss)</cell><cell>5.8e-5</cell><cell>5.6e-5</cell></row><row><cell>Copy Memory T = 1000 (loss)</cell><cell>3.5e-5</cell><cell>0.00508</cell></row><row><cell>JSB Chorales (loss)</cell><cell>8.10</cell><cell>8.13</cell></row><row><cell>Nottingham (loss)</cell><cell>3.07</cell><cell>3.12</cell></row><row><cell>Word-level PTB (ppl)</cell><cell>88.68</cell><cell>87.94</cell></row><row><cell>Char-level PTB (bpc)</cell><cell>1.31</cell><cell>1.306</cell></row><row><cell>Char text8 (bpc)</cell><cell>1.45</cell><cell>1.485</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">USA 2 Computer Science Department, Carnegie Mellon University, Pittsburgh, PA,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">USA 3 Intel Labs, Santa Clara, CA, USA. Correspondence to: Shaojie Bai &lt;shaojieb@cs.cmu.edu&gt;, J. Zico Kolter &lt;zkolter@cs.cmu.edu&gt;, Vladlen Koltun &lt;vkoltun@gmail.edu&gt;.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b12">(Chung et al., 2016)</ref> </div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Harmonising chorales by probabilistic inference</title>
		<author>
			<persName><forename type="first">Moray</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speaker-independent isolated digit recognition: Multilayer perceptrons vs. dynamic time warping</title>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><surname>Soulie</surname></persName>
		</author>
		<author>
			<persName><surname>Fogelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Blanchet</surname></persName>
		</author>
		<author>
			<persName><surname>Liénard</surname></persName>
		</author>
		<author>
			<persName><surname>Jean-Sylvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6392</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skip RNN: Learning to skip state updates in recurrent neural networks</title>
		<author>
			<persName><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><surname>Giró I Nieto</surname></persName>
		</author>
		<author>
			<persName><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Shih-Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Xiaoxiao</surname></persName>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><surname>Xiaodong</surname></persName>
		</author>
		<author>
			<persName><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Merriënboer</surname></persName>
		</author>
		<author>
			<persName><surname>Bart</surname></persName>
		</author>
		<author>
			<persName><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><surname>Dzmitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Encoder-decoder approaches</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><surname>Caglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><surname>Sungjin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704</idno>
		<title level="m">Hierarchical multiscale recurrent neural networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><surname>Léon</surname></persName>
		</author>
		<author>
			<persName><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Koray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>JMLR</publisher>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
	<note>In European Chapter</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><surname>César</surname></persName>
		</author>
		<author>
			<persName><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<meeting><address><addrLine>Dauphin, Yann N., Fan, Angela, Auli, Michael, and Grangier, David</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName><forename type="first">Cícero</forename><surname>Dos Santos</surname></persName>
		</author>
		<author>
			<persName><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName><forename type="first">El</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName><surname>Salah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><forename type="middle">;</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2002">2002. 2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Deep Learning</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training and analysing deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Connectionist learning procedures</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tunable efficient unitary neural networks (EUNN) and their application to RNNs</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Yichen</surname></persName>
		</author>
		<author>
			<persName><surname>Dubcek</surname></persName>
		</author>
		<author>
			<persName><surname>Tena</surname></persName>
		</author>
		<author>
			<persName><surname>Peurifoy</surname></persName>
		</author>
		<author>
			<persName><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Skirlo</surname></persName>
		</author>
		<author>
			<persName><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marin</forename><surname>Soljačić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><surname>Lasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Aäron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
		<title level="m">Neural machine translation in linear time</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A clockwork RNN</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08400</idno>
		<title level="m">Regularizing RNNs by stabilizing activations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zoneout: Regularizing RNNs by randomly preserving hidden activations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><surname>Tegan</surname></persName>
		</author>
		<author>
			<persName><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName><surname>János</surname></persName>
		</author>
		<author>
			<persName><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><surname>Rosemary</surname></persName>
		</author>
		<author>
			<persName><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><surname>René</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><surname>Léon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">;</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Ann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="1993">2015. 1993</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note>Fully convolutional networks for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning recurrent neural networks with Hessian-free optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Caiming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">;</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><surname>Shirish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<idno>arXiv:1708.02182</idno>
		<title level="m">Regularizing and optimizing LSTM language models</title>
				<imprint>
			<date type="published" when="2016">2016. 2017</date>
		</imprint>
	</monogr>
	<note>Pointer sentinel mixture models</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName><surname>Anoop</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hai-Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cernocky</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gated word-character recurrent language model</title>
		<author>
			<persName><forename type="first">Yasumasa</forename><surname>Miyamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01700</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Sequence Models (Course 5 of Deep Learning Specialization)</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Coursera</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><surname>Germán</surname></persName>
		</author>
		<author>
			<persName><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><surname>Angeliki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><surname>Ngoc</surname></persName>
		</author>
		<author>
			<persName><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><surname>Raffaella</surname></persName>
		</author>
		<author>
			<persName><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><surname>Sandro</surname></persName>
		</author>
		<author>
			<persName><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06031</idno>
		<title level="m">The LAM-BADA dataset: Word prediction requiring a broad discourse context</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05859</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Parallel networks that learn to pronounce English text</title>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Zhourong</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><surname>Dit-Yan</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><surname>Wai-Kin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05420</idno>
	</analytic>
	<monogr>
		<title level="m">Diagonal RNNs in symbolic music modeling</title>
				<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>Subakan, Y Cem and Smaragdis</publisher>
			<date type="published" when="2014">2014. 2017</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V ;</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Aäron</surname></persName>
		</author>
		<author>
			<persName><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><surname>Heiga</surname></persName>
		</author>
		<author>
			<persName><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><surname>Karen</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><surname>Nal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">WaveNet: A generative model for raw audio</title>
				<imprint>
			<date type="published" when="2014">2014. 2016</date>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Modeling temporal dependencies in data using a DBN-LSTM</title>
		<author>
			<persName><forename type="first">Raunaq</forename><surname>Vohra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Sahoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Science and Advanced Analytics (DSAA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Phoneme recognition using timedelay neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><surname>Toshiyuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><surname>Kiyohiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Backpropagation through time: What it does and how to do it</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On multiplicative integration with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Saizheng</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Zihang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Comparative study of CNN and RNN for natural language processing</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><surname>Katharina</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><surname>Hinrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01923</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Architectural complexity measures of recurrent neural networks</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Yuhuai</surname></persName>
		</author>
		<author>
			<persName><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Zhouhan</surname></persName>
		</author>
		<author>
			<persName><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><surname>Roland</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Characterlevel convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Jake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
