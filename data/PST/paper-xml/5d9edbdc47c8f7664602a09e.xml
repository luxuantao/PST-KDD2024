<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-07-20">July 20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
							<email>battista.biggio@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Pluribus One</orgName>
								<address>
									<settlement>Cagliari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
							<email>roli@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Pluribus One</orgName>
								<address>
									<settlement>Cagliari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-07-20">July 20, 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1712.03141v2[cs.CV]</idno>
					<note type="submission">Preprint accepted for publication on Pattern Recognition</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial Machine Learning</term>
					<term>Evasion Attacks</term>
					<term>Poisoning Attacks</term>
					<term>Adversarial Examples</term>
					<term>Secure Learning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern technologies based on pattern recognition, machine learning and data-driven artificial intelligence, especially after the advent of deep learning, have reported impressive performance in a variety of application domains, from classical pattern recognition tasks like speech and object recognition, used by self-driving cars and robots, to more modern cybersecurity tasks like spam and malware detection <ref type="bibr" target="#b0">[1]</ref>. 1 It has been thus surprising to see that such technologies can easily be fooled by adversarial examples, i.e., carefully-perturbed input samples aimed to mislead detection at test time. This has brought considerable attention since 2014, when Szegedy et al. <ref type="bibr" target="#b1">[2]</ref> and subsequent work <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> showed that deep networks for object recognition can be fooled by input images perturbed in an imperceptible manner.</p><p>Since then, an ever-increasing number of research papers have started proposing countermeasures to mitigate the threat associated to these wild patterns, not only in the area of computer vision <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. 2 This huge and growing body of work has clearly fueled a renewed interest in the research field known as adversarial machine learning, while also raising a number of misconceptions on how the security properties of learning algorithms should be evaluated and understood.</p><p>The primary misconception is about the start date of the field of adversarial machine learning, which is not 2014. This wrong start date is implicitly acknowledged in a growing number of recent papers in the area of computer security <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> and computer vision <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>, which focus mainly on the study of security and robustness of deep networks to adversarial inputs. However, as we will discuss throughout this manuscript, this research area has been independently developing and re-discovering well-known phenomena that had been largely explored in the field of adversarial machine learning before the discovery of adversarial examples against deep networks.</p><p>To the best of our knowledge, the very first, seminal work in the area of adversarial machine learning dates back to 2004. At that time, Dalvi et al. <ref type="bibr" target="#b18">[19]</ref>, and immediately later Lowd and Meek <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> studied the problem in the context of spam filtering, showing that linear classifiers could be easily tricked by few carefully-crafted changes in the content of spam emails, without significantly affecting the readability of the spam message. These were indeed the first adversarial examples against linear classifiers for spam filtering. Even earlier, Matsumoto et al. <ref type="bibr" target="#b21">[22]</ref> showed that fake fingerprints can be fabricated with plastic-like materials to mislead biometric identity recognition systems. In 2006, in their famous paper, Barreno et al. <ref type="bibr" target="#b22">[23]</ref> questioned the suitability of machine learning in adversarial settings from a broader perspective, categorizing attacks against machine-learning algorithms both at training and at test time, and envisioning potential countermeasures to mitigate such threats. Since then, and independently from the discovery of adversarial examples against deep networks <ref type="bibr" target="#b1">[2]</ref>, a large amount of work has been done to: (i) develop attacks against machine learning, both at training time (poisoning) <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> and at test time (evasion) <ref type="bibr">[19-21, 30, 34-39]</ref>; (ii) propose systematic methodologies for security evaluation of learning algorithms against such attacks <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>; and (iii) design suitable defense mechanisms to mitigate these threats <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref>.</p><p>The fact that adversarial machine learning was wellestablished before 2014 is also witnessed by a number of related events, including the 2007 NIPS Workshop on Machine Learning in Adversarial Environments for Computer Security <ref type="bibr" target="#b48">[49]</ref>, along with the subsequent special issue on the journal Machine Learning <ref type="bibr" target="#b49">[50]</ref>, the 2013 Dagstuhl Perspectives Workshop on Machine Learning Methods for Computer Security <ref type="bibr" target="#b50">[51]</ref> and, most importantly, the Workshop on Artificial Intelligence and Security (AISec), which reached its 10th edition in 2017 <ref type="bibr" target="#b51">[52]</ref>. Worth remarking, a book has also been recently published on this subject <ref type="bibr" target="#b52">[53]</ref>.</p><p>In this work, we aim to provide a thorough overview of the evolution of this interdisciplinary research area over the last ten years and beyond, from pioneering work on the security of (non-deep) learning algorithms to more recent work focused on the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. Our goal is to connect the dots between these apparently-different lines of work, while also highlighting common misconceptions related to the security evaluation of learning algorithms.</p><p>We first review the notion of arms race in computer security, advocating for a proactive security-by-design cycle that explicitly accounts for the presence of the attacker in the loop (Sect. 2). Our narrative of the security of machine learning then follows three metaphors, referred to as the three golden rules in the following: (i) know your adversary, (ii) be proactive; and (iii) protect yourself. Knowing the attacker amounts to modeling threats against the learning-based system under design. To this end, we review a comprehensive threat model which allows one to envision and simulate attacks against the system under design, to thoroughly assess its security properties under well-defined attack scenarios (Sect. 3). We then discuss how to proactively simulate test-time evasion and training-time poisoning attacks against the system under design (Sect. 4), and how to protect it with different defense mechanisms (Sect. 5). We finally discuss the main limitations of current work and the future research challenges towards the design of more secure learning algorithms (Sect. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Arms Race and Security by Design</head><p>Security is an arms race, and the security of machine learning and pattern recognition systems is not an exception to this <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref>. To better understand this phenomenon, consider that, since the 90s, computer viruses and Internet scams have increased not only in terms of absolute numbers, but also in terms of variability and sophistication, in response to the growing complexity of defense systems. Automatic tools for designing novel variants of attacks have been developed, making large-scale automatization of stealthier attacks practical also for non-skilled attackers. A very clear example of this is provided by phishing kits, which automatically compromise legitimate (vulnerable) websites in the wild, and hide phishing webpages within them <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>. The sophistication and proliferation of such attack vectors, malware and other threats, is strongly motivated by a flourishing underground economy, which enables easy monetization after attack. To tackle the increasing complexity of modern attacks, and favor the detection of never-before-seen ones, machine learning and pattern recognition techniques have been widely adopted over the last decade also in a variety of cybersecurity application domains. However, as we will see throughout this paper, machine learning and pattern recognition techniques turned out not to be the definitive answer to such threats. They introduce specific vulnerabilities that skilled attackers can exploit to compromise the whole system, i.e., machine learning itself can be the weakest link in the security chain.</p><p>To further clarify how the aforementioned arms race typically evolves, along with the notions of reactive and proactive security, we briefly summarize in the following an exemplary case in spam filtering. The spam arms race. Spam emails typically convey the spam message in textual format, which can be detected by rule-based filters and text classifiers. Spammers attempt to mislead these defenses by obfuscating the content of spam emails to evade detection, e.g., by misspelling bad words (i.e., words likely to appear in spam but not in legitimate emails), and adding good words (i.e., words typically occurring in legitimate emails, randomly guessed from a reference vocabulary) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>. In 2005, spammers invented a new trick to evade textual-based analysis, referred to as image-based spam (or image spam, for short) <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>. The idea is simply to embed the spam message within an attached image (Fig. <ref type="figure" target="#fig_1">2</ref>, left). Due to the large amount of image spam sent in 2006 and 2007, countermeasures were promptly developed based on signatures of known spam images (through hashing), and on extracting text from suspect images with OCR tools <ref type="bibr" target="#b58">[59]</ref>. To evade these defenses, spammers started obfuscating images with random noise patterns (Fig. <ref type="figure" target="#fig_1">2</ref>, right) that, ironically, were similar to those used in CAPTCHAs to protect web sites from spam bots <ref type="bibr" target="#b59">[60]</ref>. Learning-based approaches based on low-level visual features were then devised to detect spam images. Image spam volumes have since declined, but spammers have been constantly developing novel tricks to evade detection. Reactive and proactive security. As discussed for spam filtering, security problems are often cast as a reactive arms race, in which the system designer and the attacker aim to achieve their goals by adapting their behavior in response to that of the opponent, i.e., learning from the past. This can be modeled according to the following steps (Fig. <ref type="figure" target="#fig_0">1</ref>, left) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>: (i) the attacker analyzes the defense system and crafts an attack to violate its security; and (ii) the system designer analyzes  the newly-deployed attacks and designs novel countermeasures against them. However, reactive approaches are clearly not able to prevent the risk of never-before-seen attacks. To this end, the designer should follow a proactive approach to anticipate the attacker by (i) identifying relevant threats against the system under design and simulating the corresponding attacks, (ii) devising suitable countermeasures (if retained necessary), and (iii) repeating this process before system deployment (Fig. <ref type="figure" target="#fig_0">1</ref>, right). In practice, these steps are facilitated by leveraging a thorough model of the attacker, as that discussed in the next section, which helps envisioning and analyzing a number of potential attack scenarios against learning-based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Know Your Adversary: Modeling Threats</head><p>"If you know the enemy and know yourself, you need not fear the result of a hundred battles." (Sun Tzu, The Art of War, 500 BC)</p><p>We discuss here the first golden rule of the proactive security cycle discussed in the previous section, i.e., how to model threats against learning-based systems and thoroughly evaluate their security against the corresponding attacks. To this end, we exploit a framework based on the popular attack taxonomy proposed in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b60">61]</ref> and subsequently extended in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, which enables one to envision different attack scenarios against learning algorithms and deep networks, and to implement the corresponding attack strategies. Notably, these attacks include training-time poisoning and test-time evasion attacks (also recently referred to as adversarial training and test examples) <ref type="bibr">[3-5, 14-16, 26, 29, 31-33, 38, 41, 61]</ref>. It consists of defining the attacker's goal, knowledge of the targeted system, and capability of manipulating the input data, to subsequently define an optimization problem corresponding to the optimal attack strategy. The solution to this problem provides a way to manipulate input data to achieve the attacker's goal. While this framework only considers attacks against supervised learning algorithms, we refer the reader to similar threat models to evaluate the security of clustering <ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref>, and feature selection algorithms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b64">65]</ref> under different attack settings. Notation. In the following, we denote the sample and label spaces with X and Y, respectively, and the training data with D = (x i , y i ) n i=1 , being n the number of training samples. We use L(D, w) to denote the loss incurred by the classifier f : X → Y (parameterized by w) on D. We assume that the classification function f is learned by minimizing an objective function L(D, w) on the training data. Typically, this is an estimate of the generalization error, obtained by the sum of the empirical loss L on D and a regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attacker's Goal</head><p>This aspect is defined in terms of the desired security violation, attack specificity, and error specificity, as detailed below. Security Violation. The attacker may aim to cause: an integrity violation, to evade detection without compromising normal system operation; an availability violation, to compromise the normal system functionalities available to legitimate users; or a privacy violation, to obtain private information about the system, its users or data by reverse-engineering the learning algorithm. Attack Specificity. It ranges from targeted to indiscriminate, respectively, depending on whether the attacker aims to cause misclassification of a specific set of samples (to target a given system user or protected service), or of any sample (to target any system user or protected service). Error Specificity. It can be specific, if the attacker aims to have a sample misclassified as a specific class; or generic, if the attacker aims to have a sample misclassified as any of the classes different from the true class. <ref type="foot" target="#foot_0">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attacker's Knowledge</head><p>The attacker can have different levels of knowledge of the targeted system, including: (k.i) the training data D; (k.ii) the feature set X; (k.iii) the learning algorithm f , along with the objective function L minimized during training; and, possibly, (k.iv) its (trained) parameters/hyper-parameters w. The attacker's knowledge can thus be characterized in terms of a space Θ, whose elements encode the components (k.i)-(k.iv) as θ = (D, X, f, w). Depending on the assumptions made on (k.i)-(k.iv), one can describe different attack scenarios. Perfect-Knowledge (PK) White-Box Attacks. Here the attacker is assumed to know everything about the targeted system, i.e., θ PK = (D, X, f, w). This setting allows one to perform a worst-case evaluation of the security of learning algorithms, providing empirical upper bounds on the performance degradation that may be incurred by the system under attack. Limited-Knowledge (LK) Gray-Box Attacks. One may consider here different settings, depending on the attacker's knowledge about each of the components (k.i)-(k.iv). Typically, the attacker is assumed to know the feature representation X and the kind of learning algorithm f (e.g., the fact that the classifier is linear, or it is a neural network with a given architecture, etc.), but neither the training data D nor the classifier's (trained) parameters w. The attacker is however assumed to be able to collect a surrogate data set D4 from a similar source (ideally sampling from the same underlying data distribution), and potentially get feedback from the classifier about its decisions to provide labels for such data. This enables the attacker to estimate the parameters ŵ from D, by training a surrogate classifier. We refer to this case as LK attacks with Surrogate Data (LK-SD), and denote it with θ LK−SD = ( D, X, f, ŵ) .</p><p>We refer to the setting in which the attacker does not even know the kind of learning algorithm f as LK attacks with Surrogate Learners (LK-SL), and denote it with θ LK−SL = ( D, X, f , ŵ). LK-SL attacks also include the case in which the attacker knows the learning algorithm, but optimizing the attack samples against it may be not tractable or too complex. In this case, the attacker can also craft the attacks against a surrogate classifier and test them against the targeted one. This is a common procedure used also to evaluate the transferability of attacks between learning algorithms, as firstly shown in <ref type="bibr" target="#b37">[38]</ref> and subsequently in <ref type="bibr" target="#b14">[15]</ref> for deep networks. Zero-Knowledge (ZK) Black-Box Attacks. Recent work has also claimed that machine learning can be threatened without any substantial knowledge of the feature space, the learning algorithm and the training data, if the attacker can query the system in a black-box manner and get feedback on the provided labels or confidence scores <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref>. This point deserves however some clarification. First, the attacker knows (as any other potential user) that the classifier is designed to perform some task (e.g., object recognition in images, malware classification, etc.), and has to clearly have an idea of which potential transformations to apply to cause some feature changes, other-wise neither change can be inflicted to the output of the classification function, nor any useful information can be extracted from it. For example, if one attacks a malware detector based on dynamic analysis by injecting static code that will never be executed, there will be no impact at all on the classifier's decisions. This means that, although the exact feature representation may be not known to the attacker, at least she knows (or has to get to know) which kind of features are used by the system (e.g., features based on static or dynamic analysis in malware detection). Thus, knowledge of the feature representation may be partial, but not completely absent. This is even more evident for deep networks trained on images, where the attacker knows that the input features are the image pixels.</p><p>Similar considerations hold for knowledge of the training data. If the attacker knows that the classifier is used for a specific task, it is clear the she also knows which kind of data has been used to train it; for example, if a deep network aims to discriminate among classes of animals, then it is clear that it has been trained on images of such animals. Hence, also in this case the attacker effectively has some knowledge of the training data, even if not of the exact training samples.</p><p>We thus characterize this setting as θ ZK = ( D, X, f , ŵ). Even if surrogate learners are not necessarily used here <ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref>, as well as in pioneering work on black-box attacks against machine learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b69">70]</ref>, one may anyway learn a surrogate classifier (potentially on a different feature representation) and check whether the crafted attack samples transfer to the targeted classifier. Feedback from classifier's decisions on carefully-crafted query samples can then be used to refine the surrogate model, as in <ref type="bibr" target="#b14">[15]</ref>. Although the problem of learning a surrogate model while minimizing the number of queries can be casted as an active learning problem, to our knowledge well-established active learning algorithms have not yet been compared against such recently-proposed approaches <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attacker's Capability</head><p>This characteristic depends on the influence that the attacker has on the input data, and on application-specific data manipulation constraints. Attack Influence. It can be causative, if the attacker can manipulate both training and test data, or exploratory, if the attacker can only manipulate test data. These scenarios are more commonly known as poisoning and evasion attacks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b60">61]</ref>. Data Manipulation Constraints. Another aspect related to the attacker's capability depends on the presence of applicationspecific constraints on data manipulation, e.g., to evade malware detection, malicious code has to be modified without compromising its intrusive functionality. This may be done against systems based on static code analysis, by injecting instructions or code that will never be executed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref>. These constraints can be generally accounted for in the definition of the optimal attack strategy by assuming that the initial attack samples D c can only be modified according to a space of possible modifications Φ(D c ). In some cases, this space can also be mapped in terms of constraints on the feature values of the attack samples; e.g., by imposing that feature values correspond-  <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Based only on classification accuracy in the absence of attack, one may prefer C 2 to C 1 . Simulating attacks of increasing strength (e.g., by increasing the level of perturbation in input images) may however reveal that more accurate classifiers may be less robust to adversarial perturbations. Thus, one may finally prefer</p><formula xml:id="formula_0">C 1 to C 2 .</formula><p>ing to occurrences of some instructions in static malware detectors can only be incremented <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Attack Strategy</head><p>Given the attacker's knowledge θ ∈ Θ and a set of manipulated attack samples D c ∈ Φ(D c ), the attacker's goal can be defined in terms of an objective function A(D c , θ) ∈ R which measures how effective the attacks D c are. The optimal attack strategy can be thus given as:</p><formula xml:id="formula_1">D c ∈ arg max D c ∈Φ(D c ) A(D c , θ)<label>(1)</label></formula><p>We show in Sect. 4 how this high-level formulation encompasses both evasion and poisoning attacks against supervised learning algorithms, despite it has been used also to attack clustering <ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref>, and feature selection algorithms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b64">65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Security Evaluation Curves</head><p>It is worth remarking here that, to provide a thorough security evaluation of learning algorithms, one should assess their performance not only under different assumptions on the attacker's knowledge, but also increasing the attack strength, i.e., the attacker's capability Φ(D c ) of manipulating the input data. For example, this can be done by increasing the amount of perturbation used to craft evasion attacks, or the number of poisoning attack points injected into the training data. The resulting security evaluation curve, conceptually represented in Fig. <ref type="figure" target="#fig_2">3</ref>, shows the extent to which the performance of a learning algorithm drops more or less gracefully under attacks of increasing strength. This is crucial to enable a fairer comparison among different attack algorithms and defenses in the context of concrete application examples <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, as we will discuss in the remainder of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Summary of Attacks against Machine Learning</head><p>Before delving into the details of specific attacks, we provide in Fig. <ref type="figure" target="#fig_4">4</ref> a simplified categorization of the main attacks against machine-learning algorithms based on the aforementioned threat model; in particular, considering the attacker's </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attacker's Goal Attacker's Capability</head><p>Querying strategies that reveal confidential information on the learning model or its users goal and main capabilities. The most common attacks, as discussed before, include evasion and poisoning availability attacks (aimed to maximize the test error) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b60">61]</ref>. More recently, different kinds of poisoning integrity attacks (which manipulate the training data or the trained model to cause specific misclassifications, as defined in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref>) against deep networks have been also studied under the name of backdoor and trojaning attacks <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref>. These attacks maliciously manipulate pre-trained network models to create specific backdoor vulnerabilities. The corrupted models are then publicly released, to favor their adoption in proprietary systems (e.g., via fine-tuning or other transfer learning techniques). When this happens, the attacker can activate the backdoors using specific input samples that are misclassified as desired. The underlying idea behind such attacks and their impact on the learning process is conceptually depicted in Fig. <ref type="figure">5</ref>.</p><p>All the aforementioned attacks can be successfully staged under different levels of the attacker's knowledge. When knowledge is limited, as in the gray-box and black-box cases, privacy or confidential attacks can be staged to gain further knowledge about the target classifier or its users. Although we do not thoroughly cover such attacks in detail here, we refer the reader to few practical examples of such attacks reported to date, including model-extraction attacks aimed to steal machine-learning models, and model-inversion and hill-climbing attacks against biometric systems used to steal the face and fingerprint templates of their users (or any other sensitive information) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref>.</p><p>4. Be Proactive: Simulating Attacks "To know your enemy, you must become your enemy." (Sun Tzu, The Art of War, 500 BC)</p><p>We discuss here how to formalize test-time evasion and training-time poisoning attacks in terms of the optimization problem given in Eq. ( <ref type="formula" target="#formula_1">1</ref>), and consistently with the threat model discussed in Sect. 3. Note that other attacks may also be described, generally, in terms of the aforementioned optimization problem, but we focus here only on those for which this connection is tighter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evasion attacks</head><p>Evasion attacks consist of manipulating input data to evade a trained classifier at test time. These include, e.g., manipula- tion of malware code to have the corresponding sample misclassified as legitimate, or manipulation of images to mislead object recognition. We consider here the formulation reported in <ref type="bibr" target="#b6">[7]</ref>, which extends our previous work <ref type="bibr" target="#b37">[38]</ref> from two-class to multiclass classifiers, by introducing error-generic and errorspecific maximum-confidence evasion attacks. With reference to Eq. ( <ref type="formula" target="#formula_1">1</ref>), the evasion attack samples D c can be optimized one at a time, independently, aiming to maximize the classifier's confidence associated to a wrong class. We will denote with f i (x) the confidence score of the classifier on the sample x for class i. These attacks can be optimized under different levels of attacker's knowledge through the use of surrogate classifiers, so we omit the distinction between f i (x) and fi (x) below for notational convenience. Error-generic Evasion Attacks. In this case, the attacker is interested in misleading classification, regardless of the output class predicted by the classifier. The problem can be thus formulated as:</p><formula xml:id="formula_2">max x A(x , θ) = Ω(x ) = max l k f l (x) − f k (x) ,<label>(2)</label></formula><formula xml:id="formula_3">s.t. d(x, x ) ≤ d max , x lb x x ub ,<label>(3)</label></formula><p>where f k (x) denotes the discriminant function associated to the true class k of the source sample x, and max l k f l (x) is the closest competing class (i.e., the one exhibiting the highest value of the discriminant function among the remaining classes). The underlying idea behind this attack formulation, similarly to <ref type="bibr" target="#b4">[5]</ref>, is to ensure that the attack sample will be no longer classified correctly as a sample of class k, but rather misclassified as a sample of the closest candidate class. The manipulation constraints Φ(D c ) are given in terms of: (i) a distance constraint d(x, x ) ≤ d max , which sets a bound on the maximum input perturbation between x (i.e., the input sample) and the corresponding modified adversarial example x ; and (ii) a box constraint x lb x x ub (where u v means that each element of u has to be not greater than the corresponding element in v), which bounds the values of the attack sample x .</p><p>For images, the former constraint is used to implement either dense or sparse evasion attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78]</ref>. Normally, the 2 and the ∞ distances between pixel values are used to cause an indistinguishable image blurring effect (by slightly manipulating all pixels). Conversely, the 1 distance corresponds to a sparse attack in which only few pixels are significantly manipulated, yielding a salt-and-pepper noise effect on the image <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78]</ref>. In the image domain, the box constraint can be used to bound each pixel value between 0 and 255, or to ensure manipulation of only a specific region of the image. For example, if some pixels should not be manipulated, one can set the corresponding values of x lb and x ub equal to those of x. This is of interest to create real-world adversarial examples, as it avoids the manipulation of background pixels which do not belong to the object of interest <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. Similar constraints have been applied also for evading learning-based malware detectors <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78]</ref>. Error-specific Evasion Attacks. In the error-specific setting, the attacker aims to mislead classification, but she requires the adversarial examples to be misclassified as a specific class. The problem is formulated similarly to error-generic evasion (Eqs. 2-3), with the only differences that: (i) the objective function A(x , θ) = −Ω(x ) has opposite sign; and (ii) f k denotes the discriminant function associated to the targeted class, i.e., the class which the adversarial example should be (wrongly) assigned to. The rationale in this case is to maximize the confidence assigned to the wrong target class f k , while minimizing the probability of correct classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. Attack Algorithm. The two evasion settings are conceptually depicted in Fig. <ref type="figure" target="#fig_6">6</ref>. Both can be solved through a straightforward gradient-based attack, for differentiable learning algorithms (including neural networks, SVMs with differentiable kernels, etc.) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38]</ref>. Non-differentiable learning algorithms, like decision trees and random forests, can be attacked with more com-plex strategies <ref type="bibr" target="#b78">[79]</ref> or using the same algorithm against a differentiable surrogate learner <ref type="bibr" target="#b77">[78]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Application Example</head><p>We report here an excerpt of the results from our recent work <ref type="bibr" target="#b6">[7]</ref>, where we have constructed adversarial examples aimed to fool the robot-vision system of the iCub humanoid. <ref type="foot" target="#foot_3">5</ref>This system uses a deep network to compute a set of deep features from input images (i.e., by extracting the output of the penultimate layer of the network), and then learns a multiclass classifier on this representation for recognizing 28 different objects, including cups, detergents, hair sprayers, etc. The results for error-specific evasion (averaged on different target classes) are reported in Fig. <ref type="figure" target="#fig_7">7</ref>, along with some examples of perturbed input images at different levels. We trained multiclass linear SVMs (SVM), SVMs with the RBF kernel (SVM-RBF), and also a simple defense mechanism against adversarial examples based on rejecting samples that are sufficiently far (in deep space) from known training instances (SVM-adv). This will be discussed more in detail in Sect. 5.2.1 (see also Fig. <ref type="figure" target="#fig_12">12</ref> for a conceptual representation of this defense mechanism). The security evaluation curves in Fig. <ref type="figure" target="#fig_7">7</ref> show how classification accuracy decreases against an increasing 2 maximum admissible perturbation d max . Notably, the rejection mechanism of SVMadv is only effective for low input perturbations (at the cost of some additional misclassifications in the absence of attack). For higher perturbation levels, the deep features of the manipulated attacks become indistinguishable to those of the samples of the targeted class, although the input image is still far from resembling a different object. This phenomenon is connected to the instability of the deep representation learned by the underlying deep network. We refer the reader to <ref type="bibr" target="#b6">[7]</ref> for further details, and to <ref type="bibr" target="#b79">[80]</ref> (and references therein) for the problem of generating adversarial examples in the physical world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Historical Remarks</head><p>We conclude this section with some historical remarks on evasion attacks, with the goal of providing a better understanding of the connections with recent work on adversarial examples and the security of deep learning.</p><p>Evasion attacks have a long tradition. As mentioned in Sect. 1, back in 2004-2006, work in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b80">81]</ref> reported preliminary attempts in evading statistical anti-spam filters and malware detectors with ad-hoc evasion strategies. The very first evasion attacks against linear classifiers were systematized in the same period in <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, always considering spam filtering as a running example. The underlying idea was to manipulate the content of spam emails by obfuscating bad words and/or adding good words. To reduce the number of manipulated words in each spam, and preserve message readability, the idea was to modify first words which were assigned the highest absolute weight values by the linear text classifier. Heuristic countermeasures were also proposed before 2010 <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b81">82]</ref>, based on the intuition of learning linear classifiers with more uniform feature weights, to require the attacker to modify more words to get her spam misclassified. To summarize, the vulnerability of linear classifiers to evasion attacks was a known problem even prior to 2010, and simple, heuristic countermeasures were already under development. Meanwhile, <ref type="bibr">Barreno et al.</ref> (see <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40]</ref> and references therein) were providing an initial overview of the vulnerabilities of machine learning from a more general perspective, highlighting the need for adversarial machine learning, i.e., to develop learning algorithms that explicitly account for the presence of the attacker <ref type="bibr" target="#b60">[61]</ref>.</p><p>At that time, the idea that nonlinear classifiers could be more robust than linear ones against evasion was also becoming popular. In 2013, Šrndić and Laskov <ref type="bibr" target="#b82">[83]</ref> proposed a learningbased PDF malware detector, and attacked it to test its vulnerability to evasion. They reported that:</p><p>The most aggressive evasion strategy we could conceive was successful for only 0.025% of malicious examples tested against a nonlinear SVM classifier with the RBF kernel [...] we do not have a rigorous mathematical explanation for such a surprising robustness. Our intuition suggests that [...] the space of true features is hidden behind a complex nonlinear transformation which is mathematically hard to invert. [...] hence, the robustness of the RBF classifier must be rooted in its nonlinear transformation.</p><p>Today we know that this hypothesis about the robustness of nonlinear classifiers is wrong. The fact that a system could be more secure against an attack not specifically targeted against it does not provide any further meaningful information about its security to more powerful worst-case attacks. Different systems (and algorithms) should be tested under the same (worst-case) assumptions on the underlying threat model. In particular, it is not difficult to see that the attack developed in that work was somehow crafted to evade linear classifiers, but not sufficiently complex to fool nonlinear ones.</p><p>While reading that work, it was thus natural to ask ourselves: "what if the attack is carefully-crafted against nonlinear classifiers, instead? How can we invert such complex nonlinear transformation to understand which features are more relevant to the classification of a sample, and change them?" The answer was readily available: the gradient of the classification function is exactly what specifies the direction of maximum variation of the function with respect to the input features. Thus, we decided to formulate the evasion of a nonlinear classifier similarly to what we did in <ref type="bibr" target="#b81">[82]</ref> for linear classifiers, in terms of an optimization problem that minimizes the discriminant function f (x) such that x is misclassified as legitimate with maximum confidence, under a maximum amount of possible changes to its feature vector.</p><p>In a subsequent paper <ref type="bibr" target="#b37">[38]</ref>, we implemented the aforementioned strategy and showed how to evade nonlinear SVMs and neural networks through a straightforward gradient-descent attack algorithm. In the same work, we also reported the first "adversarial examples" on MNIST handwritten digit data against nonlinear learning algorithms. We furthermore showed that, when the attacker does not have perfect knowledge of the tar- geted classifier, a surrogate classifier can be learned on surrogate training data, and used to craft the attack samples which then transfer with high probability to the targeted model. This was also the first experiment showing that examples can be transferred, at least in a gray-box setting (training the same algorithm on different data). Notably, Šrndić and Laskov <ref type="bibr" target="#b38">[39]</ref> subsequently exploited this attack to show that PDF malware detectors based on nonlinear learning algorithms were also vulnerable to evasion, conversely to what they supposed in <ref type="bibr" target="#b82">[83]</ref>.</p><p>More recently, we have also exploited the theoretical findings in <ref type="bibr" target="#b83">[84]</ref>, which connect regularization and robustness in kernelbased classifiers, to provide a theoretically-sound countermeasure for linear classifiers against evasion attacks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b76">77]</ref>. These recent developments have enabled a deeper understanding on how to defend against evasion attacks in spam filtering and malware detection, also clarifying (in a formal manner) the intuitive idea of uniform feature weights only heuristically provided in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b81">82]</ref>. In particular, we have recently shown how a proper, theoretically-grounded regularization scheme can significantly outperform heuristic approaches in these contexts <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b76">77]</ref>. Security of Deep Learning. In 2014-2015, Szegedy et al. <ref type="bibr" target="#b1">[2]</ref> and subsequent work <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> showed that deep networks can be fooled by well-crafted, minimally-perturbed input images at test time, called adversarial examples. These samples are obtained by minimizing the distance of the adversarial sample x to the corresponding source sample x under the constraint that the predicted label is different, i.e., min x d(x, x ) s.t. f (x) f (x ). Interestingly, the parallel discovery of such gradient-based adversarial perturbations by Szegedy et al. <ref type="bibr" target="#b1">[2]</ref> and Biggio et al. <ref type="bibr" target="#b37">[38]</ref> started from different premises, as also explained by Ian Goodfellow in one of his popular talks. <ref type="foot" target="#foot_4">6</ref> While we were investigating how to evade detection by learning algorithms in security-related tasks with a clear adversarial nature (like spam and malware detection) <ref type="bibr" target="#b37">[38]</ref>, Szegedy et al. <ref type="bibr" target="#b1">[2]</ref> were trying to interpret and visualize the salient characteristics learned by deep networks. To this end, they started looking for minimal changes to images in the input space that cause misclassifications. They expected to see significant changes to the background of the image or to the structure and aspect of the depicted objects, while it turned out that, quite surprisingly, such modifications were almost imperceptible to the human eye. This discovery has raised an enormous interest in both the computer vision and security communities which, since then, have started proposing novel security assessment methodologies, attacks and countermeasures to mitigate this threat, independently re-discovering many other aspects and phenomena that had been already known, to some extent, in the area of adversarial machine learning. An example is given by the use of surrogate classifiers with smoother decision functions <ref type="bibr" target="#b14">[15]</ref> to attack models that either mask gradients or are not end-to-end differentiable <ref type="bibr" target="#b5">[6]</ref>. One of such defense mechanisms, known as distillation <ref type="bibr" target="#b5">[6]</ref>, has indeed shown to be vulnerable to attacks based on surrogate classifiers <ref type="bibr" target="#b14">[15]</ref>, essentially leveraging the idea behind limited-knowledge evasion attacks that we first discussed in <ref type="bibr" target="#b37">[38]</ref>. Iterative attacks based on projected gradients have also been independently re-discovered <ref type="bibr" target="#b103">[104]</ref>, despite they had been used before against other classifiers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Misconceptions on Evasion Attacks. The main misconception that is worth highlighting here is that adversarial examples should be minimally perturbed. The motivation of this misconception is easy to explain. The notion of adversarial examples was initially introduced to analyze the instability of deep networks <ref type="bibr" target="#b1">[2]</ref>, i.e., their sensitivity to minimal input perturbations; the goal of the initial work on adversarial examples was not to to perform a detailed security assessment of a machine-learning   is highlighted with markers of the same color (as reported in the legend) and connected with dashed lines to highlight independent (but related) findings. The date of publication refers to publication on peer-reviewed conferences and journals. algorithm using security evaluation curves (Fig. <ref type="figure" target="#fig_7">7</ref>). Normally, as already discussed in this paper and also in our previous work <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref>, for the purpose of thoroughly assessing the security of a learning algorithm under attack, given a feasible space of modifications to the input data, it is more reasonable to assume that the attacker will aim to maximize the classifier's confidence on the desired output class, rather than only minimally perturbing the attack samples (cf. Eqs. 2-3). For this reason, while minimally-perturbed adversarial examples can be used to analyze the sensitivity of a learning algorithm, maximumconfidence attacks are more suitable for a thorough security assessment of learning algorithms under attack. In particular, the use of the security evaluation curves described above gives us a clearer understanding of the security of a learning algorithm under more powerful attacks. By increasing the attack strength (i.e., the maximum amount of perturbation applicable to the input data), one can draw a complete security evaluation curve, reporting the evasion rate for each value of attack strength. This ensures us that, e.g., if the noise applied to the input data is not larger than , then the classification performance should not drop more than δ. Conversely, using minimally-perturbed adversarial examples one can only provide guarantees against an average level of perturbation (rather than a worst-case bound). The fact that maximum-or high-confidence attacks are better suited to the task of security evaluation of learning algorithms (as well as to improve transferability across different models) is also witnessed by the work by Carlini and Wagner <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b84">85]</ref> and follow-up work in <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b86">87]</ref>. In that work, the authors exploited a similar idea to show that several recent defenses proposed against minimally-perturbed adversarial examples are vulnerable to maximum-confidence ones, using a stronger attack similar to those proposed in our earlier work, and discussed in Sect. 4.1 <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref>. Even in the domain of malware detection, adversarial examples seem to be a novel threat <ref type="bibr" target="#b10">[11]</ref>, while the vulnerability of learning-based malware detectors to evasion is clearly a consolidated issue <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref>. Other interesting avenues to provide reliable guarantees on the security of neural networks include formal verification <ref type="bibr" target="#b87">[88]</ref> and evaluation methods inspired from software testing <ref type="bibr" target="#b88">[89]</ref>. Timeline of Evasion Attacks. To summarize, while the security of deep networks has received considerable attention from different research communities only recently, it is worth remarking that several related problems and solutions had been already considered prior to 2014 in the field of adversarial machine learning. Maximum-confidence evasion attacks and surrogate models are just two examples of similar findings in both areas of research. We compactly and conceptually highlight these connections in the timeline reported in Fig. <ref type="figure" target="#fig_9">8</ref>. <ref type="foot" target="#foot_5">7</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Poisoning Attacks</head><p>Poisoning attacks aim to increase the number of misclassified samples at test time by injecting a small fraction of poisoning samples into the training data. These attacks, conversely to evasion, are staged at the training phase. A conceptual example of how poisoning works is given in Fig. <ref type="figure">9</ref>. As for evasion attacks, we discuss here error-generic and error-specific poisoning attacks in a PK white-box setting, given that the extension to gray-box and black-box settings is trivial through the use of surrogate learners <ref type="bibr" target="#b32">[33]</ref>.</p><p>Error-Generic Poisoning Attacks. In this case, the attacker aims to cause a denial of service, by inducing as many misclassifications as possible (regardless of the classes in which they occur). Poisoning attacks are generally formulated as bilevel optimization problems, in which the outer optimization maximizes the attacker's objective A (typically, a loss function L computed on untainted data), while the inner optimization amounts to learning the classifier on the poisoned training data <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>. This can be made explicit by rewriting Eq. ( <ref type="formula" target="#formula_1">1</ref>) as:</p><formula xml:id="formula_4">D c ∈ arg max D c ∈Φ(D c ) A(D c , θ) = L(D val , w ) ,<label>(4) s</label></formula><formula xml:id="formula_5">.t. w ∈ arg min w ∈W L(D tr ∪ D c , w ) ,<label>(5)</label></formula><p>where D tr and D val are two data sets available to the attacker. The former, along with the poisoning attack samples D c , is used to train the learner on poisoned data, while the latter is used to evaluate its performance on untainted data, through the loss function L(D val , w ). Notably, the objective function implicitly depends on D c through the parameters w of the poisoned classifier.</p><p>Error-Specific Poisoning Attacks. In this setting the attacker aims to cause specific misclassifications. While the problem remains that given by Eqs. ( <ref type="formula" target="#formula_4">4</ref>)-( <ref type="formula" target="#formula_5">5</ref>), the objective is redefined as A(D c , θ) = −L(D val , w ). The set D val contains the same samples as D val , but their labels are chosen by the attacker according to the desired misclassifications. The objective L is then taken with opposite sign as the attacker effectively aims to minimize the loss on her desired labels <ref type="bibr" target="#b32">[33]</ref>. Attack Algorithm. A common trick used to solve the given bilevel optimization problems is to replace the inner optimization by its equilibrium conditions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>. This enables gradient computation in closed form and, thus, similarly to the evasion case, the derivation of gradient-based attacks (although gradient-based poisoning is much more computationally demanding, as it requires retraining the classifier iteratively on the modified attack samples). In the case of deep networks, this approach is not practical due to computational complexity and instability of the closed-form gradients. To tackle this issue, we have recently proposed a more efficient technique, named back-gradient poisoning. It relies on automatic differentiation and on reversing the learning procedure to compute the gradient of interest (see <ref type="bibr" target="#b32">[33]</ref> for further details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Application Example</head><p>We report here an exemplary poisoning attack against a multiclass softmax classifier (logistic regression) trained on MNIST handwritten digits belonging to class 0, 4, and 9. We consider error-generic poisoning, using 200 (clean) training samples and 2000 validation and test samples. Results of back-gradient poisoning compared to randomly-injected training points with wrong class labels (random label flips) are reported in Fig. <ref type="figure" target="#fig_10">10</ref>, along with some adversarial training examples generated by our back-gradient poisoning algorithm. 0% (0.01) Figure <ref type="figure">9</ref>: Conceptual example of how poisoning attacks compromise a linear classifier <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>. Each plot shows the training data (red and blue points) and the corresponding trained classifier (black solid line). The fraction of poisoning points injected into the training set is reported on top of each plot, along with the test error (in parentheses) of the poisoned classifier. Poisoning points are optimized through a gradient-based attack algorithm. They are initialized by cloning the training points denoted with white crosses and flipping their label. The gradient trajectories (black dashed lines) are then followed up to some local optima to obtain the final poisoning points (highlighted with black circles). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Historical Remarks</head><p>To our knowledge, the earliest poisoning attacks date back to 2006-2010 <ref type="bibr">[23-25, 27, 90]</ref>. Newsome et al. <ref type="bibr" target="#b89">[90]</ref> devised an attack to mislead signature generation for malware detection; Nelson et al. <ref type="bibr" target="#b23">[24]</ref> showed that spam filters can be compromised to misclassify legitimate email as spam, by learning spam emails containing good words during training; and Rubinstein et al. <ref type="bibr" target="#b24">[25]</ref> showed how to poison an anomaly detector trained on network traffic through injection of chaff traffic. In the meanwhile, exemplary attacks against learning-based centroid anomaly detectors where also demonstrated <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Using a similar formalization, we have also recently showed poisoning attacks against biometric systems <ref type="bibr" target="#b42">[43]</ref>. This background paved the way to subsequent work that formalized poisoning attacks against more complex learning algorithms (including SVMs, ridge regression, and LASSO) as bilevel optimization problems <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>. Recently, preliminary attempts towards poisoning deep networks have also been reported, showing the first adversarial training examples against deep learners <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>It is worth finally remarking that poisoning attacks against machine learning should not be considered an academic exercise in vitro. Microsoft Tay, a chatbot designed to talk to youngsters in Twitter, was shut down after only 16 hours, as it started raising racist and offensive comments after being poisoned. <ref type="foot" target="#foot_6">8</ref> Its artificial intelligence was designed to mimic the behavior of humans, but not to recognize potential misleading behaviors. Kaspersky Lab, a leading antivirus company, has been accused of poisoning competing antivirus products through the injection of false positive examples into VirusTotal, <ref type="foot" target="#foot_7">9</ref> although it is worth saying that they denied any wrongdoing, and blamed for spreading false rumors. Another avenue for poisoning arises from the fact that shared, big and open data sets are commonly used to train machine-learning algorithms. The case of Ima-geNet for object recognition is paradigmatic. In fact, people typically reuse these large-scale deep networks as feature extractors inside their pattern recognition tools. Imagine what may happen if someone could poison these data "reservoirs": many data-driven products and services could experience security and privacy issues, economic losses, with legal and ethical implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Protect Yourself: Security Measures for Learning Algorithms</head><p>"What is the rule? The rule is protect yourself at all times." (from the movie Million dollar baby, 2004)</p><p>In this section we discuss the third golden rule of the security-by-design cycle for pattern classifiers, i.e., how to react to past attacks and prevent future ones. We categorize the corresponding defenses as depicted in Fig. <ref type="figure" target="#fig_11">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Reactive Defenses</head><p>Reactive defenses aim to counter past attacks. In some applications, reactive strategies may be even more convenient </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reactive Defenses</head><p>Proactive Defenses and effective than pure proactive approaches aimed to solely mitigate the risk of potential future attacks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b90">91]</ref>. Reactive approaches include: (i) timely detection of novel attacks, (ii) frequent classifier retraining, and (iii) verification of consistency of classifier decisions against training data and ground-truth labels <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b50">51]</ref>. In practice, to timely identify and block novel security threats, one can leverage collaborative approaches and honeypots, i.e., online services purposely vulnerable with the specific goal of collecting novel spam and malware samples. To correctly detect recently-reported attacks, the classifier should be frequently retrained on newly-collected data (including them), and novel features and attack detectors may also be considered (see, e.g., the spam arms race discussed in Sect.</p><p>2). This procedure should also be automated to some extent to act more readily when necessary; e.g., using automatic drift detection techniques <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b91">92]</ref>. The correctness of classifier decisions should finally be verified by expert domains. This raises the issue of how to involve humans in the loop in a more coordinated manner, to supervise and verify the correct functionality of learning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Proactive Defenses</head><p>Proactive defenses aim to prevent future attacks. The main ones proposed thus far can be categorized according to the paradigms of security by design and security by obscurity, as discussed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Security-by-Design Defenses against White-box Attacks</head><p>The paradigm of security by design advocates that a system should be designed from the ground up to be secure. Based on this idea, several learning algorithms have been adapted to explicitly take into account different kinds of adversarial data manipulation. These defenses are designed in a white-box setting in which the attacker is assumed to have perfect knowledge of the attacked system. There is thus no need to probe the targeted classifier to improve knowledge about its behavior (as instead done in gray-box and black-box attacks). Countering Evasion Attacks. In 2004, Dalvi et al. <ref type="bibr" target="#b18">[19]</ref> proposed the first adversary-aware classifier against evasion attacks, based on iteratively retraining the classifier on the simulated attacks. This is not very different from the idea of adversarial training that has been recently used in deep networks to counter adversarial examples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, or to harden decision trees and random forests <ref type="bibr" target="#b78">[79]</ref>. These defenses are however heuristic, with no formal guarantees on convergence and robustness properties. More theoretically-sound approaches relying on game theory have been proposed to overcome these limitations. Zero-sum games have been formulated to learn invariant transformations like feature insertion, deletion and rescaling <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. <ref type="foot" target="#foot_8">10</ref> More rigorous approaches have then introduced Nash and Stackelberg games for secure learning, deriving formal conditions for existence and uniqueness of the game equilibrium, under the assumption that each player knows everything about the opponents and the game <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b93">94]</ref>. Randomized players <ref type="bibr" target="#b46">[47]</ref> and uncertainty on the players' strategies <ref type="bibr" target="#b94">[95]</ref> have also been considered to simulate less pessimistic scenarios. Despite these approaches seem promising, understanding the extent to which the resulting attack strategies are representative of practical scenarios remains an open issue <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b96">97]</ref>. Adversarial learning is not a (board) game with well-defined rules and, thus, the objective functions of real-world attackers may not even correspond to those hypothesized in the aforementioned games. It may be thus interesting to verify, reactively, whether real-world attackers behave as hypothesized, and exploit feedback from the observed attacks to improve the definition of the attack strategy. Another relevant problem of these approaches is their scalability to large datasets and high-dimensional feature spaces, as it may be too computationally costly to generate a sufficient number of attack samples to correctly represent their distribution, i.e., to effectively tackle the curse of dimensionality.</p><p>A more efficient approach relies on robust optimization. Robust optimization formulates adversarial learning as a minimax problem in which the inner problem maximizes the training loss by manipulating the training points under worst-case, bounded perturbations, while the outer problem trains the learning algorithm to minimize the corresponding worst-case training loss <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b103">104</ref>]. An interesting result by Xu et al. <ref type="bibr" target="#b83">[84]</ref> has shown that the inner problem can be solved in closed form, at least for linear SVMs, yielding a standard regularized loss formulation that penalizes the classifier parameters using the dual norm of the input noise. This means that different regularizers amount to hypothesizing different kinds of bounded worst-case noise on the input data. This has effectively established an equivalence between regularized learning problems and robust optimization, which has in turn enabled approximating computationally-demanding secure learning models (e.g., game-theoretical ones) with more efficient ones based on regularizing the objective function in a specific manner <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78]</ref>, also in structured learning <ref type="bibr" target="#b98">[99]</ref>. Notably, recent work has also derived an extension of this formulation to nonlinear classifiers <ref type="bibr" target="#b99">[100]</ref>. The main effect of the aforementioned techniques is to smooth out the decision function of the classifier, making it less sensitive to worst-case input changes. This in turn means reducing the norm of the input gradients. More direct (and sometimes equivalent) approaches obtain the same effect by penalizing the input gradients using specific regularization terms <ref type="bibr" target="#b100">[101]</ref><ref type="bibr" target="#b101">[102]</ref><ref type="bibr" target="#b102">[103]</ref><ref type="bibr" target="#b103">[104]</ref>.</p><p>Another line of defenses against evasion attacks is based on detecting and rejecting samples which are sufficiently far from the training data in feature space (similarly to the defense discussed in Sect. 4.1.1) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b105">106]</ref>. These samples are usually referred to as blind-spot evasion points, as they appear in regions of the feature space scarcely populated by training data. These regions can be assigned to any class during classifier training without any substantial increase in the training loss. In practice, this is a simple consequence of the stationarity assumption underlying many machine-learning algorithms (according to which training and test data come from the same distribution) <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b107">108]</ref>, and such rejection-based defenses simply aim to overcome this issue.</p><p>Finally, we point out that classifier ensembles have been also exploited to improve security against evasion attempts (e.g., by implementing rejection-based mechanisms or secure fusion rules) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b109">110]</ref> and even against poisoning attacks <ref type="bibr" target="#b110">[111]</ref>. They may however worsen security if the classifiers are not properly combined <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b104">105]</ref>. Many other heuristic defense techniques have also been proposed; e.g., training neural networks with bounded activation functions and training data augmentation to improve stability to input changes <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b112">113]</ref>.</p><p>Effect on Decision Boundaries. We aim to discuss here how the proposed defenses substantially change the way classifiers learn their decision boundaries. Notably, defenses involving retraining on the attack samples and rejection mechanisms achieve security against evasion by essentially countering blind-spot attacks. One potential effect of this assumption is that the resulting decision functions may tend to enclose the (stationary) training classes more tightly. This in turn may require one to trade-off between the security against potential at-tacks and the number of misclassified (stationary) samples at test time, as empirically shown in Sect. 4.1.1, and conceptually depicted in Fig. <ref type="figure" target="#fig_12">12</ref> The other relevant effect, especially induced by regularization methods inspired from robust optimization, is to provide a noise-specific margin between classes, as conceptually represented in Fig. <ref type="figure" target="#fig_14">13</ref>  <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78]</ref>. These are the two main effects induced by the aforementioned secure learning approaches in feature space.</p><p>It is finally worth remarking that, by using a secure learning algorithm, one can counter blind-spot evasion samples, but definitely not adversarial examples whose feature vectors become indistinguishable from those of training samples belonging to different classes. In this case, indeed, any learning algorithm would not be able to tell such samples apart <ref type="bibr" target="#b113">[114]</ref>. The security properties of learning algorithms should be thus considered independently from those exhibited by the chosen feature representation. Security of features should be considered as an additional, important requirement; features should not only be discriminant, but also robust to manipulation, to avoid straightforward classifier evasion by mimicking the feature values exhibited by legitimate samples. In the case of deep convolutional networks, most of the problems arise from the fact that the learned mapping from input to deep space (i.e., the feature representation) violates the smoothness assumption of learning algorithms: samples that are close in input space may be very far in deep space. In fact, as also reported in Sect. 4.1.1, adversarial examples in deep space become indistinguishable from training samples of other classes for sufficiently-high adversarial input perturbations <ref type="bibr" target="#b6">[7]</ref>. Therefore, this vulnerability can only be patched by retraining or re-engineering the deeper layers of the network (and not only the last ones) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Countering Poisoning Attacks. While most work focused on countering evasion attacks at test time, some white-box defenses have also been proposed against poisoning attacks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b114">[115]</ref><ref type="bibr" target="#b115">[116]</ref><ref type="bibr" target="#b116">[117]</ref><ref type="bibr" target="#b117">[118]</ref><ref type="bibr" target="#b118">[119]</ref>. To compromise a learning algorithm during training, an attack has to be exhibit different characteristics from those shown by the rest of the training data (otherwise it would have no impact at all) <ref type="bibr" target="#b110">[111]</ref>. Poisoning attacks can be thus regarded as outliers, and countered using data sanitization (i.e., attack detection and removal) <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b117">118]</ref>, and robust learning (i.e., learning algorithms based on robust statistics that   <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78]</ref>. The feasible domain of adversarial modifications (characterizing the equivalent robust optimization problem) is shown for some training points, respectively with 2 , 1 and ∞ balls. Note how the shape of these balls influences the orientation of the decision boundaries, i.e., how different regularizers optimally counter specific kinds of adversarial noise. are intrinsically less sensitive to outlying training samples, e.g., via bounded losses or kernel functions) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b116">117,</ref><ref type="bibr" target="#b118">[119]</ref><ref type="bibr" target="#b119">[120]</ref><ref type="bibr" target="#b120">[121]</ref><ref type="bibr" target="#b121">[122]</ref>.</p><p>5.2.2. Security-by-Obscurity Defenses against Black-box Attacks These proactive defenses, also known as disinformation techniques in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b60">61]</ref>, follow the paradigm of security by obscurity, i.e., they hide information to the attacker to improve security. These defenses aim to counter gray-box and blackbox attacks in which probing mechanisms are used to improve surrogate models or refine evasion attempts by querying the targeted classifier. Some examples include <ref type="bibr" target="#b50">[51]</ref>: (i) randomizing collection of training data (collect at different timings, and locations); (ii) using difficult to reverse-engineer classifiers (e.g., classifier ensembles); (iii) denying access to the actual classifier or training data; and (iv) randomizing the classifier's output to give imperfect feedback to the attacker. The latter approach has been firstly proposed in 2008 <ref type="bibr" target="#b44">[45]</ref> as an effective way to hide information about the classification function to the attacker, with recent follow-ups in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47]</ref> to counter adversarial examples. However, it is still an open issue to understand whether and to which extent randomization may be used to make it harder for the attacker to learn a proper surrogate model, and to implement privacy-preserving mechanisms <ref type="bibr" target="#b122">[123]</ref> against model inversion and hill-climbing attacks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref>.</p><p>Notably, security-by-obscurity defenses may not always be helpful. Gradient masking has been proposed to hide the gradient direction used to craft adversarial examples <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, but it has been shown that it can be easily circumvented with surrogate learners <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38]</ref>, exploiting the same principle behind attacking non-differentiable classifiers (discussed in Sect. 4.1) <ref type="bibr" target="#b77">[78]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In this paper, we have presented a thorough overview of work related to the security of machine learning, pattern recognition, and deep neural networks, with the goal of providing a clearer historical picture along with useful guidelines on how to assess and improve their security against adversarial attacks.</p><p>We conclude this work by discussing some future research paths arising from the fact that machine learning has been originally developed for closed-world problems where the possible "states of nature" and "actions" that a rationale agent can implement are perfectly known. Using the words of a famous speech by Donald Rumsfeld, one could argue that machine learning can deal with known unknowns. <ref type="foot" target="#foot_9">11</ref> Unfortunately, adversarial machine learning often deals with unknown unknowns. When learning systems are deployed in adversarial environments in the open world, they can misclassify (with high-confidence) never-before-seen inputs that are largely different from known training data. We know that unknown unknowns are the real threat in many security problems (e.g., zero-day attacks in computer security). Although they can be mitigated using the proactive approach described in this work, they remain a primary open issue for adversarial machine learning, as modeling attacks relies on known unknowns, while unknown unknowns are unpredictable.</p><p>We are firmly convinced that new research paths should be explored to address this fundamental issue, complementary to formal verification and certified defenses <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b117">118]</ref>. Machine learning algorithms should be able to detect unknown unknowns using robust methods for anomaly or novelty detection, potentially asking for human intervention when required. The development of practical methods for explaining, visualizing and interpreting the operation of machine-learning systems could also help system designers to investigate the behavior of such systems on cases that are not statistically represented by the training data, and decide whether to trust their decisions on such unknown unknowns or not. These future research paths lie at the intersection of the field of adversarial machine learning and the emerging fields of robust artificial intelligence and interpretability of machine learning <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b124">125]</ref>. We believe that these directions will help our society to get a more conscious understanding of the potential and limits of modern data-driven machine-learning technologies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A conceptual representation of the reactive (left) and proactive (right) arms races for pattern recognition and machine learning systems in computer security [41, 42].</figDesc><graphic url="image-1.png" coords="3,77.34,208.96,171.62,78.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of clean (top) and obfuscated (bottom) spam images [57].</figDesc><graphic url="image-2.png" coords="3,77.08,288.75,171.62,78.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Security evaluation curves for two hypothetical classifiers C 1 and C 2 , inspired from the methodology proposed in<ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Based only on classification accuracy in the absence of attack, one may prefer C 2 to C 1 . Simulating attacks of increasing strength (e.g., by increasing the level of perturbation in input images) may however reveal that more accurate classifiers may be less robust to adversarial perturbations. Thus, one may finally prefer C 1 to C 2 .</figDesc><graphic url="image-3.png" coords="5,100.40,94.67,160.99,88.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Categorization of attacks against machine learning based on our threat model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>http://pralab.diee.unica.it Training data (poisoned) Backdoored stop sign (labeled as speedlimit) Backdoor Attacks (Poisoning Integrity Attacks) T. Gu, B. Dolan-Gavitt, and S. Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. In NIPS Workshop on Machine Learning and Computer Security, 2017. X. Chen, C. Liu, B. Li, K. Lu, and D. Song. Targeted backdoor attacks on deep learning systems using data poisoning. ArXiv e-prints, 2017. M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar. Can machine learning be secure? In Proc. ACM Symp. Information, Computer and Comm. Sec., ASIACCS '06, pages 16-25, New York, NY, USA, 2006. ACM. M. Barreno, B. Nelson, A. Joseph, and J. Tygar. The security of machine learning. Machine Learning, 81:121-148, 2010. B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In J. Langford and J. Pineau, editors, 29th Int'l Conf. on Machine Learning, pages 1807-1814. Omnipress, 2012. B. Biggio, G. Fumera, and F. Roli. Security evaluation of pattern classifiers under attack. IEEE Transactions on Knowledge and Data Engineering, 26(4):984-996, April 2014. H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli. Is feature selection secure against training data poisoning? In F. Bach and D. Blei, editors, JMLR W&amp;CP -Proc. 32nd Int'l Conf. Mach. Learning (ICML), volume 37, pages 1689-1698, 2015. L. Munoz-Gonzalez, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee, E. C. Lupu, and F. Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In 10th ACM Workshop on Artificial Intelligence and Security, AISec '17, pp. 27-38, 2017. ACM. B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning. ArXiv e-prints, 2018. M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In 39th IEEE Symp. on Security and Privacy, 2018. Attack referred to as backdoor Attack referred to as 'poisoning integrity' Backdoor / poisoning integrity attacks place mislabeled training points in a region of the feature space far from the rest of training data. The learning algorithm labels such region as desired, allowing for subsequent intrusions / misclassifications at test time Training data (no poisoning) 61 Figure 5: Conceptual representation of the impact of poisoning integrity attacks (including backdoor and trojaning attacks) on the decision function of learning algorithms. The example, taken from [71], shows a backdoored stop sign misclassified, as expected, as a speedlimit sign.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Examples of error-specific (left) and error-generic (right) evasion, as reported in<ref type="bibr" target="#b6">[7]</ref>. Decision boundaries among the three classes (blue, red and green points) are shown as black lines. In the error-specific case, the initial (blue) sample is shifted towards the green class (selected as target). In the error-generic case, instead, it is shifted towards the red class, as it is the closest class to the initial sample. The gray circle represents the feasible domain, given as an upper bound on the 2 distance between the initial and the manipulated attack sample.</figDesc><graphic url="image-10.png" coords="6,169.10,243.19,101.16,65.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Error-specific evasion results from<ref type="bibr" target="#b6">[7]</ref>. Top row: Security evaluation curves reporting accuracy of the given classifiers against an increasing 2 input perturbation. The right-hand side plots depict a laundry detergent misclassified as a cup when applying the minimum input perturbation required for misclassification, along with the corresponding magnified noise mask. Bottom row: Images of the laundry detergent perturbed with an increasing level of noise. The manipulations are only barely visible for perturbation values higher than 150-200 (recall however that these values depend on the image size, as the 2 distance).</figDesc><graphic url="image-15.png" coords="8,104.19,196.40,84.11,83.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>A</head><label></label><figDesc>d v e r s a r i a l M L 2004-2005: pioneering work Dalvi et al., KDD 2004 Lowd &amp; Meek, KDD 2005 2013: Srndic &amp; Laskov, NDSS 2013: Biggio et al., ECML-PKDD -demonstrated vulnerability of nonlinear algorithms to gradient-based evasion attacks, also under limited knowledge Main contributions: 1. gradient-based adversarial perturbations (against SVMs and neural nets) 2. projected gradient descent / iterative attack (also on discrete features from malware data) transfer attack with surrogate/substitute model 3. maximum-confidence evasion (rather than minimum-distance evasion) Main contributions: -minimum-distance evasion of linear classifiers -notion of adversary-aware classifiers 2006-2010: Barreno, Nelson, Rubinstein, Joseph, Tygar The Security of Machine Learning (and references therein)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Timeline of evasion attacks (i.e., adversarial examples) in adversarial machine learning, compared to work on the security of deep networks. Related work is highlighted with markers of the same color (as reported in the legend) and connected with dashed lines to highlight independent (but related) findings. The date of publication refers to publication on peer-reviewed conferences and journals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Security evaluation curve of a softmax classifier trained on the MNIST digits 0, 4, and 9, against back-gradient poisoning and random label flips (baseline comparison). Examples of adversarial training digits generated by back-gradient poisoning are shown on the right.</figDesc><graphic url="image-24.png" coords="11,173.01,378.06,71.78,71.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Schematic categorization of the defense techniques discussed in Sect. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: Effect of class-enclosing defenses against blind-spot adversarial examples on multiclass SVMs with RBF kernels, adapted from<ref type="bibr" target="#b6">[7]</ref>. Rejected samples are highlighted with black contours. The adversarial example (black star) is misclassified only by the standard SVM (left plot), while SVM with rejection correctly identifies it as an adversarial example (middle plot). Rejection thresholds can be modified to increase classifier security by tightening class enclosure (right plot), at the expense of misclassifying more legitimate samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: Decision functions for linear SVMs with 2 , ∞ and 1 regularization on the feature weights<ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78]</ref>. The feasible domain of adversarial modifications (characterizing the equivalent robust optimization problem) is shown for some training points, respectively with 2 , 1 and ∞ balls. Note how the shape of these balls influences the orientation of the decision boundaries, i.e., how different regularizers optimally counter specific kinds of adversarial noise.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">In<ref type="bibr" target="#b13">[14]</ref>, the authors defined targeted and indiscriminate attacks (at test time) depending on whether the attacker aims to cause specific or generic errors. Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attack specificity also introduced in previous work[23, 29,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="40" xml:id="foot_1"><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We use here the hat symbol to denote limited knowledge of a given component.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">http://www.icub.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">Available at: https://youtu.be/CIfsB_EYsVI?t=6m02s</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">An online version of the timeline is also available at: https://sec-ml. pluribus-one.it, along with a web application that allows one to generate adversarial examples and evaluate if they are able to evade detection (evasion attacks).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">https://www.wired.com/2017/02/keep-ai-turning-racist-monster</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">http://virustotal.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8">Note that similar ideas have been exploited also to model uncertainty on some parameters of the data distribution and learn optimal robust classifiers against worst-case changes of such parameters<ref type="bibr" target="#b92">[93]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9">http://archive.defense.gov/Transcripts/Transcript.aspx? TranscriptID=2636</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Ambra Demontis and Marco Melis for providing the experimental results on evasion and poisoning attacks, and to Ian Goodfellow for providing feedback on the timeline of evasion attacks. This work was also partly supported by the EU H2020 project ALOHA, under the European Union's Horizon 2020 research and innovation programme (grant no. 780788).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">('93-'94)</ref><p>. In 1995, he joined the Department of Electrical and Electronic Engineering of the University of Cagliari, where he is now professor of Computer Engineering and head of the research laboratory on pattern recognition and applications. His research activity is focused on the design of pattern recognition systems and their applications. He was a very active organizer of international conferences and workshops, and established the popular workshop series on multiple classifier systems. Dr. Roli is Fellow of the IEEE and of the IAPR.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent advances in convolutional neural networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="354" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<editor>IEEE CVPR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<editor>IEEE CVPR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<editor>IEEE Symp. Security &amp; Privacy</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Is deep learning safe for robot vision? Adversarial examples against the iCub humanoid</title>
		<author>
			<persName><forename type="first">M</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICCV Workshop ViPAR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MagNet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th ACM Conf. Computer and Comm</title>
				<meeting><address><addrLine>Sec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Safetynet: Detecting and rejecting adversarial examples robustly</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Issaranon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarial examples detection in deep networks with convolutional filter statistics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial examples for malware detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">10493</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="62" to="79" />
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transcend: Detecting concept drift in malware classification models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jordaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sharad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nouretdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">USENIX Sec. Symp., USENIX Assoc</title>
		<imprint>
			<biblScope unit="page">642</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Machine learning in adversarial settings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security &amp; Privacy</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="68" to="72" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st IEEE European Symp. Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASIA CCS &apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. Security &amp; Privacy (SP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Computer and Comm. Security (CCS)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1528" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. Knowl. Disc. and Data Mining</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. Knowl. Disc. and Data Mining</title>
				<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Good word attacks on statistical spam filters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Conf. Email and Anti-Spam (CEAS)</title>
				<meeting><address><addrLine>Mountain View, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Impact of artificial &quot;gummy&quot; fingers on fingerprint systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoshino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Datensch. und Datensich</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Can machine learning be secure?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASIA CCS &apos;06</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploiting machine learning to subvert your spam filter</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I P</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>LEET &apos;08, USENIX Assoc</publisher>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Antidote: understanding and defending against poisoning of anomaly detectors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-H. Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
		<idno>IMC &apos;09</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Poisoning attacks against support vector machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th ICML</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1807" to="1814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online anomaly detection under adversarial impact</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th AISTATS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="405" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Security analysis of online centroid anomaly detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3647" to="3690" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Is feature selection secure against training data poisoning?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in: 32nd ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Security evaluation of support vector machines in adversarial environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Support Vector Machines Applications</title>
				<editor>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Int&apos;l Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="105" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Using machine teaching to identify optimal training-set attacks on machine learners</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards poisoning of deep learning algorithms with back-gradient optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Muñoz-González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paudice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wongrassamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On attacking statistical spam filters</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Wittel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Conf. Email and Anti-Spam (CEAS)</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<title level="m">Nightmare at test time: robust learning by feature deletion, in: 23rd ICML</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Convex learning with invariances</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1489" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to classify with missing and corrupted features</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-009-5124-8</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="149" to="178" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">8190</biblScope>
			<biblScope unit="page" from="387" to="402" />
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Practical evasion of a learning-based classifier: A case study</title>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. Security and Privacy, SP &apos;14</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="197" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The security of machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="121" to="148" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Security evaluation of pattern classifiers under attack</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="984" to="996" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pattern recognition systems under attack: Design issues and research challenges</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJPRAI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1460002</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial biometric recognition : A review on biometric system security from the adversarial machine-learning perspective</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Russu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Didaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teo</surname></persName>
		</author>
		<title level="m">Feature weighting for improved classifier robustness, in: 6th Conf. Email and Anti-Spam (CEAS)</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial pattern classification using multiple classifiers and randomisation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSPR 2008</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5342</biblScope>
			<biblScope unit="page" from="500" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Static prediction games for adversarial learning problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brückner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanzow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2617" to="2654" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Randomized prediction games for adversarial machine learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2466" to="2478" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Yes, machine learning can be more secure! A case study on android malware detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Trans. Dep. and Secure Comp</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m">NIPS Workshop on Machine Learning in Adversarial Environments for Computer Security</title>
				<editor>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Machine learning in adversarial environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="115" to="119" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Methods for Computer Security (Dagstuhl Perspectives Workshop 12371)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Thuraisingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<title level="m">AISec &apos;17: 10th Workshop on AI and Security</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sinha</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I P</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
		<title level="m">Adversarial Machine Learning</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adversarial attacks against intrusion detection systems: Taxonomy, solutions and open issues</title>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">239</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="201" to="225" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Phisheye: Live monitoring of sandboxed phishing kits</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kheir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balzarotti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ACM CCS</publisher>
			<biblScope unit="page" from="1402" to="1413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Contini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Corda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mereu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mureddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deltaphish: Detecting phishing webpages in compromised websites</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10492</biblScope>
			<biblScope unit="page" from="370" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A survey and experimental evaluation of image spam filtering techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1436" to="1446" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A survey of image spamming and filtering techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Attar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Atani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="105" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Spam filtering based on the analysis of text information embedded into images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2699" to="2720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Synthetic handwritten captchas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">new Frontiers in Handwriting Recognition</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Adversarial machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
		<editor>4th AISec</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="43" to="57" />
			<pubPlace>Chicago, IL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<title level="m">Is data clustering in adversarial settings secure?, in: AISec &apos;13</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Poisoning behavioral malware clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wressnegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<editor>AISec &apos;14</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Poisoning complete-linkage hierarchical clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Mequanint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">8621</biblScope>
			<biblScope unit="page" from="42" to="52" />
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adversarial feature selection against evasion attacks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cyb</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="766" to="777" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction APIs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Sec. Symp., USENIX Assoc</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="601" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Automatically evading classifiers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Network &amp; Distr. Sys. Sec. Symp. (NDSS)</title>
				<imprint>
			<publisher>The Internet Society</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Evading classifiers by morphing in the dark</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CCS &apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="119" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Query strategies for evading convex-inducing classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1293" to="1332" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Badnets: Identifying vulnerabilities in the machine learning model supply chain</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop Mach. Learn. Comp. Sec</title>
		<imprint>
			<biblScope unit="page">6733</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Targeted backdoor attacks on deep learning systems using data poisoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ArXiv e-prints abs/1712.05526</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CCS &apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1322" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Vulnerabilities in biometric encryption systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Int&apos;l Conf. Audio-and Video-Based Biometric Person Auth</title>
				<editor>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ratha</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3546</biblScope>
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">On the vulnerability of face verification systems to hill-climbing attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Rec</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1027" to="1038" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">An evaluation of indirect attacks and countermeasures in fingerprint verification systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Martinez-Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Rec. Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1643" to="1651" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On security and sparsity of linear classifiers for adversarial settings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Russu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">10029</biblScope>
			<biblScope unit="page" from="322" to="332" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Secure kernel machines against evasion attacks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Russu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<editor>AISec &apos;16</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="59" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Evasion and hardening of tree ensemble classifiers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kantchelian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2387" to="2396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Polymorphic blending attacks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fogla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perdisci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">USENIX Sec. Symp</title>
		<imprint>
			<biblScope unit="page" from="241" to="256" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Multiple classifier systems for robust classifier design in adversarial environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l JMLC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="41" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Detection of malicious pdf files based on hierarchical document structure</title>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th NDSS</title>
				<imprint>
			<publisher>The Internet Society</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Robustness and regularization of support vector machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1485" to="1510" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR.org</title>
		<editor>ICML</editor>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="274" to="283" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>W&amp;CP</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Boosting adversarial examples with momentum</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Safety verification of deep neural networks, in: 29th Int&apos;l Conf. Computer Aided Verification</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10426</biblScope>
			<biblScope unit="page" from="3" to="29" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Deepxplore: Automated whitebox testing of deep learning systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th SOSP</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Paragraph: Thwarting signature learning by training maliciously</title>
		<author>
			<persName><forename type="first">J</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<editor>RAID, LNCS</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="81" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A learning-based approach to reactive security</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Dependable and Sec. Comp</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="482" to="493" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Classifier ensembles for detecting concept change in streaming data: Overview and perspectives</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>SUEMA</publisher>
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimal robust classifiers</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1520" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Mining adversarial patterns via regularized loss minimization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="83" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Bayesian games for adversarial regression problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Großhans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sawade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brückner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Does game theory work?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wooldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE IS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Security analytics and measurements</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Landwehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security &amp; Privacy</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5" to="8" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Robust twin support vector machine for pattern classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">On robustness and regularization of structural support vector machines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Torkamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR.org</title>
		<editor>ICML</editor>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5283" to="5292" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A unified gradient regularization family for adversarial examples</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-N</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CS</title>
		<editor>ICDM</editor>
		<imprint>
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="301" to="309" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Robust large margin deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sokolić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R D</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Proc</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4265" to="4280" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">Adversarial vulnerability of neural networks increases with input dimension</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Towards Deep Learning Models Resistant to Adversarial Attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">One-and-a-half-class multiple classifier systems for secure learning against evasion attacks at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">9132</biblScope>
			<biblScope unit="page" from="168" to="180" />
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="1563" to="1572" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A unifying view on dataset shift in classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Moreno-Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alaiz-Rodri-Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Rec</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="521" to="530" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Multi-label classification with a reject option</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Rec</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2256" to="2266" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Robust multimodal face and fingerprint fusion in the presence of spoofing attacks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferryman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Rec</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="17" to="25" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Statistical meta-analysis of presentation attacks for secure multibiometric systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Marcialis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Analysis Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="561" to="575" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Bagging classifiers for fighting poisoning attacks in adversarial classification tasks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">6713</biblScope>
			<biblScope unit="page" from="350" to="359" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Efficient defenses against adversarial attacks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rawat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="39" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Improving the robustness of deep neural networks via stability training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4480" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Adversarial detection of flash malware: Limitations and open issues</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Chiappe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<idno>CoRR abs/1710.10225</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Understanding the risk factors of learning in adversarial environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISec &apos;11</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Casting out demons: Sanitizing training data for anomaly sensors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cretu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stavrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Locasto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Keromytis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE Symp. Security and Privacy, IEEE CS</publisher>
			<biblScope unit="page" from="81" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Robust linear regression against training data poisoning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vorobeychik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Certified defenses for data poisoning attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Manipulating machine learning: Poisoning attacks and countermeasures for regression learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nita-Rotaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th IEEE Symp. Security and Privacy</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Robust support vector machines based on the rescaled hinge loss function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Rec</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="139" to="148" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">On robust properties of convex risk minimization methods for pattern recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Christmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1007" to="1034" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Learning kernel logistic regression in the presence of class label noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bootkrajang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Rec</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3641" to="3655" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Learning in a large function space: Privacy-preserving mechanisms for SVM learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I P</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Privacy and Conf</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="100" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Steps Toward Robust Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Battista Biggio received the M.Sc. degree (Hons.) in Electronic Engineering and the Ph.D. degree in Electronic Engineering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The mythos of model interpretability, ICML Workshop on Human Interpretability of Machine Learning</title>
				<meeting><address><addrLine>Cagliari, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2016. 2006 and 2010</date>
		</imprint>
		<respStmt>
			<orgName>and Computer Science from the University of ; University of Cagliari</orgName>
		</respStmt>
	</monogr>
	<note>Since 2007, he has been with the Department of Electrical and Electronic Engineering. where he is currently an Assistant Professor. In 2011, he visited the University of</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">His research interests include secure machine learning, multiple classifier systems, kernel methods, biometrics and computer security. Dr. Biggio serves as a reviewer for several international conferences and journals. He is a senior member of the IEEE and a member of the IAPR. Fabio Roli received his Ph.D</title>
		<author>
			<persName><forename type="first">Germany</forename><surname>Tuebingen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Engineering from the University of</title>
				<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
	<note>and worked on the security of machine learning to training data poisoning. He was a research group of the University of Genoa</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
