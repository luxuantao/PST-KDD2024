<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangled Representation Learning for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<email>xin_wang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Jianxin Ma is with Alibaba Group. Hong Chen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Yuwei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Jianxin Ma is with Alibaba Group. Hong Chen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Yuwei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
							<email>h-chen20@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Jianxin Ma is with Alibaba Group. Hong Chen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Yuwei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Jianxin Ma is with Alibaba Group. Hong Chen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Yuwei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuwei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Jianxin Ma is with Alibaba Group. Hong Chen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Yuwei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Jianxin Ma is with Alibaba Group. Hong Chen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Yuwei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Jianxin Ma is with Alibaba Group. Hong Chen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Yuwei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Jianxin Ma is with Alibaba Group. Hong Chen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Yuwei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Jianxin Ma is with Alibaba Group. Hong Chen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing, Yuwei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangled Representation Learning for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2022.3153112</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2022.3153112, IEEE Transactions on Pattern Analysis and Machine Intelligence Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on April 28,2022 at 12:30:25 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2022.3153112, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Disentangled Representation, Recommendation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There exist complex interactions among a large number of latent factors behind the decision making processes of different individuals, which drive the various user behavior patterns in recommender systems. These factors hidden in those diverse behaviors demonstrate highly entangled patterns, covering from high-level user intentions to low-level individual preferences. Uncovering the disentanglement of these latent factors can benefit in enhanced robustness, interpretability, and controllability during representation learning for recommendation. However, the large degree of entanglement within latent factors poses great challenges for learning representations that disentangle them, and remains largely unexplored in literature. In this paper, we present the SEMantic MACRo-mIcro Disentangled Variational Auto-Encoder (SEM-MacridVAE) model for learning disentangled representations from user behaviors, taking item semantic information into account. Our SEM-MacridVAE model achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a pair of shoes or a laptop) through a prototype routing mechanism, as well as capturing the individual preferences with respect to different concepts separately. The micro disentanglement is guaranteed through a micro-disentanglement regularizer stemming from an information-theoretic interpretation of VAEs, which forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). The semantic information including visual and categorical signals extracted from candidate items is utilized to further boost the recommendation performance of the proposed SEM-MacridVAE model. Empirical experiments demonstrate that our proposed approach is able to achieve significant improvement over the state-of-the-art baselines. We also show that the learned representations are interpretable and controllable, capable of potentially leading to a new paradigm for recommendation where users have fine-grained control over some target aspects of the recommendation candidates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning representations that can accurately reflect users' preference, based chiefly on user behavior, has been an important research focus for recommender systems since the advent of collaborative filtering <ref type="bibr" target="#b56">[57]</ref>. Despite the huge success in the past decade, existing user behavior-based representation learning methods, including the deep structure approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b78">[79]</ref>, generally ignore the complex interactions among the latent factors behind the users' decision-making processes. These latent factors can be highly entangled, ranging from macro concepts that govern the intention of a user in a particular behavior, to micro individual preferences at a granular level when implementing a specific intention. Existing methods fail to disentangle these latent factors, resulting in the fact that those learned representations may mistakenly preserve the confounding of the highly entangled factors, which leads to non-robustness and low interpretability.</p><p>Disentangled representation learning, which targets at learning factorized representations capable of uncovering and disentangling the latent explanatory factors hidden in the observed data <ref type="bibr" target="#b2">[3]</ref>, has recently attracted lots of attentions in the research community. Disentangled representations benefits in more robustness, i.e., less sensitive to the misleading correlations discovered in the limited observed training data. Besides, the enhanced interpretability brought by disentangled representation also finds direct application in recommendation-related tasks, such as transparent advertising <ref type="bibr" target="#b40">[41]</ref>, customer-relationship management, and explainable recommendation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b77">[78]</ref> etc. Moreover, the controllability exhibited by many disentangled representations <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref> can provide users with explicit controls over their desired recommendation results and offer them more interactive experience, which has great potential in driving a new paradigm for recommendation. However, the existing literature on disentangled representation learning mainly focuses on computer vision <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b79">[80]</ref> rather than recommender systems.</p><p>User behaviors in recommender systems can be driven by both macro intentions and micro preferences, where macro intentions may involve high-level user intentions such as purchasing a pair of shoes or a laptop and micro preferences may refer to low-level user preferences such as the size or color of the shoes. Therefore, given that the above discrete relational user behavior data is essentially different from continuous image data, learning disentangled representations based on user behavior data for recommendation becomes largely unexplored and poses two challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>The macro intentions and micro preferences co-exist in user behaviors, requiring the disentangled representation learning to separate these two levels of factors in a way that can preserve the hierarchical relations between high-level user intentions and lowlevel individual preferences under the intentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>The observed user behavior data, such as user-item ratings and user-item consumptions, are discrete and sparse in essence, differing themselves from continuous image data, which implies that the observed user behavior data is only associated with a very small number of entries in the high-dimensional representation space. This will be especially problematic when exploring the interpretability of a particular isolated dimension through varying the value of that dimension with other dimensions fixed.</p><p>To solve the challenges, we propose the SEMantic MACRo-mIcro Disentangled Variational Auto-Encoder (SEM-MacridVAE) model for learning disentangled representations based on user behavior with item semantic information being taken into consideration in this paper. Our proposed method explicitly models the separation of macro and micro factors when performing disentanglement at each level. In particular, macro disentanglement is achieved by discovering the high-level concepts associated with user intentions through a prototype routing mechanism, and separately capturing the individual preferences of a user with respect to different concepts. Micro disentanglement is strengthened through a micro-disentanglement regularizer derived from interpreting VAEs <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b57">[58]</ref> in terms of an information-theoretic perspective, which aims at forcing each individual dimension to indicate an independent micro factor. The semantic information including visual and categorical signals from items is employed to further boost the model performances of the proposed SEM-MacridVAE model. To handle the conflict between sparse discrete user behavior observations and dense continuous latent representations, we propose a beam-search strategy for investigating the interpretability of each isolated dimension through finding a smooth trajectory within different representations.</p><p>We conduct extensive empirical experiments to show that our SEM-MacridVAE model can achieve significant improvement over several state-of-the-art baselines. Experimental results also demonstrate that the learned disentangled representations from SEM-MacridVAE can be interpretable and controllable, which may potentially bring a promising new paradigm for recommendation where users are given finegrained controls over target aspects of the recommendation candidates.</p><p>To summarize, this paper makes the following contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>We study the problem of disentangled representation learning for discrete and sparse relational user behavior data in recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>We propose SEMantic MACRo-mIcro Disentangled Variational Auto-Encoder (SEM-MacridVAE) model, which is able to conduct both macro and micro disentanglement simultaneously in representation learning for user behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>We utilize two types of semantic information, i.e., visual and categorical signals extracted from candidate items, to further boost the recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>We conduct extensive experiments on several realworld datasets to verify the advantages of our SEM-MacridVAE model in terms of recommendation accuracy, interpretability and controllability.</p><p>In particular, we would like to point out that compared to the MacridVAE model <ref type="bibr" target="#b50">[51]</ref>, our proposed SEM-Macrid model has the following expansions: The remainder of this paper is organized as follows. We review related works in Section 2 and present our proposed SEM-MacridVAE model in Section 3. Section 4 describes details about empirical evaluations over several real-world datasets in terms of various metrics. Last but not least, we conclude the whole paper and point out research directions deserving further investigations in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review existing works on user behavior representation learning and disentangled representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning Representations from User Behavior</head><p>Learning from user behavior has been a central task of recommender systems since the advent of collaborative filtering <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Being able to predict user preferences through uncovering complex and unexpected patterns hidden in users' past behaviors without any domain knowledge, factorization based recommendation <ref type="bibr" target="#b35">[36]</ref> has become one of the most popular methods in recommender systems. These factorization based collaborative filtering models factorize user and item information into latent representations to approximate user preferences and item attributes, either in a deterministic way <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b71">[72]</ref> or a probabilistic manner <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b73">[74]</ref>. In addition to early factorization based attempts, the more recent deep learning methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b78">[79]</ref> achieve massive improvement by learning highly informative representations. The entanglement of the latent factors behind user behavior, however, is mostly neglected by the black-box representation learning process adopted by the majority of the existing methods. To the extent of our knowledge, we are the first to study disentangled representation learning on user behavior data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Disentangled Representation Learning</head><p>Disentangled representation learning aims to identify and disentangle the underlying explanatory factors <ref type="bibr" target="#b2">[3]</ref>. Being  <ref type="bibr" target="#b20">[21]</ref> demonstrates that disentanglement can emerge once the KL divergence term in the VAE <ref type="bibr" target="#b32">[33]</ref> objective is aggressively penalized.</p><p>In particular, Kingma and Welling <ref type="bibr" target="#b32">[33]</ref> propose to utilize Bayesian posterior inference and variational estimation to learn the controllable factors hidden in the observed data. Higgins et al. <ref type="bibr" target="#b20">[21]</ref> propose Î² âˆ’ V AE by setting a weight Î² for the KL divergence to improve representation disentanglement learned in the observed data while sacrificing mutual information between input data and latent representations. Later approaches separate the information bottleneck term <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref> and the total correlation term, and achieve a greater level of disentanglement <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Other works either design an attentive architecture to learn aspect matrix for word embeddings <ref type="bibr" target="#b16">[17]</ref> or utilize methods based on triplets to learn aspect representations from sentences where each aspect has a separate encoder <ref type="bibr" target="#b24">[25]</ref>. Though a few existing approaches <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b29">[30]</ref> do notice that a dataset can contain samples from different concepts, i.e., follow a mixture distribution, their settings are fundamentally different from ours. To be specific, these existing approaches assume that each instance is from a concept, while we assume that each instance interacts with objects from different concepts. The majority of the existing efforts are from the field of computer vision <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b79">[80]</ref>. Disentangled representation learning on relational data, such as graph-structured data, was not explored until recently <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b75">[76]</ref>. This work focuses on disentangling user behavior from both the macro intention and micro preference in recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we describe our SEM-MacridVAE model for learning disentangled representations from user behaviors in detail, whose whole framework is demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and Problem Formulation</head><p>A user behavior dataset D consists of the interactions between N users and M items. The interaction between the u th user and the i th item is denoted by x u,i âˆˆ {0, 1}, where x u,i = 1 indicates that user u explicitly adopts item i, whereas x u,i = 0 means there is no recorded interaction between the two. For convenience, we use x u = {x u,i : x u,i = 1} to represent the items adopted by user u. The goal is to learn user representations {z u } N u=1 that achieves both macro and micro disentanglement. We use Î¸ to denote the set that contains all the trainable parameters of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Macro Disentanglement</head><p>Users may have very diverse interests, and interact with items that belong to many high-level concepts, e.g., product categories. We aim to achieve macro disentanglement, by learning a factorized representation of user u, namely</p><formula xml:id="formula_0">z u = [z (1) u ; z (2) u ; . . . ; z (K) u ] âˆˆ R d â€²</formula><p>, where d â€² = Kd, assuming that there are K high-level concepts. The k th component z</p><formula xml:id="formula_1">(k)</formula><p>u âˆˆ R d is for capturing the user's preference regarding the k th concept. Additionally, we infer a set of one-hot vectors C = {c i } M i=1 for the items, where c i = [c i,1 ; c i,2 ; . . . ; c i,K ]. If item i belongs to concept k, then c i,k = 1 and c i,k â€² = 0 for any k â€² Ì¸ = k. We infer {z u } N u=1 in an unsupervised way, and learn C in a supervised manner through the categorical signals of the semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Micro Disentanglement</head><p>High-level concepts correspond to the intentions of a user, e.g., to buy clothes or a cellphone. We are also interested in disentangling a user's preference at a more granular level regarding the various aspects of an item. For example, we would like the different dimensions of z</p><formula xml:id="formula_2">(k)</formula><p>where the meanings of x u , z u , C are described in the previous subsection. We also assume that</p><formula xml:id="formula_3">p Î¸ (z u ) = p Î¸ (z u | C)</formula><p>in Equation (1), i.e., z u and C are generated by two independent sources. Note that c i = [c i,1 ; c i,2 ; . . . ; c i,K ] is one-hot, since we assume that item i belongs to exactly one concept. We also remark that</p><formula xml:id="formula_4">p Î¸ (x u,i | z u , C) = Z âˆ’1 u â€¢ K k=1 c i,k â€¢ g (i) Î¸ (z (k) u )</formula><p>is a categorical distribution over the M items, where</p><formula xml:id="formula_5">Z u = M i=1 K k=1 c i,k â€¢ g (i) Î¸ (z (k) u ),<label>and g (i) Î¸</label></formula><p>: R d â†’ R + is a shallow neural network that estimates how much a user with a given preference is interested in item i. We use sampled softmax <ref type="bibr" target="#b28">[29]</ref> to estimate Z u based on a few sampled items when M is very large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Macro Disentanglement</head><p>We assume above that the user representation z u is sufficient for predicting how the user will interact with the items. And we further assume that using the k th component z (k) u alone is already sufficient if the prediction is about an item from concept k. This design explicitly encourages z (k) u to capture preference regarding only the k th concept, as long as the inferred concept assignment matrix C is meaningful. ğ’©ğ’©(ğŸğŸ , ğœğœ 0 2 ğ‘°ğ‘°) ğ›½ğ›½ğ·ğ· ğ¾ğ¾ğ¾ğ¾ ğ‘§ğ‘§ (1)   ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ğ’©ğ’©(ğœ‡ğœ‡ 2 , ğœğœ 2 ) ğ’©ğ’©(ğŸğŸ , ğœğœ 0 2 ğ‘°ğ‘°) ğ›½ğ›½ğ·ğ· ğ¾ğ¾ğ¾ğ¾ ğ‘§ğ‘§ (2)   ğ’©ğ’©(ğœ‡ğœ‡ 3 , ğœğœ 3 ) We will describe later the implementation details of p Î¸ (C), p Î¸ (z u ) and g</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘¥ğ‘¥ ğ‘¢ğ‘¢</head><formula xml:id="formula_6">ğ’©ğ’©(ğŸğŸ , ğœğœ 0 2 ğ‘°ğ‘°) ğ›½ğ›½ğ·ğ· ğ¾ğ¾ğ¾ğ¾ ğ‘§ğ‘§<label>(3)</label></formula><formula xml:id="formula_7">(i) Î¸ (z (k) u )</formula><p>. Nevertheless, we note that p Î¸ (C) requires careful design to prevent mode collapse, i.e., the degenerate case where almost all items are assigned to a single concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Variational Inference</head><p>We follow the variational auto-encoder (VAE) paradigm <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b57">[58]</ref>, and optimize Î¸ by maximizing a lower bound of u ln p Î¸ (x u ), where ln p Î¸ (x u ) is bounded as follows:</p><formula xml:id="formula_8">ln p Î¸ (x u )<label>(3)</label></formula><formula xml:id="formula_9">+ E q Î¸ (zu,C|xu) ln p Î¸ (z u , C) q Î¸ (z u , C | x u ) =D KL q Î¸ (z u , C | x u )âˆ¥p Î¸ (z u , C | x u ) + E q Î¸ (zu,C|xu) ln p Î¸ (x u | z u , C) âˆ’ D KL q Î¸ (z u , C | x u )âˆ¥p Î¸ (z u , C) â‰¥E q Î¸ (zu,C|xu) ln p Î¸ (x u | z u , C) âˆ’ D KL q Î¸ (z u , C | x u )âˆ¥p Î¸ (z u , C) =E p Î¸ (C) E q Î¸ (zu|xu,C) ln p Î¸ (x u | z u , C) âˆ’ D KL q Î¸ (z u | x u , C)âˆ¥p Î¸ (z u ) .</formula><p>Note that in the last line above, we have used</p><formula xml:id="formula_10">D KL q Î¸ (z u , C | x u )âˆ¥p Î¸ (z u , C) = D KL q Î¸ (z u | x u , C)p Î¸ (C)âˆ¥p Î¸ (z u )p Î¸ (C) = E p Î¸ (C) D KL q Î¸ (z u | x u , C)âˆ¥p Î¸ (z u ) ,</formula><p>which completes the proof.</p><p>Here we have introduced a variational distribution q Î¸ (z u | x u , C), whose implementation also encourages macro disentanglement and will be presented later. The two expectations, i.e., E p Î¸ (C) [â€¢] and E q Î¸ (zu|xu,C) <ref type="bibr">[â€¢]</ref>, are intractable, and are therefore estimated using the Gumbel-Softmax trick <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b53">[54]</ref> and the Gaussian re-parameterization trick <ref type="bibr" target="#b32">[33]</ref>, respectively. Once the training procedure is finished, q Î¸ (z u | x u , C) will be an approximation of the intractable posterior distribution p Î¸ (z u | x u , C). We use the mode of p Î¸ (C) as C, and the mode of q Î¸ (z u | x u , C) as the representation of user u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Micro Disentanglement</head><p>A natural strategy to encourage micro disentanglement is to force statistical independence between the dimensions, i.e., to force</p><formula xml:id="formula_11">q Î¸ (z (k) u | C) â‰ˆ d j=1 q Î¸ (z (k) u,j | C),</formula><p>so that each dimension describes an isolated factor, where</p><formula xml:id="formula_12">q Î¸ (z u | C) = q Î¸ (z u | x u , C)p data (x u ) dx u .</formula><p>Fortunately, the Kullback-Leibler (KL) divergence term in the lower bound above does provide a way to encourage independence. Specifically, the KL term of our model can be rewritten as:</p><formula xml:id="formula_13">E p data (xu) [D KL (q Î¸ (z u | x u , C)âˆ¥p Î¸ (z u ))] (4) =I q (x u ; z u ) + D KL (q Î¸ (z u | C)âˆ¥p Î¸ (z u )).</formula><p>The proof is as follows.</p><formula xml:id="formula_14">q Î¸ (z u , x u | C) =q Î¸ (z u | x u , C)p data (x u | C) =q Î¸ (z u | x u , C)p data (x u ),</formula><p>which completes the proof.</p><p>Similar decomposition of the KL term has been noted for the original VAEs previously <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Penalizing the latter KL term would encourage independence between the dimensions, if we choose a prior that satisfies</p><formula xml:id="formula_15">p Î¸ (z u ) = d â€² j=1 p Î¸ (z u,j</formula><p>). On the other hand, the former term I q (x u ; z u ) is the mutual information between x u and z u under q Î¸ (z u | x u , C) â€¢ p data (x u ). Penalizing I q (x u ; z u ) is equivalent to applying the information bottleneck principle <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b62">[63]</ref>, which encourages z u to ignore as much noise in the input as it can and to focus on merely the essential information. We therefore follow Î²-VAE <ref type="bibr" target="#b20">[21]</ref>, and strengthen these two regularization terms by a factor of Î² â‰« 1, which brings us to the following training objective:</p><formula xml:id="formula_16">E p Î¸ (C) E q Î¸ (zu|xu,C) ln p Î¸ (x u | z u , C)<label>(5)</label></formula><formula xml:id="formula_17">âˆ’Î² â€¢ D KL q Î¸ (z u | x u , C)âˆ¥p Î¸ (z u ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>In this section, we describe the implementation of p Î¸ (C), p Î¸ (x u,i | z u , C) (the decoder), p Î¸ (z u ) (the prior), q Î¸ (z u | x u , C) (the encoder), and propose an efficient strategy to combat mode collapse. The parameters Î¸ of our implementation include:</p><formula xml:id="formula_18">K concept prototypes {m k } K k=1 âˆˆ R KÃ—d , M item representations {h i } M i=1 âˆˆ R M Ã—d used by the decoder, M context representations {t i } M</formula><p>i=1 âˆˆ R M Ã—d used by the encoder, and the parameters of a neural network f nn : R d â†’ R 2d . We optimize Î¸ to maximize the training objective (see Equation <ref type="formula">6</ref>) using Adam <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Prototype-based Concept Assignment</head><p>A straightforward approach would be to assume p Î¸ (C) = M i=1 p(c i ) and parameterize each categorical distribution p(c i ) with its own set of K âˆ’ 1 parameters. This approach, however, would result in over-parameterization and low sample efficiency. We instead propose a prototype-based implementation. To be specific, we introduce K concept prototypes {m k } K k=1 and reuse the item representations {h i } M i=1 from the decoder. We then assume c i is a one-hot vector drawn from the following categorical distribution p Î¸ (c i ):</p><formula xml:id="formula_19">c i âˆ¼ CATEGORICAL SOFTMAX([s i,1 ; s i,2 ; . . . ; s i,K ]) , (6) s i,k = COSINE(h i , m k )/Ï„,</formula><p>where COSINE(a, b) = a âŠ¤ b/(âˆ¥aâˆ¥ 2 âˆ¥bâˆ¥ 2 ) is the cosine similarity, and Ï„ is a hyper-parameter that scales the similarity from</p><formula xml:id="formula_20">[âˆ’1, 1] to [âˆ’ 1 Ï„ , 1 Ï„ ].</formula><p>We set Ï„ = 0.1 to obtain a more skewed distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Preventing Mode Collapse</head><p>We use cosine similarity, instead of the inner product similarity adopted by most existing deep learning methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>. This choice is crucial for preventing mode collapse, which can be a severe issue with a mixture model <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b72">[73]</ref> such as ours if no special treatment is applied, especially when neural networks are involved <ref type="bibr" target="#b61">[62]</ref>. In fact, with inner product, the majority of the items are highly likely to be assigned to a single concept m k â€² that has an extremely large norm, i.e., âˆ¥m k â€² âˆ¥ 2 â†’ âˆ, even when the items {h i } M i=1 correctly form K clusters in the high-dimensional Euclidean space. And we observe empirically that this phenomenon does occur frequently with inner product (see Figure <ref type="figure" target="#fig_14">3c</ref> and Figure <ref type="figure" target="#fig_19">4c</ref>). In contrast, cosine similarity avoids this degenerate case due to the normalization. Moreover, cosine similarity is related with the Euclidean distance on the unit hypersphere, and the Euclidean distance is a proper metric that is more suitable for inferring the cluster structure, compared to inner product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Decoder</head><p>The decoder predicts which item out of the M ones is mostly likely to be clicked by a user, when given the user's representation</p><formula xml:id="formula_21">z u = [z (1) u ; z (2) u ; . . . ; z (K)</formula><p>u ] and the one-hot concept assignments {c i } M i=1 . We assume that</p><formula xml:id="formula_22">p Î¸ (x u,i | z u , C) âˆ K k=1 c i,k â€¢ g (i) Î¸ (z (k) u )<label>(7)</label></formula><p>is a categorical distribution over the M items, and define</p><formula xml:id="formula_23">g (i) Î¸ (z (k) u ) = exp(COSINE(z (k) u , h i )/Ï„ ).<label>(8)</label></formula><p>This design implies that {h i } M i=1 will be micro-disentangled if {z</p><formula xml:id="formula_24">(k) u } N</formula><p>u=1 is micro-disentangled, as the two's dimensions are aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Prior &amp; Encoder</head><p>The prior p Î¸ (z u ) needs to be factorized in order to achieve micro disentanglement. We therefore set p Î¸ (z u ) to N (0, Ïƒ 2 0 I). The encoder q Î¸ (z u | x u , C) is for computing the representation of a user when given the user's behavior data x u , which approximates the posterior. The encoder maintains an additional set of context representations {t i } M i=1 , rather than reusing the item representations {h i } M i=1 from the decoder, which is a common practice in the literature <ref type="bibr" target="#b39">[40]</ref>. We assume that</p><formula xml:id="formula_25">q Î¸ (z u | x u , C) = K k=1 q Î¸ (z (k) u | x u , C),</formula><p>and represent each q Î¸ (z</p><formula xml:id="formula_26">(k) u | x u , C) as a multivariate normal distribution with a diagonal covariance matrix N (Âµ (k) u , [diag(Ïƒ (k) u )] 2 )</formula><p>, where the mean and the standard deviation are parameterized by a neural network</p><formula xml:id="formula_27">f nn : R d â†’ R 2d : (a (k) u , b (k) u ) = f nn i:xu,i=+1 c i,k â€¢ t i i:xu,i=+1 c 2 i,k ,<label>(9)</label></formula><formula xml:id="formula_28">Âµ (k) u = a (k) u âˆ¥a (k) u âˆ¥ 2 , Ïƒ (k) u â† Ïƒ 0 â€¢ exp âˆ’ 1 2 b (k) u .</formula><p>The neural network f nn (â€¢) captures nonlinearity, and is shared across the K components. We normalize the mean, so as to be consistent with the use of cosine similarity which projects the representations onto a unit hypersphere. Note that Ïƒ 0 should be set to a small value, e.g., around 0.1, since the learned representations are now normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incorporating Semantic Information</head><p>In this section, we discuss the incorporation of semantic information extracted from items to further boost the model performance. Specifically, we consider two types of semantic information, i.e., visual signals and categorical signals.</p><p>Incorporating Visual Signals. The two key elements, i.e., concept prototypes {m k } K k=1 and item representations {h i } M i=1 , in the prototype mechanism which has a crucial influence on both encoder and decoder, are so far initialized randomly without taking any semantic information from items into consideration.</p><p>Therefore, to further improve the model performances, we encode visual semantic information through a pre-trained AlexNet over the raw item image to obtain visual feature v i for each item i. To match the dimension of v i with that of the item embedding, we conduct Principal Component Analysis (PCA) on v i . Then we initialize h i with the lowdimensional visual feature conduct initialization for m k by calculating the cluster center (obtained from K-means) of item representations belonging to concept k. Concretely, the visual features are obtained from the output of the last second fully-connected layers of the AlexNet, which has five convolutional layers followed by three fully-connected layers and is pre-trained on the ImageNet dataset with semantic categorical labels. Assuming the visual features output from AlexNet is denoted as {v i } M i=1 = AlexN et(â€¢), then the process of initializing {h i } M i=1 and {m k } K k=1 can be formulated as follows,</p><formula xml:id="formula_29">vi = 1 M M i=1 v i ,<label>(10)</label></formula><formula xml:id="formula_30">V = 1 M M i=1 (v i âˆ’ vi )(v i âˆ’ vi ) T , V = QÎ›Q T , P = Q T [: d], {h i } M i=1 = {P v i } M i=1 , {m k } K k=1 = Kmeans(h 1 , h 2 , â€¢ â€¢ â€¢ , h M )</formula><p>, where each column of Q represents an eigenvector of V and P contains d eigenvectors corresponding to the largest d eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>The training procedure. We add 10 âˆ’8 to prevent division-by-zero wherever appropriate. </p><formula xml:id="formula_31">{v i } M i=1 = AlexN et(â€¢).</formula><p>5:</p><formula xml:id="formula_32">for i = 1, 2, . . . , M do 6: vi = 1 M M i=1 v i . 7: V = 1 M M i=1 (v i âˆ’ vi )(v i âˆ’ vi ) T .</formula><p>8:</p><formula xml:id="formula_33">P = Q T [: d],</formula><p>where V = QÎ›Q T . 9:</p><formula xml:id="formula_34">{h i } M i=1 = {P v i } M i=1 .</formula><p>10:</p><formula xml:id="formula_35">{m k } K k=1 = Kmeans(h 1 , h 2 , â€¢ â€¢ â€¢ , h M ).</formula><p>11: for i = 1, 2, . . . , M do 14:</p><formula xml:id="formula_36">return {h i } M i=1 , {m k } K</formula><formula xml:id="formula_37">s i,k â† h âŠ¤ i m k /(Ï„ â€¢ âˆ¥h i âˆ¥ 2 â€¢ âˆ¥m k âˆ¥ 2 )</formula><p>, where k = 1, 2, . . . , K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>c i âˆ¼GUMBEL-SOFTMAX([s i,1 ; s i,2 ; . . . ; s i,K ]).</p><p>â–· At test time, c i is set to the mode. <ref type="bibr" target="#b15">16</ref>:</p><formula xml:id="formula_38">return {c i } M i=1 17: function ENCODER(x u , {c i } M i=1 )</formula><p>18:</p><formula xml:id="formula_39">for k = 1, 2, . . . , K do 19: (a k , b k ) â† f nn i:x u,i =+1 c i,k â€¢ti i:x u,i =+1 c 2 i,k<label>, 20:</label></formula><formula xml:id="formula_40">Âµ (k) â† a k /âˆ¥a k âˆ¥ 2 ,</formula><p>21:</p><formula xml:id="formula_41">Ïƒ (k) â† Ïƒ 0 â€¢ exp âˆ’ 1 2 b k . 22:</formula><p>Âµ u â† [Âµ (1) ; Âµ (2) ; . . . ; Âµ (K) ],</p><p>23:</p><p>Ïƒ u â† [Ïƒ (1) ; Ïƒ (2) ; . . . ; Ïƒ (K) ],</p><p>24:</p><p>Ïµ âˆ¼ N (0, I).</p><p>25: </p><formula xml:id="formula_42">z u = Âµ u + Ïµ â€¢ Ïƒ u . â–· z u is</formula><formula xml:id="formula_43">return z u , D KL (N (Âµ u , diag(Ïƒ u ))âˆ¥N (0, Ïƒ 0 â€¢ I)) 27: function DECODER(z u , {c i } M i=1 ) 28: p u,i â† K k=1 c i,k â€¢ exp(z (k) u âŠ¤ h i /(Ï„ â€¢ âˆ¥z (k) u âˆ¥ 2 â€¢ âˆ¥h i âˆ¥ 2 )</formula><p>), where i = 1, 2, . . . , M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>29:</head><p>[p u,1 ; p u,2 ; . . . ; p u,M ] â† SOFTMAX([ln p u,1 ; ln p u,2 ; . . . ; ln p u,M ]). â–· We replace the SOFTMAX(â€¢) above with SAMPLED-SOFTMAX(â€¢), and compute p u,i only if x u,i = 1 or item i is sampled, when M is very large. </p><formula xml:id="formula_44">return {p u,i } M i=1 31: {c i } M i=1 â† PROTOTYPECLUSTERING( ). 32: z u , D KL â† ENCODER(x u , {c i } M i=1 ). 33: {p u,i } M i=1 â† DECODER(z u , {c i } M i=1 ). 34: L = âˆ’Î² â€¢ D KL + i:xu,i=1 ln p u,i</formula><p>+ M i=1 Cross_Entropy(c i , Ä‰i ). 35: Î¸ â† Update Î¸ to maximize L, using the gradient âˆ‡ Î¸ L.</p><p>Incorporating Categorical Signals. The number of macro concepts, i.e., K, are so far preset by human experience, followed by macro disentanglement in an unsupervised manner, which may run the risk of misalignment between the macro concepts and actual categories of items despite massive cost on trying and testing. Therefore, we utilize the categorical semantic information to achieve better macro disentanglement through supervised categorical signals in the following:</p><formula xml:id="formula_45">min M i=1 Cross_Entropy(c i , Ä‰i ), (<label>11</label></formula><formula xml:id="formula_46">)</formula><p>where Ä‰i is one-hot vector that reflects the ground-truth category of the i th item and Cross_Entropy(c i , Ä‰i ) denotes the binary classification loss between the learned category and true category of the item i.</p><p>Empirical results in our experiments later show that by taking semantic information, i.e., visual and categorical signals, into account, the proposed SEM-MacridVAE model is able to outperform MacridVAE which initializes item representations and concept prototypes in a random manner. The incorporation of item semantic information is illustrated in the upper part of Figure <ref type="figure" target="#fig_0">1</ref>. Algorithm 1 presents the implementation details of the whole procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">User-controllable Recommendation</head><p>The controllability enabled by the disentangled representations can bring a new paradigm for recommendation. It allows a user to interactively search for items that are similar to an initial item except for some controlled aspects, or to explicitly adjust the disentangled representation of his/her preference, learned by the system from his/her past behaviors, to actually match the current preference. Here, we formalize the task of user-controllable recommendation, and illustrate a possible solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Task Definition</head><p>Let h * âˆˆ R d be the representation to be altered, which can be initialized as either an item representation or a component of a user representation. The task is to gradually alter its j th dimension h * ,j , while retrieving items whose representations are similar to the altered representation. This task is nontrivial, since usually no item will have exactly the same representation as the altered one, especially when we want the transition to be smooth, monotonic, and thus human-understandable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Solution</head><p>Here we illustrate our approach to this task. We first probe the suitable range (a, b) for h * ,j . Let us assume that prototype k * is the prototype closest to h * . The range (a, b) is decided such that: prototype k * remains the prototype closest to h * if and only if h * ,j âˆˆ (a, b). We can decide each endpoint of the range using binary search. We then divide the range (a, b) into B subranges, a = a 0 &lt; a 1 &lt; a 2 . . . &lt; a B = b. We ensure that the subranges contain roughly the same number of items from concept k * when dividing (a, b) . Finally, we aim to retrieve B items {i t } B t=1 âˆˆ {1, 2, . . . , M } B that belong to concept k * , each from one of the B subranges, i.e., h it,j âˆˆ (a tâˆ’1 , a t ]. We thus decide the B items by maximizing</p><formula xml:id="formula_47">1â‰¤tâ‰¤B e COSINE(h i t ,âˆ’j ,h * ,âˆ’j ) Ï„ + Î³ â€¢ 1â‰¤t&lt;t â€² â‰¤B e COSINE(h i t ,âˆ’j ,h i t â€² ,âˆ’j ) Ï„ ,<label>(12)</label></formula><p>where h i,âˆ’j = [h i,1 ; h i,2 ; . . . ; h i,jâˆ’1 ; h i,j+1 ; . . .</p><formula xml:id="formula_48">; h i,d ] âˆˆ R dâˆ’1</formula><p>and Î³ is a hyper-parameter. We approximately solve this maximization problem sequentially using beam search <ref type="bibr" target="#b44">[45]</ref>.</p><p>Intuitively, selecting items from the B subranges ensures that the items change monotonously in terms of the j th dimension. On the other hand, the first term in the maximization problem forces the retrieved items to be similar with the initial item in terms of the dimensions other than j, while the second term encourages any two retrieved items to be similar in terms of the dimensions other than j.</p><p>We highlight in Figure <ref type="figure" target="#fig_22">6</ref>, Figure <ref type="figure" target="#fig_24">7</ref> and Figure <ref type="figure">8</ref> some example cases that we found using this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL EXPERIMENTS</head><p>In this section, we demonstrate that our learned disentangled representations are not only effective for recommendation, but also interpretable and controllable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We conduct extensive experiments on five public real-world datasets, including a MovieLens dataset (i.e., ML-latestsmall) <ref type="bibr" target="#b15">[16]</ref> and four Amazon product datasets <ref type="bibr" target="#b17">[18]</ref> of different meta categories (i.e., Movies&amp;TV, Musical Instruments, Home&amp;Kitchen and Clothing&amp;Shoes&amp;Jewelry 1 ). We follow MultiVAE <ref type="bibr" target="#b39">[40]</ref>, and binarize these five datasets by labeling ratings of four or higher as 1, and keeping users who have at least fifteen rating actions. Each item in MovieLens and Amazon datasets is associated with its corresponding image, and we utilize an AlexNet pre-trained on the ImageNet dataset to obtain 4096-dimension visual features which will then be transformed to a d-dimension latent factor to initialize item representation h of this item. Given that the ImageNet dataset contains semantic guidance for visual features through providing ground-truth labels for various types of images, in this way we are able to incorporate semantic information of each item into our learning process. All datasets are preprocessed using the script provided by MultiVAE. Half of the held-out users are used for validation, while the other half of the held-out users are for testing. Table <ref type="table" target="#tab_2">1</ref> summarizes the basic statistics of the above datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines</head><p>We compare our approach with four baselines, Multi-DAE <ref type="bibr" target="#b39">[40]</ref>, MultiVAE <ref type="bibr" target="#b39">[40]</ref>, MacridVAE <ref type="bibr" target="#b50">[51]</ref> and DGCF <ref type="bibr" target="#b65">[66]</ref>. MultiDAE <ref type="bibr" target="#b39">[40]</ref> and MultiVAE <ref type="bibr" target="#b39">[40]</ref> are the two state-of-the-art methods for collaborative filtering. In particular, MultiVAE is similar to Î²-VAE <ref type="bibr" target="#b20">[21]</ref>, and has a hyper-parameter Î² that controls the strength of disentanglement. However, Multi-VAE does not learn disentangled representations, because it requires Î² â‰ª 1 to perform well. MacridVAE <ref type="bibr" target="#b50">[51]</ref> can be 1. http://jmcauley.ucsd.edu/data/amazon/links.html treated as a variant of SEM-MacridVAE which conducts random initialization without considering any semantic information from items, and we compare it with the proposed SEM-MacridVAE model to further verify the improvement brought by incorporating item semantic information. Besides, DGCF <ref type="bibr" target="#b65">[66]</ref> is chosen as the comparative baseline given that it is one of the most recent works focusing on disentangled collaborative filtering.</p><p>We would also like to point out that there are also several works related to disentangled representation for recommendation <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b76">[77]</ref>. However, we find that some of these works require multimodal <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b76">[77]</ref> or heterogeneous <ref type="bibr" target="#b70">[71]</ref> information as input, some <ref type="bibr" target="#b38">[39]</ref> in essence can be regarded as a Î²-VAE model with varying Î², and some other work <ref type="bibr" target="#b60">[61]</ref> utilizes social network information for social recommendation. These works are orthogonal to our focus in this paper and therefore are not included for comparisons in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Hyper-parameters</head><p>We constrain the number of learnable parameters to be around 2M d for each method so as to ensure fair comparison, which is equivalent to using d-dimensional representations for the M items. Note that all the methods under investigation use two sets of item representations, and we do not constrain the dimension of user representations since they are not parameters. We treat K as a hyper-parameter to be tuned and do not directly set K to the ground truth when [?], <ref type="bibr" target="#b66">[67]</ref> evaluating its performance on recommendation tasks, so as to ensure a fair comparison with the baselines. We set d = 200 and fix Ï„ to 0.1. The neural network f nn (â€¢) in our model is a multilayer perceptron (MLP), whose input and output are constrained to be d-dimensional and 2d-dimensional, respectively. We use the tanh activation function. We apply dropout before every layers, except the last layer. The model is trained using Adam. We then tune the other hyper-parameters of both our approach's and our baselines' automatically using the TPE method <ref type="bibr" target="#b4">[5]</ref> implemented by Hyepropt <ref type="bibr" target="#b3">[4]</ref>. We let Hyperopt conduct 200 trials to search for the optimal hyper-parameter configuration for each method on the validation of each dataset. The hyperparameter search space is specified as follows:</p><p>â€¢ The standard deviation of the prior Ïƒ 0 âˆˆ [0.075, 0.5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>The strength of micro disentanglement Î² âˆˆ [0, 100].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>The number of macro factors K âˆˆ {1, 2, 3, . . . , 20}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>The learning rate âˆˆ [10 âˆ’8 , 1].</p><formula xml:id="formula_49">â€¢ L2 regularization âˆˆ [10 âˆ’12 , 1]. â€¢ Dropout rate âˆˆ [0.05, 1].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>The number of hidden layers in a neural network âˆˆ {0, 1, 2, 3}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>The number of neurons in a hidden layer âˆˆ {50, 100, 150, . . . , 700}.  where p i|k := p Î¸ (c i,k = 1)/ i â€² p Î¸ (c i â€² ,k = 1). We, however, do not find this adaptive strategy to be significantly better than the naÃ¯ve strategy that treats K as a hyper-parameter to be tuned by Hyperopt, since the adaptive strategy introduces extra computational cost as well as a new hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Number of Macro Factors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Experimental Environment</head><p>We implement our model with Tensorflow, and conduct our experiments with:</p><p>â€¢ CPU: Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz.</p><p>â€¢ RAM: DDR4 1TB.</p><p>â€¢ GPU: 8x GeForce GTX 1080 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>Operating system: Ubuntu 18.04 LTS.</p><p>â€¢ Software: Python 3.6; NumPy 1.15.4; SciPy 1.2.0; scikitlearn 0.20.0; TensorFlow 1.12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Performance</head><p>We evaluate the performance of our approach on the task of collaborative filtering for implicit feedback datasets <ref type="bibr" target="#b22">[23]</ref>, one of the most common settings for recommendation. We follow the experiment protocol established by the previous work <ref type="bibr" target="#b39">[40]</ref> strictly, as well as use the same preprocessing procedure and evaluation metrics. The results on the five datasets are shown in Table <ref type="table" target="#tab_5">2</ref>.</p><p>We observe that our SEM-MacridVAE model outperforms the baseline methods significantly in all but one case for all datasets. The improvement is likely due to two desirable properties of our approach. Firstly, macro disentanglement not only allows us to accurately represent the diverse interests of a user using the different components, but also alleviates data sparsity by allowing a rarely visited item to borrow information from other items of the same category, which is the motivation behind many hierarchical methods <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b74">[75]</ref>. Secondly, as we will later show in Section 4.4 that the dimensions of the representations learned by our approach are highly disentangled, i.e., independent, which should take credits from the micro disentanglement regularizer leading to more robust performances. This second property implies that our approach can be more robust to the scenario where multiple factors are co-influencing the data generating process, especially when there is a limited amount of available data <ref type="bibr" target="#b2">[3]</ref>. For example, it would not overreact to the preference for bag size when making a prediction that is only related with the preference for bag color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEM-MacridVAE v.s. MacridVAE</head><p>The comparisons between SEM-MacridVAE with item semantic information and MacridVAE without semantic information in Table <ref type="table" target="#tab_5">2</ref> further validate the benefit of considering semantics in boosting model performances. Indeed, incorporating semantic meanings has been regarded as one effective way to improve both model accuracy and explainability of machine learning algorithms in the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With Macro &amp; Micro Disentanglement v.s. Without Macro &amp; Micro Disentanglement</head><p>Ablation studies (Without Macro and Without Micro) in Table <ref type="table" target="#tab_7">3</ref> confirm the benefit of conducting both Macro and Micro disentanglement when making recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With Visual &amp; Categorical Signals v.s. Without Visual &amp; Categorical Signals</head><p>Similarly, comparisons for Without Visual, Without Categorical and SEM-MacridVAE (Full Model) in Table <ref type="table" target="#tab_7">3</ref> further validate the necessity of incorporating semantic information to boost the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussions</head><p>The ablation studies on the two-level (macro and micro) disentanglement and the semantic (visual and categorical) signals show the improvement of the model performance brought by these core components in our proposed SEM-MacridVAE model. On the one hand, the macro disentanglement helps to capture user high-level intentions from a diversity of potential interests, while the micro disentanglement targets at learning the user low-level preferences in a more fine-grained way. Taking both levels of intentions into consideration enables the proposed SEM-MacridVAE model to more accurately infer user interests, thus improving the final recommendation performance. On the other hand, incorporating the categorical signals can more accurately align the learned macro disentangled intentions (i.e., the prototype concepts) to the ground-truth item categories, thus leading to better representation learning. Moreover, employing the visual signals to initialize the item embeddings is able to ease the process of learning detailed user visual preferences (e.g., the color of a bag) for our SEM-MacridVAE model. These two types of semantic supervision are taken into account to improve both the disentanglement and explainability of SEM-MacridVAE with human prior, which is illustrated by the ablation studies in Table <ref type="table" target="#tab_5">2</ref> as we expect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Macro Disentanglement</head><p>In order that we can qualitatively examine to which degree our proposed SEM-MacridVAE model is able to achieve macro disentanglement, the high-dimensional representations learned by our approach are visualized on three Amazon datasets, i.e., Amazon Musical Instruments, Amazon Home&amp;Kitchen and Amazon Clothing&amp;Shoes&amp;Jewelry. We pick subsets from these three Amazon datasets respectively such that every item only belongs to one category, and the number of items in every category is balanced to be closed to each other. Concretely, we set K to 4 for Amazon Musical Instruments, 5 for Amazon Home&amp;Kitchen and 3 for Amazon Clothing&amp;Shoes&amp;Jewelry i.e., the number of ground-truth categories, when training our model. We then match each learned prototype to a ground truth category by greedily minimizing the distance between the prototype and the center of the items from that category. We visualize the item representations and the user representations together using t-SNE <ref type="bibr" target="#b52">[53]</ref>, where we treat the K components of a user as K individual points and keep only the two components that have the highest confidence levels. The confidence of component k is defined as i:xu,i&gt;0 c i,k , where c i,k is the value inferred by our SEM-MacridVAE model rather than taken from the ground-truth.</p><p>Figure <ref type="figure">2</ref>, Figure <ref type="figure" target="#fig_14">3</ref> and Figure <ref type="figure" target="#fig_19">4</ref> depict the visualization results on Amazon Musical Instruments, Amazon Home&amp;Kitchen and Amazon Clothing&amp;Shoes&amp;Jewelry respectively. Specifically, item i is colored according to arg max k c i,k , i.e., the inferred category. The discovered clusters of items (see Figure <ref type="figure">2a</ref>, Figure <ref type="figure" target="#fig_14">3a</ref> and Figure <ref type="figure" target="#fig_19">4a</ref>), learned in an unsupervised manner, align well with the ground-truth categories (see Figure <ref type="figure">2b</ref>, Figure <ref type="figure" target="#fig_14">3b</ref> and Figure <ref type="figure" target="#fig_19">4b</ref>, where the color order is chosen such that the connections between the ground-truth categories and the learned clusters are easy to verify). Figure <ref type="figure">2c</ref>, Figure <ref type="figure" target="#fig_14">3c</ref> and Figure <ref type="figure" target="#fig_19">4c</ref> highlight the importance of using cosine similarity rather than inner product to combat mode collapse, where items are obtained by training a new model that uses inner product instead of cosine, colored according to the value of arg max k c i,k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Interpretability</head><p>Again, we take the Amazon datasets as instances. Figure <ref type="figure">2a</ref>, Figure <ref type="figure" target="#fig_14">3a</ref> and Figure <ref type="figure" target="#fig_19">4a</ref> show the clusters inferred based on the prototypes on Amazon Musical Instruments, Amazon Home&amp;Kitchen and Amazon Clothing&amp;Shoes&amp;Jewelry respectively, which are similar to Figure <ref type="figure">2b</ref>, Figure <ref type="figure" target="#fig_14">3b</ref> and Figure <ref type="figure" target="#fig_19">4b</ref> showing the ground-truth categories respectively. Given that the proposed SEM-MacridVAE model is trained without the ground-truth category labels, we believe that our approach is able to discover and disentangle the macro structures underlying the user behavior data in an interpretable manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Cosine vs. Inner Product</head><p>To further present the necessity of adopting cosine similarity instead of the widely used inner product similarity, we train an additional model using inner product rather than cosine to calculate similarity. The item representations obtained from this additional model on Amazon Musical Instruments, Amazon Home&amp;Kitchen and Amazon Clothing&amp;Shoes&amp;Jewelry are visualized in Figure <ref type="figure">2c</ref>, Figure <ref type="figure" target="#fig_14">3c</ref> and Figure <ref type="figure" target="#fig_19">4c</ref> respectively.</p><p>We observe that by adopting inner product as the similarity measure, the clustering results may vary for different datasets. The prototype assignments are similar on Amazon Musical Instruments (see Figure <ref type="figure">2c</ref>), while the majority of the items are assigned to the same prototype on Amazon Home&amp;Kitchen (see Figure <ref type="figure" target="#fig_14">3c</ref>) or assigned to wrong prototypes different from the ground-truth prototypes on Amazon Clothing&amp;Shoes&amp;Jewelry (see Figure <ref type="figure" target="#fig_19">4c</ref>). On the other hand, each of the prototypes learned by the cosine-based model is assigned quite a significant number of items, being consistent with the ground-truth categories (see Figure <ref type="figure">2a</ref>, Figure <ref type="figure" target="#fig_14">3a</ref> and Figure <ref type="figure" target="#fig_19">4a</ref>).</p><p>These results support our claim that an appropriate metric space such as the one defined through the cosine similarity will play an important role in preventing the mode collapse problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Micro Disentanglement</head><p>In addition to macro disentanglement, it is also necessary to examine the capability of our proposed SEM-MacridVAE in achieving micro disentanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Independence</head><p>One important motivation of disentangled representation learning is to achieve robust performance by letting the dimensions capture the underlying explanatory factors in a statistically independent way.</p><p>To gain further insight, we vary the hyper-parameters related with micro disentanglement, i.e., Î² for our proposed SEM-MacridVAE, MacridVAE and MultiVAE. In Figure <ref type="figure" target="#fig_20">5</ref>, we plot the relationships between the level of independence (micro disentanglement) achieved and the corresponding recommendation performance. Each method is evaluated on ML-latest-small, Amazon Musical Instruments, Amazon Movies&amp;TV, Amazon Home&amp;Kitchen and Amazon Clothing&amp;Shoes&amp;Jewelry. We quantify the level of independence achieved by a set of d-dimensional representations using 1âˆ’ 2 d(dâˆ’1) 1â‰¤i&lt;jâ‰¤d |corr i,j |, where corr i,j is the correlation between dimension i and j. Figure <ref type="figure" target="#fig_20">5</ref> indicates that high performance is in general associated with a relatively high level of independence (micro disentanglement) and SEM-MacridVAE achieves a higher level of micro disentanglement than MultiVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Interpretability</head><p>We train our SEM-MacridVAE model with d = 10, Î² = 50 and Ïƒ 0 = 0.3 on Amazon datasets, and investigate the interpretability of the dimensions using strategies introduced in Section 3.5.</p><p>In Figure <ref type="figure" target="#fig_22">6</ref>, Figure <ref type="figure" target="#fig_24">7</ref> and Figure <ref type="figure">8</ref>, we retrieve some representative dimensions that have humanunderstandable semantics on Amazon Musical Instruments, Amazon Home&amp;Kitchen and Amazon Clothing&amp;Shoes&amp;Jewelry respectively. The examples from these three datasets suggest that our SEM-MacridVAE model has the potential to offer users fine-grained controls over targeted aspects of the candidate items in recommendation lists. However, we note that not all dimensions are human-understandable.</p><p>Moreover, as is pointed out by Locatello et al. <ref type="bibr" target="#b42">[43]</ref>, welltrained interpretable models can only be reliably identified with the help of external knowledge, e.g., item attributes. Therefore, we encourage future efforts in investigating more semi-supervised methods <ref type="bibr" target="#b43">[44]</ref> for disentangled representation learning.   Fig. <ref type="figure">2</ref>: Visualization of macro disentanglement for Amazon Musical Instruments with K = 4, where item i is colored according to arg max k c i,k , i.e., the inferred category. The discovered clusters of items (see Figure <ref type="figure">2a</ref>) align well with the ground-truth categories (see Figure <ref type="figure">2b</ref>, where the color order is chosen such that the connections between the ground-truth categories and the learned clusters are easy to verify). Figure <ref type="figure">2c</ref> highlights the importance of using cosine similarity rather than inner product to combat mode collapse, where items are obtained by training a new model that uses inner product instead of cosine, colored according to the value of arg max k c i,k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Complexity Analysis</head><p>In addition to the performance illustration and interpretability visualization of our proposed SEM-MacridVAE model, we further provide the model complexity analysis.</p><p>Space Complexity As mentioned before, the space complexity, i.e., the number of parameters used by SEM-MacridVAE, is 2M d where M is the number of items and d is the dimension of latent factors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Complexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we study the problem of learning disentangled representations from user behaviors, and propose our SEM-MacridVAE model capable of performing disentanglement at both macro and micro levels. We relate macro factors to high-level concepts associated with user intentions (buy a pair of shoes or a laptop) and micro factors to low-level individual user preferences (the size or the color of a shirt). Extra semantic item information, including visual semantic and categorical semantic, are further taken into consideration to boost recommendation performance. Empirical results including both quantitative and qualitative experiments over five real-world datasets demonstrate the effectiveness of our approach in learning disentangled representations that are robust, interpretable, and controllable. As for future work, it will be an interesting and promising research direction for future investigation to explore novel                 (g) Shape, the same dimension as 8a.</p><p>(h) Color, the same dimension as 8b.</p><p>(i) Style, the same dimension as 8c.</p><p>Fig. <ref type="figure">8</ref>: Starting from an item representation, we gradually change the value of a target dimension and retrieve the items having similar representations with the changed representations, as is described in Section 3.5. Here we present the retrieved items in Amazon Clothing&amp;Shoes&amp;Jewelry dataset when varying the target dimension and fixing others.</p><p>applications that can benefit in the interpretability and controllability brought by the disentangled representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 ,</head><label>1</label><figDesc>ğœğœ 1 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: The whole framework of our proposed SEM-MacridVAE model. The macro disentanglement is accomplished through learning a set of prototypes (macro concepts) based on which the user intention related with each item is inferred. Different colors (blue, yellow and green) in the figure indicate different macro concepts where each macro concept (one color) has one single independent prototype, encoder and decoder (with the same color). The micro disentanglement is achieved by capturing the preferences of the target user over different intentions separately, guaranteed by magnifying the KL divergence where a term penalizing the total correlation can be separated with a factor of Î². Semantic information included categorical and visual signals extracted from candidate items is utilized to further improve model performance. In particular, visual signals are used to initialize the item factors and prototype representations, and categorical signals serve as the supervisions for learning macro concept C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>k=1 12 :</head><label>12</label><figDesc>function PROTOTYPECLUSTERING 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Our initial implementation adaptively adjusts the number of macro factors K during training. To be specific, we set K as a sufficiently large value at the beginning and shrink its value after every training epoch if the Jensen-Shannon (JS) divergence between {p i|k } M i=1 and {p i|k â€² } M i=1 for some k Ì¸ = k â€² is negligible compared to a predefined threshold, Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on April 28,2022 at 12:30:25 UTC from IEEE Xplore. Restrictions apply. 0162-8828 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2022.3153112, IEEE Transactions on Pattern Analysis and Machine Intelligence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Items in Amazon Musical Instruments, colored based on the predicted categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Items in Amazon Musical Instruments, colored based on the ground-truth categories. Users in Amazon Musical Instruments, colored based on the predicted categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Items and Users in Amazon Musical Instruments, colored based on the predicted categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>We analyze the time complexity according to the sequential execution pipeline of the proposed algorithm by calculating the times of element multiplication. Assuming that there are N users and M items, the Prototype Clustering process requires O(M dK) times of multiplications. Both the Encoding process and Decoding process need O(N M dK) times of multiplications. The incorporation of visual signals is pre-trained and does not consume extra running time during the training process. The incorporation of categorical signals needs O(M K) times of multiplications. Summing up all the above operations, the time complexity of our SEM-MacridVAE model is O(N M dK + N M dK + M dK + M K) = O(N M dK) times of element multiplications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Items in Amazon Home&amp;Kitchen, colored based on the predicted categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Items in Amazon Home&amp;Kitchen, colored based on the ground-truth categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Items obtained by training a new model using inner product instead of cosine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Users in Amazon Home&amp;Kitchen, colored based on the predicted categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Items and Users in Amazon Home&amp;Kitchen, colored based on the predicted categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Visualization of macro disentanglement for Amazon Home&amp;Kitchen with K = 5, in the same way as Figure2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Items in Amazon Clothing&amp;Shoes, colored based on the predicted categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Items in Amazon Clothing&amp;Shoes, colored based on the ground-truth categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Users in Amazon Clothing&amp;Shoes&amp;Jewelry, colored based on the predicted categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Items and Users in Amazon Clothing&amp;Shoes&amp;Jewelry, colored based on the predicted categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: Visualization of macro disentanglement for Amazon Clothing&amp;Shoes&amp;Jewelry with K = 3, in the same way as Figure2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Micro disentanglement vs. recommendation performance. By varying the hyper-parameters Î², we compare the micro disentanglement and recommendation performance. It is observed that SEM-MacridVAE overall outperforms both MacridVAE and multiVAE in terms of both micro disentanglement and recommendation performance under recall@20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Shape, the same dimension as 6a. (e) Color, the same dimension as 6b.(f) Style, the same dimension as 6c.(g) Shape, the same dimension as 6a.(h) Color, the same dimension as 6b.(i) Style, the same dimension as 6c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: Starting from an item representation, we gradually change the value of a target dimension and retrieve the items having similar representations with the changed representations, as is described in Section 3.5. Here we present the retrieved items in Amazon Musical Instruments dataset when varying the target dimension and fixing others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>Shape, the same dimension as 7a. (e) Color, the same dimension as 7b.(f) Style, the same dimension as 7c.(g) Shape, the same dimension as 7a.(h) Color, the same dimension as 7b.(i) Style, the same dimension as 7c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 7 :</head><label>7</label><figDesc>Fig.7: Starting from an item representation, we gradually change the value of a target dimension and retrieve the items having similar representations with the changed representations, as is described in Section 3.5. Here we present the retrieved items in Amazon Home&amp;Kitchen dataset when varying the target dimension and fixing others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>Shape, the same dimension as 8a. (e) Color, the same dimension as 8b.(f) Style, the same dimension as 8c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2022.3153112, IEEE Transactions on Pattern Analysis and Machine Intelligence capable of producing robust, controllable, and explainable representations, disentangled representation learning has become one of the core problems in machine learning. In general, variational methods are widely applied for disentangled representation over images. Î²-VAE</figDesc><table /><note>Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on April 28,2022 at 12:30:25 UTC from IEEE Xplore. Restrictions apply.0162-8828 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>1 :</head><label>1</label><figDesc>input: x u = {x u,i : user u clicks item i, i.e., x u,i = 1}. Concept prototypes m k âˆˆ R d for k = 1, 2, . . . , K; Item representations h i âˆˆ R d for i = 1, 2, . . . , M ; Context representations t i âˆˆ R d for i = 1, 2, . . . , M ; Parameters of a neural network f nn : R d â†’ R 2d ; Item categories: Ä‰i âˆˆ R K for i = 1, 2, . . . , M ;â–· All these parameters are collectively denoted as Î¸.</figDesc><table /><note>2: parameters: 3: function INITIALIZATION WITH SEMANTICS 4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 :</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell>Users</cell><cell>Item</cell><cell cols="2">Ratings Density</cell></row><row><cell>ML-latest-small</cell><cell>531</cell><cell>4807</cell><cell>46217</cell><cell>1.8106%</cell></row><row><cell>Movies&amp;TV</cell><cell>15187</cell><cell>46234</cell><cell cols="2">623239 0.0888%</cell></row><row><cell>Musical Instruments</cell><cell>655</cell><cell>10377</cell><cell>15540</cell><cell>0.2286%</cell></row><row><cell>Home&amp;Kitchen</cell><cell>6640</cell><cell>52900</cell><cell cols="2">154622 0.0440%</cell></row><row><cell>Clothing&amp;shoes&amp;Jewelry</cell><cell>9575</cell><cell cols="3">130742 197708 0.0158%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>Results of recommendation performance, where bold font denotes the winner. We note that all models are constrained to have around 2M d parameters, where M is the number of items and d is the dimension of each item representation. The experiments show our proposed SEM-MacridVAE model is able to beat all comparative baselines.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>NDCG@100</cell><cell>Metrics recall@20</cell><cell>recall@50</cell></row><row><cell>ML-latest</cell><cell>MultiDAE</cell><cell cols="3">0.31930(Â±0.02657) 0.27885(Â±0.03689) 0.37373(Â±0.03658)</cell></row><row><cell>-small</cell><cell>MultiVAE</cell><cell cols="3">0.33233(Â±0.03031) 0.28539(Â±0.03659) 0.38718(Â±0.03774)</cell></row><row><cell></cell><cell>MacridVAE</cell><cell cols="3">0.34180(Â±0.03190) 0.29844(Â±0.03711) 0.38994(Â±0.03696)</cell></row><row><cell></cell><cell>DGCF</cell><cell cols="3">0.34709(Â±0.02971) 0.29185(Â±0.03433) 0.39353(Â±0.03836)</cell></row><row><cell></cell><cell cols="4">SEM-MacridVAE 0.35129(Â±0.03204) 0.30293(Â±0.03591) 0.40026(Â±0.03724)</cell></row><row><cell>Movies</cell><cell>MultiDAE</cell><cell cols="3">0.09774(Â±0.00336) 0.08342(Â±0.00382) 0.13936(Â±0.00498)</cell></row><row><cell>&amp;TV</cell><cell>MultiVAE</cell><cell cols="3">0.09953(Â±0.00338) 0.08431(Â±0.00387) 0.14004(Â±0.00496)</cell></row><row><cell></cell><cell>MacirdVAE</cell><cell cols="3">0.11619(Â±0.00365) 0.10397(Â±0.00426) 0.16011(Â±0.00527)</cell></row><row><cell></cell><cell>DGCF</cell><cell cols="3">0.10060(Â±0.00328) 0.08541(Â±0.00380) 0.14666(Â±0.00498)</cell></row><row><cell></cell><cell cols="4">SEM-MacridVAE 0.11674(Â±0.00367) 0.10466(Â±0.00423) 0.16101(Â±0.00515)</cell></row><row><cell>Musical</cell><cell>MultiDAE</cell><cell cols="3">0.04508(Â±0.01194) 0.03171(Â±0.01767) 0.09709(Â±0.03196)</cell></row><row><cell>Instruments</cell><cell>MultiVAE</cell><cell cols="3">0.04420(Â±0.01156) 0.03436(Â±0.01781) 0.09590(Â±0.03022)</cell></row><row><cell></cell><cell>MacridVAE</cell><cell cols="3">0.05706(Â±0.01871) 0.04034(Â±0.01885) 0.09419(Â±0.03199)</cell></row><row><cell></cell><cell>DGCF</cell><cell cols="3">0.06109(Â±0.01657) 0.08352(Â±0.03310) 0.11870(Â±0.03754)</cell></row><row><cell></cell><cell cols="4">SEM-MacridVAE 0.06450(Â±0.01415) 0.08479(Â±0.03132) 0.13436(Â±0.03829)</cell></row><row><cell>Home</cell><cell>MultiDAE</cell><cell cols="3">0.03577(Â±0.00401) 0.03488(Â±0.00499) 0.06190(Â±0.00674)</cell></row><row><cell>&amp;Kitchen</cell><cell>MultiVAE</cell><cell cols="3">0.03761(Â±0.00420) 0.03607(Â±0.00514) 0.06094(Â±0.00671)</cell></row><row><cell></cell><cell>MacridVAE</cell><cell cols="3">0.04271(Â±0.00456) 0.03641(Â±0.00510) 0.06737(Â±0.00659)</cell></row><row><cell></cell><cell>DGCF</cell><cell cols="3">0.04370(Â±0.00404) 0.03853(Â±0.00494) 0.07699(Â±0.00708)</cell></row><row><cell></cell><cell cols="4">SEM-MacridVAE 0.04463(Â±0.00434) 0.04669(Â±0.00557) 0.07913(Â±0.00727)</cell></row><row><cell>Clothing</cell><cell>MultiDAE</cell><cell cols="3">0.01123(Â±0.00213) 0.01156(Â±0.00297) 0.01725(Â±0.00353)</cell></row><row><cell>&amp;Shoes</cell><cell>MultiVAE</cell><cell cols="3">0.01107(Â±0.00182) 0.01278(Â±0.00296) 0.02507(Â±0.00432)</cell></row><row><cell>&amp;Jewelry</cell><cell>MacridVAE</cell><cell cols="3">0.01785(Â±0.00265) 0.01540(Â±0.00313) 0.03009(Â±0.00458)</cell></row><row><cell></cell><cell>DGCF</cell><cell cols="3">0.01833(Â±0.00296) 0.02293(Â±0.00462) 0.03691(Â±0.00558)</cell></row><row><cell></cell><cell cols="2">SEM-MacridVAE 0</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>.01853(Â±0.00240) 0.02491(Â±0.00434) 0.03720(Â±0.00517)</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3 :</head><label>3</label><figDesc>Ablation studies on Macro &amp; Micro disentanglement and Visual &amp; Categorical signals, where bold font denotes the winner. The experimental results demonstrate our proposed model with full functionality (i.e., SEM-MacridVAE) achieve the best performance.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>NDCG@100</cell><cell>Metrics recall@20</cell><cell>recall@50</cell></row><row><cell>ML-latest</cell><cell>Without Macro</cell><cell cols="3">0.34571(Â±0.03125) 0.29806(Â±0.03481) 0.38994(Â±0.03898)</cell></row><row><cell>-small</cell><cell>Without Micro</cell><cell cols="3">0.33872(Â±0.02977) 0.29140(Â±0.03482) 0.38253(Â±0.03828)</cell></row><row><cell></cell><cell>Without Visual</cell><cell cols="3">0.34568(Â±0.03222) 0.29999(Â±0.03729) 0.38504(Â±0.03750)</cell></row><row><cell></cell><cell>Without Categorical</cell><cell cols="3">0.34469(Â±0.03130) 0.30278(Â±0.03492) 0.39415(Â±0.03824)</cell></row><row><cell></cell><cell cols="4">SEM-MacridVAE (Full Model) 0.35129(Â±0.03204) 0.30293(Â±0.03591) 0.40026(Â±0.03724)</cell></row><row><cell>Movies</cell><cell>Without Macro</cell><cell cols="3">0.11309(Â±0.00359) 0.10294(Â±0.00422) 0.15935(Â±0.00521)</cell></row><row><cell>&amp;TV</cell><cell>Without Micro</cell><cell cols="3">0.11064(Â±0.00345) 0.10210(Â±0.00424) 0.16062(Â±0.00522)</cell></row><row><cell></cell><cell>Without Visual</cell><cell cols="3">0.11524(Â±0.00365) 0.10360(Â±0.00417) 0.16025(Â±0.00521)</cell></row><row><cell></cell><cell>Without Categorical</cell><cell cols="3">0.11559(Â±0.00365) 0.10336(Â±0.00424) 0.16012(Â±0.00515)</cell></row><row><cell></cell><cell cols="4">SEM-MacridVAE (Full Model) 0.11674(Â±0.00367) 0.10466(Â±0.00423) 0.16101(Â±0.00515)</cell></row><row><cell>Musical</cell><cell>Without Macro</cell><cell cols="3">0.06389(Â±0.01775) 0.07111(Â±0.02798) 0.11726(Â±0.03532)</cell></row><row><cell>Instruments</cell><cell>Without Micro</cell><cell cols="3">0.06206(Â±0.01383) 0.07111(Â±0.02798) 0.13733(Â±0.03819)</cell></row><row><cell></cell><cell>Without Visual</cell><cell cols="3">0.05801(Â±0.01923) 0.04496(Â±0.02238) 0.10265(Â±0.03253)</cell></row><row><cell></cell><cell>Without Categorical</cell><cell cols="3">0.06355(Â±0.01426) 0.08265(Â±0.02881) 0.13656(Â±0.03621)</cell></row><row><cell></cell><cell cols="4">SEM-MacridVAE (Full Model) 0.06450(Â±0.01415) 0.08479(Â±0.03132) 0.13436(Â±0.03829)</cell></row><row><cell>Home</cell><cell>Without Macro</cell><cell cols="3">0.04377(Â±0.00404) 0.04412(Â±0.00556) 0.07332(Â±0.00713)</cell></row><row><cell>&amp;Kitchen</cell><cell>Without Micro</cell><cell cols="3">0.04365(Â±0.00429) 0.04243(Â±0.00535) 0.07166(Â±0.00690)</cell></row><row><cell></cell><cell>Without Visual</cell><cell cols="3">0.04264(Â±0.00437) 0.04066(Â±0.00525) 0.07030(Â±0.00689)</cell></row><row><cell></cell><cell>Without Categorical</cell><cell cols="3">0.04440(Â±0.00426) 0.04250(Â±0.00520) 0.07484(Â±0.00692)</cell></row><row><cell></cell><cell cols="4">SEM-MacridVAE (Full Model) 0.04463(Â±0.00434) 0.04669(Â±0.00557) 0.07913(Â±0.00727)</cell></row><row><cell>Clothing</cell><cell>Without Macro</cell><cell cols="3">0.01817(Â±0.00254) 0.02233(Â±0.00398) 0.03055(Â±0.00470)</cell></row><row><cell>&amp;Shoes</cell><cell>Without Micro</cell><cell cols="3">0.01805(Â±0.00257) 0.02219(Â±0.00398) 0.03660(Â±0.00511)</cell></row><row><cell>&amp;Jewelry</cell><cell>Without Visual</cell><cell cols="3">0.01645(Â±0.00246) 0.01963(Â±0.00369) 0.03107(Â±0.00469)</cell></row><row><cell></cell><cell>Without Categorical</cell><cell cols="3">0.01852(Â±0.00250) 0.02248(Â±0.00406) 0.03328(Â±0.00476)</cell></row><row><cell></cell><cell cols="2">SEM-MacridVAE (Full Model) 0</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>.01853(Â±0.00240) 0.02491(Â±0.00434) 0.03720(Â±0.00517)</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Georgia Institute of Technology. Downloaded on April 28,2022 at 12:30:25 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, JANUARY 2022</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the National Key Research and Development Program of China No. 2020AAA0106300 and National Natural Science Foundation of China No. 62102222.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information dropout: Learning optimal representations through noisy computation</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2897" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Python in science conference</title>
				<meeting>the 12th Python in science conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">RÃ©mi</forename><surname>James S Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BalÃ¡zs</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>KÃ©gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilevel variational autoencoder: Learning disentangled representations from grouped observations</title>
		<author>
			<persName><forename type="first">Diane</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Irina</forename><surname>Christopher P Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<title level="m">Understanding disentangling in beta-vae</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qi Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2610" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
				<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Item-based top-n recommendation algorithms</title>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="177" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Nat</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Am</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><surname>Shanahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02648</idno>
		<title level="m">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning disentangled joint continuous and discrete representations</title>
		<author>
			<persName><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="710" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural scene representation and rendering</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Sm Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avraham</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="issue">6394</biblScope>
			<biblScope unit="page" from="1204" to="1210" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A novel bayesian similarity measure for recommender systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yorke-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>the 23rd International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2619" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis)</title>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An unsupervised neural attention model for aspect extraction</title>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web, WWW &apos;16</title>
				<meeting>the 25th International Conference on World Wide Web, WWW &apos;16<address><addrLine>Republic and Canton of Geneva, CHE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trirank: Review-aware explainable recommendation by modeling aspects</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1661" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Willem</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><forename type="middle">J</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07212</idno>
		<title level="m">Learning disentangled representations of texts with application to biomedical abstracts</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trustwalker: a random walk model for combining trust-based and item-based recommendation</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Jamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A matrix factorization technique with trust propagation for recommendation in social networks</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Jamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM conference on Recommender systems</title>
				<meeting>the fourth ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName><forename type="first">SÃ©bastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)</title>
				<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Variational deep embedding: an unsupervised and generative approach to clustering</title>
		<author>
			<persName><forename type="first">Zhuxi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huachun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1965" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2654" to="2663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequential attend, infer, repeat: Generative modelling of moving objects</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8606" to="8616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Collaborative variational autoencoder for recommender systems</title>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>She</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning disentangled user representation based on controllable vae for recommendation</title>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database Systems for Advanced Applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="179" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference, WWW &apos;18</title>
				<meeting>the 2018 World Wide Web Conference, WWW &apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adreveal: Improving transparency into online targeted advertising</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anmol</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udi</forename><surname>Weinsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaideep</forename><surname>Chandrashekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Govindan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM Workshop on Hot Topics in Networks</title>
				<meeting>the Twelfth ACM Workshop on Hot Topics in Networks</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Eigenrank: a ranking-oriented approach to collaborative filtering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>ICML 2019</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Disentangling factors of variation using few labels</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>RÃ¤tsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01258</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The harpy speech recognition system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lowere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to recommend with social trust ensemble</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Michael R Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sorec: social recommendation using probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Information and knowledge management</title>
				<meeting>the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="931" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
				<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML 2019)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML 2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical taxonomy aware network embedding</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for recommendation</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5711" to="5722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Disentangled person image generation</title>
		<author>
			<persName><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
				<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Grouplens: an open architecture for collaborative filtering of netnews</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neophytos</forename><surname>Iacovou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><surname>Suchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 ACM conference on Computer supported cooperative work</title>
				<meeting>the 1994 ACM conference on Computer supported cooperative work</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>II-1278. JMLR. org</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
				<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using markov chain monte carlo</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
				<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Disentangling multi-facet social relations for recommendation</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Social Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Disentangled graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event</title>
				<editor>
			<persName><forename type="first">Jimmy</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vanessa</forename><surname>Murdock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">July 25-30, 2020. 2020</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multimodal disentangled representation for recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Multimedia and Expo (ICME)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning personalized preference of strong and weak ties for social recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Social recommendation with strong and weak ties</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Social recommendation with optimal limited attention</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1518" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Disenhan: Disentangled heterogeneous graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntong</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1605" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Cccf: Improving collaborative filtering via scalable user-item coclustering</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Ninth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Twenty years of mixture of experts</title>
		<author>
			<persName><forename type="first">Esen</forename><surname>Seniha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">N</forename><surname>Yuksel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><surname>Gader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1177" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficient bayesian hierarchical user modeling for recommendation system</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Efficient bayesian hierarchical user modeling for recommendation system</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Contentcollaborative disentanglement representation learning for enhanced recommendation</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Balby Marinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">M</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys 2020: Fourteenth ACM Conference on Recommender Systems, Virtual Event</title>
				<meeting><address><addrLine>Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">September 22-26, 2020. 2020</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
		<respStmt>
			<orgName>Noam Koenigstein, and Edleno Silva de Moura</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Contentcollaborative disentanglement representation learning for enhanced recommendation</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Explicit factor models for explainable recommendation based on phrase-level sentiment analysis</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
				<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Atrank: An attention-based user behavior modeling framework for recommendation</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junshuai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengchao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">His research interests include multimedia intelligence, machine learning and its applications in multimedia big data. He has published several high-quality research papers in top journals and conferences including IEEE</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Tpami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ieee</forename><surname>Tkde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ieee</forename><surname>Tmm</surname></persName>
		</author>
		<author>
			<persName><surname>Icml</surname></persName>
		</author>
		<author>
			<persName><surname>Neurips</surname></persName>
		</author>
		<author>
			<persName><surname>Multimedia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Www</forename><surname>Kdd</surname></persName>
		</author>
		<author>
			<persName><surname>Etc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="118" to="129" />
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology, Tsinghua University ; Computer Science and Technology from Zhejiang University ; Computing Science from Simon Fraser University</orgName>
		</respStmt>
	</monogr>
	<note>Visual object networks: Image generation with disentangled 3d representations. He is the recipient of 2017 China Postdoctoral innovative talents supporting program. He receives the ACM China Rising Star Award in 2020</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
