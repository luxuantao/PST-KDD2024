<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity Search Engine: Towards Agile Best-Effort Information Integration over the Web</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Cheng</surname></persName>
							<email>tcheng3@cs.uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen-Chuan</forename><surname>Chang</surname></persName>
							<email>kcchang@cs.uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Entity Search Engine: Towards Agile Best-Effort Information Integration over the Web</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2CBAEE19447E89320D88A3CD1486F148</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The immense scale and wide spread has rendered the Web as an ultimate information repository-as not only the sources where we find but also the destinations where we publish information. The dual forces have enriched the Web with all kinds of data, much beyond the conventional page view of the Web as a corpus of HTML pages, or "documents." The Web has thus become a rich collection of data-rich pages, on the "surface Web" of static URLs (e.g., personal or company homepages) as well as the "deep Web" of database-backed contents (e.g., flights from aa.com), as Figure <ref type="figure" target="#fig_0">1</ref>(a) shows. The richness of data, while a promising opportunity, has challenged us to effectively find data we need, from one or multiple sources.</p><p>In particular, we are motivated by, when building the Meta-Querier at UIUC, the need of large scale on-the-fly integration for online structured data. The MetaQuerier system, as we reported in CIDR 2005 <ref type="bibr" target="#b4">[4]</ref>, aims at finding and querying data sources on the deep Web. However, as the "last mile" for metaquerying, when users can query multiple sources on the fly <ref type="bibr" target="#b8">[8]</ref>, or when data is automatically "crawled" from sources <ref type="bibr" target="#b6">[6]</ref>, how do we identify and integrate the structured data embedded in unstructured result pages? (e.g., the query results of amazon.com and bn.com).</p><p>Further, we realized, as observed earlier, beyond our Meta-Querier experience, such data richness is pervasive-from the depth as well as the surface of the Web. While data is proliferating, however, we are currently not able to effectively access such data. To motivate, consider a few scenarios, for user Amy: Scenario 1: To call Amazon.com for her online purchase, how can Amy find their "phone number"? To begin with, what should be the right keywords for finding pages with such numbers? A query like "amazon customer service phone" may not work; often a phone is simply shown without keyword "phone" (e.g., customer service: 800-717-6688). On the other hand, "amazon customer service" could be too broad. Then, for each querying, she must sift through the returned pages to dig for the phone numbers. This overall process can be unnec- essarily time consuming. Scenario 2: To apply for graduate schools, how can she find the list of "professors" in the database area? She has to go through all the CS department pages or, even worse, check faculty's homepages one by one-a very laborious process. Scenario 3: As a graduate student, Amy needs to prepare a seminar presentation for her choice of recent papers. Can she find papers that come readily with presentations, i.e., a "PDF file" along with a "PPT file," say from SIGMOD 2006? Scenario 4: Now Amy wants to buy a copy of Shakespeare's Hamlet to read; how can she find the "prices" and "cover images" of available choices from, say, Borders.com and BN.com. She would have to look at and compare the results from multiple online bookstores one by one.</p><p>In these scenarios, like every user in many similar situations, Amy is looking for particular types of data, which we call entities, e.g., a phone number, a book cover image, a PDF, a PPT, a name, a date, an email address, etc. She is not, we stress, looking for pages as "relevant documents" to read, but entities as data for her subsequent tasks (to contact, to apply, to present, to buy, etc.). We are facing a dilemma: • On one hand, for accessing over the sheer size of the Web, we are mostly relying on search engines, such as Google, Yahoo, or MSN, which search for pages by keywords. On this extreme, while being "IR-style" with a scalable text processing framework, they are not data aware.</p><p>• On the other hand, integration services, such as Expedia.com or PriceGrabber.com, exist online for specific domains. On this extreme, while providing "DB-style" precise querying, they can hardly scale the amount of data and the number of sources on the Web.</p><p>As our solution, which this paper will propose, we believe the two extremes must meet, with a synergistic "marriage" in the middle. From the "search" perspective: Can we build a search engine with data awareness? From the "integration" perspective: Can we develop an integration system with scale awareness, even with limited "best-effort" semantics? As we will discuss in Section 3, the two perspectives together lead to our objective of entity search-As a search system, we propose our EntitySearch engine to search directly for target information "entities," and holistically across their occurrences in all pages. As an integration system, it will assume only a lightweight off-the-shelf entity extraction layer, and thus scalable to many sources; further, it will construct "derived relations" as the search output, on the fly, and thus flexible for ad-hoc user needs.</p><p>In summary, this paper proposes the concept of entity search and its initial implementation as an agile best-effort framework for realizing large scale information integration over the Web. We will motivate and define entity search in Section 2 and Section 3 respectively. Section 4 will then discuss our initial "pilot" implementation for exploring research issues. Section 5 will demonstrate the usefulness of entity search using real world scenarios. We will conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE DILEMMA</head><p>To access the Web, we must resort to search engines or integration services. The state of the art, however, presents a clear dilemma between data and scale, and we are thus lacking large scale data-based access to the Web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search Engines: Data Awareness?</head><p>On one hand, our information access is mostly relying on current search systems, such as popular engines like Google, Yahoo, and MSN Search-However, while reaching extensive parts of the Web, with their inherent "page view" of the Web, they are lacking even minimal data awareness and thus incapable for such tasks as finding "data" entities.</p><p>That is, current search systems are not data aware. In their rather simplistic page view, with a text retrieval system as their backend engine, they naturally view the Web as simply a repository of interlinked documents, or pages, each containing some keywords upon which searches can be performed. Our information access amounts to finding relevant pages by keywords-regardless of what data entities we are looking for. In other words, from this "search" perspective, we are lacking a search system that is aware of data. Specifically, this lacking leads to two major limitations of current search systems:</p><p>• Indirect Input and Output. In terms of the input and output, current engines are searching indirectly. To begin with, to formulate queries as input, users cannot directly describe what they want. Amy has to formulate her needs indirectly as keyword queries, often in a non-trivial and non-intuitive way, with a hope to hit "relevant pages" that may or may not contain target entities. For Scenario 1, will "amazon customer service phone" work? Then, as query output, users cannot directly get what they want. The engine will only take Amy, indirectly, to a list of pages, and she must manually sift through them to find the phone number. Can we help users to search directly in both describing and getting what they want?</p><p>• Singular Matching Mechanism. In terms of the matching mechanism, current search engines are finding each page singularly. Our target entities often come from multiple pages. In Scenario 1, the same phone number of Amazon may appear in the company's Web site, online user forums, or even blogs.</p><p>In this case, we should collect, for each phone, all its occurrences from multiple pages as supporting evidences of matching. In Scenario 2, the list of professors probably cannot be found in any single page. In this case, again, we must look at many pages to come up with the list of promising names (and each name may appear in multiple pages, like the earlier case). Can we help users to search holistically for matching entities across the Web corpus as a whole, instead of each individual page?</p><p>Thus, from the "search" perspective, we are facing the challenge of building data awareness into Web search systems. As data proliferates on the Web, the classic view of the Web as a page repository is increasingly inadequate. A step forward, can we search for data, or specific entities, as our target directly and across all pages holistically? That is, while keyword-based search is scalable to the Web and easy to use, can it be morphed to be data aware?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Integration Systems: Scale Awareness?</head><p>On the other hand, there already emerged many integration systems, such as the various specialized engines in comparison shopping (e.g., PriceGrabber.com, NextTag.com and BizRate.com) and vertical search services (e.g., Realtor.com for real estates, Expedia.com for airfares, and Google Base for various domains)-However, while offering precise data-based access, with their inherent "database view," they are not not designed with "scale" in mind, for the large amount of data-rich pages on the Web and the large diversity of user needs.</p><p>That is, current integration systems are not scale aware. In their rather rigid database view, they naturally assume and can only query data prepared in a structured relational format-in certain prescribed schemas-upon which SQL queries can be performed. Our information access is thus significantly limited by the availability of data in such rigid schemas, as well as the types of queries that the prescribed schema can support. In other words, from this "integration" perspective, we are lacking an integration system that is aware of scale-the proliferation of Web data and online users.</p><p>In particular, because their database view requires data in well-structured format, an integration system must build wrapper for each source, which works as a relation extractor for each site, to precisely extract its unstructured text into structured DBMS, upon which to perform SQL queries. We believe that, while achieving structural rigor, this technique is not viable, with two limitations: • Limited Sources. In terms of sources, it lacks scalability. As repeatedly reported <ref type="bibr">[7,</ref><ref type="bibr" target="#b5">5]</ref>, per-source wrappers are not only laborious to build but also fragile to maintain, and thus a cost barrier<ref type="foot" target="#foot_0">1</ref> . To incorporate myriad potential sources (and pages), we shall not rely on per-source training or construction.</p><p>• Limited Queries. In terms of queries, it lacks flexibility: A wrapper must make a hard decision on the schemas of the relations to extract-e.g., for book, to extract a relation with format, publisher, price, cover image, or all? The more complex a schema is, the more likely its extraction may fail, while a simpler scheme may be useless for many users.</p><p>Thus, from the "integration" perspective, we are facing the challenge of building scale awareness into Web integration  systems. While the prevalence of data on the Web presents novel opportunities for integration, this "Web-scale" scenario contrasts traditional settings and defeats current techniques-We must rethink not only new techniques but also realistic objectives. To bring myriads heterogeneous sources to meet ad-hoc users, as the scalability and flexibility mandate, can we develop agile integration without rigid schemas, even with "best effort" semantics?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OUR PROPOSAL: ENTITY SEARCH</head><p>Approaching the current barriers from the dual perspectives, we are inspired that they seem to converge to the same solution: The dual perspectives represent the two extremes of the spectrum: simple keyword search to fully transparent information integration. Meeting in the middle, they suggest the "marriage" of scale-aware search and data-aware integration. Towards searching directly and holistically, for finding specific types of information, we thus propose to support entity search.</p><p>• First, as input, users formulate queries to directly describe what types of data they are looking for: She can simply specify what her target entities are, and what keywords may appear in the context with a right answer. To distinguish between entities to look for and keywords in the context, we use a prefix #, e.g., #phone for the phone entity. Our scenarios will naturally lead to the following queries: Q1:</p><p>(amazon customer service #phone) Q2:</p><p>(#professor #university #research='database') Q3: ow(sigmod 2006 #pdf file #ppt file) Q4:</p><p>(#title='hamlet' #image #price)</p><p>In the queries, there are two components: (1) Context pattern, how will the target entities appear? Q1 says that #phone will appear with these keywords in the default pattern of "near" (or proximity). We may also explicitly specify the pattern, e.g., Q3 uses ow to mean order-window (the keywords must appear before #pdf file and then #ppt file.</p><p>(2) Content restriction: A target entity will match any instances of that entity type, subject to optional restriction on their content values-e.g., Q1 will match every phone instance, while Q2 will only match research area "database." (In addition to equality "=", other restriction operators are possible, such as "contain.") • Second, as output, users will get directly the entities that they are looking for. That is, as a query specifies what entity types are the targets, its results are those entity instances (or literal values) that match the query, in a ranked order by their matching scores. (We will discuss this matching next.) Figure <ref type="figure" target="#fig_1">2</ref> shows some example results for Q1 and Q3.</p><p>• Third, as search mechanism, entity search will find matching entities holistically, where an instance will be found and matched in all the pages where it occurs; e.g., a #phone 800-201-7575 may occur at multiple URLs as Figure <ref type="figure" target="#fig_1">2</ref> shows. For each instance, all its matching occurrences will be aggregated to form the final ranking; e.g., a phone number that more frequently occurs at where "amazon customer service" is mentioned may rank higher than others. Thus, entity search will not only find entities as the primary result, but also return pages where each entity is found as the secondary result and supporting "evidences."</p><p>To support entity-based querying, the system must be fundamentally entity aware: As a departure from current search engines built around the notions of pages and keywords, we must generalize them to support entity as a first-class concept. With this awareness, as our data model, we will move from the current page view, i.e., the Web as a document collection, to the new entity view, i.e., the Web as an entity repository. Upon this foundation, as our query mechanism, we can then develop entity search, where users specify what they are looking for with keywords and target entities, as Q1 -Q4 illustrated. We now formalize these notions.</p><p>Data Model: Entity View. How should we view the Web as our database to search over? In the standard page view, the Web is a set of documents (or pages) D = {d1, d2, . . . , dn}.</p><p>Our data model takes an entity view: We consider the Web as primarily a repository of entities: E = {E1, E2, . . . , En}, where each Ei is an entity type. For instance, to support Scenario 1 (Section 1), the system might be constructed with entities E = {E1 : #phone, E2 : #email}. Further, each entity type Ei is a set of entity instances that are extracted from the corpus, i.e., literal values of entity type Ei that occurs somewhere in some d ∈ D. We use ei to denote an entity instance of entity type Ei. In our example, e.g., by recognizing phonenumber patterns (say, in a regular expression of digits) from D, we may extract #phone = {"800-201-7575", "244-2919", "(217) 344-9788", . . .}</p><p>As the starting point for data-aware search, this "tagging", or entity extraction, is to recognize each entity from Web pages. Entity extraction has been well studied in the context of general information extraction, and techniques abound, from simple syntactic pattern matching to sophisticated statistical taggers, with many off-the-shelf tools available. We stress that, however, while implementations may differ, such extractors are inherently imperfect, and entity search must essentially deal with uncertainty. Used as a black box, in abstraction, an entity extractor will return, for each occurrence, the extraction confidence probability and the position of extraction. We thus transform the page view into our entity view, E = {E1, E2, . . . , En}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Entity Search Problem:</head><p>Given:</p><p>An entity collection E = {E1, E2, . . . , EN } Input:</p><p>Query: β-α(K1, K2, . . . , K l , E1, E2, . . . , Em) Output: t = e1, e2, ..., em : sorted by score(t), the tuple score of t with respect to β and α.</p><p>• As input, as Section 3 discussed, an entity-search query is similar to standard keyword queries, but now users can specify entity types, E1, E2, . . . , and Em, in addition to keywords K1, . . . , K l . Optionally, in a complete form, a query can also specify a matching pattern α (e.g., ow in Q3), to restrict when an occurrence of an instance t = e1, e2, ..., em is considered a matching tuple, and a scoring measure β, to specify how all the matching instances are ranked. Depending on the implementation and application settings, the choices of the β pattern and the α can be system built-in or user specified, and they together determine the ranking scores.</p><p>• As output, for a query with m target entities, each matching result is a m-ary entity tuple, i.e., t = e1, e2, ..., em , i.e., a combined instance. Note that an entity tuple may contain one or multiple instances, each of which is associated with one entity type. For example, David, david@hotmail.com, 315-673-9091 is an entity tuple of type #name, #email, #phone , and Canada, Ottawa of type #country, #capital city .</p><p>• The objective is to find, from the space of E1×. . .×Em, the matching tuples in ranked order by how well they match the query, i.e., how well entity tuple t and the specified keywords associate, as matched by pattern α and ranked by measure β, which result in the tuple score score(t).</p><p>To summarize, and to put our proposal in perspectives, we examine it form both the search and integration point of views:</p><p>First, as a search framework, entity search is data aware. With the probabilistic tagging of entities, we view the Web as a repository of entities. Users directly specify their target entities, and the system holistically evaluates the matchings.</p><p>Second, as an integration framework, entity search is scale aware, towards agile requirement and best-effort semantics for large scale deployment: On one hand, as deployment requirement, it is agile, requiring only minimal tagging of the data domain of interest-It can be uniformly applied to many sources, without requiring building wrappers. We stress that each entity is independently extracted in a "soft" probabilistic sense, and thus requires no rigid full relation extraction that often mandates per-source wrappers. It can adapt to diverse information needs, without fixed pre-scribed schemas. The desired entities are only associated by ad-hoc queries at query time-Thus, users may ask #phone with "ibm thinkpad" or "bill gates", or they ask to pair #phone with, say, #email for "white house". Supporting such online matching and association is exactly the challenge (and usefulness) of entity search.</p><p>On the other hand, as query semantics, it is only best effort, returning the result in ranked, best-first manner. In essence, the system assumes "integration in a probabilistic sense": Given independent entities with probabilities, entity search finds matching instances that "most likely" form a desired tuple.</p><p>To understand the promise of the system and the practical issues, we decided to start with a quick "Version 0.1" pilot implementation, which is the focus of our demonstration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE PILOT SYSTEM</head><p>To realize our proposal in Section 3, we implemented a "pilot" system for empirical study.</p><p>First, as we abstracted in the Entity Search problem, we need to come up with specific matching patterns α as well as scoring measure β. Figures <ref type="figure">3</ref> and<ref type="figure">4</ref> summarizes our currently implemented α and β. To specify a tuple function operator, the user or the application will choose the exact measures to use.</p><p>The α Measure: An α measure qualifies if match of occurrences of entities match the desired tuple, in the matching pattern α(x). In principle, any Web presentation features can be incorporated for pattern expression, e.g., linguistic, proximity, or visual features. In this work, we treat webpages as linear documents. Consequently, we implemented several simple x matches a phrase as sepcified phrase(x) own(x), and the terms are nearest neighbors as above nnown(x) uwn(x), and the terms are nearest neighbors: i.e., the sum of the distances from the left most term to all others is the smallest, among other choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>nnuwn(x)</head><p>all terms in x occur in window of size n, in the order own(x) all terms in x occur in window of size n, any order uwn(x) qualification rule (x) The β Measures: Once instances of entities are matched, they form tuples. A β measure quantifies how promising a tuple e1,. . .,em is. A tuple e1,. . .,em may appear in the corpus many times-let [ẽ1, . . ., ẽm] be one such occurrence, i.e., [ẽ1, . . ., ẽm] ∈ α(x). We note that a scoring function β for determining the tuple score, i.e., β(e1,. . .,em), can build upon the following quantities:</p><p>1. Frequency of the tuple: How many times has e1,. . .,em occurred? That is, how many [ẽ1, . . ., ẽm] are matched? 2. Strength of each occurrence: How well does each [ẽ1, . . ., ẽm] match the pattern? 3. Frequency of individual entity instance: How many times has ei appeared in the corpus? A tuple may be frequent simply because its entity instances are common. 4. Uncertainty of entity instance: What is the probability of instance ei being of the entity type Ei.</p><p>Our initial implementation thus tried, for experimentation, to use all four in various ways, as Figure <ref type="figure">4</ref> summarizes a few representative ones: tf : tuple frequency (using 1); dtf : distance weighted tuple frequency (using 1, 3); mi: mutual information (using 1, 2); t-score(using 1, 3); conf (using 4).</p><p>While these measures are validated in our experiments to be useful, we believe a more systematic study of the scoring measure is still needed. Coming up with more effective scoring measures is itself an interesting research problem. We plan to work towards this direction in the future.</p><p>Next, we describe the key system components to accomplish the goal of entity search, towards agile best-effort information integration. The overall system architecture is illustrated in Figure <ref type="figure" target="#fig_3">5</ref>. We now zoom into each specific system component. Data Collector: Our data webpages could come from both the surface Web and the deep Web. To get webpages from the surface Web, our Data Collector works like a crawler, getting webpages related with a specific topic/application. Our current implementation obtains webpages from the Stanford Web-Base Project 2 . To get webpages from the deep Web sources, 2 http://www-diglib.stanford.edu/∼testbed/doc2/WebBase/ Entity Extractor: In order to discover semantics of the domain, our Entity Extractor extracts domain entities, the key components of the domain, independently. Each entities is extracted using an entity model, which describes how to identify instances of an entity in the webpages.</p><p>Query Engine This component essentially supports online querying by performing pattern matching and tuple scoring, as we abstracted in Section 3 and described in the beginning of this Section. As we produce tuples in the results, naturally many relational operations could be performed, which corresponds to the Relational Operations sub component in Figure <ref type="figure" target="#fig_3">5</ref>. Our Query Engine is morphed using the Lemur Toolkit (version 2.2), an information retrieval engine <ref type="bibr" target="#b1">[1]</ref>. This IR engine facilitates easy storage of the extracted entities in the form of ordered lists based on document ID, much like storing inverted indices for keywords. It also enables the construction in a natural way as sort-merge-join based on document ID is one of the most common operations in answering IR queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DEMONSTRATION</head><p>In this demonstration, we specifically show the online component of our system, which is our query engine. The current sever is running on a Pentium-4 2.6GHz PC with 1GB memory. This section will first discuss the interface of our query engine. Then two demonstration scenarios are presented to show the effectiveness of our system. Finally, the demonstration plan is described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Interface</head><p>We show our query interface for the query engine of Entity-Search in Figure <ref type="figure" target="#fig_4">6</ref>.</p><p>The first three input boxes in Figure <ref type="figure" target="#fig_4">6</ref> directly refers to the operators explained in Section 4.</p><p>Matching Pattern specifies a pattern α(x), in which x is a list of terms to be joined, as either entities Ei or literal keywords Kj, and α is a pattern measure, shown in Figure <ref type="figure">3</ref>, specifying how the terms are connected into a pattern.  Scoring Measure specifies which β scoring measure, among the ones we support in Figure <ref type="figure">4</ref>, will be used to score the matched tuples.</p><p>Entity Filter enables specifying conditions on each entity. In principle, any filter operation could be applied to an entity. Our system currently supports two operators: equalto (string value of the instance matches exactly with the specified keywords) and contains (string value of the instance contains the specified keywords).</p><p>In addition, We provide three auxiliary features to improve the effectiveness of the engine. These features could be regarded as a light-weight Relational Operation layer, which performs minimal but useful relational operations upon the output relation. We discuss these three features one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus Restriction:</head><p>The Corpus Restriction operator is designed to facilitate application's control over the corpus. We support Corpus Restriction by URL domains as regular patterns. For instance, we could set this field to *.amzon.com, which matches the sub-corpus of pages at the domains with that pattern (e.g., book.amzon.com).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pages Per Answer:</head><p>This feature requests the number of Web pages to return as support pages or "evidences" for each tuple returned by the query engine because results are integrated from multiple Web pages across the corpus.</p><p>Order By specifies the order of listing results (e.g., by #research alphabetically)-much like the same clause in SQL. By default it will rank the results according to their ranking score calculated by the β Measure.</p><p>Application issues queries by filling in the interface (Matching Pattern, Scoring Measure and Entity Filter), as well as the other three auxiliary operators and then clicking the "Submit Query" button. Figure <ref type="figure" target="#fig_5">7</ref> shows a sample system output. As you can see from the figure, it is in the form of a relation. Each tuple has a score associated with it. The URL below each tuple shows the supporting pages where this tuple is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Demonstration Scenarios</head><p>In this subsection, we use two concrete scenarios on real data to demonstrate the practical usage of our system. The two scenarios we selected are very different in nature. The first scenario, regarding education domain regarding midwest CS departments, focuses on the surface Web whose pages are mostly unstructured. In contrast, the second scenario, using the book domain, focuses on the deep Web. Result pages collected from the deep Web tend to be more structured. We show that our system works well in both scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario 1: Midwest Computer Science Domain</head><p>Currently, we have collected pages regarding CS department in six midwest universities (IIT, Illinois, Indiana, Michigan, Purdue and Wisconsin) from the WebBase project. The Domain Extractor extracts the following attributes: professor, research, university, email and phone. Three example queries:</p><p>C1: emails of professors across universities C2: research areas of professors across universities C3: professors conducting DB related research across universities Above we have shown three example queries that could be asked in this domain. Detailed query operators for each query is excluded. In Query C1, the application is interested in integrating all the emails of professors across universities into one table. Figure <ref type="figure" target="#fig_5">7</ref> shows a snippet of the result, which is good. We manually checked the result returned by our system for Query C1, over 90% of professor's emails are included in the output relation. And if for each unique professor, we only remain the tuple with the highest score in the relation, we can achieve precision over 85%. Please refer to the results in our online demo for more details. Similar results are observed from other example queries, such as Query C2 and C3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario 2: Shakespeare's Book Domain</head><p>This scenario focuses on the deep Web. We issued a query by filling author attribute with "Shakespeare" to the three major online book stores: www.amazon.com, www.barnesandnoble.com, www.buy.com. We then manually collect webpages containing the top 100 results from each site. The Domain Extractor extracts the following attributes: title, author, image, price, date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Three example queries:</head><p>B1: title and price of books B2: images of books with title containing "Hamlet" B3: title of books that are in stock Above we have shown three example queries that could be asked in this domain, excluding detailed query operators. Query B1 is a very common integration scenario where the application wants to find all the books and their prices across multiple websites. More interestingly, Query B2 asks for images of titles with keyword "Hamlet" in it. As you can see from the top results shown in Figure <ref type="figure" target="#fig_6">8</ref>, the images indeed are all cover images for books with keyword "Hamlet" in their titles. Query B3 could be issued using the title attribute together with keywords "in stock" as application finds vendors normally put keywords "in stock" to indicate the books currently available for purchase. Similarly, we can query for books that are "out of stock", "on order", etc.</p><p>Due to the lack of space, we are only able to briefly show very limited example queries and results in our two scenarios. To experience more about the above two scenarios and have a real feeling of the results, we invite users to the following online demo site 3 . 3 http://parrot.cs.uiuc.edu/entitysearch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Demo Plan</head><p>Users can follow the link from our demo site to experience our two demo scenarios. For each scenario, we have listed a set of example queries, including all the example queries shown in Section 5.2. By clicking on an example query, the query operators of the chosen query will be automatically filled into the input boxes in our query interface. Alternatively, users are welcome to modify the query operators of the example queries or come up with their own queries. Query results are normally returned within seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we proposed the concept of Entity Search for supporting agile best-effort information integration over the Web. While we observe entity search, as a mid-way marriage of search and integration, is meaningful and useful to access the data-rich Web, there are several open research issues towards its full realization: First, as the core, how to rank each tuple, so that the best matching surfaces to the top? Second, to provide efficient online search, can we generalize the current page-based search engine into an entity search engine? Finally, as entity search returns a ranked list of tuples (e.g., Figure <ref type="figure" target="#fig_1">2</ref>), how can we integrate the "derived relations" with relational SQL-based querying-e.g., joining #company, #phone with #company, #email ? Or, filtering only those companies with ".com"? We are continuing to investigate these challenging and interesting problems as our future research agenda.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The data-rich Web.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Query Results of Q1 and Q3.systems. While the prevalence of data on the Web presents novel opportunities for integration, this "Web-scale" scenario contrasts traditional settings and defeats current techniques-We must rethink not only new techniques but also realistic objectives. To bring myriads heterogeneous sources to meet ad-hoc users, as the scalability and flexibility mandate, can we develop agile integration without rigid schemas, even with "best effort" semantics?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Pattern measures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: System Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: System Interface.</figDesc><graphic coords="5,335.26,158.90,188.38,66.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sample Output of Query C1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Sample Output of Query B2.</figDesc><graphic coords="6,328.67,68.89,201.70,104.40" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This is perhaps why today's Web integration systems work by assuming a small set of pre-configured sources, by user submission (e.g., Google Base), or by relying on a centralized database, e.g., Sabre<ref type="bibr" target="#b3">[3]</ref> for airfares and MLS<ref type="bibr" target="#b2">[2]</ref> for real estate</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www-2.cs.cmu.edu/∼lemur" />
		<title level="m">Lemur toolkit for language modeling and information retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/MultipleListingService" />
		<title level="m">Multiple listing service</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://www.sabre.com" />
		<title level="m">Sabre holdings corporation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward large scale integration: Building a metaquerier over databases on the web</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings CIDR</title>
		<meeting>CIDR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Some practical observations on integration of web information</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WebDB (Informal Proceedings)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Crawling the hidden web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data integration: Where does the time go?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Seligman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Light-weight domain-based form assistant: Querying web databases on the fly</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VLDB 2005</title>
		<meeting>VLDB 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
