<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PMC-LLaMA: Further Finetuning LLaMA on Medical Papers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-27">27 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>ya_zhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
							<email>wangyanfeng@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PMC-LLaMA: Further Finetuning LLaMA on Medical Papers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-27">27 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2304.14454v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding in various domains. These models can usually behave well on daily dialog, or question answering scenarios, however, in areas that value precision, for example, in medical applications, they often exhibit unsatisfactory performance due to a lack of domain-specific knowledge. In this report, we introduce PMC-LLaMA, an open-source language model that is acquired by fine-tuning an open-source language model on a total of 4.8 million biomedical academic papers for further injecting medical knowledge, enhancing its capability in medical domain. Our preliminary evaluations are conducted on three biomedical QA datasets, including PubMedQA, MedMCQA, and USMLE, showing that the our model after finetuning, i.e., PMC-LLaMA, demonstrates better understanding of biomedical domain-specific concepts, thus achieving high performance on QA benchmarks. The model and codes, along with an online demo, are publicly available 1,2 .</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rapid advancement of large language models (LLMs), for example, OpenAI's Chat-GPT <ref type="bibr">[cha, 2023]</ref> and <ref type="bibr">GPT-4 [OpenAI, 2023]</ref> has truly revolutionized artificial intelligence in various domain, for example, natural language processing, computer vision, and biomedical domain <ref type="bibr" target="#b9">[Moor et al., 2023</ref><ref type="bibr" target="#b10">, Nori et al., 2023</ref><ref type="bibr" target="#b14">, Singhal et al., 2022]</ref>, unfortunately, the training details and model architectures for ChatGPT, and its variants, still remain unclear. In contrast, as an open-source foundational language model, LLaMA <ref type="bibr" target="#b16">[Touvron et al., 2023]</ref> often performs poorly on applications that require heavy domain knowledge, which we conjecture is due to inadequate domain-specific data at the model pre-training stage.</p><p>In the recent literature, there has been a growing interest in leveraging open-source LLMs and adapting them towards specific applications or domains. For instance, Alpaca <ref type="bibr">[Taori et al., 2023]</ref> and Vicuna <ref type="bibr" target="#b1">[Chiang et al., 2023]</ref> have concentrated on enhancing the model's interactive capabilities using machine-generated instruction-following samples. In contrast to their goal on daily dialogue, in this report, our focus is to steer the foundational language model towards medical-specific corpus, by further injecting domain knowledge into one pre-trained LLaMA. seen in Figure <ref type="figure" target="#fig_0">1</ref>. With preliminary evaluation, PMC-LLaMA has demonstrated superior performance on various medical QA datasets, including PubMedQA, MedMCQA, and USMLE, for end-to-end full fine-tuning, parameter-efficient fine-tuning and data-efficient fine-tuning. We believe that such a medical-specific foundational language model would be more suitable for specialization in various healthcare sub-tasks, such as medical dialogue or consultation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specifically</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiment</head><p>In this section, we begin by outlining the fine-tuning procedure (Sec. 2.1), including the dataset used and specific training details; Subsequently, in Sec. 2.2, we provide a detailed description of the evaluation benchmark, composing three QA datasets, namely PubMedQA, MedMCQA, and UMLSE. Lastly, in Sec. 2.3, we present three evaluation scenarios: full fine-tuning, parameter-efficient fine-tuning, and data-efficient fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fine-tuning Procedure</head><p>Dataset. We start with the S2ORC <ref type="bibr" target="#b6">[Lo et al., 2020]</ref> Datasets with 81.1M English-language academic papers, and filter them with PubMed Central (PMC)-id. As a result, there are around 4.9M papers left, that are highly related to medical knowledge totaling over 75B tokens.</p><p>Fine-tuning detail. We fine-tune the LLaMA-7B model on these open-accessed PMC papers, by optimising an autoregressive generation objective, as introduced in GPT2 <ref type="bibr" target="#b13">[Radford et al., 2019]</ref>. Specifically, during our finetuning, the max context length is set as 512, with a batch size to be 128, and the model is trained with AdamW optimizer <ref type="bibr" target="#b7">[Loshchilov and Hutter, 2017]</ref> with learning rate 2e-5. To accelerate the training, we adopt the Fully Sharded Data Parallel (FSDP) acceleration strategy and bf16 (Brain Floating Point) data format. The model is trained for 5 epochs with 8 A100 GPUs in around 7 days. Note that, in each epoch, we randomly sample 512 continuous tokens per paper for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Benchmark Descriptions</head><p>In this section, we detail the three QA benchmarks for evaluation in the following.</p><p>PubMedQA <ref type="bibr" target="#b5">[Jin et al., 2019]</ref> contains questions on biomedical research, the model is provided with paper abstracts from PubMed, and is required to complete multiple-choice questions. It is split into three subsets: labeled (PQA-L), unlabeled (PQA-U), and artificially generated (PQA-A). We use PQA-A for training, which contains 211,269 Question-Answer pairs, and PQA-L for testing, which contains 1000 pairs.</p><p>MedMCQA <ref type="bibr">[Pal et al., 2022]</ref> is a dataset of multiple choice questions sourced from mock exams and past exams of two Indian medical school entrance exams called AIIMS and NEET-PG <ref type="bibr">[Pal et al., 2022]</ref>. The train set contains 182,822 questions, and the test set contains 4183 questions. Each question has 4 choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USMLE [Jin et al., 2021</head><p>] is a dataset of multiple choice questions (4 choices per question), based on the United States Medical License Exams. The dataset is collected from the professional medical board exams, covering three languages: English, simplified Chinese, and traditional Chinese, and contains 12,724, 34,251, and 14,123 questions for these three languages, respectively. We only use the English parts and split it into 10,178 questions for training, 1273 for validation, and 1273 for testing, following the official data spits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Scenario</head><p>In this section, we evaluate PMC-LLaMA by fine-tuning on the above-mentioned corresponding medical QA datasets, under three scenarios, namely full fine-tuning, parameter-efficient fine-tuning, and data-efficient fine-tuning as detailed in the following.</p><p>Full fine-tuning. We fine-tune PMC-LLAMA on the combination of the training set from PubMedQA and MedMCQA, for evaluation, we treat the test set from PubMedQA and MedMCQA as in-domain (ID) evaluation, while treating the USMLE test set as out-of-domain (OOD) evaluation, similarly to LMFlow <ref type="bibr">[Diao et al., 2023]</ref>.</p><p>Parameter-efficient fine-tuning <ref type="bibr">[Mangrulkar et al., 2022]</ref> enables efficient adaptation of pre-trained LLMs to various downstream applications without fine-tuning all parameters, greatly reducing the time and computation cost. Here we use the most widely used method, called PEFT Low-Rank Adaptation (LoRA) <ref type="bibr" target="#b3">[Hu et al., 2021]</ref>. The hyper-parameter for LoRA is set by default provided in the python package PEFT <ref type="bibr">[Mangrulkar et al., 2022]</ref>. The data settings for training and testing remain the same as full fine-tuning.</p><p>Data-efficient fine-tuning. In addition to measuring the number of training parameters, we also conduct experiments on the data side, i.e., data-efficient fine-tuning. Specifically, we conduct experiments on USMLE dataset, i.e., simply train and test on its own splits, resembling an USMLE ID evaluation. However, as the training set in USMLE is much smaller compared with that of combining PubMedQA and MedMCQA (10k vs. 400k), less medical knowledge can be learned from the downstream training data, it turns out to be rather more challenging than the previous OOD test, as will be shown by our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details.</head><p>For each downstream task, we fine-tune the network using the same setting as training on PMC papers, specifically, we set the max context length as 512 and fine-tune the models with AdamW optimizer with learning rate 2e-5, the batch size is set to be 128, and the model is trained for 3 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In Fig. <ref type="figure" target="#fig_1">2</ref> we show the fine-tuning training curve under different settings and compare the convergence speed. The final evaluation results are shown in Tab.1, generally speaking, in almost all medical QA benchmarks, our proposed PMC-LLaMA converges faster and exhibits better performance than the original LLaMA <ref type="bibr" target="#b16">[Touvron et al., 2023]</ref>.</p><p>Model convergence. In Fig. <ref type="figure" target="#fig_1">2</ref>, we name each training curve using the convention {Model Name}-{Training Setting}-{Dataset}", for example, {LLaMA-7B}-{Full-Finetune}-{PM&amp;MedMC}" refers to the full fine-tuning of the LLaMA-7B model on the PubMedQA and MedMCQA training set. As illustrated in the figure, generally, PMC-LLaMA converges more quickly and achieves lower loss compared to the original LLaMA, especially when the training dataset is larger (with PM&amp;MedMC) and has more trainable parameters (Full-Finetune). This suggests that PMC-LLaMA offers a better initialization for medical tasks.</p><p>Full fine-tuning results. As shown in Tab. 1, PMC-LLaMA-7B Full exceeds the LLaMA-7B Full in two of three test sets, improving the performance from 44.55% to 44.70% and 35.66% to 40.61% on USMLE for OOD and ID settings respectively, and 48.15% to 50.74% on MedMCQA.  ACC in percentages is reported in the table. Note that, the manual and zero-shot results with * are referred from LMFlow <ref type="bibr">[Diao et al., 2023]</ref>.</p><p>Parameter-efficient fine-tuning results. As shown in Tab. 1, PMC-LLaMA-7B PEFT demonstrates superior performance than LLaMA-7B PEFT , particularly on the in-domain datasets, 1.22% improvement on USMLE, 1.96% improvement on MedMCQA and 2.42% on PubMedQA. These results demonstrate that the original LLaMA only provides suboptimal embedding spaces for Medical QA and further fine-tuning on the biomedical corpus is beneficial for model domain adaptation.</p><p>Data-efficient fine-tuning results. We refer to USMLE ID test results as the Dataefficient fine-tuning comparison. As shown in Tab. 1, whatever the training setting is "Full fine-tuning" or "PEFT", PMC-LLaMA-7B can both achieve better results on the USMLE ID test. Specifically, we improve the ACC scores from 35.66% to 40.61% under full fine-tuning and from 27.34% to 28.52% under PEFT.</p><p>Compare to ChatGPT and its variants. As is evidently shown in Tab.1, InstructGPT and chatGPT achieve fantastic results even under zero-shot evaluation, however, we argue that, as they are not open-sourced, and do not reveal any training details, there is no guarantee on whether the evaluation data has been exploited for training their model, hence their results can only be used as a reference here, and should not be taken for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Zero-shot Case Study</head><p>In this section, we show zero-shot generation cases in Fig. <ref type="figure" target="#fig_2">3</ref> with the judgement from GPT-4, and present the differences between original LLaMA and PMC-LLaMA. The GPT-4 Judgement is inspired by <ref type="bibr" target="#b1">[Chiang et al., 2023]</ref>, and uses the following prompt:</p><p>Considering the two outputs, which one do you think is more related to the input? Input: . . . PMC-LLaMA Output: . . . Original LLaMA Output: . . . Generally speaking, we find PMC-LLaMA has more expertise in medical knowledge and responds better to some cutting-edge professional medical concepts.</p><p>Case 1: COPD. According to World Health Organization (WHO<ref type="foot" target="#foot_2">1</ref> ), "Chronic obstructive pulmonary disease (COPD) is a common and treatable disease characterized by progressive airflow limitation and tissue destruction and is the third leading cause of death worldwide", which is consistent with the output of PMC-LLaMA, in contrast, although LLaMA has pointed out the fact that COPD can be identified by spirometry results, quickly lose the attention on COPD and generate irrelevant sentences. In GPT-4 Judgement, it also points out that the original LLaMA "goes on to discuss unrelated topics".</p><p>Case 2: Robotic Cardiac Surgery. According to Johns Hopkins Medicine<ref type="foot" target="#foot_3">2</ref> , Robotic cardiac surgery involves performing heart surgery through very small incisions in the chest.</p><p>Utilizing miniature instruments and robot-assisted tools, surgeons can conduct heart surgery in a manner that is significantly less invasive than traditional open-heart surgery". The assertion made by PMC-LLaMA closely aligns with this statement, while the output from the original LLaMA appears to be unclear. Its phrase through the placement of surgical robotic tools" is inaccurate and should instead refer to "robot-assisted tools". Although GPT-4 considered the original LLaMA output to be satisfactory, it failed to detect the inaccuracies in LLaMA's descriptions, focusing merely on their relevance.</p><p>Case 3 &amp; 4: Diabetes and Pneumonia. In cases 3 and 4, PMC-LLaMA consistently provides more context about the input, suggesting that PMC-LLaMA possesses a deeper understanding of medical background knowledge. In contrast, the original LLaMA tends to introduce irrelevant content in its output. As GPT-4 notes, the rest of the text is less clear and informative compared to the PMC-LLaMA output "and the response goes on to discuss PCT concentrations, which are not directly related to the input statement".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In conclusion, in this report, we investigate the existing open-source language model, namely, LLaMA, on medical applications, showing that the model performs unsatisfactorily on QA tasks. To inject domain knowledge into the pre-trained model, We conduct a preliminary investigation by fine-tuning LLaMA with 4.8 million medical papers, resulting in a new foundation model for the medical domain PMC-LLaMA. Experimental evaluation demonstrates that PMC-LLaMA is more suitable for medical tasks compared with LLaMA and after instruction tuning it can get better performance compared to other models instruction tuned from LLaMA in the medical domain.</p><p>One limitation of PMC-LLaMA is that we have only trained 5 epochs in the current version and have not seen every token in the 4.8 million papers. In future work, we will continuously train PMC-LLaMA and update our base model on the hugging face page and gradually train PMC-LLaMA models with more parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PMC-LLaMA Training Pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The training curve compared with LLaMA under two settings with different training data. The curve is smoothed with 0.6 Exponential Moving Average. Depending on the amount of training data, the curves have different ending steps, thus showing seesaw shapes with different periods.</figDesc><graphic url="image-10.png" coords="4,108.00,72.00,395.99,161.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Zero-shot evalution on medical-related sentence completion, we compare the results between PMC-LLaMA and original LLaMA-7B with the judgement provided by GPT-4.</figDesc><graphic url="image-11.png" coords="5,108.00,214.81,396.01,476.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between LLaMA-7B and PMC-LLaMA-7B under different settings.</figDesc><table><row><cell>Method</cell><cell>Setting</cell><cell cols="3">USMLE(OOD/ID) MedMCQA(ID) PubMedQA(ID)</cell></row><row><cell>Human (pass) Human (expert)</cell><cell>Manual *</cell><cell>50.0 87.0</cell><cell>-90.0</cell><cell>60.0 78.0</cell></row><row><cell>InstructGPT-175B</cell><cell></cell><cell>46.0</cell><cell>44.0</cell><cell>73.2</cell></row><row><cell>ChatGPT LLaMA-7B</cell><cell>Zero-shot *</cell><cell>57.0 27.1</cell><cell>44.7 24.3</cell><cell>63.9 5.2</cell></row><row><cell>LLaMA-33B</cell><cell></cell><cell>43.4</cell><cell>30.3</cell><cell>1.8</cell></row><row><cell>LLaMA-7B Full PMC-LLaMA-7B Full</cell><cell>Full fine-tuning</cell><cell>44.55/35.66 44.70/40.61</cell><cell>48.15 50.54</cell><cell>73.41 69.53</cell></row><row><cell>LLaMA-7B PEFT PMC-LLaMA-7B PEFT</cell><cell>PEFT</cell><cell>29.38/27.34 30.64/28.52</cell><cell>32.37 34.33</cell><cell>65.81 68.23</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Huggingface Page: https://huggingface.co/chaoyi-wu/PMC_LLAMA_7B</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Github Page: https://github.com/chaoyi-wu/PMC-LLaMA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>WHO https://www.who.int</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>Johns Hopkins Medicine: https://www.hopkinsmedicine.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://openai.com/blog/chatgpt/,2023.1" />
	</analytic>
	<monogr>
		<title level="j">Openai. introducing chatgpt</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lmflow: An extensible toolkit for finetuning and inference of large foundation models</title>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanze</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashun</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://optimalscale.github.io/LMFlow/,2023.3" />
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">6421</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Pubmedqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06146</idno>
		<title level="m">A dataset for biomedical research question answering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.447</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.447.2" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Peft: State-of-the-art parameter-efficient fine-tuning methods</title>
		<author>
			<persName><forename type="first">Sourab</forename><surname>Mangrulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<ptr target="https://github.com/huggingface/peft,2022.3" />
	</analytic>
	<monogr>
		<title level="m">Lysandre Debut, Younes Belkada, and Sayak Paul</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Foundation models for generalist medical artificial intelligence</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oishi</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Shakeri Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harlan</forename><forename type="middle">M</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Krumholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Topol</surname></persName>
		</author>
		<author>
			<persName><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">616</biblScope>
			<biblScope unit="issue">7956</biblScope>
			<biblScope unit="page" from="259" to="265" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Capabilities of gpt-4 on medical challenge problems</title>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">Mayer</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13375</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logesh</forename><surname>Kumar Umapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malaikannan</forename><surname>Sankarasubbu</surname></persName>
		</author>
		<idno>PMLR, 2022. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Health, Inference, and Learning</title>
		<imprint>
			<biblScope unit="page" from="248" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.13138</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca,2023.1" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
