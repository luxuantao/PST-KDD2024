<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine Learning on Graphs: A Model and Comprehensive Taxonomy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-11">May 11, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
							<email>chami@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Computational and Mathematical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Information Sciences Institute ‡ ‡ Stanford University</orgName>
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Computational and Mathematical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Information Sciences Institute ‡ ‡ Stanford University</orgName>
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@acm.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Computational and Mathematical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Information Sciences Institute ‡ ‡ Stanford University</orgName>
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Computational and Mathematical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Information Sciences Institute ‡ ‡ Stanford University</orgName>
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
							<email>kpmurphy@google.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Computational and Mathematical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Information Sciences Institute ‡ ‡ Stanford University</orgName>
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine Learning on Graphs: A Model and Comprehensive Taxonomy</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-11">May 11, 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2005.03675v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning representations for complex structured data is a challenging task. In the last decade, many successful models have been developed for certain kinds of structured data, including data defined on a discretized Euclidean domain. For instance, sequential data, such as text or videos, can be modelled via recurrent neural networks, which can capture sequential information, yielding efficient representations as measured on machine translation and speech recognition tasks. Another example is convolutional neural networks (CNNs), which parameterize neural networks according to structural priors such as shift-invariance, and have achieved unprecedented performance in pattern recognition tasks such as image classification or speech recognition. These major successes have been restricted to particular types of data that have a simple relational structure (e.g. sequential data, or data following regular patterns).</p><p>In many settings, data is not nearly as regular: complex relational structures commonly arise, and extracting information from that structure is key to understanding how objects interact with each other. Graphs are a universal data structures that can represent complex relational data (composed of nodes and edges), and appear in multiple domains such as social networks, computational chemistry <ref type="bibr" target="#b40">[41]</ref>, biology <ref type="bibr" target="#b104">[105]</ref>, recommendation systems <ref type="bibr" target="#b63">[64]</ref>, semi-supervised learning <ref type="bibr" target="#b38">[39]</ref>, and others. Generalizing CNNs to graphs is not trivial For graph-structured data, it is challenging to define networks with strong structural priors, as structures can be arbitrary, and can vary significantly across different  graphs and even different nodes within the same graph. In particular, operations like convolutions cannot be directly applied on irregular graph domains. For instance in images, each pixel has the same neighborhood structure, allowing to apply the same filter weights at multiple locations in the image. However in graphs, one cant define an ordering of node since each node might have a different neighborhood structure (Figure <ref type="figure" target="#fig_1">1</ref>). Furthermore, Euclidean convolutions strongly rely on geometric priors (e.g. shift invariance) which don't generalize to non-Euclidean domains (e.g. translations might not even be defined on non-Euclidean domains).</p><p>These challenges led to the development of Geometric Deep Learning (GDL) research which aims at applying deep learning techniques to non-Euclidean data. In particular, given the widespread prevalence of graphs in realworld applications, there has been a surge of interest in applying machine learning methods to graph-structured data. Among these, Graph Representation Learning (GRL) methods aim at learning low-dimensional continuous vector representations for graph-structured data, also called embeddings.</p><p>Broadly speaking, GRL can be divided into two classes of learning problems, unsupervised and supervised (or semi-supervised) GRL. The first family aims at learning low-dimensional Euclidean representations that preserve the structure of an input graph. The second family also learns low-dimensional Euclidean representations but for a specific downstream prediction task such as node or graph classification. Different from the unsupervised setting where inputs are usually graph structures, inputs in supervised settings are usually composed of different signals defined on graphs, commonly known as node features. Additionally, the underlying discrete graph domain can be fixed, which is the transductive learning setting (e.g. predicting user properties in a large social network), but can also vary in the inductive learning setting (e.g. predicting molecules attribute where each molecule is a graph). Finally, note that while most supervised and unsupervised methods learn representations in Euclidean vector spaces, there recently has been interest for non-Euclidean representation learning, which aims at learning non-Euclidean embedding spaces such as hyperbolic or spherical spaces. The main motivations for this body of work is to use a continuous embedding space that resembles the underlying discrete structure of the input data it tries to embed (e.g. the hyperbolic space is a continuous version of trees <ref type="bibr" target="#b98">[99]</ref>).</p><p>Given the impressive pace at which the field of graph representation learning is growing, we believe it is important to summarize and describe all methods in one unified and comprehensible framework. The goal of this survey is to provide a unified view of representation learning methods for graph-structured data, to better understand the different ways to leverage graph structure in deep learning models.</p><p>A number of graph representation learning surveys exist. First, there exist several surveys that cover shallow network embedding and auto-encoding techniques and we refer to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b121">122]</ref> for a detailed overview of these methods. Second, Bronstein et al. <ref type="bibr" target="#b14">[15]</ref> also gives an extensive overview of deep learning models for non-Euclidean data such as graphs or manifolds. Third, there have been several recent surveys <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b125">126]</ref> covering methods applying deep learning to graphs, including graph neural networks. Most of these surveys focus on a specific sub-field of graph representation learning and do not draw connections between each sub-field.</p><p>In this work, we extend the encoder-decoder framework proposed by Hamilton et al. <ref type="bibr" target="#b50">[51]</ref> and introduce a general framework, the Graph Encoder Decoder Model (GRAPHEDM), which allows us to group existing work into four major categories: (i) shallow embedding methods, (ii) auto-encoding method, (iii) graph regularization methods, and (iv) graph neural networks (GNNs). Additionally, we introduce a Graph Convolution Framework (GCF), specifically designed to describe convolution-based GNNs, which have achieved state-of-the art performance in a broad range of applications. This allows us to analyze and compare a variety of GNNs, ranging in construction from methods operating in the Graph Fourier<ref type="foot" target="#foot_0">1</ref> domain to methods applying self-attention as a neighborhood aggregation function <ref type="bibr" target="#b110">[111]</ref>. We hope that this unified formalization of recent work would help the reader gain insights into the various learning methods on graphs to reason about similarities, differences, and point out potential extensions and limitations. That said, our contribution with regards to previous surveys are threefold:</p><p>• We introduce a general framework, GRAPHEDM, to describe a broad range of supervised and unsupervised methods that operate on graph-structured data, namely shallow embedding methods, graph regularization methods, graph auto-encoding methods and graph neural networks. • Our survey is the first attempt to unify and view these different lines of work from the same perspective, and we provide a general taxonomy (Figure <ref type="figure">3</ref>) to understand differences and similarities between these methods. In particular, this taxonomy encapsulates over thirty existing GRL methods. Describing these methods within a comprehensive taxonomy gives insight to exactly how these methods differ. • We release an open-source library for GRL which includes state-of-the-art GRL methods and important graph applications, including node classification and link prediction. Our implementation is publicly available at https://github.com/google/gcnn-survey-paper.</p><p>Organization of the survey We first review basic graph definitions and clearly state the problem setting for GRL in Section 2. In particular, we define and discuss the differences between important concepts in GRL, including the role of node features in GRL and how they relate to supervised GRL (Section 2.2.1), the distinctions between inductive and transductive learning (Section 2.2.2) and the differences between supervised and unsupervised embeddings (Section 2.2.3). We then introduce GRAPHEDM (Section 3) a general framework to describe both supervised and unsupervised GRL methods, with or without the presence of node features, which can be applied in both inductive and transductive learning settings. Based on GRAPHEDM, we introduce a general taxonomy of GRL methods (Figure <ref type="figure">3</ref>) which encapsulates over thirty recent GRL models, and we describe both unsupervised (Section 4) and supervised (Section 5) methods using this taxonomy. Finally, we survey graph applications in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Here we introduce the notation used throughout this article, and the generalized network embedding problem which graph representation learning methods aim to solve.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definitions</head><formula xml:id="formula_0">(v i , v j ) with v i , v j ∈ V . A graph is weighted if there exist a weight function: w : (v i , v j ) → w ij that assigns weight w ij to edge connecting nodes v i , v j ∈ V . Otherwise, we say that the graph is unweighted. A graph is undirected if (v i , v j ) ∈ E implies (v j , v i ) ∈ E, i.e.</formula><formula xml:id="formula_1">W = D −1/2 W D −1/2</formula><p>, where D is the degree matrix.</p><p>Definition 2.6. (Laplacian). The unnormalized Laplacian of an undirected graph is the</p><formula xml:id="formula_2">|V | × |V | matrix L = D − W . The symmetric normalized Laplacian is L = I − D −1/2 W D −1/2 . The random walk normalized Laplacian is the matrix L = I − D −1 W .</formula><p>The name random walk comes from the fact that D −1 W is a stochastic transition matrix that can be interpreted as the transition probability matrix of a random walk on the graph. Laplacian matrices can be used to find properties of graphs such as graph cuts, but are also very useful in graph embedding algorithms as they capture complex patterns in graphs.</p><p>Definition 2.7. (First order proximity). The first order proximity between two nodes v i and v j is a local similarity measure indicated by the edge weight w ij . In other words, the first-order proximity captures the strength of an edge between node v i and node v j (should it exist).</p><p>Definition 2.8. (Second-order proximity). The second order proximity between two nodes v i and v j is measures the similarity of their neighborhood structures. Two nodes in a network will have a high second-order proximity if they tend to share many neighbors.</p><p>Note that there exist higher-order measures of proximity between nodes such as Katz Index, Adamic Adar or Rooted PageRank <ref type="bibr" target="#b72">[73]</ref>. These notions of node proximity are particularly important in network embedding as many algorithms aim at preserving some order of node proximity in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The generalized network embedding problem</head><p>Network embedding is the task that aims at learning a mapping function from a discrete graph to a continuous domain. Formally, given a graph G = (V, E) with weighted adjacency matrix W ∈ R |V |×|V | , the goal is to learn lowdimensional vector representations {Z i } i∈V (embeddings) for nodes in the graph {v i } i∈V , such that graph properties (local and global) are preserved. For instance, if two nodes have similar connections in the original graph, they should be embedded close to one another in the learned vector representations. Let Z ∈ R |V |×d denote the node<ref type="foot" target="#foot_1">2</ref> embedding matrix. In practice, we often want low-dimensional embeddings (d |V |) for scalability purposes. That is, network embedding can be viewed as a dimensionality reduction technique for graph structured data, where the input data is defined on a non-Euclidean, high-dimensional, discrete domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Node features in network embedding</head><p>Definition 2.9. (Vertex and edge fields). A vertex field is a function defined on vertices f : V → R and similarly an edge field is a function defined on edges: F : E → R. Vertex fields and edge fields can be viewed as analogs of scalar fields and tensor fields on manifolds.</p><p>Graphs may have node attributes (e.g. gender or age in social networks; article contents for citation networks) which can be represented as multiple vertex fields, commonly referred to as node features. In this survey, we denote node features with X ∈ R |V |×d0 , where d 0 is the input feature dimension. Node features might provide useful information about a graph. Some network embedding algorithms leverage this information by learning mappings:</p><formula xml:id="formula_3">W, X → Z.</formula><p>In other scenarios, node features might be unavailable or not useful for a given task: network embedding can be featureless. That is, the goal is to learn graph representations via mappings:</p><formula xml:id="formula_4">W → Z.</formula><p>Note that depending on whether node features are used or not in the embedding algorithm, the learned representation could capture different aspects about the graph. If nodes features are being used, embeddings could capture both structural and semantic graph information. On the other hand, if node features are not being used, embeddings will only preserve structural information of the graph.</p><p>Finally, note that edge features are less common than node features in practice, but can also be used by embedding algorithms. For instance, edge features can be used as regularization for node embeddings <ref type="bibr" target="#b25">[26]</ref>, or to compute messages from neighbors as in message passing networks <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Transductive and inductive network embedding</head><p>Network embedding can be transductive or inductive. In transductive settings, the goal is to embed nodes given one fixed graph (often very large). These methods are used to infer information about existing nodes in the graph, that are seen during training (e.g. predicting the rest of node labels in a partially labelled graph). For instance, if a transductive method is used to embed the nodes of a social network, it can be used to suggest new edges (e.g. friendships) between the nodes of the graph.</p><p>Given new nodes, transductive methods have to update the model (e.g., re-train) to infer information about these nodes. On the other hand, in inductive settings, models are expected to generalize to new nodes, edges, or graphs that were not observed during training. Formally, given training graphs (G 1 , . . . , G k ), the goal is to learn a mapping to continuous representations that can generalize to unseen test graphs (G k+1 , . . . , G k+l ). For instance, inductive learning can be used to embed molecular graphs, each representing a molecule structure <ref type="bibr" target="#b40">[41]</ref>, generalizing to new graphs and showing error margins within chemical accuracy on many quantum properties. Embedding dynamic or temporally evolving graphs is also another example of an inductive graph embedding problem.</p><p>Finally, note that there is a strong connection between inductive graph embedding and node features (Section 2.2.1) as the latter are usually necessary for most inductive graph representation learning algorithms. More concretely, when node features are not available, the mapping from node to embeddings is usually an embedding look-up and therefore cannot be applied to unseen nodes or new graphs. On the other hand, when available, node features can be leveraged to learn embeddings with parametric mappings and instead of directly optimizing the embeddings, one can optimize the mappings parameters. The learned mapping can then be applied to any node (even those that were not present a training time). To our knowledge, learning embeddings in inductive settings without node features is still an open research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Unsupervised and supervised network embedding</head><p>Network embedding can be unsupervised in the sense that the only information available is the graph structure (and possibly node features) or supervised, if additional information such as node or graph labels is provided. In unsupervised network embedding, the goal is to learn embeddings that preserved the graph structure and this is usually achieved by optimizing some reconstruction loss, which measures how well the learned embeddings can approximate the original graph. In supervised network embedding, the goal is to learn embeddings for a specific purpose such as predicting node or graph attributes, and models are optimized for a specific task such as graph classification or node classification. We cover differences between supervised and unsupervised methods in more details in Section 3.</p><formula xml:id="formula_5">X W ENC(W, X; Θ E ) Z DEC(Z; Θ S ) DEC(Z; Θ D ) y S W Input Output L S SUP L G,REG y S</formula><p>Figure <ref type="figure">2</ref>: Illustration of the GRAPHEDM framework. Based on the supervision available, methods will use some or all of the branches. In particular, unsupervised methods do not leverage label decoding for training and only optimize the similarity decoder (lower branch). On the other hand, semi-supervised and supervised methods leverage the additional supervision to learn embeddings (upper branch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Taxonomy of Graph Embedding Models</head><p>We first describe our proposed framework, GRAPHEDM, a general framework for GRL (Section 3.1). In particular, GRAPHEDM is general enough that it can be used to succinctly describe over thirty GRL methods (both unsupervised and supervised). We use GRAPHEDM to introduce a comprehensive taxonomy in Sections 3.2 and 3.3, which summarizes exiting works with shared notations and simple block diagrams, making it easier to understand similarities and differences between GRL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The GRAPHEDM framework</head><p>The GRAPHEDM framework builds on top of the work of Hamilton et al. <ref type="bibr" target="#b50">[51]</ref>, which describes unsupervised network embedding methods from an encoder-decoder perspective. Different from the encoder-decoder framework in Hamilton et al., we provide a more general framework which additionally encapsulates supervised graph embedding methods, including ones utilizing the graph as a regularizer, and graph neural networks such as ones based on message passing <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b99">100]</ref> or graph convolution <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>Input Formally, the GRAPHEDM framework takes as input an undirected weighted graph G = (V, E), with adjacency matrix W ∈ R |V |×|V | , optional node features X ∈ R |V |×d0 . Additionally, in (semi-)supervised settings, we assume that we are given training target labels for nodes (denoted N ), edges (denoted E), and/or for the entire graph (denoted G). We denote the supervision signal as S ∈ {N, E, G}, as presented below.</p><p>Model The GRAPHEDM framework can be decomposed as follows:</p><formula xml:id="formula_6">• Graph encoder network ENC Θ E : R |V |×|V | × R |V |×d0 → R |V |×d</formula><p>, parameterized by Θ E , which combines the graph structure with node features to produce node embedding matrix Z ∈ R |V |×d as:</p><formula xml:id="formula_7">Z = ENC(W, X; Θ E ).</formula><p>As we shall see next, this node embedding matrix might capture different graph properties depending on the supervision used for training.</p><p>• Graph decoder network DEC Θ D : R |V |×d → R |V |×|V | , parameterized by Θ D , which uses the node embeddings Z to compute similarity scores for all node pairs, producing similarity matrix W ∈ R |V |×|V | as:</p><formula xml:id="formula_8">W = DEC(Z; Θ D ).</formula><p>• Classification network, used in supervised settings and parameterized by Θ S . Network outputs a distribution over the labels ŷS , using node embeddings, as:</p><formula xml:id="formula_9">y S = DEC(Z; Θ S ).</formula><p>Our framework (GRAPHEDM) is general. Specific choices of the aforementioned (encoder and decoder) networks allows GRAPHEDM to realize specific graph embedding methods. GRAPHEDM is illustrated in Figure <ref type="figure">2</ref>. Before presenting the taxonomy and showing realizations of various methods using our framework, we briefly discuss an application perspective.</p><p>Output The GRAPHEDM model can return a reconstructed graph similarity matrix Ŵ (often used to train unsupervised embedding algorithms), as well as a output labels y S for supervised applications. The label output space varies depending on the supervised application. ), then rather than naming the output of the decoder as y E , we instead follow the nomenclature and position link prediction as an unsupervised task (Section 4). Then in lieu of y E we utilize W , the output of the graph decoder network (which is learned to reconstruct target similarity matrix s(W )) to rank potential edges.</p><p>• Graph-level supervision, with y G ∈ Y. In the graph classification task (Section 6.2.2), the label decoder network converts node embeddings Z using input adjacency W , into graph labels, using graph pooling. More concretely, the graph pooling operation is similar to pooling in standard CNNs, where the goal is to downsample local feature representations to capture higher-level information. However, unlike images, graphs don't have a regular grid structure and it is hard to define a pooling pattern which could be applied to every node in the graph.</p><p>A possible way of doing so is via graph coarsening, which groups similar nodes into clusters to produce smaller graphs <ref type="bibr" target="#b31">[32]</ref>. There exist other pooling methods on graphs such as DiffPool <ref type="bibr" target="#b119">[120]</ref> or SortPooling <ref type="bibr" target="#b122">[123]</ref> which creates an ordering of nodes based on their structural roles in the graph. We do not cover the details of graph pooling operators and refer the reader to recent surveys <ref type="bibr" target="#b115">[116]</ref> for more details about graph pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Taxonomy of objective functions</head><p>We now focus our attention on the optimization of models that can be described in the GRAPHEDM framework by describing the loss functions used for training. Let Θ = {Θ E , Θ D , Θ S } denote all model parameters. GRAPHEDM models can be optimized using a combination of the following loss terms:</p><p>• Supervised loss term, L S SUP , which compares the predicted labels ŷS to the ground truth labels y S . This term depends on the task the model is being trained for. For instance, in semi-supervised node classification tasks (S = N ), the graph vertices are split into labelled and unlabelled nodes (V = V L ∪ V U ), and the supervised loss is computed for each labelled node in the graph:</p><formula xml:id="formula_10">L N SUP (y N , ŷN ; Θ) = i|vi∈V L (y N i , ŷN i ; Θ),</formula><p>where (•) is the loss function used for classification (e.g. cross-entropy). Similarly for graph classification tasks (S = G), the supervised loss is computed at the graph-level and can be summed across multiple training graphs:</p><formula xml:id="formula_11">L G SUP (y G , ŷG ; Θ) = (y G , ŷG ; Θ).</formula><p>• Graph regularization loss term, L G,REG , which leverages the graph structure to impose regularization constraints on the model parameters. This loss term measures the distance between the decoded similarity matrix W and a target similarity matrix s(W ), which might capture higher-order proximities than the adjacency matrix itself:</p><formula xml:id="formula_12">L G,REG (W, W ; Θ) = d 1 (s(W ), W ),<label>(1)</label></formula><p>where d 1 (•, •) is a distance or dissimilarity function. Examples for such regularization are constraining neighboring nodes to share similar embeddings, in terms of their distance in L2 norm. We will cover more examples of regularization functions in Sections 4 and 5. • Weight regularization loss, L REG , e.g. for representing prior, on trainable model parameters for reducing overfitting. The most common regularization is L2 regularization (assumes standard Gaussian prior):</p><formula xml:id="formula_13">L REG (Θ) = θ∈Θ ||θ|| 2 2 .</formula><p>Finally, models realizable by GRAPHEDM framework are trained by minimizing the total loss L defined as:</p><formula xml:id="formula_14">L = αL S SUP (y S , ŷS ; Θ) + βL G,REG (W, W ; Θ) + γL REG (Θ),<label>(2)</label></formula><p>where α, β and γ are hyper-parameters, that can be tuned or set to zero. Note that graph embedding methods can be trained in a supervised (α = 0) or unsupervised (α = 0) fashion. Supervised graph embedding approaches leverage an additional source of information to learn embeddings such as node or graph labels. On the other hand, unsupervised network embedding approaches rely on the graph structure only to learn node embeddings.</p><p>A common approach to solve supervised embedding problems is to first learn embeddings with an unsupervised method (Section 4) and then train a supervised model on the learned embeddings. However, as pointed by Weston et al. <ref type="bibr" target="#b114">[115]</ref> and others, using a two-step learning algorithm might lead to sub-optimal performances for the supervised task, and in general, supervised methods (Section 5) outperform two-step approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Taxonomy of encoders</head><p>Having introduced all the building blocks of the GRAPHEDM framework, we now introduce our graph embedding taxonomy. While most methods we describe next fall under the GRAPHEDM framework, they will significantly differ based on the encoder used to produce the node embeddings, and the loss function used to learn model parameters. We divide graph embedding models into four family of approaches:</p><p>• Shallow embedding methods, where the encoder function is a simple embedding lookup. That is, the parameters of the model Θ E are directly used as node embeddings:</p><formula xml:id="formula_15">Z = ENC(Θ E ) = Θ E ∈ R |V |×d .</formula><p>• Graph regularization methods, where the encoder network only uses node features X as input:</p><formula xml:id="formula_16">Z = ENC(X; Θ E ).</formula><p>As its name suggests, graph regularization methods leverage the graph structure through the graph regularization loss term in Equation <ref type="formula" target="#formula_14">2</ref>(β = 0) to regularize node embeddings.</p><p>• Graph auto-encoding methods, where the encoder is a function of the graph structure only:</p><formula xml:id="formula_17">Z = ENC(W ; Θ E ).</formula><p>• Neighborhood aggregation methods, including graph convolutional methods, where both the node features and the graph structure are used in the encoder network. Neighborhood aggregation methods use the graph structure to propagate information across nodes and learn embeddings that encode local and global structural information:</p><formula xml:id="formula_18">Z = ENC(W, X; Θ E ).</formula><p>Note that shallow embedding methods and graph auto-encoders do not leverage node features and are inherently transductive, that is they cannot be applied to inductive problems where the graph structure is not fixed. In what follows, we review recent methods for supervised and unsupervised graph embedding techniques using GRAPHEDM and summarize the proposed taxonomy in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unsupervised Graph Embedding</head><p>We now give an overview of recent unsupervised graph embedding approaches using the taxonomy described in the previous section. In unsupervised graph embedding, the goal is to learn embedding that preserve the graph structure. These embeddings are usually optimized so that they can be used to reconstruct some node similarity matrix, e.g. the adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shallow embedding methods</head><p>Shallow embedding methods are transductive graph embedding methods where the encoder function is a simple embedding look-up. More concretely, each node v i ∈ V has a corresponding low-dimensional learnable embedding vector Z i ∈ R d and the shallow encoder function is simply:</p><formula xml:id="formula_19">Z = ENC(Θ E ) (3) = Θ E ∈ R |V |×d .<label>(4)</label></formula><p>Embeddings of nodes can be learned such that the structure of the data in the embedding space corresponds to the underlying graph structure. At a high level, this is similar to dimensionality reduction methods such as PCA, except that the input data might not have a linear structure. There exist several attempts to generalize linear models like PCA to non-linear and irregular graph structures. Here, we analyze two major types of shallow graph embedding methods, namely distance-based and outer product-based methods.</p><p>Distance-based methods optimize embeddings such that points that are close in the graph (as measured by some similarity function) stay as close as possible in the embedding space using a predefined distance function. Formally, the decoder network computes pairwise distance for some distance function d 2 (•, •), which can be Euclidean (Section 4.1.1) or hyperbolic (Section 4.1.2):</p><formula xml:id="formula_20">W = DEC(Z; Θ D )<label>(5)</label></formula><p>with</p><formula xml:id="formula_21">W ij = d 2 (Z i , Z j )<label>(6)</label></formula><p>Outer product-based methods on the other hand rely on pairwise dot-products to compute node similarities and the decoder network can be written as:</p><formula xml:id="formula_22">W = DEC(Z; Θ D ) (7) = ZZ .<label>(8)</label></formula><p>Embeddings are then learned by minimizing the graph regularization loss in Equation <ref type="formula" target="#formula_12">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Distance-based: Euclidean methods</head><p>Euclidean distance-based methods optimize Euclidean embeddings by minimizing Euclidean distances between similar nodes. Among these, we find linear embedding methods such as MDS, which learns low-dimensional linear projection subspaces, or nonlinear methods such as Laplacian eigenmaps, IsoMAP and Local linear embedding.</p><p>Multi-dimensional scaling (MDS) <ref type="bibr" target="#b65">[66]</ref> is a visualization algorithm that learns embeddings while preserving structural information. It minimizes the regularization loss in Equation <ref type="formula" target="#formula_12">1</ref>with s(W ) set to some distance e.g. s(W</p><formula xml:id="formula_23">) ij = 1 if w ij &gt; 0 and = 0 otherwise. d 1 (s(W ), W ) = i,j (s(W ) ij − W ij ) 2 d 2 (Z i , Z j ) = ||Z i − Z j ||.</formula><p>That is, MDS finds an embedding configuration where distances in the low-dimensional embedding space correspond node distances, measured by the adjacency matrix, W . Note that the distance function d 1 (•, •) is measuring the squared Frobenius distance. If ||•|| is chosen to be the Euclidean L2 distance, then MDS is equivalent to the PCA dimensionality reduction method. Note that MDS fails when the data does not have a linear structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Legend</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shallow embeddings</head><p>Auto-encoders</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph regularization</head><p>Graph neural networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised</head><p>(α = 0) Section 4</p><formula xml:id="formula_24">X = I ENC(Θ E ) Laplacian Section 4.1.1 MDS [66]</formula><p>IsoMAP <ref type="bibr" target="#b106">[107]</ref> LLE <ref type="bibr" target="#b95">[96]</ref> LE <ref type="bibr" target="#b8">[9]</ref> Non-Euclidean Section 4.1.2</p><p>Poincaré <ref type="bibr" target="#b82">[83]</ref> Lorentz <ref type="bibr" target="#b83">[84]</ref> Product <ref type="bibr" target="#b48">[49]</ref> Matrix Factorization Section 4.1.3 GF <ref type="bibr" target="#b3">[4]</ref> GraRep <ref type="bibr" target="#b18">[19]</ref> HOPE <ref type="bibr" target="#b85">[86]</ref> Skip-gram Section 4.1.4</p><p>DeepWalk <ref type="bibr" target="#b89">[90]</ref> node2vec <ref type="bibr" target="#b46">[47]</ref> WYS <ref type="bibr" target="#b1">[2]</ref> LINE <ref type="bibr" target="#b105">[106]</ref> HARP <ref type="bibr" target="#b24">[25]</ref> ENC(W</p><formula xml:id="formula_25">; Θ E ) Autoencoders Section 4.2 SDNE [114] DNGR [20] X = I ENC(W, X; Θ E ) Spectral Section 4.3 GAE [62]</formula><p>Graphite <ref type="bibr" target="#b47">[48]</ref> Supervised (α = 0) Section 5</p><formula xml:id="formula_26">X = I ENC(Θ E ) Laplacian Section 5.1 LP [127] Consistency [125] X = I ENC(X; Θ E ) Laplacian Section 5.2.1</formula><p>ManiReg <ref type="bibr" target="#b9">[10]</ref> SemiEmbed <ref type="bibr" target="#b114">[115]</ref> NGM <ref type="bibr" target="#b16">[17]</ref> Skip-gram Section 5.2.2 Planetoid <ref type="bibr" target="#b117">[118]</ref> ENC(W, X; Θ E )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message passing</head><p>Section 5.3.1 GNN <ref type="bibr" target="#b99">[100]</ref> GGSNNs <ref type="bibr" target="#b70">[71]</ref> MPNN <ref type="bibr" target="#b40">[41]</ref> GraphNets <ref type="bibr" target="#b6">[7]</ref> Spectral Section 5.4 SCNN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b52">53]</ref> ChebyNet <ref type="bibr" target="#b31">[32]</ref> GCN <ref type="bibr" target="#b60">[61]</ref> Spatial Section 5.5 SAGE <ref type="bibr" target="#b49">[50]</ref> MoNet <ref type="bibr" target="#b80">[81]</ref> GAT <ref type="bibr" target="#b110">[111]</ref> Non-Euclidean Section 5.6</p><p>HGCN <ref type="bibr" target="#b21">[22]</ref> HGNN <ref type="bibr" target="#b74">[75]</ref> Figure <ref type="figure">3</ref>: Taxonomy of graph representation learning methods. Based on what information is used in the encoder network, we categorize graph embedding approaches into four categories: shallow embeddings, graph auto-encoders, graph-based regularization and graph neural networks. Note that message passing methods can also be viewed as spatial convolution, since messages are computed over local neighborhood in the graph domain. Reciprocally, spatial convolutions can also be described using message passing frameworks.</p><formula xml:id="formula_27">Z DEC(Z; Θ D ) W L G,REG W Figure 4</formula><p>: Shallow embedding methods. The encoder is a simple embedding look-up and the graph structure is only used in the loss function.</p><p>IsoMAP <ref type="bibr" target="#b106">[107]</ref> is an algorithm for nonlinear dimensionality reduction technique. This method is similar to MDS, except for a different definition of node similarity. Specifically, rather than minimizing the Frobenius distance to W , the algorithm minimizes the distance to a more complex similarity matrix, in order to preserve geodesic distances along the underlying manifold.</p><p>In the discrete setting (the data lies on a graph and not a manifold), s(W ) is simply taken to be the graph distance matrix, that is the matrix that contains the shortest path for every pair of nodes:</p><formula xml:id="formula_28">s(W ) ij = d G (v i , v j ).</formula><p>Different from MDS, IsoMAP does work for multiple types of manifolds, even those that might not have a local Euclidean (flat) structure. It is however computationally very expensive.</p><p>Locally linear embedding (LLE) <ref type="bibr" target="#b95">[96]</ref> is another nonlinear dimensionality reduction technique. Different from IsoMAP which preserves the global geometry via geodesics, LLE is based on the local geometry of the manifold. The main idea behind LLE is to approximate each local neighborhood using a linear embedding. The local neighborhoods are then compared globally to find the best non-linear embedding.</p><p>Laplacian eigenmaps (LE) Spectral properties of the graph Laplacian matrix capture important structural information about a graph, such as graph cut information. LE <ref type="bibr" target="#b8">[9]</ref> is a nonlinear dimensionality reduction technique, which learns embeddings that preserve local neighborhoods in a graph by solving the generalized eigenvector problem:</p><formula xml:id="formula_29">min Z∈R |V |×d Z L Z subject to Z DZ = I and Z D1 = 0.</formula><p>The minimization objective can be equivalently written as a graph regularization term using our notations:</p><formula xml:id="formula_30">d 1 (W, W ) = i,j W ij W ij d 2 (Z i , Z j ) = ||Z i − Z j || 2 2 .</formula><p>Therefore, LE learns embeddings such that the Euclidean distance in the embedding space is small for points that are connected in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Distance-based: non-Euclidean methods</head><p>The distance-based methods described so far assumed embeddings lie in a Euclidean space. Graphs are non-Euclidean discrete data structures, and recent research has proposed to learn graph embeddings into non-Euclidean spaces instead of conventional Euclidean space. In particular, hyperbolic geometry-a non-Euclidean geometry with a constant negative curvature-is well-suited to represent hierarchical data and has been considered for network science problems <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b87">88]</ref>. More recently, hyperbolic geometry has been considered for graph embedding applications. In particular, hyperbolic embeddings can embed trees with arbitrary low distortion in just two-dimensions <ref type="bibr" target="#b98">[99]</ref>. More generally, hyperbolic geometry offers an exiting alternative to Euclidean geometry for graphs that exhibit hierarchical structures, as it enables embeddings with much smaller distortion.</p><p>Poincaré embeddings Nickel et al. <ref type="bibr" target="#b82">[83]</ref> learn embeddings of hierarchical graphs such as lexical databases (e.g. WordNet) in the Poincaré model hyperbolic space. Using our notations, this approach learns hyperbolic embeddings via the Poincaré distance function:</p><formula xml:id="formula_31">d 2 (Z i , Z j ) = d Poincaré (Z i , Z j )<label>(9)</label></formula><p>= arcosh 1 + 2</p><formula xml:id="formula_32">||Z i − Z j || 2 2 (1 − ||Z i || 2 2 )(1 − ||Z j || 2 2 ) . (<label>10</label></formula><formula xml:id="formula_33">)</formula><p>Embeddings are then learned by minimizing distances between connected nodes while maximizing distances between disconnected nodes:</p><formula xml:id="formula_34">d 1 (W, W ) = i,j W ij log e − Wij k|W ik =0 e − W ik , (<label>11</label></formula><formula xml:id="formula_35">)</formula><p>where the denominator is approximated using negative sampling. Note that since the hyperbolic space has a manifold structure, embeddings need to be optimized using Riemannian optimization techniques <ref type="bibr" target="#b11">[12]</ref>. Finally, note that other variants of this methods have been proposed. In particular, Nickel et al. <ref type="bibr" target="#b83">[84]</ref> explore a different model of hyperbolic space, namely the Lorentz model (also known as the hyperboloid model), and show that it provides better numerical stability than the Poincaré model. Another line of work extends non-Euclidean embeddings to mixed-curvature product spaces <ref type="bibr" target="#b48">[49]</ref>, which provide more flexibility for other types of graphs (e.g. ring of trees). Finally, Chamberlan et al. <ref type="bibr" target="#b20">[21]</ref> extend Poincaré embeddings to incorporate skip-gram losses using hyperbolic inner products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Outer product-based: Matrix factorization methods</head><p>Matrix factorization approaches learn embeddings that lead to a low rank representation of some similarity matrix s(W ), where s : R |V |×|V | is a transformation of the weighted adjacency matrix, and many methods set it to the identity, i.e. s(W ) = W . Other transformations include the Laplacian matrix or more complex similarities derived from proximity measures such as the Katz Index, Common Neighbours or Adamic Adar. The decoder function in matrix factorization methods is a simple dot product:</p><formula xml:id="formula_36">W = DEC(Z; Θ D ) = ZZ . (<label>12</label></formula><formula xml:id="formula_37">)</formula><p>Matrix factorization methods learn embeddings by minimizing the regularization loss in Equation 1 with:</p><formula xml:id="formula_38">L G,REG (W, W ; Θ) = ||s(W ) − W || 2 F .<label>(13)</label></formula><p>That is, d 1 (•, •) in Equation <ref type="formula" target="#formula_12">1</ref>is the Frobenius norm between the reconstructed matrix and the target similarity matrix. By minimizing the regularization loss, graph factorization methods learn low rank representations that preserve structural information as defined by the similarity matrix s(W ). We now give a overview of matrix factorization approaches.</p><p>Graph factorization (GF) <ref type="bibr" target="#b3">[4]</ref> learns a low-rank factorization for the adjacency matrix by minimizing graph regularization loss in Equation 1 using:</p><formula xml:id="formula_39">d 1 (W, W ) = (vi,vj )∈E (W ij − W ij ) 2 . (<label>14</label></formula><formula xml:id="formula_40">)</formula><p>Note that if A is the binary adjacency matrix, that is A ij = 1 iif (v i , v j ) ∈ E and A ij = 0 otherwise, then we can express the graph regularization loss in terms of Frobenius norm:</p><formula xml:id="formula_41">L G,REG (W, W ; Θ) = ||A • (W − W )|| 2 F , (<label>15</label></formula><formula xml:id="formula_42">)</formula><p>where • is the element-wise matrix multiplication operator. Therefore, GF also learns a low-rank factorization of the adjacency matrix W measured in Frobenuis norm. Note that the sum is only over existing edges in the graph, which reduces the computational complexity of this method from O(|V | 2 ) to O(|E|). HOPE The methods described so far are all symmetric, that is, the similarity score between two nodes (v i , v j ) is the same a the score of (v j , v i ). This might be a limiting assumption when working with directed graphs as some edges can be strongly connected in one direction and disconnected in the other direction. HOPE <ref type="bibr" target="#b85">[86]</ref> overcomes this issue by learning two embeddings per node, a source embedding Z s and a target embedding Z t , which capture asymmetric proximity in directed networks. The distance function in HOPE is simply the Frobenius norm and the graph regularization term can be written as:</p><formula xml:id="formula_43">W = Z s Z t ,<label>(16)</label></formula><formula xml:id="formula_44">L G,REG (W, W ; Θ) = ||s(W ) − W || 2 F . (<label>17</label></formula><formula xml:id="formula_45">)</formula><p>Graph representation with global structure information (GraRep) Similarly to HOPE <ref type="bibr" target="#b85">[86]</ref>, GraRep <ref type="bibr" target="#b18">[19]</ref> learns asymmetric embeddings. However, GraRep embeddings capture higher orders of proximity between nodes by minimizing the loss in Equation <ref type="formula" target="#formula_12">1</ref>for each 1 ≤ k ≤ K with:</p><formula xml:id="formula_46">W (k) = Z (k),s Z (k),t ,<label>(18)</label></formula><formula xml:id="formula_47">L G,REG (W, W (k) ; Θ) = ||D −k W k − W (k) || 2 F .<label>(19)</label></formula><p>GraRep then concatenates all K representations to get source embeddings Z s = [Z (1),s | . . . |Z (K),s ] and target embeddings Z t = [Z (1),t | . . . |Z (K),t ]. Finally, note that GraRep is not very scalable as the powers of D −1 W might be dense matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Outer product-based: Skip-gram methods</head><p>Skip-gram graph embedding models were inspired by efficient NLP methods modeling probability distributions over words for learning word embeddings <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b88">89]</ref>. Skip-gram word embeddings are optimized to predict context words, or surrounding words, for each target word in a sentence. Given a sequence of words (w 1 , . . . , w T ), skip-gram will minimize the objective</p><formula xml:id="formula_48">L = − −K≤i≤K,i =0 log P(w k−i |w k ),</formula><p>for each target words w k . In practice, the conditional probabilities can be estimated using neural networks, and skipgram methods can be trained efficiently using negative sampling. Perozzi et al. <ref type="bibr" target="#b89">[90]</ref> empirically show the frequency statistics induced by random walks, also follows Zipf's law, thus motivating the development of skip-gram graph embedding methods. These methods exploit random walks on graphs and produce node sequences that are similar in positional distribution, as to words in sentences. In skip-gram graph embedding methods, the decoder function is also an outer product (Equation <ref type="formula" target="#formula_36">12</ref>) and the graph regularization term is computed over random walks on the graph.</p><p>DeepWalk <ref type="bibr" target="#b89">[90]</ref> was the first attempt to generalize skip-gram models to graph-structured data. DeepWalk draws analogies between graphs and language. Specifically, writing a sentence is analogous to performing a random walk, where the sequence of nodes visited during the walk, is treated as the words of the sentence. DeepWalk trains neural networks by maximizing the probability of predicting context nodes for each target node in a graph, namely nodes that are close to the target node in terms of hops and graph proximity. For this purpose, node embeddings are decoded into probability distributions over nodes using row-normalization of the decoded matrix with softmax.</p><formula xml:id="formula_49">Method Model s(W )ij DEC(Z; Θ D )ij d1(W ← s(W ), W ← DEC(Z; Θ D )) order of proximity Distance Euclidean MDS[66] Wij ||Zi − Zj|| ||W − W || 2 F 1 st order IsoMAP [107] dG(vi, vj) ||Zi − Zj|| ||W − W || 2 F high order DEC(Z; Θ D )ij = d2(Zi, Zj) LE [9] Wij ||Zi − Zj|| 2 − ij Wij Wij 1 st order Non-Euclidean Poincaré [83] Wij dPoincaré(Zi, Zj) i,j Wijlog e − W ij k|W ik =0 e − W ik 1 st order Matrix GF [4] Wij Z i Zj ij|Wij &gt;0 (Wij − Wij) 2 1 st order Outer product Factorization HOPE [86] s(W )ij Z s i Z j ||W − W || 2 F high order GraRep [86] (D −k W k )ij Z (k),s i Z (k),t j ||W − W || 2 F k th order DEC(Z; Θ D )ij = Z i Zj Skip-gram DeepWalk [90] ∝ Eq D −1 W q ij Z i Zj − ij Wij log Softmaxj( Wij) high order node2vec [47] n2vWalk(W ; p, q)ij Z i Zj − ij Wij log Softmaxj( Wij) high order WYS [2] ∝ Eq D −1 W q ij Z i Zj BCE(W, W ) high order</formula><p>To train embeddings, DeepWalk generates sequences of nodes using truncated unbiased random walks on the graph-which can be compared to sentences in natural language models-and then maximize their log-likelihood. Each random walk starts with a node v i1 ∈ V and repeatedly sample next node at uniform:</p><formula xml:id="formula_50">v ij+1 ∈ {v ∈ V | (v ij , v) ∈ E}.</formula><p>The walk length is a hyperparameter. All generated random-walk can then be passed to an NLPembedding algorithm e.g. word2vec's Skipgram model. This two-step paradigm introduced by DeepWalk [90] is followed by many subsequent works, such as node2vec <ref type="bibr" target="#b46">[47]</ref>.</p><p>We note that is common for underlying implementations to use two distinct representations for each node (one for when a node is center of a truncated random walk, and one when it is in the context). The implications of this modeling choice is studied further in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Abu-El-Haija et al. <ref type="bibr" target="#b1">[2]</ref> show that training DeepWalk, in expectation, is equivalent to first sampling integer q ∼ [1, 2, . . . , T max ] with support ∝ [1, Tmax−1 Tmax , . . . , 1  Tmax ]. Specifically, if s(W ) = E q D −1 W q , then training Deep-Walk is equivalent to minimizing:</p><formula xml:id="formula_51">L G,REG (W, W ; Θ) = log C − vi∈V,vj ∈V s(W ) ij W ij ,<label>(20)</label></formula><p>where C = i j exp( W ij ) is a normalizing constant. Note that computing C requires summing over all nodes in the graph which is computationally expensive. DeepWalk overcomes this issue by using a technique called hierarchical softmax, which computes C efficiently using binary trees. Finally, note that by computing truncated random walks on the graph, DeepWalk embeddings capture high-order node proximity.</p><p>node2vec <ref type="bibr" target="#b46">[47]</ref> is a random-walk based approach for unsupervised network embedding, that extends DeepWalk's sampling strategy. The authors introduce a technique to generate biased random walks on the graph, by combining graph exploration through breadth first search (BFS) and through depth first search (DFS). Intuitively, node2vec also preserves high order proximities in the graph but the balance between BFS and DFS allows node2vec embeddings to capture local structures in the graph, as well as global community structures, which can lead to more informative embeddings. Finally, note that negative sampling <ref type="bibr" target="#b79">[80]</ref> is used to approximate the normalization factor C in Equation <ref type="formula" target="#formula_51">20</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Watch Your</head><p>Step (WYS) Random walk methods are very sensitive to the sampling strategy used to generate random walks. For instance, some graphs may require shorter walks if local information is more informative that global graph structure, while in other graphs, global structure might be more important. Both DeepWalk and node2vec sampling strategies use hyper-parameters to control this, such as the length of the walk or ratio between breadth and depth exploration. Optimizing over these hyper-parameters through grid search can be computationally expensive and can lead to sub-optimal embeddings. WYS <ref type="bibr" target="#b1">[2]</ref> learns such random walk hyper-parameters to minimize the overall objective (in analogy: each graph gets to choose its own preferred "context size", such that the probability of predicting random walks is maximized). WYS shows that, when viewed in expectation, these hyperparameters only correspond in the objective to coefficients to the powers of the adjacency matrix (W k ) 1≤k≤K . These co-efficients are denoted q = (q k ) 1≤k≤K and are learned through back-propagation. Should q's learn a left-skewed distribution, then the embedding would prioritize local information and right-skewed distribution will enhance high-order relationships and graph global structure. This concept has been extended to other forms of attention to the 'graph context', such using a personalized context distributions for each node <ref type="bibr" target="#b55">[56]</ref>.</p><p>Large scale information network embedding (LINE) <ref type="bibr" target="#b105">[106]</ref> learns embeddings that preserve first and second order proximity. To learn first order proximity preserving embeddings, LINE minimizes the loss in Equation 2 with:</p><formula xml:id="formula_52">W (1) ij = Z (1) i Z (1) j<label>(21)</label></formula><p>L G,REG (W, W (1) ; Θ) = −</p><formula xml:id="formula_53">(i,j)|(vi,vj )∈E W ij log σ( W (1) ij ).<label>(22)</label></formula><p>LINE also assumes that nodes with multiple edges in common should have similar embeddings and learns secondorder proximity preserving embeddings by minimizing:</p><formula xml:id="formula_54">W (2) ij = Z (2) i Z (2) j<label>(23)</label></formula><formula xml:id="formula_55">L G,REG (W, W (2) ; Θ) = − (i,j)|(vi,vj )∈E W ij log exp( W (2) ij ) k exp( W<label>(2) ik )</label></formula><p>.</p><p>Intuitively, LINE with second-order proximity decodes embeddings into context conditional distributions for each node p 2 (•|v i ). Note that optimizing the second-order objective is computationally expensive as it requires a sum over the entire set of edges. LINE uses negative sampling to sample negative edges according to some noisy distribution over edges. Finally, as in GraRep, LINE combines first and second order embeddings with concatenation Z = [Z (1) |Z (1) ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matrix view of Skip-gram Methods</head><p>As noted by <ref type="bibr" target="#b69">[70]</ref>, Skip-gram methods can be viewed as implicit matrix factorization, and the methods discussed here are related to those of Matrix Factorization (Section 4.1.3). This relationship is discussed in depth by <ref type="bibr" target="#b93">[94]</ref>, who propose a general matrix factorization framework, NetMF, which uses the same underlying graph proximity information as DeepWalk, LINE, and node2vec. Casting the node embedding problem as matrix factorization can offer benefits like easier algorithmic analysis, and can also allow for efficient sparse matrix operations <ref type="bibr" target="#b92">[93]</ref>.</p><p>Hierarchical representation learning for networks (HARP) Both node2vec and DeepWalk learn node embeddings by minimizing non-convex functions, which can lead to local minimas. HARP <ref type="bibr" target="#b24">[25]</ref> introduces a strategy that computes initial embeddings, leading to more stable training and convergence. More precisely, HARP hierarchically reduces the number of nodes in the graph via graph coarsening. Nodes are iteratively grouped into super nodes that form a graph with similar properties as the original graph, leading to multiple graphs with decreasing size (G 1 , . . . , G T ). Node embeddings are then learned for each coarsened graph using existing methods such as LINE or DeepWalk, and at time-step t, embeddings learned for G t are used as initialized embedding for the random walk algorithm on G t−1 . This process is repeated until each node is embedded in the original graph. The authors show that this hierarchical embedding strategy produces stable embeddings that capture macroscopic graph information.</p><p>Node Decomposition via Persona Graphs What if a node is not the correct 'base unit' of analysis for a graph? Unlike HARP, which coarsens a graph to preserve high-level topological features, Splitter <ref type="bibr" target="#b37">[38]</ref> is a graph embedding approach designed to better model nodes which have membership in multiple communities. It uses the Persona decomposition <ref type="bibr" target="#b36">[37]</ref>, to create a derived graph, G P which may have multiple persona nodes for each original node in G. (The edges of each original node are divided among its personas.) G P can then be embedded (with some constraints) using any of the embedding methods discussed so far. The resulting representations allow persona nodes to be separated in the embedding space, and the authors show benefits to this on link prediction tasks. </p><formula xml:id="formula_57">W ENC(W ; Θ E ) Z DEC(Z; Θ D ) W L G,REG</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Auto-encoders</head><p>Shallow embedding methods hardly capture non-linear complex structures that might arise in graphs. Graph autoencoders were originally introduced to overcome this issue by using deep neural network encoder and decoder functions, due to their ability model non-linearities. Instead of exploiting the graph structure through the graph regularization term, auto-encoders directly incorporate the graph adjacency matrix in the encoder function, thus leading to more complex representations. Auto-encoders generally have an encoding and decoding function which are multiple layers of nonlinear functions. For graph auto-encoders, the encoder function has the form:</p><formula xml:id="formula_58">Z = ENC(W ; Θ E ).</formula><p>That is, the encoder is a function of the adjacency matrix W only. These models are trained by minimizing a reconstruction error objective and we review examples of such objectives next.</p><p>Structural deep network embedding (SDNE) <ref type="bibr" target="#b113">[114]</ref> learns auto-encoders that preserve first and second-order node proximity (Section 2.1). The SDNE encoder takes as input a node vector: a row of the adjaccency matrix as they explicitly set s(W ) = W , and produces node embeddings Z. The SDNE decoder return a reconstruction W , which is trained to recover the original graph adjacency matrix (Figure <ref type="figure">7</ref>). SDNE preserves second order node proximity by minimizing the graph regularization term:</p><formula xml:id="formula_59">||(s(W ) − W ) • B|| 2 F + α SDNE ij s(W ) ij ||Z i − Z j || 2 2 ,</formula><p>where B is the indicator matrix for s(W ) with B = 1[s(W ) &gt; 0]. Note that the second term in the regularization loss used by distance-based shallow embedding methods. The first term is similar to the matrix factorization regularization objective, except that W is not computed using outer products. Instead, SDNE computes a unique embedding for each node in the graph using a decoder network.</p><p>Deep neural networks for learning graph representations (DNGR) Similarly to SDNE, DNGR <ref type="bibr" target="#b19">[20]</ref> uses deep auto-encoders to encode and decode a node similarity matrix, s(W ). The similarity matrix is computed using a probabilistic method called random surfing, that returns a probabilistic similarity matrix through graph exploration with random walks. Therefore, DNGR captures higher order dependencies in the graph. s(W ) is then encoded and decoded with stacked denoising auto-encoders <ref type="bibr" target="#b112">[113]</ref>, which allows to reduce the noise in s(W ). DNGR is optimized by minimizing the L2 reconstruction error:</p><formula xml:id="formula_60">L G,REG (W, W ; Θ) = ||(s(W ) − W )|| 2 F .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph neural networks</head><p>In graph neural networks, both the graph structure and node features are used in the encoder function:</p><formula xml:id="formula_61">Z = ENC(X, W; Θ E ).</formula><p>We review unsupervised graph neural networks, and will cover supervised graph neural networks, as well as the convolution mechanism in Section 5.4.2.</p><p>Figure <ref type="figure">7</ref>: Illustration of the SDNE model. Reprinted with permission from <ref type="bibr" target="#b41">[42]</ref>. We denote the (green) embedding layer as Z.</p><p>Convolutional Graph auto-encoders (GAE) <ref type="bibr" target="#b61">[62]</ref> use a graph convolutional encoder <ref type="bibr" target="#b60">[61]</ref> to learn node embeddings Z = GCN(W, X; Θ E ) (See Section 5.3.1 for more details about graph convolutions). The decoder is an outer product: DEC(Z; Θ D ) = ZZ . The graph regularization term is the sigmoid cross entropy between the true adjacency and the predicted edge similarity scores:</p><formula xml:id="formula_62">L G,REG (W, W ; Θ) = − i,j (1 − W ij )log(1 − σ( W ij )) + W ij log σ( W ij ).</formula><p>Computing the regularization term over all possible nodes pairs is computationally challenging in practice, and GAE uses negative sampling to overcome this challenge. Note that GAE is a deterministic model but the authors also introduce variational graph auto-encoders (VGAE), where they use variational auto-encoders (VAEs) to encode and decode the graph structure. In VGAE, Z is modelled as a latent variable with a standard multivariate normal prior p(Z) = N (Z|0, I) and the amortized inference network q Φ (Z|W, X) is also a graph convolution network. VGAE is optimized by minimizing the corresponding negative evidence lower bound:</p><formula xml:id="formula_63">NELBO(W, X; Θ) = −E qΦ(Z|W,X) [log p(W |Z)] + KL(q Φ (Z|W, X)||p(Z))</formula><p>= L G,REG (W, W ; Θ) + KL(q Φ (Z|W, X)||p(Z)).</p><p>Iterative generative modelling of graphs (Graphite) <ref type="bibr" target="#b47">[48]</ref> extends GAE and VGAE by introducing a more complex decoder, which iterates between pairwise decoding functions and graph convolution encoders. In other words, the graphite decoder repeats the following iteration</p><formula xml:id="formula_64">W (k) = Z (k) Z (k) ||Z (k) || 2 2 + 11 |V | Z (k+1) = GCN( W (k) , Z (k) )</formula><p>where Z (0) are initialized using the output of the encoder network. In practice, the authors also concatenate Z (k) with the initial node features X in the decoder functions. By using this parametric iterative decoding process, Graphite learns more complex decoder functions than other methods based on non-parametric pairwise decoders. Finally, similar to GAE, Graphite can be deterministic or Z can be latent and learned via variational inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Supervised Graph Embedding</head><p>A common approach for supervised network embedding is to use an unsupervised network embedding method, like the ones described in Section 4 to first map nodes to an embedding vector space, and then use the learned embeddings as input for another neural network. However, an important limitation with this two-step approach is that the unsupervised node embeddings might not preserve important information about the graph, that could have been useful for the downstream supervised task.</p><formula xml:id="formula_65">X W ENC(W, X; Θ E ) Z DEC(Z; Θ D ) W L G,REG</formula><p>Recently, methods combining these two steps, namely learning embeddings and predicting node or graph labels, have been proposed. We describe these methods next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Shallow embedding methods</head><p>Similar to unsupervised shallow embedding methods, supervised shallow embedding methods use embedding lookups to map nodes to embeddings. However, while the goal in unsupervised shallow embeddings is to learn a good graph representation, supervised shallow embedding methods aim at doing well on some downstream prediction task such as node or graph classification.</p><p>Label propagation (LP) <ref type="bibr" target="#b126">[127]</ref> is a model for graph-based semi-supervised node classification. It directly learns embeddings in the label space, i.e. the decoder function in LP is simply the identity function:</p><formula xml:id="formula_66">ŷN = DEC(Z; Θ C ) = Z.</formula><p>In particular, LP uses the graph structure to add a regularization term to the loss function, where the underlying assumption is that neighbor nodes should have similar labels. The regularization in LP is computed with Laplacian eigenmaps:</p><formula xml:id="formula_67">W ij = ||y N i − ŷN j || 2 2<label>(25)</label></formula><p>and</p><formula xml:id="formula_68">L G,REG (W, W ; Θ) = − i,j W ij W ij .<label>(26)</label></formula><p>The supervised loss minimizes the distances between predicted labels and ground truth labels (one-hot vectors):</p><formula xml:id="formula_69">L N SUP (y N , ŷN ; Θ) = i|vi∈V L ||y N i − ŷN i || 2 2 .</formula><p>Note that the supervised loss is computed on labelled nodes only, while the regularization term is computed on all nodes in the graph. This method is expected to work well with consistent graphs, that is graphs where node proximity in the graph is positively correlated with label similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph regularization methods</head><p>Supervised graph regularization methods also aim at learning to predict graph properties such as node labels. Similar to shallow embeddings, these methods compute a graph regularization loss defined over the graph structure, and a supervised loss for the downstream task (Figure <ref type="figure" target="#fig_6">9</ref>). However, the main difference with shallow embeddings lies in the encoder network: rather than using embedding look-ups, graph regularization methods learn embeddings as parametric function defined over node features, which might capture valuable information for downstream applications. That is, encoder functions in these methods can be written as:</p><formula xml:id="formula_70">Z = ENC(X; Θ E ).</formula><p>We review two types of semi-supervised <ref type="bibr" target="#b22">[23]</ref> graph regularization approaches: methods that use random walks to regularize embeddings and Laplacian-based regularization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Laplacian</head><p>Manifold regularization (ManiReg) <ref type="bibr" target="#b9">[10]</ref> builds on the LP model and uses Laplacian Eigenmaps to regularize node embeddings via the regularization loss in Equation <ref type="formula" target="#formula_68">26</ref>. However, instead of minimizing distance between node labels and learned embeddings in the supervised loss, ManiReg uses support vector machines to predict labels. That is,</p><formula xml:id="formula_71">L N SUP (y N , ŷN ; Θ) = i|vi∈V L 1≤k≤C H(y N ik ŷN ik ),<label>(27)</label></formula><p>where H(x) = max(0, 1 − x) is the hinge loss, C is the number of classes, and ŷN i = f (X i ; Θ E ) are computed using a Reproducing Kernel Hilbert Space (RKHS) function that acts on input features.</p><p>Semi-supervised embeddings (SemiEmb) <ref type="bibr" target="#b114">[115]</ref> uses feed-forward neural networks (FF-NN) to learn embeddings Z = ENC(X; Θ E ) and regularizes intermediate or auxiliary layers in the network using the graph regularizer:</p><formula xml:id="formula_72">W ij = DEC(Z; Θ D ) ij (28) = d 2 (Z i , Z j ),<label>(29)</label></formula><p>where d 2 (•, •) is a distance metric such as the L2 or L1 norm, and the regularization loss is the same as the LP loss in Equation <ref type="formula" target="#formula_68">26</ref>. Additionally, SemiEmb also uses FF-NN in the decoder network to predict node labels, which are compared to ground truth labels via the Hinge loss in Equation <ref type="formula" target="#formula_71">27</ref>.</p><p>Note that SemiEmb leverages multi-layer neural networks and regularizes intermediate hidden representations, while LP does not learn intermediate representations, and ManiReg only regularizes the last layer.</p><p>Neural Graph Machines (NGM) More recently, Bui et al. <ref type="bibr" target="#b16">[17]</ref> introduced NGM and showed that the regularization objective in Equation 26 generalizes to more complex neural architectures than feed-forward neural networks (FF-NN), such as Long short-term memory (LSTM) networks <ref type="bibr" target="#b53">[54]</ref> or CNNs <ref type="bibr" target="#b67">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Skip-gram</head><p>The Laplacian-based regularization methods covered so far only capture first order proximities in the graphs. Skipgram graph regularization methods further extend these methods to incorporate random walks, which are effective at capturing higher-order proximities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><formula xml:id="formula_73">Z = ENC(X; Θ E ) y N = DEC(Z; Θ S ) W ij = DEC(Z; Θ D ) ij L G,REG (W, W ; Θ) L N SUP (y N , y N ; Θ) LP [125] Shallow NA: y N = Z ||Z i − Z j || 2 2 − i,j W ij W ij i|vi∈VL ||y N i − y N i || 2 2 ManiReg [9] RKHS NA: y N = Z ||Z i − Z j || 2 2 − i,j W ij W ij i|vi∈VL 1≤k≤C H(y N ik y N ik ) SemiEmb [115] FF-NN FF-NN d 2 (Z i , Z j ) − i,j W ij W ij i|vi∈VL 1≤k≤C H(y N ik y N ik ) NGM [17] CNN, LSTM . . . CNN, LSTM . . . d 2 (Z i , Z j ) − i,j W ij W ij − 1 |VL| i|vi∈VL 1≤k≤C y N ik log y N ik Planetoid [118] FF-NN FF-NN Z i Z j −E (i,j,γ) log σ γ W ij − 1 |VL| i|vi∈VL 1≤k≤C y N ik log y N ik</formula><p>Table <ref type="table">2</ref>: An overview of supervised shallow and graph regularization methods, where the graph structure is leveraged through the graph regularization term L G,REG (W, W ; Θ).</p><p>Planetoid Unsupervised skip-gram methods like node2vec and DeepWalk learn embeddings in a multi-step pipeline where random walks are first generated from the graph and then used to learn embeddings. These embeddings are not learned for a downstream classification task which might be suboptimal in scenarios where neighboring nodes might not have similar labels. Planetoid <ref type="bibr" target="#b117">[118]</ref> extends random walk methods to leverage node label information during the embedding algorithm. Planetoid first maps nodes to embeddings Z = [Z c ||Z F ] = ENC(X; Θ E ) with neural networks, where Z c are node embeddings that capture structural information while Z F capture node feature information. The authors propose two variants, a transductive variant that directly learns embedding Z c (as embedding look-up), and an inductive variant where Z c are computed with a parametric function of input features X. Embeddings are then learned by minimizing the loss in Equation <ref type="formula" target="#formula_14">2</ref>, where the regularization loss measures the ability to predict context using nodes embeddings, while the supervised loss measures the ability to predict the correct label. More specifically, the regularization loss in Planetoid is given by:</p><formula xml:id="formula_74">L G,REG (W, W ; Θ) = −E (i,j,γ) log σ γ W ij ,<label>(30)</label></formula><p>with</p><formula xml:id="formula_75">W ij = Z i Z j . Binary γ = 1 if (v i , v j ) ∈ E is a positive pair and γ = −1 if (v i , v j ) is a negative pair.</formula><p>The distribution under expectation is directly defined through a sampling process <ref type="foot" target="#foot_3">3</ref> . On the other hand, the supervised loss is the negative log-likelihood of predicting correct labels,</p><formula xml:id="formula_76">L N SUP (y N , y N ; Θ) = − 1 |V L | i|vi∈V L 1≤k≤C y N ik log y N ik , (<label>31</label></formula><formula xml:id="formula_77">)</formula><p>where i is a node's index while k indicates label classes, and y N i can be a neural network from Z i to predicted classes (in <ref type="bibr" target="#b117">[118]</ref>, softmax-output feed-forward network).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Graph convolution framework</head><p>We now focus on (semi-)supervised neighborhood aggregation methods, where the encoder function uses input features and the graph structure:</p><formula xml:id="formula_78">Z = ENC(X, W ; Θ E ).</formula><p>We first review the graph neural network model-which was the first attempt to use deep learning techniques on graphstructured data-and other related frameworks such as message passing networks <ref type="bibr" target="#b40">[41]</ref>. We then introduce a new Graph Convolution Framework (GCF), which is designed specifically for convolution-based graph neural networks. While GCF and other frameworks overlap on some methods, GCF emphasizes the geometric aspects of convolution and propagation, allowing to easily understand similarities and differences between existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">The Graph Neural Network model and related frameworks</head><p>The graph neural network model The first formulation of deep learning methods for graph-structured data dates back to the graph neural network (GNN) model <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b99">100]</ref>. This formulation views the supervised graph embedding problem as an information diffusion mechanism, where nodes send information to their neighbors until some stable equilibrium state is reached. More concretely, given randomly initialized node embeddings Z 0 , the following recursion is applied until convergence:</p><formula xml:id="formula_79">Z t+1 = ENC(X, W, Z t ; Θ E ),<label>(32)</label></formula><p>where parameters Θ E are reused at every iteration. After convergence (t = T ), the node embeddings Z T are used to predict the final output such as node or graph labels:</p><formula xml:id="formula_80">ŷS = DEC(X, Z t ; Θ S ).</formula><p>This process is repeated several times and the GNN parameters Θ E and Θ D are learned with backpropagation via the Almeda-Pineda algorithm <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b90">91]</ref>. Note that by Banach's fixed point theorem, the iteration in Equation 32 is guaranteed to converge to a unique solution when the iteration mapping is a contraction mapping. In particular, Scarselli et al. <ref type="bibr" target="#b99">[100]</ref> explore maps that can be expressed using message passing networks:</p><formula xml:id="formula_81">Z t+1 i = j|(vi,vj )∈E f (X i , X j , Z t j ; Θ E ),<label>(33)</label></formula><p>where f (•) is a multi-layer perception (MLP) constrained to be a contraction mapping. On the other hand, the decoder function in GNNs does not need to fulfill any constraint and can be any MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated graph neural networks Li et al. propose Gated Graph Sequence</head><p>Neural Networks (GGSNNs) <ref type="bibr" target="#b70">[71]</ref>, which are similar to GNNs but remove the contraction mapping requirement. In GGSNNs, the recursive algorithm in Equation 32 is relaxed and approximated by applying mapping functions for a fixed number of steps, where each mapping function is a gated recurrent unit <ref type="bibr" target="#b28">[29]</ref> with parameters shared for every iteration. The GGSNN model is particularly useful for machine learning tasks with sequential structure (such as temporal graphs) as it outputs predictions at every step.</p><p>Message passing neural networks In the same vein, Gilmer et al. provide a framework for graph neural networks using message passing neural networks (MPNNs) <ref type="bibr" target="#b40">[41]</ref>, encapsulating many recent graph neural networks. In contrast with the GNN model which runs for an indefinite number of iterations, MPNNs provide an abstraction for modern graph neural networks, which consist of multi-layer neural networks with a fixed number of layers. At every layer , message functions f (.) compute messages using neighbors' hidden representations, which are then passed to aggregation functions h (.):</p><formula xml:id="formula_82">m +1 i = j|(vi,vj )∈E f (H i , H j )<label>(34)</label></formula><formula xml:id="formula_83">H +1 i = h (H i , m +1 i ). (<label>35</label></formula><formula xml:id="formula_84">)</formula><p>After layers of message passing, nodes' hidden representations encode structural information within -hop neighborhoods. Gilmer et al. explore additional variations of message functions within the MPNN framework, and achieve state-of-the-art results for prediction tasks defined on molecular graphs.</p><p>GraphNet Battaglia et al. propose GraphNet <ref type="bibr" target="#b6">[7]</ref>, which further extends the MPNN framework to learn representations for edges, nodes and the entire graph using message passing functions. This framework is more general than the MPNN framework as it incorporates edge and graph representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Graph Convolution Framework</head><p>We now introduce our Graph Convolution Framework (GCF); and as we shall see, many recent graph neural networks can be described using in this framework. Different from the MPNN and GraphNet frameworks, our framework focuses on convolution-based methods, and draws direct connections between convolutions on grids and graph convolutions. While our framework does not include sophisticated message passing networks (e.g. messages computed with edge features), it emphasizes geometric properties of convolution operators, and provides a simple way to understand similarities and differences between state-of-the-art graph convolution methods.</p><formula xml:id="formula_85">X W ENC(W, X; Θ E ) Z DEC(Z; Θ S ) y S L S SUP y S</formula><p>Figure <ref type="figure" target="#fig_1">10</ref>: Supervised graph neural networks (GNNs). Rather than leveraging the graph structure to act as a regularizer, GNNs leverage the graph structure in the encoder to propagate information across neighbouring nodes. Labels are then decoded and compared to ground truth labels (e.g., via the cross-entropy loss).</p><p>GCF In GCF, node embeddings are initialized using input features H 0 = X ∈ R |V |×d0 , and then updated with multiple layers of graph convolutions. Graph convolution layers provide a generalization of standard convolutions to graph-structured data and are composed of four main components:</p><p>• Patch functions, which define the shape of convolutional filters (which nodes interact with each other at every step of convolution), that is matrices of size |V | × |V |:</p><formula xml:id="formula_86">(f 1 (W, H ), . . . , f K (W, H )),</formula><p>where H are node features at layer and K is the total number of patches. Note that the number of patches K might be defined in the spectral domain (e.g. rank of a matrix) or in the spatial domain (e.g. neighborhood size). In standard CNNs (which are defined in the spatial pixel domain), these patches usually have rectangular shapes, where nodes (pixels in images) communicate with their top, left, bottom, and right neighbors. However, since graphs do not have a grid-like structure, the shape of convolutional filters does not follow a regular pattern and is instead defined by the graph structure itself. While most methods use non-parametric patches at every layer, some methods such as attention-based methods (Section 5.5.2) learn patches using parametric functions. • Convolution filters' weights at every layer, which are d × d +1 matrices, representing the filter weights:</p><formula xml:id="formula_87">(Θ 1 , . . . , Θ K ).</formula><p>Each column can be interpreted as a single convolution filter's weight, and we stack d +1 filters filters to compute features in the next layer. Similarly, d and d +1 are analogous to the number of channels in CNNs for layer and + 1 respectively. At every layer in the GCF, hidden representations H are convolved with every patch using the convolution filter weights:</p><formula xml:id="formula_88">m +1 k = f k (W, H )H Θ k for 1 ≤ k ≤ K.</formula><p>• Merging functions, which combine outputs from multiple convolution steps into one representation:</p><formula xml:id="formula_89">H +1 = h(m +1 1 , . . . , m +1 K ).</formula><p>For instance, h(•) can be averaging or concatenation along the feature dimension followed by some non-linearity σ(.). Alternatively, h(•) can also be a more complicated operation parameterized by a neural network.</p><p>After L convolution layers, nodes' embeddings Z = H L can be used to decode node or graph labels. Next, we review state-of-the-art GNNs, including spectral and spatial methods using the proposed GCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Spectral Graph Convolutions</head><p>Spectral methods apply convolutions in the the spectral domain of the graph Laplacian matrix. These methods broadly fall into two categories: spectrum-based methods, which explicitly compute the Laplacian's eigendecomposition, and spectrum-free methods. One disadvantage of spectrum-based methods is that they rely on the spectrum of the graph Laplacian and are therefore domain-dependent (i.e. cannot generalize to new graphs). Moreover, computing the Laplacian's spectral decomposition is computationally expensive. Spectrum-free methods overcome these limitations by providing approximations for spectral filters.</p><formula xml:id="formula_90">Method Model g k (.) h(m 1 , . . . , m k ) Spectrum-based: L = U ΛU , f k (W, H) = g k (U ) SCNN [16, 53] g k (U ) = u k u k σ( k m k ) Spectrum-free: f k (W, H) = g k (W, D) ChebNet [32] g k (W, D) = T k ( 2(I−D −1/2 W D −1/2 ) λmax(I−D −1/2 W D −1/2 ) − I) σ( k m k ) GCN [61] g 1 (W, D) = (D + I) −1/2 (W + I)(D + I) −1/2 σ(m 1 ) Spatial: f k (W, H) = g k (W, D) SAGE-mean [51] g 1 (W, D) = I, g 2 (W, D) ∼ U norm (D −1 W, q) σ(m 1 + m 2 ) GGNN [71] g 1 (W, D) = I, g 2 (W, D) = W GRU(m 1 , m2) Attention: f k (W, H) = α(W • g k (H)) MoNet [81] g k (U s ) = exp(− 1 2 (U s − µ k ) Σ −1 k (U s − µ k )) σ( k m k ) GAT [111] g k (H) = LeakyReLU(HB b 0 ⊕ b 1 BH ) σ([m 1 || . . . ||m k ])</formula><p>Table <ref type="table">3</ref>: An overview of graph convolution methods described using GCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Spectrum-based methods</head><p>Spectrum-based graph convolutions were the first attempt to generalize convolutions to non-Euclidean graph domains. Given a signal x ∈ R |V | defined on a Euclidean discrete domain (e.g. grid), applying any linear translation-equivariant operator (i.e. with a Toeplitz structure) Θ in the discrete domain is equivalent to elementwise multiplication in the the Fourier domain:</p><formula xml:id="formula_91">F(Θx) = Fx • Fθ.<label>(36)</label></formula><p>In non-Euclidean domains, the notion of translation (shift) is not defined and it is not trivial to generalize spatial convolutions operators (Θ) to non-Euclidean domains. Note that Equation 36 can be equivalently written as:</p><formula xml:id="formula_92">Θx = F −1 (Fx • Fθ).<label>(37)</label></formula><p>While the left hand side is the Euclidean spatial convolution which is not defined for general graphs, the right hand side is a convolution in the Fourier domain which is defined for non-Euclidean domains. In particular, if</p><formula xml:id="formula_93">L = I − D −1/2 W D −1/2</formula><p>is the normalized Laplacian of a non-Euclidean graph, it is a real symmetric positive definite matrix and admits an orthonormal eigendecomposition:</p><formula xml:id="formula_94">L = U ΛU . If x ∈ R |V |</formula><p>is a signal defined on nodes in the graph, the discrete graph Fourier transform and its inverse can be written as:</p><formula xml:id="formula_95">Fx = x = U x and F −1 x = U x.<label>(38)</label></formula><p>Spectral graph convolutions build on this observation to generalize convolutions to graphs, by learning convolution filters in the spectral domain of the normalized Laplacian matrix:</p><formula xml:id="formula_96">x * θ = U (U x • U θ) = U diag(U θ)U x</formula><p>Using GCF, patch functions in spectrum-based methods can be expressed in terms of eigenvectors of the graph normalized Laplacian:</p><formula xml:id="formula_97">f k (W, H ) = g k (U )</formula><p>for some function g k (.). Note that this dependence on the spectrum of the Laplacian makes spectrum-based methods domain-dependent (i.e. they can only be used in transductive settings).</p><p>Spectral CNNs Bruna et al. propose a Spectral Convolutional Neural Network (SCNN) <ref type="bibr" target="#b15">[16]</ref> by learning convolution filters as multipliers on the eigenvalues of the normalized Laplacian. SCNN layers compute feature maps at layer + 1 with:</p><formula xml:id="formula_98">H +1 :,j = σ d i=1 U K F i,j U K H :,i , 1 ≤ j ≤ d +1 and 1 ≤ i ≤ d<label>(39)</label></formula><p>where σ(•) is a non-linear transformation, U K is a |V | × K matrix containing the top K eigenvectors of L and F i,j are K × K trainable diagonal matrices representing filters' weights in the spectral domain. We note that this spectral convolution operation can equivalently be written as:</p><formula xml:id="formula_99">H +1 :,j = σ K k=1 u k u k d i=1 F i,j,k H :,i ,<label>(40)</label></formula><formula xml:id="formula_100">Θ D −1/2 W D −1/2 H H +1 H +1 = σ( D −1/2 W D −1/2 H Θ ) (a) GCN aggregation. H × D −1/2 W D −1/2 × Θ H +1 (b) GCN layer.</formula><p>(c) GAT aggregation.</p><p>Figure <ref type="figure" target="#fig_1">11</ref>: An illustration of neighborhood aggregation methods. Reprinted with permission from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b110">111]</ref>.</p><p>where (u k ) k=1,...,K are the top K eigenvectors of L and F i,j,k is the k th diagonal element of F i,j . We can also write Equation 40 using matrix notation as:</p><formula xml:id="formula_101">H +1 = σ K k=1 u k u k H Θ k ,</formula><p>where Θ k are trainable matrices of shape d × d +1 containing the filter weights. Using notation from GCF, SCNNs use patch functions expressed in terms of eigenvectors of the graph Laplacian g k (U ) = u k u k , and the merging function h(.) is the sum operator followed by a non-linearity σ(.).</p><p>Euclidean grids have a natural ordering of nodes (top, left, bottom, right) allowing the use of spatially localized convolution filters with fixed size, independent of the input size. In contrast, SCNN layers require <ref type="bibr">Bruna and al. [16,</ref><ref type="bibr" target="#b52">53]</ref> note that spatial localization in the graph domain is equivalent to smoothness in the spectral domain, and propose smooth spectral multipliers in order to reduce the number of parameters in the model and avoid overfitting. Instead of learning K free parameters for each filter F ij , the idea behind smooth spectral multipliers is to parameterize F ij with polynomial interpolators such as cubic splines and learn a fixed number of interpolation coefficients. This modeling assumption leads to a constant number of parameters, independent of the graph size |V |.</p><formula xml:id="formula_102">O(d d +1 K) parameters, which is not scalable if K is O(|V |).</formula><p>In practice, SCNNs can be used for node classification or graph classification with graph pooling. However, SCNNs have two major limitations: (1) computing the eigendecomposition of the graph Laplacian is computationally expensive and (2) this method is domain-dependent, as its filters are eigen-basis dependent and cannot be shared across graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Spectrum-free methods</head><p>We now cover spectrum-free methods, which also learn convolutions in the spectral domain but overcome computational limitations of SCNNs by avoiding explicit computation of the Laplacian's eigendecomposition. SCNNs filters are neither localized nor parametric, in the sense that the parameters in F ij in Equation 40 are all free. To overcome this issue, sprectrum-free methods use polynomial expansions to approximate spectral filters in Equation 39 via:</p><formula xml:id="formula_103">F i,j = P ij (Λ)</formula><p>where P ij (•) is a finite degree polynomial. Therefore, the total number of free parameters per filter depends on the polynomial's degree, which is independent of the graph size. Assuming all eigenvectors are kept in Equation 39, it can be rewritten as:</p><formula xml:id="formula_104">H +1 :,j = σ d i=1 P ij (Λ)H :,i .</formula><p>If we write P ij (λ) = K k=1 θ i,j,k λ k , this yields in matrix notation:</p><formula xml:id="formula_105">H +1 = σ K k=1 L k H Θ k ,</formula><p>where Θ k is the matrix containing the polynomials' coefficients. These filters are k-localized, in the sense that the receptive field of each filter is k, and only nodes at a distance less than k will interact in the convolution operation.</p><p>Since the normalized Laplacian is expressed in terms of the graph adjacency and degree matrices, we can write patch functions in spectrum-free method using notation from GCF:</p><formula xml:id="formula_106">f k (W, H ) = g k (W, D).</formula><p>Chebyshev networks Defferrard et al. propose Chebyshev Networks (ChebNets) <ref type="bibr" target="#b31">[32]</ref> which use the Chebyshev expansion <ref type="bibr" target="#b51">[52]</ref> to approximate spectral filters. Chebyshev polynomials form an orthonormal basis in [−1, 1] and can be computed efficiently with the recurrence:</p><formula xml:id="formula_107">T 0 (x) = 1, T 1 (x) = x, and T k (x) = 2xT k−1 (x) − T k−2 (x) for k ≥ 2.<label>(41)</label></formula><p>In order to use Chebyshev polynomials, ChebNets rescale the normalized adjacency martrix L to ensure that its eigenvalues are in [−1, 1]. The convolution step in ChebNet can be written as:</p><formula xml:id="formula_108">H +1 = σ K k=1 T k 2 λ max ( L) L − I H Θ k ,</formula><p>where λ max ( L) is the largest eigenvalue of L.</p><p>Graph convolution networks Kipf et al. <ref type="bibr" target="#b60">[61]</ref> introduce Graph Convolution Networks (GCN) which further simplify ChebNets by letting K = 2, adding a weight sharing constraint for the first and second convolutions Θ 1 = −Θ 2 := Θ , and assuming λ max ( L) 2. This yields:</p><formula xml:id="formula_109">H +1 = σ((2I − L)H Θ )<label>(42)</label></formula><p>= σ((</p><formula xml:id="formula_110">I + D −1/2 W D −1/2 )H Θ ),<label>(43)</label></formula><p>Furthermore, since I + D −1/2 W D −1/2 has eigenvalues in [0, 2], applying Equation 43 multiple times might lead to numerical instabilities or exploding gradients. To overcome this issue, GCNs use a re-normalization trick, which maps the eigenvalues of I + D −1/2 W D −1/2 to [0, 1]:</p><formula xml:id="formula_111">I + D −1/2 W D −1/2 → D + I −1/2 W + ID + I −1/2 .</formula><p>Using GCF notation, GCN patch functions can be written as:</p><formula xml:id="formula_112">g 1 (W, D) = D + I −1/2 W + ID + I −1/2 ,</formula><p>and the graph convolution layer is (see Figure <ref type="figure" target="#fig_1">11</ref> for an illustration):</p><formula xml:id="formula_113">H +1 = σ(g 1 (W, D)H Θ ).<label>(44)</label></formula><p>This model has been applied to many problems including matrix completion <ref type="bibr" target="#b10">[11]</ref>, link prediction in knowledge graphs <ref type="bibr" target="#b100">[101]</ref>, and unsupervised graph embedding with variational inference <ref type="bibr" target="#b61">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Spatial Graph Convolutions</head><p>Spectrum-based methods are limited by their domain dependency and cannot be applied in inductive settings. Furthermore, spectrum-free methods such as GCNs require storing the entire graph adjacency matrix, which can be computationally expensive for large graphs.</p><p>To overcome these limitations, spatial methods borrow ideas from standard CNNs, where convolutions are applied in the spatial domain as defined by the graph topology. For instance, in computer vision, convolutional filters are spatially localized by using fixed rectangular patches around each pixel. Additionally, since pixels in images have a natural ordering (top, left, bottom, right), it is possible to reuse filters' weights at every location, significantly reducing the total number of parameters. While such spatial convolutions cannot directly be applied in graph domains, spatial graph convolutions use ideas such as neighborhood sampling and attention mechanisms to overcome challenges posed by graphs' irregularities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Sampling-based spatial methods</head><p>Inductive representation learning on large graphs While GCNs can be used in inductive settings, they were originally introduced for semi-supervised transductive settings, and the learned filters might strongly rely on the Laplacian used for training. Furthermore, GCNs require storing the entire graph in memory which can be computationally expensive for large graphs.</p><p>To overcome these limitations, Hamilton et al. <ref type="bibr" target="#b49">[50]</ref> propose SAGE, a general framework to learn inductive node embeddings while reducing the computational complexity of GCNs. Instead of averaging signals from all one-hop neighbors using multiplications with the Laplacian matrix, SAGE samples fixed neighborhoods (of size q) to remove the strong dependency on a fixed graph structure and generalize to new graphs. At every SAGE layer, nodes aggregate information from nodes sampled from their neighborhood, and the propagation rule can be written as:</p><formula xml:id="formula_114">H +1 :,i = σ(Θ 1 H :,i + Θ 2 AGG({H :,j : j|v j ∈ Sample(N (v i ), q)})),<label>(45)</label></formula><p>where AGG(•) is an aggregation function, which can be any permutation invariant operator such as averaging (SAGEmean) or max-pooling (SAGE-pool).</p><p>Note that SAGE can also be described using GCF. For simplicity, we describe SAGE-mean using GCF notation, and refer to <ref type="bibr" target="#b49">[50]</ref> for details regarding other aggregation schemes. Using our signal propagation framework, SAGEmean uses two patch learning functions with g 1 (W, D) = I being the identity, and g 2 (W, D) ∼ U norm (D −1 W, q), where U norm (•, q) indicates uniformly sampling q nonzero entries per row, followed by row normalization. Therefore, the second patch propagates information using neighborhood sampling, and the SAGE-mean layer is:</p><formula xml:id="formula_115">H +1 = σ(g 1 (W, D)H Θ 1 + g 2 (W, D)H Θ 2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Attention-based spatial methods</head><p>Attention mechanisms <ref type="bibr" target="#b109">[110]</ref> have been successfully used in language models, and are particularly useful when operating on long sequence inputs, they allow models to identify relevant parts of the inputs. Similar ideas have been applied to graph convolution networks. Graph attention-based models learn to pay attention to important neighbors during the the propagation. This provides more flexibility in inductive settings, compared to methods that rely on fixed weights such as GCNs.</p><p>Broadly speaking, attention methods learn neighbors' importance using parametric functions whose inputs are node features at the previous layer. Using GCF, we can abstract patch functions in attention-based methods as functions of the form:</p><formula xml:id="formula_116">f k (W, H ) = α(W • g k (H )),</formula><p>where • indicates element-wise multiplication and α(•) is an activation function such as softmax or ReLU.</p><p>Graph attention networks Graph attention networks (GAT) <ref type="bibr" target="#b110">[111]</ref> are an attention-based version of GCNs, which incorporate self-attention mechanisms when computing patches. At every layer, GAT attends over the neighborhood of each node and learns to selectively pick nodes which lead to the best performance for some downstream task. The highlevel intuition is similar to SAGE <ref type="bibr" target="#b49">[50]</ref> and makes GAT suitable for inductive and transductive problems. However, instead of limiting the convolution step to fixed size-neighborhoods as in SAGE, GAT allows each node to attend over the entirety of its neighbors and uses attention to assign different weights to different nodes in a neighborhood. The attention parameters are trained through backpropagation, and the GAT self-attention mechanism is:</p><formula xml:id="formula_117">g k (H ) = LeakyReLU(H B b 0 ⊕ b 1 BH )</formula><p>where ⊕ indicates summation of row and column vectors with broadcasting, and (b 0 , b 1 ) and B are trainable attention weight vectors and weight matrix respectively. The edge scores are then row normalized with softmax. In practice, the authors propose to use multi-headed attention and combine the propagated signals with a concatenation of the average operator followed by some activation function. GAT can be implemented efficiently by computing the self-attention scores in parallel across edges, as well as the computing the output features in parallel across nodes.</p><p>Mixture model networks <ref type="bibr">Monti et al.</ref> propose MoNet <ref type="bibr" target="#b80">[81]</ref>, a general framework that works particularly well when the node features lie in multiple domains such as 3D point clouds or meshes. MoNet can be interpreted as an attention method as it learns patches using parametric functions in a pre-defined spatial domain (e.g. spatial coordinates), and then applies convolution filters in the graph domain.</p><p>Note that MoNet is a generalization of previous spatial approaches such as Geodesic CNN (GCNN) <ref type="bibr" target="#b77">[78]</ref> and Anisotropic CNN (ACNN) <ref type="bibr" target="#b12">[13]</ref>, which both introduced constructions for convolution layers on manifolds. However, both GCNN and ACNN use fixed patches that are defined on a specific coordinate system and therefore cannot generalize to graph-structured data. The MoNet framework is more general; any pseudo-coordinates such as local graph features (e.g. vertex degree) or manifold features (e.g. 3D spatial coordinates) can be used to compute the patches. More specifically, if U s are pseudo-coordinates and H are features from another domain, then using GCF, the MoNet layer can be expressed as:</p><formula xml:id="formula_118">H +1 = σ K k=1 (W • g k (U s ))H Θ k , (<label>46</label></formula><formula xml:id="formula_119">)</formula><p>where • is element-wise multiplication and g k (U s ) are the learned parametric patches, which are |V | × |V | matrices.</p><p>In practice, MoNet uses Gaussian kernels to learn patches, such that:</p><formula xml:id="formula_120">g k (U s ) = exp − 1 2 (U s − µ k ) Σ −1 k (U s − µ k ) ,</formula><p>where µ k and Σ k are learned parameters, and Monti et al. restrict Σ k to be a diagonal matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Non-Euclidean Graph Convolutions</head><p>Hyperbolic shallow embeddings enable embeddings of hierarchical graphs with smaller distortion than Euclidean embeddings. However, one major downside of shallow embeddings is that they are inherently transductive and cannot generalize to new graphs. On the other hand, Graph Neural Networks, which leverage node features, have achieved state-of-the-art performance on inductive graph embedding tasks.</p><p>Recently, there has been interest in extending Graph Neural Networks to learn non-Euclidean embeddings and thus benefit from both the expressiveness of Graph Neural Networks and hyperbolic geometry. One major challenge in doing so is how to perform convolutions in a non-Euclidean space, where standard operations such as inner products and matrix multiplications are not defined.</p><p>Hyperbolic Graph Convolutional Neural Networks Hyperbolic Graph Convolution Networks (HGCN) <ref type="bibr" target="#b21">[22]</ref> and Hyperbolic Graph Neural Networks (HGNN) <ref type="bibr" target="#b74">[75]</ref> apply graph convolutions in hyperbolic space by leveraging the Euclidean tangent space, which provides a first-order approximation of the hyperbolic manifold at a point. For every graph convolution step, node embeddings are mapped to the Euclidean tangent space at the origin, where convolutions are applied, and then mapped back to the hyperbolic space. These approaches yield significant improvements on graphs that exhibit hierarchical structure (Figure <ref type="figure" target="#fig_9">13</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Applications</head><p>Graph representation learning methods can be applied to a wide range of applications, which can be supervised or unsupervised. In supervised applications, node embeddings are optimized for some specific task, such as classifying nodes or graphs. In this setting, supervised embedding methods (Section 5, lower branch of the Taxonomy in Figure <ref type="figure">3</ref>) can be applied. On the other hand, in unsupervised applications, the goal is to learn embeddings that preserve the graph structure and unsupervised supervised embedding methods (Section 4, upper branch of the Taxonomy in Figure <ref type="figure">3</ref>) can be applied. We review common supervised and unsupervised graph applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Unsupervised applications 6.1.1 Graph reconstruction</head><p>The most standard unsupervised graph application is graph reconstruction. In this setting, the goal is to learn mapping functions (which can be parametric or not) that map nodes to dense distributed representations which preserve graph properties such as node similarity. Graph reconstruction doesn't require any supervision and models can be trained by minimizing a reconstruction error, which is the error in recovering the original graph from learned embeddings. Several algorithms were designed specifically for this task, and we refer to Section 4 for some examples of reconstruction objectives. At a high level, graph reconstruction is similar to PCA in the sense that the main goal is summarize some input data. Instead of compressing high dimensional vectors into low-dimensional ones as PCA does, the goal of graph reconstruction models is to compress data defined on graphs into low-dimensional vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Link prediction</head><p>Link prediction is the task of predicting links in a graph. In other words, the goal in link prediction tasks is to predict missing or unobserved links (e.g. links that may appear in the future for dynamic and temporal networks). Link prediction can also help identifying spurious link and remove them. It is a major application of graph learning models in industry, and common example of applications include predicting friendships in social networks or predicting userproduct interactions in recommendation systems.</p><p>A common approach for training link prediction models is to mask some edges in the graph (positive and negative edges), train a model with the remaining edges and then test it on the masked set of edges. Note that link prediction is different from graph reconstruction. In link prediction, we aim at predicting links that are not observed in the original graph while in graph reconstruction, we only want to compute embeddings that preserve the graph structure through reconstruction error minimization.</p><p>Finally, while link prediction has similarities with supervised tasks in the sense that we have labels for edges (positive, negative, unobserved), we group it under the unsupervised class of applications since edge labels are usually not used during training, but only used to measure the predictive quality of embeddings. That is, models described in Section 4 can be applied to the link prediction problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Clustering</head><p>Clustering is particularly useful for discovering communities and has many real-world applications. For instance, clusters exist in biological networks (e.g. as groups of proteins with similar properties), or in social networks (e.g. as groups of people with similar interests).</p><p>Note that unsupervised methods introduced in this survey can be used to solve clustering problems: one can run a clustering algorithm (e.g. k-means) on embeddings that are output by an encoder. Further, clustering can be joined with the learning algorithm while learning a shallow <ref type="bibr" target="#b96">[97]</ref> or Graph Convolution <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> embedding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Visualization</head><p>There are many off-the-shelf tools for mapping graph nodes onto two-dimensional manifolds for the purpose of visualization. Visualizations allow network scientists to qualitatively understand graph properties, understand relationships between nodes or visualize node clusters. Among the popular tools are methods based on Force-Directed Layouts, with various web-app Javascript implementations.</p><p>Note that unsupervised graph embedding methods are also used for visualization purposes: by first training an encoder-decoder model (corresponding to a shallow embedding or graph convolution network), and then mapping every node representation onto a two-dimensional space using, t-distributed stochastic neighbor embeddings (t-SNE) <ref type="bibr" target="#b76">[77]</ref> or PCA <ref type="bibr" target="#b57">[58]</ref>. Such a process (embedding → dimensionality reduction) is commonly used to qualitatively evaluate the performance of graph learning algorithms. If nodes have attributes, one can use these attributes to color the nodes on 2D visualization plots. Good embedding algorithms embed nodes that have similar attributes nearby in the embedding space, as demonstrated in visualizations of various methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b89">90]</ref>. Finally, beyond mapping every node to a 2D coordinate, methods which map every graph to a representation <ref type="bibr" target="#b4">[5]</ref> can similarly be projected into two dimensions to visualize and qualitatively analyze graph-level properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Supervised applications 6.2.1 Node classification</head><p>Node classification is an important supervised graph application, where the goal is to learn node representations that can accurately predict node labels. For instance, node labels could be scientific topics in citation networks, or gender and other attributes in social networks.</p><p>Since labelling large graphs can be time-consuming and expensive, semi-supervised node classification is a particularly common application. In semi-supervised settings, only a small fraction of nodes is labelled and the goal is to leverage links between nodes to predict attributes of unlabelled nodes. This setting is transductive since there is only one partially labelled fixed graph. It is also possible to do inductive node classification, which corresponds to the task of classifying nodes in multiple graphs.</p><p>Note that node features can significantly boost the performance on node classification tasks if these are descriptive for the target label. Indeed, recent methods such as GCN <ref type="bibr" target="#b60">[61]</ref> or GraphSAGE <ref type="bibr" target="#b49">[50]</ref> have achieved state-of-the-art performance on multiple node classification benchmarks due to their ability to combine structural information and semantics coming from features. On the other hand, other methods such as random walk methods fail to leverage feature information and therefore achieve lower performance on these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Graph classification</head><p>Graph classification is a supervised application where the goal is to predict graph labels. Graph classification problems are inductive and a common example is classifying chemical compounds.</p><p>Graph classification is a particularly challenging task because it requires some notion of pooling, in order to aggregate node-level information into graph-level information. As discussed earlier, generalizing this notion of pooling to arbitrary graphs is non trivial because of the lack of regularity in the graph structure making graph pooling an open research question. In addition to the supervised methods discussed above, a number of unsupervised methods for learning graph-level representations have been proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b108">109]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Open Research Directions</head><p>In this survey, we introduced a unified framework to compare machine learning models for graph-structured data. We presented a generalized GRAPHEDM framework, previously applied to unsupervised network embedding, that encapsulates shallow graph embedding methods, graph auto-encoders, graph regularization methods and graph neural networks. We also introduced a graph convolution framework (GCF), which is used to describe and compare convolution-based graph neural networks, including spatial and spectral graph convolutions. Using this framework, we introduced a comprehensive taxonomy of graph representation learning methods, encapsulating over thirty methods for graph embedding (both supervised and unsupervised).</p><p>We hope that this survey will help and encourage future research in graph representation learning, to hopefully solve the challenges that these models are currently facing. In particular, practitioners can reference the taxonomy to better understand the available tools and applications, and easily identify the best method for a given problem. Additionally, researchers with new research questions can use the taxonomy to better classify their research questions, reference the existing work, identify the right baselines to compare to, and find the appropriate tools to answer their questions.</p><p>While graph representation learning methods have achieved state-of-the-art performance on node classification or link prediction tasks, many challenges remain unsolved. Next, we discuss ongoing research directions and challenges that graph embedding models are facing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation and benchmarks</head><p>The methods covered in this survey are typically evaluated using standard node classification or link prediction benchmarks. For instance, citation networks are very often used as benchmarks to evaluate graph embedding methods. However, these small citation benchmarks have drawbacks since results might significantly vary based on datasets' splits, or training procedures (e.g. early stopping), as shown in recent work <ref type="bibr" target="#b102">[103]</ref>.</p><p>To better advance graph representation learning methods, it is important to use robust and unified evaluation protocols, and evaluate these methods beyond small node classification and link prediction benchmarks. Recently, there has been progress in this direction and graph benchmarks with leaderboards have been proposed <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fairness in Graph Learning</head><p>The emerging field of Fairness in Machine Learning seeks to ensure that models avoid correlation between 'sensitive' features and the model's predicted output <ref type="bibr" target="#b78">[79]</ref>. These concerns can be especially relevant for graph learning problems, where we must also consider the correlation of the graph structure (the edges) in addition to the feature vectors of the nodes with the final output.</p><p>The most popular technique for adding fairness constraints to models relies on using adversarial learning to debias the model's predictions relative to the sensitive feature(s), and can be extended to graph representation learning <ref type="bibr" target="#b13">[14]</ref>. However, adversarial methods do not offer strong guarantees about the actual amount of bias removed. In addition, many debiasing methods may not be effective at the debiasing task in practice <ref type="bibr" target="#b42">[43]</ref>. Recent work in the area aims to provide provable guarantees for debiasing graph representation learning <ref type="bibr" target="#b86">[87]</ref>.</p><p>Application to large and realistic graphs Most learning methods on graphs are applied only on smaller datasets, with sizes of up to hundred of thousands of nodes. However, many real-world graphs are much larger, containing up to billions of nodes. Methods that scale for large graphs <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b118">119]</ref> require a Distributed Systems setup with many machines, such as MapReduce <ref type="bibr" target="#b30">[31]</ref>. Given a large graph that fits on a single hard disk (e.g. with one terabyte size) but does not fit on RAM, how can a researcher apply a learning method on such a large graphs, using just a personal computer? Contrast this with a computer vision task by considering a large image dataset <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b66">67]</ref>. It is possible to train such models on personal computers, as long as the model can fit on RAM, regardless how large the dataset is. This problem may be particularly challenging for graph embedding models, especially those which have parameters that scale with the number of nodes in the graph.</p><p>We foresee engineering and mathematical challenges in learning methods for large graphs, while still being operable on a single machine. We hope that researchers can focus on this direction to expose such learning tools to non-expert practitioners, such as a Neurologist wishing to analyze the sub-graph of the human brain given its neurons and synapses, stored as nodes and edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Molecule generation</head><p>Learning on graphs has a great potential for helping molecular scientists to reduce cost and time in the laboratory. Researchers proposed methods for predicting quantum properties of molecules <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref> and for generating molecules with some desired properties <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b120">121]</ref>. A review of recent methods can be found in <ref type="bibr" target="#b35">[36]</ref>. Many of these methods are concerned with manufacturing materials with certain properties (e.g. conductance and malleability), and others are concerned drug design <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b94">95]</ref>.</p><p>Combinatorial optimization Computationally hard problems arise in a broad range of areas including routing science, cryptography, decision making and planning. Broadly speaking, a problem is computationally hard when the algorithms that compute the optimal solution scale poorly with the problem size. Many hard problems (e.g. SAT, vertex cover...) can be expressed in terms of graphs and recently, there has been interest <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b101">102]</ref> in leveraging graph embeddings to approximate solutions of NP-hard problems. More concretely, these methods tackle computationally hard problems from a data-driven perspective, where given multiple instances of a problem, the task is to predict whether a particular instance (e.g. node) belongs to the optimal solution. One motivation for these approaches is the relational inductive biases found in GNNs which enable them to better represent graphs compared to standard neural networks (e.g. permutation invariance). While these data-driven methods are still outperformed by existing solvers, promising results show that GNNs can generalize to larger problem instances <ref type="bibr" target="#b84">[85]</ref>.</p><p>Non-Euclidean embeddings As we saw in Sections 4.1.2 and 5.6, an important aspect of graph embeddings is the underlying space geometry. Graphs are discrete, high-dimensional, non-Euclidean structures, and there is no straightforward way to encode this information into low-dimensional Euclidean embeddings that preserve the graph topology <ref type="bibr" target="#b14">[15]</ref>. Recently, there has been interest and progress into learning non-Euclidean embeddings such as hyperbolic <ref type="bibr" target="#b82">[83]</ref> or mixed-product space <ref type="bibr" target="#b48">[49]</ref> embeddings. These non-Euclidean embeddings provide a promise for more expressive embeddings, compared to Euclidean embeddings. For instance, hyperbolic embeddings can represent hierarchical graphs with much smaller distortion than Euclidean embeddings <ref type="bibr" target="#b98">[99]</ref>.</p><p>Two common challenges that arise with non-Euclidean embeddings are the limited machine precision <ref type="bibr" target="#b97">[98]</ref> and challenging Riemannian optimization <ref type="bibr" target="#b11">[12]</ref>. Additionally, it is also unclear how to pick the right geometry for a given input graph. While there exists some discrete measures of graph curvature (e.g. Gromov's four-point condition <ref type="bibr" target="#b58">[59]</ref>), an interesting open research direction is how to pick or learn the right geometry for a given discrete graph.</p><p>Theoretical guarantees There have been significant advances in the design of graph embedding models, which improved over the state-of-the-art in many applications. However, there is still limited understanding about theoretical guarantees and limitations of graph embedding models. Understanding the representational power of GNNs is a nascent area of research, and recent works adapt existing results from learning theory to the problem of graph representation learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b116">117]</ref>. The development of theoretical frameworks is critical to pursue in order to understand the theoretical guarantees and limitations of graph embedding methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Grid (Euclidean). (b) Arbitrary graph (Non-Euclidean).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of Euclidean vs. non-Euclidean graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 2 . 1 .</head><label>21</label><figDesc>(Graph). A graph G given as a pair: G = (V, E), comprises a set of vertices (or nodes) V = {v 1 , . . . , v |V | } connected by edges E = {e 1 , . . . , e |E| }, where each edge e k is a pair</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An overview of the pipeline for random-walk graph embedding methods. Reprinted with permission from [42].</figDesc><graphic url="image-1.png" coords="13,72.00,72.86,467.96,82.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Auto-encoder methods. The graph structure (stored as the graph adjacency matrix) is encoded and reconstructed using encoder-decoder networks. Models are trained by optimizing the graph regularization loss computed on the reconstructed adjacency matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Unsupervised graph neural networks. Graph structure and input features are mapped to low dimensional embeddings using a graph neural network encoder. Embeddings are then decoded to compute a graph regularization loss (unsupervised).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Supervised graph regularization methods. The graph structure is not used in the encoder nor the decoder networks. It instead acts as a regularizer in the loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Illustration of the GraphSAGE model. Reprinted with permission from [50].</figDesc><graphic url="image-4.png" coords="26,118.80,72.86,374.40,131.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: Euclidean (left) and hyperbolic (right) embeddings of a tree graph. Hyperbolic embeddings learn natural hierarchies in the embedding space (depth indicated by color). Reprinted with permission from<ref type="bibr" target="#b21">[22]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Definition 2.3. (Distance). Given two nodes (u, v) in a graph G, we define the distance from u to v, denoted d G (u, v), to be the length of the shortest path from u to v, or ∞ if there exist no path from u to v.The distance between two nodes is the analog of geodesic lines on manifolds for graphs. Definition 2.4. (Vertex degree). The degree, deg(v i ), of a vertex v i in an unweighted graph is the number of edges incident to it. Similarly, the degree of a vertex v i in a weighted graph is the sum of incident edges weights. The degree matrix D of a graph with vertex set V is the|V | × |V | diagonal matrix such that D ii = deg(v i ).Definition 2.5. (Adjacency matrix). A finite graph G = (V, E) can be represented as a square |V | × |V | adjacency matrix, where the elements of the matrix indicate whether pairs of nodes are adjacent or not. The adjacency matrix is binary for unweighted graph, A ∈ {0, 1} |V |×|V | , and non-binary for weighted graphs W ∈ R |V |×|V | . Undirected graphs have symmetric adjacency matrices, in which case, W denotes symmetrically-normalized adjacency matrix:</figDesc><table /><note>the relationships are symmetric, and directed if the existence of edge (v i , v j ) ∈ E does not necessarily imply (v j , v i ) ∈ E. Finally, a graph can be homogeneous if nodes refer to one type of entity and edges to one relationship. It can be heterogeneous if it contains different types of nodes and edges.For instance, social networks are homogeneous graphs that can be undirected (e.g. to encode symmetric relations like friendship) or directed (e.g. to encode the relation following); weighted (e.g. co-activities) or unweighted. Definition 2.2. (Path). A path P is a sequence of edges (u i1 , u i2 ), (u i2 , u i3 ), . . . , (u i k , u i k+1 ) of length k. A path is called simple if all u ij are distinct from each other. Otherwise, if a path visits a node more than once, it is said to contain a cycle.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Edge-level supervision, with y E ∈ Y |V |×|V | , where Y represents the edge label space. For example, Y can be multinomial in knowledge graphs (for describing the types of relationships between two entities), setting Y = {0, 1} #(relation types) . It is common to have #(relation types) = 1, and this is is known as link prediction, where edge relations are binary. In this review, when y E = {0, 1} |V |×|V | (i.e. Y = {0, 1}</figDesc><table><row><cell>• Node-level supervision, with y N ∈ Y |V | , where Y represents the node label space. If Y is categorical, then this</cell></row><row><cell>is also known as (semi-)supervised node classification (Section 6.2.1), in which case the label decoder network</cell></row><row><cell>produces labels for each node in the graph. If d-dimensional Z is such that d = |Y|, then the label decoder net-</cell></row><row><cell>work can be just a simple softmax activation across the rows of Z, producing a distribution over labels for each</cell></row><row><cell>node. Additionally, the graph decoder network might also be leveraged in supervised node-classification tasks,</cell></row><row><cell>as it can be used to regularize embeddings (e.g. neighbor nodes should have nearby embeddings, regardless of</cell></row><row><cell>node labels).</cell></row><row><cell>•</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>An overview of unsupervised shallow embedding methods, where the encoding function is a simple embedding look-up Z = ENC(Θ E ). Softmax represents sampled/hierarchical softmax; ∝ for approximating random walks; n2vWalk is a traversal algorithm with (back) teleportation (approximates combination of BFS &amp; DFS). BCE is the sigmoid cross entropy loss for binary classification.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">As defined by the eigenspace of the graph Laplacian.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Although we present the model taxonomy via embedding nodes yielding Z ∈ R |V |×d , it can also be extended for models that embed an entire graph i.e. with Z ∈ R d as a d-dimensional vector for the whole graph (e.g.[5,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="34" xml:id="foot_2">]), or embed graph edges Z ∈ R |V |×|V |×d as a (potentially sparse) 3D matrix with Zu,v ∈ R d representing the embedding of edge (u, v).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">There are two kinds of samples: (i.) drawn by conducting random walks, similar to DeepWalk, with i, j ∈ V are co-visited within the windowsize hyperparameter in a simulated random walk; and (ii.) samples are drawn from the same class i.e. y i = y j . These samples are positive i.e. with γ = 1. The negative samples simply replace one of the nodes with another randomly-sampled (negative) node yielding γ = −1. The ratio of these kinds of samples are determined by hyperparameters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Meissane Chami, Aram Galstyan, Megan Leszczynski, John Palowitch, Laurel Orr, and Nimit Sohoni for their helpful feedback and discussions. We gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, the HAI-AWS Cloud Credits for Research program, TOTAL, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning edge representations via low-rank asymmetric projections</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">17871796</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Watch your step: Learning node embeddings via graph attention</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9180" to="9190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed largescale natural graph factorization</title>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
				<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ddgk: Learning graph representations for deep divergence graph kernels</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 World Wide Web Conference on World Wide Web</title>
				<meeting>the 2019 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A learning rule for asynchronous perceptrons with feedback in a combinatorial environment</title>
		<author>
			<persName><surname>Luis B Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, 1st First International Conference on Neural Networks</title>
				<meeting>1st First International Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="609" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent on riemannian manifolds</title>
		<author>
			<persName><forename type="first">Silvere</forename><surname>Bonnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2217" to="2229" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Compositional fairness constraints for graph embeddings</title>
		<author>
			<persName><forename type="first">Joey</forename><surname>Avishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10674</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral networks and locally connected networks on graphs international conference on learning representations</title>
				<imprint>
			<date type="published" when="2014-04">iclr2014. April, 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural graph learning: Training neural networks using graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><surname>Ramavajjala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: problems, techniques and applications</title>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deisenroth</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10359</idno>
		<title level="m">Neural embeddings of graphs in hyperbolic space</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4869" to="4880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised learning (chapelle, o</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006">2006. 2009</date>
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02590</idno>
		<title level="m">A tutorial on network embeddings</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Harp: Hierarchical representation learning for networks</title>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhanced network embeddings via exploiting edge labels</title>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">15791582</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><forename type="middle">Bruna</forename><surname>Estrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="page">107113</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning for molecular designa review of the state of the art</title>
		<author>
			<persName><forename type="first">Zois</forename><surname>Daniel C Elton</surname></persName>
		</author>
		<author>
			<persName><surname>Boukouvalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Mark D Fuge</surname></persName>
		</author>
		<author>
			<persName><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Systems Design &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="828" to="849" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ego-splitting framework: From non-overlapping to overlapping clusters</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Epasto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Lattanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><surname>Paes Leme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">145154</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Is a single embedding enough? learning node representations that capture multiple social contexts</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Epasto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference, WWW 19</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">394404</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06157</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Primo</forename><surname>Godec</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/graph-embeddings-the-summary-cc6075aba007" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03862</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
				<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gem: a python package for graph embedding methods</title>
		<author>
			<persName><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">876</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Graph embedding techniques, applications, and performance: A survey. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="78" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graphite: Iterative generative modeling of graphs</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2434" to="2444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning mixed-curvature representations in product spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>David K Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph embedding with personalized context distribution</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the Web Conference</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">655661</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International encyclopedia of statistical science</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1094" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scaled gromov hyperbolic graphs</title>
		<author>
			<persName><forename type="first">Edmond</forename><surname>Jonckheere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poonsuk</forename><surname>Lohsoonthorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bonahon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graph Theory</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bistra</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6348" to="6358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Geographic routing using hyperbolic space</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2007-26th IEEE International Conference on Computer Communications</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1902" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On social networks and collaborative recommendation</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilios</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marián</forename><surname>Boguná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis</title>
		<author>
			<persName><surname>Joseph B Kruskal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">PyTorch-BigGraph: A Large-scale Graph Embedding System</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference</title>
				<meeting>the 2nd SysML Conference<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7795" to="7804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8228" to="8239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03199</idno>
		<title level="m">What graph neural networks cannot learn: depth vs width</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
				<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09635</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><surname>Michael M Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning continuous hierarchies in the lorentz model of hyperbolic geometry</title>
		<author>
			<persName><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3779" to="3788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Revised note on learning algorithms for quadratic assignment with graph neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afonso</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07450</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Monet: Debiasing graph embeddings via the metadata-orthogonal training unit</title>
		<author>
			<persName><forename type="first">John</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11793</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Popularity versus similarity in growing networks</title>
		<author>
			<persName><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ángeles</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marián</forename><surname>Boguná</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">489</biblScope>
			<biblScope unit="issue">7417</biblScope>
			<biblScope unit="page" from="537" to="540" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Generalization of back propagation to recurrent and higher order neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems</title>
				<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="602" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Padme: A deep learning-based framework for drug-target interaction prediction</title>
		<author>
			<persName><forename type="first">Artem</forename><surname>Cherkasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgenia</forename><surname>Dueva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09741</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Netsmf: Large-scale network embedding as sparse matrix factorization</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference, WWW 19</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">15091520</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Proteinligand scoring with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Hochuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Idrobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jocelyn</forename><surname>Sunseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koes</forename></persName>
		</author>
		<idno type="PMID">28368587</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="942" to="957" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Gemsec: Graph embedding with self clustering</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 19</title>
				<meeting>the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6572</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Representation tradeoffs for hyperbolic embeddings</title>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4460" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Low distortion delaunay embedding of trees in hyperbolic plane</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Graph Drawing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Bünz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03685</idno>
		<title level="m">Learning a sat solver from single-bit supervision</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03480</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Biogrid: a general repository for interaction datasets</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby-Joe</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Reguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrie</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashton</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="D535" to="D539" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Netlsd: Hearing the shape of a graph</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">23472356</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Just slaq when you approximate: Accurate spectral distances for web-scale graphs</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Munkhoeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">26972703</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR. org</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08773</idno>
		<title level="m">Graphrnn: A deep generative model for graphs</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Network representation learning: A survey</title>
		<author>
			<persName><forename type="first">Daokun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Thomas N Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
