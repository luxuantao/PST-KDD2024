<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University ‡ Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
							<email>lijian83@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
							<email>kuansanw@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University ‡ Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3159652.3159706</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the invention of word2vec <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b30">29]</ref>, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk [31] empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE <ref type="bibr" target="#b38">[37]</ref>, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE <ref type="bibr" target="#b37">[36]</ref> can be viewed as the joint factorization of multiple networks' Laplacians; (4) node2vec <ref type="bibr" target="#b17">[16]</ref> is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method 1 as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The conventional paradigm of mining and learning with networks usually starts from the explicit exploration of their structural properties <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b33">32]</ref>. But many of such properties, such as betweenness Table <ref type="table">1</ref>: The matrices that are implicitly approximated and factorized by DeepWalk, LINE, PTE, and node2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Matrix</head><p>DeepWalk log vol(G) 1</p><formula xml:id="formula_0">T T r =1 (D −1 A) r D −1 − log b LINE log vol(G)D −1 AD −1 − log b PTE log       α vol(G ww )(D ww row ) −1 A ww (D ww col ) −1 β vol(G dw )(D dw row ) −1 A dw (D dw col ) −1 γ vol(G lw )(D lw row ) −1 A lw (D lw col ) −1       − log b node2vec log 1 2T</formula><p>T r =1 u Xw,u P r c,w,u + u Xc,u P r w,c,u</p><formula xml:id="formula_1">( u Xw,u )( u Xc,u ) − log b</formula><p>Notations in DeepWalk and LINE are introduced below. See detailed notations for PTE and node2vec in Section 2.</p><p>A: A ∈ R</p><p>|V |×|V | + is G's adjacency matrix with A i, j as the edge weight between vertices i and j; D col : D col = diag(A ⊤ e) is the diagonal matrix with column sum of A; Drow: Drow = diag(Ae) is the diagonal matrix with row sum of A; D : For undirected graphs (A ⊤ = A), D col = Drow. centrality, triangle count, and modularity, require extensive domain knowledge and expensive computation to handcraft. In light of these issues, as well as the opportunities offered by the recent emergence of representation learning <ref type="bibr" target="#b3">[2]</ref>, learning latent representations for networks, a.k.a., network embedding, has been extensively studied in order to automatically discover and map a network's structural properties into a latent space. Formally, the problem of network embedding is often formalized as follows: Given an undirected and weighted graph G = (V , E, A) with V as the node set, E as the edge set and A as the adjacency matrix, the goal is to learn a function V → R d that maps each vertex to a d-dimensional (d ≪ |V |) vector that captures its structural properties. The output representations can be used as the input of mining and learning algorithms for a variety of network science tasks, such as label classification and community detection.</p><p>The attempt to address this problem can date back to spectral graph theory <ref type="bibr" target="#b12">[11]</ref> and social dimension learning <ref type="bibr" target="#b39">[38]</ref>. Its very recent advances have been largely influenced by the skip-gram model originally proposed for word embedding <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b30">29]</ref>, whose input is a text corpus composed of sentences in natural language and output is the latent vector representation for each word in the corpus. Notably, inspired by this setting, DeepWalk <ref type="bibr" target="#b32">[31]</ref> pioneers network embedding by considering the vertex paths traversed by random walks over networks as the sentences and leveraging skipgram for learning latent vertex representations. With the advent of DeepWalk, many network embedding models have been developed, such as LINE <ref type="bibr" target="#b38">[37]</ref>, PTE <ref type="bibr" target="#b37">[36]</ref>, and node2vec <ref type="bibr" target="#b17">[16]</ref>.</p><p>The above models have thus far been demonstrated quite effective empirically. However, the theoretical mechanism behind them is much less well-understood. We note that the skip-gram model with negative sampling for word embedding has been shown to be an implicit factorization of a certain word-context matrix <ref type="bibr" target="#b25">[24]</ref>, and there is recent effort to theoretically explaining the word embedding models from geometric perspectives <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b19">18]</ref>. But it is unclear what is the relation between the word-context matrix and the network structure. Moreover, there was an early attempt to theoretically analyze DeepWalk's behavior <ref type="bibr" target="#b48">[47]</ref>. However, their main theoretical results are not fully consistent with the setting of the original DeepWalk paper. In addition, despite the superficial similarity among DeepWalk, LINE, PTE, and node2vec, there is a lack of deeper understanding of their underlying connections. Contributions In this work, we provide theoretical results concerning several skip-gram powered network embedding methods. More concretely, we first show that the models we mentioned-DeepWalk, LINE, PTE, and node2vec-are in theory performing implicit matrix factorizations. We derive the closed form of the matrix for each model (see Table <ref type="table">1</ref> for a summary). For example, DeepWalk (random walks on a graph + skip-gram) is in essence factorizing a random matrix that converges in probability to our closed-form matrix as the length of random walks goes to infinity.</p><p>Second, observed from their matrices' closed forms, we find that, interestingly, LINE can be seen as a special case of DeepWalk, when the window size T of contexts is set to 1 in skip-gram. Furthermore, we demonstrate that PTE, as an extension of LINE, is actually an implicit factorization of the joint matrix of multiple networks.</p><p>Third, we discover a theoretical connection between DeepWalk's implicit matrix and graph Laplacians. Based on the connection, we propose a new algorithm NetMF to approximate the closed form of DeepWalk's implicit matrix. By explicitly factorizing this matrix using SVD, our extensive experiments in four networks (used in the DeepWalk and node2vec approaches) demonstrate NetMF's outstanding performance (relative improvements by up to 50%) over DeepWalk and LINE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THEORETICAL ANALYSIS AND PROOFS</head><p>In this section, we present the detailed theoretical analysis and proofs for four popular network embedding approaches: LINE, PTE, DeepWalk, and node2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LINE and PTE</head><p>LINE <ref type="bibr" target="#b38">[37]</ref> Given an undirected and weighted network G = (V , E, A), LINE with the second order proximity (aka LINE (2nd)) aims to learn two representation matrices X , Y ∈ R |V |×d , whose rows are denoted by x i and y</p><formula xml:id="formula_2">i , i = 1, • • • , |V |, respectively. The objective of LINE (2nd) is to maximize ℓ = |V | i =1 |V | j=1 A i, j log д x ⊤ i y j + bE j ′ ∼P N log д −x ⊤ i y j ′ ,</formula><p>where д is the sigmoid function; b is the parameter for negative sampling; P N is known as the noise distribution that generates negative samples. In LINE, the authors empirically set P N (j) ∝ d</p><formula xml:id="formula_3">3/4 j ,</formula><p>where</p><formula xml:id="formula_4">d j = |V | k =1 A j,k</formula><p>is the generalized degree of vertex j. In our analysis, we assume P N (j) ∝ d j because the normalization factor has a closed form solution in graph theory, i.e., P N (j) = d j /vol(G), where vol(G) =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|V | i=1</head><p>|V | j=1 A i, j . <ref type="foot" target="#foot_0">2</ref> Then we rewrite the objective as</p><formula xml:id="formula_5">ℓ = |V | i =1 |V | j=1 A i, j log д x ⊤ i y j + b |V | i =1 d i E j ′ ∼P N log д −x ⊤ i y j ′ ,<label>(1)</label></formula><p>and express the expectation term E j ′ ∼P N log д −x ⊤ i y j ′ as</p><formula xml:id="formula_6">d j vol(G) log д −x ⊤ i y j + j ′ j d j ′ vol(G) log д −x ⊤ i y j ′ .<label>(2)</label></formula><p>By combining Eq. 1 and Eq. 2, and considering the local objective function for a specific pair of vertices (i, j), we have</p><formula xml:id="formula_7">ℓ(i, j) = A i, j log д x ⊤ i y j + b d i d j vol(G) log д −x ⊤ i y j .</formula><p>Let us define z i, j = x ⊤ i y j . Following Levy and Goldberg <ref type="bibr" target="#b25">[24]</ref>, where the authors suggested that for a sufficient large embedding dimension, each individual z i, j can assume a value independence, we can take the derivative w.r.t. z i, j and get:</p><formula xml:id="formula_8">∂ℓ ∂z i, j = ∂ℓ(i, j) ∂z i, j = A i, j д(−z i, j ) − b d i d j vol(G) д(z i, j ).</formula><p>Setting the derivative to be zero reveals</p><formula xml:id="formula_9">e 2z i, j − vol(G)A i, j bd i d j − 1 e z i, j − vol(G)A i, j bd i d j = 0.</formula><p>The above quadratic equation has two solutions (1) e z i, j = −1, which is invalid; and (2) e z i, j = vol(G)A i, j bd i d j , i.e.,</p><p>x ⊤ i y j = z i, j = log vol(G)A i, j bd i d j .</p><p>(</p><formula xml:id="formula_10">)<label>3</label></formula><p>Writing Eq. 3 in matrix form, LINE (2nd) is factoring the matrix</p><formula xml:id="formula_11">log vol(G)D −1 AD −1 − log b = X Y ⊤ ,<label>(4)</label></formula><p>where log(•) denotes the element-wise matrix logarithm, and</p><formula xml:id="formula_12">D = diag(d 1 , • • • , d |V | ).</formula><p>PTE <ref type="bibr" target="#b37">[36]</ref> PTE is an extension of LINE (2nd) in heterogeneous text networks. To examine it, we first adapt our analysis of LINE (2nd) to bipartite networks. Consider a bipartite network</p><formula xml:id="formula_13">G = (V 1 ∪V 2 , E, A) where V 1 , V 2 are two disjoint sets of vertices, E ⊆ V 1 ×V 2 is the edge set, and A ∈ R |V 1 |× |V 2 | +</formula><p>is the bipartite adjacency matrix. The volume of G is defined to be vol(G)</p><formula xml:id="formula_14">= |V 1 | i=1 |V 2 | j=1 A i, j . The goal is to learn a representation x i for each vertex v i ∈ V 1 and a representation y j for each vertex v j ∈ V 2 . The objective function is ℓ = |V 1 | i =1 |V 2 | j =1 A i, j log д x ⊤ i y j + bE j ′ ∼P N log д −x ⊤ i y j ′ .</formula><p>Applying the same analysis procedure of LINE, we can see that maximizing ℓ is actually factorizing</p><formula xml:id="formula_15">log vol(G)D −1 row AD −1 col − log b = X Y ⊤</formula><p>where we denote D row = diag(Ae) and D col = diag(A ⊤ e).</p><p>Given the above discussion, let us consider the heterogeneous text network used in PTE, which is composed of three sub-networks -the word-word network G ww , the document-word network G dw , and the label-word network G lw , where G dw and G lw are bipartite. Take the document-word network G dw as an example, we use A dw ∈ R #doc×#word to denote its adjacency matrix, and use D dw row and D dw col to denote its diagonal matrices with row and column sum, respectively. By leveraging the analysis of LINE and the above notations, we find that PTE is factorizing log</p><formula xml:id="formula_16">      α vol(G ww )(D ww row ) −1 A ww (D ww col ) −1 β vol(G dw )(D dw row ) −1 A dw (D dw col ) −1 γ vol(G lw )(D lw row ) −1 A lw (D lw col ) −1       − log b,<label>(5)</label></formula><p>where the factorized matrix is of shape (#word + #doc + #label) × #word, b is the parameter for negative sampling, and {α, β, γ } are non-negative hyper-parameters to balance the weights of the three sub-networks. In PTE, {α,</p><formula xml:id="formula_17">β, γ } satisfy α vol(G ww ) = β vol(G dw ) = γ vol(G lw )</formula><p>. This is because the authors perform edge sampling during training wherein edges are sampled from each of three sub-networks alternatively (see Section 4.2 in <ref type="bibr" target="#b37">[36]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DeepWalk</head><p>In this section, we analyze DeepWalk <ref type="bibr" target="#b32">[31]</ref> and illustrate the essence of DeepWalk is actually performing an implicit matrix factorization (See the matrix form solution in Thm. 2.3). DeepWalk first generates a "corpus" D by random walks on graphs <ref type="bibr" target="#b27">[26]</ref>. To be formal, the corpus D is a multiset that counts the multiplicity of vertex-context pairs. DeepWalk then trains a skip-gram model on the multiset D. In this work, we focus on skipgram with negative sampling (SGNS). For clarity, we summarize the DeepWalk method in Algorithm 1. The outer loop (Line 1-7) specifies the total number of times, N , for which we should run random walks. For each random walk of length L, the first vertex is sampled from a prior distribution P(w). The inner loop (Line 4-7) specifies the construction of the multiset D. Once we have D, we run an SGNS to attain the network embedding (Line 8). Next, we introduce some necessary background about the SGNS technique, followed by our analysis of the DeepWalk method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary on Skip-gram with Negative Sampling (SGNS)</head><p>The skip-gram model assumes a corpus of words w and their context c. More concretely, the words come from a textual corpus of words w 1 , • • • w L and the contexts for word w i are words surrounding it in a T -sized window</p><formula xml:id="formula_18">w i−T , • • • , w i−1 , w i+1 , • • • , w i+T .</formula><p>Following the work by Levy and Goldberg <ref type="bibr" target="#b25">[24]</ref>, SGNS is implicitly factorizing</p><formula xml:id="formula_19">log #(w, c) | D | #(w ) • #(c) − log b,<label>(6)</label></formula><p>where #(w, c), #(w) and #(c) denote the number of times wordcontext pair (w, c), word w and context c appear in the corpus, respectively; b is the number of negative samples.</p><p>Proofs and Analysis Our analysis of DeepWalk is based on the following key assumptions. Firstly, assume the used graph is undirected, connected, and non-bipartite, making P(w) = d w /vol(G) a unique stationary distribution of the random walks. Secondly, suppose the first vertex of a random walk is sampled from the stationary distribution P(w) = d w /vol(G).</p><p>To characterize DeepWalk, we want to reinterpret Eq. 6 by using graph theory terminologies. Our general idea is to partition the multiset D into several sub-multisets according to the way in which vertex and its context appear in a random walk sequence. More formally, for r = 1, • • • ,T , we define</p><formula xml:id="formula_20">D− → r = (w, c) : (w, c) ∈ D, w = w n j , c = w n j+r , D← − r = (w, c) : (w, c) ∈ D, w = w n j +r , c = w n j .</formula><p>That is, D− → r /D← − r is the sub-multiset of D such that the context c is r steps after/before the vertex w in random walks. Moreover, we use #(w, c)− → r and #(w, c)← − r to denote the number of times vertex-context pair (w, c) appears in D− → r and D← − r , respectively. The following three theorems characterize DeepWalk step by step. Theorem 2.1.</p><formula xml:id="formula_21">Denote P = D −1 A, when L → ∞, we have #(w, c)− → r D− → r p → d w vol(G) (P r ) w,c and #(w, c)← − r D← − r p → d c vol(G) (P r ) c,w .</formula><p>Proof. See Appendix. □ Remark 1. What if we start random walks with other distributions (e.g., the uniform distribution in the original DeepWalk work <ref type="bibr" target="#b32">[31]</ref>)? It turns out that, for a connected, undirected, and non-bipartite graph, P(w j = w, w j+r = c) → d w vol(G) (P r ) w,c as j → ∞. So when the length of random walks L → ∞, Thm. 2.1 still holds. Theorem 2.2. When L → ∞, we have</p><formula xml:id="formula_22">#(w, c) | D | p → 1 2T T r =1 d w vol(G) (P r ) w,c + d c vol(G) (P r ) c,w .</formula><p>Proof. Note the fact that</p><formula xml:id="formula_23">| D− → r | | D | = | D← − r | | D | = 1 2T</formula><p>. By using Thm. 2.1 and the continuous mapping theorem, we get</p><formula xml:id="formula_24">#(w, c) | D | = T r =1 #(w, c)− → r + #(w, c)← − r T r =1 D− → r + D← − r = 1 2T T r =1 #(w, c)− → r D− → r + #(w, c)← − r D← − r p → 1 2T T r =1 d w vol(G) (P r ) w,c + d c vol(G) (P r ) c,w .</formula><p>Further, marginalizing w and c respectively reveals the fact that</p><formula xml:id="formula_25">#(w ) | D | p → d w vol(G) and #(c) | D | p → d c vol(G) , as L → ∞. □ Theorem 2.3. For DeepWalk, when L → ∞, #(w, c) | D | #(w ) • #(c) p → vol(G) 2T 1 d c T r =1 (P r ) w,c + 1 d w T r =1 (P r ) c,w .</formula><p>In matrix form, DeepWalk is equivalent to factorize</p><formula xml:id="formula_26">log vol(G) T T r =1 P r D −1 − log(b).<label>(7)</label></formula><p>Proof. Using the results in Thm. 2.2 and the continuous mapping theorem, we get</p><formula xml:id="formula_27">#(w, c) | D | #(w ) • #(c) = #(w,c ) |D | #(w ) |D | • #(c ) |D | p → 1 2T T r =1 dw vol(G ) (P r ) w,c + dc vol(G) (P r ) c,w dw vol(G ) • dc vol(G ) = vol(G) 2T 1 d c T r =1 (P r ) w,c + 1 d w T r =1 (P r ) c,w .</formula><p>Write it in matrix form:</p><formula xml:id="formula_28">vol(G) 2T T r =1 P r D −1 + T r =1 D −1 (P r ) ⊤ = vol(G) 2T T r =1 D −1 A × • • • × D −1 A r terms D −1 + T r =1 D −1 AD −1 × • • • × AD −1 r terms = vol(G) T T r =1 D −1 A × • • • × D −1 A r terms D −1 = vol(G) 1 T T r =1 P r D −1 .</formula><p>□ Corollary 2.4. Comparing Eq. 4 and Eq. 7, we can easily observe that LINE (2nd) is a special case of DeepWalk when T = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">node2vec</head><p>node2vec <ref type="bibr" target="#b17">[16]</ref> is a recently proposed network embedding method, which is briefly summarized in Algorithm 2. First, it defines an unnormalized transition probability tensor T with parameters p and q, and then normalizes it to be the transition probability of a 2nd-order random walk (Line 1). Formally,</p><formula xml:id="formula_29">T u,v,w =                1 p (u, v) ∈ E, (v, w ) ∈ E, u = w ; 1 (u, v) ∈ E, (v, w ) ∈ E, u w, (w, u) ∈ E; 1 q (u, v) ∈ E, (v, w ) ∈ E, u w, (w, u) E; 0 otherwise. P u,v,w = Prob w j+1 = u |w j = v, w j−1 = w = T u,v,w u T u,v,w .</formula><p>Second, node2vec performs the 2nd-order random walks to generate a multiset D (Line 2-8) and then trains an SGNS model (Line 9) on it. To facilitate the analysis, we instead record triplets to form the multiset D for node2vec rather than vertex-context pairs. Take a vertex w = w n j and its context c = w n j+r as an example, we denote u = w n j−1 and add a triplet (w, c, u) into D (Line 7). Similar to our analysis of DeepWalk, we partition the multiset D according to the way in which the vertex and its context appear in a random walk sequence. More formally, for r = 1, • • • ,T , we define In this analysis, we assume the first two vertices of node2vec's 2nd-order random walk are sampled from its stationary distribution X . The stationary distribution X of the 2nd-order random walk satisfies w P u,v,w X v,w = X u,v , and the existence of such X is guaranteed by the Perron-Frobenius theorem <ref type="bibr" target="#b5">[4]</ref>. Additionally, the higher-order transition probability tensor is defined to be (P r ) u,v,w = Prob w j+r = u|w j = v, w j−1 = w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>Limited by space, we list the main results of node2vec without proofs. The idea is similar to the analysis of DeepWalk.</p><formula xml:id="formula_30">• #(w,c,u)− → r | D− → r | p → X w,u (P r ) c,w,u and #(w,c,u)← − r | D← − r | p → X c,u (P r ) w,c,u . • #(w,c)− → r | D− → r | = u #(w,c,u)− → r | D− → r | p → u X w,u (P r ) c,w,u . • #(w,c)← − r | D← − r | = u #(w,c,u)← − r | D← − r | p → u X c,u (P r ) w,c,u . • #(w,c) | D | p → 1 2T T r =1</formula><p>u X w,u (P r ) c,w,u + u X c,u (P r ) w,c,u .</p><p>•</p><formula xml:id="formula_31">#(w ) | D | p → u X w,u and #(c) | D | p → u X c,u .</formula><p>Combining all together, we conclude that, for node2vec, Eq. 6 has the form</p><formula xml:id="formula_32">#(w, c) | D | #(w ) • #(c) p → 1 2T T r =1 u X w,u P r c,w,u + u X c,u P r w,c,u u X w,u u X c,u .<label>(8)</label></formula><p>Though the closed form for node2vec has been achieved, we leave the formulation of its matrix form for future research. Note that both the computation and storage of the transition probability tensor P r and its corresponding stationary distribution X are very expensive, making the modeling of the full 2nd-order dynamics difficult. However, we have noticed some recent progresses <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b16">15]</ref> that try to understand or approximate the 2ndorder random walk by assuming a rank-one factorization X u,v = x u x v for its stationary distribution X . Due to the page limitation, in the rest of this paper, we mainly focus on the matrix factorization framework depending on the 1st-order random walk (DeepWalk).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NetMF: NETWORK EMBEDDING AS MATRIX FACTORIZATION</head><p>Based on the analysis in Section 2, we unify LINE, PTE, DeepWalk, and node2vec in the framework of matrix factorization, where the factorized matrices have closed forms as showed in Eq. 4, Eq. 5, Eq. 7, and Eq. 8, respectively. In this section, we study the DeepWalk matrix (Eq. 7) because it is more general than the LINE matrix and computationally more efficient than the node2vec matrix. We first discuss the connection between the DeepWalk matrix and graph Laplacian in Section 3.1. Then in Section 3.2, we present a matrix factorization based framework-NetMF-for network embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Connection between DeepWalk Matrix and Normalized Graph Laplacian</head><p>In this section, we show that the DeepWalk matrix has a close relationship with the normalized graph Laplacian. To facilitate our analysis, we introduce the following four theorems. </p><formula xml:id="formula_33">Quotient R(A, x) = (x ⊤ Ax)/(x ⊤ x) satisfies λ min (A) = min x 0 R(A, x) and λ max (A) = max x 0 R(A, x).</formula><p>By ignoring the element-wise matrix logarithm and the constant term in Eq. 7, we focus on studying the matrix 1 T T r =1 P r D −1 can be decomposed to be the product of there symmetric matrices:</p><formula xml:id="formula_34">1 T T r =1 P r D −1 = D −1/2 U 1 T T r =1 Λ r U ⊤ D −1/2 .<label>(9)</label></formula><p>The goal here is to characterize the spectrum of 1 T T r =1 P r D −1 . To achieve this, we first analyze the second matrix at RHS of Eq. 9, and then extend our analysis to the targeted matrix at LHS.</p><formula xml:id="formula_35">Spectrum of U 1 T T r =1 Λ r U ⊤ The matrix U 1 T T r =1 Λ r U ⊤ has eigenvalues 1 T T r =1 λ r i , i = 1, • • • , n,</formula><p>which can be treated as the output of a transformation applied on D −1/2 AD −1/2 's eigenvalue λ i , i.e., a kind of filter! The effect of this transformation (filter)</p><formula xml:id="formula_36">f (x) = 1 T T r =1</formula><p>x r is plotted in Figure <ref type="figure" target="#fig_6">1</ref>(a), from which we observe the following two properties of this filter. Firstly, it prefers positive large eigenvalues; Secondly, the preference becomes stronger as the window size T increases. In other words, as T grows, this filter tries to approximate a low-rank positive semi-definite matrix by keeping large positive eigenvalues. Eigenvalue the non-increasing order such that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectrum of 1</head><formula xml:id="formula_37">D −1/2 AD −1/2 U 1 T ∑ T r=1 Λ r U ⊤ 1 T ∑ T r=1 P r D −1 (b)</formula><formula xml:id="formula_38">1 T T r =1 λ r p 1 ≥ 1 T T r =1 λ r p 2 ≥ • • • ≥ 1 T T r =1 λ r pn ,</formula><p>where</p><formula xml:id="formula_39">{p 1 , p 2 , • • • , p n } is a permutation of {1, 2, • • • , n}.</formula><p>Similarly, since every d i is positive, the decreasingly ordered singular values of the matrix D −1/2 can be constructed by sorting 1/</p><formula xml:id="formula_40">√ d i in the non-increasing order such that 1/ d q 1 ≥ 1/ d q 2 ≥ • • • ≥ 1/ d q n where {q 1 , q 2 , • • • , q n } is a permutation of {1, 2, • • • , n}.</formula><p>In particular, d q 1 = d min is the smallest vertex degree. By applying Thm. 3.3 twice, we can see that the s-th singular value satisfies</p><formula xml:id="formula_41">σ s 1 T T r =1 P r D −1 ≤ σ 1 D − 1 2 σ s U 1 T T r =1 Λ r U ⊤ σ 1 D − 1 2 = 1 d q 1 1 T T r =1 λ r ps 1 d q 1 = 1 d min 1 T T r =1 λ r ps ,<label>(10)</label></formula><p>which reveals that the magnitude of 1</p><formula xml:id="formula_42">T T r =1 P r D −1 's eigenval-</formula><p>ues is always bounded by the magnitude of U 1 T T r =1 Λ r U ⊤ 's eigenvalues. In addition to the magnitude of eigenvalues, we also want to bound its smallest eigenvalue. Observe that the Rayleigh Quotient of 1 T T r =1 P r D −1 has a lower bound as follows</p><formula xml:id="formula_43">R 1 T T r =1 P r D −1 , x = R U 1 T T r =1 Λ r U ⊤ , D −1/2 x R D −1 , x ≥λ min U 1 T T r =1 Λ r U ⊤ λ max D −1 = 1 d min λ min U 1 T T r =1 Λ r U ⊤ .</formula><p>By applying Thm. 3.4, we can bound the smallest eigenvalue of</p><formula xml:id="formula_44">1 T T r =1 P r D −1 by the smallest eigenvalue of U 1 T T r =1 Λ r U ⊤ : λ min 1 T T r =1 P r D −1 ≥ 1 d min λ min U 1 T T r =1 Λ r U ⊤ .</formula><p>Illustrative Example: Cora In order to illustrate the filtering effect we discuss above, we analyze a small citation network Cora <ref type="bibr" target="#b28">[27]</ref>. we make the citation links undirected and choose its largest connected component. In Figure <ref type="figure" target="#fig_6">1</ref>(b), we plot the decreasingly ordered eigenvalues of matrices</p><formula xml:id="formula_45">D −1/2 AD −1/2 , U 1 T T r =1 Λ r U ⊤ , Algorithm 3: NetMF for a Small Window Size T 1 Compute P 1 , • • • , P T ; 2 Compute M = vol(G) bT T r =1 P r D −1 ; 3 Compute M ′ = max(M, 1); 4 Rank-d approximation by SVD: log M ′ = U d Σ d V ⊤ d ; 5 return U d Σ d as network embedding. Algorithm 4: NetMF for a Large Window Size T 1 Eigen-decomposition D −1/2 AD −1/2 ≈ U h Λ h U ⊤ h ; 2 Approximate M with M = vol(G) b D −1/2 U h 1 T T r =1 Λ r h U ⊤ h D −1/2 ; 3 Compute M ′ = max( M, 1); 4 Rank-d approximation by SVD: log M ′ = U d Σ d V ⊤ d ; 5 return U d Σ d as network embedding. and 1 T T r =1 P r D −1</formula><p>, respectively, with T = 10. For D −1/2 AD −1/2 , the largest eigenvalue λ 1 = 1, and the smallest eigenvalue λ n = −0.971. For U 1 T T r =1 Λ r U ⊤ , we observe that all its negative eigenvalues and small positive eigenvalues are "filtered out" in spectrum. Finally, for the matrix 1 T T r =1 P r D −1 , we observe that both the magnitude of its eigenvalues and its smallest eigenvalue are bounded by those of U 1</p><formula xml:id="formula_46">T T r =1 Λ r U ⊤ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NetMF</head><p>Built upon the theoretical analysis above, we propose a matrix factorization framework NetMF for empirical understanding of and improving on DeepWalk and LINE. For simplicity, we denote M = vol(G) bT T r =1 P r D −1 , and refer to log M as the DeepWalk matrix.</p><p>NetMF for a Small Window Size T NetMF for a small T is quite intuitive. The basic idea is to directly compute and factorize the DeepWalk matrix. The detail is listed in Algorithm 3. In the first step (Line 1-2), we compute the matrix power from P 1 to P T and then get M. However, the factorization of log M presents computational challenges due to the element-wise matrix logarithm. The matrix is not only ill-defined (since log 0 = −∞), but also dense. Inspired by the Shifted PPMI approach <ref type="bibr" target="#b25">[24]</ref>, we define M ′ such that M ′ i, j = max(M i, j , 1) (Line 3). In this way, log M ′ is a sparse and consistent version of log M. Finally, we factorize log M ′ by using Singular Value Decomposition (SVD) and construct network embedding by using its top-d singular values/vectors (Line 4-5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NetMF for a Large Window SizeT</head><p>The direct computation of the matrix M presents computing challenges for a large window size T , mainly due to its high time complexity. Hereby we propose an approximation algorithm as listed in Algorithm 4. The general idea comes from our analysis in section 3.1, wherein we reveal M's close relationship with the normalized graph Laplacian and show its lowrank nature theoretically. In our algorithm, we first approximate </p><formula xml:id="formula_47">D −1/2 AD −1/2 with its top-h eigenpairs U h Λ h U ⊤ h (Line 1)</formula><p>. Since only the top-h eigenpairs are required and the involved matrix is sparse, we can use Arnoldi method <ref type="bibr" target="#b23">[22]</ref> to achieve significant time reduction. In step two (Line 2), we approximate</p><formula xml:id="formula_48">M with M = vol(G) b D −1/2 U h 1 T T r =1 Λ r h U ⊤ h D −1/2</formula><p>. The final step is the same as that in Algorithm 3, in which we form M ′ = max( M, 1) (Line 3) and then perform SVD on log M ′ to get network embedding (Line 4-5).</p><p>For NetMF with large window sizes, we develop the following error bound theorem for the approximation of M and the approximation of log M ′ . Theorem 3.5. Let ∥•∥ F be the matrix Frobenius norm. Then</p><formula xml:id="formula_49">M − M F ≤ vol(G) bd min n j =k +1 1 T T r =1 λ r j 2 ; log M ′ − log M ′ F ≤ M ′ − M ′ F ≤ M − M F .</formula><p>Proof. See Appendix. □ Thm. 3.5 reveals that the error for approximating log M ′ is bounded by the error bound for the approximation of M. Nevertheless, the major drawback of NetMF lies in this element-wise matrix logarithm. Since good tools are currently not available to analyze this operator, we have to compute it explicitly even after we have already achieved a good low-rank approximation of M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed NetMF method on the multi-label vertex classification task, which has also been used in the works of DeepWalk, LINE, and node2vec.</p><p>Datasets We employ four widely-used datasets for this task. The statistics of these datasets are listed in Table <ref type="table" target="#tab_0">2</ref>.</p><p>BlogCatalog <ref type="bibr" target="#b39">[38]</ref> is a network of social relationships of online bloggers. The vertex labels represent interests of the bloggers.</p><p>Protein-Protein Interactions (PPI) <ref type="bibr" target="#b36">[35]</ref> is a subgraph of the PPI network for Homo Sapiens. The labels are obtained from the hallmark gene sets and represent biological states.</p><p>Wikipedia 3 is a co-occurrence network of words appearing in the first million bytes of the Wikipedia dump. The labels are the Part-of-Speech (POS) tags inferred by Stanford POS-Tagger <ref type="bibr" target="#b41">[40]</ref>.</p><p>Flickr <ref type="bibr" target="#b39">[38]</ref> is the user contact network in Flickr. The labels represent the interest groups of the users.</p><p>Baseline Methods We compare our methods NetMF (T = 1) and NetMF (T = 10) with LINE (2nd) <ref type="bibr" target="#b38">[37]</ref> and DeepWalk <ref type="bibr" target="#b32">[31]</ref>, which we have introduced in previous sections. For NetMF (T = 10), we choose h = 16384 for Flickr, and h = 256 for BlogCatelog, PPI, and  Wikipedia. For DeepWalk, we present its results with the authors' preferred parameters-window size 10, walk length 40, and the number of walks starting from each vertex to be 80. Finally, we set embedding dimension to be 128 for all methods.</p><p>Prediction Setting Following the same experimental procedure in DeepWalk <ref type="bibr" target="#b32">[31]</ref>, we randomly sample a portion of labeled vertices for training and use the rest for testing. For BlogCatalog, PPI, and Wikipedia datasets, the training ratio is varied from 10% to 90%. For Flickr, the training ratio is varied from 1% to 10%. We use the onevs-rest logistic regression model implemented by LIBLINEAR <ref type="bibr" target="#b15">[14]</ref> for the multi-label classification task. In the test phase, the onevs-rest model yields a ranking of labels rather than an exact label assignment. To avoid the thresholding effect <ref type="bibr" target="#b40">[39]</ref>, we assume that the number of labels for test data is given <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b40">39]</ref>. We repeat the prediction procedure 10 times and evaluate the performance in terms of average Micro-F1 and average Macro-F1 <ref type="bibr" target="#b43">[42]</ref>.</p><p>Experimental Results Figure <ref type="figure" target="#fig_8">2</ref> summarizes the prediction performance of all methods on the four datasets and Table <ref type="table">3</ref> lists the quantitative and relative gaps between our methods and baselines.</p><p>In specific, we show NetMF (T = 1)'s relative performance gain over LINE (2nd) and NetMF (T = 10)'s relative improvements over Deep-Walk, respectively, as each pair of them share the same window size T . We have the following key observations and insights:</p><p>(1) In BlogCatalog, PPI, and Flickr, the proposed NetMF method (T = 10) achieves significantly better predictive performance over baseline approaches as measured by both Micro-F1 and Macro-F1, demonstrating the effectiveness of the theoretical foundation we lay out for network embedding.</p><p>(2) In Wikipedia, NetMF (T = 1) shows better performance than other methods in terms of Micro-F1, while LINE outperforms other methods regarding Macro-F1. This observation implies that shortterm dependence is enough to model Wikipedia's network structure. This is because the used Wikipedia network is a dense word cooccurrence network with the average degree = 77.11, in which an edge between a pair of words are connected if they co-occur in a two-length window in the Wikipedia corpus.</p><p>(3) As shown in Table <ref type="table">3</ref>, the proposed NetMF method (T = 10 and T = 1) outperforms DeepWalk and LINE by large margins in most cases when sparsely labeled vertices are provided. Take the PPI dataset with 10% training data as an example, NetMF (T = 1) achieves relatively 46.34% and 33.85% gains over LINE (2nd) regarding Micro-F1 and Macro-F1 scores, respectively; More impressively, NetMF (T = 10) outperforms DeepWalk by 50.71% and 39.16% relatively as measured by two metrics.</p><p>(4) DeepWalk tries to approximate the exact vertex-context joint distribution with an empirical distribution through random walk sampling. Although the convergence is guaranteed by the law of large numbers, there still exist gaps between the exact and estimated distributions due to the large size of real-world networks and the relatively limited scale of random walks in practice (e.g., #walks and the walk length), negatively affecting DeepWalk's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>The story of network embedding stems from Spectral Clustering <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b46">45]</ref>, a data clustering technique which selects eigenvalues/eigenvectors of a data affinity matrix to obtain representations that can be clustered or embedded in a low-dimensional space. Spectral Clustering has been widely used in fields such as community detection <ref type="bibr" target="#b24">[23]</ref> and image segmentation <ref type="bibr" target="#b34">[33]</ref>. In recent years, there is an increasing interest in network embedding. Following a few pioneer works such as SocDim <ref type="bibr" target="#b39">[38]</ref> and DeepWalk <ref type="bibr" target="#b32">[31]</ref>, a growing number of literature has tried to address the problem from various of perspectives, such as heterogeneous network embedding <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b37">36]</ref>, semi-supervised network embedding <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b49">48]</ref>, network embedding with rich vertex attributes <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b50">49]</ref>, network embedding with high order structure <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b17">16]</ref>, signed network embedding <ref type="bibr" target="#b11">[10]</ref>, direct network embedding <ref type="bibr" target="#b31">[30]</ref>, network embedding via deep neural network <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b47">46]</ref>, etc.</p><p>Among the above research, a commonly used technique is to define the "context" for each vertex, and then to train a predictive model to perform context prediction. For example, DeepWalk <ref type="bibr" target="#b32">[31]</ref>, node2vec <ref type="bibr" target="#b17">[16]</ref>, and metapath2vec <ref type="bibr" target="#b13">[12]</ref> define vertices' context by the 1st-, 2nd-order, and meta-path based random walks, respectively; The idea of leveraging the context information are largely motivated by the skip-gram model with negative sampling (SGNS) <ref type="bibr" target="#b30">[29]</ref>. Recently, there has been effort in understanding this model. For example, Levy and Goldberg <ref type="bibr" target="#b25">[24]</ref> prove that SGNS is actually conducting an implicit matrix factorization, which provides us with a tool to analyze the above network embedding models; Arora et al. <ref type="bibr" target="#b2">[1]</ref> propose a generative model RAND-WALK to explain word embedding models; and Hashimoto et al. <ref type="bibr" target="#b19">[18]</ref> frame word embedding as a metric learning problem. Built upon the work in <ref type="bibr" target="#b25">[24]</ref>, we theoretically analyze popular skip-gram based network embedding models and connect them with spectral graph theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we provide a theoretical analysis of four impactful network embedding methods-DeepWalk, LINE, PTE, and node2vecthat were recently proposed between the years 2014 and 2016. We show that all of the four methods are essentially performing implicit matrix factorizations and the closed forms of their matrices offer not only the relationships between those methods but also their intrinsic connections with graph We further propose NetMF-a general framework to explicitly factorize the closed-form matrices that DeepWalk and LINE aim to implicitly approximate and factorize. Our extensive experiments suggest that NetMF's direct factorization achieves consistent performance improvements over the implicit approximation models-DeepWalk and LINE.</p><p>In the future, we would like to further explore promising directions to deepen our understanding of network embedding. It would be necessary to investigate whether and how the development in random-walk polynomials <ref type="bibr" target="#b10">[9]</ref> can support fast approximations of the closed-form matrices. The computation and approximation of the 2nd-order random walks employed by node2vec is another interesting topic to follow. Finally, it is exciting to study the nature of skip-gram based dynamic and heterogeneous network embedding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For brevity, D represents both D col &amp; Drow. D = diag(d 1 , • • • , d |V | ), where d i represents generalized degree of vertex i; vol(G): vol(G) = i j A i, j = i d i is the volume of a weighted graph G; T &amp; b: The context window size and the number of negative sampling in skip-gram, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 : 3 Generate a vertex sequence (w n 1 ,</head><label>131</label><figDesc>DeepWalk 1 for n = 1, 2, . . . , N do 2 Pick w n 1 according to a probability distribution P(w 1 ); • • • , w n L ) of length L by a random walk on network G; 4 for j = 1, 2, . . . , L − T do 5 for r = 1, . . . ,T do 6 Add vertex-context pair (w n j , w n j+r ) to multiset D; 7 Add vertex-context pair (w n j+r , w n j ) to multiset D; 8 Run SGNS on D with b negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>DAlgorithm 2 : node2vec 1 4 Generate a vertex sequence (w n 1 , 8 Add triplet (w n j+r , w n j , w n j− 1 )</head><label>214181</label><figDesc>− → r = (w, c, u) : (w, c, u) ∈ D, w n j = w, w n j +r = c, w n j −1 = u , D← − r = (w, c, u) : (w, c, u) ∈ D, w n j+r = w, w n j = c, w n j −1 = u . In addition, for a triplet (w, c, u), we use #(w, c, u)− → r and #(w, c, u)← − r to denote the number of times it appears in D− → r and D← − r , respectively. Construct transition probability tensor P; 2 for n = 1, 2, . . . , N do 3 Pick w n 1 , w n 2 according to a distribution P(w 1 , w 2 ); • • • , w n L ) of length L by the 2nd-order random walk on network G; 5 for j = 2, 3, . . . , L − T do 6 for r = 1, . . . ,T do 7 Add triplet (w n j , w n j+r , w n j−1 ) to multiset D; to multiset D; 9 Run SGNS on multiset D ′ = {(w, c) : (w, c, u) ∈ D};</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 3 . 1 .</head><label>31</label><figDesc>([11]) For normalized graph Laplacian L = I − D −1/2 AD −1/2 ∈ R n×n , all its eigenvalues are real numbers and lie in [0, 2], with λ min (L) = 0. For a connected graph with n &gt; 1, λ max (L) ≥ n/(n − 1). Theorem 3.2. ([41]) Singular values of a real symmetric matrix are the absolute values of its eigenvalues. Theorem 3.3. ([19]) Let B, C be two n × n symmetric matrices. Then for the decreasingly ordered singular values σ of B, C and BC, σ i+j−1 (BC) ≤ σ i (B)×σ j (C) holds for any 1 ≤ i, j ≤ n and i+j ≤ n+1. Theorem 3.4. ([41]) For a real symmetric matrix A, its Rayleigh</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>r</head><label></label><figDesc>=1 P r D −1 . By Thm. 3.1, D −1/2 AD −1/2 = I − L has eigen-decomposition U ΛU ⊤ such that U orthonormal and Λ = diag(λ 1 , • • • , λ n ), where 1 = λ 1 ≥ λ 2 ≥ • • • ≥ λ n ≥ −1and λ n &lt; 0. Based on this eigendecomposition,1   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>r</head><label></label><figDesc>=1 P r D −1 Guided by Thm. 3.2, the decreasingly ordered singular values of the matrix U 1 T T r =1 Λ r U ⊤ can be constructed by sorting the absolute value of its eigenvalues in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: DeepWalk Matrix as Filtering: (a) Function f(x ) = 1 T T r =1 x r with dom f = [−1, 1], where T ∈ {1, 2, 5, 10}; (b) Eigenvalues of D −1/2 AD −1/2 , U 1 T T r =1 Λ r U ⊤ ,and 1 T T r =1 P r D −1 for Cora network (T = 10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>3 http://mattmahoney.net/dc/text.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Predictive performance on varying the ratio of training data. The x-axis represents the ratio of labeled data (%), and the y-axis in the top and bottom rows denote the Micro-F1 and Macro-F1 scores respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Statistics of Datasets.</figDesc><table><row><cell cols="2">Dataset BlogCatalog</cell><cell>PPI</cell><cell>Wikipedia</cell><cell>Flickr</cell></row><row><cell>|V |</cell><cell>10,312</cell><cell>3,890</cell><cell>4,777</cell><cell>80,513</cell></row><row><cell>|E |</cell><cell>333,983</cell><cell>76,584</cell><cell>184,812</cell><cell>5,899,882</cell></row><row><cell>#Labels</cell><cell>39</cell><cell>50</cell><cell>40</cell><cell>195</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">A similar result could be achieved if we use P N (j) ∝ d</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">/4 j .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Hou Pong Chan for his comments. Jiezhong Qiu and Jie Tang are supported by NSFC 61561130160. Jian Li is supported in part by the National Basic Research Program of China Grant 2015CB358700, and NSFC 61772297 &amp; 61632016.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX:</head><p>Proof. First consider the special case when N = 1, thus we only have one vertex sequence w 1 , • • • , w L generated by random walk as described in Algorithm 1. Consider one certain vertex-context pair (w, c), let Y j (j = 1, • • • , L − T ) be the indicator function for event that w j = w and w j+r = c. We have the following two observations:</p><p>• Based on our assumptions about the graph and the random walk, E[Y j ] = d w vol(G) (P r ) w,c , and when j &gt; i + r ,</p><p>In this way, we can evaluate the covariance Cov(Y i , Y j ) when j &gt; i + r and calculate its limit when j − i → ∞ by using the fact that our random walk converges to its stationary distribution:</p><p>goes to 0 as j −i →∞ (P r ) w,c → 0.</p><p>Then we can apply Lemma 6.1 and conclude that the sample average converges in probability towards the expected value, i.e.,</p><p>Similarly, we also have</p><p>to be the indicator function for event w n j = w and w n j+r = c, and organize Y n j 's as</p><p>• • • . This r.v. sequence still satisfies the condition of S.N. Bernstein LLN, so the above conclusion still holds. □ Theorem 3.5. Let ∥•∥ F be the matrix Frobenius norm. Then</p><p>Proof. The first inequality can be seen by applying the definition of Frobenius norm and Eq. 10.</p><p>For the second inequality, first to show log</p><p>for any i, j. Without loss of generality, assume</p><p>where the first inequality is because log(1+x) ≤ x for x ≥ 0, and the the second inequality is because</p><p>. Sufficient to show M ′ i, j − M ′ i, j ≤ M i, j − Mi, j for any i, j. Recall the definition of M ′ and M ′ , we get M ′ i, j − M ′ i, j = max(M i, j , 1) − max( Mi, j , 1) ≤ M i, j − Mi, j . □</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">In Flickr, 1% of vertices are labeled for training [31], and in the other three datasets, 10% of vertices are labeled for training</title>
	</analytic>
	<monogr>
		<title level="m">Micro/Macro-F1 Score(%) for Multilabel Classification on BlogCatalog, PPI, Wikipedia, and Flickr datasets</title>
				<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Algorithm BlogCatalog (10%) PPI (10%) Wikipeida (10%) Flickr (1%)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A latent variable model approach to pmi-based word embeddings</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="385" to="399" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensor spectral clustering for partitioning higher-order network structures</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Austin R Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM. SIAM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Spacey Random Walk: A stochastic Process for Higher-Order Data</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Austin R Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lek-Heng</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="321" to="345" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unifying theorem for spectral embedding and clustering</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GraRep: Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Learning Graph Representations</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Heterogeneous network embedding via deep architectures</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient sampling for Gaussian graphical models via spectral sparsification</title>
		<author>
			<persName><forename type="first">Dehua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="364" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Selection in Signed Social Networks</title>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<title level="m">Networks, Crowds, and Markets: Reasoning about a Highly Connected World</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08">2008. Aug (2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilinear pagerank</title>
		<author>
			<persName><forename type="first">Lek-Heng</forename><surname>David F Gleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyang</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1507" to="1541" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word embeddings as metric recovery in semantic spaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="273" to="286" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511840371</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511840371" />
		<title level="m">Topics in Matrix Analysis</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning latent representations of nodes for classifying in heterogeneous social networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM. ACM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ARPACK users&apos; guide: solution of large-scale eigenvalue problems with implicitly restarted Arnoldi methods</title>
		<author>
			<persName><forename type="first">Danny</forename><forename type="middle">C</forename><surname>Richard B Lehoucq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Empirical comparison of algorithms for network community detection</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="631" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural Word Embedding as Implicit Matrix Factorization</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Variation Autoencoder Based Network Representation Learning for Classification</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masato</forename><surname>Odagaki</surname></persName>
		</author>
		<idno>ACL. 56</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Random walks on graphs</title>
		<author>
			<persName><forename type="first">László</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Combinatorics, Paul erdos is eighty</title>
				<imprint>
			<date type="published" when="1993">1993. 1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Link-based Classification</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Asymmetric Transitivity Preserving Graph Embedding</title>
		<author>
			<persName><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepWalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<title level="m">Link mining: Models, algorithms, and applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Shiryaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lyasoff</surname></persName>
		</author>
		<title level="m">Problems in Probability</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The BioGRID interaction database: 2011 update</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby-Joe</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chatr-Aryamontri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrie</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Oughtred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Michael S Livstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Van Auken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="D698" to="D704" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PTE: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">LINE: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large scale multi-label classification via metalabeler</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suju</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">K</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Numerical linear algebra</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lloyd N Trefethen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Bau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">50</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mining multi-label data</title>
		<author>
			<persName><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data mining and knowledge discovery handbook</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CANE: Contextaware network embedding for relation modeling</title>
		<author>
			<persName><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Max-Margin DeepWalk: Discriminative Learning of Network Representation</title>
		<author>
			<persName><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3889" to="3895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Network Representation Learning with Rich Text Information</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-Modal Bayesian Embeddings for Learning Social Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2287" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
