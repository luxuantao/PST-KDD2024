<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A comprehensive evaluation of ChatGPT&apos;s zero-shot Text-to-SQL capability</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-12">12 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aiwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A comprehensive evaluation of ChatGPT&apos;s zero-shot Text-to-SQL capability</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-12">12 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.13547v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL ability. Given the recent emergence of large-scale conversational language model ChatGPT and its impressive capabilities in both conversational abilities and code generation, we sought to evaluate its Text-to-SQL performance. We conducted experiments on 12 benchmark datasets with different languages, settings, or scenarios, and the results demonstrate that ChatGPT has strong text-to-SQL abilities. Although there is still a gap from the current state-of-the-art (SOTA) model performance, considering that the experiment was conducted in a zero-shot scenario, ChatGPT's performance is still impressive. Notably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms the SOTA model that requires fine-tuning on the Spider dataset by 4.1%, demonstrating its potential for use in practical applications. To support further research in related fields, we have made the data generated by ChatGPT publicly available at https://github.com/THU-BPM/chatgpt-sql.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing attention given to large-scale language models, they have become an essential component in natural language processing. As the size of pre-trained models grows, their usage is also gradually changing. Different from models such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> and T5 <ref type="bibr" target="#b16">(Raffel et al., 2020)</ref>, which require fine-tuning with a small amount of data, models such as GPT-3 <ref type="bibr" target="#b0">(Brown et al., 2020)</ref>, require the prompt design to generate target outputs. The recent ChatGPT<ref type="foot" target="#foot_0">1</ref> model, which employs Reinforcement Learning for Human Feedback (RLHF) <ref type="bibr" target="#b4">(Christiano et al., 2017)</ref>, simplifies prompt design, enabling better utilization of the zero-shot ability of large-scale pre-trained models in a conversational way. Based on this, many works have begun to analyze the zero-shot ability of Chat-GPT in various natural language processing tasks, such as information extraction <ref type="bibr" target="#b23">(Wei et al., 2023</ref>), text summarization <ref type="bibr" target="#b20">(Wang et al., 2023)</ref>, and mathematical abilities <ref type="bibr" target="#b7">(Frieder et al., 2023)</ref>. Due to Chat-GPT's strong ability in code generation and the fact that code generation models usually require a large amount of annotated data to produce good results, a zero-shot code generation model is very important. This paper first conducts a comprehensive evaluation of ChatGPT's zero-shot performance on a challenging code generation task: Text-to-SQL.</p><p>The Text-to-SQL task involves converting user input text into SQL statements that can be executed on a database, allowing non-expert users to better access the contents of a database. The design of Text-to-SQL models is typically challenging because they need to work across different databases and consider various user text input text and database structures. Due to the complexity of the Text-to-SQL task, a comprehensive evaluation of its performance requires consideration of a variety of scenarios in addition to the classic Spider dataset <ref type="bibr" target="#b26">(Yu et al., 2018)</ref>. For example, Spider-SYN <ref type="bibr">(Gan et al., 2021a)</ref> focuses on scenarios where the data schema mentioned in the user text input is synonymous with the database schema, Spider-DK <ref type="bibr">(Gan et al., 2021b)</ref> considers scenarios where the input question contains additional knowledge, Spider-CG <ref type="bibr" target="#b8">(Gan et al., 2022)</ref> emphasizes the combination generalization ability of models, and ADVETA <ref type="bibr" target="#b14">(Pi et al., 2022)</ref> considers scenarios where column names in the database have been modified. Additionally, to better reflect real-world scenarios, SParC <ref type="bibr">(Yu et al., 2019b)</ref> and CoSQL <ref type="bibr">(Yu et al., 2019a)</ref> incorporate multi-turn interaction between the user and the system. Finally, to evaluate models' multilingual capabilities, CSpider <ref type="bibr" target="#b13">(Min et al., 2019)</ref> and DuSQL <ref type="bibr" target="#b22">(Wang et al., 2020)</ref> evaluate Text-to-SQL performance in Chinese.</p><p>During our experiments, we evaluate the ability of ChatGPT on 12 different Text-to-SQL benchmark datasets. Based on the experimental results, we conclude the following observations:</p><p>1. Compared to the current state-of-the-art (SOTA) model that uses complete training data, ChatGPT without using task-specific training data only performs 14% worse. This already demonstrates that ChatGPT is a strong zero-shot Text-to-SQL converter.</p><p>2. The robustness of ChatGPT in generating SQL statements is very strong, and the performance gap between ChatGPT and the SOTA models is only 7.8% on some robustness settings of the Spider dataset, which is lower than the 14% gap on the standard Spider dataset.</p><p>3. In the ADVETA <ref type="bibr" target="#b14">(Pi et al., 2022)</ref> scenario where the column names in the database are adversarially modified, ChatGPT's performance even surpasses that of the current SOTA models by 4.1%.</p><p>4. The Exact Match metric of the data generated by ChatGPT is very low because there are many different ways to express SQLs with the same purpose. Therefore, we mainly use execution accuracy as the evaluation metric.</p><p>Overall, our experiments demonstrate that Chat-GPT has strong Text-to-SQL capabilities and robustness, and it outperforms SOTA models in certain scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In order to enable ChatGPT to generate accurate SQL outputs, we utilized the prompt as shown in Figure <ref type="figure" target="#fig_0">1</ref>. To ensure a fair demonstration of ChatGPT's Text-to-SQL capabilities, we directly adopted the Text-to-SQL prompt used in the Ope-nAI demo webwite 2 without conducting further prompt exploration. The upper half of Figure <ref type="figure" target="#fig_0">1</ref> represents the prompt in a single-turn Text-to-SQL scenario, where only the database and question information is required in the prompt. Meanwhile, in order to facilitate further evaluations, we emphasize in the prompt that the generated SQL statements can be executed in an SQLite database. The lower half of Figure <ref type="figure" target="#fig_0">1</ref> represents the prompt in a multi-turn Text-to-SQL scenario, where the prompt for the first interaction is the same as that in the single-turn scenario, and for subsequent interactions, only the new questions are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Setup</head><p>Datasets. We conduct extensive experiments on twelve public benchmark datasets as follows:</p><p>(1) Spider <ref type="bibr" target="#b26">(Yu et al., 2018</ref>) is a large-scale cross-domain Text-to-SQL benchmark. It contains 8659 training samples across 146 databases and 1034 evaluation samples across 20 databases. (2) Spider-SYN <ref type="bibr">(Gan et al., 2021a</ref>) is a challenging variant of the Spider evaluation dataset. Spider-SYN is constructed by manually modifying natural language questions with synonym substitutions.</p><p>(3) Spider-DK <ref type="bibr">(Gan et al., 2021b</ref>) is a humancurated dataset based on Spider, which samples 535 question-SQL pairs across 10 databases from the Spider development set and modifies them to incorporate the domain knowledge. (4) Spider-Realistic <ref type="bibr" target="#b5">(Deng et al., 2020)</ref>   <ref type="bibr" target="#b28">(Zhong et al., 2020)</ref> could achieve high code coverage from a distilled test suite of the database, which is also based on execution. Note that we do not use the main-stream exact match accuracy, as SQL queries that achieve the same goal can often be expressed in different ways, making it difficult for zero-shot ChatGPT models to achieve high exact match accuracy.</p><p>Baselines. Due to our exclusive reliance on execution-based evaluation, we did not employ baselines such as RatSQL <ref type="bibr" target="#b19">(Wang et al., 2019)</ref> and LGESQL <ref type="bibr" target="#b2">(Cao et al., 2021)</ref>, which generate only SQL skeletons without generating values. Instead, we primarily utilized three baselines: (1) PICARD <ref type="bibr" target="#b18">(Scholak et al., 2021</ref>) is a method for constraining auto-regressive decoders of language models through incremental parsing.</p><p>(2) RASAT <ref type="bibr" target="#b15">(Qi et al., 2022)</ref> introduces relation-aware selfattention into transformer models and also utilizes constrained auto-regressive decoders.</p><p>(3) RESD-SQL <ref type="bibr" target="#b11">(Li et al., 2023)</ref> proposes a ranking-enhanced encoding and skeleton-aware decoding framework to decouple the schema linking and the skeleton parsing. Among those, PICARD and RASAT are based on T5-3B <ref type="bibr" target="#b16">(Raffel et al., 2020)</ref> model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main Experiment</head><p>Evaluation on Spider Dataset. Overall, although ChatGPT still performs well on these two settings, the performance gap between ChatGPT and the original SOTA models becomes slightly larger than that on the Spider dataset. This suggests that the current models have already achieved sufficient robustness in these two scenarios.</p><p>Evaluation on Spider-DK and ADVETA Datasets. In Table <ref type="table" target="#tab_1">2</ref>, we further compare and analyze ChatGPT's performance on Spider-DK, ADVETA (RPL), and ADVETA (ADD). We find that ChatGPT performs exceptionally well on these datasets, with very small performance gaps compared to the current SOTA models. In fact, ChatGPT outperforms all current SOTA models on ADVETA (RPL). For the Spider-DK dataset, we speculate that ChatGPT's excellent performance is due to its additional knowledge provided by the large-scale pretraining. As for scenarios such as ADVETA, where the dataset's column names undergo adversarial modifications, the poor generalization performance of current models may be due to the significant distribution difference from the original dataset. Overall, ChatGPT exhibits strong robustness in scenarios that require additional knowledge or adversarial modifications are applied to the database column names. Evaluation on Chinese Text-to-SQL scenarios.</p><p>We further evaluate ChatGPT's Text-to-SQL ability on other languages in Table <ref type="table" target="#tab_4">5</ref>. The experiments are mainly conducted on two datasets, CSpider and DuSQL, where only the questions are in Chinese for CSpider and both the schema names and questions are in Chinese for DuSQL. The results show that while ChatGPT performs well in the Chinese Text-to-SQL scenario, there is still a performance gap compared to the English Text-to-SQL scenario. Moreover, the performance is even worse when the table names and column names are also in Chinese, with a large number of generated SQL queries being non-executable and a lower execution accuracy. This suggests the cross-lingual generalization ability of ChatGPT requires further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Case Study</head><p>In Table <ref type="table" target="#tab_5">6</ref>, we present four typical prediction errors made by ChatGPT on the Spider dev dataset. The first error case shows that ChatGPT tends to design JOIN statements more finely by using LEFT JOIN, but this level of granularity is not present in the original Spider dev dataset. The second error case arises from ChatGPT's confusion regarding the database structure, and it is not clear which column the term "full name" specifically refers to. The third example's error was due to the generated SQL statement lacking correct semantic interpretation, resulting in incorrect output for the "where" clauses with nested SQL statements. The fourth case of error is due to errors in copying specific values, where the case sensitivity of the original value was not preserved when regenerating the value. In summary, ChatGPT's errors mostly occur in small details, and some of these issues can be addressed and improved in later stages of development, such as in the first, third, and fourth cases. However, for errors like the second case, which indicate a lack of understanding of the database schema, further improvements to the model's ability may be necessary to resolve them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Text-to-SQL is an important semantic parsing task that converts natural language questions posed by users into SQL statements that can be executed on a database. On the classic Spider dataset <ref type="bibr" target="#b26">(Yu et al., 2018)</ref>, many classic works such as RatSQL <ref type="bibr" target="#b19">(Wang et al., 2019)</ref> and LGESQL <ref type="bibr" target="#b2">(Cao et al., 2021)</ref> have achieved excellent results. Since Text-to-SQL is a very complex task involving both user input questions and database structure, the robustness of the model is crucial. To further explore this issue, <ref type="bibr">Gan et al. (2021a)</ref> proposed the Spider-SYN dataset to evaluate the robustness of models under synonym substitution scenarios. Some works, such as Proton <ref type="bibr" target="#b21">(Wang et al., 2022)</ref> and ISESL-SQL <ref type="bibr" target="#b12">(Liu et al., 2022)</ref>, are also devoted to improving the robustness of models in this scenario. Meanwhile, many works explore the robustness of the Textto-SQL task in other scenarios. The Spider-DK dataset <ref type="bibr">(Gan et al., 2021b)</ref> evaluates the robustness of models in scenarios requiring additional knowl-edge. The Spider-Realistic dataset <ref type="bibr" target="#b5">(Deng et al., 2020)</ref> removes the explicit appearance of dataset schema information in user questions, thereby increasing the difficulty of the original task. The Spider-CG dataset <ref type="bibr" target="#b8">(Gan et al., 2022)</ref> evaluates the robustness of models in compositional generalization scenarios. The ADVETA dataset <ref type="bibr" target="#b14">(Pi et al., 2022)</ref> evaluates the robustness of models in scenarios involving adversarial modifications of database table information. In addition, to verify the robustness of models in cross-lingual scenarios, CSpider <ref type="bibr" target="#b13">(Min et al., 2019)</ref> and DuSQL <ref type="bibr" target="#b22">(Wang et al., 2020)</ref> have been proposed to evaluate the robustness of models in the Chinese language. To evaluate the performance of Text-to-SQL in more realistic scenarios, SParC <ref type="bibr">(Yu et al., 2019b)</ref> and CoSQL <ref type="bibr">(Yu et al., 2019a)</ref> have been proposed to evaluate the performance of multi-turn Text-to-SQL. Models such as STAR <ref type="bibr" target="#b1">(Cai et al., 2022)</ref> and CQR-SQL <ref type="bibr" target="#b24">(Xiao et al., 2022)</ref> have also achieved good results in this scenario.</p><p>Currently, several methods have been attempted to explore the improvement of large-scale language models for Text-to-SQL models. The PICARD <ref type="bibr" target="#b18">(Scholak et al., 2021)</ref> and RASAT <ref type="bibr" target="#b15">(Qi et al., 2022)</ref> utilize the T5-3B model, but still require the training data for fine-tuning. <ref type="bibr" target="#b17">Rajkumar et al. (2022)</ref> investigated the Text-to-SQL capabilities of the GPT3 model in a zero-shot setting. <ref type="bibr" target="#b3">Cheng et al. (2022)</ref> proposed the BINDER model based on the GPT3 codex, which has similar Text-to-SQL generation capabilities with the need for in-context exemplar annotations. However, these works do not provide a comprehensive evaluation of Textto-SQL and are limited to a few datasets without other robustness settings. In this work, we are the first to evaluate the comprehensive Text-to-SQL capabilities of ChatGPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we conducted a comprehensive analysis of ChatGPT's zero-shot ability in Text-to-SQL. We found that even without using any training data, ChatGPT still has strong Text-to-SQL ability, although there is still some gap compared to the current SOTA models. Additionally, ChatGPT demonstrated strong robustness, performing relatively better on most robustness benchmarks and even surpassing the current SOTA models on the ADVETA benchmark. Although this paper has made some findings, we only utilize a common prompt to eval-uate ChatGPT's ability. And in future work, better prompts could be designed to explore ChatGPT's Text-to-SQL ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future work</head><p>In future work, we will primarily consider the following two directions to further explore ChatGPT's capabilities in the Text-to-SQL task. Firstly, we will conduct more interactions with ChatGPT to address the issue of generating non-executable SQL statements. We can design ChatGPT to engage in multi-turn dialogues with the provided database error messages to further ensure the validity of generated SQL statements. Secondly, we will add more highly correlated in-context examples to the prompt to enhance ChatGPT's ability to generate Text-to-SQL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example prompts for Text-to-SQL using ChatGPT. The prompt at the top is for a single-turn scenario, while the one below is for multi-turn scenarios where only new questions are added in each interaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>is a new evaluation set based on the Spider dev set with explicit mentions of column names removed, which contains 508 samples. (5) Spider-CG(SUB) Comparison of the performance of ChatGPT and other models on Spider, Spider-SYN, and Spider-Realistic datasets.</figDesc><table><row><cell>Methods / Datasets</cell><cell></cell><cell>SPIDER</cell><cell></cell><cell></cell><cell>SPIDER-SYN</cell><cell></cell><cell cols="3">SPIDER-REALISTIC</cell></row><row><cell></cell><cell>VA</cell><cell>EX</cell><cell>TS</cell><cell>VA</cell><cell>EX</cell><cell>TS</cell><cell>VA</cell><cell>EX</cell><cell>TS</cell></row><row><cell>T5-3B + PICARD</cell><cell>98.4</cell><cell>79.3</cell><cell cols="2">69.4 98.2</cell><cell>69.8</cell><cell cols="2">61.8 97.1</cell><cell>71.4</cell><cell>61.7</cell></row><row><cell>RASAT + PICARD</cell><cell>98.8</cell><cell>80.5</cell><cell cols="2">70.3 98.3</cell><cell>70.7</cell><cell cols="2">62.4 97.4</cell><cell>71.9</cell><cell>62.6</cell></row><row><cell cols="2">RESDSQL-3B + NatSQL 99.1</cell><cell>84.1</cell><cell cols="2">73.5 98.8</cell><cell>76.9</cell><cell cols="2">66.8 98.4</cell><cell>81.9</cell><cell>70.1</cell></row><row><cell>ChatGPT</cell><cell cols="9">97.7 70.1(14?) 60.1 96.2 58.6(18.3?) 48.5 96.8 63.4(18.5 ?) 49.2</cell></row><row><cell>Methods / Datasets</cell><cell></cell><cell>SPIDER-DK</cell><cell></cell><cell cols="3">ADVETA(RPL)</cell><cell cols="2">ADVETA(ADD)</cell><cell></cell></row><row><cell></cell><cell>VA</cell><cell>EX</cell><cell cols="2">TS VA</cell><cell>EX</cell><cell cols="2">TS VA</cell><cell>EX</cell><cell>TS</cell></row><row><cell>T5-3B + PICARD</cell><cell>97.8</cell><cell>62.5</cell><cell>-</cell><cell>92.7</cell><cell>50.6</cell><cell>-</cell><cell>97.2</cell><cell>69.4</cell><cell>-</cell></row><row><cell>RASAT + PICARD</cell><cell>98.5</cell><cell>63.9</cell><cell>-</cell><cell>92.9</cell><cell>51.5</cell><cell>-</cell><cell>97.4</cell><cell>70.7</cell><cell>-</cell></row><row><cell cols="2">RESDSQL-3B + NatSQL 98.8</cell><cell>66.0</cell><cell>-</cell><cell>93.9</cell><cell>54.4</cell><cell>-</cell><cell>97.9</cell><cell>71.9</cell><cell>-</cell></row><row><cell>ChatGPT</cell><cell cols="2">96.4 62.6(3.4 ?)</cell><cell>-</cell><cell cols="2">91.4 58.5(4.1 ?)</cell><cell>-</cell><cell cols="2">93.1 68.1(3.8 ?)</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of different methods on the Spider-DK, ADVETA(RPL) and ADVETA(ADD) benchmark datasets.</figDesc><table><row><cell>Evaluation Metrics. We mainly adopt three</cell></row><row><cell>evaluation metrics which are valid SQL (VA),</cell></row><row><cell>execution accuracy(EX), and test-suite accuracy</cell></row><row><cell>(TS). Valid SQL (VA) is the proportion of SQL</cell></row><row><cell>statements that can be executed successfully.</cell></row><row><cell>Execution accuracy (EX) is the proportion of data</cell></row><row><cell>where the execution results match the standard</cell></row><row><cell>SQL statements. Test-suite accuracy (TS)</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>and Spider-CG(APP)</p><ref type="bibr" target="#b8">(Gan et al., 2022)</ref> </p>are two evaluation datasets to measure the compositional generalization of models, which is constructed by sub-sentence substitution between different examples and appending a sub-sentence into another sentence separately. (</p>6</p>) ADVETA(rpl) and ADVETA(add)</p><ref type="bibr" target="#b14">(Pi et al., 2022)</ref> </p>are two challenging test datasets for the Spider dataset which are composed of adversarial replacements of column names and the addition of new column names, respectively. (</p>7</p>) CSpider (Min et al., 2019) dataset is constructed by translating Spider into Chinese, which is the same size as the origin Spider dataset (8) DuSQL</p><ref type="bibr" target="#b22">(Wang et al., 2020</ref></p>) is a larger scale Chinese Text-to-SQL dataset with 23,797 question/SQL pairs. (</p>9</p>) SParC</p>(Yu et al.,  2019b)  </p>and CoSQL</p>(Yu et al., 2019a)  </p>are two multi-turn Text-to-SQL dataset with 1625 and 1007 questions in the dev set separately.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of different methods on the Spider-CG(SUB) and Spider-CG(APP) benchmark datasets.</figDesc><table><row><cell>Methods / Datasets</cell><cell cols="3">SPIDER-CG(SUB)</cell><cell cols="3">SPIDER-CG(APP)</cell></row><row><cell></cell><cell>VA</cell><cell>EX</cell><cell>TS</cell><cell>VA</cell><cell>EX</cell><cell>TS</cell></row><row><cell>T5-3B + PICARD</cell><cell>98.4</cell><cell>82.1</cell><cell cols="2">74.3 95.8</cell><cell>68.0</cell><cell>60.5</cell></row><row><cell>RASAT + PICARD</cell><cell>99.0</cell><cell>82.6</cell><cell cols="2">76.1 96.2</cell><cell>68.6</cell><cell>61.0</cell></row><row><cell cols="2">RESDSQL-3B + NatSQL 99.4</cell><cell>83.3</cell><cell cols="2">77.5 96.4</cell><cell>69.4</cell><cell>62.4</cell></row><row><cell>ChatGPT</cell><cell cols="6">98.3 76.6(6.7 ?) 67.2 91.2 61.3(8.1 ?) 47.9</cell></row><row><cell cols="2">gap in test suite accuracy, it is remarkable that</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ChatGPT achieved such results in a zero-shot</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">scenario considering that it was not fine-tuned on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the Spider training set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Evaluation on Spider-SYN and Spider-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Realistic Datasets. Table 1 also includes a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">comparison of ChatGPT's performance on the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Spider-SYN and Spider-Realistic datasets. The</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">main difference between these datasets and the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Spider dev set is that they eliminate the explicit</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">appearance of the database schema in the questions.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>In Table</p>1</p>, we present a comparison between ChatGPT and the current state-of-the-art (SOTA) models. Overall, ChatGPT exhibits a strong Text-to-SQL ability.Despite the 14% gap in execution accuracy compared to the current SOTA models and a 13.4%</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The performance of ChatGPT on two multiturn Text-to-SQL datasets: SParC and CoSQL.</figDesc><table><row><cell>Evaluation on Spider-CG Dataset. In Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The performance of ChatGPT on two Chinese Text-to-SQL datasets: CSpider and DuSQL.GPT also performs better on CoSQL datasets with more average interactions, which also indicates that ChatGPT's strong contextual modeling ability is very helpful for multi-turn Text-to-SQL.</figDesc><table><row><cell>3, we further analyze ChatGPT's ability in the</cell></row><row><cell>compositional generalization scenario. We found</cell></row><row><cell>that in Spider-CG (SUB), SQL substructures are</cell></row><row><cell>replaced to form combinations that do not exist in</cell></row><row><cell>the training set. In this scenario, ChatGPT even</cell></row><row><cell>outperforms the original Spider dev set. Even on</cell></row><row><cell>the more challenging Spider-CG (APP) dataset,</cell></row><row><cell>ChatGPT achieves strong performance, and the</cell></row><row><cell>performance gap with SOTA models is relatively</cell></row><row><cell>smaller than that on the original Spider dataset.</cell></row><row><cell>Overall, since ChatGPT is a zero-shot model, it is</cell></row><row><cell>not as affected by compositional generalization</cell></row><row><cell>as the SOTA models. Overall, zero-shot models</cell></row><row><cell>have greater advantages in the compositional</cell></row><row><cell>generalization setting.</cell></row><row><cell>Evaluation on multi-turn Text-to-SQL scenar-</cell></row><row><cell>ios. Given ChatGPT's strong contextual modeling</cell></row><row><cell>ability, we further evaluate its performance on</cell></row><row><cell>multi-turn Text-to-SQL scenarios: SPARC and</cell></row></table><note><p><p><p>COSQL. As shown in Table</p>4</p>, ChatGPT exhibits strong multi-turn Text-to-SQL ability. Although there is still a gap compared to the current SOTA models, the gap is relatively smaller compared to the single-turn Spider dataset. Meanwhile, Chat-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Question: Show the stadium name and the number of concerts in each stadium. Case study: We selected four cases of incorrect predictions generated by ChatGPT on the Spider development set for analysis.</figDesc><table><row><cell>ChatGPT:</cell><cell></cell><cell>SELECT</cell><cell cols="2">stadium.Name,</cell></row><row><cell cols="3">COUNT(concert.concert_ID)</cell><cell>FROM</cell><cell>stadium</cell></row><row><cell cols="5">LEFT JOIN concert ON stadium.Stadium_ID =</cell></row><row><cell cols="5">concert.Stadium_ID GROUP BY stadium.Name;</cell></row><row><cell cols="5">Gold: SELECT T2.name , count(*) FROM concert</cell></row><row><cell cols="5">AS T1 JOIN stadium AS T2 ON T1.stadium_id =</cell></row><row><cell cols="4">T2.stadium_id GROUP BY T1.stadium_id</cell><cell></cell></row><row><cell cols="5">Question:How many car models were produced by the</cell></row><row><cell cols="5">maker with full name American Motor Company?</cell></row><row><cell cols="5">ChatGPT: SELECT COUNT(*) FROM model_list</cell></row><row><cell cols="5">WHERE Maker = 'American Motor Company'</cell></row><row><cell cols="5">Gold: SELECT count(*) FROM CAR_MAKERS AS T1</cell></row><row><cell cols="5">JOIN model_list AS T2 ON T1.Id = T2.Maker</cell></row><row><cell cols="5">WHERE T1.FullName = 'American Motor Company';</cell></row><row><cell cols="5">Question: How many cars have a larger accelerate than</cell></row><row><cell cols="4">the car with the largest horsepower?</cell><cell></cell></row><row><cell cols="5">ChatGPT: SELECT COUNT(*) FROM cars_data WHERE</cell></row><row><cell cols="5">Accelerate &gt; (SELECT MAX(Horsepower) FROM</cell></row><row><cell>cars_data)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Gold: SELECT COUNT(*) FROM CARS_DATA WHERE</cell></row><row><cell>Accelerate</cell><cell>&gt;</cell><cell>(SELECT</cell><cell>Accelerate</cell><cell>FROM</cell></row><row><cell cols="5">CARS_DATA ORDER BY Horsepower DESC LIMIT 1);</cell></row><row><cell cols="5">Question: What is the abbreviation of Airline "JetBlue</cell></row><row><cell>Airways"?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ChatGPT: SELECT Abbreviation FROM airlines</cell></row><row><cell cols="4">WHERE Airline = 'Jetblue Airways' ;</cell><cell></cell></row><row><cell cols="5">Gold: SELECT Abbreviation FROM AIRLINES WHERE</cell></row><row><cell cols="4">Airline = "JetBlue Airways";</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://chat.openai.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Star: Sql guided pretraining for context-dependent text-to-sql parsing</title>
		<author>
			<persName><forename type="first">Zefeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11888</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lgesql: line graph enhanced text-to-sql model with mixed local and nonlocal relations</title>
		<author>
			<persName><forename type="first">Ruisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01093</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengzu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Nadkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02875</idno>
		<title level="m">Binding language models in symbolic languages</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning from human preferences. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miljan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12773</idno>
		<title level="m">Structure-grounded pretraining for text-to-sql</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Simon</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Pinchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan-Rhys</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Christian Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Berner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13867</idno>
		<title level="m">Mathematical capabilities of chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Measuring and improving compositional generalization in text-to-sql via component alignment</title>
		<author>
			<persName><forename type="first">Yujian</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.02054</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards robustness of textto-sql models against synonym substitution</title>
		<author>
			<persName><forename type="first">Yujian</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxia</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengsheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2505" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring underexplored limitations of cross-domain text-to-sql generalization</title>
		<author>
			<persName><forename type="first">Yujian</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05157</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Decoupling the skeleton parsing and schema linking for text-to-sql</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05965</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic enhanced text-to-sql parsing via iteratively learning schema linking graph</title>
		<author>
			<persName><forename type="first">Aiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A pilot study for chinese sql semantic parsing</title>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Qingkai Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13293</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards robustness of text-to-SQL models against natural and realistic adversarial table perturbation</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2007" to="2022" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rasat: Integrating relational structures into pretrained seq2seq model for text-tosql</title>
		<author>
			<persName><forename type="first">Jiexing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangpeng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06983</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Evaluating the text-to-sql capabilities of large language models</title>
		<author>
			<persName><forename type="first">Nitarshan</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00498</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Scholak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05093</idno>
		<title level="m">Parsing incrementally for constrained auto-regressive decoding from language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers</title>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunlong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.14229</idno>
		<title level="m">Crosslingual summarization via chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Proton: Probing schema linking information from pre-trained language models for text-to-sql parsing</title>
		<author>
			<persName><forename type="first">Lihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1889" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dusql: A large-scale and pragmatic chinese text-tosql dataset</title>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6923" to="6935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10205</idno>
		<title level="m">Zeroshot information extraction via chatting with chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cqr-sql: Conversational question reformulation enhanced context-dependent text-to-sql parsers</title>
		<author>
			<persName><forename type="first">Dongling</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linzheng</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian-Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07686</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Yang Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianze</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05378</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingning</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanelle</forename><surname>Roman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08887</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyang</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02285</idno>
		<title level="m">Sparc: Crossdomain semantic parsing in context</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semantic evaluation for text-to-sql with distilled test suites</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02840</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
