<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Passive Photometric Stereo from Motion *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CS Department</orgName>
								<orgName type="department" key="dep2">CISE Department Honda Research Institute † † CSE Department</orgName>
								<orgName type="institution">University of Illinois University of Florida</orgName>
								<address>
									<addrLine>800 California St</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California Urbana</orgName>
								<address>
									<postCode>61801, 32611, 94041, 92093</postCode>
									<settlement>Gainesville, Mountain View, La Jolla</settlement>
									<region>IL, FL, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><surname>Ho</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CS Department</orgName>
								<orgName type="department" key="dep2">CISE Department Honda Research Institute † † CSE Department</orgName>
								<orgName type="institution">University of Illinois University of Florida</orgName>
								<address>
									<addrLine>800 California St</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California Urbana</orgName>
								<address>
									<postCode>61801, 32611, 94041, 92093</postCode>
									<settlement>Gainesville, Mountain View, La Jolla</settlement>
									<region>IL, FL, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Kriegman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CS Department</orgName>
								<orgName type="department" key="dep2">CISE Department Honda Research Institute † † CSE Department</orgName>
								<orgName type="institution">University of Illinois University of Florida</orgName>
								<address>
									<addrLine>800 California St</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California Urbana</orgName>
								<address>
									<postCode>61801, 32611, 94041, 92093</postCode>
									<settlement>Gainesville, Mountain View, La Jolla</settlement>
									<region>IL, FL, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Passive Photometric Stereo from Motion *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D6C5BF28023064F4A47E5341B860E91A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce an iterative algorithm for shape reconstruction from multiple images of a moving (Lambertian) object illuminated by distant (and possibly time varying) lighting. Starting with an initial piecewise linear surface, the algorithm iteratively estimates a new surface based on the previous surface estimate and the photometric information available from the input image sequence. During each iteration, standard photometric stereo techniques are applied to estimate the surface normals up to an unknown generalized bas-relief transform, and a new surface is computed by integrating the estimated normals. The algorithm essentially consists of a sequence of matrix factorizations (of intensity values) followed by minimization using gradient descent (integration of the normals). Conceptually, the algorithm admits a clear geometric interpretation, which is used to provide a qualitative analysis of the algorithm's convergence. Implementation-wise, it is straightforward, being based on several established photometric stereo and structure from motion algorithms. We demonstrate experimentally the effectiveness of our algorithm using several videos of hand-held objects moving in front of a fixed light and camera.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we propose a simple and efficient algorithm for shape reconstruction of moving rigid 3D objects from videos. We assume that the camera is orthographic and that the object can be segmented from the background in each frame. The object is also assumed to have Lambertian reflectance with at least a few distinctive feature points that can be tracked throughout the video sequence in order to determine the extrinsic camera parameters. Furthermore, the scene illumination is assumed to be constant, although this condition can be relaxed somewhat. Relative to the fixed illumination source, the object's movement induces illumination changes in the image sequence, and this induced illumination variation is the major cue that we exploit in our shape reconstruction algorithm.</p><p>Shape recovery from multiple images has, of course, a long tradition in computer vision. The problem has been studied from various perspectives over the years, and two of which directly related to our work are multi-view stereo (e.g., <ref type="bibr" target="#b4">[5]</ref>) and photometric stereo (e.g., <ref type="bibr" target="#b15">[16]</ref>). From multiview stereo, we know how to recover the 3D position of a scene point (point on the object's surface) if the pixel correspondences across different frames are known. In particular, points in textured regions are generally less difficult to handle because their correspondences can be estimated relatively accurately compared to points in textureless regions. For the latter points, pixel correspondences are not easily computable, and in a dynamic environment where illumination changes, many cherished methods for intensity matching, such as those based on the brightness constancy assumption, are usually not effective if they are valid at all.</p><p>However, with a fixed viewpoint, photometric stereo recovers the 3D shape using images taken under different lighting. In particular, photometric stereo is able to reconstruct points in textureless regions if images with sufficient illumination variation are provided. The algorithm introduced in <ref type="bibr" target="#b15">[16]</ref> recovers the shape by computing the depth values relative to the image plane. The main idea is to compute a rank three factorization of the intensity matrix I, which is an m-by-n matrix, where m is the number of pixels in an image and n is the number of images. Under the Lambertian assumption, I factors as the product of normals and lighting directions: I = B • L. The rows of B are the normals and the columns of L are the lighting directions. More work is needed to ensure that the normal vectors represented in B are indeed integrable. Once an integrable normal vector field is obtained, the depth values can be computed by integrating the normals.</p><p>Unfortunately, the algorithm by itself cannot be generalized immediately to handle images taken under different views, and the chief obstacle is that in the multi-view setting, it is no longer possible to compute the intensity matrix I directly because we do not know the pixel correspondences across different images. On the other hand, if we know the correspondences between pixels, then given camera parameters, the 3D shape can be recovered without any input from photometric stereo. However, it is then an interesting question to ask if given a rough (inaccurate) cor-respondences between pixels across frames, whether photometric stereo can improve the estimates of the pixel correspondences. The point, of course, is that given such pixel correspondences, we can compute an intensity matrix using the correspondences, and the entire machinery of photometric stereo then comes alive to produce new correspondences using the recovered depth values.</p><p>Answering the question above is the main motivation of this paper. Surprisingly enough, the answer appears to be affirmative. The idea is to start with some initial surface estimate (depth values) S 1 , which is assumed to be not too far away from the true surface. Given any hypothesized surface S and the camera parameters for each frame, we can compute the intensity matrix I without any difficulty. Therefore, an integrable normal vector field can be estimated, and a new surface S can then be computed based on the estimated normals. We can repeat this process to produce a sequence of surfaces S 1 , • • • , S n , .., and of course, the hope is that the true surface is the limit of this sequence. In the experiments reported below, this is exactly what we have observed. This suggests that a simple and efficient algorithm based on a sequence of matrix factorization followed by integration of normal vector field is able to recover the shape of a moving Lambertian object. This is somewhat of a surprise, and it clearly begs for an explanation. Foremost, is it even realistic to expect the sequence S 1 , • • • , S n , .. produced by the algorithm actually converge? In the second part of this paper, we will provide a qualitative argument that explains this observed convergence.</p><p>This paper is organized as follows. We detail the proposed algorithm in the next section, and experimental results are shown in Section 3. Section 4 contains a detailed comparison between our algorithm and other recently published methods, and Section 5 contains a discussion on the convergence issues related to our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Reconstruction Algorithm</head><p>In this section, we detail our reconstruction algorithm. Let {I 1 , • • • , I j , • • • , I F } denote the input sequence of F images. Fix a coordinate system centered at the object, and in the following, we will express all vectorial quantities using this coordinate system. We assume that the object is both rigid and Lambertian; furthermore, the scene illumination is assumed to be modelled by a constant ambient illumination plus a directional source which can vary across frames. The observed intensity of a point x at frame j on the object's surface is given by the following equation:</p><formula xml:id="formula_0">I j (p) = α + ρ(p)L j • N p , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where N p is the unit normal vector at p, and ρ(p) is the albedo. L j is the directional light for frame j, and α accounts for the homogeneous ambient illumination.</p><p>The proposed algorithm is shown in Figure <ref type="figure">1</ref>. The algorithm essentially has two parts. The first part, which is both traditional and indispensable, estimates the camera parameters from a few tracked feature points using a standard structure from motion techniques (e.g., <ref type="bibr" target="#b12">[13]</ref>). This gives us F orthographic projections P j , 1 ≤ j ≤ F , for each image I j . Tomasi-Kanade's factorization algorithm also estimates the 3D positions of these feature points, and a piecewise planar surface, S 0 , is constructed from these 3D points. There are many possible ways to compute such a surface. In our implementation below, the image plane of the first image, I 1 , is used as the reference plane. If R is a region in I 1 containing the object, we triangulate R using points on the boundary of R and the tracked feature points. An initial depth map is then computed for every pixel in R by linearly interpolating the known depth values of the tracked feature points. The usage of these feature points in our algorithm differs slightly from some of the previous work <ref type="bibr" target="#b9">[10]</ref>[12] <ref type="bibr" target="#b14">[15]</ref>. In these work, the feature points are used mainly to estimate the camera projections and in some cases <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b14">[15]</ref>, to estimate light source directions. We go one step further by estimating a piecewise linear surface from them. Note that if we have sufficiently many feature points to track, then, the initial surface S 0 is already a good approximation of the true surface.</p><p>With the initial surface S 0 computed, the rest of the algorithm is straightforward: just let the machinery of photometric stereo run its own course. Given any surface S t and its associated depth map z t (x, y), we can compute the intensity matrix I, by collecting various pixel values across images into an r-by-F matrix (r is the number of pixels in R):</p><formula xml:id="formula_2">I ij = I j (P j (x i , y i , z t (x i , y i ))).<label>(2)</label></formula><p>Photometric stereo <ref type="bibr" target="#b15">[16]</ref> then provides us with a recipe for producing an integrable (normal) vector field N defined on R. The idea is to find an integrable vector field N that minimizes the following error function:</p><formula xml:id="formula_3">F i=1 (x,y)∈R I j (P j (x, y, z t (x, y))) -ρL t • N p -α 2<label>(3)</label></formula><p>N can be solved in a least-square sense and because it is assumed to be integrable, we can integrate it to obtain a new surface S t+1 and its associated depth map z t+1 (x, y). Note that our viewpoint here is slightly different from that of <ref type="bibr" target="#b15">[16]</ref>.</p><p>In photometric stereo, one assumes that images were taken under a single fixed view and the factorization of the intensity matrix (with more processing) yields a normal vector field of the underlying surface. In our multi-view setting, because of incorrect pixel correspondences, there is, in general, no surface that can account for the intensity matrix. However, we can still try to find an integrable vector field N that minimizes the error function above. And a surface S t+1 is defined to be one such that N is its (unit) normal vector field. In Section 5, we will re-interpret Equation <ref type="formula" target="#formula_3">3</ref>by formulating an analogous expression on the surface S.</p><p>Once the normals N = (N x , N y , N z ) have been estimated, the depth map z(x, y) is computed by minimizing the following objective function:</p><formula xml:id="formula_4">E(z(x, y)) = x,y ∂z(x, y) ∂x + N x N z 2 + ∂z(x, y) ∂y + N y N z 2<label>(4</label></formula><p>) Because of the Generalized Bas-Relief (GBR) ambiguity <ref type="bibr" target="#b0">[1]</ref>, which can be traced back to the least square problem in Equation <ref type="formula" target="#formula_3">3</ref>, the depth values of the tracked feature points will be, in general, in poor agreement with the values estimated using structure from motion technique. The last step in computing S t+1 is to correct the depth values by a GBR transform that brings the surface closest to the tracked feature points.</p><p>This process can be repeated indefinitely, and we get a sequence of surfaces S 0 , S 1 , • • • , S t , • • • . At this moment, there is no obvious reason to believe that it would converge to anything reasonable. In the next section, we will demonstrate that the algorithm does indeed converge to the "correct" surfaces most of the time, and in Section 5, we will provide a qualitative argument explaining this convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation and Experimental Results</head><p>In this section, we demonstrate some experimental results of the proposed algorithm. The algorithm is implemented in C++. The experiments are run on a laptop with 1.6GHz CPU and 512MB RAM. The feature point tracking and camera calibration steps take a couple of minutes and each iteration of the surface evolution takes about one to two minutes (depending on the number of iterations in minimizing Equation <ref type="formula" target="#formula_4">4</ref>) for most experiments. First, we present the reconstruction result of a paper cup. The video sequence is recorded with a Firewire CCD camera with a long focal length (for our orthographic projection assumption). The cup is moving in front of the camera in a dark room with a distant point light source. Feature points are manually chosen from the output of a feature point detector, and the feature point tracker tracks them throughout the sequence (Figure <ref type="figure">2</ref>.a). Due to memory limitations, we do not use all frames in the video sequence. Instead, we subsampled the sequence every three frames (box and figurine sequences are sampled every five frames). The resulting estimated normal field was good enough to give a convincing result.</p><p>The initial depth map (Figure <ref type="figure">2</ref>.b) is computed from the 3D positions of the tracked feature points. It is not close to the true surface; however, it provides rough estimates Given a collection of F images (frames), I t , indexed by t = 1, • • • , F , and m scene points indexed by p = 1, • • • , m with x t,p = [x t,p , y t,p ] t denoting the position of the scene point p in I t . The algorithm produces a depth map z(x, y) with respect to the image plane of the initial frame I 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Estimate Camera Parameters</head><p>Using Tomasi-Kanade factorization algorithm, we can recover (up to some unknown rotation) the camera projection matrix P t for each frame t and the 3D positions [x p , y p , z p ] t of the m scene points. They are represented by their respective depth values z p with respect to the image plane of I 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Construct an Initial Piecewise surface S 0</head><p>Use the 3D positions of the m scene points to compute an initial piecewise planar surface. Let R be the region in I 1 containing the object, and r denote the number of pixels in R. We compute a Delaunay triangulation of R using the projections of the m-scene points on I 1 and some points on the boundary of R.</p><p>All m-scene points are assumed to be projected onto the interior of R and each triangle in the triangulation is assumed to contain at least one scene point as its vertex. We linearly interpolate the depth values across each triangle using the depth values of the vertices. If the vertex is on the boundary of R, its depth value is interpolated using its projection onto the edge spanned by vertices that are one of the m scene points. The result is a piecewise planar surface S 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Iterate Until Converge</head><p>For t = 0, • • • , a surface S t and its associated depth map z t (x, y):</p><p>(a) Compute Intensity Matrix I: an r-by-F matrix such that  for back-warping frames to the reference frame. Figure <ref type="figure">2</ref>.f shows the estimated normals from the photometric stereo algorithm with warped images using the initial depth map. One may notice that there are some noisy normals in the textured region in the left boundary of the cup, and also the y-components of the normals along the cup lid and the lower edge of the holder change their directions due to the incorrect initial surface estimate. After integrating the normal field and correcting the GBR ambiguity, the depth map (Figure <ref type="figure">2</ref>.c) looks much better, but there exist regions where the normals are noisy or incorrect. However the estimated surface is much closer to the true surface, and the backwarped images based on this surface produce more accurate normals than the previous ones (Figure <ref type="figure">2</ref>.g). Integration of this normal field gives the next surface (Figure <ref type="figure">2</ref>.d), and it is already very close to our final result (Figure <ref type="figure">2</ref>.e and 2.h: after 4 iterations).</p><formula xml:id="formula_5">I ij = I j (P j (x i , y i , z t (x i , y i )))<label>(</label></formula><p>We have also applied our algorithm to the sequences presented in <ref type="bibr" target="#b14">[15]</ref> (Figure <ref type="figure" target="#fig_3">5</ref>). The two sequences are publicly available at the authors' website. Figure <ref type="figure">3</ref> shows the result of the box sequence. Thanks to the initialization, the initial surface (Figure <ref type="figure">3</ref>.a) is already close to the true surface except that the folds between the faces are missing. Our algorithm gives a nice reconstruction result, in which each face is smooth and the folds between faces are visible and sharp. Figure <ref type="figure">3</ref>.c and 3.d show the evolution of the surface produced by the proposed algorithm.</p><p>The figurine sequence is more interesting since the object has many fine structures on its surface, such as facial parts, eyeglasses, bumps and creases. Since the surface is complex and the textureless region is large, the initial surface does not give a good approximation of the true surface.  After a few iterations, most of the details on the surface have been recovered. Notice that after the first iteration, the glasses frame is not correctly reconstructed, and the bumps in the lower part of the figurine are not clear. In the final surface produced after six iterations, these inaccuracies have all disappeared, and the reconstruction result looks convincing (Figure <ref type="figure">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparisons with Previous Work</head><p>Our work is partially inspired by the the recent works of <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b14">[15]</ref>. In this section, we will concentrate on com-paring our work with these two methods. More detailed surveys of the related literature can be found in these two papers.</p><p>In <ref type="bibr" target="#b14">[15]</ref>, the problem of shape reconstruction from video sequences is addressed by reformulating the optical flow algorithm in order to better handle illumination change. Our algorithm shares two similarities with their algorithm. First, shape recovery in both algorithms is based on integration of normal vector fields. Second, both algorithms are iterative. However, the similarities are somewhat superficial, and fundamentally, the two algorithms are completely opposite of each other. The way the normals are estimated in <ref type="bibr" target="#b14">[15]</ref> is a local process in the sense that each individual normal vector is estimated separately in a linear system that includes also the estimate of optical flow. Because of locality, it seems unlikely that the optical flows and normals can be directly estimated from full-resolution images. Therefore, in their algorithm, the iterative step is a coarse-to-fine refinement using an image pyramid, where the optical flow and the normal vector field are estimated first at a lower resolution and then successively at higher resolutions. In our algorithm, normals are estimated globally in a single matrix factoriza-tion, and we work directly with full-resolution images. It can be argued that a global estimate, instead of local estimates, is more robust against noise. Partially because of this, there is no need for a coarse-to-fine refinement in our algorithm. Although our iterative step can also be considered as a refinement step, the refinement is over the entire surface at full resolution.</p><p>The reconstruction algorithm proposed in <ref type="bibr" target="#b11">[12]</ref> is based on stereo matchings. The algorithm assumes knowledge of the relative motion between the object and the illumination source. The idea is that this knowledge can be exploited to define a correspondence measure that is insensitive to illumination change. The shape is then recovered by minimizing an energy function defined on some graph using these correspondence measures. Because normals are not estimated, textureless planar regions can not be resolved by this algorithm. In an environment with fixed lighting and camera, relative motion between the object and the illumination source can be computed from the relative motion between the object and the camera. However, in the more general setting when both the camera and object (and possibly the illumination source) are moving, it is not clear to us how the relative motion between the object and the illumination source can be estimated directly from images. Presumably, one can estimate the light source directions using a few feature points as in <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b14">[15]</ref> and this estimate could be used to determine the relative motion between the object and light source. However, in general, there is an unresolved GBR amguity in the lighting directions <ref type="bibr" target="#b0">[1]</ref>, and it is unclear how this ambiguity can be resolved in the algorithm proposed in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Convergence Analysis</head><p>In this section, we give a qualitative argument showing that the convergence of our algorithm demonstrated in the previous section is not fortuitous. The detailed quantitative analysis of the convergence question is beyond the scope of this paper;</p><p>To this end, we will first formulate the reconstruction problem following the usual variational approach <ref type="bibr" target="#b2">[3]</ref></p><formula xml:id="formula_6">[8] in multi-view stereo. As before, let {I 1 , • • • , I F } denote the input collection of F images. Let {P 1 , • • • , P F } denote the F orthogonal projections of points in IR 3 to the im- ages {I 1 , • • • , I F }, respectively. Each pair of (I i , P i ) de- fines a function Im i : IR 3 → IR: Im i (x) = I i (P i (x)).</formula><p>In the usual variational approach, the reconstructed surface S should be a (local) minimum of the following functional: The "piecewise" flow followed by our algorithm is denoted by the black line segments.</p><formula xml:id="formula_7">E(S) = S F i=1 (Im i (x) -L i • N ) 2 dA S = S Φ(X, N )dA S (<label>5</label></formula><p>where N is the unit normal vector field of S, and L i is the lighting direction at frame i. We use Φ(X, N ) to denote the integrand in the integral above with X denoting the spatial variables in IR 3 and N denoting the surface normal (see <ref type="bibr" target="#b2">[3]</ref>). To simplify the discussion, we have assumed the following: 1) we know the lighting direction at each frame, 2) the albedos are constant (with value 1). These simplifications have no effect on the convergence question. We remind the reader that although we are following the usual formulation by working directly with the surface S ⊂ IR 3 , our algorithm actually operates only on the part of the surface S that can be parametrized by the image plane.</p><p>Given the functional E above, the usual PDE approach to solve the reconstruction problem is to start with an initial surface S, and compute a surface evolution S t , t ≥ 0, with S 0 = S, that is the gradient flow of E. For E given in Equation <ref type="formula" target="#formula_7">5</ref>, its gradient is <ref type="bibr">[</ref></p><formula xml:id="formula_8">3] ∇E = (Φ X N -2H(Φ -Φ N N ) + Tr((Φ XN ) T S + dN • (Φ NN ) T S ) N (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where H denotes the mean curvature of S, dN the derivative of the Gauss map, and Tr denotes the trace of the linear transform. T S denotes the tangent spaces of S. The gradient flow for ∇E is simply the following PDE that is usually solved using the level-set method:</p><formula xml:id="formula_10">∂S t ∂t = -∇E = -(Φ X N -2H(Φ -Φ N N ) +Tr((Φ XN ) T S + dN • (Φ NN ) T S ) N (7)</formula><p>We will take as a faith that the gradient flow above will converge to a critical point of E given a (sufficiently nice) initial surface S. With this, our argument for convergence proceeds in the following two steps:</p><p>1. Given a surface S, we interpret Equation 3 (with ρ = 1 and α = 0) geometrically in terms of some new functional E S . In fact, E S will be a modified version of E defined in Equation <ref type="formula" target="#formula_7">5</ref>, and E S will be defined only for surfaces "close" to S. In the following, we will omit the subscript S and denote E S by E . 2. The gradient vector ∇E at S can then be considered as a (small) perturbation of ∇E at S.</p><p>Note that we claim only that ∇E is a small perturbation of ∇E at S (and hence also in a small neighborhood around S). We make no claim that globally (at every surface S) ∇E is a small perturbation of ∇E. Schematically, our argument is depicted in Figure <ref type="figure" target="#fig_4">6</ref>(Right). Let S t be the solution to the PDE in Equation <ref type="formula">7</ref>. The red curve in the Figure denotes the gradient flow S t from an initial surface S 0 to a critical point S c of E. What we want to explain (qualitatively) is that from S 0 , our algorithm is heading towards the same critical point S c using a different path. For our algorithm, starting at S 0 , it tries to find a surface S 1 (close to S 0 ) that minimizes the functional E . This is basically the re-interpretation of Equation 3 in 1) above. The point is that this minimum can be computed using a linear method (and without PDE) when phrased in the form of Equation <ref type="formula" target="#formula_3">3</ref>. However, because of 2) above, we know that for a short time, flowing down along -∇E (that's how we get S 1 ), is not going to get too far away from flowing down along -∇E because -∇E is a (small) of ∇E. In the figure, this is indicated by the jagged path. Instead of flowing down smoothly to S c , at each S i , we move down along a small perturbation of the gradient for a short time to get S i+1 . Because of the small perturbation at each step, we argue that at the end, our algorithm will still flow down to the same critical point S c as in the usual gradient flow of E.</p><p>We can interpret Equation 3 as follows: given a surface S, we try to find another surface S close to S, such that 1. There is a one-to-one correspondence ϕ between S and S . For each point x ∈ S, ϕ(x) ∈ S . In Equation <ref type="formula" target="#formula_3">3</ref>, ϕ is given by the (x, y) on the image plane.</p><p>2. The surface S minimizes the following functional:</p><formula xml:id="formula_11">E (S ) = S F i=1 (Im i (x) -L i • N S (ϕ(x))) 2 dA S ( * )</formula><p>where the integration is on S not S , and N S (φ(x)) denotes the normal vector of S at the point ϕ(x). Let Φ (X, N ) denote the integrand above. Note that the spatial part (X) of Φ (X, N ) is defined over a neighborhood U of S using ϕ. See Figure <ref type="figure" target="#fig_4">6</ref>(Left).</p><p>Note that the integral above is what has been computed in Equation 3 <ref type="foot" target="#foot_0">1</ref> . In both places, each normal vector of the new surface S is determined by the image intensity values of corresponding point on the old surface S. In Equation 3, the normal vector field computed through SVD is in general not integrable. Instead, we estimate a near-by integrable vector field, and this allows us to interpret the resulting surface S as the one that minimizes ( * ) above. The comparison between our approach and that of <ref type="bibr" target="#b6">[7]</ref> is quite interesting. The problem studied in <ref type="bibr" target="#b6">[7]</ref> is very similar to ours, and in principle, their solution surface is a critical point of the functional E in Equation <ref type="formula" target="#formula_7">5</ref>. Because of negative curvature flow which can cause numerical instability, a straightforward level-set implementation for solving the PDE in equation 6 is not feasible. So <ref type="bibr" target="#b6">[7]</ref> modifies E by including an auxiliary (unit) vector field V . Although the energy functional proposed in <ref type="bibr" target="#b6">[7]</ref> is a little complicated, basically, the vector field V is "determined" by the photometric data (the image intensities and lighting) while the surface normals is a "near-by" vector field of V . This is clearly very similar to our discussion above. The difference, however, is that their modified energy function again leads to a PDE solution of the problem. Our (locally) modified problem (Equation (*) above) leads to a simple linear least-square solution.</p><p>What's left now is to compute the gradient of E at S:</p><formula xml:id="formula_12">∇E = (Φ X N + 2HΦ N N + Tr((Φ XN ) T S + dN • (Φ NN ) T S )) N (8)</formula><p>Since E (S ) involves only the integral over S (not S ), the corresponding term 2HΦ in Equation <ref type="formula" target="#formula_8">6</ref>, which comes from the area variation, is not present in the gradient of E . Note also that, at S, Φ N = Φ N as well as Φ NN = Φ NN . Therefore, at S, the difference ∇E -∇E is</p><formula xml:id="formula_13">(2HΦ + (Φ X -Φ X )N + Tr((Φ XN -Φ XN ) T S )) N. (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>Our task now is to show that the magnitude of this term is relatively small compared to ∇E. Since our algorithm starts with a piecewise planar surface S 1 . Therefore, at the initial surface S 0 , -2HΦ = 0. In general, when we are close to the true surface, the curvature H will of course no longer be zero. However, the term Φ will be small and therefore, -2HΦ is also small as well provided that the curvature H is bounded, which is usually the case. The term Φ X -Φ X (as well as (Φ XN -Φ XN ) T S ) is related to the image gradients and the relative motion between the camera and the object. Using orthorgraphic camera model, Φ X and Φ X can be easily computed. In our case, the motion has a rough symmetry with respect to the initial relative position between the camera and the object. That is, the object is rotated first to the right and then to the left and so on. This can be used to argue that there are some cancellations among the terms that make up Φ X , and the difference Φ X -Φ X is usually small. To empirically verify these claims, we compute both gradient vectors ∇E and ∇E for the surfaces produced during six iterations in the solution of the figurine sequence (Figure <ref type="figure">4</ref>). The results are listed in Table <ref type="table" target="#tab_0">1</ref>. Our here is to justify that ∇E can be considered as a small perturbation of ∇E. Therefore, we show the "angle" between the two gradient vectors and the relative magnitude of their difference are both small. The angle ∠(∇E , ∇E) between ∇E and ∇E is defined as</p><formula xml:id="formula_15">∠(∇E , ∇E) = cos -1 ( &lt; ∇E, ∇E &gt; S ∇E S ∇E S ).<label>(10)</label></formula><p>In the above, the inner product &lt; ∇E, ∇E &gt; S between ∇E and ∇E is defined as</p><formula xml:id="formula_16">&lt; ∇E, ∇E &gt; S = S &lt; ∇E, ∇E &gt; IR 3 dA,<label>(11)</label></formula><p>and ∇E 2 S =&lt; ∇E, ∇E &gt; S . The results in Table <ref type="table" target="#tab_0">1</ref> show that it is indeed reasonable to regard ∇E as a small perturbation of ∇E. Note also that the value E(S) always decreases after each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>We have presented a method for shape reconstruction from multiple images of a moving object. Our algorithm is iterative: it starts with a rough piecewise planar estimate of the true surface, and then successively refines the estimate using photometric stereo techniques. The algorithm is simple both conceptually and implementation-wise. Results of our experiments have demonstrated the feasibility of our algorithm, and a qualitative explanation of our algorithm's convergence is also introduced.</p><p>The first item on our list of future work is a more detailed numerical analysis of the convergence issues that were raised in the previous section. A detailed study of this question, both numerically and mathematically, could potentially not only improve our algorithm but also unearth previously unknown fundamentals of 3D reconstruction and shading. One serious limitation of our algorithm is its dependence on some reference image for integrating normals. This dependency makes it awkward to generalize our algorithm to a 360-degree reconstruction of an object. What is needed here is a scheme to integrate normals directly on surfaces in IR 3 instead of on the image plane.</p><p>Finally, the reconstructions in Figs. ?? to 4 show a common problem of larger reconstruction error near image intensity discontinuities (step edges), and this arises due to inaccuracies in alignment that may accumulate during the iterative process. On the other hand, it is at just these locations where conventional structure-from-motion techniques excel. What is needed is a technique which can merge multi-view geometric constraints with the result of photometric stereo</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>b) Matrix factorization: Perform a rank three SVD on I to obtain N , an r-by-3 normal matrix and L, a 3-by-F lighting matrix. Determine (up to an unknown GBR transform) a new N from N that is integrable. (c) Integrating Normals: Determine a new depth map zt+1 by minimizing Equation 4. The depth map zt+1 defines a new surface St+1 . (d) GBR Correction: Use the known positions of the m scene points to determine the unknown GBR transform. Find a GBR transform that brings the surface St+1 closest to these m scene points. S t+1 and z t+1 are then the GBR-corrected surface and depth map, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Shape Reconstruction Algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: The box sequence : (a) The initial piecewise planar surface. (b) The final depth map. (c) The rendering of the surface after one iteration. There are sharp spikes along the left edge, and the folds between faces are not very clear. (d) The rendering of the final surface. The spikes are removed, and the folds become more clear.</figDesc><graphic coords="5,311.46,229.18,109.88,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: The original images from the box and figurine sequences.<ref type="bibr" target="#b14">[15]</ref> </figDesc><graphic coords="5,60.67,449.80,105.44,79.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a) Definition of ϕ. We use the image plane of the first image to define correspondences between points on S and S . (b) Comparison of Flows: The smooth red curve denotes the smooth gradient flow according to Equation 7.The "piecewise" flow followed by our algorithm is denoted by the black line segments.</figDesc><graphic coords="6,462.16,72.08,76.84,79.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between ∇E and ∇E for the figurine sequence. ∠(∇E , ∇E) is reported in degrees below. Surface S ∠(∇E , ∇E)</figDesc><table><row><cell>S 0</cell><cell>12.7 •</cell><cell>∇E -∇E S ∇E S 0.225</cell><cell>E(S) 225.98</cell></row><row><cell>S 1</cell><cell>8.9 •</cell><cell>0.152</cell><cell>187.78</cell></row><row><cell>S 2</cell><cell>8.1 •</cell><cell>0.141</cell><cell>186.05</cell></row><row><cell>S 3</cell><cell>4.9 •</cell><cell>0.087</cell><cell>170.25</cell></row><row><cell>S 4</cell><cell>4.9 •</cell><cell>0.088</cell><cell>171.95</cell></row><row><cell>S 5</cell><cell>4.0 •</cell><cell>0.07</cell><cell>164.84</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Strictly speaking, this would require a weighted sum of squares in Equation3, with weights given by the area elements.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Support was provided by NSF grants IIS-0308185, EIA-0224431 and CCR 00-86094.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The bas-relief ambiquity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="44" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Probabilistic framework for space carving</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broadhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="388" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Complete dense stereovision using level set methods</title>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keriven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="379" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From few to many: Generative models for recognition under variable pose and illumination</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic refinement of the visual hull to satisfy photometric and silhouette consistency constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Isidoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1335" to="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shedding light on stereoscopic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yezzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stereoscopic shading: Integrating multi-frame shape cues in a variational framework</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yezzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A theory of shape by space carving</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="218" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geotensity: Combining motion and lighting for 3D surface reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wiles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="90" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Good feature to track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense shape reconstruction of a moving object under arbitrary unknown lighting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1202" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shape and motion from image streams under orthography: A factorization method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Space-time tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="801" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shape and motion under varying illumination: Unifying structure from motion, photometric stereo, and multi-view stereo</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="618" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape and albedo from multiple images using integrability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="158" to="164" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
