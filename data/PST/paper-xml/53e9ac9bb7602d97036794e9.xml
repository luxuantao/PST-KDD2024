<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonnegative matrix factorization with constrained second-order optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-02-11">11 February 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Rafal</forename><surname>Zdunek</surname></persName>
							<email>zdunek@brain.riken.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Advanced Brain Signal Processing</orgName>
								<orgName type="institution" key="instit1">Brain Science Institute</orgName>
								<orgName type="institution" key="instit2">RIKEN</orgName>
								<address>
									<addrLine>Wako-shi</addrLine>
									<postCode>351-0198</postCode>
									<settlement>Saitama</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Telecommunications, Teleinformatics, and Acoustics</orgName>
								<orgName type="institution">Wroclaw University of Technology</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Advanced Brain Signal Processing</orgName>
								<orgName type="institution" key="instit1">Brain Science Institute</orgName>
								<orgName type="institution" key="instit2">RIKEN</orgName>
								<address>
									<addrLine>Wako-shi</addrLine>
									<postCode>351-0198</postCode>
									<settlement>Saitama</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Warsaw University of Technology</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Laboratory for Advanced Brain Signal Processing</orgName>
								<orgName type="institution" key="instit1">Brain Science Institute</orgName>
								<orgName type="institution" key="instit2">RIKEN</orgName>
								<address>
									<addrLine>Wako-shi</addrLine>
									<postCode>351-0198</postCode>
									<settlement>Saitama</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Nonnegative matrix factorization with constrained second-order optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-02-11">11 February 2007</date>
						</imprint>
					</monogr>
					<idno type="MD5">50E157BA6EB07098AEBBA3C892BDFC1A</idno>
					<idno type="DOI">10.1016/j.sigpro.2007.01.024</idno>
					<note type="submission">Received 1 August 2006; received in revised form 8 December 2006; accepted 26 January 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Nonnegative matrix factorization</term>
					<term>Blind source separation</term>
					<term>Quasi-Newton method</term>
					<term>GPCG</term>
					<term>Second-order optimization</term>
					<term>Fixedpoint algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nonnegative matrix factorization (NMF) solves the following problem: find nonnegative matrices</p><p>such that Y ffi AX, given only Y 2 R MÂT and the assigned index R. This method has found a wide spectrum of applications in signal and image processing, such as blind source separation (BSS), spectra recovering, pattern recognition, segmentation or clustering. Such a factorization is usually performed with an alternating gradient descent technique that is applied to the squared Euclidean distance or Kullback-Leibler divergence. This approach has been used in the widely known Lee-Seung NMF algorithms that belong to a class of multiplicative iterative algorithms. It is well known that these algorithms, in spite of their low complexity, are slowly convergent, give only a strictly positive solution, and can easily fall into local minima of a nonconvex cost function. In this paper, we propose to take advantage of the second-order terms of a cost function to overcome the disadvantages of gradient (multiplicative) algorithms. First, a projected quasi-Newton method is presented, where a regularized Hessian with the Levenberg-Marquardt approach is inverted with the Q-less QR decomposition. Since the matrices A and/or X are usually sparse, a more sophisticated hybrid approach based on the gradient projection conjugate gradient (GPCG) algorithm, which was invented by More and Toraldo, is adapted for NMF. The gradient projection (GP) method is exploited to find zero-value components (active), and then the Newton steps are taken only to compute positive components (inactive) with the conjugate gradient (CG) method. As a cost function, we used the a-divergence that unifies many well-known cost functions. We applied our new NMF method to a BSS problem with mixed signals and images. The results demonstrate the high robustness of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nonnegative matrix factorization (NMF) attempts to recover hidden nonnegative structures or patterns from usually redundant data. This technique has been successfully applied in many applications, e.g. in data analysis (pattern recognition, segmentation, clustering, dimensionality reduction) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, signal and image processing (blind source separation, spectra recovering) <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, language modeling, text analysis <ref type="bibr" target="#b17">[18]</ref>, music transcription <ref type="bibr" target="#b18">[19]</ref>, or neurobiology (gene separation) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>NMF decomposes the data matrix Y ¼ ½y mt 2 R MÂT as a product of two nonnegative matrices A ¼ ½a mr 2 R MÂR and X ¼ ½x rt 2 R RÂT , where 8m; r; t : a mr X0; x rt X0. Although some matrix factorizations provide exact factors (i.e. Y ¼ AX), here we shall consider a factorization that is approximative in nature, i.e.</p><formula xml:id="formula_0">Y ¼ AX þ V,<label>(1)</label></formula><p>where V 2 R MÂT represents a noise or error matrix.</p><p>Depending on an application, the hidden structures may have different interpretation. For example, Lee and Seung in <ref type="bibr" target="#b5">[6]</ref> introduced NMF as a method to decompose an image (face) into parts-based representations (parts reminiscent of features such as lips, eyes, nose, etc.). In BSS <ref type="bibr" target="#b21">[22]</ref>, the matrix Y represents the observed mixed (superposed) signals or images, A is a mixing operator, and X is a matrix of true source signals or images. Each row of Y or X is a signal or 1D image representation, where M is a number of observed mixed signals and R is a number of hidden (source) components. The index t usually denotes a sample (discrete time instant), where T is a number of samples. In BSS, we usually have TbMXR, and R is known or can be relatively easily estimated using SVD or PCA.</p><p>Our objective is to estimate the mixing matrix A and sources X subject to nonnegativity constraints of all the entries, given Y and possibly the knowledge on a statistical distribution of noisy disturbances.</p><p>The basic approach to NMF, which is presented in Algorithm 1, is the alternating minimization of a specific cost function. In general, the cost function DðYjjAXÞ in <ref type="bibr">Step</ref>  A ðsþ1Þ ¼ arg min a mrX0 DðYjjAX ðsþ1Þ Þj A ðsÞ End Lee and Seung <ref type="bibr" target="#b5">[6]</ref> were the first to apply Algorithm 1 separately to two different cost functions: squared Euclidean distance (Frobenius norm) and Kullback-Leibler (KL) divergence. Using a gradient descent approach to perform Steps 1 and 2, they finally obtained multiplicative algorithms that were previously known in other applications as the EMML (expectation maximization maximum likelihood) or Richardson-Lucy algorithm (RLA) <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> for minimization of the KL divergence, and the ISRA algorithm <ref type="bibr" target="#b26">[27]</ref> which minimizes the Euclidean distance.</p><p>However, the multiplicative algorithms are known to be very slowly convergent, give only a strictly positive solution, and easily get stuck in local minima. Many approaches have been proposed in the literature to relax these problems. One of them is to apply projected gradient (PG) algorithms <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref> or projected alternating least-squares (ALS) algorithms <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> instead of multiplicative ones. Another improvement concerns modification of the learning rate (relaxation parameter) to speed up the convergence, and better recovering zero-value entries (for sparse solutions). Lin <ref type="bibr" target="#b28">[29]</ref> suggested applying the Armijo rule to estimate the learning parameters in PG updates for NMF. Also, interior-point gradient (IPG) algorithms (see, e.g. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>) address the issue with selecting such a learning parameter that is the steepest descent and also keeps some distance to a boundary of nonnegative orthant.</p><p>In this paper, we extend the idea presented in <ref type="bibr" target="#b35">[36]</ref>, which concerns exploiting the information from the second-order term in the Taylor expansion of a cost function to speed up the convergence. First, we discuss a projected quasi-Newton method in which a regularized Hessian with the Levenberg-Marquardt approach is inverted using the Q-less QR decomposition. An application of a quasi-Newton method to NMF has also been suggested in <ref type="bibr" target="#b29">[30]</ref>. Then, we also propose to use an alternative approach that involves using the gradient projection conjugate gradient (GPCG) algorithm. To our best knowledge, the GPCG algorithm has not been applied to NMF problems so far. Originally, the algorithm was invented by More and Toraldo <ref type="bibr" target="#b36">[37]</ref> to solve largescale obstacle problems, elastic-plastic torsion problem, and journal bearing problems, and latter, Bardsley <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> applied it for solving large-scale problems in image reconstruction and restoration. This algorithm is based on the ''reduced'' Newton method which is only applied to update positive components (inactive). Due to the sparseness, a set of inactive components is not very large, which keeps a relatively low computational cost. Moreover, the reduced Hessian is not inverted directly, but it is used to compute gradient updates with the conjugate gradient (CG) method <ref type="bibr" target="#b39">[40]</ref>. The CG iterates converge within a finite number of iterations because the reduced Hessian is positive-definite. Then, the gradient updates are used with the gradient projection (GP) method to find zero-value components (active).</p><p>As a cost function, we used the a-divergence <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> that unifies many well-known cost functions, and it adapts the algorithm to a statistical distribution of noise with only one parameter.</p><p>This paper is organized as follows: Section 2 presents the quasi-Newton method in application to NMF. Section 3 shortly explains the modified fixedpoint (FP) algorithm. In Section 4, we present the GPCG algorithm and its extension to NMF. The numerical results from a BSS problem of mixed signals and images are illustrated in Section 5. Finally, the conclusions and future work are presented in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Quasi-Newton optimization</head><p>The a-divergence <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref> can be expressed as follows:</p><formula xml:id="formula_1">D A ðYjjAXÞ ¼ X mt y mt ðy mt =z mt Þ aÀ1 À 1 aða À 1Þ þ z mt À y mt a , z mt ¼ ½AX mt ; y mt ¼ ½Y mt .<label>(2)</label></formula><p>The KL divergence can be obtained for a ! 1, but if a ! 0 the dual KL divergence can be derived. For a ¼ 2; 0:5; À1, we obtain the Pearson's, Hellinger's, and Neyman's Chi-square distances, respectively. Applying the projected Newton method to (2), we have</p><formula xml:id="formula_2">X P O X ½X À ½H ðX Þ D A À1 r X D A ,<label>(3)</label></formula><formula xml:id="formula_3">A P O A ½A À ½H ðAÞ D A À1 r A D A ,<label>(4)</label></formula><p>where H ðX Þ D A and H ðAÞ D A are Hessians, r X D A and r A D A are gradient matrices for (2) with respect to X and A, respectively. The applications P O X ½: and</p><formula xml:id="formula_4">P O A ½: project from R RÂT and R MÂR into the corresponding feasible sets O X 2 R RÂT þ and O A 2 R MÂR þ</formula><p>which are defined as follows:</p><formula xml:id="formula_5">O X ¼ fX 2 R RÂT : x rt X0g,<label>(5)</label></formula><formula xml:id="formula_6">O A ¼ fA 2 R MÂR : a mr X0g.<label>(6)</label></formula><p>For aa0, the gradient G ðX Þ D A 2 R RÂT with respect to X can be expressed as</p><formula xml:id="formula_7">G ðX Þ D A ¼ r X D A ¼ 1 a A T ðE À ðY AXÞ a Þ,<label>(7)</label></formula><p>where means a element-wise division, and E is a matrix of all ones. The Hessian has the form</p><formula xml:id="formula_8">H ðX Þ D A ¼ 1 a diagf½h ðX Þ t t¼1;...;T g 2 R RTÂRT ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">h ðX Þ t ¼ A T diagf½Y a ðAXÞ aþ1 Ã;t gA 2 R RÂR .</formula><p>Similarly for A, we get</p><formula xml:id="formula_10">G ðAÞ D A ¼ r A D A ¼ 1 a ðE À ðY AXÞ a ÞX T 2 R MÂR ,<label>ð9Þ</label></formula><p>and the Hessian H ðAÞ D A 2 R MRÂMR is as follows:</p><formula xml:id="formula_11">H ðAÞ D A ¼ 1 a diagf½h ðAÞ m m¼1;...;M g,<label>(10)</label></formula><p>where</p><formula xml:id="formula_12">h ðAÞ m ¼ X diagf½Y a ðAXÞ aþ1 m;Ã gX T 2 R RÂR .</formula><p>For the specific case, when a ! 0, the a-divergence converges to the dual KL divergence, i.e.</p><formula xml:id="formula_13">D KL2 ðAXjjYÞ ¼ lim a!0 D A ðYjjAXÞ ¼ X mt z mt ln z mt y mt þ y mt À z mt ,<label>ð11Þ</label></formula><formula xml:id="formula_14">z mt ¼ ½AX mt ,</formula><p>and consequently, the gradient and Hessian matrices simplify as follows:</p><p>For X:</p><formula xml:id="formula_15">G ðX Þ D KL2 ¼ r X D KL2 ¼ A T lnðAX YÞ 2 R RÂT ,<label>ð12Þ</label></formula><p>and</p><formula xml:id="formula_16">H ðX Þ D KL2 ¼ diagf½h ðX Þ t t¼1;...;T g 2 R RTÂRT ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_17">h ðX Þ t ¼ A T diagf½E ðAXÞ Ã;t gA 2 R RÂR .</formula><p>For A:</p><formula xml:id="formula_18">G ðAÞ D KL2 ¼ r A D KL2 ¼ lnðAX YÞX T 2 R MÂR ,<label>ð14Þ</label></formula><p>and</p><formula xml:id="formula_19">H ðAÞ D KL2 ¼ diagf½h ðAÞ m m¼1;...;M g 2 R MRÂMR ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_20">h ðAÞ m ¼ X diagf½E ðAXÞ m;Ã gX T 2 R RÂR .</formula><p>The a-divergence unifies many well-known statistical distances, which makes our NMF algorithm more flexible to various distributions of noise. For a ¼ 2 we have the Pearson's distance which can be regarded as a normalized squared Euclidean distance. However, the basic Euclidean distance cannot be derived from the a-divergence. This case may be very useful in practice, since a normally distributed noise happens very often. For the squared Euclidean distance:</p><formula xml:id="formula_21">D F ðYjjAXÞ ¼ 1 2 jjY À AXjj 2 F ,<label>(16)</label></formula><p>the gradients and Hessians have the corresponding forms</p><formula xml:id="formula_22">G ðX Þ D F ¼ A T ðAX À YÞ 2 R RÂT ,<label>(17)</label></formula><formula xml:id="formula_23">G ðAÞ D F ¼ ðAX À YÞX T 2 R MÂR , (<label>18</label></formula><formula xml:id="formula_24">)</formula><formula xml:id="formula_25">H ðX Þ D F ¼ I T A T A 2 R RTÂRT ,<label>(19)</label></formula><formula xml:id="formula_26">H ðAÞ D F ¼ I M XX T 2 R MRÂMR , (<label>20</label></formula><formula xml:id="formula_27">)</formula><p>where I T 2 R TÂT and I M 2 R MÂM are identity matrices, and stands for a Kronecker product.</p><p>In each alternating step, the columns of A are normalized to a unity of l 1 norm, i.e. we have:</p><formula xml:id="formula_28">a mr a mr = P M m¼1 a mr . Remark 1. All the Hessians H ðX Þ D A , H ðAÞ D A , H ðX Þ D KL2 , H ðAÞ D KL2 , H ðX Þ D F</formula><p>and H ðAÞ D F have a block-diagonal structure. For a40, the Hessian given by ( <ref type="formula" target="#formula_8">8</ref>) is not positive-definite if 9t; 8m : y mt ¼ 0. Similarly, if 9m; 8t : y mt ¼ 0, the Hessian (10) is also not positive-definite. This case may happen if A and X are very sparse. Also, if Y has many zero-value entries, the Hessians may be semi-positive definite. For the Hessians ( <ref type="formula" target="#formula_16">13</ref>), ( <ref type="formula" target="#formula_19">15</ref>), <ref type="bibr" target="#b18">(19)</ref> and <ref type="bibr" target="#b19">(20)</ref> the normalization of the columns in A in each alternating step keeps their positive-definiteness, however, they can be still very ill-conditioned, especially in early updates. Thus, to avoid a breakdown of Newton iterations, some regularization of the Hessian is essential, which leads to quasi-Newton iterations. We applied the Levenberg-Marquardt approach with the exponentially decreasing regularization parameter: l ¼ lðHÞ 0 þ l ðHÞ 0 expfÀt ðHÞ kg. Such regularization substantially reduces possible ill-conditioning of the Hessian (random initialization) during initial iterations when the updates are still very far from the solution.</p><p>Additionally, we control the convergence by a slight relaxation of the iterative updates. To reduce a computational cost substantially, the inversion of the Hessian is replaced with the Q-less QR factorization computed with LAPACK. Thus,</p><formula xml:id="formula_29">H ðX Þ D A þ lI X ¼ Q X R X ; W X ¼ Q T X r X D A , H ðAÞ D A þ lI A ¼ Q A R A ; W A ¼ Q T A r A D A</formula><p>, where R X and R A are upper triangular matrices, and Q X and Q A are orthogonal matrices that are not explicitly computed with the Q-less QR factorization. The final form of the algorithm with the quasi-Newton algorithm is</p><formula xml:id="formula_30">X P O X ½X À gR À1 X W X ,<label>(21)</label></formula><formula xml:id="formula_31">A P O A ½A À gR À1 A W A ,<label>(22)</label></formula><p>where I X 2 R RTÂRT , I A 2 R MRÂMR are identity matrices, and g controls the relaxation. We set g ¼ 0:9. Since the matrices R X and R A are upper triangular, the computational complexity of the ( <ref type="formula" target="#formula_30">21</ref>) and ( <ref type="formula" target="#formula_31">22</ref>) is the lowest with the Gaussian elimination, however, a direct inversion or even a pseudo-inversion may be more suitable for ill-conditioned NMF problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fixed-point algorithm</head><p>In our application, X has much larger dimensions than A, and hence, the computation of X with the Newton method may be highly time consuming or even intractable, even though the Hessian is very sparse. Let us assume some typical case: M ¼ 20, R ¼ 10, and T ¼ 1000. Thus, the Hessian H ðAÞ has size 200 Â 200 with MR 2 ¼ 2 Â 10 3 nonzero entries, but the size of H ðX Þ is 10 4 by 10 4 with TR 2 ¼ 10 5 nonzero entries. For this reason, we do not apply the Newton method for updating X. This can be also justified by the fact that the computation of A needs to solve the system which is much more over determined than for X, and hence, this may be better done with the second-order method since the information about the curvature of the cost function is exploited.</p><p>In this paper, the nonnegative components in X are basically estimated with the modified FP algorithm that solves a regularized least-squares problem</p><formula xml:id="formula_32">X Ã ¼ arg min X 1 2 jjY À AXjj 2 F þ l X 2 JðXÞ . (<label>23</label></formula><formula xml:id="formula_33">)</formula><p>The regularization term</p><formula xml:id="formula_34">JðXÞ ¼ X t ðjjx t jj 1 Þ 2 ¼ trfX T EXg,</formula><p>where x t is the tth column of X and E 2 R RÂR is a matrix of all ones entries, enforces sparsity in the columns of X and it is motivated by the diversity measure 1 used in the M-FOCUSS algorithm <ref type="bibr" target="#b43">[44]</ref>. Let CðXÞ be the objective function in <ref type="bibr" target="#b22">(23)</ref>. Thus, the stationary point of CðXÞ is reached when</p><formula xml:id="formula_35">r X CðXÞ ¼ A T ðAX À YÞ þ l X EX ¼ 0,</formula><p>which leads to the following regularized least-squares solution</p><formula xml:id="formula_36">X LS ¼ ðA T A þ l X EÞ À1 A T Y.</formula><p>Then, to satisfy the nonnegativity constraints, X LS is projected into O X that is defined by <ref type="bibr" target="#b4">(5)</ref>. The cost function given by the regularized square Euclidean distance as in <ref type="bibr" target="#b22">(23)</ref> works the best with a Gaussian noise (matrix V in (1)), however, the computation of A uses the a-divergence which is optimal for a wide spectrum of signal distributions. We set the regularization parameter l X in (23) according to the exponential rule, i.e.</p><formula xml:id="formula_37">l X ¼ l ðkÞ X ¼ l 0 expfÀtkg, (<label>24</label></formula><formula xml:id="formula_38">)</formula><p>where k is a number of alternating steps. This rule is motivated by a temperature schedule in the simulated annealing that steers the solution towards a global one. Larger parameter l 0 and smaller t should give better results but at the cost of high increase in a number of alternating steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Reduced newton optimization</head><p>The GPCG is a hybrid nested iterative method for nonnegatively constrained convex optimization. In this section, we apply the modified GPCG to perform the Steps 1 and 2 in Algorithm 1, where the cost functions DðYjjAXÞ and DðYjjAXÞ are defined by the a-divergence (2) for a40, and by the dual KL divergence <ref type="bibr" target="#b10">(11)</ref> for a ¼ 0. In NMF, we solve two separate problems:</p><formula xml:id="formula_39">X Ã ¼ arg min x rt X0 DðYjjAXÞ,<label>(25)</label></formula><p>A Ã ¼ arg min a mr X0</p><p>DðYjjAXÞ.</p><p>We restrict the description of the GPCG only to the case of solving problem <ref type="bibr" target="#b24">(25)</ref>. Problem ( <ref type="formula" target="#formula_40">26</ref>) can be also similarly treated by applying the GPCG to the transposed system: X T A T ¼ Y T , where A is unknown given X and Y.</p><p>In the remainder of the paper, we use the same notation for a gradient and Hessian of the adivergence and dual KL divergence, i.e.</p><formula xml:id="formula_41">P ðX Þ ¼ G ðX Þ D A , P ðAÞ ¼ G ðAÞ D A , H X ¼ H ðX Þ D A and H A ¼ H ðAÞ D A for a40, and P ðX Þ ¼ G ðX Þ D KL2 , P ðAÞ ¼ G ðAÞ D KL2 , H X ¼ H ðX Þ D KL2 and H A ¼ H ðAÞ D KL2 for a ¼ 0.</formula><p>The solution X Ã of the constrained problem <ref type="bibr" target="#b24">(25)</ref> should satisfy the KKT conditions, i.e. 8r; t :</p><formula xml:id="formula_42">q qx rt DðYjjAX Ã Þ ¼ 0 if x Ã rt 40,<label>(27)</label></formula><p>8r; t :</p><formula xml:id="formula_43">q qx rt DðYjjAX Ã Þ40 if x Ã rt ¼ 0. (<label>28</label></formula><formula xml:id="formula_44">)</formula><p>The GPCG is a two-step algorithm: in the first step, a solution is updated with GP iterations, and in the other step, only the gradient P ðX Þ ¼ r X DðYjjAXÞ is updated with the Newton iterations applied to solve a subproblem, i.e. the reduced system that is obtained from the original one by removing the components that satisfy <ref type="bibr" target="#b27">(28)</ref>. Hence, it is called the ''reduced'' Newton optimization.</p><p>The GP updates are as follows:</p><formula xml:id="formula_45">X P O X ½X À Z p P ðX Þ ,<label>(29)</label></formula><p>where P O X ½x is a projection of x onto the feasible set O X defined by ( <ref type="formula" target="#formula_5">5</ref>). The step length Z p is inexactly estimated with the Armijo rule, i.e. Z p ¼ b p Z 0 , for p ¼ 1; 2; . . . ; and b 2 ð0; 1Þ. The iterations for Z p are stopped at the first p for which</p><formula xml:id="formula_46">DðYjjAXÞ À DðYjjAXðZ p ÞÞX m Z p jjX À XðZ p Þjj 2 F ,</formula><p>and the initial step length Z 0 is defined for each update as the Cauchy point: 1 The diversity measure J ðp;qÞ ¼ P n i¼1 ðjjx½ijj q Þ p , pX0, qX1, where x½i is the ith row of the matrix X (n Â L), was introduced by Cotter et al. <ref type="bibr" target="#b43">[44]</ref> to enforce sparsity in the rows of X. Since we are more concerned with a sparsity column profile, we apply the measure to the columns in X instead of the rows, assuming q ¼ 1 and</p><formula xml:id="formula_47">Z 0 ¼ p T X p X p T X H X p X , ARTICLE IN PRESS</formula><formula xml:id="formula_48">p ¼ 2.</formula><p>where H X is the Hessian of DðYjjAXÞ. The vector p X is a vectorized version of the gradient matrix</p><formula xml:id="formula_49">P ðX Þ ¼ ½p rt 2 R RÂT , i.e. p X ¼ vecðP ðX Þ Þ ¼ ½p ðX Þ 11 ; p ðX Þ 21 ; . . . ; p ðX Þ R1 ; p ðX Þ 12 ; . . . ; p ðX Þ RT T 2 R RT .<label>ð30Þ</label></formula><p>In the second step, only gradient p X is updated with the standard CG method <ref type="bibr" target="#b39">[40]</ref> that solves, socalled, the reduced system:</p><formula xml:id="formula_50">H ðX Þ R pX ¼ Àp ðX Þ R ,<label>(31)</label></formula><p>where</p><formula xml:id="formula_51">p ðX Þ R ¼ D ðX Þ p X , H ðX Þ R ¼ D ðX Þ H X D ðX Þ À D ðX Þ þ I ðX Þ</formula><p>, are the reduced gradient and reduced Hessian. The diagonal matrix D ðX Þ consists of the indices of the inactive variables, i.e. the entries of the solution X that are positive at a given outer iterative step. Thus,</p><formula xml:id="formula_52">D ðX Þ ¼ diagfz ðX Þ g, where z X ¼ vecðZ ðX Þ Þ ¼ ½z ðX Þ 11 ; z ðX Þ 21 ; . . . ; z ðX Þ R1 ; z ðX Þ 12 ; . . . ; z ðX Þ RT T 2 R RT ,<label>ð32Þ</label></formula><formula xml:id="formula_53">Z ðX Þ ¼ ½z ðX Þ rt ; z ðX Þ rt ¼ 1 if x rt 40;</formula><p>0 otherwise;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(</head><p>The matrix I ðX Þ 2 R RTÂRT denotes an identity matrix. By applying the diagonal scaling, the zero-value components of the current GP update are not considered in evaluation of the gradient which will be used in the next GP update. In this way, system (31) is very sparse.</p><p>Remark 2. The convergence of the CG is guaranteed in a finite number of iterations since H ðX Þ R is positivedefinite. This is because D A ðYjjAXÞ is convex for aX0 and P ðX Þ is a nonsingular matrix. The GP with the Armijo rule is always convergent to a stationary point of D A ðYjjAXÞ <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, which is a unique solution.</p><p>Algorithm 2. GPCG-NMF.</p><p>Set Random initialize: A, X, b 2 ð0; 1Þ, m 2 ð0; 1Þ, g GP 2 ð0; 1Þ, For s ¼ 0; 1; . . . ; % Alternating</p><p>Step 1: Do X-GPCG iterations with Algorithm 3,</p><p>Step 2: Do A-GPCG iterations with Algorithm 4, End % Alternating</p><p>The solution pX of ( <ref type="formula" target="#formula_50">31</ref>) is then used in the GP iterations in <ref type="bibr" target="#b28">(29)</ref>, i.e. the following gradient matrix is created from the vector pX :</p><formula xml:id="formula_54">P ðX Þ Matrixðp X Þ 2 R RÂT . (<label>33</label></formula><formula xml:id="formula_55">)</formula><p>The termination of both GP and CG iterations is achieved with the gradient descent criteria that are given in <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. The Algorithm 2 is the simplified version of the GPCG adapted for NMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Numerical results</head><p>The proposed NMF algorithms have been extensively tested for many difficult benchmarks for signals and images with various statistical distributions. Here we show two illustrative examples. The performance of the algorithms is also estimated with a quantity measure: signal-to-interference ratio (SIR).</p><p>The five statistically dependent nonnegative signals shown in Fig. <ref type="figure" target="#fig_0">1(a</ref>) have been mixed by randomly generated nonnegative well-conditioned matrix A 2 R 5Â5 (condðAÞ ' 10) with a uniform distribution. The mixed signals are shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Using the standard multiplicative NMF algorithms we failed to estimate the original sources. Figs. <ref type="figure" target="#fig_1">2(a</ref>) and 3(left) illustrate the results obtained with the standard Lee-Seung algorithm, referred here to as the EMML for alternating minimization of the KL divergence. The results shown in Figs. <ref type="figure" target="#fig_1">2(b</ref>) and 3(right) are obtained with the hybrid algorithm: FP for X (Step 1) and quasi-Newton for A (Step 2). Fig. <ref type="figure" target="#fig_2">3</ref> presents the histograms from 100 mean-SIR samples generated with the Monte Carlo (MC) analysis for two different algorithms. In each MC run only initial matrices A ð0Þ and X ð0Þ were randomized. The worst case with the quasi-Newton algorithm has the mean-SIR value greater than 140 dB. Also, the standard deviation of the mean-SIRs with the quasi-Newton algorithm is very small (less than 1 dB). This suggests that our algorithm is rather convergent to the global minimum of the cost function. <ref type="foot" target="#foot_1">2</ref> In the experiments for a fixed number of alternating steps (usually 1500), we set l 0 ¼ 200 and t ¼ 0:02 in the exponential rule for computing X. The parameters for the Levenberg-Marquardt regularization are set heuristically as follows: lðHÞ 0 ¼ 10 À12 , l ðHÞ 0 ¼ 10 8 and t ðHÞ ¼ 1 for the Euclidean distance, and lðHÞ 0 ¼ 10 À12 , l ðHÞ 0 ¼ 10 15 and t ðHÞ ¼ 2 for the a-divergence. Algorithm 3. X-GPCG.</p><p>For k ¼ 0; 1; . . . ; % Inner loop for X Step 1:</p><formula xml:id="formula_56">P ðX Þ Àr X DðYjjAXÞ, vecðP ðX Þ Þ ¼ ½p ðX Þ 11 ; p ðX Þ 21 ; . . . ; p ðX Þ R1 ; p ðX Þ 12 ; . . . ; p ðX Þ RT T 2 R RT ,</formula><p>where</p><formula xml:id="formula_57">P ðX Þ ¼ ½p ðX Þ rt , % Vectorization H X ¼ r 2 X DðYjjAXÞ, % Hessian Step 2: X maxfX þ Z p P ðX Þ ; 0g, % Projection where Z p ¼ b p Z 0 , Z 0 ¼ vecðP ðX Þ Þ T vecðP ðX Þ Þ vecðP ðX Þ Þ T H X vecðP ðX Þ Þ</formula><p>, and p ¼ 0; 1; . . . is the first nonnegative integer for which:  Step 3:  Figs. <ref type="figure" target="#fig_5">5</ref> and<ref type="figure" target="#fig_6">6</ref> present the separation results. Again, the standard Lee-Seung algorithm fails to give satisfactory results, which is visible in Fig. <ref type="figure" target="#fig_5">5(a)</ref>. The images separated with the hybrid algorithm: FP for X and quasi-Newton for A are illustrated in Fig. <ref type="figure" target="#fig_5">5(b)</ref>. The results obtained with the GPCG applied for the Euclidean distance with five inner iteration (k ¼ 5) are shown in Fig. <ref type="figure" target="#fig_6">6</ref>. In this case, we have also used the multilayer technique that has been presented in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>   very efficiently with the GPCG. However, the best results are obtained with the hybrid quasi-Newton and FP algorithm.</p><formula xml:id="formula_58">DðYjjAXðZ p ÞÞ À DðYjjAXÞp À m Z p jjX À XðZ p Þjj 2 F ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><formula xml:id="formula_59">Z ðX Þ ¼ ½z ðX Þ rt , z ðX Þ rt ¼ 1 if x rt 40; 0 otherwise; ( z ðX Þ ¼ vecðZ ðX Þ Þ ¼ ½z ðX Þ 11 ; z ðX Þ 21 ; . . . ; z ðX Þ R1 ; z ðX Þ 12 ; . . . ; z ðX Þ RT T 2 R RT , D ðX Þ ¼ diagfz ðX Þ g, Step 4: P ðX Þ r X DðYjjAXÞ, % Gradient Step 5: p ðX Þ R ¼ D ðX Þ vecðP ðX Þ Þ, % Reduced grad. H ðX Þ R ¼ D ðX Þ H X D ðX Þ þ I ðX Þ À D ðX Þ , Step 6: Solve: H ðX Þ R p X ¼ Àp ðX Þ R with CG algorithm P ðX Þ Matrixðp X Þ 2 R RÂT , Step7: X maxfX þ Z p P ðX Þ ; 0g, % Projection where Z p ¼ b p Z 0 , Z 0 ¼ 1,</formula><p>The simulation results confirmed that the developed algorithms are efficient and stable for a wide set of parameters, however, the NMF problem cannot be too much ill-conditioned (especially mixing matrix A). Additionally, to improve the ill-conditioning the rows in Y are scaled to have the unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we proposed the second-order methods to the NMF problem. We derived the new method for NMF, which uses the quasi-Newton iterates for updating A and the FP regularized leastsquares algorithm for computing X. As alternative approach, we have also developed the GPCG algorithm for NMF with the a-divergence. Using synthetic data for BSS applications, we demonstrated the robustness and high performance of our new algorithms. We obtained the best results with the quasi-Newton FP algorithm. The GPCG gives slightly worse results but together with the multilayer technique it may be competitive to the quasi-Newton FP algorithm, especially for noisy or large-scale data. We have tested our algorithms on many difficult benchmarks of signals and images, and we believe that the GPCG algorithm can be also very useful for many NMF applications. The discussed algorithms have been implemented in the MATLAB toolbox: NMFLAB for Signal and Image Processing <ref type="bibr" target="#b44">[45]</ref>. The Matlab source code of the hybrid algorithm: FP for computing X and quasi-Newton for A is included in Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Original five source signals, (b) observed five mixed signals with dense mixing matrix (noise-free data).</figDesc><graphic coords="7,42.51,66.73,218.52,416.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Estimated sources with: (a) standard Lee-Seung algorithm (EMML) (SIR ¼ 1:9, 11.4, 2.3, 11.4, 5.4 dB, respectively), (b) FP for X (Step 1) and quasi-Newton for A (Step 2) (SIR ¼ 141:7, 145.7, 138.8, 141.4, 154.2 dB, respectively).</figDesc><graphic coords="7,287.59,66.73,218.52,398.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Histograms from 100 mean-SIR samples generated with the following algorithms: (left) standard Lee-Seung algorithm (EMML); (right) FP for X (Step 1) and quasi-Newton for A (Step 2).</figDesc><graphic coords="8,84.04,66.73,372.24,197.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figs.5 and 6present the separation results. Again, the standard Lee-Seung algorithm fails to give satisfactory results, which is visible in Fig.5(a). The images separated with the hybrid algorithm: FP for X and quasi-Newton for A are illustrated in Fig.5(b). The results obtained with the GPCG applied for the Euclidean distance with five inner iteration (k ¼ 5) are shown in Fig.6. In this case, we have also used the multilayer technique that has been presented in<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>. Figs.6(a) and (b) show the images separated with the GPCG with one and three layers, respectively. The multilayer technique also works</figDesc><graphic coords="9,287.59,225.84,218.52,412.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Original four source images, (b) observed nine mixed images.</figDesc><graphic coords="9,42.51,249.59,218.52,409.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Estimated sources with: (a) standard Lee-Seung algorithm (ISRA) (SIR ¼ 9:5, 13.1, 8.3, 6.9 dB, respectively), (b) FP for X (Step 1) and quasi-Newton for A (Step 2) (SIR ¼ 39:6, 17.3, 32.5, 22.1 dB, respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Estimated sources with: (a) GPCG algorithm with one layer (SIR ¼ 11.7, 12.9, 13.2, 19.5 dB, respectively), (b) GPCG algorithm with three layers (SIR ¼ 15.2, 18.5, 16.6, 18.6 dB, respectively).</figDesc><graphic coords="10,38.21,66.73,218.52,413.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 can be different than the function</figDesc><table><row><cell>DðYjjAXÞ</cell><cell>in</cell><cell>Step</cell><cell>2,</cell><cell>however,</cell><cell>usually</cell></row><row><cell cols="3">DðYjjAXÞ ¼ DðYjjAXÞ.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Algorithm 1. General form of NMF.</cell><cell></cell></row><row><cell cols="5">Set Randomly initialize: A ð0Þ , X ð0Þ ,</cell><cell></cell></row><row><cell cols="5">For s ¼ 1; 2; . . . ; until convergence do</cell><cell></cell></row><row><cell>Step 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>ðsþ1Þ ¼ arg min x rtX0 DðYjjA ðsÞ XÞj X ðsÞ Step 2:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and p ¼ 0; 1; . . . is the first nonnegative integer for which: DðYjjAXðZ p ÞÞoDðYjjAXÞ, Projection where Z p ¼ b p Z 0 , Z 0 ¼ 1, and p ¼ 0; 1; . . . is the first nonnegative</figDesc><table><row><cell>If DðYjjAX ðkÀ1Þ Þ À DðYjjAX ðkÞ Þp g GP maxfDðYjjAX ðlÀ1Þ Þ ÀDðYjjAX ðlÞ Þjl ¼ 1; . . . ; k À 1g, Break End If End % Inner loop for X Algorithm 4. A-GPCG.</cell><cell>Z 0 ¼ and p ¼ 0; 1; . . . is the first nonnegative vecðP ðAÞ Þ T vecðP ðAÞ Þ vecðP ðAÞ Þ T H X vecðP ðAÞ Þ , integer for which: DðYjjAðZ p ÞXÞ À DðYjjAXÞp À m Z p jjA À AðZ p Þjj 2 F , Z ðAÞ ¼ ½z ðAÞ mr , z ðAÞ mr ¼ 1 if a mr 40; 0 otherwise; ( z ðAÞ ¼ vecðZ ðAÞ Þ ¼ ½z ðAÞ 11 ; z ðAÞ 12 ; . . . ; z ðAÞ 1R ; z ðAÞ 21 ; . . . ; z ðAÞ MR T 2 R MR , D ðAÞ ¼ diagfz ðAÞ g, Step 4: P ðAÞ Step 3: r A DðYjjAXÞ, % Gradient Step 5: p ðAÞ R ¼ D ðAÞ vecðP ðAÞ Þ, % Reduced grad. H ðAÞ R ¼ D ðAÞ H A D ðAÞ þ I ðAÞ À D ðAÞ , Step 6: Solve: H ðAÞ R p A ¼ Àp ðAÞ R with CG algorithm P ðAÞ Matrixðp ARTICLE IN PRESS</cell></row></table><note><p>For k ¼ 0; 1; . . . ; % Inner loop for A Step 1: P ðAÞ Àr A DðYjjAXÞ 2 R MÂR , vecðP ðAÞ Þ ¼ ½p ðAÞ 11 ; p ðAÞ 12 ; . . . ; p ðAÞ 1R ; p ðAÞ 21 ; . . . ; p ðAÞ MR T 2 R MR , where P ðAÞ ¼ ½p ðAÞ mr , % Vectorization H A ¼ r 2 A DðYjjAXÞ, % Hessian Step 2: A maxfA þ Z p P ðAÞ ; 0g, % Projection where Z p ¼ b p Z 0 , A Þ 2 R MÂR , Step 7: X maxfA þ Z p P ðAÞ ; 0g, %</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>R. Zdunek, A. Cichocki / Signal Processing 87(2007) 1904-1916  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Even a quadratic cost function DðYjjAXÞ ¼ jjY À AXjj F with respect to both sets of arguments (A and X) may have many local minima. R. Zdunek, A. Cichocki / Signal Processing 87(2007) 1904-1916  </p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>% Second-order NMF algorithm function [A,X] ¼ nmf_newton(Y,R,CostFun,MaxIter,Alpha0,Tau,Alpha) % INPUTS: % Y: Data matrix (M by T): (model Y ¼ AX s.t . nonnegativity constraints in A and X), % R: Low-rank, % CostFun: Cost function: 1 -Frobenius norm (default), 2 -Alpha-divergence, % MaxIter: Max. number of alternating steps (default: MaxIter ¼ 1500), % Alpha0: Initial value of regular.parameter (default: Alpha0 ¼ 100), % Tau: Damping factor (default: Tau ¼ 50), % Alpha: Free parameter in alpha-divergence (default: Alpha ¼ 1), % % OUTPUT: % A and X: Estimated NMF factors, </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introducing a weighted nonnegative matrix factorization for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guillamet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">`</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="2447" to="2454" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing non-negative matrix factorization for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guillamet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on Pattern Recognition (ICPR&apos;02)</title>
		<meeting><address><addrLine>Quebec City, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="116" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying faces with nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guillamet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Fifth Catalan Conference for Artificial Intelligence</title>
		<meeting>of the Fifth Catalan Conference for Artificial Intelligence<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Castello de la Plana</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multiple nonnegativematrix factorization of dynamic PET images</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Application of nonnegative matrix factorization to dynamic positron emission tomography</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Independent Component Analysis and Blind Signal Separation</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with orthogonality constraints for chemical agent detection in raman spectra</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Machine Learning for Signal Processing</title>
		<meeting><address><addrLine>Mystic, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Biclustering of gene expression data by non-smooth non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carmona-Saez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Pascual-Marqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tirado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Carazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pascual-Montano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">78</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Pascual-Montano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Carazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kochi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lehmean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascual-Marqui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonsmooth nonnegative matrix factorization (nsNMF)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="403" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Document clustering using non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="373" to="386" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast nonnegative matrix factorization and its application for protein fold recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Okun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Priisalu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">c</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Article ID 71817</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization framework for face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognition Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization based methods for object recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="893" to="897" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning image components for object recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Spratling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="793" to="815" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization for rapid recovery of constituent spectra in magnetic resonance chemical shift imaging of the brain</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S D C</forename><surname>Shungu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1453" to="1465" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recovery of constituent spectra in 3d chemical shift imaging using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stoyanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Symposium on Independent Component Analysis and Blind Signal Separation</title>
		<meeting><address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recovery of constituent spectra using non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proceedings of SPIE-Wavelets: Applications in Signal and Image Processing</title>
		<imprint>
			<biblScope unit="volume">5207</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Concept decompositions for large sparse text data using clustering</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. J</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="143" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonnegative features of spectro-temporal sounds for classification</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1327" to="1336" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Metagenes and molecular pattern discovery using matrix factorization</title>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Mesirov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="4164" to="4169" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting characteristic patterns from genome-wide expression data by non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computational Systems Bioinformatics Conference (CSB 2004)</title>
		<meeting>the 2004 IEEE Computational Systems Bioinformatics Conference (CSB 2004)<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Csiszar&apos;s divergences for non-negative matrix factorization: family of new algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3889</biblScope>
			<biblScope unit="page" from="32" to="39" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian-based iterative method of image restoration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="59" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An iterative technique for the rectification of observed distributions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lucy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astron. J</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="745" to="754" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EM reconstruction algorithms for emission and transmission tomography</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Assisted Tomogr</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="306" to="316" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accelerating the EMML algorithm and related iterative algorithms by rescaled block-iterative (RBI) methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="100" to="109" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the relation between the ISRA and the EM algorithm for positron emission tomography</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R D</forename><surname>Pierro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="333" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Projected gradient methods for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="hhttp://www.csie.ntu.edu.tw/$cjlini" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Optimality, computation, and interpretation of nonnegative matrix factorizations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Diele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ragni</surname></persName>
		</author>
		<ptr target="hhttp://www.citeseer.ist.psu.edu/758183.htmli" />
		<imprint>
			<date type="published" when="2004-10-18">October 18, 2004</date>
		</imprint>
	</monogr>
	<note>unpublished report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Algorithms and applications for approximate nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Langville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csda.2006.11.006</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal., in press</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multilayer nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="947" to="948" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularized alternating least squares algorithms for non-negative matrix/tensor factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Symposium on Neural Networks (ISNN)</title>
		<meeting>the Fourth International Symposium on Neural Networks (ISNN)<address><addrLine>Nanjing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">June 3-7, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multilayer nonnegative matrix factorization using projected gradient approaches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Neural Information Processing</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An interior-point gradient method for large-scale totally nonnegative least squares problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with quasi-Newton optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4029</biblScope>
			<biblScope unit="page" from="870" to="879" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the solution of large quadratic programming problems with bound constraints</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toraldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="113" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonnegatively constrained convex programming method for image reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1326" to="1343" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A nonnegatively constrained trust region algorithm for the restoration of images with an unknown blur</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bardsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Trans. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="139" to="153" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Method of conjugate gradients for solving linear systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Hestenes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stiefel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Res. Nat. Bur. Stand</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="409" to="436" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
		<title level="m">Differential-Geometrical Methods in Statistics</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Goodness-of-Fit Statistics for Discrete Multivariate Data</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Cressie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Read</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extended SMART algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kompass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4029</biblScope>
			<biblScope unit="page" from="548" to="562" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sparse solutions to linear inverse problems with multiple measurement vectors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Engan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreutz-Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2477" to="2488" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<ptr target="hhttp://www.bsp.brain.riken.jpi" />
	</analytic>
	<monogr>
		<title level="m">NMFLAB for Signal and Image Processing</title>
		<meeting><address><addrLine>Saitama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Laboratory for Advanced Brain Signal Processing</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
