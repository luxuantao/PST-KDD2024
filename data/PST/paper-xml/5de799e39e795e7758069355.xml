<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quadratic Video Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
							<email>xuxiangyu2014@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Siyao</surname></persName>
							<email>lisiyao1@sensetime.com</email>
						</author>
						<author>
							<persName><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
							<email>sunwenxiu@sensetime.com</email>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Yin</surname></persName>
							<email>yinqian@bnu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Merced Google</orgName>
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quadratic Video Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A8423BDC1BD07E65992B3EFE1C13B60E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video interpolation is an important problem in computer vision, which helps overcome the temporal limitation of camera sensors. Existing video interpolation methods usually assume uniform motion between consecutive frames and use linear models for interpolation, which cannot well approximate the complex motion in the real world. To address these issues, we propose a quadratic video interpolation method which exploits the acceleration information in videos. This method allows prediction with curvilinear trajectory and variable velocity, and generates more accurate interpolation results. For high-quality frame synthesis, we develop a flow reversal layer to estimate flow fields starting from the unknown target frame to the source frame. In addition, we present techniques for flow refinement. Extensive experiments demonstrate that our approach performs favorably against the existing linear models on a wide variety of video datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video interpolation aims to synthesize intermediate frames between the original input images, which can temporally upsample low-frame rate videos to higher-frame rates. It is a fundamental problem in computer vision as it helps overcome the temporal limitations of camera sensors and can be used in numerous applications, such as motion deblurring <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>, video editing <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38]</ref>, virtual reality <ref type="bibr" target="#b0">[1]</ref>, and medical imaging <ref type="bibr" target="#b10">[11]</ref>.</p><p>Most state-of-the-art video interpolation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref> explicitly or implicitly assume uniform motion between consecutive frames, where the objects move along a straight line at a constant speed. As such, these approaches usually adopt linear models for synthesizing intermediate frames. However, the motion in real scenarios can be complex and non-uniform, and the uniform assumption may not always hold in the input videos, which often leads to inaccurate interpolation results. Moreover, the existing models are mainly developed based on two consecutive frames for interpolation, and the higher-order motion information of the video (e.g., acceleration) has not been well exploited. An effective frame interpolation algorithm should use additional input frames and estimate the higher-order information for more accurate motion prediction.</p><p>To this end, we propose a quadratic video interpolation method to exploit additional input frames to overcome the limitations of linear models. Specifically, we develop a data-driven model which integrates convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref> and quadratic models <ref type="bibr" target="#b14">[15]</ref> for accurate motion estimation and image synthesis. The proposed algorithm is acceleration-aware, and thus The other three subfigures show the interpolated results between frame 0 and 1 by different algorithms.</p><p>Note that we overlap these results for better visualizing the interpolation trajectories. Since the linear model <ref type="bibr" target="#b30">[31]</ref> assumes uniform motion between the two frames, it does not approximate the movement in real world well. In contrast, our quadratic approach can exploit the acceleration information from the four neighboring frames and generate more accurate in-between video frames.</p><p>allows predictions with curvilinear trajectory and variable velocity. Although the ideas of our method are intuitive and sensible, this task is challenging as we need to estimate the flow field from the unknown target frame to the source frame (i.e., backward flow) for image synthesis, which cannot be easily obtained with existing approaches. To address this issue, we propose a flow reversal layer to effectively convert forward flow to backward flow. In addition, we introduce new techniques for filtering the estimated flow maps. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the proposed quadratic model can better approximate pixel motion in real world and thus obtain more accurate interpolation results.</p><p>The contributions of this work can be summarized as follows. First, we propose a quadratic interpolation algorithm for synthesizing accurate intermediate video frames. Our method exploits the acceleration information of the video, which can better model the nonlinear movements in the real world. Second, we develop a flow reversal layer to estimate the flow field from the target frame to the source frame, thereby facilitating high-quality frame synthesis. In addition, we present novel techniques for refining flow fields in the proposed method. We demonstrate that our method performs favorably against the state-of-the-art video interpolation methods on different video datasets. While we focus on quadratic functions in this work, the proposed framework for exploiting the acceleration information is general, and can be further extended to higher-order models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most state-of-the-art approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> for video interpolation explicitly or implicitly assume uniform motion between consecutive frames. As a typical example, Baker et al. <ref type="bibr" target="#b1">[2]</ref> use optical flow and forward warping to linearly move pixels to the intermediate frames. Liu et al. <ref type="bibr" target="#b13">[14]</ref> develop a CNN model to directly learn the uniform motion for interpolating the middle frame. Similarly, Jiang et al. <ref type="bibr" target="#b8">[9]</ref> explicitly assume uniform motion with flow estimation networks, which enables a multi-frame interpolation model.</p><p>On the other hand, Meyer et al. <ref type="bibr" target="#b16">[17]</ref> develop a phase-based method to combine the phase information across different levels of a multi-scale pyramid, where the phase is modeled as a linear function of time with the implicit uniform motion assumption. Since the above linear approaches do not exploit higher-order information in videos, the interpolation results are less accurate.</p><p>Kernel-based algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref> have also been proposed for frame interpolation. While these methods are not constrained by the uniform motion model, existing schemes do not handle nonlinear motion in complex scenarios well as only the visual information of two consecutive frames is used for interpolation.</p><p>Closely related to our work is the method by McAllister and Roulier <ref type="bibr" target="#b14">[15]</ref> which uses quadratic splines for data interpolation to preserve the convexity of the input. However, this method can only be applied to low-dimensional data, while we solve the problem of video interpolation which is in much higher dimensions. f →-</p><formula xml:id="formula_0">0 1 f → 1 0 f → 1 2 f → 0 t f → 1 t f → 0 t f → 1 t f → Figure 2:</formula><p>Overview of the quadratic video interpolation algorithm. We first use the off-the-shelf model to estimate flow fields for the input frames. Then we introduce quadratic flow prediction and flow reversal layers to estimate f t→0 and f t→1 . We describe the estimation process of f t→0 in details in this paper, and f t→1 can be computed similarly. Finally, we synthesize the in-between frame by warping and fusing the input frames with f t→0 and f t→1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Algorithm</head><p>To synthesize an intermediate frame Ît where t ∈ (0, 1), existing algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref> usually assume uniform motion between the two consecutive frames I 0 , I 1 , and adopt linear models for interpolation. However, this assumption cannot approximate the complex motion in real world well and often leads to inaccurately interpolated results. To solve this problem, we propose a quadratic interpolation method for predicting more accurate intermediate frames. The proposed method is acceleration-aware, and thus can better approximate real-world scene motion.</p><p>An overview of our quadratic interpolation algorithm is shown in Figure <ref type="figure">2</ref>, where we synthesize the frame Ît by fusing pixels warped from I 0 and I 1 . We use I -1 , I 0 , and I 1 to warp pixels from I 0 and describe this part in details in the following sections, and the warping from the other side (i.e., I 1 ) can be performed similarly by using I 0 , I 1 , and I 2 . Specifically, we first compute optical flow f 0→1 and f 0→-1 with the state-of-the-art flow estimation network PWC-Net <ref type="bibr" target="#b30">[31]</ref>. We then predict the intermediate flow map f 0→t using f 0→1 and f 0→-1 in Section 3.1. In Section 3.2, we propose a new method to estimate the backward flow f t→0 by reversing the forward flow f 0→t . Finally, we synthesize the interpolated results with the backward flow in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quadratic flow prediction</head><p>To interpolate frame Ît , we first consider the motion model of a pixel from I 0 :</p><formula xml:id="formula_1">f 0→t = t 0 v 0 + κ 0 a τ dτ dκ,<label>(1)</label></formula><p>where f 0→t denotes the displacement of the pixel from frame 0 to t, v 0 is the velocity at frame 0, and a τ represents the acceleration at frame τ .</p><p>Existing models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref> usually explicitly or implicitly assume uniform motion and set a τ = 0 between consecutive frames, where (1) can be rewritten as a linear function of t:</p><formula xml:id="formula_2">f 0→t = tf 0→1 .<label>(2)</label></formula><p>However, the objects in real scenarios do not always travel in a straight line at a constant velocity,. Thus, these linear approaches cannot effectively model the complex non-uniform motion and often lead to inaccurate interpolation results.</p><p>In contrast, we take higher-order information into consideration and assume a constant a τ for τ ∈ [-1, 1]. Correspondingly, the flow from frame 0 to t can be derived as:</p><formula xml:id="formula_3">f 0→t = (f 0→1 + f 0→-1 )/2 × t 2 + (f 0→1 -f 0→-1 )/2 × t,<label>(3)</label></formula><p>which is equivalent to temporally interpolating pixels with a quadratic function. This formulation relaxes the constraint of constant velocity and rectilinear movement of linear models, and thus allows accelerated and curvilinear motion prediction between frames. In addition, existing methods with linear models only use the two closest frames I 0 , I 1 , whereas our algorithm naturally exploits visual information from more neighboring frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Flow reversal layer</head><p>While we obtain forward flow f 0→t from quadratic flow prediction, it cannot be easily used for synthesizing images. Instead, we need backward flow f t→0 for high-quality frame synthesis <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref>. To estimate the backward flow, Jiang et al. <ref type="bibr" target="#b8">[9]</ref> introduce a simple method which linearly combines f 0→1 and f 1→0 to approximate f t→0 . However, this approach does not perform well around motion boundaries as shown in Figure <ref type="figure" target="#fig_2">3</ref>(a). More importantly, this approach cannot be applied in our quadratic method to exploit the acceleration information.</p><p>In this work, we propose a flow reversal layer for better prediction of f t→0 . We first project the flow map f 0→t to frame t, where a pixel x on I 0 corresponds to x + f 0→t (x) on I t . Next, we compute the flow of a pixel u on I t by reversing and averaging the projected flow values that fall into the neighborhood N (u) of pixel u. Mathematically, this process can be written as:</p><formula xml:id="formula_4">f t→0 (u) = x+f0→t(x)∈N (u) w( x + f 0→t (x) -u 2 )(-f 0→t (x)) x+f0→t(x)∈N (u) w( x + f 0→t (x) -u 2 ) ,<label>(4)</label></formula><p>where w(d) = e -d 2 /σ 2 is the Gaussian weight for each flow. The proposed flow reversal layer is conceptually similar to the surface splatting <ref type="bibr" target="#b38">[39]</ref> in computer graphics where the optical flow in our work is replaced by camera projection. During training, while the reversal layer itself does not have learnable parameters, it is differentiable and allows the gradients to be backpropagated to the flow estimation module in Figure <ref type="figure">2</ref>, and thus enables end-to-end training of the whole system.</p><p>Note that the proposed reversal approach can lead to holes in the estimated flow map f t→0 , which is mostly due to the objects visible in I t but occluded in I 0 . And the missing objects are filled with the pixels warped from I 1 which is on the other side of the interpolation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Frame synthesis</head><p>In this section, we first refine the reversed flow field with adaptive filtering. Then we use the refined flow to generate the interpolated results with backward warping and frame fusion.</p><p>Adaptive flow filtering. While our approach is effective in reversing flow maps, the generated backward flow f t→0 may still have some ringing artifacts around edges as shown in Figure <ref type="figure" target="#fig_2">3(b)</ref>, which are mainly due to outliers in the original estimations of the PWC-Net. A straightforward way to reduce these artifacts <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref> is to train deep CNNs with residual connections to refine the initial flow maps. However, this strategy does not work well in our practice as shown in Figure <ref type="figure" target="#fig_2">3(c)</ref>. This is because the artifacts from the flow reversal layer are mostly thin streaks with spike values (Figure <ref type="figure" target="#fig_2">3(b)</ref>). Such outliers cannot be easily removed since the weighted averaging of convolution can be affected by the spiky outliers.</p><p>Inspired by the median filter <ref type="bibr" target="#b6">[7]</ref> which samples only one pixel from a neighborhood and avoids the issues of weighted averaging, we propose a flow filtering network to adaptively sample the flow map for removing outliers. While the classical median filter involves indifferentiable operation and cannot be easily trained in our end-to-end model, the proposed method learns to sample one pixel in a neighborhood with neural networks and can more effectively reduce the artifacts of the flow map. Specifically, we formulate the adaptive filtering process as follows:</p><formula xml:id="formula_5">f t→0 (u) = f t→0 (u + δ(u)) + r(u),<label>(5)</label></formula><p>where f t→0 denotes the filtered backward flow, and δ(u) is the learned sampling offset of pixel u. We constrain δ(u) ∈ [-k, k] by using k × tanh(•) as the activation function of δ such that the proposed flow filter has a local receptive field of 2k + 1. Since the flow map is sparse and smooth in most regions, we do not directly rectify the artifacts with CNNs as the schemes in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref>. Instead, we rely on the flow values around outliers by sampling in a neighborhood, where δ is trained to find the suitable sampling locations. The residual map r is learned for further improvement. Our filtering method enables spatially-variant and nonlinear refinement of f t→0 , which could be seen as a learnable median filter in spirit. As show in Figure <ref type="figure" target="#fig_2">3</ref>(d), the proposed algorithm can effectively reduce the artifacts in the reversed flow maps. More implementation details are presented in Section 4.2.</p><p>Warping and fusing source frames. While we obtain f t→0 with the input frames I -1 , I 0 , and I 1 , we can also estimate f t→1 in a similar way with I 0 , I 1 , and I 2 . Finally, we synthesize the intermediate video frames as:</p><formula xml:id="formula_6">Ît (u) = (1 -t)m(u)I 0 (u + f t→0 (u)) + t(1 -m(u))I 1 (u + f t→1 (u)) (1 -t)m(u) + t(1 -m(u))<label>(6)</label></formula><p>where I i (u + f t→i (u)) denotes the pixel warped from frame i to t with bilinear function <ref type="bibr" target="#b7">[8]</ref>. m is a mask learned with a CNN to fuse the warped frames. Similar to <ref type="bibr" target="#b8">[9]</ref>, we also use the temporal distance 1 -t and t for the source frames I 0 and I 1 , such that we can give higher confidence to temporally-closer pixels. Note that we do not directly use the pixels in I -1 and I 2 for image synthesis, as almost all the contents in the intermediate frame can be found in I 0 and I 1 . Instead, I -1 and I 2 are exploited for acceleration-aware motion estimation.</p><p>Since all the above steps of our method are differentiable, we can train the proposed interpolation model in an end-to-end manner. The loss function for training our network is a combination of the 1 loss and perceptual loss <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref>:</p><formula xml:id="formula_7">Ît -I t 1 + λ φ( Ît ) -φ(I t ) 2 ,<label>(7)</label></formula><p>where I t is the ground truth, and φ is the conv4_3 feature extractor of the VGG16 model <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first provide implementation details of the proposed model, including training data, network structure, and hyper-parameters. We then present evaluation results of our algorithm with comparisons to the state-of-the-art methods on video datasets. The source code, data, and the trained models are available at: https://sites.google.com/view/xiangyuxu/qvi_nips19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training data</head><p>To train the proposed interpolation model, we collect high-quality videos from the Internet, where each frame is of 1080×1920 pixels at the frame rate of 960 fps. From the collected videos, we select the clips with both camera shake and dynamic object motion, which are beneficial for more effective network training. The final training dataset consists of 173 video clips of different scenes and 36926 frames in total. In addition, the 960 fps video clips are randomly downsampled to 240 fps and 480 fps for data augmentation. During the training process, we extract non-overlapped frame groups from these video clips, where each has 4 input frames I -1 , I 0 , I 1 , I 2 , and 7 target frames I t , t = 0.125, 0.25, . . . , 0.875. We resize the frames into 360×640 and randomly crop 352×352 patches for training. Image flipping and sequence reversal are also performed to fully utilize the video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>We learn the adaptive flow filtering with a 23-layer U-Net <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref> which is an encoder-decoder network. The encoder is composed of 12 convolution layers with 5 average pooling layers for dowsampling, and the decoder has 11 convolution layers with 5 bilinear layers for upsampling. We add skip connections with pixel-wise summation between the same-resolution layers in the encoder and decoder to jointly use low-level and high-level features. The input of our network is a concatenation of I 0 , I 1 , I t 0 , I t 1 , f 0→1 , f 1→0 , f t→0 , and f t→1 , where I t i (u) = I i (u + f t→i (u)) denotes the pixel warped with f t→i . The U-Net produces the output δ and r which are used to estimate the filtered flow map f t→0 and f t→1 with ( <ref type="formula" target="#formula_5">5</ref>). Then we warp I 0 and I 1 with flow f t→0 and f t→1 , and feed the warped images to a 3-layer CNN to estimate the fusion mask m which is finally used for frame interpolation with <ref type="bibr" target="#b5">(6)</ref>.</p><p>We first train the proposed network with the flow estimation module fixed for 200 epochs, and then finetune the whole system for another 40 epochs. Similar to <ref type="bibr" target="#b33">[34]</ref>, we use the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> for training. We initialize the learning rate as 10 -4 and further decrease it by a factor of 0.1 at the end of the 100 th and 150 th epochs. The trade-off parameter λ of the loss function ( <ref type="formula" target="#formula_7">7</ref>) is set to be 0.005. k in the activation function of δ is set to be 10. In the flow reversal layer, we set the Gaussian standard deviation σ = 1.</p><p>For evaluation, we report PSNR, Structural Similarity Index (SSIM) <ref type="bibr" target="#b31">[32]</ref>, and the interpolation error (IE) <ref type="bibr" target="#b1">[2]</ref> between the predictions and ground truth intermediate frames, where IE is defined as root-mean-squared (RMS) difference between the reference and interpolated image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the state-of-the-arts</head><p>We evaluate our model with the state-of-the-art video interpolation approaches, including the phase-based method (Phase) <ref type="bibr" target="#b16">[17]</ref>, separable adaptive convolution (SepConv) <ref type="bibr" target="#b20">[21]</ref>, deep voxel flow (DVF) <ref type="bibr" target="#b13">[14]</ref>, and SuperSloMo <ref type="bibr" target="#b8">[9]</ref>. We use the original implementations for Phase, SepConv, DVF and the implementation from <ref type="bibr" target="#b21">[22]</ref> for SuperSlomo. We retrain DVF and SuperSlomo with our data. We were not able to retrain SepConv as the training code is not publicly available, and directly use the original models <ref type="bibr" target="#b20">[21]</ref> in our experiments instead. Note that the proposed quadratic video interpolation can be used for synthesizing arbitrary intermediate frames, which is evaluated on the high frame rate video datasets such as GOPRO <ref type="bibr" target="#b17">[18]</ref> and Adobe240 <ref type="bibr" target="#b29">[30]</ref>. We also conduct experiments on the UCF101 <ref type="bibr" target="#b28">[29]</ref> and DAVIS <ref type="bibr" target="#b22">[23]</ref> datasets for performance evaluation of single-frame interpolation.</p><p>Multi-frame interpolation on the GOPRO <ref type="bibr" target="#b17">[18]</ref> dataset. This dataset is composed of 33 highquality videos with a frame rate of 720 fps and image resolution of 720×1280. These videos are recorded with hand-held cameras, which often contain non-linear camera motion. In addition, this dataset has dynamic object motion from both indoor and outdoor scenes, which are challenging for existing interpolation algorithms.</p><p>We extract 4275 non-overlapped frame sequences with a length of 25 from the GOPRO videos. To evaluate the proposed quadratic model, we use the 1 st , 9 th , 17 th , and 25 th frames of each sequence as our inputs, which respectively correspond to I -1 , I 0 , I 1 , I 2 in the proposed model. As discussed in Section 1, the baseline methods only exploit the 9 th and 17 th frames for video interpolation. We synthesize 7 frames between the 9 th and 17 th frames, and thus all the corresponding ground truth frames are available for evaluation.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, we separately evaluate the scores of the center frame (i.e., the 4 th frame, denoted as center) and the average of all the 7 interpolated frames (denoted as whole). The quadratic interpolation model consistently performs favorably against all the other linear methods. Noticeably,  the PSNRs of our results on either the center frame or the average of the whole frames improve over the second best method by more than 1 dB.</p><p>To understand the effectiveness of the proposed quadratic interpolation algorithm, we visualize the trajectories of the interpolated results in Figure <ref type="figure" target="#fig_4">4</ref> and compare with the baseline methods both qualitatively and quantitatively. Specifically, for each test sequence from the GOPRO dataset, we use the classic feature tracking algorithm <ref type="bibr" target="#b26">[27]</ref> to select 10000 feature points in the 9 th frame, and track them through the 7 synthesized in-between frames. For better performance evaluation, we exclude the points that disappear or move out of the image boundaries during tracking.</p><p>We show two typical examples in Figure <ref type="figure" target="#fig_4">4</ref> and visualize the interpolation trajectory by connecting the tracking points (i.e., the red lines). In the the first example, the object moves along a quite sharp curve, mostly due to a sudden violent change of the camera's moving direction. All the existing methods fail on this example as the linear models assume uniform motion and cannot predict the motion change well. In contrast, our quadratic model enables higher-order video interpolation and exploits the acceleration information from the neighboring frames. As shown in Figure <ref type="figure" target="#fig_4">4</ref>(e), the proposed method approximates the curvilinear motion well against the ground truth.</p><p>In addition, we overlap the predicted center frame with its ground truth to evaluate the interpolation accuracy of different methods (first row of each example of Figure <ref type="figure" target="#fig_4">4</ref>). For linear models, the overlapped frames are severely blurred, which demonstrates the large shift between ground truth and the linearly interpolated results. In contrast, the generated frames by our approach align with the ground truth well, which indicates better interpolation results with smaller errors. Different from the first example which contains severe non-linear movements, we present a video with motion trajectory closer to straight lines in the second example. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, although the motion in this video is closer to the uniform assumption of linear models, existing approaches still do not generate accurate interpolation results. This demonstrates the importance of the proposed quadratic algorithm, since there are few scenes strictly satisfying uniform motion, and minor perturbations to this strict motion assumption can lead to obvious shifts in the synthesized images. As shown in the second example of Figure <ref type="figure" target="#fig_4">4</ref>(e), the proposed quadratic method estimates the moving trajectory well against the ground truth and thus generates more accurate interpolation results.   In addition, if we do not consider the acceleration in the proposed method (i.e., using (2) to replace (3)), the interpolation performance of our model decreases drastically to that of linear models ("Ours w/o qua." in Table <ref type="table" target="#tab_0">1</ref> and Figure <ref type="figure" target="#fig_4">4</ref>), which shows the importance of the higher-order information.</p><p>To quantitatively measure the shifts between the synthesized frames and ground truth, we define a new error metric for video interpolation denoted as average shift of feature points (ASFP):</p><formula xml:id="formula_8">ASF P (I t , Ît ) = 1 N N i=1 p(I t , i) -p( Ît , i) 2 ,<label>(8)</label></formula><p>where p(I t , i) denotes the position of the i th feature point on I t , and N is the number of feature points. We respectively compute the average ASFP of the center frame and the whole 7 interpolated frames on the GOPRO dataset. Table <ref type="table" target="#tab_1">2</ref> shows the proposed quadratic algorithm performs favorably against the state-of-the-art methods while significantly reducing the average shift.</p><p>Evaluations on the Adobe240 <ref type="bibr" target="#b29">[30]</ref> dataset. This dataset consists of 133 videos with a frame rate of 240 fps and image resolution of 720×1280 pixels. The frames are resized to 360×480 during testing. We extract 8702 non-overlapped frame sequences from the videos in the Adobe240 dataset, and each sequence contain 25 consecutive frames similar with the settings of the GOPRO dataset.</p><p>We also synthesize 7 in-between frames for 8 times temporally upsampling. As shown in Table <ref type="table" target="#tab_0">1</ref>, the proposed quadratic algorithm performs favorably against the state-of-the-art linear interpolation methods.</p><p>Single-frame interpolation on the UCF101 <ref type="bibr" target="#b28">[29]</ref> and DAVIS <ref type="bibr" target="#b22">[23]</ref> datasets. In addition to the multi-frame interpolation evaluated on high frame rate videos, we test the proposed quadratic model on single-frame interpolation using videos with 30 fps, i.e., UCF101 <ref type="bibr" target="#b28">[29]</ref> and DAVIS <ref type="bibr" target="#b22">[23]</ref> datasets.</p><p>Liu et al. <ref type="bibr" target="#b13">[14]</ref> previously extract 100 triplets from the videos of the UCF101 dataset as test data, which cannot be used to evaluate our algorithms since we need four consecutive frames as inputs. Thus, we re-generate the test data by first temporally downsampling the original videos to 15 fps and then randomly extracting 4 adjacent frames (i.e., I -1 , I 0 , I 1 , I 2 ) from these videos. The sequences with static scenes are removed for more accurate evaluations. We collect 100 quintuples (4 input frames I -1 , I 0 , I 1 , I 2 and 1 target frame I 0.5 ) where each frame is resized to 225×225 pixels as <ref type="bibr" target="#b13">[14]</ref>.</p><p>For the DAVIS dataset, we evaluate our method on the whole 90 video clips which are divided into 2847 quintuples using the original image resolution.</p><p>We interpolate the center frame for these two dataset, which is equivalent to converting a 15 fps video to a 30 fps one. As shown in Table <ref type="table" target="#tab_2">3</ref>, the quadratic interpolation approach performs slightly better than the baseline models on the UCF101 dataset as the videos are of relatively low quality with low image resolution and slow motion. For the DAVIS dataset which contains complex motion from both camera shake and dynamic scenes, our method significantly outperforms other approaches in terms of all evaluation metrics. We show one example from the DAVIS dataset in Figure <ref type="figure" target="#fig_5">5</ref> for visual comparisons.  Overall, the quadratic approach achieves state-of-the-art performance on a wide variety of video datasets for both single-frame and multi-frame interpolations. More importantly, experimental results demonstrate that it is important and effective to exploit the acceleration information for accurate video frame interpolation. We analyze the contribution of each component in our model on the DAVIS video dataset <ref type="bibr" target="#b22">[23]</ref> in Table <ref type="table" target="#tab_3">4</ref>. In particular, we study the impact of quadratic interpolation by replacing the quadratic flow prediction (3) with the linear function (2) (w/o qua.). We further study the effectiveness of the adaptive flow filtering by directly learning residuals for flow refinement similar with <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref> (w/o ada.). In addition, we compare the flow reversal layer with the linear combination strategy in <ref type="bibr" target="#b8">[9]</ref> which approximates f t→0 by simply fusing f 0→1 and f 1→0 (w/o rev.). As shown in Table <ref type="table" target="#tab_3">4</ref>, removing each of the three components degrades performance in all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>Particularly, the quadratic flow prediction plays a crucial role, which verifies our approach to exploit the acceleration information from additional neighboring frames. Note that while the quantitative improvement from the adaptive flow filtering is small, this component is effective in generating high-quality interpolation results by reducing artifacts of the flow fields as shown in Figure <ref type="figure" target="#fig_7">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we propose a quadratic video interpolation algorithm which can synthesize high-quality intermediate frames. This method exploits the acceleration information from neighboring frames of a video for non-linear video frame interpolation, and facilitates end-to-end training. The proposed method is able to model complex motion in real world more accurately and generate more favorable results than existing linear models on different video datasets. While we focus on quadratic function in this work, the proposed formulation is general and can be extended to even higher-order interpolation methods, e.g., the cubic model. We also expect this framework to be applied to other related tasks, such as multi-frame optical flow and novel view synthesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Exploiting the quadratic model for acceleration-aware video interpolation. The leftmost subfigure shows four consecutive frames from a video, describing the projectile motion of a football. The other three subfigures show the interpolated results between frame 0 and 1 by different algorithms.Note that we overlap these results for better visualizing the interpolation trajectories. Since the linear model<ref type="bibr" target="#b30">[31]</ref> assumes uniform motion between the two frames, it does not approximate the movement in real world well. In contrast, our quadratic approach can exploit the acceleration information from the four neighboring frames and generate more accurate in-between video frames.</figDesc><graphic coords="2,285.92,65.62,203.83,115.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effectiveness of the flow reversal layer and the adaptive flow filtering. The car is moving along the arrow direction in the frame sequence. (a) is the f t→0 estimated with the naive strategy from [9]. (b) is the backward flow generated by our flow reversal layer. (c) and (d) represent the results of the deep CNNs in [9] and our adaptive flow filtering, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>frame 0 &amp; frame 1 (a) GT (b) SepConv (c) SuperSloMo (d) Ours w/o qua. (e) Ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative results on the GOPRO dataset. The first row of each example shows the overlap of the interpolated center frame and the ground truth. A clearer overlapped image indicates more accurate interpolation result. The second row of each example shows the interpolation trajectory of all the 7 interpolated frames by feature point tracking.</figDesc><graphic coords="7,112.67,246.34,98.98,55.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visual results from the DAVIS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Adaptive flow filtering reduces artifacts in (a) and generates higher-quality image (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluations on the GOPRO and Adobe240 datasets. "Ours w/o qua." represents our model without using the quadratic flow prediction.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">GOPRO</cell><cell></cell><cell></cell><cell cols="2">Adobe240</cell><cell></cell></row><row><cell></cell><cell>whole</cell><cell></cell><cell>center</cell><cell></cell><cell>whole</cell><cell></cell><cell>center</cell><cell></cell></row><row><cell>Method</cell><cell>PSNR SSIM</cell><cell>IE</cell><cell>PSNR SSIM</cell><cell>IE</cell><cell>PSNR SSIM</cell><cell>IE</cell><cell>PSNR SSIM</cell><cell>IE</cell></row><row><cell>Phase</cell><cell cols="8">23.95 0.700 17.89 22.05 0.620 22.08 25.60 0.735 16.93 23.65 0.647 20.65</cell></row><row><cell>DVF</cell><cell cols="8">21.94 0.776 21.30 20.55 0.720 25.14 28.23 0.896 11.76 26.90 0.871 13.30</cell></row><row><cell>SepConv</cell><cell>29.52 0.922</cell><cell cols="4">9.26 27.69 0.895 11.38 32.19 0.954</cell><cell cols="2">7.71 30.87 0.940</cell><cell>8.91</cell></row><row><cell>SuperSloMo</cell><cell>29.00 0.918</cell><cell cols="4">9.51 27.33 0.892 11.50 31.30 0.949</cell><cell cols="2">8.18 30.17 0.935</cell><cell>9.22</cell></row><row><cell>Ours w/o qua.</cell><cell>29.57 0.923</cell><cell cols="4">9.02 27.86 0.898 10.93 31.64 0.952</cell><cell cols="2">7.93 30.48 0.939</cell><cell>8.96</cell></row><row><cell>Ours</cell><cell>31.27 0.948</cell><cell cols="2">7.23 29.62 0.929</cell><cell cols="2">8.73 32.95 0.966</cell><cell cols="2">6.84 32.09 0.959</cell><cell>7.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ASFP on the GOPRO dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">whole center</cell></row><row><cell>SepConv</cell><cell>1.79</cell><cell>2.17</cell></row><row><cell>SuperSloMo</cell><cell>2.04</cell><cell>2.38</cell></row><row><cell>Ours w/o qua.</cell><cell>1.33</cell><cell>1.69</cell></row><row><cell>Ours</cell><cell>0.97</cell><cell>1.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluations on the UCF101 and DAVIS datasets.</figDesc><table><row><cell></cell><cell>UCF101</cell><cell>DAVIS</cell></row><row><cell>Method</cell><cell>PSNR SSIM IE</cell><cell>PSNR SSIM IE</cell></row><row><cell>Phase</cell><cell>29.84 0.900 7.97</cell><cell>21.54 0.556 26.76</cell></row><row><cell>DVF</cell><cell>29.88 0.916 7.66</cell><cell>22.24 0.742 23.66</cell></row><row><cell>SepConv</cell><cell>31.97 0.943 5.89</cell><cell>26.21 0.857 15.84</cell></row><row><cell>SuperSloMo</cell><cell>32.04 0.945 5.99</cell><cell>25.76 0.850 15.93</cell></row><row><cell>Ours w/o qua.</cell><cell>32.02 0.945 5.99</cell><cell>26.83 0.874 13.69</cell></row><row><cell>Ours</cell><cell>32.54 0.948 5.79</cell><cell>27.73 0.894 12.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the DAVIS dataset.</figDesc><table><row><cell>Method</cell><cell>PSNR SSIM IE</cell></row><row><cell>Ours w/o rev.</cell><cell>26.71 0.873 13.84</cell></row><row><cell>Ours w/o qua.</cell><cell>26.83 0.874 13.69</cell></row><row><cell>Ours w/o ada.</cell><cell>27.60 0.892 12.41</cell></row><row><cell>Ours full model</cell><cell>27.73 0.894 12.32</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jump: virtual reality video</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">198</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2006">2011. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08768</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Beauchemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="43" to="77" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to synthesize motion blur</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with affinity, vertical pooling, and label enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital image processing</title>
		<imprint>
			<publisher>Prentice hall New Jersey</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2018. 1, 2, 3, 4, 5, 6, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Three-dimensional reconstruction of the digestive wall in capsule endoscopy videos using elastic video interpolation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bourbakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="957" to="971" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008">2017. 1, 2, 3, 4, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpolation by convex quadratic splines</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Roulier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2015. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pytorch implementation of super slomo</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paliwal</surname></persName>
		</author>
		<ptr target="https://github.com/avinashpaliwal/Super-SloMo" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Low-light image enhancement via a deep hybrid network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4364" to="4375" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep video dehazing with semantic segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1895" to="1908" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2018. 2, 3, 4, 5, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning deformable kernels for image and video denoising</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06903</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards real scene super-resolution with raw images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Motion blur kernel estimation via deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="205" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rendering portraitures from monocular camera and beyond</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to super-resolve blurry face and text images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">High-quality video view interpolation using a layered representation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="600" to="608" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Surface splatting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Baar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
