<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Focus Attention: Promoting Faithfulness and Diversity in Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
							<email>shashinarayan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
							<email>joshuahm@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
							<email>rothe@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Focus Attention: Promoting Faithfulness and Diversity in Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Professional summaries are written with document-level information, such as the theme of the document, in mind. This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step. With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. Further, we propose a Focus Sampling method to enable generation of diverse summaries, an area currently understudied in summarization. When evaluated on the BBC extreme summarization task, two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents, outperforming their vanilla counterparts on ROUGE and multiple faithfulness measures. We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-k or nucleus samplingbased decoding methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document summarization -producing the shorter version of a document while preserving salient information <ref type="bibr" target="#b49">(Mani, 2001;</ref><ref type="bibr" target="#b55">Nenkova and McKeown, 2011)</ref> -is challenging even for humans. Today, systems can generate summaries with a high level of fluency and coherence. This is due to recent advances such as sequence-to-sequence architectures (seq2seq) with attention and copy mechanism <ref type="bibr" target="#b32">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b29">Gu et al., 2016)</ref>, fully attention-based Transformer architectures <ref type="bibr" target="#b70">(Vaswani et al., 2017)</ref>, and large pretrained language models <ref type="bibr">(Devlin et al.</ref>, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEGASUS with Nucleus Sampling</head><p>Israel hasracuse withdrawn an envoy after the Australian government said it concluded that Israeli agents used forged passports used to kill a Dubai Bendigo businessman. The Australian government has recalled an Israeli diplomat over accusation that fake Australian passports used 436 kilometres (300 miles) from Canberra in the death of a Hamas militant were stolen by Israeli agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C</head><p>Our PEGFAME model with novel Focus Sampling Australia has expelled an Israeli diplomatic staff after accusing the country's security agency, the Israeli military's intelligence agency, of being responsible for the use of Australian visas used in the killing of a Palestinian. The Australian government has expelled an Israeli diplomatic staff after it said the country was responsible for the use of Australian visas used in the killing of a Palestinian in the Middle East.</p><p>Figure <ref type="figure">1</ref>: Block A shows the best predictions from PEGASUS and our PEGFAME (PEGASUS with FAME) model, along with the GOLD summary for an XSUM article. Block B presents diverse summaries generated from PEGASUS using top-k and nucleus sampling. Block C shows diverse summaries generated using our PEGFAME model with Focus sampling. The text in orange is not supported by the input article.</p><p>2019; <ref type="bibr" target="#b58">Radford et al., 2018;</ref><ref type="bibr" target="#b83">Yang et al., 2019;</ref><ref type="bibr" target="#b46">Liu et al., 2019;</ref><ref type="bibr" target="#b16">Dong et al., 2019a;</ref><ref type="bibr" target="#b67">Song et al., 2019;</ref><ref type="bibr" target="#b41">Lewis et al., 2019;</ref><ref type="bibr" target="#b62">Rothe et al., 2020;</ref><ref type="bibr" target="#b59">Raffel et al., 2019;</ref><ref type="bibr" target="#b84">Zhang et al., 2019)</ref>.</p><p>However, in terms of summary quality, many challenges remain. For example, generating summaries that are faithful to the input is an unsolved problem <ref type="bibr" target="#b37">(Kryscinski et al., 2020;</ref><ref type="bibr" target="#b51">Maynez et al., 2020;</ref><ref type="bibr" target="#b24">Gabriel et al., 2020)</ref>. Furthermore, there can be multiple equally good summaries per source docu-arXiv:2105.11921v1 [cs.CL] 25 May 2021 ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets <ref type="bibr" target="#b21">(Fan et al., 2018;</ref><ref type="bibr" target="#b40">Kulikov et al., 2019;</ref><ref type="bibr" target="#b23">Freitag et al., 2020;</ref><ref type="bibr" target="#b10">Choi et al., 2020)</ref>. Not much attention has been given to generation of diverse, yet faithful summaries -two goals are often challenging to achieve simultaneously <ref type="bibr" target="#b30">(Hashimoto et al., 2019)</ref>; a model can produce diverse outputs through sampling <ref type="bibr" target="#b21">(Fan et al., 2018;</ref><ref type="bibr" target="#b33">Holtzman et al., 2020)</ref>, but at the cost of quality.</p><p>In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supported and topical content. FAME achieves this through a novel technique which augments standard contextual representations with a dynamic source-conditioned vocabulary biasing layer. We present the following experimental findings: FAME promotes summaries faithful to the source When evaluated on the BBC extreme summarization task (XSUM; <ref type="bibr" target="#b53">Narayan et al., 2018)</ref>, experiments with two state-of-the-art summarizers -ROBERTAS2S <ref type="bibr" target="#b62">(Rothe et al., 2020)</ref> and PEGA-SUS <ref type="bibr" target="#b84">(Zhang et al., 2019)</ref> -show that both models generate summaries that are more faithful to their input documents when augmented with FAME, in comparison with their vanilla counterparts. <ref type="foot" target="#foot_0">1</ref> Faithfulness is measured through a variety of previously proposed metrics. In addition, we leverage the manually annotated document-summary pairs for faithfulness from <ref type="bibr" target="#b51">Maynez et al. (2020)</ref> and train a scorer which serves as an efficient proxy for expensive human evaluations. We call this metric BERTFaithful.</p><p>FAME enables diverse summaries FAME, by design, supports Focus Sampling -a technique that is more effective in sampling topically relevant tokens to generate diverse, yet topically consistent and faithful outputs, than other sampling methods <ref type="bibr" target="#b21">(Fan et al., 2018;</ref><ref type="bibr" target="#b33">Holtzman et al., 2020)</ref>. Figure <ref type="figure">1</ref> illustrates how focus sampling generates better summaries than other sampling methods. We demonstrate the effectiveness of our new Focus Sampling technique using a variety of existing diversity and faithfulness measures. Empirically, we find that optimizing for high diversity often comes at the cost of faithfulness. Thus FAME provides a mechanism for trading-off high faithfulness with better diversity in summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models <ref type="bibr" target="#b64">(See et al., 2017;</ref><ref type="bibr" target="#b81">Xu et al., 2020)</ref> can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models <ref type="bibr" target="#b48">(Malmi et al., 2019;</ref><ref type="bibr" target="#b17">Dong et al., 2019b;</ref><ref type="bibr" target="#b47">Mallinson et al., 2020)</ref> cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases <ref type="bibr" target="#b26">(Gehrmann et al., 2018)</ref> or by improving the representation of relevant input tokens <ref type="bibr" target="#b87">(Zhou et al., 2017)</ref>. Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In fact, our FAME models achieve state-of-the-art on text-editing tasks (Appendix C).</p><p>Topic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. <ref type="bibr" target="#b3">Barzilay and</ref><ref type="bibr">Elhadad (1997) use WordNet (Fellbaum, 1998)</ref> to model a text's content relative to a topic based on lexical chains. <ref type="bibr" target="#b44">Lin and Hovy (2000)</ref> propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models <ref type="bibr" target="#b52">(Mikolov and Zweig, 2012;</ref><ref type="bibr" target="#b28">Ghosh et al., 2016;</ref><ref type="bibr" target="#b14">Dieng et al., 2017;</ref><ref type="bibr" target="#b34">Karmaker Santu et al., 2019)</ref>, neural response generators <ref type="bibr" target="#b79">(Xing et al., 2017;</ref><ref type="bibr" target="#b19">Dziri et al., 2019)</ref>, and not surprisingly, neural summarizers <ref type="bibr" target="#b53">(Narayan et al., 2018;</ref><ref type="bibr" target="#b0">Ailem et al., 2019;</ref><ref type="bibr" target="#b74">Wang et al., 2020c)</ref>. Both, <ref type="bibr" target="#b53">Narayan et al. (2018)</ref> and <ref type="bibr" target="#b0">Ailem et al. (2019)</ref>, use a pretrained Latent Dirichlet Allocation (LDA; <ref type="bibr" target="#b4">Blei et al., 2003)</ref> model, whereas, <ref type="bibr" target="#b74">Wang et al. (2020c)</ref> use Poisson factor analysis <ref type="bibr" target="#b86">(Zhou et al., 2012)</ref>, to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written summary is a good proxy for the input document. <ref type="bibr" target="#b8">Cao et al. (2017)</ref> force faithful generation by conditioning on both source text and extracted fact descriptions from the source text. <ref type="bibr" target="#b81">Song et al. (2020)</ref> propose to jointly generate a sentence and its syntactic dependency parse to induce grammaticality and faithfulness. <ref type="bibr" target="#b69">Tian et al. (2019)</ref> learn a confidence score to ensure that the model attends to the source whenever necessary. <ref type="bibr" target="#b75">Wang et al. (2020d)</ref> introduce new inputoutput matching and embedding similarity losses to alleviate hallucination issues. Yet, the task of generating text that is consistent with the input remains an open problem <ref type="bibr" target="#b24">(Gabriel et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Faithful Generation Models</head><p>Diverse Generation Models There has been a surge of interest in making language models generate more diverse and human-like outputs. <ref type="bibr" target="#b71">Vijayakumar et al. (2018)</ref> and <ref type="bibr" target="#b40">Kulikov et al. (2019)</ref> diversify beam search, using a task-specific scoring function, or constrain beam hypotheses to be sufficiently different. Others avoid text degeneration by truncating the unreliable tail of the probability distribution at each decoding step, either by sampling from the top-k tokens (Top-k Sampling; <ref type="bibr" target="#b21">Fan et al., 2018)</ref> or by sampling from a dynamic nucleus of tokens with the bulk of the probability mass (Nucleus Sampling; <ref type="bibr" target="#b33">Holtzman et al., 2020)</ref>. Others modify the training objective to make the distribution sparse <ref type="bibr" target="#b50">(Martins et al., 2020)</ref> or assign lower probability to unlikely generations <ref type="bibr" target="#b76">(Welleck et al., 2019a)</ref>.</p><p>For conditional text generation, most work focuses on generating diverse questions <ref type="bibr" target="#b54">(Narayan et al., 2016;</ref><ref type="bibr" target="#b15">Dong et al., 2017;</ref><ref type="bibr" target="#b68">Sultan et al., 2020;</ref><ref type="bibr" target="#b73">Wang et al., 2020b)</ref> or paraphrases <ref type="bibr" target="#b43">(Li et al., 2016b;</ref><ref type="bibr" target="#b11">Dai et al., 2017;</ref><ref type="bibr" target="#b80">Xu et al., 2018;</ref><ref type="bibr" target="#b7">Cao and Wan, 2020)</ref>. Following <ref type="bibr" target="#b26">Gehrmann et al. (2018)</ref>, <ref type="bibr" target="#b9">Cho et al. (2019)</ref> use a mixture of experts to sample different binary masks on the source sequence for diverse content selection for summarization. Our focus sampling is similar to top-k and nucleus sampling methods; in that it truncates the tail of the probability distribution. However, instead of truncating it at each decoding step, it biases the decoder proactively to generate output from a set of tokens which are topically-relevant to the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Summarization with Focus Attention</head><p>Given an input document X 1:n , we aim to generate its summary Y 1:m , where n and m are input and output sequence lengths. We address this prob- lem using seq2seq architectures with Transformer encoder and decoder, augmented with FAME, as depicted in Figure <ref type="figure">2</ref>. FAME learns a distribution t x i for each input token x i over the vocabulary, measuring similarity of x i (in context) to the tokens in the vocabulary. The vocabulary distributions, t x i , for all x i are combined to form a dynamic vocabulary bias that is added to the decoder logits. This mechanism enhances the conditioning on the input source and encourages the decoder to generate tokens that are topically similar to the input.</p><p>Transformer-based seq2seq Model The encoder uses BERT Transformer layers with multiheaded self-attention to encode X to a vector sequence X = x 1 , . . . , x n , with x i ∈ R h , where h is the size of hidden representation. The decoder uses an identical architecture, except that at decoding step t, layer l adds a conditional representation y l t ∈ R h for the token y t by attending to the output representation Y l−1 1:t−1 = y l−1 1 , . . . , y l−1 t−1 generated so far through self-attention and by attending to the input contextual representation X through encoderdecoder attention. The probability of predicting the next token y t from a vocabulary V is:</p><formula xml:id="formula_0">p(y t |Y 1:t−1 , X; θ) = softmax(Ey L t ),<label>(1)</label></formula><p>where, y L t is the representation from the final decoder layer L, E ∈ R |V |×h the embedding matrix and θ the model parameters. Parameters are trained by minimizing cross-entropy at each decoding step:</p><formula xml:id="formula_1">L MLE (θ) = − 1 m m i=1 log p(ŷ t | Ŷ1:t−1 , X; θ),</formula><p>where, Ŷ1:m is the human-written summary.</p><p>Focus Attention MEchansim (FAME) It is challenging for a decoder to obtain all relevant information from the conditional representation y L t to learn the vocabulary output logits such that predictions y t are consistent with the input. Other modeling factors, specifically the decoder language model, can overwhelm model predictions. FAME (Figure <ref type="figure">2</ref>) addresses this by introducing a short-circuit from the source to the vocabulary output logits via a source-conditioned bias on vocabulary items.</p><p>We take the encoder representation X = x 1 , . . . , x n and learn a Token-level Vocabulary Distribution t x i = gelu(x i W 1 )W 2 E ∈ R |V | , for each token x i in the input sequence X. t x i measures the contextual similarity of the input token x i to the tokens in the vocabulary; W 1 ∈ R h×h and W 2 ∈ R h ×h are parameters of newly introduced dense layers, h is the intermediate filter size. We define a Source-conditioned Vocabulary Distribution as t X = 1/n n i=1 t x i ∈ R |V | as an average of token-level vocabulary distributions for tokens present in the input sequence X, capturing the similarity of X to the tokens in the vocabulary.</p><p>Let a L t ∈ R n be the encoder-decoder attention distribution over the source tokens for the output token y t and the final decoder layer L. We use a L t to produce a weighted sum of the token-level vocabulary distributions to compute a dynamic vocabulary bias, or Focus Bias f t = n i=1 a L t,i t x i ∈ R |V | at decoding step t. We modify the probability of predicting the next token y t from a vocabulary V as:</p><formula xml:id="formula_2">p(y t |Y 1:t−1 , X; θ) = softmax(y L t E + f t ) (2)</formula><p>We call this Focused Probability Distribution, and it modifies the output logits dynamically to put more focus on those tokens in the vocabulary which are similar to the attended tokens in X. The focus bias introduces a human-inspired control to the model where we do not generate the output in a fully abstractive manner (as in Eq. ( <ref type="formula" target="#formula_0">1</ref>)), but we proactively generate output tokens that are similar to the input tokens (as in Eq. ( <ref type="formula">2</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary-induced Topic Focused Distribution</head><p>We aim to guide our focus bias f t to be a better representative of the topical content relevant for the task. We achieve this by using the human-written summary Ŷ as a proxy for the topical content of the input and impose the following prior on the source-conditioned vocabulary distribution t X :</p><formula xml:id="formula_3">L Topic (θ) = − 1 |V | |V | i=1 ([v i ∈ Ŷ ] log(σ(t X,i )) + [v i / ∈ Ŷ ] log(1 − σ(t X,i ))).<label>(3)</label></formula><p>We further refine Eq. ( <ref type="formula" target="#formula_3">3</ref>) by replacing Ŷ with Ŷc = Ŷ −F , where F is a set of |F | most frequent tokens in the vocabulary,<ref type="foot" target="#foot_1">2</ref> to improve focus on content words. Our final loss function is then</p><formula xml:id="formula_4">L = λL MLE + (1 − λ)L Topic ,<label>(4)</label></formula><p>where, λ is an hyper parameter.<ref type="foot" target="#foot_2">3</ref> By enforcing t X to be a topic distribution for the input X, we encourage the focus bias f t to promote topically relevant tokens, and subsequently generate topically consistent outputs. Importantly, our focus bias with target-induced topic distribution is task-agnostic and less vulnerable to reference divergence issues <ref type="bibr" target="#b13">(Dhingra et al., 2019;</ref><ref type="bibr" target="#b51">Maynez et al., 2020)</ref>, and can learn any property embodied in the target relevant for the task. For example, depending on the task, f t can learn to favour input tokens (e.g., for mostly extractive summaries) or new tokens (e.g., for mostly abstractive summaries). This is in sharp contrast to models that introduce task-specific priors, e.g., the pointer-generator network <ref type="bibr" target="#b64">(See et al., 2017)</ref> that can copy words from the source text, but does not do well on extreme summarization which is highly abstractive in nature <ref type="bibr" target="#b53">(Narayan et al., 2018)</ref>.</p><p>Focus Sampling: Promoting Diversity in Faithful Generation We introduce Focus Sampling with FAME to construct a subset V k ⊆ V by sampling k tokens from the topic distribution t X (Focus sample,k ). Then, we modify Eq. (2) as</p><formula xml:id="formula_5">p(y t |Y 1:t−1 , X; θ) = softmax(y L t E + f t ) i if v i ∈ V k ∪ F 0, otherwise.<label>(5)</label></formula><p>For document summarization, the subset V k will capture topically salient tokens necessary to generate a summary; F is always added to V k to ensure that the model has access to function words. By tuning the parameters of sampling, we can enforce the model to control the faithfulness or diversity of the outputs. Focus sampling has similarities to top-k (Div top,k ; <ref type="bibr" target="#b21">Fan et al., 2018)</ref> and nucleus sampling (Div nucleus ; <ref type="bibr" target="#b33">Holtzman et al., 2020)</ref>; in that they all aim to promote diversity. At each decoding step, the top-k sampling diversifies the generation process by sampling a token from the top k tokens in the final output distribution. Similarly, nucleus sampling samples from a dynamic nucleus of tokens containing the vast majority (with a cumulative probability p) of the probability distribution. Both top-k and nucleus sampling shorten the tail of the output distribution at each decoding step, whereas focus sampling constrains the decoder to use a fixed and topically relevant vocabulary V k . Unlike the other two techniques, Focus sample,k can also benefit from standard beam search decoding, leading to superior generation that is not only diverse, but also consistent with the input document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this section we present our experimental setup to assess the ability of our FAME models to generate faithful summaries and to demonstrate that focus sampling is more effective in generating diverse and faithful summaries than other sampling-based decoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Extreme Summarization</head><p>We evaluate FAME models on extreme document summarization (XSUM; <ref type="bibr" target="#b53">Narayan et al., 2018)</ref>. The XSUM summaries, are extreme in that the documents are summarized into single-sentence summaries. These summaries demonstrate a high level of abstractiveness, and generating them automatically requires document-level inference, abstraction, and paraphrasing. Due to their extreme nature, XSUM summaries are ideal to evaluate FAME models' ability to capture the theme of the document. <ref type="foot" target="#foot_3">4</ref>We use on the original cased version consisting of 204,045/11,332/11,334 training/validation/test document-summary pairs. During training, the input documents are truncated to 512 tokens. The length of the summaries are limited to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pretrained Models with FAME</head><p>We introduce FAME to two popular seq2seq architectures: RoBERTa initialized seq2seq (ROBERTAS2S, <ref type="bibr" target="#b62">Rothe et al., 2020)</ref> and PEGASUS <ref type="bibr" target="#b84">(Zhang et al., 2019)</ref>. We refer ROBERTAS2S models with FAME as ROBFAME and PEGASUS with FAME with PEGFAME.</p><p>We experiment with ROBERTAS2S-Large with shared encoder and decoder; it has 24 layers, a hidden size of 1024, filter size of 4096, 16 attention heads, and a vocabulary with 50K sentence pieces <ref type="bibr" target="#b39">(Kudo and Richardson, 2018)</ref>. ROBERTAS2S has around 455M parameters and ROBFAME has an additional 8M parameters.</p><p>The best-performing PEGASUS model from <ref type="bibr" target="#b84">Zhang et al. (2019)</ref> is not directly comparable with ROBERTAS2S. It does not share the encoder and decoder, it only has 16 layers, a hidden size of 1024, filter size of 4096, 16 attention heads, with a total of 568M parameters, and it also uses a much larger vocabulary with 91K sentence pieces. Hence, we trained our own PEGASUS model. We use the same architecture as ROBERTAS2S and pretrain it on a mixture of C4 <ref type="bibr" target="#b59">(Raffel et al., 2019)</ref> and Huge-News <ref type="bibr" target="#b84">(Zhang et al., 2019)</ref> datasets with the original objective of generating salient GAP-sentences.</p><p>Our experiments focus on this newly trained PEGASUS model which has same number of parameters and vocabulary as ROBERTAS2S. But in contrast to ROBERTAS2S, the encoder-decoder attention in PEGASUS is pretrained. This allows us to analyse how focus attention affects pretrained (PEGASUS) vs randomly-initialized (ROBERTAS2S) encoder-decoder attentions.<ref type="foot" target="#foot_4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>Lexical Overlap We report ROUGE F1 scores <ref type="bibr" target="#b45">(Lin and Hovy, 2003)</ref> against reference summaries; in particular, we report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L for fluency. <ref type="foot" target="#foot_5">6</ref>Semantic Similarity We report BERTScore <ref type="bibr" target="#b85">(Zhang et al., 2020)</ref> which computes the contextual similarity between a candidate and its reference summary. Faithfulness ROUGE and BERTScore do not correlate well with faithfulness of the generated summaries <ref type="bibr" target="#b51">(Maynez et al., 2020)</ref>. Human evaluation is traditionally considered as the gold standard for measuring faithfulness. But recent research has shown that even human evaluation has shortcomings <ref type="bibr" target="#b63">(Schoch et al., 2020)</ref>. Moreover, it is prohibitively expensive. This has led to the proposal of meta-evaluation metrics for various generation tasks <ref type="bibr" target="#b18">(Durmus et al., 2020;</ref><ref type="bibr" target="#b38">Kryściński et al., 2019;</ref><ref type="bibr" target="#b65">Sellam et al., 2020;</ref><ref type="bibr" target="#b61">Rei et al., 2020)</ref>. We evaluate FAME models on semantic inference metrics such as textual entailment <ref type="bibr" target="#b56">(Pasunuru and Bansal, 2018;</ref><ref type="bibr" target="#b77">Welleck et al., 2019b;</ref><ref type="bibr" target="#b20">Falke et al., 2019;</ref><ref type="bibr" target="#b36">Kryscinski et al., 2019</ref>) and question answering <ref type="bibr" target="#b1">(Arumae and Liu, 2019;</ref><ref type="bibr" target="#b72">Wang et al., 2020a)</ref>. In particular, we report the probability of a summary entailing (ent.) its input document <ref type="bibr" target="#b51">(Maynez et al., 2020)</ref> and QA-based Feqa scores <ref type="bibr" target="#b18">(Durmus et al., 2020)</ref>. For ent. scores, we train an entailment classifier by fine-tuning a BERT-Large pretrained model <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> on the Multi-NLI dataset <ref type="bibr" target="#b78">(Williams et al., 2018)</ref>. For Feqa, we use a fine-tuned BART <ref type="bibr" target="#b41">(Lewis et al., 2019)</ref> language model for question generation to generate questions from the summaries, and a BERTbase model fine-tuned on SQuAD <ref type="bibr" target="#b60">(Rajpurkar et al., 2018)</ref> to answer the generated questions with input document as context. <ref type="foot" target="#foot_6">7</ref>In addition to ent. and Feqa, we train a scorer leveraging manually annotated document-summary pairs for faithfulness, as a surrogate for human evaluation and call this metric BERTFaithful. <ref type="foot" target="#foot_7">8</ref>In particular, we finetune a BERT-Base classi-fier on 500 manually annotated document and gold summary pairs for the XSum dataset from <ref type="bibr" target="#b51">Maynez et al. (2020)</ref> to predict whether a summary is faithful to the input document or not. 9  We report the percentage of summaries that were faithful ( 1 N i 1[p i (faithful) &gt; 0.5]) and the model's confidence to generate faithful summaries ( 1 N i p i (faithful)); N is the total number of examples in the test set.</p><p>Diversity We report the number of times (out of n), a model is able to generate a completely new summary (Unique), and Distinct-N <ref type="bibr" target="#b42">(Li et al., 2016a)</ref>, measuring the lexical diversity in the generated summaries. Distinct-N is estimated as the number of distinct n-grams of order n divided by the total number of n-grams of the same order, in all generated summaries.</p><p>Finally, we also report the average length of summaries (Len.), repetition errors (Rep., estimated as the percentage of summaries with at least one repetition of rare or content words), and ROUGE-1 precision against the input document (R1, P%), to better understand their quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>FAME Summaries are More Fluent, Informative and Faithful. Table <ref type="table" target="#tab_1">1</ref> presents results comparing our FAME models, ROBFAME and PEG-FAME, against their counterparts ROBERTAS2S 9 Out of 500, 90% of the document-summary pairs were used for training and the rest 50 document-summary pairs were used for validation. We used the validation set to estimate Spearman's correlation coefficients of different metrics with the human assessment for faithfulness. We found that both entailment scores (ent.) and BERTFaithful are moderately correlated with faithfulness with correlation coefficients of 0.4387 and 0.3889, respectively. As such, we believe that BERTFaithful works as an efficient proxy for expensive human evaluation for faithfulness for XSum summaries. We further did pairwise comparisons for all measures in Table <ref type="table" target="#tab_1">1</ref> and found that all differences are statistically significant except for BERTScore and faithfulness measures between PEGASUS and PEGFAME. 10 These assessments demonstrate that FAME models aid both ROBERTAS2S and PEGA-SUS in generating fluent, faithful and relevant summaries, but are more effective in ROBERTAS2S than in PEGASUS for extreme summarization.</p><p>Generating Diverse and Faithful Summaries with Focus Sampling. Table <ref type="table" target="#tab_2">2</ref> presents results assessing focus sampling (Focus sample,k ), top-k sampling (Div top,k ) and nucleus sampling (Div nucleus ), for their abilities to generate diverse and faithful summaries. For Focus sample,k , we choose k = 10, 000. We follow <ref type="bibr" target="#b33">Holtzman et al. (2020)</ref> and choose k = 640 and the nucleus probability p = 0.95, for Div top,k and Div nucleus , respectively. For Focus sample,k , we decode with a beam size of 4. We also report Focus sample,k with Div top,k and Div nucleus to assess if they can benefit one-another. In each setting we sample 10 sum-10 All significance tests in this work are pairwise comparisons (one-way ANOVA with posthoc Tukey HSD tests; p &lt; 0.01).</p><p>maries for each input document. For all metrics, we report the average over all 10 samples. <ref type="foot" target="#foot_8">11</ref>Both Div top,k and Div nucleus almost always generate a new summary. In comparison Focus sample,k generates 1.61 and 2.77 unique summaries using ROBFAME and PEGFAME models, respectively. Div nucleus tends to generate the most distinct unigrams, bigrams, and trigrams. Interestingly, Focus sample,k summaries have a more diverse collection of unigrams than in Div top,k summaries (3.5% vs 2.3% for ROBFAME and 2.4% vs 1.9% for PEGFAME).</p><p>The high diversity in Div top,k and Div nucleus comes at the cost of faithfulness; summaries generated with these sampling techniques have poor entailment scores. Focus sample,k , on the other hand, generates summaries which entail documents the most. It also has the highest ROUGE scores across the board. Some of the generated examples can be seen in Figure <ref type="figure">1</ref>. More predictions from other models can be found in Appendix E. Augmenting Div top,k and Div nucleus with Focus sample,k is not desirable because, though it increases diversity in terms of uniqueness and Distinct-3 scores, faithfulness suffers again.</p><p>Comparing results in Table <ref type="table" target="#tab_2">2</ref> to the results in Table 1, it is clear that diversity comes at the cost of quality (e.g., RL/ent. scores for ROBFAME and ROBFAME-Focus sample,k are 34.81/41.3 and 31.0/34.3, respectively). However, Focus sample,k is superior to both Div top,k and Div nucleus in gen-erating better quality summaries.  Focus Attention and Sampling Work Differently in ROBFAME and PEGFAME. Since both encoder-decoder and focus attention parameters of ROBFAME are randomly initialized, they learn to compliment each other and learn a peaky topic distribution. On the other hand, since PEGFAME's encoder-decoder attention is pre-trained, there is a push-pull effect between it and focus attention. This results in a smoother topic distribution, as seen in Figure <ref type="figure" target="#fig_1">3</ref>. 12  Although we see that both models' token sets capture the target intent well, the peaky distribu- 12 This difference in topic distributions is consistent across the whole test set. We compute the peakiness score of a topic distribution as the slope of the line connecting logits of the top-1st token to the top-100th token. The average peakiness scores across the XSUM testset for ROBFAME and PEGFAME are 1.25 (51   <ref type="figure" target="#fig_2">4</ref> where we show how ROUGE-1 scores vary when we use only top-k tokens from t X for generation. <ref type="foot" target="#foot_9">13</ref> We observe that ROBFAME consistently outperforms PEGFAME with the lower values of k ∈ {50, 100, 200, 500, 1000}.</p><p>Further, we observe that ROBFAME generates fewer unique summaries (1.61 vs 2.77) but has higher Distinct-N scores (3.5/22.4/43.9 vs 2.4/16.5/34.2) than PEGFAME, with Focus sample,k in Table <ref type="table" target="#tab_2">2</ref>. This can be again be attributed to how FAME works differently in ROBFAME and PEG-FAME. When V k is sampled from ROBFAME's peaky distribution, the beam search decoding often tends to generate similar summaries (leading to a lower Uniqueness score) as the sampled V k s do not diverge by much from each other. But when it does diverge, the decoder tends to generate completely new summaries (leading to higher Distinct-N scores).</p><p>Currently, we set k = 10, 000 for our focus sampling experiments following our observations in Figure <ref type="figure" target="#fig_2">4</ref>. Future work will focus on how to better leverage trade-off between diversity and faithfulness by controlling the peakiness of the topic distribution t X .</p><p>Ablations and SOTA Comparisons We emphasize that FAME or focus sampling does not aim to improve on state-of-the-results in terms of ROUGE, but to generate more faithful or diverse summaries while maintaining their quality. For completeness, we compare our ROBFAME and PEGFAME models to their ablations and other state-of-the-art models on XSUM in Table <ref type="table" target="#tab_4">3</ref>.</p><p>We report ROUGE scores for FAME in the ideal scenario (ORACLE) where it focuses on all the correct tokens in the input, i.e., the topic distribution t X is identical to the distribution observed in the reference summary. These models generate summaries with very high ROUGE scores when the model is given the correct tokens to focus on. The gap between the ORACLE and FAME scores suggests that there is still a lot of work to be done in this space. Focus attention without any topical supervision (models w/o Eq. ( <ref type="formula" target="#formula_3">3</ref>)) is not significantly better than the baselines. But ROBFAME and PEG-FAME (trained with joint supervision in Eq. ( <ref type="formula" target="#formula_4">4</ref>)) significantly outperform ROBERTAS2S and PEGA-SUS, respectively.</p><p>Our best model PEGFAME performs better than PtGen <ref type="bibr" target="#b64">(See et al., 2017)</ref>, ConvS2S <ref type="bibr" target="#b53">(Narayan et al., 2018)</ref>, MMN <ref type="bibr" target="#b35">(Kim et al., 2019)</ref>, MASS <ref type="bibr" target="#b67">(Song et al., 2019)</ref> and BART <ref type="bibr" target="#b41">(Lewis et al., 2019)</ref>, but worse when the original PEGASUS <ref type="bibr" target="#b84">(Zhang et al., 2019)</ref>. This can be expected as the number of parameters in PEGFAME is far less than that in the original PEGASUS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced FAME, a new attention mechanism which dynamically biases the decoder to proactively generate tokens that are topically similar to the input. FAME enhances the faithfulness of existing state-of-the-art abstract summarization models while improving their overall ROUGE scores. Finally, our newly introduced focus sampling technique is a better alternative to top-k or nucleus sampling to generate diverse set of faithful summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>The nature of text generation leads to multiple ethical considerations when applied to applications. The main failure mode is that the model can learn to mimic target properties in the training data that are not desirable.</p><p>Faithfulness and Factuality Since models create new text, there is the danger that they may neither be faithful to the source material nor factual. This can be exacerbated when the data itself has highly abstractive targets, which require the model to generate words not seen in the source material during training. This often leads the model to generate content inconsistent with the source material <ref type="bibr" target="#b37">(Kryscinski et al., 2020;</ref><ref type="bibr" target="#b51">Maynez et al., 2020;</ref><ref type="bibr" target="#b24">Gabriel et al., 2020)</ref>.</p><p>Trustworthy Data If the data itself is not trustworthy (comes from suspect or malicious sources) the model itself will naturally become untrustworthy as it will ultimately learn the language and topics of the training data. For instance, if the training data is about Obama birther conspiracies, and the model is asked to generate information about the early life of Obama, there is a risk that such false claims will be predicted by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias in Data</head><p>Similarly, biases in the data around gender, race, etc., risk being propagated in the model predictions, which is common for most NLP tasks. This is especially true when the models are trained from non-contemporary data that do not represent current norms and practices <ref type="bibr" target="#b5">(Blodgett et al., 2020)</ref>.</p><p>The above considerations are non-malicious, in that the model is merely learning to behave as its underlying source material. If users of such models are not aware of these issues and do not account for them, e.g., with better data selection, evaluation, etc., then the generated text can be damaging.</p><p>Generation models can also be misused in malicious ways. These include generating fake news, spam, and other text meant to mislead large parts of the general population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation and Reproducibility Details</head><p>Following <ref type="bibr" target="#b62">Rothe et al. (2020)</ref>, the encoder and decoder of ROBERTAS2S and ROBFAME models are initialized with public RoBERTa checkpoints. The encoder and decoder parameters are shared in both cases. Only the encoder-decoder attention parameters are initialized randomly. For ROBFAME, the focus attention parameters are also randomly initialized. We experiment with large RoBERTa checkpoints with 24 layers, a hidden size of 1024, filter size of 4096, 16 attention heads, and a vocabulary with 50K sentence pieces <ref type="bibr" target="#b39">(Kudo and Richardson, 2018)</ref>. ROBERTAS2S has around 455M parameters and ROBFAME has 463M parameters, with an additional 8M parameters. Our PEGASUS and PEGFAME implementation also have the same configuration, except for the encoder-decoder attention parameters which are pretrained. We used Cloud TPU v3 accelerators for training. All models are fine-tuned on the target task using Adam with a learning rate of 0.05. We use a linear learning rate warm up with 40k steps, normalized by the square root of the hidden size, and a square root decay. We do not perform any tuning on these hyperparameters. We use a global batch size of 128 document-summary pairs. We adapt to different number of training steps depending on the training data sizes. Models are trained for 400k and 200k steps for CNN/DM and XSUM respectively, saving check-points every 1000 steps. We choose the best model based on ROUGE-L performance on the respective validation set.</p><p>The vocabulary for functional tokens F is constructed by taking the most frequent sentence pieces in the training set. We tune |F | using the respective validation sets; for XSUM, we choose f = 500 frequent sentence pieces and for CNN/DM, f = 1000. For all our experiments with the FAME models, the beam size is set to 4.</p><p>We use Cloud TPU v3 accelerators for computing entailment scores which takes about 20 minutes for the two datasets' test sets. Question generation and answering for Feqa are run on a NVIDIA V100 GPU, and it takes between 8-12 hours for one setting of each test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Abstractive Summarization Results on CNN/DailyMail</head><p>The  <ref type="table">4</ref> and 5 present complete results for CNN/DM dataset. We see similar kind of improvements as observed in Table <ref type="table" target="#tab_1">1</ref>, except for ROUGE-2 for ROBFAME which is 0.23 points worse than the ROBERTAS2S baseline. Our best model PEG-FAME performs better than both copy mechanism models: LSTM-based PtGen <ref type="bibr" target="#b64">(See et al., 2017)</ref> and Transformer-based SAGCopy <ref type="bibr" target="#b81">(Xu et al., 2020)</ref>. PEGFAME performs worse when compared with T5 <ref type="bibr" target="#b59">(Raffel et al., 2019)</ref>, the original PEGASUS <ref type="bibr" target="#b84">(Zhang et al., 2019)</ref> and ProphetNet <ref type="bibr" target="#b57">(Qi et al., 2020)</ref>. This can be expected as the number of parameters in PEGFAME is almost half of T5 or ProphetNet, and is 100M less than that in the original PEGASUS.</p><p>ROBFAME performs worse than ROBERTAS2S on both ent. and Feqa measures for CNN/DM, similar to ROUGE-2 in Table <ref type="table">4</ref>. We hypothesize that this is due to the extractive nature of the CNN/DM dataset and the fact that it is not able to copy to- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Text Editing Results</head><p>We also train the FAME models on two text editing tasks: (i) for sentence fusion -the problem of combining multiple sentences into a single coherent sentence -we used the "balanced Wikipedia" portion of the DiscoFuse dataset <ref type="bibr" target="#b27">(Geva et al., 2019)</ref>, and (ii) for split-and-rephrase -the reverse task of sentence fusion -we used the WikiSplit dataset <ref type="bibr" target="#b6">(Botha et al., 2018)</ref>, which consists of 1M examples of sentence splits extracted from the Wikipedia edit history. As the name suggests, both text editing tasks require a low degree of abstraction.</p><p>For both the tasks, we train the models for 300k steps with a global batch size of 256. The input and output are padded to a length of 128, which covers 100% of the training, evaluation and test data. The vocabulary for functional tokens F is constructed by taking the top 100 and 500 sentence pieces for DiscoFuse and WikiSplit respectively.</p><p>We report corpus-level BLEU<ref type="foot" target="#foot_10">14</ref> , the exact match accuracy, and SARI scores <ref type="bibr" target="#b82">(Xu et al., 2016)</ref>  <ref type="foot" target="#foot_11">15</ref> . The results can be seen in Table <ref type="table" target="#tab_7">6</ref>. The vanilla PEGA-SUS model already beats the current state-of-the-art on both DiscoFuse and WikiSplit. The PEGFAME DiscoFuse Exact SARI BLEU <ref type="bibr" target="#b27">(Geva et al., 2019)</ref> 51.1 84.5 -LaserTagger <ref type="bibr" target="#b48">(Malmi et al., 2019)</ref> 53.8 85.5 -Felix <ref type="bibr" target="#b47">(Mallinson et al., 2020)</ref> 61.3 88.8 -ROBERTAS2S <ref type="bibr" target="#b62">(Rothe et al., 2020)</ref> 66 model performs better, albeit by a small margin, on all metrics on DiscoFuse. On WikiSplit, it has a higher exact match accuracy while maintaining the SARI score and performs 0.1 BLEU worse than PEGASUS. We experiment with limiting FAME models to different sizes of vocabulary V k using the topic distribution t X ; in particular, we experiment with k = {50, 100, 200, 500, 1000, 10000}. We also report numbers for ROBERTAS2S, ROBFAME, PEGASUS and PEGFAME, using the whole vocabulary of size 50k. The bold results in each block are the best performing ROBERTAS2S-based and PEGASUS-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Controlled Generation with focus attention using Top-k tokens</head><p>GOLD Australia has expelled an Israeli diplomat saying Israel was behind the forging of Australian passports linked to the murder of a Hamas operative in Dubai.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article</head><p>Australia's foreign minister said these were "not the actions of a friend".</p><p>The UK took similar action in March, after concluding that Israel was responsible for the use of forged UK passports in the plot. The Israeli foreign ministry said Australia's decision was disappointing. Ministry spokesman Yigal Palmor said it was "not in line with the importance and the quality of the relationship between our countries". 'Sorrow not anger' At least four forged Australian passports were used in the killing of Mahmoud al-Mabhouh in Dubai in January. The originals belonged to Australians living in Israel.</p><p>The Australian government said a police investigation had left it in no doubt that the Israeli authorities were behind "the abuse and counterfeiting of the passports". As a result Foreign Minister Stephen Smith asked Israel to withdraw a diplomat, whom he did not identify. "The decision to ask Israel to remove from Australia one of its officers at the Israeli embassy in Canberra is not something which fills the Australian government with any joy," he said. "On the contrary, the decision was made much more in sorrow than in anger." Passports from France, Ireland, Germany and Britain were used in the operation, and in March, the British government expelled an Israeli diplomat from London.</p><p>The Israeli government has said there is no proof that it was behind the killing, although Dubai officials have said they are 99.9% sure that agents from Mossad were responsible.</p><p>ROBERTAS2S Australia has asked Australia to withdraw an Israeli diplomat from its embassy in Canberra after an alleged plot to kill a Abu Dhabi militant in Dubai.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBFAME</head><p>Australia has asked Israel to withdraw one of its diplomats from its embassy in Canberra after it admitted it used forged passports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEGASUS</head><p>Australia has expelled an Israeli diplomat after concluding that forged Australian passports used in the killing of a Hamas militant in Dubai were issued by Israel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEGFAME</head><p>The Australian government has expelled an Israeli diplomat over the use of forged Australian passports in the killing of a Hamas militant in Dubai.</p><p>Figure <ref type="figure">5</ref>: A 2010 BBC article from the XSUM testset, its human written summary and model predictions from ROBERTAS2S, and PEGASUS, with and without FAME. The text in orange is not supported by the input article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBFAME (Focus top,k=50 )</head><p>Australia has said it will not be expelled an ambassador from Australia following the alleged s agent for the so-called Arab Arab State.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBFAME (Focus top,k=100 )</head><p>Australia has said it will not be expelled an ambassador from Australia following the killing of a terror agent in the Arab world. ROBFAME (Focus top,k=200 )</p><p>Australia has said it will not be expelled an ambassador from Australia following the killing of an Australian terror suspect in the Arab world. ROBFAME (Focus top,k=500 )</p><p>Australia has asked Israel to end its diplomatic investigation into an alleged plot to murder an Australian terror suspect. ROBFAME (Focus top,k=1000 )</p><p>Australia has asked Israel to strip an ambassador from its embassy following the death of an Arab man in Dubai. ROBFAME (Focus top,k=10000 ) Australia has asked Israel to withdraw one of its diplomats from its embassy in Canberra following the death of a terror suspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEGFAME (Focus top,k=50 )</head><p>The Israeli government has been expelled from the country after it was found that the country's security agency, the Israeli intelligence agency, was to be to be found to have used a number of the country's out-of-country p when it was used in the Emirates car-j best. PEGFAME (Focus top,k=100 )</p><p>The Israeli government has been expelled from the country after it was found that the country's security agency, the Israeli intelligence agency, had used the country's visas in the Emirates terror. PEGFAME (Focus top,k=200 )</p><p>The Australian government has expelled an Israeli diplomats after it found that the country's security agency, the Israeli intelligence agency, had used the country's visas in the Emirates terror attack. PEGFAME (Focus top,k=500 )</p><p>The Australian government has expelled an Israeli diplomatic staff after accusing the country's security agency, the Israeli intelligence agency, of using a number of Australian visas in the Emirates terror attack. PEGFAME (Focus top,k=1000 )</p><p>Australia has expelled an Israeli diplomatic staff after accusing the country's security agency, the Israeli military's intelligence agency, of being responsible for the use of Australian visas used in the killing of a Palestinian. PEGFAME (Focus top,k=10000 ) Australia has expelled an Israeli diplomat over the use of forged Australian passports in the killing of a Hamas militant in Dubai.</p><p>Figure <ref type="figure">6</ref>: Model predictions with focus sampling Focus top,k , a controlled generation setting. The text in orange is not supported by the input article. We note that with smaller values of k, both ROBERTAS2S-based and PEGASUSbased models tend to hallucinate more often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBFAME (Focus sample,k )</head><p>Australia has asked Israel to strip one of its diplomats from its embassy following the death of an Arab man in Dubai.</p><p>Australia has asked Israel to end its diplomatic investigation into an alleged plot to murder an Australian terror suspect.</p><p>Australia has asked Israel to strip one of its diplomats from its embassy in Australia over the death of a terror suspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEGFAME (Focus sample,k )</head><p>The Australian government has expelled an Israeli diplomatic staff after accusing it of using a number of Australian visas in the killing of a Palestinian in a car bombing.</p><p>The Australian government has expelled an Israeli diplomatic staff after it said the country was responsible for the use of Australian visas used in the killing of a Palestinian in a car bombing. Australia has expelled an Israeli diplomatic staff after accusing the country's security agency, the Israeli military's intelligence agency, of being responsible for the use of Australian visas used in the killing of a Palestinian. Australia has expelled an Israeli diplomatic mission after accusing the country's security agency, the Israeli military's intelligence agency, of being responsible for the use of Australian visas used in the killing of a Palestinian in the Arab city of Emirates.</p><p>The Australian government has expelled an Israeli diplomatic staff after it said the country was responsible for the use of Australian visas used in the killing of a Palestinian in the Middle East.</p><p>Figure <ref type="figure">7</ref>: FAME model predictions with Focus sample,k (k = 10000). The text in orange is not supported by the input article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBERTAS2S (Div top,k )</head><p>Australia has asked for an Ivan "shivers" officer to be asked to leave Australia after the performance of an Israeli flag was alleged to have been used as terrorism suspects in Dubai.</p><p>Australia has asked an Israeli ambassador to Sydney over an alleged implicated Australian diplomat alleging the murder of a Australian national in Dubai.</p><p>Israel has asked Israel to withdraw an Israeli ambassador from Canberra amid claims that the alleged invasion of its territory by a foreign agent was behind the murder of a terror suspect in Abuabad.</p><p>Australia has asked Israel to withdraw a diplomat Izzy Kanhuh, an Israeli diplomat involved in solving tensions over the sale of imported shotguns for the Dubai Abu Dhabiuddin bombing.</p><p>Australia has asked Australia to withdraw an ambassador from the country, amid a growing row over the alleged role of an Israel-based Abu Abu Malak director of agents.</p><p>Australia has asked Israel to replace its ambassador over a fatal stabbing in Sydney last week.</p><p>Australia has asked Israel to withdraw an Egyptian diplomat following the suicide of a suspected Abu Abu Mabhulas in the Australian capital, Canberra.</p><p>Australia has asked Australia for an official withdrawal from its embassy in Sydney after the death of a Palestinian diplomat in a Dublin diplomatic fanbase earlier this month.</p><p>Australia has asked Israel to withdraw an Israeli diplomat as part of a probe into the alleged involvement in the murder of a Abu Abuab militant.</p><p>Australia has asked an Israeli diplomat to be withdrawn from the country over the Diamondad bombing of a Abu Waduh as part of an investigation into its 2002 murders of a Abu Abu Baye bomber.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBFAME (Div top,k )</head><p>Australia has played down claims its state ambassador was involved in finding out why the Mossad spy agent was behind the Rio stabbing.</p><p>Australia has asked Israel to withdraw one of its diplomats after it confessed the so-called Mossad agent agent had used a fake Melbourne funery. Australia says it will withdraw an envoy after the Israel spy agent accused of involvement in the murder of an Arab smuggler was suspended.</p><p>Australia has asked Israel to expel one of its citizens after the country leaked the state agent that led later a deadly mafia murder in Dubai.</p><p>Australia has asked Israel to withdraw its consulate at Canberra because from its embassy after it claimed it used the Falcon fuelling plan for a suicide bomb.</p><p>Australia has asked Israel to withdraw its support for Europe's embassy for its arrest of an Edinburgh diplomat over the death of a heroin smuggling gang.</p><p>Australia has asked Israel to remove an ambassador from its embassy over the shooting dead of an Australian man on a Dubai delivery scheme.</p><p>Australia is to withdraw a diplomat from its embassy in Canberra over allegations it worked on the mastermind for an alleged spying plot for the Mossad operation. Australiachas asked Israel to withdraw an anonymous diplomat from its embassy following investigation into the passage of a Falcon recruiting device.</p><p>Australia has asked the Israeli embassy to pull out of its alleged response to the murder of a British terror suspect, accusing it of responsibility.</p><p>ROBFAME (Focus sample,k , Div top,k ) Australia has asked Israel to answer the decision to honour its state ambassador following the alleged involvement in the killing of a Dubai terror suspect. Australia has asked Israel for a second diplomat to be expelled from Australia after an alleged plot to murder a man in a bomb plot linked to Mossad. Australia has asked Israel to make a state diplomat its top diplomat after an alleged plot to bomb an Arab Emirates terror operation was blamed on a terror agent.</p><p>Australia has asked Egypt to end its diplomatic at-top diplomatic response to the murder of a top Arab diplomat in the Arab world.</p><p>Australia has asked Israel to be expelled from the embassy in Australia following the death of a Sydney spy in a spy investigation.</p><p>Australia has asked Israel to to strip an diplomat of its consulate from its embassy since a deadly operation against the Mossad spy agent at a terror squad in Australia last month.</p><p>Australia has asked the Israel embassy to withdrawing its diplomats following the death of an Arab man by Mossad agents.</p><p>Australia has asked Israel to end the original accusations that a diplomat is responsible for the killing of an agent from Mossad.</p><p>Australia has asked Israel to answer the investigation that admitted its diplomats used his agent as a suicide bomb in a Dubai plot.</p><p>Australia has asked Israel to support its ambassador after it admitted being involved in the murder of a suspect in the deadly one-off terror killing in a Melbourne bomb attack.</p><p>Figure <ref type="figure">8</ref>: Diverse summaries predicted using ROBERTAS2S and ROBFAME models with Div top,k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBERTAS2S (Div nucleus )</head><p>Australia says man hasenzelled an Israeli envoy following the arrest of one of its diplomats in Dubai from the countries' deepest-running terrorism resistant group. badly documented.</p><p>Australia has asked for Israel to out retrieving an Israeli diplomat who was expelled from the country after Australia accused the FBI of involvement in a 2013 murder in rogue Myersad drug smuggling operation.</p><p>Australia has asked Israel to remove an envoy from its embassy in Sydney in an escalating row over the killing of a Yazad Bin Ab alcohol dealer in the United Arab Emirates.</p><p>Australia has asked Israel to clarify its response to a data breach cull from rendition with a relapse of a suspected Abu Abuabuded jihadist.</p><p>Australia has asked Australia to pull out of Israel after an Israeli diplomat was accused of having used sreleased Australian agent Abuadab in the murder of an Abu Dhabi carrier.</p><p>Australia Herb Allen has led Australia's ambassadorsaints over an investigation into what was allegedly led by one of its diplomats at Nessadab consultancy in Dubai. Australia has urged Israel to withdraw an ambassador pshorze over alleged links to the murder of a Sydney binnington.</p><p>Australia has asked the Israeli ambassador to Australia over an inferno at a Sydney diplomatic consulate for a senior recruiter which printers had wanted a Willis bin Laden agent to be charged.</p><p>Australia has asked Australia for the withdrawal of an Israeli ambassador after an investigation into it was linked to a Vietnam-based gang in which a young dungeonsad spy was killed.</p><p>Australia has asked Israel for an emotional withdrawal from its embassy in Canberra, accusing an Israeli diplomat of involvement in a feuding plot to kill a terror suspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBFAME (Div nucleus )</head><p>Australia has asked Israel to withdraw an Israeli official over a Team Mossad bomb plot that left one of its suspects in the Dubai Arab desert.</p><p>Australia has asked all Israeli diplomats to leave Canberra after the living place of an alleged Russian special forces agent was identified at the email bug held bymacadad.</p><p>Australia is to withdraw an official sensitivity inquiry from its foreign ministers after Israel was accused of involvement in a plot to kill a Dubai terror suspect.</p><p>Australia has asked Israel deep back into allegations it carried out a wanted plot Cunning deaths in a Dubai plot by Mossad agents.</p><p>AustraliaplayedAX has asked the Israeli government to withdraw an official language envoy from its embassies following the killing of a murdered cons consulate officer.</p><p>Australia has asked an Israeli official to withdraw an official ambassador after it made a murder in a deadly shooting Presumably by Mossad.</p><p>Australia has asked Israel over allegations that an agent used forged passports to plot the Woolstroken murder by agentsbased in Pakistan.</p><p>Australia has asked Israeli authorities to withdraw an official diplomat from Australia after the mafia was accused by the Israel embassy of contributing to its alleged failed murder of an Alquer Arab Shia terrorist.</p><p>Australia has asked an Israeli embassy to withdraw a diplomat from Australia following the Jewlands' murder of an unnamed man. Australia has annexed its embassy up tolishes at the start of the year after Israel confirmed it assessed the role of an undercover officer during the Dubai heroinmer plot.</p><p>ROBFAME (Focus sample,k , Div nucleus ) Australia has asked Israel to expelled an embassy diplomat over a deadly Sydney plot to spy on the Mossad operation.</p><p>Australia has asked Israel to end its diplomatic inruru from Australia after it accused its diplomatic staff of involvement in last year's deadly attack on a Melbourne terror attack.</p><p>Israel has asked Israel to make an embassy ambassador over a deadly email killing of a man in a terror plot.</p><p>Australia has asked Israel to strip its diplomatic staff of its passport following an alleged plot to murder a Dubai terror suspect.</p><p>Israel has asked Israel to expelled one of its diplomats after the Mossad agent accused a Melbourne man of being the agent for the Mossad spy agent for his role in an alleged plot to murder a man. Australia has asked Israel to strip a top envoy from his embassy following its investigation into the killing of an alleged spy in a Melbourne email plot.</p><p>Australia has asked Israel to expelled one diplomat following allegations it used a military agent to spy for Mossad.</p><p>Australia has asked the Israel embassy to be expelled from Australia after an Australian diplomat was found guilty of his role in the murder of an Australian terror suspect. Australia is to expelled its top diplomat from Australia after his country was accused by the UN of being responsible for an alleged plot to murder a Melbourne-Arab m intelligence agent.</p><p>Australia has asked Israel to strip an ambassador from its embassy, in response to the death of a Sydney-from-agent for the so-called "Mossad, was responsible".</p><p>Figure <ref type="figure">9</ref>: Diverse summaries predicted using ROBERTAS2S and ROBFAME models with Div nucleus . The Australian government has expelled an Israeli diplomat, after it concluded that his desk was responsible for the issuance of forged Australian passports used in the killing of a Hamas militant. Australia has recalled her envoy from Israel, after finding that an Israeli diplomat was responsible for the counterfeiting of passports used by the Unesco agency director who was killed in Dubai.</p><p>The Australian government has asked Israel to withdraw from its Embassy in Melbourne after accusing it of using forged Australian passports to fund the killing of a Palestinian militant.</p><p>The Australian government has asked Israel to withdraw its ambassador for failing to acknowledge its role in the use of forged Australian passports in the killing of a British businessman. Australia has formally demanded the removal of anIsrael diplomat in response to a decision to accuse the Jewish organisation Mossad of use of forged Australian passports mentioned in a Dubai bombing plot.</p><p>PEGFAME (Focus sample,k , Div top,k ) Australia has expelled an Israeli because "its anti-espionage agents" used visas from other nations to issue a Palestinian agent's body £2.3m (£"2.3,1) car and land lift to Hezbollah in the killing of a senior Palestinian in the city:</p><p>The Australian government has ejected an Israeli at its embassy over the use of Australia's visas in the killing of a terror attack in the city of D'scale. Australia has expelled an Israeli diplomats following an investigation into the use of the country's travel services as cards used in a terror attack. Australia has expelled an Israeli embassy transport staff after a police investigation found the country's intelligence agency, the Israeli intelligence agency, was at responsible. Australia has expelled an Israeli embassy gathering after it said the country was responsible for the use of the use of Australia's Australian emails in an emailed attack on a former Australian consulate in the Arab world. Australia has expelled an Israeli diplomatic side after accusing it of using at first issued Australian DNA test cards to produce the Irish agent in the stepped-up Emirates bombing. Australia has expelled Israeli diplomats after its foreign minister said the country had been to be responsible for the use of staged Australian emails by Israeli intelligence. Australia has told Israel to withdraw a diplomatic mission from its country -after it said it was "in no Petroleum to Finish" the killing of a Palestinian in the killing of a terror the network by the Israeli security agency, enzymes. Australia has demanded Israel withdraw a diplomats following the Israel Security Service's use of Israeli-issued Australian travel visas to help one of its agents commit a terror attack.</p><p>Figure <ref type="figure">10</ref>: Diverse summaries predicted using PEGASUS and PEGFAME models with Div top,k .</p><p>PEGASUS (Div nucleus ) Israel hasracuse withdrawn an envoy after the Australian government said it concluded that Israeli agents used forged passports used to kill a Dubai Bendigo businessman. Australia has demanded the withdrawal of an Israeli diplomat, saying his arrival in Canberra was only necessary to deal with a spillover from the killing of a Hamas militant in Dubai in January.</p><p>The Australian government has recalled an Israeli diplomat over accusation that fake Australian passports used 436 kilometres (300 miles) from Canberra in the death of a Hamas militant were stolen by Israeli agents.</p><p>The Australian government has recalled aLatis from Israel for having strong evidence that their embassy was used to counterfeite passports used in the killing of a bidder in Dubai. Australia has expelled an Israeli diplomat in a row about the use429 Australian passportsocally used in the killing of an intends in Dubai. slew of passports were used cosmetics in the killing. Australia is seeking to expel an Israeli envoyrolet over the use of forged Australian passports in the murder of a militant in Dubaiselection. Australia has expelled an Israeli diplomat after saying it was "certain" eagle-eyed undercover agents were Rhys Shapiro and Glenn Clift, who used forged Australian passports to kill an Soros geneticist in Dubai in 2015. Australia has removed the Israeli ambassador following a decision to conclude that forged Australian passports used in the death of a Palestinian Deals in the DesertDex were collaborated from Israel. Australia has recalled an Israeli diplomat, accusing Tel Aviv of "engaging in a pattern of alarming behaviour", after concluding that forged Australian passports were used in the killing of a Hamas operative in Dubai.</p><p>Israel has expelled one of its diplomats because of allegations that it helped Isabel al-Mabhouh, a British-based PalestinianORED to be kill in January, by using forged Australian passports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEGFAME (Div nucleus )</head><p>Australia has summoned Idair Kernatic, a Jerusalem consulate official, inv summoned after the extraction of a document touting the use of forged passports for a deadly bomb plot. Australia has recalled a diplomat from Israel, claiming Israel stole the original identities of passports used to kill a Hamas operative.</p><p>Australia has asked Israel Fever to withdraw a diplomat after New Zealand said Israeli agents used the fake local passports used to identify a keyoine Killer.</p><p>The Australian government has asked Israel to withdraw a diplomat after claiming the Jewish terror group Mossad used forged Australian passports in a plot to murder a Dubai imam. Australia has expelled an Israeli diplomat after confirming fake Australian passports were used to help the killing ofmagazine boss Mahmoud al-Mabhouh in Dubai. Australia has withdrawn a military characteristic of Israel after alleging its officials were behind the use of stolen Australian passportsxiety in a Dubai cash-in-transit plot. Australia has expel an Israeli diplomat over allegations that the country's Mossad spy was behind at least Jong-Bam's Becket murder. Australia has withdrawn an Israeli diplomat halves its embassy in Canberra over accusations the country's security service, Mossad, was responsible for issuing forged Australian passports.</p><p>Australia has asked Israel to withdraw one of its diplomats from Canberra after finding that phony Australian passports were used to kill an Egyptian cleric.</p><p>Australia has asked Israel to withdraw a diplomat after it said Israel was behinduse parts of forged Australian passports used in the bombing of a kayaker in Dubai.</p><p>PEGFAME (Focus sample,k , Div nucleus ) Australia has expelled an Israeli government agent after accusing it of using the use of Australian travel visas to help Israel's intelligence agency, arrest a Palestinian in a drug operation. Australia has expelled an Israeli pulled over the use of Australian espionage proteins in a terror attack.</p><p>The Australian government says Israel should withdraw a senior police mission from its embassy following an investigation into the use of Australian gel-making equipment in the killing of a Palestinian in a car bombing in a Emirates airport.</p><p>Australia has expelled an Israeli diplomats for its support for a Palestinian that was used to hack the email messages of the former head of the intelligence agency, reasoning that the expulsion was "in the best security" of the two countries. Australia has expelled Israel's second in service special operations, after accusing the country's intelligence agency, theahl, of a "poisoning". Australia has expelled an Israeli government in protest at "the use" of an Australian denied diplomatic entry in a diplomatic killing in the Arab city of controversives. Australia has expelled an Israeli posting at its embassy in a "diplomatic action", after it was found that Israeli agents had used issued Australian visas in the killing of an Egyptian man. Australia has expelled an Israeli diplomatic, accusing it of using a home-grown Palestinian with a crime on the plane he was using to be arrested in the West.</p><p>The Australian government has expel an Israeli diplomatic team, following a public investigation into the use of Australian visas to help the killing of a Palestinian in the Emirates.</p><p>The Australian government has expelled an Israeli embassy consulate in Australia after saying it was "left in no suggestions" it was responsible for the use of Australian terror attack credentials in the killing of a Palestinian in the desert.</p><p>Figure <ref type="figure">11</ref>: Diverse summaries predicted using PEGASUS and PEGFAME models with Div nucleus .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>AGOLD:</head><label></label><figDesc>Australia has expelled an Israeli diplomat saying Israel was behind the forging of Australian passports linked to the murder of a Hamas operative in Dubai. PEGASUS: Australia has expelled an Israeli diplomat after concluding that forged Australian passports used in the killing of a Hamas militant in Dubai were issued by Israel. Our PEGFAME model: The Australian government has expelled an Israeli diplomat over the use of forged Australian passports in the killing of a Hamas militant in Dubai. B PEGASUS with Top-k Sampling Israel has summoned the Australian ambassador to complain after the Australian government said forged passports used in the killing of a Hamas operative in Dubai belonged to Netanyahu's foreign ministry. The Australian government has ordered Israel to withdraw an officer over the use of forged Australian passports used by the 2013 murder of a Lebanese opposition figure in Dubai.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top 40 sentence pieces and their logits from topic distribution t X in ROBFAME and PEGFAME for the XSUM article discussed in Figure 1.</figDesc><graphic url="image-1.png" coords="8,72.00,88.17,218.26,170.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ROUGE-1 F1 scores of ROBFAME and PEGFAME models with different top-k vocabularies (Eq. (5)) on the XSUM test set. Similar patters are observed for ROUGE-2 and ROUGE-L scores.</figDesc><graphic url="image-2.png" coords="8,72.00,328.81,218.26,127.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Abstractive Summarization results on XSUM test set comparing FAME models with their baselines. For all our models, we use standard beam decoding with a beam size of 4 to generate the single best summary for a document. Focus sampling is not used here. See Section 4.3 for details on the evaluation metrics reported. Best number for each metric is boldfaced.</figDesc><table><row><cell>Models</cell><cell cols="2">Lexical Overlap (w/ ref) Sem. Sim. R1 R2 RL BERTSc.</cell><cell cols="3">Faithfulness ent. Feqa BERTFaithful Len. Rep.(↓) others % conf.</cell><cell>R1(P%) With doc.</cell></row><row><cell cols="2">ROBERTAS2S 41.45 18.79 33.90</cell><cell>80.6</cell><cell>39.1 19.8 21.5 0.216</cell><cell>21.2</cell><cell>24.2</cell><cell>71.1</cell></row><row><cell cols="2">ROBFAME 42.15 19.68 34.81</cell><cell>80.8</cell><cell>41.3 21.2 22.7 0.226</cell><cell>20.8</cell><cell>20.7</cell><cell>72.5</cell></row><row><cell cols="2">PEGASUS 44.85 22.26 37.03</cell><cell>81.7</cell><cell>43.6 24.5 27.0 0.263</cell><cell>21.1</cell><cell>6.0</cell><cell>73.8</cell></row><row><cell cols="2">PEGFAME 45.31 22.75 37.46</cell><cell>81.9</cell><cell>44.8 24.8 27.3 0.269</cell><cell>20.8</cell><cell>5.3</cell><cell>74.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>More work is needed to understand if BERTFaithful generalizes to other datasets. Assessment of diversity, relevance and faithfulness with focus sampling on the XSUM test set.</figDesc><table><row><cell cols="2">Metrics Unique</cell><cell>1</cell><cell>Dist.-N 2</cell><cell>3</cell><cell>R1</cell><cell>ROUGE R2</cell><cell>RL</cell><cell>ent.</cell><cell>BERTSc.</cell></row><row><cell>ROBERTAS2S (Div top,k )</cell><cell>9.98</cell><cell cols="7">2.5 25.0 57.7 33.6 12.0 26.5 21.8</cell><cell>76.9</cell></row><row><cell>ROBERTAS2S (Div nucleus )</cell><cell>9.99</cell><cell cols="7">4.1 30.1 62.2 32.4 11.4 25.6 19.7</cell><cell>75.7</cell></row><row><cell>ROBFAME (Div top,k )</cell><cell>9.99</cell><cell cols="7">2.3 25.0 58.1 32.7 11.3 25.7 20.3</cell><cell>76.6</cell></row><row><cell>ROBFAME (Div nucleus )</cell><cell>9.99</cell><cell cols="7">4.1 30.7 63.2 31.3 10.6 24.7 18.0</cell><cell>75.4</cell></row><row><cell>ROBFAME (Focus sample,k )</cell><cell>1.61</cell><cell cols="7">3.5 22.4 43.9 38.0 15.7 31.0 34.3</cell><cell>78.6</cell></row><row><cell>ROBFAME (Focus sample,k , Div top,k )</cell><cell>9.99</cell><cell cols="7">2.1 20.3 51.8 31.8 10.2 24.7 24.3</cell><cell>75.4</cell></row><row><cell>ROBFAME (Focus sample,k , Div nucleus )</cell><cell>9.98</cell><cell cols="7">1.9 18.4 48.2 32.9 11.1 25.8 25.9</cell><cell>76.1</cell></row><row><cell>PEGASUS (Div top,k )</cell><cell>9.98</cell><cell cols="7">1.9 23.2 55.3 36.6 14.3 28.8 27.7</cell><cell>78.4</cell></row><row><cell>PEGASUS (Div nucleus )</cell><cell>9.99</cell><cell cols="7">3.8 30.5 63.1 34.1 12.8 26.9 22.7</cell><cell>76.5</cell></row><row><cell>PEGFAME (Div top,k )</cell><cell>9.98</cell><cell cols="7">1.9 23.2 55.5 36.7 14.5 29.0 28.5</cell><cell>78.5</cell></row><row><cell>PEGFAME (Div nucleus )</cell><cell>9.99</cell><cell cols="7">3.8 30.4 63.1 34.2 12.8 27.0 23.2</cell><cell>76.6</cell></row><row><cell>PEGFAME (Focus sample,k )</cell><cell>2.77</cell><cell cols="7">2.4 16.5 34.2 37.5 15.4 30.3 33.6</cell><cell>77.9</cell></row><row><cell>PEGFAME (Focus sample,k , Div top,k )</cell><cell>8.99</cell><cell cols="7">2.8 23.0 54.7 31.5 10.3 24.4 22.8</cell><cell>74.7</cell></row><row><cell>PEGFAME (Focus sample,k , Div nucleus )</cell><cell>9.98</cell><cell cols="7">2.6 20.8 50.9 32.5 11.0 25.3 24.8</cell><cell>75.3</cell></row></table><note>and PEGASUS, respectively. Both FAME models clearly outperform their vanilla counterparts in terms of generating summaries that are more fluent (see RL and Rep.), more informative (see R1, R2 and BERTSc.) and more faithful (see ent., Feqa and BERTFaithful). Among all four models, PEGFAME summaries are most fluent, informative and faithful.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablations and SOTA comparisons on XSUM dataset. The underlined bold results are from the best performing models from literature and the bold results are the best performing FAME models.</figDesc><table /><note>tion of ROBFAME enables more accurate predictions than that of PEGFAME, in a controlled generation setting. A comparison is presented in Figure</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Faithfulness and qualitative assessment of summaries on CNN/DM dataset.</figDesc><table><row><cell>Models Len.</cell><cell>Rep. %</cell><cell cols="5">R1(P%) With doc. ent. (↑) ¬ cont. acc. avg.(#Q) doc. → sum. Feqa</cell><cell>BERTSc.</cell></row><row><cell cols="2">ROBERTAS2S 52.1 77.6</cell><cell>92.7</cell><cell>88.8</cell><cell>96.4</cell><cell>37.3</cell><cell>18.1</cell><cell>76.0</cell></row><row><cell cols="2">ROBFAME 55.5 79.6</cell><cell>92.5</cell><cell>87.3</cell><cell>96.3</cell><cell>35.2</cell><cell>19.3</cell><cell>76.1</cell></row><row><cell cols="2">PEGASUS 58.1 69.4</cell><cell>95.0</cell><cell>90.9</cell><cell>97.5</cell><cell>40.3</cell><cell>21.0</cell><cell>76.8</cell></row><row><cell cols="2">PEGFAME 58.5 71.0</cell><cell>95.3</cell><cell>91.0</cell><cell>97.6</cell><cell>41.1</cell><cell>21.1</cell><cell>76.9</cell></row><row><cell cols="3">kens from the input to the necessary extent as the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">encoder-decoder attention is not pre-trained. More-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">over, Feqa scores for ROBERTAS2S and ROBFAME</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">may not be fully comparable due to variation in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">their summary lengths and the number of Feqa</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">questions generated; the ROBFAME summaries, on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">average, are 3 words longer and generate 1.2 more</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">questions than that of ROBERTAS2S. Nevertheless,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">we don't see this kind of drop in ¬cont. scores (i.e.,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">summary not contradicting, either entailed by or</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">neutral to the document) and BERTScores.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Text editing results on Discofuse and Wik-iSplit. The underlined scores beat the current state-ofthe-art and the bold scores are the new state-of-the-art.</figDesc><table><row><cell></cell><cell>.6</cell><cell>90.3</cell><cell>-</cell></row><row><cell>PEGASUS (ours)</cell><cell>67.4</cell><cell>90.5</cell><cell>95.8</cell></row><row><cell>PEGFAME (ours)</cell><cell>67.8</cell><cell>90.7</cell><cell>95.9</cell></row><row><cell>WikiSplit</cell><cell>Exact</cell><cell>SARI</cell><cell>BLEU</cell></row><row><cell>(Botha et al., 2018)</cell><cell>14.3</cell><cell>61.5</cell><cell>76.4</cell></row><row><cell>LaseTagger (Malmi et al., 2019)</cell><cell>15.2</cell><cell>61.7</cell><cell>76.3</cell></row><row><cell>ROBERTAS2S (Rothe et al., 2020)</cell><cell>16.4</cell><cell>63.8</cell><cell>77.4</cell></row><row><cell>PEGASUS (ours)</cell><cell>16.6</cell><cell>64.1</cell><cell>77.4</cell></row><row><cell>PEGFAME (ours)</cell><cell>16.8</cell><cell>64.1</cell><cell>77.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Focus sample,k , Div top,k and Div nucleus sampling methods for the article shown in Figure5. Assessment of controlled summary generation with focus sampling Focus top,k on the XSUM test set.</figDesc><table><row><cell>Table 7 presents results from our controlled sum-</cell></row><row><cell>mary generation experiments with top-k tokens</cell></row><row><cell>from t X using focus attention (Focus top,k ) on the</cell></row><row><cell>XSUM test set. In Figures 3 and 4, we describe how</cell></row><row><cell>ROBFAME consistently outperforms PEGFAME at</cell></row><row><cell>lower values of k ∈ {50, 100, 200, 500, 1000} due</cell></row><row><cell>to their peaky and smooth t X , respectively. While</cell></row><row><cell>Figure 4 only plots ROUGE-1 F1 scores, Table 7 ad-</cell></row><row><cell>ditionally reports ROUGE-2, ROUGE-L, entailment,</cell></row><row><cell>Feqa, and BERTScores. Figure 6 presents predic-</cell></row><row><cell>tions from models using Focus top,k for the article</cell></row><row><cell>presented in Figures 1 and 5.</cell></row><row><cell>E Diverse Summarization with Div top,k ,</cell></row><row><cell>Div nucleus and Focus sample,k</cell></row><row><cell>Figures 7, 8, 9, 10 and 11 show the diverse sum-</cell></row><row><cell>maries generated using</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>PEGASUS (Div top,k ) Australia has expelled an Israeli diplomat over the use of forged Australian passports in the killing of Hamas detainee Mahmoud al-Mabhouh in Dubai. Israel has summoned the Australian ambassador to complain after the Australian government said forged passports used in the killing of a Hamas operative in Dubai belonged to Netanyahu's foreign ministry. The Australian government has ordered Israel to withdraw an officer over the use of forged Australian passports used by the 2013 murder of a Lebanese opposition figure in Dubai. The Australian government has expelled an Israeli diplomat over allegations that fake Australian passports were used to kill a Lebanese militant in Egypt two years ago. Australia has asked Israel to withdraw a diplomat over the use of forged Australian passports to kill a Hamas operative in January. Australia has expelled an Israeli diplomat in a row over the authenticated use of forged Australian passports in last year's killing of a Hamas figure in Dubai. Australia says it is expulsion an Israeli diplomat in protest over Israel's alleged role in the killing of a Hamas militant in Dubai. Australia has recalled a diplomat from Israel after accusing Berlin of fabricating false passports used in the assassination of a Hamas operative in Dubai. Israel has been asked to withdraw an official from Australia, accusing it of complicity in the falsification of Australian passports used in the killing of a Hamas operative in Dubai in January. Israel has withdrawn one of its diplomats after Canberra said it concluded that Passport Bureau agents participated in an internal Mossad plot to kill a Hamas operative in Dubai.PEGFAME (Div top,k )Australia has expelled an Israeli diplomat after it concluded somebody close to Israel's security agency, Mossad, owned forged passports which were used to abduct a Hamas rocket maker. Australia has expelled an Israeli diplomat over allegations that its intelligence agency Mossad was behind the use of forged passports in the killing of a suspected Palestinian militant. Australia has expelled an Israeli diplomat amid accusations Israel-run Mossad used forged Australian passports in the killing of a Hamas militant. Australia has expelled an Israeli diplomat in a dispute over the use of stolen Australian passports for a hit in the Dubai killing of a Lebanese militant earlier this year. An Israeli diplomat has been expelled from Australia after a Sydney police team concluded that agents from the country's security agency Mossad took part in the poisoning of Egypt's president.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In the paper we focus on assessing FAME on XSUM. But other summarization and text editing results can be found in Appendix B and C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">which are usually articles or other function words.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">λ is set to 0.5 for all experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We further experiment with long-form story highlight generation(CNN/DM;<ref type="bibr" target="#b31">Hermann et al., 2015)</ref> and two text editing tasks: Sentence Fusion<ref type="bibr" target="#b27">(Geva et al., 2019)</ref> and Sentence Splitting<ref type="bibr" target="#b6">(Botha et al., 2018)</ref>. Their results can be found in Appendix B and C. Our FAME models achieve SOTA on both text-editing tasks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">See Appendix A for implementation details and hyperparameter settings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">  6  We lowercased candidate and reference summaries and used pyrouge with parameters "-a -c 95 -m -n 4 -w 1.2."</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">We used the Feqa code available here: https:// github.com/esdurmus/feqa/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">A very similar scorer was used in the GEM benchmark<ref type="bibr" target="#b25">(Gehrmann et al., 2021)</ref> to identify and extract the subset with faithful reference summaries from the XSum dataset<ref type="bibr" target="#b53">(Narayan et al., 2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8">Feqa and BERTFaithful scores are dropped due to time constraints.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9">Additional results and model predictions for these experiments can be found in Appendix D.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10">We use NLTK v3.2.2 with case sensitive scoring to estimate BLEU scores.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11">15 SARI is a lexical similarity metric which compares the model's output to multiple references and the input in order to assess the model's ability to add, delete, and keep an n-gram.It's implementation is available at: https://github.com/tensorflow/ tensor2tensor/blob/master/tensor2tensor/ utils/sari_hook.py.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Sebastian Gehrmann, Slav Petrov, the reviewers, and the action editor for their invaluable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Topic augmented generator for abstractive summarization</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Ailem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno>CoRR, abs/1908.07026</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guiding extractive summarization with question-answering rewards</title>
		<author>
			<persName><forename type="first">Kristjan</forename><surname>Arumae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2566" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using lexical chains for text summarization</title>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Scalable Text Summarization</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language (technology) is power: A critical survey of &quot;bias&quot; in NLP</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.485</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5454" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to split and rephrase from Wikipedia edit history</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="732" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DivGAN: Towards diverse paraphrase generation via diversified generative adversarial network</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2411" to="2421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Faithful to the original: Fact aware neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mixture content selection for diverse sequence generation</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1308</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3121" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fˆ2-softmax: Diversifying neural text generation via frequency factorized softmax</title>
		<author>
			<persName><forename type="first">Byung-Ju</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><forename type="middle">Wan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9167" to="9182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Towards diverse and natural image descriptions via a conditional GAN</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno>CoRR, abs/1703.06029</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Handling divergent reference texts when evaluating table-to-text generation</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4884" to="4895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TopicRNN: A recurrent neural network with long-range semantic dependency</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to paraphrase for question answering</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1091</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="875" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1331</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="3393" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization</title>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.454</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5055" to="5070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Augmenting neural response generation with context-aware topical attention</title>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Kamalloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kory</forename><surname>Mathewson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osmar</forename><surname>Zaiane</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on NLP for Conversational AI</title>
				<meeting>the First Workshop on NLP for Conversational AI<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="18" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ranking generated summaries by correctness: An interesting but challenging application for natural language inference</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Falke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2214" to="2220" />
		</imprint>
	</monogr>
	<note>Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">WordNet: an electronic lexical database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BLEU might be guilty but references are not innocent</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="61" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Go figure! a meta evaluation of factuality in summarization</title>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tosin</forename><forename type="middle">P</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karmanya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aremu</forename><surname>Anuoluwapo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miruna-Adriana</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Clinciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaustubh</forename><forename type="middle">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Dhole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Garbacea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shailza</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounica</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyati</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saad</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><surname>Mahamood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Henrique</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Emiel Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubungo</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salomey</forename><surname>Andre Niyongabo</surname></persName>
		</author>
		<author>
			<persName><surname>Osei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Raunak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Diego</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Sedoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<idno>CoRR, abs/2102.01672</idno>
		<title level="m">The GEM benchmark: Natural language generation, its evaluation and metrics</title>
				<editor>
			<persName><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nishant</forename><surname>Subramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Akhila</forename><surname>Yerukola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiawei</forename><surname>Zhou</surname></persName>
		</editor>
		<imprint>
			<publisher>Marco Antonio Sobrevilla Cabezudo</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bottom-up abstractive summarization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4098" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DiscoFuse: A large-scale dataset for discourse-based sentence fusion</title>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3443" to="3455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Contextual LSTM (CLSTM) models for large scale NLP tasks</title>
		<author>
			<persName><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<idno>CoRR, abs/1602.06291</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unifying human and statistical evaluation for natural language generation</title>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1169</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1689" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TILM: Neural language models with evolving topical influence</title>
		<author>
			<persName><forename type="first">Shubhra</forename><surname>Kanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karmaker</forename><surname>Santu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
				<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="778" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Abstractive summarization of Reddit posts with multi-level memory networks</title>
		<author>
			<persName><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2519" to="2531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Evaluating the factual consistency of abstractive text summarization</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryscinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1910.12840</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating the factual consistency of abstractive text summarization</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryscinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9332" to="9346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Evaluating the factual consistency of abstractive text summarization</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Importance of search and evaluation strategies in neural dialogue modeling</title>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-8609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Generation</title>
				<meeting>the 12th International Conference on Natural Language Generation<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/1910.13461</idno>
		<imprint>
			<date type="published" when="2019">Marjan Ghazvininejad,. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>CoRR, abs/1611.08562</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The automated acquisition of topic signatures for text summarization</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 18th International Conference on Computational Linguistics</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>COLING 2000</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram co-occurrence statistics</title>
		<author>
			<persName><forename type="first">Chin</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter</title>
				<meeting>the 2003 Human Language Technology Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="150" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
		</author>
		<title level="m">Felix: Flexible text editing through tagging and insertion</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Encode, tag, realize: High-precision text editing</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Mirylenka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1510</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5054" to="5065" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Automatic summarization</title>
		<author>
			<persName><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Benjamins Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sparse text generation</title>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zita</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4252" to="4273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.173</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1906" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Spoken Language Technology Workshop</title>
				<meeting>the Spoken Language Technology Workshop</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Paraphrase generation from latent-variable PCFGs for semantic parsing</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-6625</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Natural Language Generation conference</title>
				<meeting>the 9th International Natural Language Generation conference<address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<publisher>UK. Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automatic summarization</title>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="103" to="233" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multireward reinforced summarization with saliency and entailment</title>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="646" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">ProphetNet: Predicting future n-gram for sequence-to-SequencePre-training</title>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2401" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OpenAI</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1901.07291</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">COMET: A neural framework for MT evaluation</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Farinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2685" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Leveraging pre-trained checkpoints for sequence generation tasks</title>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="264" to="280" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">this is a problem, don&apos;t you agree?&quot; framing and bias in human evaluation for natural language generation</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Schoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating NLG Evaluation</title>
				<meeting>the 1st Workshop on Evaluating NLG Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.704</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Joint parsing and generation for abstractive summarization</title>
		<author>
			<persName><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On the importance of diversity in question generation for QA</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Md Arafat Sultan</surname></persName>
		</author>
		<author>
			<persName><surname>Chandel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5651" to="5656" />
		</imprint>
	</monogr>
	<note>Ramón Fernandez Astudillo, and Vittorio Castelli</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Sticking to the facts: Confident decoding for faithful data-to-text generation</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Diverse beam search for improved description of complex scenes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="7371" to="7379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Asking and answering questions to evaluate the factual consistency of summaries</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.450</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="5008" to="5020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Diversify question generation with continuous content selectors and question type modeling</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangjian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="2134" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Friendly topic assistant for transformer based abstractive summarization</title>
		<author>
			<persName><forename type="first">Zhengjue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020c</date>
			<biblScope unit="page" from="485" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Towards faithful neural table-to-text generation with content-matching constraints</title>
		<author>
			<persName><forename type="first">Zhenyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020d</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Neural text generation with unlikelihood training</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>CoRR, abs/1908.04319</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dialogue natural language inference</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="3731" to="3741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Diversity-promoting GAN: A crossentropy based generative adversarial network for diversified text generation</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1428</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3940" to="3949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Self-attention guided copy mechanism for abstractive summarization</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1355" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">BERTScore: Evaluating text generation with BERT</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations, Virtual Conference</title>
				<meeting>the 8th International Conference on Learning Representations, Virtual Conference<address><addrLine>Formerly Addis Ababa Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Beta-negative binomial process and poisson factor analysis</title>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Fifteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>La Palma, Canary Islands</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1101</idno>
		<idno>ROBERTAS2S 41.45 18.79 33.90 39.1 19.8 80.6 ROBFAME 42.15 19.68 34.81 41.3 21.2 80.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1095" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">ROBFAME (Focus top,k=50 )</title>
		<idno>30.90 10.60 24.85 27.1 10.6 74.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<idno>33.62 12.39 27.14 30.3 12.4 74.2</idno>
		<title level="m">ROBFAME (Focus top,k=100 )</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<idno>35.99 14.12 29.23 32.4 13.9 77.3</idno>
		<title level="m">ROBFAME (Focus top,k=200 )</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<idno>38.29 16.04 31.30 35.8 15.9 78.6</idno>
		<title level="m">ROBFAME (Focus top,k=500 )</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<idno>39.58 17.18 32.49 37.3 17.3 79.3</idno>
		<title level="m">ROBFAME (Focus top,k=1000 )</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title/>
		<idno>k=50 ) 24.30 7.52 19.32 20.8</idno>
	</analytic>
	<monogr>
		<title level="j">PEGFAME (Focus top</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<idno>27.77 9.26 22.09 24.1</idno>
		<title level="m">PEGFAME (Focus top,k=100 )</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<idno>31.05 11.14 24.82 27.0 10.8 73.6</idno>
		<title level="m">PEGFAME (Focus top,k=200 )</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<idno>34.99 13.65 28.19 31.0 13.0 76.2</idno>
		<title level="m">PEGFAME (Focus top,k=500 )</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<idno>37.40 15.30 30.16 33.6 14.9 75.9</idno>
		<title level="m">PEGFAME (Focus top,k=1000 )</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<idno>42.76 19.89 34.97 40.2 20.1 80.5</idno>
		<title level="m">PEGFAME (Focus top,k=10000 )</title>
				<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
