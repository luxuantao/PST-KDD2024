<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPS: Navigating Weak Memory with Ghosts, Protocols, and Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Turon</surname></persName>
							<email>turon@mpi-sws.org</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<orgName type="institution">MPI-SWS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Viktor</forename><surname>Vafeiadis</surname></persName>
							<email>viktor@mpi-sws.org</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<orgName type="institution">MPI-SWS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Derek</forename><surname>Dreyer</surname></persName>
							<email>dreyer@mpi-sws.org</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<orgName type="institution">MPI-SWS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GPS: Navigating Weak Memory with Ghosts, Protocols, and Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BFFE9C42F23F03656487B7B1F52110E0</idno>
					<idno type="DOI">10.1145/2660193.2660243</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.3.1 [Programming Languages]: Formal Definitions and Theory</term>
					<term>F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs Concurrency</term>
					<term>Weak memory models</term>
					<term>C/C++</term>
					<term>Program logic</term>
					<term>Separation logic</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weak memory models formalize the inconsistent behaviors that one can expect to observe in multithreaded programs running on modern hardware. In so doing, however, they complicate the already-difficult task of reasoning about correctness of concurrent code. Worse, they render impotent the sophisticated formal methods that have been developed to tame concurrency, which almost universally assume a strong (i.e., sequentially consistent) memory model.</p><p>This paper introduces GPS, the first program logic to provide a full-fledged suite of modern verification techniquesincluding ghost state, protocols, and separation logic-for high-level, structured reasoning about weak memory. We demonstrate the effectiveness of GPS by applying it to challenging examples drawn from the Linux kernel as well as lock-free data structures. We also define the semantics of GPS and prove in Coq that it is sound with respect to the axiomatic C11 weak memory model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When reasoning about the behavior of a multithreaded program, what can we assume about the interactions between concurrent threads and the shared memory they operate on? In the vast majority of the research on concurrent program verification, it is assumed that shared memory accesses are sequentially consistent (SC)-i.e., there is a single global RAM, threads take turns interacting with it, and any memory update performed by one thread is immediately visible to all other threads. Even assuming sequential consistency, concurrent program verification is a highly challenging problem, since one must account for the myriad interleavings of threads. But fortunately there has been tremendous progress in recent years on advanced program logics and verification tools to help tame the complexity of interleaved execution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Unfortunately, the assumption of sequential consistency is unrealistically "strong": the synchronization required to implement it on modern architectures precludes useful compiler optimizations that reorder memory operations, and is thus considered by many to be too expensive in general <ref type="bibr" target="#b5">[6]</ref>. Instead, languages like C/C++ <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> and Java <ref type="bibr" target="#b25">[26]</ref> support weak (or relaxed) models of memory, in which different threads may observe operations on shared memory occurring in different orders. To characterize precisely what types of inconsistent observations are permitted, these languagelevel memory models eschew the fiction of a single global RAM and an interleaving semantics; rather, they model valid program executions using event graphs, which track dependencies between memory accesses subject to a variety of consistency axioms, e.g., "if this event is visible to a thread t, then so are these other events."</p><p>In short, weak memory models are useful in enabling compilers and hardware to aggressively optimize memory accesses, but they also invalidate the basic assumptions underlying existing verification tools and complicate the semantics of concurrent code. As such, they have led to a serious gap between the theory and practice of concurrency.</p><p>This paper takes a substantial step toward closing that gap by presenting the first concurrent program logic that is sound under weak memory assumptions but also supports a full suite of modern verification techniques: ghost state, protocols, and separation (GPS). Below, we briefly explain why these techniques have proven important in reasoning under strong memory assumptions ( §1.1) and the obstacles we face in adapting them to weak memory ( §1.2), before describing our contributions in more detail ( §1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Concurrent program logics: the state of the art</head><p>The goal of most program logics is to prove "deep" correctness properties of code, and to do so in a modular fashion, whereby different components of a program can be verified in isolation, given only logical specifications (specs) of the other components. Modern logics for SC concurrency meet this goal through a variety of mechanisms-among the most widespread and effective are the following:</p><p>Ownership and separation. Concurrent programs are often inherently modular in the sense that different threads within a program control (or "own") disjoint pieces of the program state. This modularity is important for simplifying verification: if a thread owns a piece of state, one should be able to verify the thread's manipulations of that state without worrying about interference from other threads. Modern logics encapsulate this kind of reasoning through the mechanisms of ownership and separation.</p><p>Consider, for instance, the parallel composition rule of concurrent separation logic (CSL) <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_0">{P1} e1 {Q1} {P2} e2 {Q2} {P1 * P2} e1 || e2 {Q1 * Q2}</formula><p>Here, P i and Q i not only describe the facts that hold before and after the execution of thread e i , they also characterize the piece of the program state that e i "owns". The rule thus says that e 1 and e 2 may be safely run in parallel-without any interference checking-so long as P 1 and P 2 describe disjoint pieces of state, as enforced implicitly by the use of the "separating conjunction" P 1 * P 2 in the precondition.</p><p>Protocols. Separation lets one dispense with interference implicitly when threads do not in fact interfere. But sometimes explicit reasoning about interference is unavoidable, e.g., when reasoning about racy (lock-free) data structures. In such cases, the most basic mechanism for restoring modular reasoning is the invariant, which describes a property holding of a piece of shared state at all times. With an invariant installed, different threads can be verified modularly so long as they all respect the invariant.</p><p>More generally, since invariants can be overly restrictive, modern logics support various forms of protocols for legislating interference. The best-known protocol mechanism is rely-guarantee <ref type="bibr" target="#b22">[23]</ref>, which describes the state transitions a thread may perform (the guarantee) vs. those its environment may perform (the rely). Recent protocol mechanisms improve upon rely-guarantee by supporting more abstract/concise forms of shared state transition systems <ref type="bibr" target="#b36">[37]</ref>.</p><p>Ghost state. Last but not least, ghost (or auxiliary) state refers generally to any behavior-preserving instrumentation of a program (or its proof) with additional "logical" state for the purposes of verification. Ghost state is often used to expose control flow, or to summarize execution history, in a way that could not be done just in terms of the "physical" state manipulated by the program. Furthermore, it is essential for the completeness of basic concurrency logics.</p><p>In newer logics, ghost state, protocols, and separation are used in tandem to great effect. For example, ghost state can be used to encode logical "permissions" (or "tokens"), which are ownable resources that control the ability to make certain transitions in shared state protocols. Ownership of permissions can then be transferred back and forth between threads via the same shared protocols, in turn providing a way to model the dynamic "role-playing" that occurs in realistic concurrent code. Logics such as RGSep <ref type="bibr" target="#b38">[39]</ref>, LRG <ref type="bibr" target="#b15">[16]</ref>, Deny-Guarantee <ref type="bibr" target="#b14">[15]</ref>, VCC <ref type="bibr" target="#b8">[9]</ref>, Chalice <ref type="bibr" target="#b23">[24]</ref>, CAP <ref type="bibr" target="#b12">[13]</ref>, CaReSL <ref type="bibr" target="#b36">[37]</ref>, FCSL <ref type="bibr" target="#b28">[29]</ref>, iCAP <ref type="bibr" target="#b35">[36]</ref>, and TaDA <ref type="bibr" target="#b10">[11]</ref> depend on such a synthesis of ghost state, protocols, and separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Obstacles to modular weak memory reasoning</head><p>While the aforementioned mechanisms provide powerful, modular reasoning about concurrency, there are serious obstacles to adapting them to weak memory models like those of C/C++ or Java:</p><p>Separation obstacles. Models of concurrent separation logics have generally assumed the existence of a single global RAM (pieces of which may be owned by different threads) and a single global notion of "time" (based on an interleaving semantics). However, in weak memory models based on event graphs, there is no clear global notion of a heap or of time, making it unclear how to model basic notions like Hoare triples and separation.</p><p>Protocol obstacles. Most logics support protocols that govern multiple memory locations simultaneously, connecting the value of one location to another. But even this simple mechanism is unsound for weak memory: updates to different locations may appear in contradictory orders to different threads, so a thread can appear to be following the protocol from its own point of view while violating it from the point of view of other threads.</p><p>Ghost state obstacles. Traditional ghost state is incorporated by introducing explicit reads and writes to a program text, with the constraint that these operations must not change the code's observable behavior. But in weak memory models it is not clear how to usefully incorporate such reads and writes without also introducing events and ordering into the event graph that ultimately affect the program's behavior.</p><p>An important first step toward overcoming these obstacles is the recent work of Vafeiadis and Narayan on Relaxed Separation Logic (RSL) <ref type="bibr" target="#b37">[38]</ref>, the first logic for the C11 memory model. RSL supports simple, high-level reasoning about resource invariants and ownership transfer à la concurrent separation logic (CSL) <ref type="bibr" target="#b29">[30]</ref>-a particularly simple combination of protocols and separation. But RSL provides no support for ghost state or for more complex forms of protocol (e.g., rely-guarantee) or ownership transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">This paper</head><p>In this paper, we present GPS, the first logic to support ghost state, protocols and separation in a weak memory setting. GPS builds on the groundwork laid by RSL, extending and generalizing it in several useful ways:</p><p>• Protocols. GPS supports per-location (PL) protocols, which are modeled after the protocols in recent concurrency logics but restricted in order to be sound under weak memory. The key to regaining soundness is to insist that a protocol may only precisely dictate the evolution of a single shared memory location, although it may make bounded assertions about the state of other memory locations, e.g., "x's value may only grow over time, and when x contains n, y must contain at least n as well."</p><p>• Ghost state. The states of PL-protocols already constitute a useful form of ghost state for summarizing, e.g., the history of an execution. To support ownable logical resources (e.g., permissions), GPS offers an additional facility called ghosts. Ghosts enable one to create and manipulate whatever kind of logical resource one needs for a particular verification, so long as it can be formulated as a partial commutative monoid <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>• Ownership transfer. In prior SC logics, threads can transfer ownership of resources to other threads through the medium of a shared protocol. GPS's PL-protocols also support ownership transfer between threads, but for soundness purposes it is somewhat restricted: the acquiring thread must perform an explicit synchronization operation like CAS in order to ensure that it is the exclusive recipient of the transfer. To facilitate ownership transfer even when the threads use only plain reads and writes, we introduce an additional mechanism called escrows.</p><p>We demonstrate the use of the above logical features through a series of concrete motivating examples in §3.</p><p>GPS targets the recent C11 <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> memory model, which offers portable but fine-grained control over memory consistency guarantees. GPS supports verification of programs that use the three most important consistency modes for C11: nonatomic, release-acquire and sequentially-consistent (see §2). Since sequentially-consistent reasoning is relatively well understood, the paper presents the details only for the first two modes (but see §6 for further discussion of the SC mode).</p><p>It is nonetheless worth stressing that verifications in GPS hold good under the full axioms of the C11 model (and thus for any compliant compiler). Moreover, the entire logic, model and soundness proof of GPS have been formalized in Coq <ref type="bibr" target="#b0">[1]</ref>.</p><p>For space reasons, we focus in this paper almost entirely on the proof theory of GPS; §3.7 sketches some details of the semantic model and soundness proof, but for full details we refer the reader to the appendix and Coq development.</p><p>To evaluate GPS, we have applied it to several challenging case studies drawn from the Linux kernel and lock-free data structures, as we describe in §4. These examples extend the reach of existing program logics: we know of no other logic that can verify them under C11's weak memory assumptions. We conclude in §5 and §6 with related and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The C11 memory model</head><p>Memory models answer a seemingly simple question: when a thread reads from a location, what values can it encounter?</p><p>• Sequential consistency (SC) provides an equally simple answer: threads read the last value written. SC is based on interleaving, where threads interact atomically through a global heap holding each location's current value. • In weaker consistency models, the "last value written" to a location plays no special role-it may not even be welldefined. Instead, threads can read out-of-date values due to CPU or compiler optimizations.</p><p>The C11 memory model <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> strikes a careful balance between these extremes by offering a menu of consistency levels. Broadly, memory operations are classified as either nonatomic (the default) or atomic. Nonatomic accesses are intended for "normal data", while atomic accesses are used for synchronization. Nonatomics are governed by a peculiar contract: the programmer can assume them to be SC, but must (under this assumption!) never create a data race-roughly, a thread must never write nonatomically if another thread might access the same location concurrently. This rule prevents the program from observing compiler/CPU optimizations on nonatomics.</p><p>Atomics offer the opposite tradeoff: concurrent threads may race to e.g., update a location atomically, but the memory model provides weaker guarantees (and admits fewer optimizations) for atomic accesses in general. The precise guarantees are determined by an "ordering annotation", ranging from SC to fully relaxed. In this paper, we focus on the release-acquire ordering, which is the primary building block for non-SC synchronization. As such, we will use two ordering annotations, O ∈ {at, na}, for atomic (release-acquire) and nonatomic accesses, respectively. The full version of the memory model and GPS logic, as formalized in Coq <ref type="bibr" target="#b0">[1]</ref>, also includes sequentially-consistent accesses.</p><p>Examples Before introducing C11 formally, we build some intuition through two classic examples. The first is a simplified version of Dekker's algorithm, which provided the first solution to the mutual-exclusion problem <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_1">[x]at := 1 if [y]at == 0 then /* crit. section */ [y]at := 1 if [x]at == 0 then /* crit. section */</formula><p>We presume at the outset that x and y are pointers to distinct locations, both with initial value 0. <ref type="foot" target="#foot_0">1</ref> The two threads race to announce their intent to enter a critical section; each thread then checks whether it announced first. In this simplified version, even under SC, it is possible for both threads to lose. Unfortunately, in the C11 model, it is also possible for both threads to win! The intuition is that C11 allows the reads to</p><formula xml:id="formula_2">Syntax v ::= x | V where V ∈ N e ::= v | v + v | v == v | v mod v | let x = e in e | repeat e end | if v then e else e | fork e | alloc(n) | [v]O | [v]O := v | CAS(v, v, v) K ::= [ ] | let x = K in e T ∈ N fin (ActName × Exp) Event steps e α -→ e K[e] α -→ K[e ] if e α -→ e alloc(n) A( .. +n-1) --------→ [ ]O R( ,V,O) -----→ V [ ]O := V W( ,V,O) ------→ 0 CAS( , Vo, Vn) U( ,Vo,Vn) -------→ 1 CAS( , Vo, Vn) R( ,V ,at) ------→ 0 if V = Vo Machine steps T ; G -→ T ; G e α -→ e consistentC11(G ) G .A = G.A [a → α] G .sb = G.sb (a, a ) G .mo ⊇ G.mo G .rf ∈ {G.rf, G.rf [a → b]} T [i → (a, e)]; G -→ T [i → (a , e )]; G T [i → (a, K[fork e])]; G -→ T [i → (a, K[0])] [j → (a, e)]; G Figure 1.</formula><p>Syntax and semantics of a language for C11 concurrency be performed before the writes have become visible to all threads: the two threads can read stale values.</p><p>The second example illustrates a case where C11 atomics do enforce some ordering. The goal is to pass a "message" (in this case, just a single value, 37, but more generally a data structure) from one thread to another:</p><formula xml:id="formula_3">[x]na := 37; [y]at := 1; repeat [y]at end; [x]na</formula><p>Again, we presume x and y are pointers to distinct locations, initially 0. The repeat construct executes an expression repeatedly until its value is nonzero, so the second thread will "spin" until it sees the write to y by the first thread. Unlike in Dekker's algorithm, here C11 will guarantee that the subsequent read from x will return 37. The key difference is that reading 1 from y yields positive information about what the first thread has done: if an atomic (release) write by a given thread is seen by another thread, so is everything that "happened before" the write, including all the writes that appear prior to it in the thread's code. Dekker's algorithm, by contrast, draws conclusions from not seeing a write by another thread.</p><p>In general, then, release-acquire in C11 doesn't guarantee that threads see the "globally-latest" write to a location, but does guarantee that (1) if a thread sees a particular write, it also sees everything that happened before it, and (2) of the writes a thread sees for a location, it reads from the latest.</p><p>A final point about the code: its use of y guarantees that the write to x by the first thread happens before-not concurrently with-the read of x by the second thread. So the code is data-race free, despite nonatomic accesses to x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event graphs</head><p>We now present the C11 model formally, following Batty et al. <ref type="bibr" target="#b2">[3]</ref> and subsequent simplifications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38]</ref>. Our presentation makes some further simplifications due to our focus on release-acquire atomics, but GPS is sound for reasoning about non-atomic, release-acquire and SC accesses under the official C11 axioms. Reasoning about more advanced features of the C11 model-namely, relaxed and consume atomics-is left as future work and discussed in §6.</p><p>Since weak memory models allow threads to see stale values, they must track the history of an execution and use it to specify the values a read can return. The C11 model takes the axiomatic approach: it treats each step of a program execution as a node in a graph, and then constrains the graph through a collection of global axioms on several kinds of edges. Each node is labeled with an action:</p><formula xml:id="formula_4">α ::= S | A( .. ) | R( , V, O) | W( , V, O) | U( , V, V )</formula><p>where O ∈ {na, at}. The actions are Skip (no memory interaction), Allocate, Read, Write, and atomic Update. Reads and writes record the location, value read/written, and ordering annotation. An atomic update U( , V o , V n ) simultaneously reads the value V o from location and updates it with the new value V n (used for e.g., compare-and-set).</p><p>We assume an infinite set of event IDs; an action map A is then a finite partial map from event IDs to actions, which defines the nodes (and node labels) of a graph. An event graph G = (A, sb, mo, rf) connects the nodes with three kinds of directed edges:</p><p>Sequenced-before (sb ⊆ dom(A) × dom(A)), which records the order of events as they appear in the code (i.e., "program order"). For convenience, sb is not transitive: it relates each node only to its immediate successors in program order (see <ref type="bibr" target="#b37">[38]</ref>).</p><p>Modification order (mo ⊆ dom(A) × dom(A)), which is a strict, total order on all the writes to each location, but does not relate writes to different locations. It determines which of any pair of (possibly concurrent) writes to a location is considered to "take effect" first-a determination that is agreed upon globally.</p><p>Reads-from (rf ∈ dom(A) dom(A)), which maps each read to the unique write, if any, that it is reading from. It is undefined for reads from uninitialized locations.</p><p>The goal of the C11 axioms is to constrain the rf relation so that it provides the guarantees mentioned informally above. The axioms rely on a pair of derived relations:</p><p>Synchronized-with (sw ⊆ dom(A) × dom(A)) defines those read-write pairs that induce "transitive visibility", as in the message-passing example above. In the release-acquire fragment of C11, these include any read/write pair marked as atomic:</p><formula xml:id="formula_5">sw {(a, b) | rf(b) = a, isAtomic(a), isAtomic(b)}</formula><p>Happens-before (hb (sb ∪ sw) + ) is the heart of the model: hb(a, b) means that if a thread has observed event b, then it has observed event a as well; it bounds staleness.</p><p>Axioms Only the sb order is determined by the program as written. The other orders are chosen arbitrarily-except that they must satisfy C11's axioms. These axioms include some sanity checks:</p><p>• hb is acyclic (an event cannot happen before itself),</p><p>• a location cannot be allocated more than once,</p><p>• rf maps reads to writes of the same location and value, and it is not possible to read a value from a write that happens later:<ref type="foot" target="#foot_1">2</ref> rf(b) = a =⇒ ∃ , V. writes(a, , V ), reads(b, , V ), ¬hb(b, a)</p><p>• atomic updates must, in fact, be atomic: the update must immediately follow the event it reads from in mo: To see how coherence formally ensures the intuitive guarantees we gave above, we apply it to the simple message-passing example, this time in graph form: In the depicted execution, the event d in the second thread reads from the event c in the first thread (which writes 1 to y). We use coherence to deduce that the subsequent read of x in event e must read from event b (which writes 37 to x):</p><p>• Since sb(a, b), and thus hb(a, b), we have ¬mo(b, a). But mo is a total order on writes to a location, so mo(a, b). The key is the second step, where we deduce the existence of an sw edge (and thus the transitive visibility, by hb, of previous writes). In Dekker's algorithm, by contrast, when one thread reads the other's flag, there are no hb edges that ensure it sees the "latest" write. We write consistentC11(G) if a graph G satisfies the axioms above (plus one more for uninitialized reads).</p><formula xml:id="formula_6">• Since rf(d) = c,</formula><p>A language for C11 concurrency Figure <ref type="figure" target="#fig_3">1</ref> gives a simple language of expressions e with allocation, pointer arithmetic, thread forking and order-annotated memory operations. To streamline the semantics, we adopt A-normal form <ref type="bibr" target="#b17">[18]</ref>, which requires intermediate computations to be named through let-binding (the only evaluation context K). The if expression takes the then branch when its guard is non-zero. Similarly, repeat executes the given subexpression until it produces a non-zero value, which is returned.</p><p>The semantics is given in two layers. First, expressions e freely generate actions α through the relation e α -→ e . Pure expressions generate the S action (e.g., let x = V in e S -→ e[V /x]), while expressions that interact with memory generate corresponding memory model actions. Note that reading generates an R action for an arbitrary value. The actual value read is constrained by the second layer, which governs machine configurations T ; G .</p><p>Machine configurations track the current pool of threads, T , and the event graph built up so far, G. For each thread, the pool maintains (1) the identity of the last event produced by the thread and (2) an expression giving the thread's continuation. To take a (non-fork) step, a thread's continuation must generate some action α, which is then incorporated into an updated event graph G , where it is placed in sb order after the thread's previous event. The mo order for G can arbitrarily extend the one for G, but because it is a strict total order on writes, the extension will only add relationships to the new node. The rf order can likewise only add a read for the new node, which must read from some previously-existing write. Finally, the new graph G is assumed to satisfy the C11 axioms, constraining both the possible events and edges. The validity of this semantics for C11 is discussed in the appendix <ref type="bibr" target="#b0">[1]</ref>.</p><p>We write e for the set of final values e can produce, starting with a single-node event graph (where the start node is action S). If at any point e creates a data race or memory error (defined formally in the appendix), then e = err; the C11 semantics leaves such programs undefined. Any expression verified by GPS is guaranteed to be free of data races on non-atomic locations and memory errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GPS</head><p>The C11 memory model successfully serves as a contract between compiler and programmer, making it possiblein principle-to resolve disputes (can a read of x here return 0?) by reference to global axioms. These axiomsagain, in principle-also support certain intuitions about, e.g., transitive visibility. But, even with an example as simple as one-shot message passing ( §2), the intuitions are not directly captured by the axioms. Rather, they emerge through chains of subtle reasoning showing that certain edges must, or must not, exist. Axiomatic reasoning is relentlessly global: a read event can potentially read from any write in the graph, so the axioms must be applied to each write to rule it in or out.</p><p>Our goal is to supplement the C11 memory model with a program logic that (1) directly captures intuitions about transitive visibility and (2) supports thread-local reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPS achieves both goals through</head><p>• per-location protocols that abstract away event graphs;</p><p>• ghosts and escrows, which govern logical permissions in the style of recent separation logics.</p><p>Setup GPS is a separation logic for an expression language. Its central judgment is the Hoare triple, {P } e {x. Q}, which says that when given resources described by P , the expression e is memory safe and data-race free. If, moreover, e terminates with a value V , it will do so with resources satisfying Q[V /x]. We will introduce assertions P gradually. For now, we assume they include the basic operators of multi-sorted first-order logic:</p><formula xml:id="formula_7">P ::= t = t | P ∧ P | P ∨ P | P ⇒ P | ∀X.P | ∃X.P</formula><p>where t ranges over terms. We write t : θ if t has sort θ, and assume that variables X are broken into classes by sort. For now, the only sort is Val, ranged over by variables , x, y, z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Per-location protocols</head><p>We start with a slight variation on the message-passing example from §2:</p><p>[x]at := 37;</p><p>[y]at := 1;</p><formula xml:id="formula_8">• • • [x]at := 37; [y]at := 1; repeat [y]at end; [x]at</formula><p>In this variant, there are multiple threads sending the same message (37), <ref type="foot" target="#foot_2">3</ref> and intuitively this works for the same reason the original does: transitive visibility. We want to articulate this common intuition in a way that doesn't depend on how many threads are sending the message 37 or involve global reasoning about the event graph.</p><p>A tempting starting point is to simply say that the values that x and y point to progress from (0, 0) to (37, 0) to (37, 1). In other words, we would like to impose a protocol on the evolution of x and y. Such protocols are the lifeblood of prior SC concurrency logics (see §5.1 for details), but alas, the kind of reasoning they support is unsound for weak memory in general: it assumes that all threads will see writes to different locations in the same order. In actuality, independent (i.e., hbunrelated) writes to different locations can appear to threads in different orders, which is why Dekker's algorithm fails. If we want thread-local reasoning, we need an approach that accounts for what our thread may see, while capturing the happens-before relationship between writes.</p><p>The key insight of GPS is that we can constrain the evolution of values if we focus on one location at a time: mo provides a linear order, seen by all threads, on the writes to a given location. Toward this end, GPS provides per-location protocols, which are state transition systems governing a single shared location. Using protocols, we can express the changes to x and y independently:</p><p>x : 0 37 y : 0 1</p><p>These transition systems offer an abstraction of the event graph: each state represents a set of write events, while edges represent mo relationships between them. Thus, for x, we see that all of the writes of 37 are mo-later than the initial write of 0. But these independent constraints alone are not enough: we must ensure that y can only be in state 1 if x is "known" to be in state 37.</p><p>In general, protocol states are abstract; the labels on the transition systems above are merely suggestive. Each state is given an interpretation, which constrains the values that may be written to the location in that state, but may also impose other constraints-including, as we will see, constraints on other protocols. (Treating states abstractly allows us to, in effect, associate a ghost variable with each memory location, as §4 will show.)</p><p>Formally, we assume a sort State of protocol states, ranged over by variables s. GPS is parameterized by (1) the grammar of terms of sort State and (2) a set of protocol types (metavariable τ ). For each protocol type τ , the user of the logic specifies:</p><p>• A transition relation τ , a partial order on states.</p><p>• A state interpretation τ (s, z), an assertion in which s and z appear free (i.e., a predicate on s and z). The assertion represents what must be true of a value z for a thread to be permitted to write it to the location in state s.</p><p>For the message passing example, we introduce a protocol type Dat governing location x. Writing abstract states in bold, we say 0 Dat 0, 0 Dat 37, 37 Dat 37, and define</p><formula xml:id="formula_9">Dat(s, z) (s = 0 ∧ z = 0) ∨ (s = 37 ∧ z = 37)</formula><p>To give the protocol for y, however, we need a way of talking about the protocol for x in its state interpretations. For this purpose, GPS offers protocol assertions, : s τ , which say that location is governed by the protocol type τ , and has been observed in state s, thus giving a lower bound on the current protocol state.</p><p>We can now give the protocol for y. We introduce a protocol type Flg( ) that is parameterized over a location (which we will instantiate with x). Again writing abstract states in bold, we say 0 Flg 0, 0 Flg 1, 1 Flg 1, and</p><formula xml:id="formula_10">Flg( )(s, z) (s = 0 ∧ z = 0) ∨ (s = 1 ∧ z = 1 ∧ : 37 Dat )</formula><p>Thus, to move to state 1 in Flg(x), a thread must (1) write 1 to y and (2) have already observed that x : 37 Dat , which it can ensure by first writing 37 to x itself.</p><p>What happens when a thread reads y? GPS supports the following Hoare triple for atomic reads of a location :<ref type="foot" target="#foot_3">4</ref> </p><formula xml:id="formula_11">∀s τ s. ∀z. τ (s , z) ⇒ Q : s τ [ ]at z. ∃s . : s τ ∧ Q</formula><p>The precondition requires some pre-existing knowledge about 's protocol. (For the message receiver, this knowledge will be y : 0 Flg(x) .) The pre-existing knowledge gives a lower bound on the possible writes the read could read from: they must be at least as far as state s in the protocol. The premise of the rule then quantifies, abstractly, over the write we might be reading from: it must have moved to some future state s in the protocol, and have written some value z such that τ (s , z) holds. From all such possible writes, we derive a common assertion Q-but note that s and z can appear in Q, so it can tie together the value read and the state observed. Altogether, we have:</p><formula xml:id="formula_12">y : 0 Flg(x) [y]at    z. y : 0 Flg(x) ∧ z = 0 ∨ y : 1 Flg(x) ∧ z = 1 ∧ x : 37 Dat   </formula><p>So if a thread reads 1 from y, it learns a lower bound on the protocol state for x. If it subsequently reads x, it is guaranteed to see 37. Before describing the rest of GPS, we briefly consider the connection to the C11 model. GPS assertions say what is known at each point in a thread's code, with each such point corresponding to a node in the event graph. A thread will only be able to claim : s τ if a write moving to (abstract) state s happens before the corresponding node in the event graph. But because writes to in mo order correspond to moves within the protocol, the thread can subsequently read only from a write in some state s τ s. PL-protocols have allowed us to abstract away from the event graph and to reason threadlocally: the thread receiving the message does not need to know anything about the code/events of the sending threads except that they follow the protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Physical resources</head><p>GPS makes the simplifying assumption that each location is either always used nonatomically (i.e., for data), or always used atomically (i.e., for synchronization). Atomic locations can be freely shared between threads, which can only make protocol assertions about them; since protocol assertions are </p><formula xml:id="formula_13">P ::= • • • | uninit( ) | → v | P * P</formula><p>which resemble traditional separation logic, except that locations begin uninitialized. The heap assertion → v means that is classified as nonatomic, and currently points to value v. We thus get the usual separation logic axioms for nonatomic locations:</p><formula xml:id="formula_14">{true} alloc(n) x. uninit(x) * • • • * uninit(x + n -1) {uninit( ) ∨ → -} [ ]na := v { → v} { → v} [ ]na {x. x = v * → v}</formula><p>The separating conjunction P * Q requires that resources claimed by P are disjoint from those of Q, e.g.,</p><formula xml:id="formula_15">uninit( ) * uninit( ) ⇒ = → v * : s τ ⇒ =</formula><p>but since atomic locations are shared, separation enforces only that different observations about their state cohere:</p><formula xml:id="formula_16">: s τ * : s τ ⇒ τ = τ ∧ (s τ s ∨ s τ s)</formula><p>In addition to these axioms, GPS supports the usual rules for a concurrent separation logic; see Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ghost resources</head><p>Our earlier presentation of protocols implicitly assumed that all threads can make the same moves within a protocol. But we often want to say that only certain threads have the right to make a particular move. To do so, we add non-physical resources-ghosts-to GPS. These purely logical resources are used to express arbitrary notions of permission that can be divided amongst threads. Here we explain what ghosts are; the subsequent subsections explain how they are used together with protocols. Following recent work in separation logic <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>, we model ghosts as partial commutative monoids (PCMs): GPS is parameterized by a collection of PCMs µ, such that • There is a sort PCM µ for each µ,</p><p>• Terms of sort PCM µ include unit ε µ and composition • µ .</p><p>The unit represents the empty permission, while t • µ t combines the permissions t and t . We do not want all compositions to be defined: we want certain permissions to be exclusive, meaning that they do not compose with themselves. So composition is a partial function, but is commutative and associative where defined (and ε µ • µ t = t for any t).</p><p>Within the logic, we add ghost assertions, γ : t µ , which claim ownership of the ghost permission t drawn from some PCM µ. Since we may want to use many instances of a particular PCM, ghosts have an identity γ. Being nonphysical, ghosts are manipulated entirely through the rule of consequence, which is generalized to allow ghost moves , rather than just implications; see Figure <ref type="figure">2</ref>. These moves allow new ghosts t to appear out of thin air, with a fresh identity: true ∃γ. γ : t µ . Once a ghost is created, it can be split apart using * , as follows:</p><formula xml:id="formula_17">γ : t •µ t µ ⇔ γ : t µ * γ : t µ</formula><p>We take γ : t •µ t µ to be false if t • µ t is undefined.</p><p>A very simple but useful kind of permission is a token, which is meant to be owned by exactly one thread at a time. We can model this as a PCM, Tok, with two elements, ε and (the token), with ε • = = • ε. We leave the composition • undefined, so that γ : Tok * γ : Tok ⇒ false. Hence, GPS ensures the token for ghost γ cannot be owned twice. (We use this PCM in several examples in §3.6.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Taking stock: resource ownership vs. knowledge</head><p>We have now seen the full complement of resource ownership assertions (physical and ghost) provided by GPS, with * combining or separating them. Ownership can be divided by the fork rule (Figure <ref type="figure">2</ref>), which allows the parent thread to donate some of its resources to the child thread. But we will also need to transfer ownership between alreadyrunning threads-while ensuring, of course, that claims of ownership are not duplicated in the process. GPS provides two mechanisms for doing so, one physical and the other nonphysical, described in the next two subsections.</p><p>Both mechanisms rely on a fundamental distinction between assertions possibly involving resource ownership (like → v) and assertions only involving knowledge (like t = t ). The key is that, while ownership can come and go, knowledge remains true forever.</p><p>GPS has a modality for knowledge, where P holds if P is true and does not depend on resource ownershipand therefore will remain true forever. These properties of knowledge are captured in two axioms: Knowledge includes assertions that are "pure" in the parlance of separation logic, like equalities on terms, but it also includes protocol observations:</p><formula xml:id="formula_18">t = t ⇒ (t = t ) : s τ ⇒ : s τ</formula><p>On the other hand, assertions about ownership never constitute knowledge: the axiom ( → v) ⇒ false says that it is impossible to treat nonatomic ownership as knowledge.</p><p>Finally, the modality distributes over ∧, ∨, ∀, and ∃.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ownership transfer through protocols</head><p>To explain physically-based ownership transfers, we consider a simple spinlock:</p><formula xml:id="formula_19">newLock() let x = alloc(1) in [x]at := unlocked; x lock(x)</formula><p>repeat CAS(x, unlocked, locked) end unlock(x)</p><p>[x]at := unlocked where unlocked = 0 and locked = 1. We want to reason about this lock in the style of concurrent separation logic <ref type="bibr" target="#b29">[30]</ref>, i.e., we want to be able to prove the following triples:</p><formula xml:id="formula_20">{P } newLock() {x. isLock(x)} {isLock(x)} lock(x) {P } {isLock(x) * P } unlock(x) {true}</formula><p>Here, the assertion P is an arbitrary resource invariant (e.g., claiming ownership of nonatomic locations) protected by the lock, while isLock represents the permission to use the lock. These triples reflect a transfer of ownership of the resources satisfying P , first upon creation of the lock, and then between each successive thread that acquires the lock. But the whole point of the lock is to ensure that when multiple threads race to acquire it, only one will win-and it is the use of CAS that guarantees this, by physical atomicity. We want to leverage the fact that CAS physically arbitrates races to logically arbitrate ownership transfers.</p><p>To do so, we revise our understanding of protocol state interpretations: rather than just a way to communicate knowledge between threads, they are more generally a way to transfer resource ownership between threads. For the spinlock, we can get away with a simple protocol type LP having a single state Inv, where</p><formula xml:id="formula_21">LP(Inv, z) (z = unlocked * P ) ∨ z = locked</formula><p>Intuitively, whenever a thread releases the lock, it must have reestablished the resource invariant P , which it then relinquishes, allowing P to be transferred to the next thread acquiring the lock. We can then define isLock(x)</p><p>x : Inv LP . To initialize an atomic location with state s and value v, a thread must relinquish resources τ (s, v):</p><formula xml:id="formula_22">{uninit( ) * τ (s, v)} [ ]at := v : s τ</formula><p>which is reflected in the triple for newLock() above. The two premises of the rule correspond to the CAS succeeding or failing, respectively. In the successful case, we observe the protocol in some state s , and choose a new state s that is reachable from it. To make the move from s to s , we (1) gain the resources τ (s , V o ), because we won the race to CAS, but (2) must relinquish resources τ (s , V n ), which can be transferred to the next successful CAS on . We can use any resources P we owned beforehand, and we get to keep any leftover resources Q.</p><p>The failure case works like an atomic read, except that we do not learn the exact value observed; we know only that it differs from the expected value V o . Since multiple threads can read from the same write, it should not be possible to gain resources by reading alone-but it should still be possible to gain knowledge. Thus the full read rule is:</p><formula xml:id="formula_23">∀s τ s. ∀z. τ (s , z) * P ⇒ Q : s τ * P [ ]at z. ∃s . : s τ * P * Q</formula><p>This rule differs from the version we gave earlier in two respects. First, the assertion Q is placed under the modality, ensuring that readers only gain knowledge, not resources, through the protocol. Second, the precondition includes an arbitrary assertion P , which we combine via * with the interpretation of the state we are reading.</p><p>The inclusion of the assertion P enables rely-guarantee reasoning through protocols. For the protocol to be in state s , some thread must have written z to while also giving up resources τ (s , z). If we read from this write, we know that the resources involved must be disjoint from any resources P we currently own. We can therefore rule out certain protocol states on this basis. The typical way to do so is through ghosts: we can require that, to move to a certain protocol state s , a thread must give up a ghost t (e.g., a token). Thus, if a thread owns some ghost t such that t•t is undefined, then the thread knows that the protocol cannot be in state s . We illustrate this kind of reasoning in the next subsection.</p><p>Finally, we have a rule for atomic writes:</p><formula xml:id="formula_24">P τ (s , V ) * Q ∀s τ s. τ (s , -) * P ⇒ s τ s : s τ * P [ ]at := V : s τ * Q</formula><p>Writes are surprisingly subtle. Prior to writing, our thread knows some lower bound s on the protocol state. But because the write may be racing with unknown other writes (or CASes), we do not know (or learn!) the "current" state of the protocol. Instead, we must move to a state s that is reachable from any state s τ s that concurrent threads may be moving to. As with reads and CASes, though, we know that any such state s must be satisfiable with resources disjoint from our resources, P . In particular, if τ (s , -) * P ⇒ false, then we do not have to show that s τ s . In summary:</p><p>• Reads relinquish nothing and gain knowledge.</p><p>• Writes relinquish ownership and gain nothing.</p><p>• CASes relinquish and gain ownership when successful, and behave like reads when unsuccessful.</p><p>Returning to the simple spinlock example introduced at the beginning of this subsection, Figure <ref type="figure" target="#fig_5">3</ref> contains the proof outlines for newLock(), lock(x), and unlock(x). The proofs are straightforward and follow immediately from the rules for atomic accesses given in this subsection and the rules of Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ownership transfer through escrows</head><p>We have just shown how GPS axiomatizes ownership transfer for programs that use the explicit, built-in form of synchronization offered by CAS. But in fact programs can and do build up their own implicit mechanisms for ownership transfer without using CAS (which is relatively expensive). We already saw such implicit synchronization at work in the original version of our "message-passing" example from §2: Unlike the version of this example that we showed how to verify in §3.1, this version transfers ownership of a nonatomic location (x → 37) from one thread to another, and it does so without using CAS. Intuitively, the reason this works is that the threads have agreed ahead of time-implicitly-that once y is set to 1, the second thread will have the exclusive permission to take ownership of x. (Indeed, the transfer would be unsound if there were two copies of the second thread operating concurrently.) However, since the second thread does not use CAS, it cannot transfer ownership of x directly out of y's protocol-some additional mechanism is needed.</p><p>Thus we are led to the final concept in GPS: escrows. 5  The idea is that a thread may indirectly transfer a resource to another thread by placing it "under escrow": it is then inaccessible to any thread until some exclusive, logical condition is met, at which point the thread meeting the condition gains ownership of it. GPS is parameterized over a set of escrow types (metavariable σ) and definitions, written σ : P Q. Here Q represents the resource to be placed under escrow, while P represents the transfer condition, which must be exclusive (P * P ⇒ false) to ensure that ownership of Q is only transferred out of the escrow to one receiving thread.</p><p>Escrows are created and used via ghost moves, where the assertion [σ] says that an escrow of type σ is known to exist:</p><formula xml:id="formula_25">σ : P Q Q [σ] σ : P Q P ∧ [σ] Q [σ] ⇒ [σ]</formula><p>The first rule allows Q to be put under escrow; ownership is lost, in exchange for the knowledge [σ]-and because [σ] is knowledge, it can be learned about through reading. When later extracting the resource Q from the escrow [σ], the condition P is consumed; this fact, together with the exclusivity of P , ensures that an escrow can only be used to transfer ownership once.</p><p>Returning to the message-passing example, the idea is to define an escrow type, XE(γ), which governs the transfer of the resource x → 37. The escrow type is parameterized by γ, which is the name of an exclusive ghost token, γ : Tok , that will be used to guard the escrow (i.e., as its transfer condition). The second thread will start out as the (unique) owner of this token, but then exchange it for ownership of x. Formally, we define XE(γ) as follows:</p><formula xml:id="formula_26">XE(γ) : γ : Tok x → 37</formula><p>We then define a single protocol governing y, namely YP(γ), with states 0 and 1 and transition relation ≤, and the following state interpretations:</p><formula xml:id="formula_27">YP(γ)(0, z) z = 0 YP(γ)(1, z) z = 1 * [XE(γ)]</formula><p>This protocol enforces that y progresses from 0 to 1, and when it is set to 1, the escrow XE(γ) must exist. Thus, before the first thread sets y to 1, it must first transfer the resource x → 37 into the escrow XE(γ) so that it can then pass the knowledge of this escrow's existence into the protocol.</p><p>Once the second thread receives this knowledge from the protocol (by reading y as 1), it can trade in its ghost token for ownership of the resource x → 37, as desired. This reasoning is summarized in the proof outline in Figure <ref type="figure" target="#fig_7">4</ref> (omitting the Tok type in the ghost assertions for brevity). 5 As we discuss in Section 5, escrows are closely related to Bugliesi et al.'s notion of "exponential serialization" <ref type="bibr" target="#b6">[7]</ref>.</p><p>x → 0 * y : 0 YP(γ)</p><p>[x]na := 37;</p><p>x → 37 * y : 0 YP(γ)</p><p>[XE(γ)] * y : 0 YP(γ)</p><p>[y]at := 1; A more challenging application of escrows Although the above example succinctly illustrates the basic idea of escrows, it is perhaps not the most compelling one, given that it can be handled by other means in prior logics (such as RSL <ref type="bibr" target="#b37">[38]</ref>). We therefore turn now to an interesting synchronization algorithm (suggested to us by Ernie Cohen), whose GPS verification demonstrates an elegant use of escrows and which, to our knowledge, is beyond the reach of prior logics:</p><formula xml:id="formula_28">y : 1 YP(γ) γ : * y : 0 YP(γ) repeat [y]at end; γ : * y : 1 YP(γ) * [XE(γ)] x → 37 [x]na z. z = 37 * x → 37</formula><formula xml:id="formula_29">[x]at := choose(1, 2); repeat [y]at end; if [x]at == [y]at then /* crit. section */ [y]at := choose(1, 2); repeat [x]at end; if [x]at != [y]at then /* crit. section */</formula><p>The goal of this algorithm is to guarantee mutual exclusion using release-acquire atomics, but without using CAS. The idea is that each thread sets its respective variable (x or y) to either 1 or 2 (using a nondeterministic choice operator, choose) and then checks the value chosen by the other thread. This enables the threads to synchronize implicitly based on a logical condition: the first thread wins if the values pointed to by x and y are equal, and the second wins if they are not. Implicit in the algorithm is the invariant that once each thread sets its variable to 1 or 2, it will not change it further. As a consequence, unlike in Dekker's algorithm ( §2), each thread in Cohen's algorithm relies only on positive information about the progress of the other thread-e.g., has y been set to some nonzero value yet and, if so, what?-in order to determine if it has won the race. Intuitively, it is this restriction to positive reasoning that makes Cohen's algorithm work under release-acquire semantics while Dekker's doesn't.</p><p>We now sketch the verification of Cohen's algorithm (full details are given in the appendix <ref type="bibr" target="#b0">[1]</ref>). Suppose that the winning thread should gain exclusive access to some shared resource P . To verify Cohen's algorithm, our basic idea is to place P under an escrow PE at the beginning (prior to the execution of either thread). The transfer condition for this escrow will be defined so as to be satisfiable only by whichever thread wins the race. Thus, once that thread knows it has won, it can unlock the escrow and gain access to P .</p><p>Formally, at the beginning of the proof, four tokens will be created and passed to the two threads: the first thread (which sets x) will be given the tokens γ x 1 : and γ x 2 : , and the second thread (which sets y) will be given the tokens γ y 1 :</p><p>and γ y 2 : . The γ 1 's will be used to guard access to the aforementioned escrow, and the γ 2 's will be used to guard access to transitions in the protocols governing x and y. Speaking of which: when x and y are initialized to 0, they will be associated with Choice(γ x 2 ) and Choice(γ y 2 ), respectively, where Choice(γ) is the following protocol with states 0, 1, 2:</p><formula xml:id="formula_30">0 1 2 Choice(γ)(s, z) s = z ∧ (s = 0 ∨ γ : Tok )</formula><p>This protocol captures not just the irreversible choices made for x and y, but also control over who can make these choices: only the owner of the ghost token γ x 2 : -i.e., the first thread-will be able to change the state of x and transition from the 0 state of Choice(γ x</p><p>2 ) to the 1 or 2 states, and similarly only the second thread will be able to change the state of y's protocol.</p><p>Finally, we come to the definition of the PE escrow, under which P is placed at the beginning of the proof:</p><formula xml:id="formula_31">PE(γ x 1 , γ y 1 ) : ∃i, j &gt; 0. x : i Choice(γ x 2 ) ∧ y : j Choice(γ y 2 ) ∧   γ x 1 : Tok ∧ i = j ∨ γ y 1 : Tok ∧ i = j   P</formula><p>The transfer condition on this escrow says that, in order to access P , a thread must know that x and y are both in nonzero states-i.e., that both threads have made their irreversible choices-and that either the states are equal and the thread owns γ x 1 , or they are distinct and the thread owns γ y 1 . In either case, the thread that owns the relevant token is the winning thread. Note that the fact that the transfer condition is exclusive, which is necessary in order for it to be a valid transfer condition, follows easily from the combined use of tokens and protocol assertions. In particular, the proof rule shown at the end of §3.2 dictates that if x : i and x : i for i, i &gt; 0, then i = i (and similarly for y, j, j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Soundness</head><p>The main soundness result for GPS is simple to state:</p><formula xml:id="formula_32">Theorem 1 (Soundness). If {true} e {x. P } is provable then e ⊆ {V | P [V /x] = ∅}.</formula><p>The theorem says that Hoare triples proved in GPS accurately predict the final result of a closed program, according to the C11 memory model.</p><p>But to prove this theorem, we must be able to relate the Hoare triples of the logic to C11's event graphs, which do not provide the global notion of "current state" that the semantics of triples usually depend on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of the model and proof</head><p>We structure the semantics of triples into two layers: local safety and global safety.</p><p>Local safety steps through the execution of a single thread, but instead of using an event graph and the C11 axioms to restrict the actions produced by the thread, it essentially replays the rules of GPS. For example, when a thread performs an acquire read, local safety enforces that the location being read is governed by a PL-protocol, and the value read is then constrained by that protocol-exactly mirroring the logic's rule for acquire reads. Thus local safety provides a kind of "rely/guarantee semantics": it checks that an abstract execution of a given thread follows the rules of the logic (i.e., making valid protocol moves) while relying on protocols to predict the outcome of reads (i.e., valid protocols moves made by other threads). When a new thread is forked, local safety is checked independently for the new thread and its parent. Since local safety is just a restatement of GPS's rules in small-step form, it is easy to show that the proof rules of GPS preserve local safety.</p><p>Global safety applies to labeled event graphs, where the labels annotate graph edges with resource and knowledge transfers from the point of view of GPS. By imposing appropriate constraints on the labeling, global safety connects the logical assumptions made in local safety with the physical reality of the event graph.</p><p>The heart of the soundness argument is then to show that if a whole program is locally safe, it is globally safe. We do this by building up the C11 event graph step-by-step (much like the operational semantics), showing for each new event that (1) the existing labeling implies the rely for the event, and (2) the event's guarantee, which we know by local safety, implies that we can extend the labeling to include it.</p><p>Unfortunately, space constraints prevent us from describing the semantic model and proof-both of which are substantial-in full detail here. Below, we sketch a few of the key details. The full semantic model is described in the appendix, and the entire model and soundness proof (as well as an extension of the logic to handle SC accesses) have been formalized and checked in our Coq development <ref type="bibr" target="#b0">[1]</ref>.</p><p>Resources In the semantics of GPS, a resource r is a tuple (Π, g, Σ) containing:</p><p>• A physical location map Π from locations to either a value (for nonatomics) or a protocol and state (for atomics). • A ghost identity map g from ghost names to an element of the corresponding ghost PCM. • A known escrow set Σ containing all escrow types currently in play.</p><p>Resources form a PCM with composition ⊕, and assertions are interpreted as sets of resources, e.g.,</p><formula xml:id="formula_33">r ∈ P 1 * P 2 ∃r 1 , r 2 . r = r 1 ⊕r 2 , r 1 ∈ P 1 , r 2 ∈ P 2</formula><p>The structure of resources and definition of ⊕ are designed to support the axioms on assertions we gave in §3.</p><p>Local safety With resources in hand, we can define a semantic version of ghost moves r P, which says that from resource r it is possible to take a ghost move to resources described by the (semantic) assertion P. We can also define two functions rely, guar : Resource × Action → P(Resource) that describe the rely and guarantee constraints on updating resources, given that we are performing some action α. For example, if α = R( , V, na) and r claims that → V , then rely(r, α) = if V = V then {r} else ∅ which says that the action is only possible if the value it claims to read is the one the logic says the location has. This precisely mirrors the rule for nonatomic reading, and in particular yields no new resources. For atomic locations, the protocol state is allowed to advance, again mirroring the logic's rule for atomic reads.</p><p>We can then define local safety:</p><formula xml:id="formula_34">r pre ∈ LSafe 0 (e, Φ) always r pre ∈ LSafe n+1 (e, Φ) ≈ (simplified; see appendix) If e ∈ Val then r pre Φ(e) If e = K[fork e ] then r pre ∈ LSafe n (K[0], Φ) * LSafe n (e , true) If e α</formula><p>-→ e then ∀r ∈ rely(r pre , α). ∃P. r P and ∀r ∈ P. ∃r post ∈ guar(r , α). r post ∈ LSafe n (e , Φ) which is indexed by the number of steps for which we demand safety. (An expression is "locally safe" if LSafe n holds for all n.) Local safety can be understood as giving weakest preconditions: LSafe n (e, Φ) is the set of starting resources for which e can safely execute for n steps with postcondition Φ (a semantic predicate). We then define |= {P } e {x.Q} ∀n. ∀r ∈ P . r LSafe n (e, x. Q ) Theorem 2 (Local soundness). All of the proof rules given in §3 are sound for this semantics of Hoare triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global safety</head><p>We then define a notion of global safety, written GSafe n (T , G, L), over an instrumented thread pool T , an event graph G, and a labeling L.</p><p>The instrumented thread pool T maps each thread to a tuple (a, e, r, Φ), specifying the last event a that the thread performed in the event graph, the remainder e of its computation, the resources r that it currently holds, and its postcondition Φ. Global safety at n assumes that each thread is locally safe for n more steps, given its resources and postcondition.</p><p>The labeling L annotates hb edges of the graph with resource transfers between the nodes, and is constrained to ensure that each node obeys the corresponding guar condition. Since each atomic write to a location is associated logically with a move in 's protocol, the labeling L also annotates each write event for with information about the corresponding state to which 's protocol was updated.</p><p>The labeling must then globally ensure the following:</p><p>• Compatibility: any set of "concurrent" resource transfers (i.e., roughly, those that are not hb-related) must be composable with one another, ensuring that exclusive resources are never duplicated. • Conformance: if mo(a, b) for two atomic writes/updates to with protocol τ , the protocol states with which a and b are labeled must be related by τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soundness</head><p>The key theorem is a kind of simulation between the expression semantics and global safety:</p><formula xml:id="formula_35">Theorem 3 (Instrumented execution). If GSafe n+1 (T , G, L)</formula><p>and erase(T ); G -→ T ; G then there is some T , L such that erase(T ) = T and GSafe n (T , G , L ).</p><p>Our main soundness result, given at the beginning of the section, is then a corollary connecting the proof theory all the way to the C11 execution (for closed expressions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Case studies</head><p>We have applied GPS to three challenging case studies for weak memory reasoning: Michael and Scott's lock-free queue <ref type="bibr" target="#b27">[28]</ref>, as well as circular buffers <ref type="bibr" target="#b18">[19]</ref> and bounded ticket locks <ref type="bibr" target="#b9">[10]</ref> (both adapted from the Linux kernel). Note that the first two of these exhibit non-SC behavior to their clients (cf. §5). For space reasons, we focus here on the proof for circular buffers, which we describe in some detail. For full details of all three examples, see the appendix <ref type="bibr" target="#b0">[1]</ref>.</p><p>Circular buffers Figure <ref type="figure">6</ref> shows the code for a simplified variant of the circular buffer data structure drawn from the Linux kernel. It is a fixed-size queue, implemented using an array that "wraps around". Specifically, the queue pointed to by q consists of an N -cell array (at q + b), together with a reader index (at q + ri) specifying the array offset of the next item to be consumed, and a writer index (at q + wi) specifying the array offset of the next item to be produced. The "active" part of the queue consists of the array elements starting at the reader index and ending at the one prior to the writer index, wrapping around modulo N . Hence, if the two indices are equal, then the buffer is empty, and if the writer index is one before the reader index (modulo N ), then the buffer is full (with N -1 elements).</p><p>The tryProd and tryCons operations first check the two indices to see whether the buffer is full or empty, respectively. If so, they return 0. Otherwise, they proceed by writing/reading the element at the writer/reader index and then incrementing that index (modulo N ). Since accesses to the actual data in the buffer are completely synchronized, the cells comprising the array itself can be read and written nonatomically. All synchronization is performed through the reader/writer indices. Note, however, that (as in Cohen's example from §3.6) this synchronization is entirely implicit: the algorithm uses plain writes, not CAS, to increment the indices. While this is an efficiency win (e.g., on x86, the algorithm requires no fences), it means that only one producer and one consumer can operate simultaneously.</p><p>The specification We will prove the following spec: {true} newBuffer() {q. Prod(q) * Cons(q)} {Prod(q) * P (x)} tryProd(q, x) {z. Prod(q) * (z = 0 ∨ P (x))} {Cons(q)} tryCons(q) {x. Cons(q) * (x = 0 ∨ P (x))}</p><p>The spec is parameterized over a predicate P that should hold of all the elements in the buffer; it guarantees that P (x) all (N, N, N, N)</p><formula xml:id="formula_36">restP(i) ((&gt; i), (≥ i), ∅, ∅) restC(i) (∅, ∅, (&gt; i), (≥ i)) protP(i) ({i}, ∅, ∅, ∅) escP(i) (∅, {i}, ∅, ∅) protC(i) (∅, ∅, {i}, ∅) escC(i) (∅, ∅, ∅, {i})</formula><p>Prod(q) ∃γ, i, j. i &lt; j + N * q + wi : i PP(γ, q) * q + ri : j CP(γ, q) * γ : restP(i) Cons(q) ∃γ, i, j. j ≤ i * q + wi : i PP(γ, q) * q + ri : j CP(γ, q) * γ : restC(j)</p><formula xml:id="formula_37">PP(γ, q)(i, x) γ : protP(i) ∧ x = i mod N ∧ ∀j &lt; i. [CE(γ, q, j)] CP(γ, q)(j, x) γ : protC(j) ∧ x = j mod N ∧ ∀i &lt; j + N. [PE(γ, q, i)] PE(γ, q, i) : γ : escP(i) uninit(q + b + (i mod N )) ∨ (q + b + (i mod N )) → - CE(γ, q, j) : γ : escC(j) ∃x. P (x) * (q + b + (j mod N )) → x Figure 5</formula><p>. Technical setup for the circular buffer case study holds of all elements x that the consumer consumes so long as it holds of all elements x that the producer produces. This predicate can thus be used in typical separation-logic style to transfer ownership of data structures from producer to consumer. <ref type="foot" target="#foot_4">6</ref> The spec also employs two predicates Prod(q) and Cons(q), which describe the privilege of acting as producer or consumer, respectively. These predicates are exclusive resources, ensuring that there can only be one call to tryProd and one call to tryCons running concurrently. Their definitions (in Figure <ref type="figure">5</ref>) are described below.</p><p>Note that this spec is rather weak because it does not enforce that the buffer actually implements a queue. This is merely for simplicity-it is easy to generalize our proof to handle a stronger spec, e.g., in which P , Prod, and Cons are allowed to keep track of the entire sequence of elements produced thus far.</p><p>High-level picture Our proof of the above spec (Figure <ref type="figure">6</ref>) depends on all the features of GPS working in concert. Figure <ref type="figure">5</ref> shows the technical setup for the proof.</p><p>First, we use protocols PP and CP to govern the states of the writer and reader indices, respectively. The state of each of these protocols tracks the "absolute state" of the corresponding index, meaning the total number of writes/reads that have ever occurred, which can only increase over time (the state ordering is ≤). The state interpretation of PP/CP then dictates that the "physical state" of the writer/reader index equal the absolute state modulo N .</p><p>Second, since the buffer does not use CAS, it is not possible to use the PP and CP protocols to directly transfer ownership of the cells in the buffer between the producer and consumer. Fortunately, we can indirectly exchange ownership of the buffer cells instead, by (a) placing the cells under escrows, and (b) using PP and CP as a conduit for the knowledge that these escrows, once created, exist. Specifically, after filling a buffer cell with a new element, the producer will pass control of the cell to the consumer via the CE escrow (see Step 10 in the proof of tryProd); upon consumption, the consumer will pass control of the cell back to the producer via the PE escrow. The state interpretations of PP and CP offer a way to communicate awareness of these escrows back and forth.</p><p>Third, we use ghost tokens in a manner similar to the proof of Cohen's example from the previous section. The protP(i) and protC(i) tokens are needed in order to transition to (absolute) state i of the PP and CP protocols, respectively, while the escP and escC tokens are used as transfer conditions for the PE and CE escrows. In both cases, the producer and consumer each start out with all the tokens they will ever need (i.e., restP(0) and restC(0)) as part of their exclusive resource predicates Prod(q) and Cons(q), and they proceed to "spend" one protocol token and one escrow token upon each call to tryProd/tryCons. All these tokens are defined in Figure <ref type="figure">5</ref> as elements of the ghost PCM P(N) × P(N) × P(N) × P(N) (with composition defined as componentwise ).</p><p>Finally, tying everything together, Prod(q) and Cons(q) assert bounded knowledge about the states of the PP and CP protocols, thus enforcing the fundamental invariant of circular buffers:</p><p>The absolute writer index is at least 0 and less than N cells ahead of the absolute reader index. Now, the reader (of this paper, not the buffer) may rightly wonder: how can this fundamental invariant possibly be enforced in the weak memory setting, given that it concerns the states of two separate cells being updated by different threads? The answer is that, although neither the producer nor the consumer can fully assume or maintain this invariant themselves, they are each able to enforce a piece of it sufficient to verify their own correctness. In particular, the consumer controls the progress of the reader index, and can therefore assume and maintain the invariant that the reader index never overtakes the writer index (the "at least 0" part), while the producer controls the progress of the writer index, and can therefore assume and maintain the invariant that the writer index never leaves the reader index more than N -1 cells behind (the "less than N " part). Together, these piecemeal enforcements of the fundamental invariant are enough to perform the full verification.</p><p>Proof outline for tryProd Figure <ref type="figure">6</ref> displays the proof outline for tryProd(q, x). (The proof for tryCons is almost dual, and the proof for newBuffer is comparatively simple; see the appendix.) We explain here some of the most important steps in the proof. Throughout, note that assertions under wi 0, ri 1, b 2 newBuffer() let q = alloc(N +2) [q + ri]at := 0; [q + wi]at := 0; q tryProd(q, x)</p><formula xml:id="formula_38">let w = [q + wi]at let r = [q + ri]at let w = w + 1 mod N if w == r then 0 else [q + b + w]na := x; [q + wi]at := w ; 1 tryCons(q) let w = [q + wi]at let r = [q + ri]at let r = r + 1 mod N if w == r then 0 else let x = [q + b + r]na [q + ri]at := r ; x</formula><p>Proof outline for tryProd(q, x):</p><p>Prod(q) * P (x)</p><p>(1) γ : restP(i) * P (x) * i &lt; j0 + N ∧ q + wi : i PP(γ, q) ∧ q + ri : j0 CP(γ, q) let w = [q + wi]at (2) γ : restP(i) * P (x) * (w = i mod N ∧ ∀k &lt; i. [CE(γ, q, k)]) let r = [q + ri]at <ref type="bibr" target="#b2">(3)</ref> γ : restP(i) * P (x) * r = j mod N ∧ q + ri : j CP(γ, q) ∧ j0 ≤ j ∧ ∀k &lt; j + N. [PE(γ, q, k)]</p><p>(4) γ : restP(i) * P (x) * (i &lt; j + N ∧ [PE(γ, q, i)])</p><formula xml:id="formula_39">let w = w + 1 mod N γ : restP(i) * P (x) * (w = w + 1 mod N ) (5) if w == r then γ : restP(i) * P (x) 0 z. Prod(q) * z = 0 * P (x) else γ : restP(i) * P (x) * (w = r) (6) γ : restP(i) * P (x) * (i + 1 &lt; j + N )<label>(7)</label></formula><p>γ : restP(i + 1) * γ : protP(i + 1) * P (x) * γ : escP(i)</p><p>γ : restP(i + 1) * γ : protP(i + 1) * P (x) * (uninit(q + b + w) ∨ (q + b + w) → -)</p><p>[q + b + w]na := x;</p><p>(9) γ : restP(i + 1) * γ : protP(i + 1) * P (x) * (q + b + w) → x (10) γ : restP(i + 1) * γ : protP(i + 1) * [CE(γ, q, i)]</p><p>[q + wi]at := w ;</p><p>(11) γ : restP(i + 1) * q + wi : i + 1 PP(γ, q)</p><formula xml:id="formula_41">1 (12)</formula><p>z. Prod(q) * z = 1 Figure <ref type="figure">6</ref>. Proof excerpt for the circular buffer case study are only written once and then used freely in the rest of the proof since they hold true forever after.</p><p>Step 1: By unfolding Prod(q), we gain access to our piece of the fundamental invariant, namely that the absolute writer index i is less than N past the absolute reader index, which is at least j 0 .</p><p>Step 2: The reason we know exactly what i is-but merely have a lower bound on j 0 -is that we own the protocol tokens protP(k) for all k &gt; i, constraining the possible "rely" moves that other threads can make in the PP protocol. In this step, we exploit that knowledge to assert that the value w we read is exactly i mod N .</p><p>Step 3: Here we read the current reader index r, whose absolute state j must be at least j 0 (as mentioned already). From the read of protocol CP at state j, we also gain knowledge of the escrows PE(γ, q, k) for all k &lt; j + N .</p><p>Step 4: Since i &lt; j 0 + N ≤ j + N , the escrows we just learned about in the previous step include PE(γ, q, i), which we need later.</p><p>Step 5: If the buffer is full, i.e., r = (w + 1) mod N , then the operation is a no-op and we simply return P (x) back to the caller.</p><p>Step 6: Otherwise, r = (w + 1) mod N . We know from Step 4 that i &lt; j + N , and we want to show i + 1 &lt; j + N because this is the piece of the fundamental invariant that we are responsible for maintaining when we bump up the writer index at the end of the operation (Step 12). To prove this, we must establish i + 1 = j + N . So suppose the opposite is true: i + 1 = j + N . Then, since w = i mod N , we obtain (w + 1) mod N = (i + 1) mod N = (j + N ) mod N = j mod N = r. Contradiction.</p><p>Step 7: From our stash of tokens (restP(i)), we peel off a protocol token (protP(i + 1)) for advancing to the (i + 1)-th state of the PP protocol, and an escrow token (escP(i)) for accessing the escrow PE(γ, q, i) that we learned about in Step 4.</p><p>Step 8: We access the escrow, thereby gaining ownership of the buffer cell at index w.</p><p>Step 9: We non-atomically write x to the buffer cell.</p><p>Step 10: We pass control of the buffer cell back to the consumer by placing it under the consumer escrow CE(γ, q, i).</p><p>Step 11: We advance the absolute writer index (i.e., the state of the PP protocol) to i+1, which we can do because (a) we own the token protP(i + 1), and (b) we have knowledge of CE(γ, q, i).</p><p>Step 12: Thanks to Step 6, we have preserved the "less than N" part of the fundamental invariant, as demanded by Prod(q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work 5.1 From SC reasoning to weak memory reasoning</head><p>As explained in the introduction, the various logical mechanisms employed by GPS are not fundamentally new: they are all either descendants or restrictions of mechanisms proposed in prior logics for strong (SC) concurrency.</p><p>First and foremost, many recent SC logics support some form of protocol for describing fine-grained invariants on shared state; GPS's per-location (PL) protocols are inspired most directly by the protocols of CaReSL <ref type="bibr" target="#b36">[37]</ref>. CaReSL's protocols take the form of state transition systems (STSs) wherein each STS state is associated with an invariant about some underlying shared state. The primary difference between GPS's protocols and CaReSL's protocols is that CaReSL's protocols are not restricted to governing the contents of a single location: they may govern arbitrary heap regions, and this additional flexibility renders them suitable for verifying programs that assume sequential consistency.</p><p>For instance, the CaReSL protocol for verifying Dekker's algorithm ( §2) would look something like this:</p><formula xml:id="formula_42">0, 0 1, 0 0, 1 1, 1</formula><p>Here, each protocol state governs the contents of x and y simultaneously. In the (1, 0) state the first thread has won the race; in the (0, 1) state the second thread has won the race; and in the (1, 1) state the race is over (and it is impossible to tell who won). The verification of Dekker's algorithm just has to ensure that (a) each thread only makes state changes according to the protocol, which is easy since updating x or y from 0 to 1 is always legal according to the protocol, and (b) each thread only accesses the shared resource once it has observed the protocol being in its respective winning state.</p><p>In the weak memory setting, the kind of simultaneous invariant represented by the above protocol, relating the "current" states of x and y, is unsound because the updates to x and y may appear in different orders to the first and second threads. It is a key original insight in the design of GPS that the soundness of CaReSL-style protocols for weak memory can in fact be regained if we simply restrict them to governing a single location at a time.</p><p>GPS's support for ghost state is also inspired by CaReSL, but the mechanisms are somewhat different. CaReSL supports ghost state through "tokens", which are coupled with its protocol mechanism, whereas in GPS ghost state is handled separately via ghost PCMs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. (In this paper, we have only made use of simple token-like ghost PCMs, but the "bounded ticket lock" example, shown in the appendix <ref type="bibr" target="#b0">[1]</ref>, employs a much more interesting PCM.) GPS's separation of orthogonal mechanisms has the side benefit of removing CaReSL's "token purity" restriction-e.g., in the circular buffer example from Section 4, we did not require any side condition on the per-item predicate P (x), whereas an analogous proof in CaReSL would have required that P (x) be a "token-pure" (i.e., duplicable) assertion. GPS's escrows, P Q, can be viewed as yet another kind of CaReSL-style protocol, restricted in a different way than PL-protocols are. Escrows are essentially protocols with two states: before and after the resource being held in escrow, Q, has been exchanged for the escrow condition, P . Escrows are sound in the weak memory setting because the only thread that can observe anything at all about the protocol is the thread that exchanges P for Q. Since that thread owns P , and P is exclusive, it can deduce that the escrow is in the before state, and therefore safely transition to the after state, without any concern about the observations of other threads.</p><p>Although in the context of concurrency logics the escrow mechanism is unusual, there is some precedent for it: escrows are very similar to "exponential serialization", a mechanism proposed by Bugliesi et al. <ref type="bibr" target="#b6">[7]</ref> as part of an affine type system for verifying cryptographic protocols. Bugliesi et al. employ this mechanism for much the same reasons we do-namely, as a way of indirectly transferring control of an exclusive resource from one thread to another across a duplicable, "knowledge-only" channel. However, in their case the channel takes the form of a cryptographic signing key, whereas for us it is a shared memory location. Logically, the main difference between escrows and exponential serialization is that the precondition of escrow creation-i.e., that the escrow transfer condition P is exclusive (P * P ⇒ false)-is something we can prove easily within the logic of GPS. In contrast, since the primitive affine predicates of Bugliesi et al.'s type system have no underlying semantic interpretation, they can only ensure the analogous exclusiveness condition via a complex and syntactic "guardedness" check on typing contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relaxed Separation Logic (RSL)</head><p>The closest related work to GPS is the recent Relaxed Separation Logic (RSL) introduced by Vafeiadis and Narayan <ref type="bibr" target="#b37">[38]</ref>, which is the only prior program logic for the C11 memory model. The goal of RSL is to support simple reasoning about release-acquire accesses in the style of Concurrent Separation Logic (CSL) <ref type="bibr" target="#b29">[30]</ref>. Unlike in GPS, it is possible in RSL for a release write to directly transfer resource ownership to an acquire read (e.g., in verifying the nonatomic message-passing example, for which GPS required escrows). To manage such transfers, RSL employs release/acquire permissions describing the resources to be transferred upon a write to a given location. The choice of resources depends solely on the value being written, and so any given value can only be used to perform a transfer once per location.</p><p>GPS draws much inspiration from RSL, particularly in its proof of soundness, whose structure is based closely on RSL's. There are many significant differences, however. Most importantly, GPS offers a much more flexible way of coordinating ownership and knowledge transfers between threads-including rely-guarantee reasoning-through its protocols, ghosts, and escrows. These mechanisms refactor and generalize the permission-based reasoning of RSL, thus allowing us to lift several of RSL's restrictions, including the one on repeated writes of the same value. Lifting this restriction is crucial for handling the indices in the circular buffer, and the ticket numbers in the bounded ticket lock, as these are cases where the same value is "recycled" (i.e., written to a location multiple times, and each time used to perform a different resource transfer). To our knowledge, none of our case studies can be verified in RSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Alternative approaches</head><p>Most existing approaches to reasoning about weak memory rely in some way on recovering strong memory assumptions, either by imposing a synchronization discipline or by reasoning directly about low-level hardware details.</p><p>Recovering SC by synchronization discipline Most memory models satisfy the so-called fundamental property <ref type="bibr" target="#b33">[34]</ref>: they guarantee sequential consistency for "sufficiently synchronized" code. (Synchronization operations like memory fences effectively thwart compiler and CPU optimizations.) Thus, if one can use a concurrency logic or some other means to enforce a strong synchronization discipline, one can recover strong memory reasoning for programs that follow that discipline. Instances of this approach include:</p><p>• Owens <ref type="bibr" target="#b30">[31]</ref> proves that data-race free and "triangular-race" free programs on x86-TSO have SC behavior. • Batty et al. <ref type="bibr" target="#b3">[4]</ref> prove that for C11 restricted to nonatomics and SC-atomics, data-race freedom ensures SC behavior. • Cohen and Schirmer <ref type="bibr" target="#b7">[8]</ref> prove that programs following a certain ownership discipline and flushing write buffers at certain times on TSO models have SC behavior. • Ferreira et al. <ref type="bibr" target="#b16">[17]</ref> prove that concurrent separation logic is sound for a class of weak memory models satisfying a data-race freedom guarantee.</p><p>All of these disciplines force programs to use enough synchronization to keep weak memory behavior unobservable. We view them as complementary to GPS: they delimit an important subset of programs for which SC reasoning is sound within a weak memory model. Ultimately, our goal is to derive such disciplines within a more general weak memory program logic like GPS. Our treatment of locks in §3 already does this for the simple case of recovering CSL-style reasoning within weak memory: our lock spec provides the key concurrency rules for CSL as a derived set of rules in GPS.</p><p>We believe the extra generality of GPS is important because it enables us to verify a wider class of weak memory programs, including those whose observable behavior is not SC. The circular buffer and Michael-Scott queue are good examples of this (see the appendix <ref type="bibr" target="#b0">[1]</ref>). Singh et al. <ref type="bibr" target="#b34">[35]</ref> argue that one should not expose the high-level programmer to such non-SC data structures, but GPS shows that in fact it is possible to reason sensibly and modularly about them.</p><p>Recovering SC through low-level reasoning Another way of recovering strong memory is to explicitly model low-level hardware details (e.g., per-processor write buffers) within one's logic <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>, or to transform the program being verified so that interactions with write buffers, for instance, are made manifest in its code <ref type="bibr" target="#b1">[2]</ref>. While this type of approach can accommodate arbitrary programs and enable the reuse of existing SC techniques, it provides little abstraction or modularity: users of such an approach must reason directly with the low-level hardware details, with relatively little help given in structuring this reasoning.</p><p>Ridge <ref type="bibr" target="#b32">[33]</ref> provides a program logic for x86-TSO that supports rely-guarantee reasoning. The logic works directly with the operational x86-TSO model <ref type="bibr" target="#b31">[32]</ref>, and includes assertions about both program counters and write buffers. Rely constraints must be stable under the (nondeterministic) flushing of write buffers.</p><p>Wehrman and Berdine <ref type="bibr" target="#b39">[40]</ref> propose a separation logic for x86-TSO which directly models store buffers and provides both temporal and spatial separating conjunctions, as well as resource invariants in the style of CSL. Unfortunately, the logic as proposed has some (known) soundness gaps, and to our knowledge a sound version has not yet been developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future work</head><p>While GPS makes a significant step forward in reasoning about release/acquire semantics, there is much work left to do to develop a full understanding of the C11 memory model.</p><p>Interaction with SC In our Coq development, we show that the reasoning principles for release-acquire atomics apply to SC atomics as well. In addition, we believe that if each memory location were uniquely used in conjunction with one access mode (e.g., always release-acquire or always SC), then it would be straightforward to supplement GPS with completely separate (and stronger) reasoning principles for SC atomics, along the lines of prior SC logics. However, the C11 model allows programmers to freely mix memory orderings, and ideally program logics should support such mixed reasoning as well. Early investigation suggests that the C11 model has some corner cases when mixing memory orderings that may obstruct compositional reasoning principles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consume reads</head><p>The C11 memory model supports a weaker mode for reads, called consume reads, under which happensbefore relationships are only introduced for subsequent actions that depend on the value that was read. Such consume reads are used crucially, for example, in the implementation of read-copy-update (RCU) synchronization in the Linux kernel <ref type="bibr" target="#b26">[27]</ref>. We believe it should be possible to extend GPS with support for consume reads, and that reasoning about compositionally about them will likely require the introduction of a modality encapsulating possible data dependencies.</p><p>Relaxed operations Finally, C11 offers fully relaxed memory orderings, which induce no happens-before relationships.</p><p>If both relaxed reads and writes are allowed, the formal C11 model permits causal cycles: an execution can produce a value "out of thin air" through a cycle of relaxed read and write operations <ref type="bibr" target="#b4">[5]</ref>. As noted in the RSL paper <ref type="bibr" target="#b37">[38]</ref>, these cycles inhibit even very basic forms of logical reasoning, including single-location invariants, and they also inhibit program analyses that are routinely used for optimization. (RSL includes rules for reasoning about relaxed accesses, but only under a severely restricted version of the C11 memory model.) We therefore believe that C11 should be revised to rule out these and other causal cycles, which will enable us to find sound reasoning principles for relaxed operations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>isUpd(c), rf(c) = a =⇒ mo(a, c), b. mo(a, b), mo(b, c) But the heavy lifting of the C11 model is done by a final axiom, called coherence, which connects mo, rf, and hb: hb(a, b) =⇒ ¬mo(b, a), ¬mo(rf(b), a), ¬mo(rf(b), rf(a)), ¬mo(b, rf(a))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>we have sw(c, d) and thus hb(c, d). By transitivity of hb, we have hb(b, d) and hence hb(b, e). • Coherence then says that ¬mo(rf(e), b), i.e., that e cannot read from any write earlier (in mo) than b; in particular, e cannot read from a. It must read from b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>y : 1</head><label>1</label><figDesc>Flg(x) ∧ x : 37 Dat [x]at z. y : 1 Flg(x) ∧ x : 37 Dat ∧ z = 37</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>P ⇒ P P ⇔ P * P Using the second axiom and the frame rule, we can derive: {P * R} e {x. Q} {P * R} e {x. Q * R} Knowledge is retained no matter what an expression does.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>PFigure 3 .</head><label>3</label><figDesc>Figure 3. Proof outlines for the simple spinlock implementation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>[x]na := 37;[y]at := 1;repeat [y]at end; [x]na</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Proof outline for nonatomic message-passing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>{P } e {x. Q} {P * R} e {x. Q * R} {Q} e {true} {P * Q} fork e {P } {P } e {x. Q} ∀x. {Q} e {y. R} {P } let x = e in e {y. R}</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">P ⇒ Q</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>P</cell><cell>Q</cell></row><row><cell cols="3">{P } e {x. (x = 0 ∧ P ) ∨ (x = 0 ∧ Q)}</cell><cell></cell><cell>P</cell><cell>Q</cell></row><row><cell cols="3">{P } repeat e end {x. Q}</cell><cell cols="2">P  *  R</cell><cell>Q  *  R</cell></row><row><cell>P</cell><cell>P</cell><cell>{P } e {x. Q}</cell><cell>∀x. Q</cell><cell>Q</cell></row><row><cell></cell><cell></cell><cell>P e x. Q</cell><cell></cell><cell></cell></row><row><cell cols="6">Figure 2. A selection of basic logical rules for GPS</cell></row><row><cell cols="6">just lower bounds, they are invariant under interference from</cell></row><row><cell cols="6">other threads. Nonatomic locations, on the other hand, must</cell></row><row><cell cols="6">be treated as resources to ensure that only one thread can</cell></row><row><cell cols="6">write to them at a time, in order to avoid data races. GPS thus</cell></row><row><cell cols="3">includes the assertions</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We are using here the program logic notation for pointer dereferencing, [-], which avoids ambiguity with the * of separation logic.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The reads and writes functions extract the locations and values from normal read/writes as well as atomic updates.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that the writes to x here must be atomic to avoid data races.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>This rule is sound only for the assertions we have introduced so far; the general rule is given in "Ownership transfer through protocols", below.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>In the case that the buffer is full, i.e., return value z = 0, the tryProd operation simply returns ownership of P (x) to the caller.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by the EC FP7 FET project ADVENT. We would also like to thank Xiao Jia, Ralf Jung, and Joe Tassarotti for helpful comments and corrections.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://plv.mpi-sws.org/gps/" />
		<title level="m">Appendix and Coq development for this paper available at the following URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Software verification for weak memory via program transformation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alglave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kroening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nimal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tautschnig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESOP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mathematizing C++ concurrency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Batty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clarifying and compiling C/C++ concurrency: From C++11 to POWER</title>
		<author>
			<persName><forename type="first">M</forename><surname>Batty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Memarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sewell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Library abstraction for C/C++ concurrency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Batty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gotsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Foundations of the C++ concurrency memory model</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Boehm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Logical foundations of secure resource management in protocol implementations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bugliesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calzavara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eigner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maffei</surname></persName>
		</author>
		<editor>POST</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From total store order to sequential consistency: A practical reduction theorem</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schirmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITP</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">VCC: A practical system for verifying concurrent C</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dahlweid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hillebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leinenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moskal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Santen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tobies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPHOLs</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ticket spinlocks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="http://lwn.net/Articles/267968/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TaDA: A logic for time and data abstraction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Da Rocha Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dinsdale-Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECOOP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cooperating Sequential Processes</title>
		<imprint>
			<date type="published" when="1965">1965</date>
			<biblScope unit="volume">123</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Concurrent abstract predicates</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dinsdale-Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vafeiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECOOP 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6183</biblScope>
			<biblScope unit="page" from="504" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Views: Compositional reasoning for concurrent programs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dinsdale-Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Birkedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Parkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denyguarantee reasoning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vafeiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESOP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local rely-guarantee reasoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parameterized memory models and concurrent separation logic</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESOP</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The essence of compiling with continuations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Flanagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Duba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felleisen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Howells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Mckenney</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/circular-buffers.txt" />
		<title level="m">Circular buffers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m">ISO/IEC 14882:2011. Programming language C++</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m">ISO/IEC 9899:2011. Programming language C</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fictional separation logic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Birkedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESOP</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tentative steps toward a development method for interfering programs</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOPLAS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="596" to="619" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Verification of concurrent programs with Chalice</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R M</forename><surname>Leino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Security Analysis and Design V</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5705</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Subjective auxiliary state for coarse-grained concurrency</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ley-Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nanevski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Java memory model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Manson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Exploiting deferred destruction: an analysis of read-copy-update techniques in operating system kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mckenney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Oregon Graduate Institute</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonblocking algorithms and preemption-safe locking on multiprogrammed shared memory multiprocessors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JPDC</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Communicating state transition systems for fine-grained concurrent resources</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nanevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ley-Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Delbianco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESOP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Resources, concurrency, and local reasoning</title>
		<author>
			<persName><forename type="first">P</forename><surname>O'hearn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="271" to="307" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reasoning about the implementation of concurrency abstractions on x86-TSO</title>
		<author>
			<persName><forename type="first">S</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECOOP</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A better x86 memory model: x86-TSO</title>
		<author>
			<persName><forename type="first">S</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sewell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPHOLs</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A rely-guarantee proof system for x86-TSO</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VSTTE</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A theory of memory models</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Saraswat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jagadeesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Von Praun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end sequential consistency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Millstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musuvathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Impredicative concurrent abstract predicates</title>
		<author>
			<persName><forename type="first">K</forename><surname>Svendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Birkedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESOP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unifying refinement and Hoare-style reasoning in a logic for higher-order concurrency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Turon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Birkedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICFP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Relaxed separation logic: A program logic for C11 concurrency</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vafeiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Narayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A marriage of rely/guarantee and separation logic</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vafeiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONCUR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A proposal for weak-memory local reasoning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Wehrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berdine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LOLA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
