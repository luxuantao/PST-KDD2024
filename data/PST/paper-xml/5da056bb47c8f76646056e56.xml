<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auto-weighted multi-view clustering via deep matrix decomposition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-28">28 August 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shudong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">SMILE Lab</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">SMILE Lab</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
							<email>zlxu@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">SMILE Lab</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Auto-weighted multi-view clustering via deep matrix decomposition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-28">28 August 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">3346906CD6CCB22D850D0C1A4D81B3E2</idno>
					<idno type="DOI">10.1016/j.patcog.2019.107015</idno>
					<note type="submission">Received 16 February 2019 Revised 8 August 2019 Accepted 17 August 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multi-view learning Deep matrix decomposition Clustering Optimization algorithm</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real data are often collected from multiple channels or comprised of different representations (i.e., views). Multi-view learning provides an elegant way to analyze the multi-view data for low-dimensional representation. In recent years, several multi-view learning methods have been designed and successfully applied in various tasks. However, existing multi-view learning methods usually work in a single layer formulation. Since the mapping between the obtained representation and the original data contains rather complex hierarchical information with implicit lower-level hidden attributes, it is desirable to fully explore the hidden structures hierarchically. In this paper, a novel deep multi-view clustering model is proposed by uncovering the hierarchical semantics of the input data in a layer-wise way. By utilizing a novel collaborative deep matrix decomposition framework, the hidden representations are learned with respect to different attributes. The proposed model is able to collaboratively learn the hierarchical semantics obtained by each layer. The instances from the same class are forced to be closer layer by layer in the low-dimensional space, which is beneficial for the subsequent clustering task. Furthermore, an idea weight is automatically assigned to each view without introducing extra hyperparameter as previous methods do. To solve the optimization problem of our model, an efficient iterative updating algorithm is proposed and its convergence is also guaranteed theoretically. Our empirical study on multi-view clustering task shows encouraging results of our model in comparison to the state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many real-world applications, datasets have heterogeneous features which are collected from multiple channels or with multiple modalities. For instance, images can be introduced by different visual descriptors like LBP <ref type="bibr" target="#b0">[1]</ref> , HOG <ref type="bibr" target="#b1">[2]</ref> and SIFT <ref type="bibr" target="#b2">[3]</ref> ; a video can be represented by different types (views) of features such as images and sounds <ref type="bibr" target="#b3">[4]</ref> . Different views usually capture different aspects of information, any of which suffices for mining knowledge. Furthermore, the encoded information of different views is consistent and complementary to each other, which is instrumental to produce better performance. Thus it is expected to exploit multiple views in depth to generate more promising results rather than relying on a single view. Therefore, it is critical to develop a new learning paradigm, i.e., multi-view learning, to efficiently analyze these heterogeneous features such that the accuracy and robustness of learning algorithms can be improved. Previous works have also shown that the hidden common structure share by different views can be fully extracted <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> .</p><p>In recent years, a number of multi-view learning models have been proposed under the framework of different theories and methodologies <ref type="bibr" target="#b6">[7]</ref> . Moreover, these methods have been widely applied in supervised and unsupervised framework (e.g., multi-view clustering) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> . The key point of utilizing the multiple views to deal with the multi-view learning task is to reasonably fuses these views to produce a more accurate and robust result <ref type="bibr" target="#b9">[10]</ref> . Existing multi-view learning methods can be roughly categorized into graph-based approaches and subspace approaches. The former considers the multi-view learning problem as a graph partition problem by transforming the multi-view learning process into a multiple graph fusion situation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> . While the latter tries to uncover the common latent subspace shared by multiple views <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> . It is worth noting that graph-based multi-view methods are generally studied by extending the traditional spectral methods, while subspace multi-view methods are designed by utilizing the matrix decomposition (also called matrix factorization (MF)). Since graph-based methods suffer from the problem of time-consuming due to the graph construction as well as the eigen decomposition, more and more researchers focus on applying the MF strategy to solve the multi-view learning problems <ref type="bibr" target="#b13">[14]</ref> . There are several MF-based multi-view works based on information theoretic <ref type="bibr" target="#b14">[15]</ref> , which essentially can be treated as a special kind of subspace methods. Some multi-view learning methods are also in- vestigated based on low-rank learning <ref type="bibr" target="#b13">[14]</ref> , canonical correlation analysis <ref type="bibr" target="#b15">[16]</ref> , latent subspace learning <ref type="bibr" target="#b16">[17]</ref> , and so on. Multi-view learning has been successfully applied in face recognition <ref type="bibr" target="#b17">[18]</ref> , image classification <ref type="bibr" target="#b18">[19]</ref> , text mining <ref type="bibr" target="#b19">[20]</ref> , etc .</p><p>As mentioned before, MF-based multi-view learning models have been widely studied in recent years and were reported with good performance. However, these models usually work in a single layer formulation, in which the mapping between the obtained representation and the original data contains rather complex hierarchical information with implicit lower-level hidden attributes. In addition, determining on the weights of different views of data is a crucial challenge to a number of multi-view clustering algorithms, since different views can play different roles to clustering. In this paper, a novel deep multi-view learning model is proposed by uncovering the hierarchical semantics of the input data in a layer-wise way, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> . As illustrated, by utilizing a novel deep matrix factorization framework, the hidden representations are learned with respect to different attributes. Taking the human face dataset as an example, the variability of data not only stems from the difference in the appearance of the subjects, but also the other attributes such as the facial expression or the pose of the subject. Since such attributes can help identify the faces, the face recognition problem can be better solved using our deep MF framework, as each subsequent layers can capture the corresponding hierarchical structures. As a result, the instances from the same class but from different views are forced to be closer layer by layer in the low-dimensional space, which is beneficial for the subsequent learning task. In addition, in order to automatically determine the weights of different views, we introduce the autoweighting scheme into the deep multi-view clustering algorithm. Furthermore, to solve the optimization problem of our model, an efficient iterative updating algorithm is proposed with a theoretical guarantee of its convergence.</p><p>The rest of the paper is organized as follows. We present a brief review of previous related research in Section 2 . We give a detailed description of our model in Section 3 . Experimental results are presented in Section 4 . The paper ends with a conclusion in Section 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Before presenting the details of our method, we first briefly review previous research closely related to this paper.</p><p>Matrix decomposition techniques have been widely applied in data mining and pattern recognition. Among them, Nonnegative Matrix Factorization (NMF) has received much attention due to its physiological and psychological interpretation. In this section, we will briefly review NMF <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> . Given a nonnegative data matrix</p><formula xml:id="formula_0">X = [x 1 , x 2 , • • • , x n ] ∈ R f ×n ,</formula><p>where n is the number of data points and f is the data dimension. NMF aims to find two nonnegative matrices U ∈ R f ×C and V ∈ R n ×C which minimize the objective function as follows</p><formula xml:id="formula_1">J NMF = n i =1 m j=1 X i j -( UV T ) i j 2 = X -UV T 2 F s . t . U ≥ 0 , V ≥ 0 , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where • F denotes the Frobenius norm and X ij denotes the ( i, j )th element of X . It has been proved that the objective function J NMF in Eq. ( <ref type="formula" target="#formula_1">1</ref>) is convex in U only or V only. To optimize the objective function, Lee et al. <ref type="bibr" target="#b20">[21]</ref> proposed the following iterative updating algorithm</p><formula xml:id="formula_3">U i j ← U i j (XV ) i j (UV T V ) i j , V i j ← V i j (X T U ) i j (VU T U ) i j .</formula><p>In the clustering setting of NMF <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> , U ∈ R f ×C represents the basis matrix, V ∈ R n ×C denotes the representation matrix and C is the number of hidden patterns. Since C n and C f , NMF would find a low-dimensional representation V of X . It is worth noting that previous studies have shown that NMF is equivalent to relaxed k -means clustering, which is a centroid based method and only works for single view data clustering <ref type="bibr" target="#b24">[25]</ref> .</p><p>Real-world data set usually contains multiple modalities (i.e., factors). Take the face data set as an example, it contains modalities including pose, expression, etc. Traditional NMF cannot fully uncovers the hidden structure of those factors. The multi-layer de-composition process of a data matrix can be formulated as</p><formula xml:id="formula_4">X ≈ U 1 V T 1 , X ≈ U 1 U 2 V T 2 , . . . X ≈ U 1 U 2 • • • U r V T r ,<label>(2)</label></formula><p>where U i is the i -th layer basis matrix and V i denotes i -th layer representation matrix. It is clear that the deep NMF model also aims to find a low-dimensional representation of the original data matrix that has a similar interpretation at the top layer, i.e., the last factor V r ( r is the number of layers). By further factorizing V i , the deep model can automatically learn what this latent hierarchy of attributes is. In other words, by introducing an extra layer of abstraction minimizing the dimensionality of obtained representation, we can automatically exploit the corresponding latent attributes as well as the intermediate hidden representations that are implied, allowing for a better higher-level representation V r . Finally, a better high-level, finallayer representation for the for subsequent learning task according to the attribute with the lowest variability. Compared with the single layer NMF model as described in Eq. ( <ref type="formula" target="#formula_1">1</ref>) , deep NMF model has a better ability to uncover the hidden structure since the representations of each layer can identify the different attributes <ref type="bibr" target="#b25">[26]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem formulation</head><p>The MF models mentioned above only work for single-view data clustering. In this paper, we focus on extending the traditional MF model to a novel deep matrix factorization framework for learning multi-view clustering. Generally, a multi-view clustering system consists of four components: feature extraction, algorithm design, view fusion and data clustering. The first component is not our main concern as we compare with other methods on benchmark multi-view datasets. This section introduces the last three components in detail. It is noteworthy that the last two components, i.e., view fusion and data clustering, can be completed simultaneously in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The objective function</head><p>We first introduce the following notations. Denote the multiview data with M views as X = { X (1) , X (2) </p><formula xml:id="formula_5">, • • • , X (M) } . { X (m ) }| M m =1 = [x (m ) 1 , x (m ) 2 , • • • , x (m ) n ] ∈ R f (m ) ×n ,</formula><p>where n is the number of data points ( x (m ) j denotes the j -th data point of X ( m ) ), f ( m ) is the feature dimension of the m -th view. Denote U (m )   i and V (m )   i as the basis matrix and the obtained representation matrix of the i -th layer mapping for the m -th view, respectively.</p><p>The key point of utilizing the multiple views to deal with the multi-view learning task is to reasonably fuses these views to produce a more accurate and robust result. It is obviously unwise to analyse the multiple views based on the concatenated features of different views with equal weight, as the good views and weak views are treated equally in this way. In order to exploit the complementary aspects of information shared by different views, a reasonable strategy is to combine the views with suitable weights α (m ) (m = 1 , . . . , M) , and keep the smooth of the weights distribution by introducing an extra parameter λ. Moreover, multi-view learning methods seek to find the solution which uncovers the common latent structure shared by multiple views, thus the representation matrix obtained in the final layer of different views should be unique, i.e., V (1)    <ref type="formula" target="#formula_1">1</ref>) , we remove the non-negative constraint on U (m )   i , thus the input data matrix X is allowed to have mixed signs, which extends the applicable range of our model. Meanwhile, to obtain more stable data representation performance with respect to a fixed initialization, the robust multi-view clustering method is desired. In this paper, we further utilize a sparsity-inducing norm to reduce the effects of the data noise, inspired by recent developments in l 2,1norm <ref type="bibr" target="#b26">[27]</ref> . We propose a novel deep matrix decomposition framework for learning multi-view clustering by solving: min</p><formula xml:id="formula_6">r = V (2) r = • • • = V (m ) r = V r . Unlike Eq. (</formula><formula xml:id="formula_7">U (m ) i , V (m ) i ,α (m ) M m =1 α (m ) λ X (m ) -U (m ) 1 • • • U (m ) r V T r 2 , 1 , s . t . V (m ) i ≥ 0 , i ∈ [1 , . . . , r -1] , ( V r ) .c = { 0 , 1 } , C c=1 ( V r ) .c = 1 , M m =1 α (m ) = 1 , α (m ) ≥ 0 ,<label>(3)</label></formula><p>where ( V r ) . c denotes a row element of V r , α ( m ) is the weight for the m -th view, λ is a positive scalar and it could be regularization parameter in another form: min</p><formula xml:id="formula_8">U (m ) i , V (m ) i ,α (m ) M m =1 α (m ) n i =1 X (m ) -U (m ) 1 • • • U (m ) r V T r 2 , 1 + λ α (m ) 2 2 s . t . V (m ) i ≥ 0 , i ∈ [1 , . . . , r -1] , ( V r ) .c = { 0 , 1 } , C c=1 ( V r ) .c = 1 , M m =1 α (m ) = 1 , α (m ) ≥ 0 . (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>As we can see, each row of V r in our model is coded as 1-of-C scheme. The main purpose is to guarantee the uniqueness of our solution. Furthermore, the partition result can be directly obtained without any post process.</p><p>Since λ can be searched in a large range and the choice of its value is crucial to final multi-view clustering performance. Thus we expect to remove such a parameter while pursuing good performance. In this paper, we propose an auto-weighted strategy to address such challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Auto-weighted deep MF multi-view clustering</head><p>Motivated by recently proposed Iteratively Re-weighted technique <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> , we propose a novel model for multi-view clustering as min</p><formula xml:id="formula_10">U (m ) i , V (m ) i M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 s . t . V (m ) i ≥ 0 , i ∈ [1 , . . . , r -1] , ( V r ) .c = { 0 , 1 } , C c=1 ( V r ) .c = 1 .</formula><p>(</p><formula xml:id="formula_11">)<label>5</label></formula><p>We can see there is no weight hyperparameter explicitly defined therein. The Lagrange function of Eq. ( <ref type="formula" target="#formula_11">5</ref>) can be defined as min</p><formula xml:id="formula_12">U (m ) i , V (m ) i M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 + ( , V (m ) i ) , (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>where denotes the Lagrange multiplier for constraints on</p><formula xml:id="formula_14">V (m ) i ,</formula><p>and</p><formula xml:id="formula_15">( , V (m ) i</formula><p>) is the formalized term derived from constraints.</p><p>Taking the derivative of Eq. ( <ref type="formula" target="#formula_12">6</ref>) w.r.t V (m )   i and setting the derivative to zero, we have</p><formula xml:id="formula_16">M m =1 α (m ) ∂ X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 ∂V (m ) i + ∂ ( , V (m ) i ) ∂V (m ) i = 0 , (<label>7</label></formula><formula xml:id="formula_17">)</formula><p>where</p><formula xml:id="formula_18">α (m ) = 1 2 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 .<label>(8)</label></formula><p>Since the weights α ( m ) are dependent on target variables V (m )   i and U (m )   i , we cannot solve Eq. ( <ref type="formula" target="#formula_16">7</ref>) directly. But if α ( m ) are set to be stationary, Eq. ( <ref type="formula" target="#formula_16">7</ref>) can be denoted as the solution to the following problem min</p><formula xml:id="formula_19">U (m ) i , V (m ) i M m =1 α (m ) X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 s . t . V (m ) i ≥ 0 , i ∈ [1 , . . . , r -1] , ( V r ) .c = { 0 , 1 } , C c=1 ( V r ) .c = 1 . (<label>9</label></formula><formula xml:id="formula_20">)</formula><p>Based on the assumption that the weights α ( m ) are stationary, the Lagrange function of Eq. ( <ref type="formula" target="#formula_11">5</ref>) can also be applied to Eq. ( <ref type="formula" target="#formula_19">9</ref>) . If we update U (m )   i and V (m )   i by solving Eq. ( <ref type="formula" target="#formula_19">9</ref>) , the values of α ( m ) can be further updated correspondingly. This inspires us to optimize Eq. ( <ref type="formula" target="#formula_11">5</ref>) using an iterative way. When the proposed iterative algorithm converges, the converged values of U (m )   i and V (m )   i are the local optimal solution to Eq. ( <ref type="formula" target="#formula_11">5</ref>) according to Eqs. ( <ref type="formula" target="#formula_16">7</ref>) and ( <ref type="formula" target="#formula_18">8</ref>) . Similarly, α ( m ) are tuned to the optimal values correspondingly, and they are exactly the learned weights for all views.</p><p>It is worth mentioning that our model is different from previous deep structure based multi-view learning methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> . The major difference is that <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> are based on Canonical Correlation Analysis. And these methods can only handle the 2-view case, while our model has no such limitation. Another related work is proposed in <ref type="bibr" target="#b31">[32]</ref> , which is designed for deep multi-view clustering (DMVC). However, the residual calculation of DMVC is based on traditional Frobenius norm, which is sensitive to data noise in practical problems. In addition, the obtained data representation in DMVC cannot directly assign the partition result, therefore needs a post-processing (e.g., spectral clustering algorithm used in <ref type="bibr" target="#b31">[32]</ref> ) to assign the class label to each data point. Furthermore, the weight for each view in DMVC is computed by introducing an hyperparameter, thus the final performance is highly dependent on parameter searching.</p><p>In the following, we propose an efficient iterative updating algorithm to solve the optimization problem of Eq. ( <ref type="formula" target="#formula_19">9</ref>) . We will optimize the objective with respect to one variable while fixing the other variables. This procedure repeats until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>Before the optimization, we conduct the pre-training by de-</p><formula xml:id="formula_21">composing each view X (m ) ≈ U (m ) 1 V (m ) T 1</formula><p>, where</p><formula xml:id="formula_22">V (m ) 1 ∈ R n ×k 1 and</formula><formula xml:id="formula_23">U (m ) 1 ∈ R f (m ) ×k 1 .</formula><p>Then the representation matrix is further decomposed as</p><formula xml:id="formula_24">V (m ) 1 ≈ U (m ) 2 V (m ) T 2</formula><p>, where</p><formula xml:id="formula_25">V (m ) 2 ∈ R n ×k 2 and U (m ) 2 ∈ R k 1 ×k 2 .</formula><p>Here we define k 1 and k 2 as the dimensionalities of layer 1 and layer 2, respectively. 1 Note that the pre-training process can be accomplished by simply using k -means, as the relaxed k -means is equivalent to the matrix factorization method <ref type="bibr" target="#b32">[33]</ref> . The process will be repeated till all layers are pre-trained. 1 For convenience of discussion, we simply denote the dimensionalities (layer size) from layer 1 to layer r as</p><formula xml:id="formula_26">p = [ k 1 , k 2 , • • • , k r ]</formula><p>in the experiments. E.g., if we define p = [100 50 C ], it means there are three layers in the deep framework, and</p><formula xml:id="formula_27">X ( m ) is decomposed as X (m ) ≈ U (m ) 1 U (m ) 2 U (m ) 3 V (m ) T 3</formula><p>, where</p><formula xml:id="formula_28">U (m ) 1 ∈ R f (m ) ×100 , U (m ) 2 ∈ R 100 ×50 , U (m ) 3 ∈ R 50 ×C and V (m ) 3 ∈ R n ×C .</formula><p>First we rewrite Eq. ( <ref type="formula" target="#formula_19">9</ref>) as follows min</p><formula xml:id="formula_29">U (m ) i , V (m ) i M m =1 α (m ) Tr Y (m ) D (m ) Y (m ) T s . t . V (m ) i ≥ 0 , i ∈ [1 , . . . , r -1] , ( V r ) .c = { 0 , 1 } , C c=1 ( V r ) .c = 1 ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_30">Y (m ) = X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r , (<label>11</label></formula><formula xml:id="formula_31">)</formula><p>and D ( m ) denotes a diagonal matrix:</p><formula xml:id="formula_32">D (m ) = 1 2 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 . (<label>12</label></formula><formula xml:id="formula_33">)</formula><p>Computation of U (m )   i .</p><p>Optimizing Eq. ( <ref type="formula" target="#formula_29">10</ref>) w.r.t. U (m )   i is equivalent to minimizing the problem as follows</p><formula xml:id="formula_34">J U = min U (m ) i M m =1 α (m ) Tr Y (m ) D (m ) Y (m ) T . (<label>13</label></formula><formula xml:id="formula_35">)</formula><p>Taking the partial derivation of J U w.r.t. U (m )   i , we have</p><formula xml:id="formula_36">∂J U ∂ U (m ) i = -2 T X (m ) D (m ) V (m ) i + 2 T U (m ) i V (m ) T i D (m ) V (m ) i ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_37">D (m ) = α (m ) D (m ) ,<label>(15)</label></formula><p>and</p><formula xml:id="formula_38">(m ) = U (m ) 1 U (m ) 2 • • • U (m ) i -1 and V (m )</formula><p>i is the reconstruction of the i -th layer's feature matrix.</p><p>Setting Eq. ( <ref type="formula" target="#formula_36">14</ref>) = 0 leads to the following update rule</p><formula xml:id="formula_39">U (m ) i = (m ) T (m ) -1 (m ) T X (m ) D (m ) V (m ) i V (m ) T i D (m ) V (m ) i -1<label>(16)</label></formula><p>Computation of V (m )   i</p><formula xml:id="formula_40">( i &lt; r ).</formula><p>Optimizing Eq. ( <ref type="formula" target="#formula_29">10</ref>) w.r.t. V (m )   i is equivalent to minimizing</p><formula xml:id="formula_41">J V = min V (m ) i M m =1 α (m ) Tr Y (m ) D (m ) Y (m ) T s . t . V (m ) i ≥ 0 , i ∈ [1 , . . . , r -1] . (<label>17</label></formula><formula xml:id="formula_42">)</formula><p>Taking the partial derivation of</p><formula xml:id="formula_43">J V w.r.t. V (m ) i : J V ∂ V (m ) i = 2 D (m ) V (m ) i U (m ) T i (m ) T (m ) U (m ) i -2 D (m ) X (m ) T (m ) U (m ) i (<label>18</label></formula><formula xml:id="formula_44">) Setting L ∂ V (m ) i = 0 , we have V (m ) i = X (m ) T (m ) U (m ) i U (m ) T i (m ) T (m ) U (m ) i -1 . (<label>19</label></formula><formula xml:id="formula_45">)</formula><p>Computation of V r (i.e., V (m )   i , (i = r) ).</p><p>Optimizing Eq. ( <ref type="formula" target="#formula_29">10</ref>) w.r.t. V r is equivalent to solving min</p><formula xml:id="formula_46">V r M m =1 α (m ) Tr Y (m ) D (m ) Y (m ) T = min v M m =1 n i =1 d (m ) i x (m ) i -U (m ) 1 U (m ) 2 • • • U (m ) r v i 2 2 = min v n i =1 M m =1 d (m ) i x (m ) i -U (m ) 1 U (m ) 2 • • • U (m ) r v i 2 2 s . t . ( V r ) .c = { 0 , 1 } , C c=1 ( V r ) .c = 1 ,<label>(20)</label></formula><p>where x (m )   i is the i -th data point of X ( m ) , v i is the i -th column of V T r and d (m )   i is the i -th diagonal element of D (m ) . The optimization problem in Eq. ( <ref type="formula" target="#formula_46">20</ref>) can be solved by decoupling the data and assigning the cluster indicator for them one by one independently. In other words, for a fixed specific j , we solve the following problem</p><formula xml:id="formula_47">min v M m =1 d (m ) x (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r v 2 2 s . t . v c = { 0 , 1 } , C c=1 v c = 1 , (<label>21</label></formula><formula xml:id="formula_48">)</formula><formula xml:id="formula_49">here we define v = [ v 1 , v 2 , • • • , v C ] T ∈ R C×1 . Since v satisfies 1-of-</formula><p>C coding scheme, there are C candidates to be the solution of Eq. ( <ref type="formula" target="#formula_47">21</ref>) . Thus we can do an exhaustive search to find out the solution of Eq. ( <ref type="formula" target="#formula_47">21</ref>) as</p><formula xml:id="formula_50">v * = v c , (<label>22</label></formula><formula xml:id="formula_51">)</formula><p>where c is obtained by setting</p><formula xml:id="formula_52">c = arg min c M m =1 d (m ) x (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r v c 2 2 . (<label>23</label></formula><formula xml:id="formula_53">)</formula><p>We summarize the proposed algorithm in Algorithm 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>Deep MF for multi-view data representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Data set with M views X = { X (1) , X (2) ,</p><formula xml:id="formula_54">• • • , X (M) } and X (m ) ∈ R f (m ) ×n ;</formula><p>The layer size p;</p><p>The number of hidden structures C;</p><p>Initialize the weight factor α (m ) = 1 M for each view; Output: U (m )   i and V (m )   i for each of the layers;</p><formula xml:id="formula_55">Initialize: for all layers do U (m ) i , V (m ) i ← k -means V i -1 ,layers( i ) end for Initialize V r ∈ R n ×C , such that V r satisfies the 1-of-C coding scheme;</formula><p>Initialize D as defined in Eq. <ref type="bibr" target="#b11">(12)</ref>. repeat for all layers do</p><formula xml:id="formula_56">1. Update V (m ) i = V r if i = r , U (m ) i +1 V (m ) T i +1</formula><p>otherwise . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Compute</head><formula xml:id="formula_57">= i j=1 U (m ) j . 3.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Convergence analysis</head><p>To prove the convergence of the proposed algorithm, we need to utilize the lemma introduce by <ref type="bibr" target="#b33">[34]</ref> : Lemma 1. For any positive real number a and b, the following inequality holds:</p><formula xml:id="formula_58">√ a - a 2 √ b ≤ b - b 2 √ b . (<label>24</label></formula><formula xml:id="formula_59">)</formula><p>Theorem 1. In Algorithm 1 , updated U (m )   i and V (m )   i will monotonically decrease the objective in Eq. ( <ref type="formula" target="#formula_19">9</ref>) (i.e., Eq. ( <ref type="formula" target="#formula_11">5</ref>) ) until converge.</p><p>Proof. Suppose the alternatively updated U (m )   i and V (m )     <ref type="formula" target="#formula_39">16</ref>) and ( <ref type="formula" target="#formula_52">23</ref>) , considering the fixed weights α</p><formula xml:id="formula_60">(m ) = 1 2 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 in current iteration, we have M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 2 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 ≤ M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 2 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 . (<label>25</label></formula><formula xml:id="formula_61">)</formula><p>According to Lemma 1 , we have</p><formula xml:id="formula_62">M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 - M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 2 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 ≤ M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 - M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 2 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 . (<label>26</label></formula><formula xml:id="formula_63">)</formula><p>By summing over Eqs. <ref type="bibr" target="#b24">(25)</ref> and <ref type="bibr" target="#b25">(26)</ref> in the two sides, we obtain</p><formula xml:id="formula_64">M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 ≤ M m =1 X (m ) -U (m ) 1 U (m ) 2 • • • U (m ) r V T r 2 , 1 . (<label>27</label></formula><formula xml:id="formula_65">)</formula><p>As a result, we prove that the objective value of current iteration is less than or equal to that of previous iteration. That is to say, the object is monotonically decreasing with iterations, and thus the convergence is certainly guaranteed. Finally, we complete the prove.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Time complexity analysis</head><p>For the proposed method, it is composed of two stages including pre-training and fine-tuning. For convenience of analysis, we assume the dimensions of all the layers (i.e., layer size) are the same, namely p . The original feature dimensions for all the views are the same, namely f. M is the number of views. k is the number of layers. In pre-training stage, the time complexity of kmeans process is O (kt 1 M( f np + np 2 + p f 2 )) , where t 1 is the iteration number of k -means. Similarly, the time complexity for the fine-tuning stag is O (kt 2 M( f np + np 2 + p f 2 )) , where t 2 is the iteration number in this fine-tuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Recent studies have focused on the case when dealing with multi-view learning tasks in the unsupervised setting, particularly, multi-view clustering. As mentioned before, the target data representation matrix V r satisfies the 1-of-C coding scheme, thus the clustering partition result can be directly obtained without any post process, which is a major advantage for unsupervised learning. In our experiments, we will show the performance of the proposed method on both toy data and real-world benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on toy data</head><p>In this subsection, a randomly generated toy data is used to demonstrate our algorithms ability to make the data points more distinctive layer by layer. The toy dataset is comprised of two views and each view has 200 data points, which is grouped into two clusters, as shown in Fig. <ref type="figure" target="#fig_2">2</ref> (a) and (b). The data points in each cluster are sampled from a Gaussian distribution with distinct center and variance.</p><p>We perform a three layer experiment on the toy dataset as an example. For convenience of demonstration, the layer size of all layers is set to two (i.e., p = [2 2 2]), that is, { V (1)  1 , V (1)  2 , V (1)  3 , V (2)  1 , V (2)  2 , V (2)  3 } ∈ R 200 ×2 . Noting that V (1)   1 denotes the first layer's representation matrix of the first view, as shown in the left subfigure of Fig. <ref type="figure" target="#fig_3">3 (a)</ref>. As we can see, data points from the same class are forced to be closer layer by layer, for both of the two views. This demonstrates the effectiveness of the proposed method, which is able to uncover the hierarchical semantics of the input data in a layer-wise way. It is worth mentioning that, on view-2 (as shown in Fig. <ref type="figure" target="#fig_3">3 (b)</ref>), there is a data point misclassified. Fortunately, by leveraging the weight of each view as described in Eq. ( <ref type="formula" target="#formula_18">8</ref>) , the data point could be correctly classified, as the clustering capacity of view-1 is better than that of view-2. In our model, the representation matrix of last layer satisfies the 1-of-C coding scheme, thus ideally, the data points that belong to the same class would share the position with the same coordinate, as shown in the right subfigures in Fig. <ref type="figure" target="#fig_3">3</ref> (a) and (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on benchmark data</head><p>To demonstrate the effectiveness of our method in terms of clustering performance, we compare the following state-of-the-art multi-view clustering methods:</p><p>• Co-trained multi-view clustering (Co-train) <ref type="bibr" target="#b10">[11]</ref> : Co-train is proposed based on the assumption that the underlying common structure would assign a instance to the same class irrespective of the view. That is, if two instances are assigned in different clusters in one view, it should be so in all the views, and vice versa. It makes use of compatibility assumption of co-training. • Co-regularized multi-view clustering (Co-reg) <ref type="bibr" target="#b9">[10]</ref> : Co-reg enforces the view-specific eigenvectors of different views to look similar by regularizing them towards a common consensus. The objective combines the graph Laplacians of all views such that the latent structures resulting from each Laplacian look consistent. • Multi-view k -means clustering (MVKKM) <ref type="bibr" target="#b34">[35]</ref> : It performs the multi-view clustering task by utilizing unsupervised multiple kernel learning. All views in MVKKM are expressed by a given kernel matrices, and a weighted combination of the kernels is learned in parallel to the partitioning. • Robust multi-view k -means clustering (RMKMC) <ref type="bibr" target="#b32">[33]</ref> : This method is designed by extending the original single-view kmeans clustering algorithm to a robust multi-view k -means clustering method. • Multi-view NMF with local graph regularization (Multi-NMF) <ref type="bibr" target="#b35">[36]</ref> : MultiNMF is proposed based on manifold learning and multi-view NMF. A local graph regularization is constructed by taking the inner-view relatedness into consideration. With this local graph regularization, it is expected to integrate local geometrical information of each view. • Multi-view clustering with adaptive structure concept factorization (MVCF) <ref type="bibr" target="#b36">[37]</ref> : As an unsupervised multifeature learning method, MVCF aims to make the best utilization of the correlation among multiple features in an adaptive way. An affinity graph for each view is constructed, and the weight of affinity graph is learned by solving optimization problem. • Self-weighted multi-view clustering with soft capped norm (SCaMVC) <ref type="bibr" target="#b4">[5]</ref> : It is proposed to deal with different level noises and outliers by using soft capped norm. In SCaMVC, the residual of outliers is capped as a constant value and provides a probability for certain data point being an outlier.</p><p>• Latent multi-view subspace clustering (LMSC) <ref type="bibr" target="#b16">[17]</ref> : Unlike most existing single-view subspace clustering methods, which directly reconstruct data points using original features, LMSC explores underlying complementary information from multiple views and simultaneously seeks the underlying latent representation. Using the complementarity of multiple views, the latent representation depicts data more comprehensively than each individual view, accord- </p><p>ingly making subspace representation more accurate and robust. • Multi-view clustering via deep semi-NMF (DMVC) <ref type="bibr" target="#b31">[32]</ref> :</p><p>DMVC applies the semi-nonnegative matrix factorization to exploit the hierarchical semantics of the input multi-view data. It introduces a graph regularizer into the objective by considering the intrinsic geometric information. This work is also one of the most recent multi-view clustering approaches based on a deep structure framework. Similar to our method, the goal of DMVC is to find a common representation matrix. Then it employs an additional clustering algorithm (e.g., spectral clustering algorithm) on the common representation to obtain the final clustering results.</p><p>In addition, the classic clustering algorithm, k -means (KM), is included as baseline. We apply KM on every dataset using each view of features (e.g., KM(1) means performing KM on the 1st view). We also apply KM using concatenated features of all views with equal weight (i.e., All features, AllFea in short), which assumes that all the views are of the same importance to the clustering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data sets</head><p>Several benchmark datasets including image datasets and document datasets are used to assess the performance of our method. The details of these datasets are given below:</p><p>• Handwritten (HW): 2 This dataset contains 20 0 0 instances for 0-9 ten digit classes, which is collected from two sources, i.e., USPS Handwritten Digits and MNIST Handwritten Digits.</p><p>2 https://cs.nyu.edu/roweis/data.html .</p><p>• 3 sources: <ref type="foot" target="#foot_0">3</ref> It consists of 169 news collected from BBC, Reuters and The Guardian. The topic of each news is associated with one of the following six topical labels: technology, health, business, politics, entertainment, and sport. • BBC 4 [38] : BBC is derived from the BBC news corpora. It contains of 685 documents, and each document is split into four segments. The topic of each news is associated with one of five topical labels. • BBCSport<ref type="foot" target="#foot_2">5</ref>  <ref type="bibr" target="#b37">[38]</ref> : It consists of 544 documents collected from the BBC Sport website. Each document was split into two segments. Similar to BBC, the topic of each news is associated with one of five topical labels. • CiteSeer 6 [39] : This dataset contains 3312 documents over the 6 labels. Each document is desribed by content view and citations view. In detail, the documents are described by 3703 words in the content view, and 4732 links between them in the inbound, outbound and cites views. • Reuters<ref type="foot" target="#foot_4">7</ref>  <ref type="bibr" target="#b38">[39]</ref> : The archive contains 6 samples of 1200 documents. Each document is described by five views of 20 0 0 words each. The documents were initially in English, other four views correspond to the words of their traductions in Italian, French, German, and Spanish, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiment setup</head><p>For each data set, the number of clusters is set to the true number of classes for all methods. In our model, inspired by Zhao et al. <ref type="bibr" target="#b31">[32]</ref> , the layer sizes are empirically searched in [100 C ], [50 C ],  [100 50 C ] for simplicity, where C is the number of clusters. For compared methods, we obtained the source code from its authors' website, and we set their parameters to the optimal value if they have. In order to randomize the experiments, we repeat the experiments 10 times, and the means and standard deviations are reported. Three standard metrics accuracy (ACC), normalized mutual information (NMI) and Purity are used to evaluate the experimental performance. These metrics are positive correlated, a larger value represents a better results. The best mean and standard deviation of the results are highlighted in boldface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Experimental results</head><p>The clustering performance measured by ACC, NMI and Purity are shown in Tables <ref type="table" target="#tab_1">1</ref><ref type="table">2</ref><ref type="table" target="#tab_2">3</ref><ref type="table" target="#tab_3">4</ref><ref type="table">5</ref><ref type="table">6</ref>. As we can see, our method is very competitive, outperforms other compared methods in most cases. Specifically, the superiority of the proposed method arises the following aspects:</p><p>-It can be observed that the clustering performance of KM(AllFea) (i.e., performing k -means on the concatenated features of all views) is better than that obtained on a single view for most of the times. This indicates that the clustering performance can be enhanced by considering all information hidden in data, even just simply concatenates the features of all views with equal weight. -The clustering performance obtain by multi-view clustering approaches, including ours, generally outperforms baseline method on both each individual view and the concatenated features. This states that the clustering quality can be further improved by leveraging the consistent and complemen- tary information shared by different views. With the help of multi-view clustering formulations, the learned weight for a good view is large while a relatively small weight will be assigned to a weak view. This is the core reason why multiview learning methods are more efficient than traditional single-view methods. -The results of the proposed method are better than the other compared methods on most datasets, thus demonstrates the effectiveness of our method, which is based on deep matrix decomposition framework. Considering the mapping between the obtained representation and the original data contains rather complex hierarchical information with implicit lower-level hidden attributes, our model is able to learn a better representation by uncovering the hierarchical semantics of the input data in a layerwise way. By in-troducing the deep framework, we expect to fully extracted the hierarchical information in general case. Noting that the dataset Resters is very sparse (contains only a few nonzero elements in data matrix). This may be the reason why MVCF, LMSC and DMVC cannot handle this dataset, as recorded in Table <ref type="table">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Parameter tuning</head><p>Our model has only one parameter, i.e., the layer size p . In this subsection, we investigate the sensitivity of our model with respect to this parameter, as shown in Table <ref type="table" target="#tab_4">7</ref> .</p><p>As we can see, our model is very stable when p is searched in  [50 C ] may be a better choice, as the model consistently achieves good performance under this parameter setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Convergence study</head><p>We have proven that the updating rules for optimizing the proposed objective are convergent, as decribed in last section. Here we empirically investigate how fast these rules can converge. For comparison, the convergence curve of another deep multi-view clustering method, DMVC, is also recorded. For our model, we record the value of our objective as introduced in Eq. ( <ref type="formula" target="#formula_19">9</ref>) . While for DMVC, we report the value of Eq. ( <ref type="formula" target="#formula_7">3</ref>) as described in <ref type="bibr" target="#b31">[32]</ref> .</p><p>The convergence curves of our model and DMVC are shown in Fig. <ref type="figure" target="#fig_4">4</ref> . Noting that when the optimization process converges or the number of iterations reaches the predefined maximum value (which is empirically set to 100 in our experiment), the proposed algorithm stops the iterative process. It can be seen that the updating rules for our model converge very fast. Compared with DMVC, our model needs less iteration to achieve the convergence, which demonstrates the effectiveness of proposed optimization algorithm. Note that the measurement of residual calculation is different in our model and DMVC (we use l 2,1 -norm, while DMVC uses the square of l 2 -norm), thus it is natural that the value of the two objectives may differ significantly. Fortunately, the convergence speed is irrelevant to the objective value, as it is typically depended on update rules of the optimization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a novel deep multi-view clustering model is proposed by uncovering the hierarchical semantics of the input data in a layerwise way. By utilizing the deep matrix decomposition framework, the hidden representations are learned with respect to different attributes. We derive an efficient updating algorithm to solve the optimization problem and its convergence is also guaranteed theoretically. Furthermore, our model is able to automatically assign the optimal weight to each view without introducing extra hyperparameter as previous methods do. Experimental results on both toy and benchmark data demonstrate the efficacy of the proposed algorithm. Considering that our model is essentially an unsupervised algorithm that does not make use of the priori information, it would be interesting to extend the proposed framework to other machine learning areas such as semi-supervised learning and classification problems. In the future, we are also interested in studying how to extend the multi-view learning idea to nonlinear latent subspace cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. There are three deep MF structures proposed to demonstrate the framework of our model. Same shape represents the instances belong to the same class. We can see the variability in face data can stem from the attributes such as the pose (eyes left, right or front) or the facial expression (with or without a smile) of the subject. By utilizing the proposed deep structure, the hierarchical information share by the views can be fully exploited layer by layer. And finally, a more discriminative representation is generated.</figDesc><graphic coords="2,125.03,57.46,336.00,237.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the original toy data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustrations of each layer of each view. (a) view-1, (b) view-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The convergence curve of DMVC and our method. (a) HW, (b) BBCSport, (c) CiteSeer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>[100 C ], [50 C ], [100 50 C ]. This indicates the proposed model is not sensitive to p . Relatively speaking, tuning the layer size to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Compute the diagonal matrices D</figDesc><table><row><cell>(m ) according to Eq.</cell></row><row><cell>(15); 4. Update basis matrices U (m )</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p>i according to Eq. (</p>16</p>); 5. Update V</p>(m )   </p>i (i &lt; r) according to Eq. (</p>19</p>); 6. Update each row of the last layer representation matrix one by one according to Eq. (</p>23</p>); 7. Update the weight factor α (m ) according to Eq. (</p>8</p>); end for until Converges</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Clustering performance on HW (%).</figDesc><table><row><cell>Method</cell><cell>ACC</cell><cell>NMI</cell><cell>Purity</cell></row><row><cell>KM(1)</cell><cell>54.89(3.19)</cell><cell>48.92(1.02)</cell><cell>57.29(2.30)</cell></row><row><cell>KM(2)</cell><cell>44.75(2.42)</cell><cell>44.24(1.63)</cell><cell>47.76(2.09)</cell></row><row><cell>KM(AllFea)</cell><cell>61.05(5.26)</cell><cell>59.67(3.32)</cell><cell>65.20(4.23)</cell></row><row><cell>Co-train</cell><cell>50.33(0.46)</cell><cell>38.38(0.83)</cell><cell>50.93(0.18)</cell></row><row><cell>Co-reg</cell><cell>55.15(4.31)</cell><cell>47.91(3.27)</cell><cell>57.93(3.36)</cell></row><row><cell>MVKKM</cell><cell>49.82(2.79)</cell><cell>48.58(0.76)</cell><cell>55.33(1.38)</cell></row><row><cell>RMKMC</cell><cell>78.30(1.49)</cell><cell>76.71(1.50)</cell><cell>83.55(0.57)</cell></row><row><cell>MultiNMF</cell><cell>68.95(2.97)</cell><cell>62.65(2.66)</cell><cell>68.98(2.93)</cell></row><row><cell>MVCF</cell><cell>64.78(5.90)</cell><cell>60.72(0.04)</cell><cell>65.58(4.77)</cell></row><row><cell>SCaMVC</cell><cell>75.35(0.78)</cell><cell>74.49(1.27)</cell><cell>79.40(1.23)</cell></row><row><cell>LMSC</cell><cell>73.40(0.45)</cell><cell>63.19(1.02)</cell><cell>75.85(0.65)</cell></row><row><cell>DMVC</cell><cell>52.17(3.57)</cell><cell>50.20(6.24)</cell><cell>61.55(2.93)</cell></row><row><cell>Ours</cell><cell>80.35(7.74)</cell><cell>83.48(8.38)</cell><cell>85.25(7.71)</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Clustering performance on 3Sources (%).</cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell>NMI</cell><cell>Purity</cell></row><row><cell>KM(1)</cell><cell>37.81(2.97)</cell><cell>14.35(4.08)</cell><cell>41.01(4.58)</cell></row><row><cell>KM(2)</cell><cell>40.41(7.56)</cell><cell>16.67(9.06)</cell><cell>43.14(8.33)</cell></row><row><cell>KM(3)</cell><cell>40.89(6.86)</cell><cell>18.34(8.63)</cell><cell>44.02(7.42)</cell></row><row><cell>KM(AllFea)</cell><cell>41.78(7.99)</cell><cell>20.71(9.87)</cell><cell>46.92(9.40)</cell></row><row><cell>Co-train</cell><cell>33.14(0.01)</cell><cell>10.04(0.37)</cell><cell>34.91(0.01)</cell></row><row><cell>Co-reg</cell><cell>32.55(1.68)</cell><cell>12.56(0.76)</cell><cell>36.69(0.84)</cell></row><row><cell>MVKKM</cell><cell>41.22(5.17)</cell><cell>18.10(8.61)</cell><cell>43.39(6.49)</cell></row><row><cell>RMKMC</cell><cell>34.71(0.68)</cell><cell>14.23(1.04)</cell><cell>36.29(0.35)</cell></row><row><cell>MultiNMF</cell><cell>45.76(4.52)</cell><cell>37.67(4.80)</cell><cell>59.76(3.29)</cell></row><row><cell>MVCF</cell><cell>53.61(4.60)</cell><cell>40.40(7.76)</cell><cell>55.45(4.60)</cell></row><row><cell>SCaMVC</cell><cell>49.70(4.53)</cell><cell>28.54(3.03)</cell><cell>57.99(2.34)</cell></row><row><cell>LMSC</cell><cell>41.42(2.37)</cell><cell>23.90(1.12)</cell><cell>51.48(1.90)</cell></row><row><cell>DMVC</cell><cell>43.79(0.42)</cell><cell>32.62(0.73)</cell><cell>61.55(1.25)</cell></row><row><cell>Ours</cell><cell>55.03(0.83)</cell><cell>41.82(4.27)</cell><cell>60.36(2.09)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Clustering performance on BBC (%).</figDesc><table><row><cell>Method</cell><cell>ACC</cell><cell>NMI</cell><cell>Purity</cell></row><row><cell>KM(1)</cell><cell>38.74(6.63)</cell><cell>18.63(8.93)</cell><cell>39.74(6.73)</cell></row><row><cell>KM(2)</cell><cell>35.40(3.29)</cell><cell>13.81(3.72)</cell><cell>36.20(3.19)</cell></row><row><cell>KM(3)</cell><cell>36.76(4.37)</cell><cell>16.39(5.18)</cell><cell>38.22(4.24)</cell></row><row><cell>KM(4)</cell><cell>35.87(4.87)</cell><cell>15.21(5.16)</cell><cell>36.35(4.69)</cell></row><row><cell>KM(AllFea)</cell><cell>37.01(3.35)</cell><cell>16.10(5.96)</cell><cell>37.46(3.17)</cell></row><row><cell>Co-train</cell><cell>32.70(0.01)</cell><cell>10.93(0.01)</cell><cell>33.14(0.01)</cell></row><row><cell>Co-reg</cell><cell>39.27(1.34)</cell><cell>11.04(0.24)</cell><cell>34.03(0.21)</cell></row><row><cell>MVKKM</cell><cell>40.53(4.39)</cell><cell>17.39(3.57)</cell><cell>41.36(4.99)</cell></row><row><cell>RMKMC</cell><cell>32.60(1.22)</cell><cell>11.64(0.47)</cell><cell>33.48(0.34)</cell></row><row><cell>MultiNMF</cell><cell>46.13(2.13)</cell><cell>24.45(2.92)</cell><cell>46.28(1.97)</cell></row><row><cell>MVCF</cell><cell>61.75(4.00)</cell><cell>37.77(5.03)</cell><cell>61.90(3.94)</cell></row><row><cell>SCaMVC</cell><cell>50.22(1.73)</cell><cell>19.27(0.91)</cell><cell>50.38(2.18)</cell></row><row><cell>LMSC</cell><cell>38.39(0.85)</cell><cell>11.47(0.34)</cell><cell>48.03(3.63)</cell></row><row><cell>DMVC</cell><cell>48.18(1.30)</cell><cell>18.91(1.25)</cell><cell>46.06(2.32)</cell></row><row><cell>Ours</cell><cell>57.34(7.70)</cell><cell>39.53(6.21)</cell><cell>68.85(8.70)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Clustering performance on BBCSport (%).</figDesc><table><row><cell>Method</cell><cell>ACC</cell><cell>NMI</cell><cell>Purity</cell></row><row><cell>KM(1)</cell><cell>37.94(4.30)</cell><cell>16.71(3.67)</cell><cell>40.18(3.45)</cell></row><row><cell>KM(2)</cell><cell>37.81(5.12)</cell><cell>16.04(4.71)</cell><cell>40.29(4.55)</cell></row><row><cell>KM(AllFea)</cell><cell>41.56(6.41)</cell><cell>20.24(7.40)</cell><cell>43.22(6.14)</cell></row><row><cell>Co-train</cell><cell>38.42(0.76)</cell><cell>15.69(0.79)</cell><cell>41.54(2.14)</cell></row><row><cell>Co-reg</cell><cell>29.41(0.21)</cell><cell>12.78(0.40)</cell><cell>36.21(0.10)</cell></row><row><cell>MVKKM</cell><cell>38.79(1.66)</cell><cell>17.04(2.05)</cell><cell>36.40(1.21)</cell></row><row><cell>RMKMC</cell><cell>41.73(4.20)</cell><cell>18.29(5.98)</cell><cell>42.28(3.66)</cell></row><row><cell>MultiNMF</cell><cell>53.74(3.77)</cell><cell>32.71(5.25)</cell><cell>54.72(4.51)</cell></row><row><cell>MVCF</cell><cell>61.40(1.84)</cell><cell>37.93(2.52)</cell><cell>61.58(1.84)</cell></row><row><cell>SCaMVC</cell><cell>41.54(2.13)</cell><cell>18.27(2.09)</cell><cell>42.28(1.98)</cell></row><row><cell>LMSC</cell><cell>38.79(3.47)</cell><cell>20.16(1.43)</cell><cell>43.38(1.85)</cell></row><row><cell>DMVC</cell><cell>41.47(2.34)</cell><cell>23.35(2.69)</cell><cell>49.15(2.21)</cell></row><row><cell>Ours</cell><cell>65.33(5.43)</cell><cell>41.52(5.30)</cell><cell>62.62(3.37)</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Clustering performance on Reuters (%).</cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell>NMI</cell><cell>Purity</cell></row><row><cell>KM(1)</cell><cell>24.22(5.46)</cell><cell>17.52(4.46)</cell><cell>24.48(5.38)</cell></row><row><cell>KM(2)</cell><cell>20.65(5.77)</cell><cell>14.09(5.35)</cell><cell>20.92(5.74)</cell></row><row><cell>KM(3)</cell><cell>19.12(3.44)</cell><cell>12.45(2.98)</cell><cell>19.32(3.45)</cell></row><row><cell>KM(4)</cell><cell>23.38(6.88)</cell><cell>15.67(5.49)</cell><cell>23.68(6.82)</cell></row><row><cell>KM(5)</cell><cell>23.08(6.58)</cell><cell>15.55(5.34)</cell><cell>23.48(6.72)</cell></row><row><cell>KM(AllFea)</cell><cell>23.11(6.93)</cell><cell>16.15(5.61)</cell><cell>23.45(6.89)</cell></row><row><cell>Co-train</cell><cell>23.44(2.36)</cell><cell>15.58(2.66)</cell><cell>23.48(2.31)</cell></row><row><cell>Co-reg</cell><cell>22.51(0.59)</cell><cell>12.58(0.07)</cell><cell>22.81(0.48)</cell></row><row><cell>MVKKM</cell><cell>21.53(1.61)</cell><cell>11.37(3.11)</cell><cell>22.46(1.64)</cell></row><row><cell>RMKMC</cell><cell>34.33(3.69)</cell><cell>15.29(4.35)</cell><cell>34.33(3.69)</cell></row><row><cell>MultiNMF</cell><cell>41.17(1.79)</cell><cell>23.45(0.87)</cell><cell>45.83(2.25)</cell></row><row><cell>MVCF</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SCaMVC</cell><cell>30.00(5.69)</cell><cell>20.62(4.34)</cell><cell>30.17(5.73)</cell></row><row><cell>LMSC</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DMVC</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>46.18(1.93)</cell><cell>27.83(1.14)</cell><cell>49.82(1.65)</cell></row><row><cell>Table 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Clustering performance on CiteSeer (%).</cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell>NMI</cell><cell>Purity</cell></row><row><cell>KM(1)</cell><cell>21.88(1.12)</cell><cell>10.92(1.93)</cell><cell>22.06(1.08)</cell></row><row><cell>KM(2)</cell><cell>36.13(5.30)</cell><cell>12.98(3.43)</cell><cell>37.18(5.41)</cell></row><row><cell>KM(AllFea)</cell><cell>39.08(6.94)</cell><cell>15.84(4.37)</cell><cell>39.95(5.75)</cell></row><row><cell>Co-train</cell><cell>25.99(0.45)</cell><cell>12.13(0.12)</cell><cell>27.87(0.77)</cell></row><row><cell>Co-reg</cell><cell>21.26(0.03)</cell><cell>10.31(0.13)</cell><cell>21.32(0.00)</cell></row><row><cell>MVKKM</cell><cell>23.15(0.60)</cell><cell>11.62(0.23)</cell><cell>23.74(0.79)</cell></row><row><cell>RMKMC</cell><cell>22.47(0.30)</cell><cell>11.25(0.18)</cell><cell>21.66(0.10)</cell></row><row><cell>MultiNMF</cell><cell>35.11(5.11)</cell><cell>17.76(2.34)</cell><cell>38.76(3.16)</cell></row><row><cell>MVCF</cell><cell>46.58(0.63)</cell><cell>20.30(0.80)</cell><cell>48.34(0.43)</cell></row><row><cell>SCaMVC</cell><cell>22.74(0.73)</cell><cell>11.73(0.56)</cell><cell>23.13(0.83)</cell></row><row><cell>LMSC</cell><cell>28.14(0.04)</cell><cell>16.25(0.17)</cell><cell>32.67(1.25)</cell></row><row><cell>DMVC</cell><cell>24.57(0.26)</cell><cell>12.23(0.78)</cell><cell>26.73(1.41)</cell></row><row><cell>Ours</cell><cell>51.33(1.08)</cell><cell>29.51(1.03)</cell><cell>53.15(3.68)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7</head><label>7</label><figDesc>The clustering accuracy with respect to different layer size.</figDesc><table><row><cell>Data set</cell><cell>HW</cell><cell>BBCSport</cell><cell>Reuters</cell><cell>CiteSeer</cell></row><row><cell>[50 C ]</cell><cell>80.35</cell><cell>65.33</cell><cell>44.92</cell><cell>48.69</cell></row><row><cell>[100 C ]</cell><cell>75.95</cell><cell>53.89</cell><cell>37.83</cell><cell>51.33</cell></row><row><cell>[100 50 C ]</cell><cell>80.10</cell><cell>57.21</cell><cell>46.18</cell><cell>44.98</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>http://mlg.ucd.ie/datasets/3sources.html .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>http://mlg.ucd.ie/datasets/segment.html .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>http://mlg.ucd.ie/datasets/segment.html .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>http://lig-membres.imag.fr/grimal/data.html .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>http://lig-membres.imag.fr/grimal/data.html .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by NSF China (project no. 61572111 ) and Research Fund for the Central Universities of China (project nos. ZYGX2016Z003 , ZYGX2017KYQD177 ).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietik</surname></persName>
		</author>
		<author>
			<persName><surname>Inen</surname></persName>
		</author>
		<author>
			<persName><surname>Topi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach.Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust multi-view data clustering with multi-view capped-norm k-means</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="page" from="197" to="208" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-weighted multi-view clustering with soft capped norm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple partitions aligned clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2701" to="2707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of multi-view representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-layer multi-view topic model for classifying advertising video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="66" to="81" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Auto-weighted multi-view clustering via kernelized graph learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="174" to="184" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A co-training approach for multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-view clustering via joint nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 SIAM International Conference on Data Mining</title>
		<meeting>the 2013 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="252" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ensemble manifold regularized sparse low-rank approximation for multiview feature embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3102" to="3112" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-rank common subspace for multi-view learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Mining</title>
		<meeting>the IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Information-theoretic multi-view domain adaptation: a theoretical and empirical study</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="501" to="525" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-view clustering via canonical correlation analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized latent multi--view subspace clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach.Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical learning of multi-view face detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-view multi-label active learning for image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="258" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-view learning of word embeddings via CCA</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-paced and soft-weighted nonnegative matrix factorization for data representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with adaptive neighbors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust graph regularized nonnegative matrix factorization for clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="483" to="503" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Orthogonal nonnegative matrix t-factorizations for clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deep semi-nmf model for learning hidden representations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1692" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust nonnegative matrix factorization using l21-norm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 20th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="673" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Iteratively reweighted least squares minimization for sparse recovery</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fornasier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Güntürk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parameter-free auto-weighted multiple graph learning: a framework for multiview clustering and semi-supervised classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1881" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On deep multi-view representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-view clustering via deep matrix factorization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2921" to="2927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-view k-means clustering on big data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2598" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient and robust feature selection via joint 2,1 -norms minimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1813" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kernel-based weighted multi-view clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzortzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE International Conference on Data Mining</title>
		<meeting>the 12th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature extraction via multi-view non-negative matrix factorization with local graph regularization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3500" to="3504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive structure discovery for multimedia analysis using multiple features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Practical solutions to the problem of diagonal dominance in kernel document clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An architecture to efficiently learn co-similarities from multi-view datasets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing</title>
		<meeting>the International Conference on Neural Information Processing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">He is a student member of CCF. Zhao Kang Dr. Kang received his Ph.D. degree from the Department of Computer Science at Southern Illinois University in May 2017. At present, Dr. Kang focuses on the research of theory and method design in machine learning, as well as applying those methods on practical problems in compute vision, social network, information retrieval and data mining</title>
		<author>
			<persName><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He published more than 20 papers in top conferences and journals in the area of artificial intelligence and data mining, including AAAI, IJCAI, SIGKDD, TISTM, TKDD, Neurocomputing. At the same time</title>
		<meeting><address><addrLine>China; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science and Engineering, University of Electronic Science and Technology of China ; Information Science and Technology from Southwest Jiaotong University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include semi-supervised learning and ensemble learning. he has been invited to serve as reviewer and member of the Procedural Committee for the top journals and conferences in related fields for times</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Xu&apos;s research interests include machine learning and its applications in information retrieval, health informatics, and social network analysis. He currently serves as an associate editor of Neural Networks, Neurocomputing and Big Data Analytics</title>
		<author>
			<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dr</forename></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of Electronic Science &amp; Technology of China ; Cluster of Excellence at Saarland University and Max Planck Institute for Informatics, and later Purdue University. Dr</orgName>
		</respStmt>
	</monogr>
	<note>He received the Ph.D. degree in computer science and engineering from the Chinese University of Hong Kong. He has been working at Michigan State University. He is the recipient of the outstanding student paper honorable mention of AAAI 2015, the best student paper runner up of ACML 2016, and the 2016 young researcher award from APNNS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
