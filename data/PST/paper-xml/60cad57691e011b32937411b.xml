<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Guided Contrastive Learning for BERT Sentence Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
						</author>
						<author>
							<persName><surname>Yoo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER AI Lab</orgName>
								<address>
									<settlement>Seongnam</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
							<email>sglee@europa.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Guided Contrastive Learning for BERT Sentence Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although BERT and its variants have reshaped the NLP landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers. In this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations. Our method fine-tunes BERT in a self-supervised fashion, does not rely on data augmentation, and enables the usual [CLS] token embeddings to function as sentence vectors. Moreover, we redesign the contrastive learning objective (NT-Xent) and apply it to sentence representation learning. We demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks. We also show it is efficient at inference and robust to domain shifts.</p><p>* This work has been mainly conducted when TK was a research intern at NAVER AI Lab.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained Transformer <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> language models such as BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> and <ref type="bibr">RoBERTa (Liu et al., 2019)</ref> have been integral to achieving recent improvements in natural language understanding. However, it is not straightforward to directly utilize these models for sentencelevel tasks, as they are basically pre-trained to focus on predicting (sub)word tokens given context. The most typical way of converting the models into sentence encoders is to fine-tune them with supervision from a downstream task. In the process, as initially proposed by <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>, a pre-defined token's (a.k.a. [CLS]) embedding from the last layer of the encoder is deemed as the representation of an input sequence. This simple but effective method is possible because, during supervised fine-tuning, the [CLS] embedding functions as the only communication gate between the pre-trained encoder Figure <ref type="figure">1</ref>: BERT(-base)'s layer-wise performance with different pooling methods on the STS-B test set. We observe that the performance can be dramatically varied according to the selected layer and pooling strategy. Our self-guided training (SG / SG-OPT) assures much improved results compared to those of the baselines. and a task-specific layer, encouraging the <ref type="bibr">[CLS]</ref> vector to capture the holistic information.</p><p>On the other hand, in cases where labeled datasets are unavailable, it is unclear what the best strategy is for deriving sentence embeddings from BERT. <ref type="foot" target="#foot_0">1</ref> In practice, previous studies <ref type="bibr" target="#b29">(Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b21">Li et al., 2020;</ref><ref type="bibr" target="#b17">Hu et al., 2020)</ref> reported that naïvely (i.e., without any processing) leveraging the [CLS] embedding as a sentence representation, as is the case of supervised finetuning, results in disappointing outcomes. Currently, the most common rule of thumb for building BERT sentence embeddings without supervision is to apply mean pooling on the last layer(s) of BERT.</p><p>Yet, this approach can be still sub-optimal. In a preliminary experiment, we constructed sentence embeddings by employing various combinations of different BERT layers and pooling methods, and tested them on the Semantic Textual Similarity (STS) benchmark dataset <ref type="bibr" target="#b7">(Cer et al., 2017)</ref>. <ref type="foot" target="#foot_1">2</ref> We discovered that BERT(-base)'s performance, measured in Spearman correlation (× 100), can range from as low as 16.71 <ref type="bibr">([CLS]</ref>, the 10 th layer) to 63.19 (max pooling, the 2 nd layer) depending on the selected layer and pooling method (see <ref type="bibr">Figure</ref>  <ref type="figure">1</ref>). This result suggests that the current practice of building BERT sentence vectors is not solid enough, and that there is room to bring out more of BERT's expressiveness.</p><p>In this work, we propose a contrastive learning method that makes use of a newly proposed selfguidance mechanism to tackle the aforementioned problem. The core idea is to recycle intermediate BERT hidden representations as positive samples to which the final sentence embedding should be close. As our method does not require data augmentation, which is essential in most recent contrastive learning frameworks, it is much simpler and easier to use than existing methods <ref type="bibr" target="#b12">(Fang and Xie, 2020;</ref><ref type="bibr" target="#b38">Xie et al., 2020)</ref>. Moreover, we customize the NT-Xent loss <ref type="bibr" target="#b39">(Chen et al., 2020)</ref>, a contrastive learning objective widely used in computer vision, for better sentence representation learning with BERT. We demonstrate that our approach outperforms competitive baselines designed for building BERT sentence vectors <ref type="bibr" target="#b21">(Li et al., 2020;</ref><ref type="bibr">Wang and Kuo, 2020)</ref> in various environments. With comprehensive analyses, we also show that our method is more computationally efficient than the baselines at inference in addition to being more robust to domain shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Contrastive Representation Learning. Contrastive learning has been long considered as effective in constructing meaningful representations. For instance, <ref type="bibr" target="#b24">Mikolov et al. (2013)</ref> propose to learn word embeddings by framing words nearby a target word as positive samples while others as negative. <ref type="bibr" target="#b22">Logeswaran and Lee (2018)</ref> generalize the approach of <ref type="bibr" target="#b24">Mikolov et al. (2013)</ref> for sentence representation learning. More recently, several studies <ref type="bibr" target="#b12">(Fang and Xie, 2020;</ref><ref type="bibr" target="#b14">Giorgi et al., 2020;</ref><ref type="bibr" target="#b37">Wu et al., 2020)</ref> suggest to utilize contrastive learning for training Transformer models, similar to our approach. However, they generally require data augmentation techniques, e.g., back-translation <ref type="bibr" target="#b30">(Sennrich et al., 2016)</ref>, or prior knowledge on training data such as order information, while our method does not. Furthermore, we focus on revising BERT for computing better sentence embeddings rather than training a language model from scratch.</p><p>On the other hand, contrastive learning has been also receiving much attention from the computer vision community <ref type="bibr" target="#b39">(Chen et al. (2020)</ref>; Chen and He (2020); <ref type="bibr" target="#b16">He et al. (2020)</ref>, inter alia). We improve the framework of <ref type="bibr" target="#b39">Chen et al. (2020)</ref> by optimizing its learning objective for pre-trained Transformerbased sentence representation learning. For extensive surveys on contrastive learning, refer to <ref type="bibr" target="#b20">Le-Khac et al. (2020)</ref> and <ref type="bibr" target="#b18">Jaiswal et al. (2020)</ref>.</p><p>Fine-tuning BERT with Supervision. It is not always trivial to fine-tune pre-trained Transformer models of gigantic size with success, especially when the number of target domain data is limited <ref type="bibr" target="#b26">(Mosbach et al., 2020)</ref>. To mitigate this training instability problem, several approaches <ref type="bibr" target="#b0">(Aghajanyan et al., 2020;</ref><ref type="bibr" target="#b19">Jiang et al., 2020;</ref><ref type="bibr" target="#b39">Zhu et al., 2020)</ref> have been recently proposed. In particular, <ref type="bibr">Gunel et al. (2021)</ref> propose to exploit contrastive learning as an auxiliary training objective during fine-tuning BERT with supervision from target tasks. In contrast, we deal with the problem of adjusting BERT when such supervision is not available.</p><p>Sentence Embeddings from BERT. Since BERT and its variants are originally designed to be fine-tuned on each downstream task to attain their optimal performance, it remains ambiguous how best to extract general sentence representations from them, which are broadly applicable across diverse sentence-related tasks. Following <ref type="bibr" target="#b10">Conneau et al. (2017)</ref>, <ref type="bibr" target="#b29">Reimers and Gurevych (2019)</ref> (SBERT) propose to compute sentence embeddings by conducting mean pooling on the last layer of BERT and then fine-tuning the pooled vectors on the natural language inference (NLI) datasets <ref type="bibr" target="#b6">(Bowman et al., 2015;</ref><ref type="bibr" target="#b35">Williams et al., 2018)</ref>. Meanwhile, some other studies concentrate on more effectively leveraging the knowledge embedded in BERT to construct sentence embeddings without supervision. Specifically, <ref type="bibr">Wang and Kuo (2020)</ref> propose a pooling method based on linear algebraic algorithms to draw sentence vectors from BERT's intermediate layers. <ref type="bibr" target="#b21">Li et al. (2020)</ref> suggest to learn a mapping from the average of the embeddings obtained from the last two layers of BERT to a spherical Gaussian distribution using a flow model, and to leverage the redistributed embeddings in place of the original BERT representations. We follow the setting of <ref type="bibr" target="#b21">Li et al. (2020)</ref> in that we only utilize plain text during training, however, unlike all the others that rely on a certain pooling method even after training, we directly refine BERT so that the typical [CLS] vector can function as a sentence embedding. Note also that there exists concurrent work <ref type="bibr">(Carlsson et al., 2021;</ref><ref type="bibr" target="#b13">Gao et al., 2021;</ref><ref type="bibr" target="#b34">Wang et al., 2021)</ref> whose motivation is analogous to ours, attempting to improve BERT sentence embeddings in an unsupervised fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>As BERT mostly requires some type of adaptation to be properly applied to a task of interest, it might not be desirable to derive sentence embeddings directly from BERT without fine-tuning. While <ref type="bibr" target="#b29">Reimers and Gurevych (2019)</ref> attempt to alleviate this problem with typical supervised fine-tuning, we restrict ourselves to revising BERT in an unsupervised manner, meaning that our method only demands a bunch of raw sentences for training.</p><p>Among possible unsupervised learning strategies, we concentrate on contrastive learning which can inherently motivate BERT to be aware of similarities between different sentence embeddings. Considering that sentence vectors are widely used in computing the similarity of two sentences, the inductive bias introduced by contrastive learning can be helpful for BERT to work well on such tasks. The problem is that sentence-level contrastive learning usually requires data augmentation <ref type="bibr" target="#b12">(Fang and Xie, 2020)</ref> or prior knowledge on training data, e.g., order information <ref type="bibr" target="#b22">(Logeswaran and Lee, 2018)</ref>, to make plausible positive/negative samples. We attempt to circumvent these constraints by utilizing the hidden representations of BERT, which are readily accessible, as samples in the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contrastive Learning with Self-Guidance</head><p>We aim at developing a contrastive learning method that is free from external procedure such as data augmentation. A possible solution is to leverage (virtual) adversarial training <ref type="bibr" target="#b25">(Miyato et al., 2018)</ref> in the embedding space. However, there is no assurance that the semantics of a sentence embedding would remain unchanged when it is added with a random noise. As an alternative, we propose to utilize the hidden representations from BERT's intermediate layers, which are conceptually guaranteed to represent corresponding sentences, as pivots that BERT sentence vectors should be close to or be away from. We call our method as self-guided contrastive learning since we exploit internal training signals made by BERT itself to fine-tune it. We describe our training framework in Figure <ref type="figure" target="#fig_0">2</ref>. First, we clone BERT into two copies, BERT F (fixed) and BERT T (tuned) respectively. BERT F is fixed during training to provide a training signal while BERT T is fine-tuned to construct better sentence embeddings. The reason why we differentiate BERT F from BERT T is that we want to prevent the training signal computed by BERT F from being degenerated as the training procedure continues, which often happens when BERT F = BERT T . This design decision also reflects our philosophy that our goal is to dynamically conflate the knowledge stored in BERT's different layers to produce sentence embeddings, rather than introducing new information via extra training. Note that in our setting, the [CLS] vector from the last layer of BERT T , i.e., c i , is regarded as the final sentence embedding we aim to optimize/utilize during/after fine-tuning.</p><p>Second, given b sentences in a mini-batch, say s 1 , s 2 , • • • , s b , we feed each sentence s i into BERT F and compute token-level hidden representations H i,k ∈ R len(s i )×d :</p><formula xml:id="formula_0">[H i,0 ; H i,1 ; • • • ; H i,k ; • • • ; H i,l ] = BERT F (s i ),</formula><p>where 0 ≤ k ≤ l (0: the non-contextualized layer), l is the number of hidden layers in BERT, len(s i ) is the length of the tokenized sentence, and d is the size of BERT's hidden representations. Then, we apply a pooling function p to H i,k for deriving diverse sentence-level views h i,k ∈ R d from all layers, i.e., h i,k = p(H i,k ). Finally, we choose the final view to be utilized by applying a sampling function σ:</p><formula xml:id="formula_1">h i = σ({h i,k |0 ≤ k ≤ l}).</formula><p>As we have no specific constraints in defining p and σ, we employ max pooling as p and a uniform sampler as σ for simplicity, unless otherwise stated. This simple choice for the sampler implies that each h i,k has the same importance, which is persuasive considering it is known that different BERT layers are specialized at capturing disparate linguistic concepts <ref type="bibr">(Jawahar et al., 2019)</ref>. <ref type="foot" target="#foot_3">3</ref>Third, we compute our sentence embedding c i for s i as follows:</p><formula xml:id="formula_2">c i = BERT T (s i ) [CLS] ,</formula><p>where BERT(•) <ref type="bibr">[CLS]</ref> corresponds to the [CLS] vector obtained from the last layer of BERT. Next, we collect the set of the computed vectors into X = {x|x ∈ {c i } ∪ {h i }}, and for all x m ∈ X, we compute the NT-Xent loss <ref type="bibr" target="#b39">(Chen et al., 2020)</ref>:</p><formula xml:id="formula_3">L base m = − log (φ(x m , µ(x m ))/Z), where φ(u, v) = exp(g(f (u), f (v))/τ ) and Z = 2b n=1,n =m φ(x m , x n ).</formula><p>Note that τ is a temperature hyperparameter, f is a projection head consisting of MLP layers,<ref type="foot" target="#foot_4">4</ref> g(u, v) = u • v/ u v is the cosine similarity function, and µ(•) is the matching function defined as follows,</p><formula xml:id="formula_4">µ(x) = h i if x is equal to c i . c i if x is equal to h i .</formula><p>Lastly, we sum all L base m divided by 2b, and add a regularizer As a result, the final loss L base is:</p><formula xml:id="formula_5">L reg = BERT F − BERT T 𝒄 ! 𝒄 " 𝒉 ! 𝒉 " (1) (2) (3) (4)</formula><formula xml:id="formula_6">L base = 1 2b 2b m=1 L base m + λ • L reg ,</formula><p>where the coefficient λ is a hyperparameter.</p><p>To summarize, our method refines BERT so that the sentence embedding c i has a higher similarity with h i , which is another representation for the sentence s i , in the subspace projected by f while being relatively dissimilar with c j,j =i and h j,j =i . After training is completed, we remove all the components except BERT T and simply use c i as the final sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Objective Optimization</head><p>In Section 3.1, we relied on a simple variation of the general NT-Xent loss, which is composed of four factors. Given sentence s i and s j without loss of generality, the factors are as follows (Figure <ref type="figure" target="#fig_1">3</ref>):</p><p>(1) c i →← h i (or c j →← h j ): The main component that mirrors our core motivation that a BERT sentence vector (c i ) should be consistent with intermediate views (h i ) from BERT. (2) c i ←→ c j : A factor that forces sentence embeddings (c i , c j ) to be distant from each other. (3) c i ←→ h j (or c j ←→ h i ): An element that makes c i being inconsistent with views for other sentences (h j ). (4) h i ←→ h j : A factor that causes a discrepancy between views of different sentences (h i , h j ).</p><p>Even though all the four factors play a certain role, some components may be useless or even cause a negative influence on our goal. For instance, Chen and He (2020) have recently reported that in image representation learning, only (1) is vital while others are nonessential. Likewise, we customize the training loss with three major modifications so that it can be more well-suited for our purpose. First, as our aim is to improve c i with the aid of h i , we re-define our loss focusing more on c i rather than considering c i and h i as equivalent entities:</p><formula xml:id="formula_7">L opt1 i = − log (φ(c i , h i )/ Ẑ),</formula><p>where Ẑ = b j=1,j =i φ(c i , c j ) + b j=1 φ(c i , h j ). In other words, h i only functions as points that c i is encouraged to be close to or away from, and is not deemed as targets to be optimized. This revision naturally results in removing (4). Furthermore, we discover that (2) is also insignificant for improving performance, and thus derive L opt2 i :</p><formula xml:id="formula_8">L opt2 i = − log(φ(c i , h i )/ b j=1 φ(c i , h j ))</formula><p>. Lastly, we diversify signals from ( <ref type="formula">1</ref>) and ( <ref type="formula">3</ref>) by allowing multiple views {h i,k } to guide c i :</p><formula xml:id="formula_9">L opt3 i,k = − log φ(c i ,h i,k ) φ(c i ,h i,k )+ b m=1,m =i l n=0 φ(c i ,hm,n) .</formula><p>We expect with this refinement that the learning objective can provide more precise and fruitful training signals by considering additional (and freely available) samples being provided with. The final form of our optimized loss is:</p><formula xml:id="formula_10">L opt = 1 b(l + 1) b i=1 l k=0 L opt3 i,k + λ • L reg .</formula><p>In Section 5.1, we show the decisions made in this section contribute to improvements in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">General Configurations</head><p>In terms of pre-trained encoders, we leverage BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> for English datasets and MBERT, which is a multilingual variant of BERT, for multilingual datasets. We also employ RoBERTa <ref type="bibr">(Liu et al., 2019)</ref> and SBERT <ref type="bibr" target="#b29">(Reimers and Gurevych, 2019)</ref> in some cases to evaluate the generalizability of tested methods. We use the suffixes '-base' and '-large' to distinguish small and large models. Every trainable model's performance is reported as the average of 8 separate runs to reduce randomness. Hyperparameters are optimized on the STS-B validation set using BERTbase and utilized across different models. See </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Textual Similarity Tasks</head><p>We first evaluate our method and baselines on Semantic Textual Similarity (STS) tasks. Given two sentences, we derive their similarity score by computing the cosine similarity of their embeddings.</p><p>Datasets and Metrics. Following the literature, we evaluate models on 7 datasets in total, that is, STS-B <ref type="bibr" target="#b7">(Cer et al., 2017)</ref>, SICK-R <ref type="bibr" target="#b23">(Marelli et al., 2014)</ref>, and STS12-16 <ref type="bibr" target="#b4">(Agirre et al., 2012</ref><ref type="bibr" target="#b5">(Agirre et al., , 2013</ref><ref type="bibr" target="#b2">(Agirre et al., , 2014</ref><ref type="bibr" target="#b1">(Agirre et al., , 2015</ref><ref type="bibr" target="#b3">(Agirre et al., , 2016))</ref>. These datasets contain pairs of two sentences, whose similarity scores are labeled from 0 to 5. The relevance between gold annotations and the scores predicted by sentence vectors is measured in Spearman correlation (× 100).</p><p>Baselines and Model Specification. We first prepare two non-BERT approaches as baselines, i.e., Glove <ref type="bibr" target="#b28">(Pennington et al., 2014)</ref> mean embeddings and Universal Sentence Encoder (USE; Cer et al. ( <ref type="formula">2018</ref>)). In addition, various methods for BERT sentence embeddings that do not require supervision are also introduced as baselines:</p><p>• CLS token embedding: It regards the [CLS] vector from the last layer of BERT as a sentence representation. • Mean pooling: This method conducts mean pooling on the last layer of BERT and use the output as a sentence embedding. • WK pooling: This follows the method of <ref type="bibr">Wang and Kuo (2020)</ref>, which exploits QR decomposition and extra techniques to derive meaningful sentence vectors from BERT. • Flow: This is BERT-flow proposed by <ref type="bibr" target="#b21">Li et al. (2020)</ref>, which is a flow-based model that maps the vectors made by taking mean pooling on the last two layers of BERT to a Gaussian space.<ref type="foot" target="#foot_6">6</ref> • Contrastive (BT): Following Fang and Xie (2020), we revise BERT with contrastive learning. However, this method relies on back-translation to obtain positive samples, unlike ours. Details about this baseline are specified in Appendix A.2.</p><p>We make use of plain sentences from STS-B to fine-tune BERT using our approach, identical with Flow. <ref type="foot" target="#foot_7">7</ref> We name the BERT instances trained with our self-guided method as Contrastive (SG) and Contrastive (SG-OPT), which utilize L base and L opt in Section 3 respectively.</p><p>Results. We report the performance of different approaches on STS tasks in Table <ref type="table">1 and Table 11</ref> (Appendix A.6). From the results, we confirm the fact that our methods (SG and SG-OPT) mostly outperform other baselines in a variety of experimental settings. As reported in earlier studies, the naïve [CLS] embedding and mean pooling are turned out to be inferior to sophisticated methods.</p><p>To our surprise, WK pooling's performance is even lower than that of mean pooling in most cases, and the only exception is when WK pooling is applied to SBERT-base. Flow shows its strength outperforming the simple strategies. Nevertheless, its performance is shown to be worse than that of our methods (although some exceptions exist in the case of SBERT-large). Note that contrastive learning becomes much more competitive when it is combined with our self-guidance algorithm rather than back-translation. It is also worth mentioning that the optimized version of our method (SG-OPT) generally shows better performance than the basic one (SG), proving the efficacy of learning objective optimization (Section 3.2). To conclude, we demonstrate that our self-guided contrastive learning is effective in improving the quality of BERT sentence embeddings when tested on STS tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multilingual STS Tasks</head><p>We expand our experiments to multilingual settings by utilizing MBERT and cross-lingual zero-shot transfer. Specifically, we refine MBERT using only From Table <ref type="table" target="#tab_2">2</ref>, we see that MBERT with mean pooling already outperforms the best system (at the time of the competition was held) on SemEval-2014 and that our method further boosts the model's performance. In contrast, in the case of SemEval-2017 (Table <ref type="table">3</ref>), MBERT with mean pooling even fails to beat the strong Cosine baseline. <ref type="foot" target="#foot_8">8</ref>However, MBERT becomes capable of outperforming (in English/Spanish) or being comparable with (Arabic) the baseline by adopting our algorithm. We observe that while cross-lingual transfer using MBERT looks promising for the languages analogous to English (e.g., Spanish), its effectiveness may shrink on distant languages (e.g., Arabic). Compared against the best system which is trained on task-specific data, MBERT shows reasonable performance considering that it is never exposed to any labeled STS datasets. In summary, we demonstrate that MBERT fine-tuned with our method has a potential to be used as a simple but effective tool for multilingual (especially European) STS tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SentEval and Supervised Fine-tuning</head><p>We also evaluate BERT sentence vectors using the SentEval <ref type="bibr">(Conneau and Kiela, 2018)</ref> toolkit. Given sentence embeddings, SentEval trains linear classifiers on top of them and estimates the quality of the vectors via their performance (accuracy) on down-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>MR CR SUBJ MPQA SST2 TREC MRPC Avg. stream tasks. Among available tasks, we employ 7: MR, CR, SUBJ, MPQA, SST2, TREC, MRPC. <ref type="foot" target="#foot_9">9</ref>In Table <ref type="table" target="#tab_4">4</ref>, we compare our method (SG-OPT) with two baselines. <ref type="foot" target="#foot_10">10</ref> We find that our method is helpful over usual mean pooling in improving the performance of BERT-like models on SentEval. SG-OPT also outperforms WK pooling on BERTbase/large while being comparable on SBERT-base. From the results, we conjecture that self-guided contrastive learning and SBERT training suggest a similar inductive bias in a sense, as the benefit we earn by revising SBERT with our method is relatively lower than the gain we obtain when fine-tuning BERT. Meanwhile, it seems that WK pooling provides an orthogonal contribution that is effective in the focused case, i.e., SBERT-base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-base</head><p>In addition, we examine how our algorithm impacts on supervised fine-tuning of BERT, although it is not the main concern of this work. Briefly reporting, we identify that the original BERT(-base) and one tuned with SG-OPT show comparable performance on the GLUE <ref type="bibr" target="#b33">(Wang et al., 2019)</ref> validation set, implying that our method does not influence much on BERT's supervised fine-tuning. We refer readers to Appendix A.4 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We here further investigate the working mechanism of our method with supplementary experiments. All the experiments conducted in this section follow the configurations stipulated in Section 4.1 and 4.2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>We conduct an ablation study to justify the decisions made in optimizing our algorithm. To this end, we evaluate each possible variant on the test sets of STS tasks. From Table <ref type="table" target="#tab_5">5</ref>, we confirm that all our modifications to the NT-Xent loss contribute to improvements in performance. Moreover, we show that correct choices for hyperparameters are important for achieving the optimal performance, and that the projection head (f ) plays a significant role as in <ref type="bibr" target="#b39">Chen et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Robustness to Domain Shifts</head><p>Although our method in principle can accept any sentences in training, its performance might be varied with the training data it employs (especially depending on whether the training and test data share the same domain). To explore this issue, we apply SG-OPT on BERT-base by leveraging the mix of NLI datasets <ref type="bibr" target="#b6">(Bowman et al., 2015;</ref><ref type="bibr" target="#b35">Williams et al., 2018)</ref>  that no matter which test set is utilized (STS-B or all the seven STS tasks), our method clearly outperforms Flow in every case, showing its relative robustness to domain shifts. SG-OPT only loses 1.83 (on the STS-B test set) and 1.63 (on average when applied to all the STS tasks) points respectively when trained with NLI rather than STS-B, while Flow suffers from the considerable losses of 12.16 and 4.19 for each case. Note, however, that follow-up experiments in more diverse conditions might be desired as future work, as the NLI dataset inherently shares some similarities with STS tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Computational Efficiency</head><p>In this part, we compare the computational efficiency of our method to that of other baselines. For each algorithm, we measure the time elapsed during training (if required) and inference when tested on STS-B. All methods are run on the same machine (an Intel Xeon CPU E5-2620 v4 @ 2.10GHz and a Titan Xp GPU) using batch size 16. The experimental results specified in Table <ref type="table" target="#tab_6">6</ref> show that although our method demands a moderate amount of time (&lt; 8 min.) for training, it is the most efficient at inference, since our method is free from any post-processing such as pooling once training is completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Representation Visualization</head><p>We visualize a few variants of BERT sentence representations to grasp an intuition on why our method is effective in improving performance. Specifically, we sample 20 positive pairs (red, whose similarity scores are 5) and 20 negative pairs (blue, whose scores are 0) from the STS-B validation set. Then we compute their vectors and draw them on the 2D space with the aid of t-SNE. In Figure <ref type="figure" target="#fig_3">5</ref>, we confirm that our SG-OPT encourages BERT sentence embeddings to be more well-aligned with their positive pairs while still being relatively far from their negative pairs. We also visualize embeddings from SBERT (Figure <ref type="figure" target="#fig_5">6</ref> in Appendix A.5), and identify that our approach and the supervised fine-tuning  used in SBERT provide a similar effect, making the resulting embeddings more suitable for calculating correct similarities between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this section, we discuss a few weaknesses of our method in its current form and look into some possible avenues for future work.</p><p>First, while defining the proposed method in Section 3, we have made decisions on some parts without much consideration about their optimality, prioritizing simplicity instead. For instance, although we proposed utilizing all the intermediate layers of BERT and max pooling in a normal set-ting (indeed, it worked pretty well for most cases), a specific subset of the layers or another pooling method might bring better performance in a particular environment, as we observed in Section 4.4 that we could achieve higher numbers by employing mean pooling and excluding lower layers in the case of SentEval (refer to Appendix A.3 for details). Therefore, in future work, it is encouraged to develop a systematic way of making more optimized design choices in specifying our method by considering the characteristics of target tasks.</p><p>Second, we expect that the effectiveness of contrastive learning in revising BERT can be improved further by properly combining different techniques developed for it. As an initial attempt towards this direction, we conduct an extra experiment where we test the ensemble of back-translation and our self-guidance algorithm by inserting the original sentence into BERT T and its back-translation into BERT F when running our framework. In Table <ref type="table" target="#tab_7">7</ref>, we show that the fusion of the two techniques generally results in better performance, shedding some light on our future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have proposed a contrastive learning method with self-guidance for improving BERT sentence embeddings. Through extensive experiments, we have demonstrated that our method can enjoy the benefit of contrastive learning without relying on external procedures such as data augmentation or back-translation, succeeding in generating higher-quality sentence representations compared to competitive baselines. Furthermore, our method is efficient at inference because it does not require any post-processing once its training is completed, and is relatively robust to domain shifts. We here investigate the impact of our method on typical supervised fine-tuning of BERT models. Concretely, we compare the original BERT with one fine-tuned using our SG-OPT method on the GLUE <ref type="bibr" target="#b33">(Wang et al., 2019)</ref> benchmark. Note that we use the first 10% of the GLUE validation set as the real validation set and the last 90% as the test set, as the benchmark does not officially provide its test data. We report experimental results tested on 5 sub-tasks in Table <ref type="table" target="#tab_8">10</ref>. The results show that our method brings performance improvements for 3 tasks (QNLI, SST2, and RTE). However, it seems that SG-OPT does not influence much on supervised fine-tuning results, considering that the absolute performance gap between the two models is not significant. We leave more analysis on this part as future work.  A.6 RoBERTa's Performance on STS Tasks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Representation Visualization (SBERT)</head><p>In Table <ref type="table">11</ref>, we additionally report the performance of sentence embeddings extracted from RoBERTa using different methods. Our methods, SG and SG-OPT, demonstrate their competitive performance</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Self-guided contrastive learning framework.We clone BERT into two copies at the beginning of training. BERT T (except Layer 0) is then fine-tuned to optimize the sentence vector c i while BERT F is fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Four factors of the original NT-Xent loss. Green and yellow arrows represent the force of attraction and repulsion, respectively. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Domain robustness study. The yellow bars indicate the performance gaps each method has according to which data it is trained with: in-domain (STS-B) or out-of-domain (NLI). Our method (SG-OPT) clearly shows its relative robustness compared to Flow.</figDesc><graphic url="image-6.png" coords="8,82.91,214.55,196.00,146.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sentence representation visualization. (Top) Embeddings from the original BERT. (Bottom) Embeddings from the BERT instance fine-tuned with SG-OPT. Red numbers correspond to positive sentence pairs and blue to negative pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of sentence vectors computed by SBERT-base.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 8</head><label>8</label><figDesc></figDesc><table /><note>in Appendix A.1 for details. Our implementation is based on the HuggingFace's Transformers<ref type="bibr" target="#b36">(Wolf et al., 2019)</ref> and SBERT (Reimers and Gurevych, 2019) library, and publicly available at https://github.com/galsang/SG-BERT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>SemEval-2014  Task 10 Spanish task.</figDesc><table><row><cell>Models</cell><cell>Spanish</cell></row><row><cell>Baseline (Agirre et al., 2014)</cell><cell></cell></row><row><cell>UMCC-DLSI-run2 (Rank #1)</cell><cell>80.69</cell></row><row><cell>MBERT</cell><cell></cell></row><row><cell>+ CLS</cell><cell>12.60</cell></row><row><cell>+ Mean pooling</cell><cell>81.14</cell></row><row><cell>+ WK pooling</cell><cell>79.78</cell></row><row><cell>+ Contrastive (BT)</cell><cell>78.04</cell></row><row><cell>+ Contrastive (SG)</cell><cell>82.09</cell></row><row><cell>+ Contrastive (SG-OPT)</cell><cell>82.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>+ Mean  81.46 86.71 95.37 87.90 85.83 90.30 73.36  85.85 + WK 80.64 85.53 95.27 88.63 85.03 94.03 71.71 85.83 + SG-OPT 82.47 87.42 95.40 88.92 86.20 91.60 74.21 86.60 Experimental results on SentEval.</figDesc><table><row><cell>BERT-large</cell><cell></cell></row><row><cell>+ Mean</cell><cell>84.38 89.01 95.60 86.69 89.20 90.90 72.79 86.94</cell></row><row><cell>+ WK</cell><cell>82.68 87.92 95.32 87.25 87.81 91.18 70.13 86.04</cell></row><row><cell cols="2">+ SG-OPT 86.03 90.18 95.82 87.08 90.73 94.65 73.31 88.26</cell></row><row><cell>SBERT-base</cell><cell></cell></row><row><cell>+ Mean</cell><cell>82.80 89.03 94.07 89.79 88.08 86.93 75.11 86.54</cell></row><row><cell>+ WK</cell><cell>82.96 89.33 95.13 90.56 88.10 91.98 76.66 87.82</cell></row><row><cell cols="2">+ SG-OPT 83.34 89.45 94.68 89.78 88.57 87.30 75.26 86.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>instead of STS-B, and observe the difference. From Figure4, we confirm the fact Computational efficiency tested on STS-B.</figDesc><table><row><cell></cell><cell cols="2">Elapsed Time</cell></row><row><cell>Layer</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Training (sec.)</cell><cell>Inference (sec.)</cell></row><row><cell>BERT-base</cell><cell></cell><cell></cell></row><row><cell>+ Mean pooling</cell><cell>-</cell><cell>13.94</cell></row><row><cell>+ WK pooling</cell><cell>-</cell><cell>197.03 (≈ 3.3 min.)</cell></row><row><cell>+ Flow</cell><cell>155.37 (≈ 2.6 min.)</cell><cell>28.49</cell></row><row><cell cols="2">+ Contrastive (SG-OPT) 455.02 (≈ 7.5 min.)</cell><cell>10.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>23±0.43 68.16±0.50 66.84±0.73 80.13±0.51 71.23±0.40 81.56±0.28 77.17±0.22 74.62±0.25 + Contrastive (BT + SG-OPT) CLS 77.99±0.23 68.75±0.79 68.49±0.38 80.00±0.78 71.34±0.40 81.71±0.29 77.43±0.46 75.10±0.15 Ensemble of the techniques for contrastive learning: back-translation (BT) and self-guidance (SG-OPT).</figDesc><table><row><cell>Models</cell><cell>Pooling</cell><cell>STS-B</cell><cell>SICK-R</cell><cell>STS12</cell><cell>STS13</cell><cell>STS14</cell><cell>STS15</cell><cell>STS16</cell><cell>Avg.</cell></row><row><cell>BERT-base</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ Contrastive (BT)</cell><cell cols="9">CLS 63.27±1.48 66.91±1.29 54.26±1.84 64.03±2.35 54.28±1.87 68.19±0.95 67.50±0.96 62.63±1.28</cell></row><row><cell>+ Contrastive (SG-OPT)</cell><cell cols="2">CLS 77.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Experimental results on a portion of the GLUE validation set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this paper, the term BERT has two meanings: Narrowly, the BERT model itself, and more broadly, pre-trained Transformer encoders that share the same spirit with BERT.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">In the experiment, we employ the settings identical with ones used in Chapter 4. Refer to Chapter 4 for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">2 to prevent BERT T from being too distant from BERT F . 5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">  3  We can also potentially make use of another sampler functions to inject our bias or prior knowledge on target tasks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">.4  We employ a two-layered MLP whose hidden size is 4096. Each linear layer in the MLP is followed by a GELU</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">function.5  To be specific, L reg is the square of the L2 norm of the difference between BERTF and BERTT . As shown in Figure2, we also freeze the 0 th layer of BERTT for stable learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">We restrictively utilize this model, as we find it difficult to exactly reproduce the model's result with its official code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7">For training,<ref type="bibr" target="#b21">Li et al. (2020)</ref> utilize the concatenation of the STS-B training, validation, and test set (without gold annotations). We also follow the same setting for a fair comparison.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8">The Cosine baseline computes its score as the cosine similarity of binary sentence vectors with each dimension representing whether an individual word appears in a sentence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9">Refer toConneau and Kiela (2018)  for each task's spec.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_10">We focus on reporting our own results as we discovered that the toolkit's outcomes can be fluctuating depending on its configuration (we list our settings in Appendix A.3). We also restrict ourselves to evaluating SG-OPT for simplicity, as SG-OPT consistently showed better performance than other contrastive methods in previous experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank anonymous reviewers for their fruitful feedback. We are also grateful to Jung-Woo Ha, Sang-Woo Lee, Gyuwan Kim, and other members in NAVER AI Lab in addition to Reinald Kim Amplayo for their insightful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Table <ref type="table">9</ref>, we stipulate the hyperparameters of the SentEval toolkit used in our experiment. Additionally, we specify some minor modifications applied on our contrastive method (SG-OPT). First, we use the portion of the concatenation of SNLI <ref type="bibr" target="#b6">(Bowman et al., 2015)</ref> and MNLI <ref type="bibr" target="#b35">(Williams et al., 2018)</ref> datasets as the training data instead of STS-B. Second, we do not leverage the first several layers of PLMs when making positive samples, similar to <ref type="bibr">Wang and Kuo (2020)</ref>, and utilize mean pooling instead of max pooling. overall. Note that contrastive learning with backtranslation (BT) also shows its remarkable performance in the case of RoBERTa-base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 GLUE Experiments</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03156</idno>
		<title level="m">Better fine-tuning by reducing representational collapse</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<title level="m">SemEval-2015 task 2: Semantic textual similarity</title>
				<meeting><address><addrLine>English, Spanish</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>SemEval</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In SemEval</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">SEM 2013 shared task: Semantic textual similarity</title>
				<editor>
			<persName><forename type="first">*</forename><surname>Sem</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<editor>
			<persName><forename type="first">Emnlp</forename><surname>Fredrik Carlsson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evangelia</forename><surname>Gogoulou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Erik</forename><surname>Ylipää</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amaru</forename><surname>Cuba Gyllensten</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015. 2021</date>
		</imprint>
	</monogr>
	<note>Semantic re-tuning with contrastive tension</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<title level="m">Universal sentence encoder</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SentEval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
	</analytic>
	<monogr>
		<title level="m">ICML. Xinlei Chen and Kaiming He. 2020. Exploring simple siamese representation learning</title>
				<imprint>
			<date type="published" when="2018">2020. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>LREC</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cert: Contrastive self-supervised learning for language understanding</title>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12766</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<title level="m">Simcse: Simple contrastive learning of sentence embeddings</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Osvald</forename><surname>John M Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">D</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03659</idno>
		<title level="m">Declutr: Deep contrastive learning for unsupervised textual representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Alexis Conneau, and Veselin Stoyanov. 2021. Supervised contrastive learning for pre-trained language model fine-tuning</title>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on contrastive self-supervised learning</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Zaki</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debapriya</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fillia</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><surname>Makedon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00362</idno>
	</analytic>
	<monogr>
		<title level="m">Ganesh Jawahar, Benoît Sagot, and Djamé Seddah</title>
				<imprint>
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>What does BERT learn about the structure of language? In ACL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SMART: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Contrastive representation learning: A framework and review</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Phuc H Le-Khac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><surname>Smeaton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li ; Naman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<editor>
			<persName><surname>Emnlp. Yinhan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Myle</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ott</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: a regularization method for supervised and semisupervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04884</idno>
		<title level="m">On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facebook fair&apos;s wmt19 news translation task submission</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyra</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
				<meeting>the Fourth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ECNU at SemEval-2017 task 1: Leverage kernel-based traditional NLP features and neural networks to build a universal model for multilingual and cross-lingual semantic textual similarity</title>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017">2017. SemEval-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06652</idno>
	</analytic>
	<monogr>
		<title level="m">Sbert-wk: A sentence embedding method by dissecting bert-based word models</title>
				<editor>
			<persName><forename type="first">Iclr</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06979</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Clear: Contrastive learning for sentence representation</title>
		<author>
			<persName><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15466</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Freelb: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. Models Pooling STS-B SICK-R STS12 STS13 STS14 STS15 STS16 Avg</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><surname>Contrastive</surname></persName>
		</author>
		<idno>BT) CLS 79.93±1.08 71.97±1.00 62.34±2.41 78.60±1.74 68.65±1.48 79.31±0.65 77.49±1.29 74.04±1.16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Contrastive</surname></persName>
		</author>
		<idno>SG) CLS 78.38±0.43 69.74±1.00 62.85±0.88 78.37±1.55 68.28±0.89 80.42±0.65 77.69±0.76 73.67±0.62</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Contrastive</surname></persName>
		</author>
		<idno>SG-OPT) CLS 77.60±0.30 68.42±0.71 62.57±1.12 78.96±0.67 69.24±0.44 79.99±0.44 77.17±0.24 73.42±0.31</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Contrastive</surname></persName>
		</author>
		<idno>BT) CLS 77.05±1.22 67.83±1.34 57.60±3.57 72.14±1.16 62.25±2.10 71.49±3.24 71.75±1.73 68.59±1.53</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><surname>Contrastive</surname></persName>
		</author>
		<idno>SG) CLS 76.15±0.54 66.07±0.82 64.77±2.52 71.96±1.53 64.54±1.04 78.06±0.52 75.14±0.94 70.95±1.13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><surname>Contrastive</surname></persName>
		</author>
		<idno>SG-OPT) CLS 78.14±0.72 67.97±1.09 64.29±1.54 76.36±1.47 68.48±1.58 80.10±1.05 76.60±0.98 73.13±1.20</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
