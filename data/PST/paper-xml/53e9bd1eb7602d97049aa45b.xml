<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HMM-Based Word Spotting in Handwritten Documents Using Subword Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Fischer</surname></persName>
							<email>afischer@iam.unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<addrLine>Neubrückstrasse 10</addrLine>
									<postCode>3012</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Keller</surname></persName>
							<email>andreas.keller@students.unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<addrLine>Neubrückstrasse 10</addrLine>
									<postCode>3012</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Volkmar</forename><surname>Frinken</surname></persName>
							<email>frinken@iam.unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<addrLine>Neubrückstrasse 10</addrLine>
									<postCode>3012</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
							<email>bunke@iam.unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<addrLine>Neubrückstrasse 10</addrLine>
									<postCode>3012</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HMM-Based Word Spotting in Handwritten Documents Using Subword Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7FC3ABC7203BA1A796123BEA9CA25E7D</idno>
					<idno type="DOI">10.1109/ICPR.2010.834</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Handwriting recognition; Hidden Markov models;</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Handwritten word spotting aims at making document images amenable to browsing and searching by keyword retrieval. In this paper, we present a word spotting system based on Hidden Markov Models (HMM) that uses trained subword models to spot keywords. With the proposed method, arbitrary keywords can be spotted that do not need to be present in the training set. Also, no text line segmentation is required. On the modern IAM off-line database and the historical George Washington database we show that the proposed system outperforms a standard template matching approach based on dynamic time warping (DTW).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Offline recognition of unconstrained cursively handwritten text is still a widely unsolved problem and an active area of research. For large vocabularies and different writing styles in general <ref type="bibr" target="#b0">[1]</ref>, and for degraded historical manuscripts in particular <ref type="bibr" target="#b1">[2]</ref>, the accuracy of an automatic transcription is far from being perfect. Under these conditions, word spotting has been proposed instead of a complete recognition for the restricted task of retrieving keywords from document images. In this way, the documents under consideration become amenable to searching and browsing <ref type="bibr" target="#b2">[3]</ref>.</p><p>Two different approaches for handwritten word spotting can be distinguished. On the one hand, template-based methods match a query image with labeled template images using, e.g., holistic gradient-based binary features (GSC) <ref type="bibr" target="#b3">[4]</ref> or local information by means of dynamic time warping (DTW) <ref type="bibr" target="#b4">[5]</ref>. Typically, no training is needed for templatebased methods and word template images are rather easy to obtain.</p><p>Learning-based methods, on the other hand, train keyword models for scoring query images. In <ref type="bibr" target="#b5">[6]</ref>, a very general approach is introduced for handwritten word spotting, based on Hidden Markov Models (HMM). It is proposed to train one HMM for each keyword separately. Obviously, this method is limited in three aspects. Firstly, the keywords are required to appear in the training set. Secondly, for each keyword a considerable amount of data is required for training the word HMM. And thirdly, the method is dependent on line segmentation.</p><p>When training subword models, e.g., letters rather than words, only a small number of models need to be trained. They can be used to spot arbitrary keywords that are not required to be present in the training set. In <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, generalized HMM (gHMM) letter models are trained based on a small set of manually extracted letter template images. This approach is particularly appropriate when no aligned transcription is available for training. However, only a small variety of letter shapes is considered and letter template images are rather difficult to obtain.</p><p>In this paper, we present a learning-based word spotting system that uses HMM subword models. This approach is inspired by the successful application of letter HMMs for handwriting recognition <ref type="bibr" target="#b8">[9]</ref>. The proposed method is based on an aligned transcription for training, is able to spot arbitrary keywords, and is segmentation-free, i.e., text lines are not required to be segmented into words. On the publicly available modern IAM off-line database (IAMDB) and the historical George Washington database (GWDB) it is demonstrated that the proposed method outperforms a standard template-based approach using DTW.</p><p>In the remainder of this paper, HMM-based word spotting is discussed in detail in Section II, the DTW reference system is introduced in Section III, Section IV presents the experimental results, and Section V draws some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HMM-BASED WORD SPOTTING</head><p>The aim of word spotting is to retrieve keywords from document images. In this paper, we focus on handwritten documents, such as letters or historical manuscripts, and consider complete text line images for keyword matching, i.e., no segmentation of text lines into words is required. Based on Hidden Markov Models (HMM), the proposed word spotting system uses a sequence of letter models to represent text line images and returns a score for each text line that is based on the likelihood ratio between a keyword model and a filler model representing general non-keyword text. If this score is greater than a certain threshold, the text line is returned as a positive match.</p><p>In the following the proposed system is discussed in detail. In Section II-A, image normalization and feature extraction are addressed, Section II-B discusses HMM training and decoding, and Section II-C presents text line scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preprocessing</head><p>For word spotting, binary text line images are considered that are normalized prior to feature extraction in order to cope with different writing styles. As proposed in <ref type="bibr" target="#b8">[9]</ref>, the skew, i.e., the inclination of the text line, is removed first by rotation. Then, the slant, i.e., the inclination of the letters, is removed using a shear transformation. Next, a vertical scaling procedure is applied to normalize the height with respect to the lower and upper baseline and finally, horizontal scaling normalizes the width of the text line with respect to the estimated number of letters.</p><p>For algorithmic processing, a normalized text line image is represented by a sequence of N feature vectors x 1 , . . . , x N with x i ∈ IR n . This sequence is extracted by a sliding window of one pixel width moving from left to right over the image. At each of the N positions of the sliding window, n = 9 geometrical features are extracted. These features were originally presented in <ref type="bibr" target="#b8">[9]</ref>. Three global features capture the fraction of black pixels, the center of gravity, and the second order moment. The remaining six local features consist of the position of the upper and lower contour, the gradient of the upper and lower contour, the number of black-white transitions, and the fraction of black pixels between the contours.</p><p>For more details on image normalization and feature extraction, we refer to <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hidden Markov Models</head><p>The proposed word spotting system is based on Hidden Markov Models (HMM) of individual letters. Each letter model has a certain number m of hidden states s 1 , . . . , s m arranged in a linear topology as shown in Figure <ref type="figure" target="#fig_0">1a</ref>. The states s j with 1 ≤ j ≤ m emit observable feature vectors x ∈ IR n with output probability distributions p sj (x) given by a mixture of Gaussians. Starting from the first state s 1 , the model either rests in a state or changes to the next state with transition probabilities P (s j , s j ) and P (s j , s j+1 ), respectively.</p><p>The letter models are trained using labeled text line images. First, a text line model is created as a sequence of letter models according to the transcription. Then, the probability of this text line model to emit the observed feature vector sequence x 1 , . . . , x N is maximized by iteratively adapting the initial output probability distributions p sj (x) and the transition probabilities P (s j , s j ) and P (s j , s j+1 ) with the Baum-Welch algorithm <ref type="bibr" target="#b9">[10]</ref>.</p><p>Using trained letter models, different text line models can be created. In Figure <ref type="figure" target="#fig_0">1b</ref>, an unconstrained filler model of a text line is shown that represents an arbitrary sequence of letters. For a given text line model, the likelihood of the observed feature vector sequence x 1 , . . . , x N is calculated, using the Viterbi algorithm <ref type="bibr" target="#b9">[10]</ref>, and output as a result the most likely letter sequence together with the start and end position of the letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Text Line Scoring</head><p>The score for word spotting is based on the likelihood ratio between two text line models. The first model is the keyword model shown in Figure <ref type="figure" target="#fig_0">1c</ref> for the keyword "word" and the second model is the filler model shown in Figure <ref type="figure" target="#fig_0">1b</ref>. The keyword model is constrained to contain the exact keyword letter sequence at the beginning, in the middle, or at the end of the text line, respectively, separated with the space character "sp". A common approach to scoring query images for keyword spotting is given by the likelihood ratio R = LK LF between the likelihood L K of the keyword model and the likelihood L F of the filler model. R is bounded by 0 ≤ R ≤ 1 in our case, because the same letter models  The final text line score is obtained by normalizing the likelihood ratio with the length l K = i ei s of the keyword, given by the difference of its end position i e and start position i s . Independently of the keyword, a positive match is returned if this normalized score is greater than a given threshold T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R l K</head><p>&gt; T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. REFERENCE SYSTEM</head><p>The performance of the proposed word spotting system is compared with a standard word image matching approach described in <ref type="bibr" target="#b4">[5]</ref> that is based on Dynamic Time Warping (DTW). For the feature vector sequence representations x and y of two text images, DTW finds an alignment with minimum cost between x and y by means of dynamic programming. Here, each feature vector of the first sequence is mapped to one or several feature vectors of the second sequence. Each mapping produces a cost with respect to the distance of the mapped feature vectors. In our implementation, we use the feature vector representation described in Section II-A and employ the squared Euclidean distance d(x, y) = n i=1 (x iy i ) 2 as the alignment cost. The word image score is then given by the minimum alignment cost of all keyword images present in the training set, normalized with the length of the alignment path. The text line score is finally given by the minimum word image score calculated over all words of the text line.</p><p>Note that unlike the proposed system, the reference system requires the text lines to be segmented into words and the keyword to be present in the training set. Disregarding possible segmentation errors, we used perfectly segmented words for the evaluation of the reference system. If a keyword is not present in the training set, a negative match is returned. Table <ref type="table">I</ref>: Mean precision on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>We used two different data sets for testing. The IAM off-line database (IAMDB) <ref type="bibr" target="#b10">[11]</ref> consists of 1,539 pages of handwritten modern English text, written by 657 writers. From this database, 6,161 writer independent text lines are used for training and 929 for testing. Among the 4,000 most frequent words, 3,421 non stop words <ref type="bibr" target="#b11">[12]</ref> are used as keywords.</p><p>The George Washington database (GWDB) <ref type="bibr" target="#b12">[13]</ref> consists of 20 pages of historical letters, written by George Washington and his associates. Due to the small size of this database (it includes only 675 text lines altogether), a four-fold cross validation is performed and the results are averaged. All 1,067 non stop words are used as keywords.</p><p>In Figure <ref type="figure" target="#fig_2">2a</ref> and 2b, sample text line images are shown for the IAMDB and the GWDB, respectively. The binary images result from preprocessing as discussed in Section II-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>While for DTW no training is required, the HMMs are trained on the IAMDB with an optimal number of states and Gaussian mixtures adopted from previous work <ref type="bibr" target="#b8">[9]</ref>. For the GWDB, the number of states and Gaussian mixtures was optimized with respect to text line recognition on the validation set based on a closed vocabulary without using a language model.</p><p>For evaluating the performance of the word spotting systems, all observed text line scores are used as possible thresholds. If the text line score is better than a given threshold (greater for HMM and lower for DTW), the text line is returned as a positive match. From the number of true positives (TP), false positives (FP), and false negatives (FN), the precision T P T P +F P and the recall T P T P +F N are calculated for each threshold. The proposed HMM-based system and the DTW-based reference system are then compared based on their mean precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>In Figure <ref type="figure" target="#fig_3">3</ref>, the precision-recall curves are plotted for both data sets and in Table <ref type="table">I</ref>, the corresponding mean precision is listed. Note that in case of the GWDB, the average precision from the four-fold cross validation is reported. The proposed word spotting system clearly outperforms the reference system on both datasets. While the precision of the DTW system quickly drops as the recall value increases, the precision of the HMM system remains considerably higher which leads to a higher mean precision. When comparing the two systems with each other, one should furthermore keep in mind that the DTW system is based on a (perfect) manual segmentation of a text line into individual words, while the HMM system operates completely automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we presented a learning-based approach for handwritten word spotting based on HMM subword models. With this approach, arbitrary keywords can be spotted and text line images are not required to be segmented into words. On the modern IAMDB as well as on the historical GWDB, the proposed system was compared with a standard templatebased reference system using DTW.</p><p>Two difficult tasks were investigated for evaluating the proposed learning-based word spotting system. The first task was to spot 3,421 keywords in the unconstrained modern IAMDB database using a large amount of training data. The second task was to spot 1,067 keywords in the small historical GWDB, making use of only a small number of training data. For both tasks it was demonstrated that the proposed learning-based system clearly outperformed the template-based reference system. Note, however, that for training the HMMs a transcription is needed that is aligned with the text lines.</p><p>Future research includes the optimization of the score normalization, i.e., the filler model, the investigation of language models, and a broader comparison with other word spotting systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hidden Markov Models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample Text Line Images</figDesc><graphic coords="3,314.95,68.59,250.24,175.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3: Precision-Recall Curves System IAMDB GWDB HMM 0.125 0.315 DTW 0.023 0.151</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENTS This work has been supported by the Swiss National Science Foundation (Project CRSI22 125220/1) and by the Swiss National Center of Competence in Research (NCCR) on Interactive Multimodal Information Management (IM2).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Off-line Roman cursive handwriting recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Varga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Document Processing: Major Directions and Recent Advances, ser. Advances in Pattern Recognition</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Special issue on the analysis of historical documents</title>
	</analytic>
	<monogr>
		<title level="j">Int. Journal on Document Analysis and Recognition</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Antonacopoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Downton</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="75" to="77" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Word Spotting: a New Approach to Indexing Handwriting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="631" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word Image Retrieval Using Binary Features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Document Recognition and Retrieval XI</title>
		<meeting>Document Recognition and Retrieval XI</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5296</biblScope>
			<biblScope unit="page" from="45" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word spotting for historical documents</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="139" to="152" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Handwritten word-spotting using hidden Markov models and universal vocabularies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2106" to="2116" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Searching off-line arabic documents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ziftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1455" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making Latin Manuscripts Searchable using gHMM&apos;s</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vesom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="385" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using a statistical language model to improve the performance of an HMM-based cursive handwriting recognition system</title>
		<author>
			<persName><forename type="first">U.-V</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Pattern Rec. and Art. Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="65" to="90" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989-02">Feb. 1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The IAM-database: an English sentence database for off-line handwriting recognition</title>
		<author>
			<persName><forename type="first">U.-V</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The SMART Retrieval System-Experiments in Automatic Document Processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall, Inc</publisher>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Holistic word recognition for handwritten historical documents</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Workshop on Document Image Analysis for Libraries</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
