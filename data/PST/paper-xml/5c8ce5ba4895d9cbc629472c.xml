<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Convolutional Framelets: A General Deep Learning Framework for Inverse Problems *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-04-24">April 24, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Bio and Brain Engineering</orgName>
								<orgName type="laboratory">Corresponding author. Bio Imaging and Signal Processing Laboratory</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoseob</forename><surname>Han</surname></persName>
							<email>hanyoseob@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Bio and Brain Engineering</orgName>
								<orgName type="laboratory">Bio Imaging and Signal Processing Laboratory</orgName>
								<address>
									<country>Korea Advanced</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Science and Technology</orgName>
								<address>
									<addrLine>Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eunju</forename><surname>Cha</surname></persName>
							<email>eunju.cha@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Bio and Brain Engineering</orgName>
								<orgName type="laboratory">Bio Imaging and Signal Processing Laboratory</orgName>
								<address>
									<country>Korea Advanced</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Science and Technology</orgName>
								<address>
									<addrLine>Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Convolutional Framelets: A General Deep Learning Framework for Inverse Problems *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-04-24">April 24, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">69DA51A194E8F294AB29FBCA8F6F8727</idno>
					<idno type="DOI">10.1137/17M1141771</idno>
					<note type="submission">Received by the editors August 2, 2017; accepted for publication (in revised form) January 22, 2018;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolutional neural network</term>
					<term>framelets</term>
					<term>deep learning</term>
					<term>inverse problems</term>
					<term>ReLU</term>
					<term>perfect reconstruction condition AMS subject classifications. Primary</term>
					<term>94A08</term>
					<term>97R40</term>
					<term>94A12</term>
					<term>92C55</term>
					<term>65T60</term>
					<term>42C40; Secondary</term>
					<term>44A12</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep learning approaches with various network architectures have achieved significant performance improvement over existing iterative reconstruction methods in various imaging problems. However, it is still unclear why these deep learning architectures work for specific inverse problems. Moreover, in contrast to the usual evolution of signal processing theory around the classical theories, the link between deep learning and the classical signal processing approaches, such as wavelets, nonlocal processing, and compressed sensing, are not yet well understood. To address these issues, here we show that the long-sought missing link is the convolution framelets for representing a signal by convolving local and nonlocal bases. The convolution framelets were originally developed to generalize the theory of low-rank Hankel matrix approaches for inverse problems, and this paper further extends this idea so as to obtain a deep neural network using multilayer convolution framelets with perfect reconstruction (PR) under rectified linear unit (ReLU) nonlinearity. Our analysis also shows that the popular deep network components such as residual blocks, redundant filter channels, and concatenated ReLU (CReLU) do indeed help to achieve PR, while the pooling and unpooling layers should be augmented with high-pass branches to meet the PR condition. Moreover, by changing the number of filter channels and bias, we can control the shrinkage behaviors of the neural network. This discovery reveals the limitations of many existing deep learning architectures for inverse problems, and leads us to propose a novel theory for a deep convolutional framelet neural network. Using numerical experiments with various inverse problems, we demonstrate that our deep convolutional framelets network shows consistent improvement over existing deep architectures. This discovery suggests that the success of deep learning stems not from a magical black box, but rather from the power of a novel signal representation using a nonlocal basis combined with a data-driven local basis, which is indeed a natural extension of classical signal processing theory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction. Deep learning approaches have achieved tremendous success in classification problems <ref type="bibr" target="#b43">[43]</ref> as well as low-level computer vision problems such as segmentation <ref type="bibr" target="#b58">[58]</ref>, denoising <ref type="bibr" target="#b75">[75]</ref>, and superresolution <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b60">60]</ref>, among others. The theoretical origin of their success has been investigated <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b62">62]</ref>, and the exponential expressivity under a given network complexity (in terms of the Vapnik-Chervonenkis (VC) dimension <ref type="bibr" target="#b2">[3]</ref> or the Rademacher complexity <ref type="bibr" target="#b5">[5]</ref>) has often been attributed to this success. A deep network is also known to be able to learn high-level abstractions/features of the data similar to visual processing in the human brain using multiple layers of neurons with nonlinearity <ref type="bibr" target="#b46">[46]</ref>.</p><p>Inspired by the success of deep learning in low-level computer vision, several machine learning approaches have recently been proposed for image reconstruction problems. In X-ray computed tomography (CT), Kang, Min, and Ye <ref type="bibr" target="#b38">[38]</ref> provided the first systematic study of a deep convolutional neural network (CNN) for low-dose CT and showed that a deep CNN using directional wavelets is more efficient in removing low-dose-related CT noise. Unlike these low-dose artifacts from reduced tube currents, the streaking artifacts originating from sparse projection views show globalized artifacts that are difficult to remove using conventional denoising CNNs <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b51">51]</ref>. Han, Yoo, and Ye <ref type="bibr" target="#b28">[28]</ref> and Jin et al. <ref type="bibr" target="#b34">[34]</ref> independently proposed residual learning using U-Net <ref type="bibr" target="#b58">[58]</ref> to remove the global streaking artifacts caused by sparse projection views. In magnetic resonance imaging (MRI), Wang et al. <ref type="bibr" target="#b66">[66]</ref> were the first to apply deep learning to compressed sensing MRI (CS-MRI). They trained the deep neural network from downsampled reconstruction images to learn a fully sampled reconstruction. Then they used the deep learning result either as an initialization or as a regularization term in classical compressed sensing (CS) approaches. A multilayer perceptron was developed for accelerated parallel MRI <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b44">44]</ref>. A deep network architecture using an unfolded iterative CS algorithm was also proposed <ref type="bibr" target="#b24">[24]</ref>. Instead of using handcrafted regularizers, the authors <ref type="bibr" target="#b24">[24]</ref> tried to learn a set of optimal regularizers. Domain adaptation from a sparse-view CT network to projection reconstruction MRI was also proposed <ref type="bibr" target="#b29">[29]</ref>. These pioneering works have consistently demonstrated impressive reconstruction performance, which is often superior to that of existing iterative approaches. However, the more impressive empirical results we have observed in image reconstruction problems, the more unanswered questions we encounter. For example, to the best of our knowledge, we do not have complete answers to the following questions that are critical to a network design.</p><p>1. What is the role of the filter channels in convolutional layers? 2. Why do some networks need fully connected layers whereas others do not? 3. What is the role of a nonlinearity such as the rectified linear unit (ReLU)? 4. Why do we need pooling and unpooling in some architectures? 5. What is the role of the bypass connection or residual network? 6. How many layers do we need? Furthermore, the most troubling issue for the signal processing community is that the link to classical signal processing theory is still not fully understood. For example, wavelets <ref type="bibr" target="#b17">[17]</ref> have been extensively investigated as an efficient signal representation theory for many image processing applications by exploiting the energy compaction property of wavelet bases. Compressed sensing theory <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b14">14]</ref> has further extended the idea to demonstrate that an accurate recovery is possible from undersampled data if the signal is sparse in some frames and the sensing matrix is incoherent. Nonlocal image processing techniques such as nonlocal means <ref type="bibr" target="#b8">[8]</ref> and BM3D <ref type="bibr" target="#b16">[16]</ref> have also demonstrated impressive performance for many image processing applications. The link between these algorithms has been extensively studied in the last few years using various mathematical tools from harmonic analysis, convex optimiza-tion, etc. However, recent years have witnessed that a blind application of deep learning toolboxes sometimes provides even better performance than mathematics-driven classical signal processing approaches. Does this imply a dark age in signal processing or a new opportunity?</p><p>The main goal of this paper is to address these open questions. In fact, our paper is not the only attempt to address these issues. For instance, Papyan, Romano, and Elad <ref type="bibr" target="#b55">[55]</ref> showed that once ReLU nonlinearity is employed, the forward pass of a network can be interpreted as a deep sparse coding algorithm. Wiatowski and Bölcskei <ref type="bibr" target="#b68">[68]</ref> discussed the importance of pooling for networks, proving that it leads to translation invariance. Moreover, several works including <ref type="bibr" target="#b22">[22]</ref> provided explanations for residual networks. The interpretation of a deep network in terms of unfolded (or unrolled) sparse recovery is another prevailing view in the research community <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b34">34]</ref>. However, this interpretation still does not give answers to several key questions; for example, why do we need multichannel filters? Therefore, in this paper we depart from these existing views and propose a new interpretation of a deep network as a novel signal representation scheme. In fact, signal representation theory, such as wavelets and frames, has been an active area of research for many years <ref type="bibr" target="#b49">[49]</ref>, and Mallat <ref type="bibr" target="#b50">[50]</ref> and Bruna and Mallat <ref type="bibr" target="#b7">[7]</ref> proposed the wavelet scattering network as a translation-invariant and deformation-robust image representation. However, this approach does not have the learning components found in existing deep learning networks.</p><p>Then what is missing here? One of the most important contributions of our work is to show that the geometry of deep learning can be revealed by lifting a signal to a high-dimensional space using the Hankel structured matrix. More specifically, many types of input signals that occur in signal processing can be factored into the left and right bases as well as into a sparse matrix with energy compaction properties when lifted into the Hankel structured matrix. This results in a frame representation of the signal using the left and right bases, referred to as the nonlocal and local basis matrices, respectively. The origin of this nomenclature will become clear later. One of our novel contributions is the realization that the nonlocal basis determines the network architecture such as pooling/unpooling, while the local basis allows the network to learn convolutional filters. More specifically, the application-specific domain knowledge leads to a better choice of the nonlocal basis on which to learn the local basis to maximize the performance.</p><p>In fact, the idea of exploiting the two bases by the so-called convolution framelets was originally proposed by Yin et al. <ref type="bibr" target="#b73">[73]</ref>. However, the aforementioned close link to the deep neural network was not revealed in <ref type="bibr" target="#b73">[73]</ref>. Most importantly, we demonstrate for the first time that the convolution framelet representation can be equivalently represented as an encoder-decoder convolution layer, and multilayer convolution framelet expansion is also feasible by relaxing the conditions in <ref type="bibr" target="#b73">[73]</ref>. Furthermore, we derive the perfect reconstruction (PR) condition under the rectified linear unit (ReLU). The mysterious role of the redundant multichannel filters can then be easily understood as an important tool to meet the PR condition. Moreover, by augmenting local filters with paired filters with opposite phase, the ReLU nonlinearity disappears and the deep convolutional framelet becomes a linear signal representation. However, in order for the deep network to satisfy the PR condition, the number of channels should increase exponentially along the layer, which is difficult to achieve in practice. Interestingly, we can show that an insufficient number of filter channels results in shrinkage behavior via a low-rank approximation of an extended Hankel matrix, and this shrinkage behavior can Downloaded 04/24/18 to 131.172. <ref type="bibr">36.29</ref>. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php be exploited to maximize network performance. Finally, to overcome the limitation of the pooling and unpooling layers, we introduce a multiresolution analysis (MRA) for convolution framelets using a wavelet nonlocal basis for generalized pooling/unpooling. We call the new class of deep networks using convolution framelets the deep convolutional framelets.</p><p>1.1. Notation. For a matrix A, R(A) denotes the range space of A and N (A) refers to the null space of A. P R(A) denotes the projection to the range space of A, whereas P ⊥ R(A) denotes the projection to the orthogonal complement of R(A). The notation 1 n denotes an n-dimensional vector with 1's. The n × n identity matrix is referred to as I n×n . For a given matrix A ∈ R m×n , the notation A † refers to the generalized inverse. The superscript of A denotes the Hermitian transpose. Because we are mainly interested in real valued cases, is equivalent to the transpose T . The inner product in the matrix space is defined by A, B = Tr(A B), where A, B ∈ R n×m . For a matrix A, A F denotes its Frobenius norm. For a given matrix C ∈ R n×m , c j denotes its jth column, and c ij the (i, j) elements of C. If a matrix Ψ ∈ R pd×q is partitioned as Ψ = [Ψ 1 • • • Ψ p ] with submatrix Ψ i ∈ R d×q , then ψ i j refers to the jth column of Ψ i . A vector v ∈ R n is referred to as the flipped version of a vector v ∈ R n , i.e., its indices are reversed. Similarly, for a given matrix Ψ ∈ R d×q , the notation Ψ ∈ R d×q refers to a matrix composed of flipped vectors, i.e., Ψ = [ψ 1 • • • ψ q ]. For a block-structured matrix Ψ ∈ R pd×q , with a slight abuse of notation, we define Ψ as</p><formula xml:id="formula_0">Ψ =    Ψ 1 . . . Ψ p    , where Ψ i = ψ i 1 • • • ψ i q ∈ R d×q .<label>(1)</label></formula><p>Finally, Table <ref type="table" target="#tab_1">1</ref> summarizes the notation used throughout the paper.</p><p>2. Mathematics of the Hankel matrix. Since the Hankel structured matrix is the key component in our theory, in this section we discuss various properties of the Hankel matrix that will be used extensively throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hankel matrix representation of convolution.</head><p>Hankel matrices arise repeatedly from many different contexts in signal processing and control theory, such as system identification <ref type="bibr" target="#b20">[20]</ref>, harmonic retrieval, array signal processing <ref type="bibr" target="#b32">[32]</ref>, and subspace-based channel identification <ref type="bibr" target="#b63">[63]</ref>. A Hankel matrix can also be obtained from a convolution operation <ref type="bibr" target="#b71">[71]</ref>, which is of particular interest in this paper. Here, to avoid special treatment of the boundary condition, our theory is mainly derived using the circular convolution.</p><p>Let</p><formula xml:id="formula_1">f = [f [1], . . . , f [n]] T ∈ R n and ψ = [ψ[1], . . . , ψ[d]] T ∈ R d .</formula><p>Then, a single-input single-output (SISO) convolution of the input f and the filter ψ can be represented in matrix form:</p><formula xml:id="formula_2">y = f ψ = H d (f )ψ, (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where H d (f ) is a wrap-around Hankel matrix </p><formula xml:id="formula_4">H d (f ) =      f [1] f [2] • • • f [d] f [2] f [3] • • • f [d + 1] . . . . . . . . . . . . f [n] f [1] • • • f [d -1]      . (<label>3</label></formula><formula xml:id="formula_5">C d (•) n × d-circulant matrix</formula><p>Similarly, a single-input multi-output (SIMO) convolution using q filters ψ 1 , . . . , ψ q ∈ R d can be represented by</p><formula xml:id="formula_6">Y = f Ψ = H d (f )Ψ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">Y := y 1 • • • y q ∈ R n×q , Ψ := ψ 1 • • • ψ q ∈ R d×q .</formula><p>On the other hand, multi-input multi-output (MIMO) convolution for the p-channel input Z = [z 1 , . . . , z p ] can be represented by</p><formula xml:id="formula_8">y i = p j=1 z j ψ j i , i = 1, . . . , q,<label>(5)</label></formula><p>where p and q are the number of input and output channels, respectively, and ψ j i ∈ R d denotes the length d filter that convolves the jth channel input to compute its contribution to the ith output channel. By defining the MIMO filter kernel Φ as</p><formula xml:id="formula_9">Ψ =    Ψ 1 . . . Ψ p    , where Ψ j = ψ j 1 • • • ψ j q ∈ R d×q ,<label>(6)</label></formula><p>Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php the corresponding matrix representation of the MIMO convolution is then given by</p><formula xml:id="formula_10">Y = Z Ψ (7) = p j=1 H d (z j )Ψ j (8) = H d|p (Z) Ψ, (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>where Ψ is a flipped block-structured matrix in the sense of <ref type="bibr" target="#b0">(1)</ref>, and H d|p (Z) is an extended Hankel matrix by stacking p Hankel matrices side by side:</p><formula xml:id="formula_12">H d|p (Z) := H d (z 1 ) H d (z 2 ) • • • H d (z p ) . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>For notational simplicity, we denote H d|1 ([z]) = H d (z). Figure <ref type="figure" target="#fig_0">1</ref> illustrates the procedure for constructing an extended Hankel matrix from [z 1 , z 2 , z 3 ] ∈ R 8×3 when the convolution filter length d is 2.</p><p>Finally, as a special case of MIMO convolution for q = 1, the multi-input single-output (MISO) convolution is defined by</p><formula xml:id="formula_14">y = p j=1 z j ψ j = Z Ψ = H d|p (Z) Ψ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_15">Ψ =    ψ 1 . . . ψ p    .</formula><p>The SISO, SIMO, MIMO, and MISO convolutional operations are illustrated in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The extension to the multichannel 2-D convolution operation for an image domain CNN (and multidimensional convolutions in general) is straightforward, since similar matrix vector operations can also be used. The only required change is the definition of the (extended)  Hankel matrix, which is now defined as a block Hankel matrix. Specifically, for a 2-D input X = [x 1 , . . . , x n 2 ] ∈ R n 1 ×n 2 with x i ∈ R n 1 , the block Hankel matrix associated with filtering with a d 1 × d 2 filter is given by</p><formula xml:id="formula_16">H d 1 ,d 2 (X) =      H d 1 (x 1 ) H d 1 (x 2 ) • • • H d 1 (x d 2 ) H d 1 (x 2 ) H d 1 (x 3 ) • • • H d 1 (x d 2 +1 ) . . . . . . . . . . . . H d 1 (x n 2 ) H d 1 (x 1 ) • • • H d 1 (x d 2 -1 )      ∈ R n 1 n 2 ×d 1 d 2 . (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>Similarly, an extended block Hankel matrix from the p-channel n 1 ×n 2 input image</p><formula xml:id="formula_18">X (i) = [x (i) 1 , . . . , x (i) n 2 ], i = 1, . . . , p, is defined by H d 1 ,d 2 |p [X (1) • • • X (p) ] = H d 1 ,d 2 (X (1) ) • • • H d 1 ,d 2 (X (p) ) ∈ R n 1 n 2 ×d 1 d 2 p . (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>Then, the output Y ∈ R n 1 ×n 2 from the 2-D SISO convolution for a given image X ∈ R n 1 ×n 2 with the 2-D filter K ∈ R d 1 ×d 2 can be represented by a matrix vector form:</p><formula xml:id="formula_20">Vec(Y ) = H d 1 ,d 2 (X)Vec(K),</formula><p>where Vec(Y ) denotes the vectorization operation by stacking the column vectors of the 2-D matrix Y . Similarly, 2-D MIMO convolution for given p input images</p><formula xml:id="formula_21">X (j) ∈ R n 1 ×n 2 , j = 1, . . . , p, with 2-D filter K (j) (i) ∈ R d 1 ×d 2 can</formula><p>be represented by a matrix vector form:</p><formula xml:id="formula_22">Vec(Y (i) ) = p j=1 H d 1 ,d 2 (X (j) )Vec(K (j) (i) ), i = 1, . . . , q. (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>Therefore, by defining</p><formula xml:id="formula_24">Y = Vec(Y (1) ) • • • Vec(Y (q) ) ,<label>(15)</label></formula><formula xml:id="formula_25">K =     Vec(K (1) (1) ) • • • Vec(K (1) (q) ) . . . . . . . . . Vec(K (p) (1) ) • • • Vec(K (p) (q) )     ,<label>(16)</label></formula><p>the 2-D MIMO convolution can be represented by</p><formula xml:id="formula_26">Y = H d 1 ,d 2 |p [X (1) • • • X (p) ] K. (<label>17</label></formula><formula xml:id="formula_27">)</formula><p>Due to the similarities between the 1-D and 2-D convolutions, we will use the 1-D notation throughout the paper for the sake of simplicity; however, readers are advised that the same theory applies to 2-D cases.</p><p>In convolutional neural networks (CNNS), unique multidimensional convolutions are used. Specifically, to generate q output channels from p input channels, each channel output is computed by first convolving p 2-D filters and p input channel images, and then applying the weighted sum to the outputs (which is often referred to as 1 × 1 convolution). For 1-D signals, this operation can be written as</p><formula xml:id="formula_28">y i = p j=1 w j z j ψ j i , i = 1, . . . , q,<label>(18)</label></formula><p>where w j denotes the 1-D weighting. Note that this is equivalent to a MIMO convolution, since we have</p><formula xml:id="formula_29">Y = p j=1 w j H d (z j )Ψ j = p j=1 H d (z j )Ψ w j = H d|p (Z) Ψ w = Z Ψ w ,<label>(19)</label></formula><p>where</p><formula xml:id="formula_30">Ψ w =    w 1 Ψ 1 . . . w p Ψ p    .<label>(20)</label></formula><p>The aforementioned matrix vector operations using the extended Hankel matrix also describe the filtering operation ( <ref type="formula" target="#formula_22">14</ref>) in 2-D CNNs, as shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Throughout the paper, we denote the space of the wrap-around Hankel structure matrices of the form in (3) as H(n, d), and an extended Hankel matrix composed of p Hankel matrices of the form in <ref type="bibr" target="#b10">(10)</ref> as H(n, d; p). The basic properties of the Hankel matrix used in this paper are described in Lemma 14 in Appendix A. In the next section, we describe advanced properties of the Hankel matrix that will be extensively used in this paper. </p><formula xml:id="formula_31">(j) ) = 2 i=1 H d 1 ,d 2 (X (i) )Vec(K (i) (j) )</formula><p>, where X (i) and Y (j) denote the ith input and jth output channel images, respectively; and K (i) (j) denotes the ith input channel filter to yield the jth channel output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Low-rank property of Hankel matrices. One of the most intriguing features of the</head><p>Hankel matrix is that it often has a low-rank structure, and its low-rankness is related to the sparsity in the Fourier domain (for the case of Fourier samples, it is related to the sparsity in the spatial domain) <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b36">36]</ref>.</p><p>Note that many types of image patches have sparsely distributed Fourier spectra. For example, as shown in Figure <ref type="figure" target="#fig_3">4</ref>(a), a smoothly varying patch usually has spectrum content in the low-frequency regions, while the other frequency regions have very few spectral components. Similar spectral domain sparsity can be observed in the texture patch shown in Figure <ref type="figure" target="#fig_3">4</ref>(b), where the spectral components of the patch are determined by the spectrum of the patterns. For the case of an abrupt transition along the edge as shown in Figure <ref type="figure" target="#fig_3">4</ref>(c), Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php the spectral components are mostly localized along the ω x axis. In these cases, if we construct a Hankel matrix using the corresponding image patch, the resulting Hankel matrix is low-ranked <ref type="bibr" target="#b71">[71]</ref>. This property is extremely useful, as demonstrated by many applications <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b35">35]</ref>. For example, this idea can be used for image denoising <ref type="bibr" target="#b37">[37]</ref> and deconvolution <ref type="bibr" target="#b52">[52]</ref> by modeling the underlying intact signals to have low-rank Hankel structure, from which the artifacts or blur components can be easily removed.</p><p>In order to understand this intriguing relationship, consider a 1-D signal whose spectrum in the Fourier domain is sparse and can be modeled as the sum of Diracs functions:</p><formula xml:id="formula_32">(21) f (ω) = 2π r-1 j=0 c j δ (ω -ω j ) , ω j ∈ [0, 2π],</formula><p>where {ω j } r-1 j=0 refer to the corresponding harmonic components in the Fourier domain. Then, the corresponding discrete time-domain signal is given by</p><formula xml:id="formula_33">f [k] = r-1 j=0 c j e -ikω j . (<label>22</label></formula><formula xml:id="formula_34">)</formula><p>Suppose that we have an r + 1-length filter h[k] which has the following z-transform representation <ref type="bibr" target="#b65">[65]</ref>:</p><formula xml:id="formula_35">ĥ(z) = r l=0 h[l]z -l = r-1 j=0</formula><p>(1 -e -iω j z -1 ). <ref type="bibr" target="#b23">(23)</ref> Then, it is easy to see that</p><formula xml:id="formula_36">(f h)[k] = 0 ∀k,<label>(24) because</label></formula><formula xml:id="formula_37">(h * f )[k] = r l=0 h[l]f [k -l] = r l=0 r-1 j=0 c j h[l]u k-l j = r-1 j=0 c j r l=0 h[p]u -l j ĥ(u j ) u k j = 0,<label>(25)</label></formula><p>where u j = e -iω j and the last equality comes from <ref type="bibr" target="#b23">(23)</ref>  <ref type="bibr" target="#b65">[65]</ref>. Thus, the filter h annihilates the signal f , so it is referred to as the annihilating filter. Moreover, using the notation in (2), <ref type="bibr" target="#b24">(24)</ref> can be represented by H d (f )h = 0. This implies that the Hankel matrix H d (f ) is rank-deficient. In fact, the rank of the Hankel matrix can be explicitly calculated as shown in the following theorem.</p><p>Theorem 1 (Ye et al. <ref type="bibr" target="#b71">[71]</ref>). Let r + 1 denote the minimum length of the annihilating filters that annihilate the signal f = [f <ref type="bibr" target="#b0">[1]</ref>, . . . , f [n]] T . Then, for a given Hankel structured matrix H d (f ) ∈ H(n, d) with d &gt; r, we have</p><formula xml:id="formula_38">rankH d (f ) = r,<label>(26)</label></formula><p>where rank(•) denotes a matrix rank.</p><p>Thus, if we choose sufficiently large d, the resulting Hankel matrix is low-rank. This relationship is quite general, and Ye et al. <ref type="bibr" target="#b71">[71]</ref> further show that the rank of the associated Hankel matrix H d (f ) is r if and only if f can be represented by <ref type="bibr" target="#b27">(27)</ref> f</p><formula xml:id="formula_39">[k] = p-1 j=0 m j -1 l=0 c j,l k l λ j k , where r = p-1 j=0 m j &lt; d</formula><p>for some |λ j | ≤ 1, j = 1, . . . , m j . If λ j = e -iω j , then it is directly related to the signals with the finite rate of innovations (FRI) <ref type="bibr" target="#b65">[65]</ref>. Thus, the low-rank Hankel matrix provides an important link between FRI sampling theory and compressed sensing such that a sparse recovery problem can be solved using the measurement domain low-rank interpolation <ref type="bibr" target="#b71">[71]</ref>.</p><p>In <ref type="bibr" target="#b33">[33]</ref>, we also showed that the rank of the extended Hankel matrix in <ref type="bibr" target="#b10">(10)</ref> is low when the multiple signals Z = [z 1 , • • • z p ] have the following structure: <ref type="bibr" target="#b28">(28)</ref> ẑi = f h i , i = 1, . . . , p, such that the Hankel matrix H d (z i ) has the following decomposition:</p><formula xml:id="formula_40">(29) H d (z i ) = H n (f )C d (h i ) ∈ C n×d ,</formula><p>where H n (f ) is an n × n wrap-around Hankel matrix, and C d (h) for any h ∈ R m with m ≤ n is defined by</p><formula xml:id="formula_41">C d (h) = d                h[1] • • • 0 . . . . . . 0 h[m] . . . h[1] 0 . . . . . . . . . . . . h[m] . . . . . . . . . 0 0 0                ∈ C n×d . (<label>30</label></formula><formula xml:id="formula_42">)</formula><p>Accordingly, the extended Hankel matrix H d|p (Z) has the following decomposition:</p><formula xml:id="formula_43">H d|p (Z) = H n (f ) C d (h 1 ) • • • C d (h p ) . (<label>31</label></formula><formula xml:id="formula_44">)</formula><p>Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Due to the rank inequality rank(AB) ≤ min{rank(A), rank(B)}, we therefore have the following rank bound:</p><formula xml:id="formula_45">rankH d|p (Z) ≤ min rankH n (f ), rank C(h 1 ) • • • C(h p ) = min{r, pd}.<label>(32)</label></formula><p>Therefore, if the filter length d is chosen such that the number of columns of the extended matrix is sufficiently large, i.e., pd &gt; r, then the concatenated matrix becomes low-rank.</p><p>Note that the low-rank Hankel matrix algorithms are usually performed in a patch-bypatch manner <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b36">36]</ref>. It is also remarkable that this is similar to the current practice of using deep CNNs for low-level computer vision applications, where the network input is usually given as a patch. Later, we will show that this is not a coincidence; rather it suggests an important link between the low-rank Hankel matrix approach and a CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.</head><p>Hankel matrix decomposition and the convolution framelets. The last but no less important property of the Hankel matrix is that a Hankel matrix decomposition results in a framelet representation whose bases are constructed by the convolution of so-called local and nonlocal bases <ref type="bibr" target="#b73">[73]</ref>. More specifically, for a given input vector f ∈ R n , suppose that the Hankel matrix H d (f ) with the rank r &lt; d has the singular value decomposition (SVD)</p><formula xml:id="formula_46">H d (f ) = U ΣV ,<label>(33)</label></formula><p>where</p><formula xml:id="formula_47">U = [u 1 • • • u r ] ∈ R n×r and V = [v 1 • • • v r ] ∈ R d×r</formula><p>denote the left and right singular vector basis matrices, respectively, and Σ ∈ R r×r is the diagonal matrix whose diagonal components contain the singular values. Then, by multiplying U and V to the left and right of the Hankel matrix, we have</p><formula xml:id="formula_48">Σ = U H d (f )V. (<label>34</label></formula><formula xml:id="formula_49">)</formula><p>Note that the (i, j)th element of Σ is given by</p><formula xml:id="formula_50">σ ij = u i H d (f )v j = f, u i v j , 1 ≤ i, j ≤ r,<label>(35)</label></formula><p>where the last equality comes from (A.3). Since the number of rows and columns of H d (f ) are n and d, the right-multiplied vector v j interacts locally with the d neighborhood of the f vector, whereas the left-multiplied vector u i has a global interaction with the entire set of n-elements of the f vector. Accordingly, <ref type="bibr" target="#b35">(35)</ref> represents the strength of simultaneous global and local interaction of the signal f with bases. Thus, we call u i and v j nonlocal and local bases, respectively. This relation holds for the arbitrary basis matrices Φ = [φ 1 , . . . , φ m ] ∈ R n×n and Ψ = [ψ 1 , . . . , ψ d ] ∈ R d×d , which are multiplied to the left and right of the Hankel matrix, respectively, to yield the coefficient matrix</p><formula xml:id="formula_51">c ij = φ i H d (f )ψ j = f, φ i ψ j , i = 1, . . . , n, j = 1, . . . , d,<label>(36)</label></formula><p>which represents the interaction of f with the nonlocal basis φ i and the local basis ψ j . Using <ref type="bibr" target="#b36">(36)</ref> as the expansion coefficients, Yin et al. derived the following signal expansion, which they called the convolution framelet expansion <ref type="bibr" target="#b73">[73]</ref>. Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Proposition 2 (Yin et al. <ref type="bibr" target="#b73">[73]</ref>). Let φ i and ψ j denote the ith and jth columns of orthonormal matrices Φ ∈ R n×n and Ψ ∈ R d×d , respectively. Then, for any n-dimensional vector</p><formula xml:id="formula_52">f ∈ R n , f = 1 d n i=1 d j=1 f, φ i ψ j φ i ψ j .<label>(37)</label></formula><p>Furthermore, φ i ψ j with i = 1, . . . , n and j = 1, . . . , d forms a tight frame for R n with the frame constant d.</p><p>This implies that any input signal f ∈ R n can be expanded by using the convolution frame φ i ψ j and the expansion coefficient f, φ i ψ j . Although the framelet coefficient matrix [c ij ] in <ref type="bibr" target="#b36">(36)</ref> for general nonlocal and local bases is not as sparse as <ref type="bibr" target="#b34">(34)</ref> from SVD bases, Yin et al. <ref type="bibr" target="#b73">[73]</ref> show that the framelet coefficients can be made sufficiently sparse by optimally learning Ψ for a given nonlocal basis Φ. Therefore, the choice of the nonlocal bases is one of the key factors in determining the efficiency of the framelet expansion. In the following, several examples of nonlocal bases Φ in <ref type="bibr" target="#b73">[73]</ref> are discussed.</p><p>• SVD: From the singular value decomposition in <ref type="bibr" target="#b33">(33)</ref>, the SVD basis is constructed by augmenting the left singular vector basis U ∈ R n×r with an orthogonal matrix</p><formula xml:id="formula_53">U ext ∈ R n×(n-r) : Φ SVD = U U ext</formula><p>such that Φ SVD Φ SVD = I. Thanks to <ref type="bibr" target="#b35">(35)</ref>, this is the most energy compacting basis. However, the SVD basis is input-signal dependent and the calculation of the SVD is computationally expensive. • Haar: The Haar basis comes from the Haar wavelet transform and is constructed as follows:</p><formula xml:id="formula_54">Φ = Φ low Φ high ,</formula><p>where the low-pass and high-pass operators Φ low , Φ high ∈ R n× n 2 are defined by</p><formula xml:id="formula_55">Φ low = 1 √ 2             1 0 • • • 0 1 0 • • • 0 0 1 • • • 0 0 1 • • • 0 . . . . . . . . . . . . 0 0 • • • 1 0 0 . . . 1             , Φ high = 1 √ 2             1 0 • • • 0 -1 0 • • • 0 0 1 • • • 0 0 -1 • • • 0 . . . . . . . . . . . . 0 0 • • • 1 0 0 . . . -1             .</formula><p>Note that the number of nonzero elements in each column of the Haar basis is two, so one level of Haar decomposition does not represent a global interaction. However, by cascading the Haar basis, the interaction becomes global, resulting in a multiresolution decomposition of the input signal. Moreover, the Haar basis is a useful global basis because it can sparsify the piecewise constant signals. Later, we will show that the average pooling operation is closely related to the Haar basis.</p><p>• DCT: The discrete cosine transform (DCT) basis proposed by Yin et al. <ref type="bibr" target="#b73">[73]</ref> is an interesting global basis due to its energy compaction property proven by the JPEG image compression standard. The DCT basis matrix is a fully populated dense matrix, which clearly represents a global interaction. To the best of our knowledge, the DCT bases have never been used in deep CNN, which could be an interesting direction of research. In addition to the nonlocal bases used in <ref type="bibr" target="#b73">[73]</ref>, we will also investigate the following nonlocal bases.</p><p>• Identity matrix: In this case, Φ = I n×n , so there is no global interaction between the basis and the signal. Interestingly, this nonlocal basis is quite often used in CNNs that do not have a pooling layer. In this case, it is believed that the local structure of the signal is more important, and local bases are trained to be able to maximally capture the local correlation structure of the signal. • Learned basis: In extreme cases where we do not have specific knowledge of the signal, the nonlocal bases can still be trained. However, care must be taken, since the learned nonlocal basis has a size of n × n that quickly becomes too large for image processing applications. For example, if one is interested in processing a 512×512 (i.e., n = 2 9 ×2 9 ) image, the required memory to store the learnable nonlocal basis becomes 2 37 , which is not possible to store or estimate. However, if the input patch size is sufficiently small, this may be another interesting direction of research in deep CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main contributions:</head><p>Deep convolutional framelet neural networks. In this section, which is our main theoretical contribution, we will show that the convolution framelets Yin et al. <ref type="bibr" target="#b73">[73]</ref> are directly related to the deep neural network if we relax the condition of the original convolution framelets to allow for multilayer implementation. The multilayer extension of convolution framelets, which we call deep convolutional framelets, can explain many important components of deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep convolutional framelet expansion.</head><p>While the original convolution framelets by Yin et al. <ref type="bibr" target="#b73">[73]</ref> exploit the advantages of the low-rank Hankel matrix approaches using two bases, there are several limitations. First, their convolution framelets use only the orthonormal basis. Second, the significance of multilayer implementation was not noticed. Here, we discuss its extension to relax these limitations. As will become clear, this is a basic building block toward a deep convolutional framelet neural network.</p><formula xml:id="formula_56">Proposition 3. Let Φ = [φ 1 , . . . , φ m ] ∈ R n×m and Ψ = [ψ 1 , . . . , ψ q ] ∈</formula><p>R d×q denote the nonlocal and local basis matrices, respectively. Suppose, furthermore, that Φ = [ φ1 , . . . , φm ] ∈ R n×m and Ψ = [ ψ1 , . . . , ψq ] ∈ R d×q denote their dual basis matrices such that they satisfy the frame conditions</p><formula xml:id="formula_57">ΦΦ = m i=1 φi φ i = I n×n ,<label>(38)</label></formula><formula xml:id="formula_58">Ψ Ψ = q j=1 ψ j ψ j = I d×d . (<label>39</label></formula><formula xml:id="formula_59">)</formula><p>Then, for any input signal f ∈ R n , we have</p><formula xml:id="formula_60">f = 1 d m i=1 q j=1 f, φ i ψ j φi ψj ,<label>(40)</label></formula><p>or equivalently,</p><formula xml:id="formula_61">f = 1 d m i=1 Φc j ψj ,<label>(41)</label></formula><p>where c j is the jth column of the framelet coefficient matrix</p><formula xml:id="formula_62">C = Φ f Ψ (42) =    f, φ 1 ψ 1 • • • f, φ 1 ψ q . . . . . . . . . f, φ m ψ 1 • • • f, φ m ψ q    ∈ R m×q . (<label>43</label></formula><formula xml:id="formula_63">)</formula><p>Proof. Using the frame conditions ( <ref type="formula" target="#formula_57">38</ref>) and ( <ref type="formula" target="#formula_58">39</ref>), we have</p><formula xml:id="formula_64">H d (f ) = ΦΦ H d (f )Ψ Ψ = ΦC Ψ ,</formula><p>where C ∈ R m×q denotes the framelet coefficient matrix computed by</p><formula xml:id="formula_65">C = Φ H d (f )Ψ = Φ f Ψ</formula><p>and its (i, j)th element is given by</p><formula xml:id="formula_66">c ij = φ i H d (f )ψ j = f, φ i ψ j ,</formula><p>where we use (A.3) for the last equality. Furthermore, using (A.6) and (A.7), we have</p><formula xml:id="formula_67">f = H † d (H d (f )) = H † d ΦC Ψ = 1 d q j=1 Φc j ψj = m i=1 q j=1 f, φ i ψ j φi ψj .</formula><p>This concludes the proof.</p><p>Note that the so-called perfect reconstruction (PR) condition represented by ( <ref type="formula" target="#formula_60">40</ref>) can be equivalently studied using</p><formula xml:id="formula_68">f = H † d Φ(Φ H d (f )Ψ) Ψ .<label>(44)</label></formula><p>Similarly, for a given matrix input Z ∈ R n×p , the PR condition for a matrix input Z can be given by</p><formula xml:id="formula_69">Z = H † d|p Φ(Φ H d|p (Z)Ψ) Ψ ,<label>(45)</label></formula><p>which is explicitly represented in the following proposition. Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Proposition 4. Let Φ, Φ ∈ R n×m denote the nonlocal basis and its dual, and let Ψ, Ψ ∈ R pd×q denote the local basis and its dual, respectively, which satisfy the frame conditions</p><formula xml:id="formula_70">ΦΦ = m i=1 φi φ i = I n×n ,<label>(46)</label></formula><formula xml:id="formula_71">Ψ Ψ = q j=1 ψ j ψ j = I pd×pd . (<label>47</label></formula><formula xml:id="formula_72">)</formula><p>Suppose, furthermore, that the local basis matrices have the block structure</p><formula xml:id="formula_73">Ψ = Ψ 1 • • • Ψ p , Ψ = Ψ 1 • • • Ψ p (48)</formula><p>with Ψ i , Ψi ∈ R d×q , whose jth column is represented by ψ i j and ψi j , respectively. Then, for any matrix</p><formula xml:id="formula_74">Z = [z 1 • • • z p ] ∈ R n×p , we have Z = 1 d m i=1 q j=1 p k=1 z k , φ i ψ k j φi ψ1 j • • • z k , φ i ψ k j φi ψp j ,<label>(49)</label></formula><p>or equivalently,</p><formula xml:id="formula_75">Z = 1 d q j=1 Φc j ψ1 j • • • q j=1 Φc j ψp j ,<label>(50)</label></formula><p>where c j is the jth column of the framelet coefficient matrix</p><formula xml:id="formula_76">C = Φ Z Ψ (51) = p k=1    z k , φ 1 ψ k 1 • • • z k , φ 1 ψ k q . . . . . . . . . z k , φ m ψ k 1 • • • z k , φ m ψ k q    ∈ R m×q .</formula><p>Proof. For a given Z ∈ R n×p , using the frame conditions ( <ref type="formula" target="#formula_57">38</ref>) and ( <ref type="formula" target="#formula_58">39</ref>), we have</p><formula xml:id="formula_77">H d|p (Z) = ΦΦ H d|p (Z)Ψ Ψ = ΦC Ψ ,</formula><p>where C ∈ R m×q denotes the framelet coefficient matrix computed by</p><formula xml:id="formula_78">C = Φ H d|p (Z)Ψ = Φ Z Ψ</formula><p>and its (i, j)th element is given by</p><formula xml:id="formula_79">c ij = φ i H d|p (Z)ψ j = p k=1 z k , φ i ψ k j .</formula><p>Furthermore, using (A.6), (A.7), and (A.8), we have</p><formula xml:id="formula_80">Z = H † d|p H d|p (Z) = H † d|p ΦC Ψ = H † d ΦC Ψ 1 • • • H † d ΦC Ψ p = 1 d q j=1 Φc j ψ1 j • • • q j=1 Φc j ψp j = 1 d m i=1 q j=1 p k=1 z k , φ i ψ k j φi ψ1 j • • • z k , φ i ψ k j φi ψp j .</formula><p>This concludes the proof.</p><p>Remark 1. Compared to Proposition 2, Propositions 3 and 4 are more general, since they consider the redundant and nonorthonormal nonlocal and local bases by allowing relaxed conditions, i.e., m ≥ n or q ≥ d. The specific purpose of q ≥ d is to investigate existing CNNs that have large numbers of filter channels at lower layers. The redundant global basis with m ≥ n is also believed to be useful for future research, so Proposition 3 is derived by considering further extension. However, since most of the existing deep networks use the condition m = n, we will mainly focus on this special case.</p><p>Remark 2. For the given SVD in <ref type="bibr" target="#b33">(33)</ref>, the frame conditions ( <ref type="formula" target="#formula_57">38</ref>) and ( <ref type="formula" target="#formula_58">39</ref>) can be further relaxed to</p><formula xml:id="formula_81">ΦΦ = P R(U ) , Ψ Ψ = P R(V )</formula><p>due to the following matrix identity:</p><formula xml:id="formula_82">H d (f ) = P R(U ) H d (f )P R(V ) = Φ Φ H d (f )Ψ Ψ .</formula><p>In this case, the number of bases for the nonlocal and local basis matrices can be smaller than those of Proposition 3 and Proposition 4, i.e., m = r &lt; n and q = r &lt; d. Therefore, a smaller number of bases still suffices for PR.</p><p>Finally, using Propositions 3 and 4 we will show that the convolution framelet expansion can be realized by two matched convolution layers, which bears a striking similarity to neural networks with encoder-decoder structure <ref type="bibr" target="#b53">[53]</ref>. Our main contribution is summarized in the following theorem.</p><p>Theorem 5 (deep convolutional framelet expansion). Under the assumptions of Proposition 4, we have the following decomposition of input Z ∈ R n×p :</p><formula xml:id="formula_83">Z = ΦC ν( Ψ),<label>(52)</label></formula><formula xml:id="formula_84">C = Φ Z Ψ , (<label>53</label></formula><formula xml:id="formula_85">)</formula><p>where the decoder-layer convolutional filter ν( Ψ) is defined by</p><formula xml:id="formula_86">ν( Ψ) := 1 d    ψ1 1 • • • ψp 1 . . . . . . . . . ψ1 q • • • ψp q    ∈ R dq×p . (<label>54</label></formula><formula xml:id="formula_87">)</formula><p>Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Similarly, under the assumptions of Proposition 3, we have the following decomposition of f ∈ R n :</p><formula xml:id="formula_88">f = ΦC ν( Ψ),<label>(55)</label></formula><formula xml:id="formula_89">C = Φ f Ψ ,<label>(56)</label></formula><p>where</p><formula xml:id="formula_90">ν( Ψ) := 1 d    ψ1 . . . ψq    ∈ R dq . (<label>57</label></formula><formula xml:id="formula_91">)</formula><p>Proof. First, note that (50) corresponds to a decoder-layer convolution:</p><formula xml:id="formula_92">Z = 1 d q j=1 Φc j ψ1 j • • • q j=1 Φc j ψp j = ΦC ν( Ψ),</formula><p>where ν( Ψ) is defined by ( <ref type="formula" target="#formula_86">54</ref>) and the last equality comes from the definition of the MIMO convolution in ( <ref type="formula" target="#formula_8">5</ref>) and ( <ref type="formula">7</ref>). On the other hand, from (51), we have</p><formula xml:id="formula_93">C = Φ Z Ψ .</formula><p>Similarly, using the definition of the MISO convolution ( <ref type="formula" target="#formula_14">11</ref>), ( <ref type="formula" target="#formula_61">41</ref>) can be represented by</p><formula xml:id="formula_94">f = ΦC ν( Ψ),</formula><p>where ν( Ψ) is defined by <ref type="bibr" target="#b57">(57)</ref>. Finally, (56) comes from <ref type="bibr" target="#b42">(42)</ref>. This concludes the proof.</p><p>Remark 3 (nonlocal basis for generalized pooling/unpooling). Note that there is a major difference between the encoder and decoder layer convolutions. Aside from the differences in the specific convolutional filters, the nonlocal basis matrix Φ should be applied later to the filtered signal in the case of the encoder <ref type="bibr" target="#b53">(53)</ref>, whereas the nonlocal basis matrix Φ should be multiplied first before the local filtering is applied to the decoder layer. This is in fact similar to the pooling and unpooling operations, because the pooling is performed after filtering while unpooling is applied before filtering. Hence, we can see that the nonlocal basis is a generalization of the pooling/unpooling operations.</p><p>Existing CNNs often incorporate bias estimation for each layer of the convolution. Accordingly, we are interested in extending our deep convolutional framelet expansion under bias estimation. Specifically, with the bias estimation, the encoder and decoder convolutions should be modified as</p><formula xml:id="formula_95">C = Φ (Z Ψ + 1 n b enc ), (<label>58</label></formula><formula xml:id="formula_96">) Ẑ = ΦC ν( Ψ) + 1 n b dec ,<label>(59)</label></formula><p>where 1 n ∈ R n denotes the vector with 1, and b enc ∈ R q and b dec ∈ R p denote the encoder and decoder layer biases, respectively. Then, the following theorem shows that there exists a unique bias vector b dec ∈ R p for a given encoder bias vector b enc ∈ R q that satisfies the PR. Theorem 6 (matched bias). Suppose that Φ, Φ ∈ R n×m and Ψ, Ψ ∈ R pd×q satisfy the assumptions of Proposition 4. Then, for a given bias b enc ∈ R q at the encoder layer, (58) and (59) satisfy the PR if the decoder bias is given by The simple convolutional framelet expansion using ( <ref type="formula" target="#formula_89">56</ref>), ( <ref type="formula" target="#formula_88">55</ref>) and ( <ref type="formula" target="#formula_84">53</ref>), (52) (or <ref type="bibr" target="#b58">(58)</ref>, <ref type="bibr" target="#b59">(59)</ref> when including bias) is so powerful that a CNN with the encoder-decoder architecture can be produced from it by inserting the encoder-decoder pair <ref type="bibr" target="#b56">(56)</ref>, <ref type="bibr" target="#b55">(55)</ref> between the encoderdecoder pair (53), <ref type="bibr" target="#b52">(52)</ref>, as illustrated by the red and blue lines, respectively, in Figure <ref type="figure" target="#fig_4">5</ref>. In general, the L-layer implementation of the convolutional framelets can be recursively defined. For example, the first layer encoder-decoder architecture without considering bias is given by</p><formula xml:id="formula_97">1 n b dec = -H † d|p 1 n b enc Ψ ,<label>(60)</label></formula><formula xml:id="formula_98">f = H † d (1) |p (1) Φ(1) Ĉ(1) Ψ(1) = Φ(1) Ĉ(1) ν( Ψ(1) ),</formula><p>where the decoder part of the framelet coefficient at the ith layer, Ĉ(i</p><formula xml:id="formula_99">) ∈ R n×q (i) , q (i) ≥ d (i) p (i)</formula><p>, is given by </p><formula xml:id="formula_100">Ĉ(i) = H † d (i+1) |p (i+1) Φ(i+1) Ĉ(i+1) Ψ(i+1) = Φ(i+1) Ĉ(i+1) ν( Ψ(i+1) ), 1 ≤ i &lt; L, C (L) , i = L,<label>(62)</label></formula><formula xml:id="formula_101">C (i) = Φ (i) H d (i) |p (i) (C (i-1) )Ψ (i) = Φ (i) C (i-1) Ψ (i) , 1 ≤ i ≤ L, f, i = 0.<label>(63)</label></formula><p>Here, d (i) and p (i) denote the filter length and the number of input channels at the ith layer, respectively, and q (i) refers to the number of output channels. The specific number of channels will be analyzed in the following section.</p><p>3.2. Properties of deep convolutional framelets. In this section, several important properties of deep convolutional framelets are explained in detail. First, the PR conditions in ( <ref type="formula" target="#formula_89">56</ref>), ( <ref type="formula" target="#formula_88">55</ref>) and ( <ref type="formula" target="#formula_84">53</ref>), ( <ref type="formula" target="#formula_83">52</ref>) can be analyzed in the Fourier domain as shown by the following proposition.</p><p>Proposition 7 (Fourier analysis of filter channels). Suppose that Φ and Φ satisfy the frame condition <ref type="bibr" target="#b38">(38)</ref>. Then, the PR condition given by ( <ref type="formula" target="#formula_84">53</ref>) and <ref type="bibr" target="#b52">(52)</ref> </p><formula xml:id="formula_102">with Ψ = [Ψ 1 • • • Ψ p ] and Ψ = [ Ψ 1 • • • Ψ p ]</formula><p>can be represented by</p><formula xml:id="formula_103">I p×p = 1 d     ψ 1 1 * • • • ψ 1 q * . . . . . . . . . ψ p 1 * • • • ψ p q *         ψ1 1 • • • ψp 1 . . . . . . . . . ψ1 q • • • ψp q     ,<label>(64)</label></formula><p>where ψ denotes the Fourier transform of ψ and the superscript * is the complex conjugate.</p><p>In particular, the PR condition given by ( <ref type="formula" target="#formula_89">56</ref>) and (55) can be represented by</p><formula xml:id="formula_104">1 d q i=1 ψ i * ψi = 1. (65)</formula><p>Furthermore, if the local basis Ψ is orthonormal, then we have</p><formula xml:id="formula_105">1 d d i=1 | ψ i | 2 = 1. (<label>66</label></formula><formula xml:id="formula_106">)</formula><p>Proof. See Appendix C. Remark 4. Note that (66) and ( <ref type="formula">65</ref>) are equivalent to the PR conditions for the orthogonal and biorthogonal wavelet decompositions, respectively <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b17">17]</ref>. However, without the frame condition <ref type="bibr" target="#b38">(38)</ref> for the nonlocal bases, such a simplification is not true. This is another reason why we are interested in imposing the frame condition <ref type="bibr" target="#b38">(38)</ref> for the nonlocal bases in order to use the connection to the classical wavelet theory <ref type="bibr" target="#b49">[49]</ref>.</p><p>The following proposition shows that a sufficient condition for achieving PR is that the number of input channels should increase multiplicatively along the layers.</p><p>Proposition 8 (number of filter channels). A sufficient condition for achieving PR is that the number of output channels q (l) , l = 1, . . . , L, satisfies</p><formula xml:id="formula_107">q (l) ≥ q (l-1) d (l) , l = 1, . . . , L,<label>(67)</label></formula><p>where d (i) is the filter length at the ith layer and q (0) = 1. Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Proof. See Appendix D. An intuitive explanation of Proposition 8 is that the cascaded application of two multichannel filter banks is equivalent to applying a combined filter bank generated by each combination of two filters at each stage, so the output dimension of the filter bank increases multiplicatively. As a special case of Proposition 8, we can derive the following sufficient condition:</p><formula xml:id="formula_108">q (l) = l i=1 d (i) , l = 1, . . . , L,</formula><p>which is obtained by choosing the minimum number of output channels at each layer. This implies that the number of channels increases exponentially with respect to the layers as shown in Figure <ref type="figure" target="#fig_6">6</ref>, which is difficult to achieve in practice due to the memory requirement. A natural question is then why we still prefer a deep network to a shallow one. Proposition 9 provides an answer to this question. Proposition 9 (rank bound of the Hankel matrix). Suppose that Φ (l) = I n×n ∀l ≥ 1. Then, the rank of the extended Hankel matrix</p><formula xml:id="formula_109">H d (l) |p (l) (C (l-1) ) in (63) is upper-bounded by rankH d (l) |p (l) (C (l-1) ) ≤ min{rankH n (f ), d (l) p (l) }. (<label>68</label></formula><formula xml:id="formula_110">)</formula><p>Proof. See Appendix E. Remark 5. Proposition 9 is derived by assuming the identity matrix as a nonlocal basis. While we expect similar results for the general nonlocal basis that satisfies the frame condition <ref type="bibr" target="#b38">(38)</ref>, the introduction of such a nonlocal basis makes the analysis very complicated due to the noncommutative nature of matrix multiplication and convolution, so we defer its analysis for future study.</p><p>We believe that (68) is the key inequality for revealing the role of the depth. Specifically, to exploit the low-rank structure of the input signal, the upper bound of (68) should be determined by the intrinsic property of the signal r := rankH n (f ), instead of the number of columns of the Hankel matrix d (l) p (l) , which can be chosen arbitrarily. Thus, if d (l) = d ∀l, then <ref type="bibr" target="#b68">(68)</ref> inform us that the number of encoder-decoder layer depths L is given by</p><formula xml:id="formula_111">r ≤ d L ⇐⇒ L ≥ log d (r). (<label>69</label></formula><formula xml:id="formula_112">)</formula><p>This implies that the minimum number of encoder-decoder layers is dependent on the rank structure of the input signal and we therefore need a deeper network for a more complicated signal, which is consistent with the empirical findings. Moreover, as shown in Figure <ref type="figure" target="#fig_6">6</ref>, for a given intrinsic rank, the depth of the network also depends on the filter length. For example, if the intrinsic rank of the Hankel matrix is 100, then the network depth with respect to d = 2 is 7, whereas it is 5 when a longer filter with d = 3 is used.</p><p>Suppose that there is an in sufficient number of output channels at a specific layer, say l = l * , and the signal content is approximated. Then, from the proof of Proposition 9 (in particular, (E.2)), we can easily see that rankH d (l) |p (l) (C (l-1) ) is upper-bounded by the rank structure of the approximated signal at l = l * . Moreover, if the number of channels is not sufficient for all layers, the rank will gradually decrease along the layers. This theoretical prediction will be confirmed later in the discussion section using empirical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ReLU nonlinearity.</head><p>In order to reveal the link between the convolutional framelets and the deep network, we further need to consider a nonlinearity. Note that the ReLU nonlinearity <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b30">30]</ref> is currently the most widely used for deep learning approaches. Specifically, the ReLU ρ(•) is an elementwise operation for a matrix such that for a matrix</p><formula xml:id="formula_113">X = [x ij ] n,m i,j=1 ∈ R n×m , the ReLU operator provides a nonnegative part, i.e., ρ(X) = [max(0, x ij )] n,m i,j=1</formula><p>. By inserting ReLUs, the L-layer encoder-decoder architecture neural network with bias is defined (with a slight abuse of notation) by</p><formula xml:id="formula_114">Q(f ) := Q f ; {Ψ (j) , Ψ(j) , b (j) enc , b (j) dec } L j=1 = Φ(1) ρ( Ĉ(1) ) ν( Ψ(1) ) + 1 n b (1) dec ,<label>(70)</label></formula><p>where</p><formula xml:id="formula_115">Ĉ(i) = Φ(i+1) ρ( Ĉ(i+1) ) ν( Ψ(i+1) ) + 1 n b (i+1) dec , 1 ≤ i &lt; L, ρ(C (L) ), i = L,<label>(71)</label></formula><p>and</p><formula xml:id="formula_116">C (i) = Φ (i) ρ(C (i-1) ) Ψ (i) + 1 n b (i) enc , 1 ≤ i ≤ L, f, i = 0. (<label>72</label></formula><formula xml:id="formula_117">)</formula><p>Recall that the PR condition for the deep convolutional framelets was derived without assuming any nonlinearity. Thus, the introduction of ReLU appears counterintuitive in the context of PR. Interestingly, in spite of the ReLU nonlinearity, the following theorem shows that the PR condition can be satisfied when filter channels and bias having opposite phase are available.</p><p>Proposition 10 (PR under ReLU using opposite phase filter channels). Suppose that Φ(l) Φ (l) = I n×n and Ψ (l) , Ψ(l) ∈ R p (l) d (l) ×2m (l) with m (l) ≥ p (l) d (l) for all l = 1, . . . , L. Then, the neural network output in (70) satisfies the PR condition, i.e., f = Q(f ), if the encoder-decoder filter channels are given by</p><formula xml:id="formula_118">Ψ (l) = Ψ (l) + -Ψ (l) + , Ψ(l) = Ψ(l) + - Ψ(l) + ,<label>(73)</label></formula><p>satisfying the condition</p><formula xml:id="formula_119">Ψ (l) + Ψ(l) + = I p (l) d (l) ×p (l) d (l) , Ψ (l) + , Ψ(l) + ∈ R p (l) d (l) ×m (l) ,<label>(74)</label></formula><p>and the encoder-decoder bias pairs b</p><formula xml:id="formula_120">(l) enc , b (l) dec are given by b (l) enc = b (l)T enc,+ -b (l)T enc,+ , 1 n b (l) dec = -H † d|p 1 n b (l) enc,+ Ψ(l) . (<label>75</label></formula><formula xml:id="formula_121">)</formula><p>Proof. See Appendix F. Remark 6. Equation ( <ref type="formula" target="#formula_118">73</ref>) in Proposition 10 predicts the existence of filter pairs with opposite phase. Amazingly, this theoretical prediction coincides with the empirical observations in deep learning literature. For example, Shang et al. <ref type="bibr" target="#b59">[59]</ref> observed an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). To exploit this property for further network performance improvement, the authors proposed the so-called concatenated ReLU (CReLU) network to explicitly retrieve the negative part of Φ F Ψ + using ρ(Φ F (-Ψ + )) <ref type="bibr" target="#b59">[59]</ref>.</p><p>Remark 7. Note that there is an infinite number of filters satisfying <ref type="bibr" target="#b73">(73)</ref>. In fact, the most important requirement for PR is the existence of opposite phase filters as in <ref type="bibr" target="#b73">(73)</ref>, satisfying the frame condition (74) rather than the specific filter coefficients. This may suggest excellent generalization performance of a deep network even from a small set of training data.</p><p>Proposition 10 deals with the PR condition using redundant local filters Ψ ∈ R pd×2m with m ≥ pd. This can be easily satisfied at the lower layers of the deep convolutional framelets; however, the number of filter channels for PR grows exponentially with the layers as shown in <ref type="bibr" target="#b67">(67)</ref>. Thus, at higher layers of deep convolutional framelets, the condition m ≥ pd for Proposition 10 may not be satisfied. However, even in this case, we can still achieve PR as long as the extended Hankel matrix at that layer is sufficiently low-rank.</p><p>Proposition 11 (low-rank approximation with insufficient filter channels). For a given input X ∈ R n×p , let H d|p (X) denote its extended Hankel matrix whose rank is r. Suppose, furthermore, that ΦΦ = I n×n . Then, there exists</p><formula xml:id="formula_122">Ψ ∈ R pd×2m and Ψ ∈ R pd×2m with r ≤ m &lt; pd such that X = H † d|p (Φρ Φ H d|p (X)Ψ Ψ ). Proof. See Appendix G.</formula><p>However, as the network gets deeper, more layers cannot satisfy the condition for Proposition 11. To address this, the residual net <ref type="bibr" target="#b31">[31]</ref>  has been widely used for image classification as well as image reconstruction. More specifically, the residual net architecture shown in Figure <ref type="figure" target="#fig_7">7</ref>(a) can be represented by</p><formula xml:id="formula_123">X = R(F ; Ψ, Ψ) := ρ F -ρ(F Ψ) Ψ ,<label>(76)</label></formula><p>where F := H d|p (X). The following result shows that the residual network truncates the least significant subspaces.</p><p>Proposition 12 (high-rank approximation using residual nets). For a given nonnegative input X ∈ R n×p , let H d|p (X) denote its extended Hankel matrix and its SVD is given by</p><formula xml:id="formula_124">H d|p (X) = U ΣV = r i=1 σ i u i v i ,<label>(77)</label></formula><p>where u i and v i denote the left and right singular vectors, σ 1 ≥ σ 2 ≥ • • • ≥ σ r &gt; 0 are the singular values, and r denotes the rank. Then, there exist Ψ ∈ R pd×2m and Ψ ∈ R pd×2m with r &lt; pd such that</p><formula xml:id="formula_125">X = H † d|p (R(F ; Φ, Ψ, Ψ)), (<label>78</label></formula><formula xml:id="formula_126">)</formula><p>where R(F ; Ψ, Ψ) is given by <ref type="bibr" target="#b76">(76)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix H.</head><p>Note that the PR condition for the residual network is much more relaxed than that of Proposition 11, since there is no constraint on m that determines the number of output filter channels. Specifically, for the case of Proposition 11, the number of output filter channels should be q = 2m ≥ 2r. On the other hand, the PR condition for the residual net only requires the existence of the null space in the extended Hankel matrix (i.e., r &lt; pd), and it does not depend on the number of output channels. Therefore, we have more freedom to choose filters.</p><p>In addition, Proposition 11 also implies the low-rank approximation if the number of output channels is not sufficient. Specifically, with matched opposite phase filters, we have</p><formula xml:id="formula_127">Φρ Φ H d|p (X)Ψ Ψ = H d|p (X)Ψ + Ψ + .<label>(79)</label></formula><p>Thus, we can choose Ψ + , Ψ+ such that (79) results in the rank-m approximation of H d|p (X). On the other hand, from the proof in Appendix H, we can see that</p><formula xml:id="formula_128">R(F ; Φ, Ψ + , Ψ+ ) = ρ H d|p (X) -U ΣV Ψ + Ψ + . (80)</formula><p>Accordingly, by choosing Ψ + , Ψ+ such that Ψ + Ψ + = P R(v pd ) , where v pd is the singular vector for the least singular value, we can minimize the error of approximating H d|p (X) using R(F ; Ψ + , Ψ+ ). This is why we refer to Proposition 12 as the high-rank approximation using the residual net.</p><p>3.4. Role of the PR condition and shrinkage behavior of the network. So far, we have investigated the PR condition for deep convolutional framelets. We are now ready to explain why PR is useful for inverse problems.</p><p>Suppose that an analysis operator W, which is composed of frame bases, and the associated synthesis operator W satisfy the following frame condition:</p><formula xml:id="formula_129">f = W Wf ∀f ∈ H,<label>(81)</label></formula><p>where H denotes the Hilbert space of interest. Signal processing using frame bases satisfying (81) has been extensively studied for frame-based image processing <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b10">10]</ref>. One of the important advantages of these frame-based algorithms is the proven convergence <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b10">10]</ref>, which makes the algorithm powerful.</p><p>For example, consider a signal denoising algorithm used to recover the noiseless signal f * from the noisy measurement g = f * + e, where e is the additive noise. Then, a frame-based denoising algorithm <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b12">12]</ref> recovers the unknown signal by substituting g for f in the right side of (81) and applies a shrinkage operator S τ to the framelet coefficients to eliminate the small magnitude noise signals:</p><formula xml:id="formula_130">f = W S τ (Wg), (<label>82</label></formula><formula xml:id="formula_131">)</formula><p>where τ denotes a shrinkage parameter. In denoising, soft or hard thresholding shrinkage operations are most widely used. Thus, to make the frame-based denoising algorithm successful, the frame should be energy compacting so that most of the signals are concentrated in a small number of framelet coefficients, whereas the noises are spread out across all framelet coefficients.</p><p>As another example, consider a signal inpainting problem <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b10">10]</ref>, where, for a given signal f ∈ H, we measure g only on the index set Λ and our goal is then to estimate the unknown image f on the complementary index set Λ c . Let P Λ be the diagonal matrix with diagonal entries 1 for the indices in Λ and 0 otherwise. Then we have the following identities:</p><formula xml:id="formula_132">f = µP Λ f + (I -µP Λ )f = µP Λ g + (I -µP Λ ) W Wf, (<label>83</label></formula><formula xml:id="formula_133">)</formula><p>where we use the frame condition (81) for the last equality. A straightforward frame-based inpainting algorithm <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b10">10]</ref> can then be obtained from (83) as</p><formula xml:id="formula_134">f n+1 = µP Λ g + (I -µP Λ )W S τ (Wf n ) ,<label>(84)</label></formula><p>where S τ denotes a shrinkage operator with the parameter τ and f 0 is initialized to P Λ g. Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Now, note that the computation of our deep convolutional framelet coefficients can represent an analysis operator</p><formula xml:id="formula_135">Wf := C = Φ (f Ψ),</formula><p>whereas the synthesis operator is given by the decoder part of the convolution:</p><formula xml:id="formula_136">W C := (ΦC) ν( Ψ).</formula><p>If the nonlocal and local bases satisfy the frame condition <ref type="bibr" target="#b38">(38)</ref> and ( <ref type="formula" target="#formula_58">39</ref>) for all layers, they satisfy the PR condition (81). Thus, we could use (82) and (84) for denoising and inpainting applications. What, then, is the shrinkage operator in our deep convolutional framelets? Although one could still use a similar soft or hard thresholding operator, one of the unique aspects of deep convolutional framelets is that the number of filter channels can control the shrinkage behavior. Moreover, the optimal local bases are trained from the training data so that they give the best shrinkage behavior. More specifically, the low-rank shrinkage behavior emerges when the number of output filter channels is insufficient. These results are very useful in practice, since the low-rank approximation of the Hankel matrix is good for reducing the noise and artifacts as demonstrated in image denoising <ref type="bibr" target="#b37">[37]</ref>, artifact removal <ref type="bibr" target="#b35">[35]</ref>, and deconvolution <ref type="bibr" target="#b52">[52]</ref>.</p><p>To understand this claim, consider the following regression problem under the Hankel structured low-rank constraint:</p><formula xml:id="formula_137">min f ∈R n f * -f 2 subject to rankH d (f ) ≤ r &lt; d,<label>(85)</label></formula><p>where f * ∈ R d denotes the ground-truth signal and we are interested in finding the rank-r approximation. Then, for any feasible solution f for (85), its Hankel structured matrix</p><formula xml:id="formula_138">H d (f ) has the SVD H d (f ) = U ΣV , where U = [u 1 • • • u r ] ∈ R n×r and V = [v 1 • • • v r ] ∈ R d×r</formula><p>denote the left and right singular vector basis matrices, respectively; Σ = (σ ij ) ∈ R r×r is the diagonal matrix with singular values. Then, we can find two matrix pairs Φ, Φ ∈ R n×n and Ψ, Ψ ∈ R d×r satisfying the conditions</p><formula xml:id="formula_139">ΦΦ = I n×n , Ψ Ψ = P R(V ) (86) such that H d (f ) = ΦΦ H d (f )Ψ Ψ ,</formula><p>which leads to the decomposition of f using a single-layer deep convolutional framelet expansion:</p><formula xml:id="formula_140">f = ΦC ν( Ψ), where C = Φ f Ψ . (<label>87</label></formula><formula xml:id="formula_141">)</formula><p>Note that (87) is the general form of the signals associated with the rank-r Hankel structured matrix.</p><p>Note that the bases (Φ, Φ) and (Ψ, Ψ) are not specified in (87). In our deep convolutional framelets, Φ and Φ correspond to the generalized pooling and unpooling which are chosen based on domain knowledge, so we are only interested in estimating the filters Ψ, Ψ. To restrict the search space further, let H r denote the space of such signals that have a positive framelet coefficient, i.e., C ≥ 0: </p><formula xml:id="formula_142">H r = f ∈ R n | f = ΦC ν( Ψ) + 1 n b dec , C = Φ f Ψ + 1 n b enc ≥ 0 ,<label>(88)</label></formula><formula xml:id="formula_143">{f (i) }∈Hr N i=1 f * (i) -f (i) 2 = min (Ψ, Ψ,benc,b dec ) N i=1 f * (i) -Q(f (i) ; Ψ, Ψ, b enc , b dec ) 2 , (<label>89</label></formula><formula xml:id="formula_144">)</formula><p>where</p><formula xml:id="formula_145">Q(f (i) ; Ψ, Ψ, b enc , b dec ) = ΦC[f (i) ] ν( Ψ) + 1 n b dec , C[f (i) ] = ρ Φ f (i) Ψ + 1 n b enc ,</formula><p>where ρ(•) is the ReLU for imposing the positivity. Once the network is fully trained, the inference for a given noisy input f is simply done by Q(f ; Ψ, Ψ, b enc , b dec ), which is equivalent to finding a denoised solution that has the rank-r Hankel structured matrix. Therefore, using deep convolutional framelets with insufficient channels, we do not need an explicit shrinkage operation. The idea can be further extended to the multilayer deep convolutional framelet expansion as follows.</p><p>Definition 13 (deep convolutional framelet training). Let {f (i) , f * (i) } N i=1 denote the input and target sample pairs. Then, the deep convolutional framelet training problem is given by min</p><formula xml:id="formula_146">Ψ (j) , Ψ(j) ,b (j) enc ,b (j) (dec) L j=1 N i=1 f * (i) -Q f (i) ; Ψ (j) , Ψ(j) , b (j) enc , b (j) (dec) L j=1 2 , (<label>90</label></formula><formula xml:id="formula_147">)</formula><p>where Q is defined by <ref type="bibr" target="#b70">(70)</ref>.</p><p>Thanks to the inherent shrinkage behavior from the neural network training, (82) and (84) can be written as</p><formula xml:id="formula_148">f = Q(g),<label>(91)</label></formula><formula xml:id="formula_149">f n+1 = µP Λ g + (I -µP Λ )Q(f n ), (<label>92</label></formula><formula xml:id="formula_150">)</formula><p>where Q(•) denotes the deep convolutional framelet output. Note that (91) is in fact the existing deep learning denoising algorithm <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b3">4]</ref>, whereas the first iteration of (92) corresponds to the existing deep learning-based inpainting algorithm <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b72">72]</ref>. This again confirms that deep convolutional framelets is a general deep learning framework. Later, we will provide numerical experiments using (91) and (92) for image denoising and inpainting applications, respectively.  <ref type="figure" target="#fig_7">7</ref>(a), except that the final level of nonlinearity is not commonly used and the bypass connection is normally placed between the input and the output of the network. In practice, researchers empirically determine whether to use bypass connections or not by trial and error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Bypass connection versus no bypass connection. The bypass connection in</head><p>In order to understand the role of the bypass connection, we need to revisit the single-layer training procedure in (89). Then, the network with the bypass connection can be written as</p><formula xml:id="formula_151">Q(f (i) ; Ψ, Ψ, b enc , b dec ) = Q(f (i) ; Ψ, Ψ, b enc , b dec ) + f (i) , where Q(f (i) ; Ψ, Ψ, b enc , b dec ) = ΦC[f (i) ] ν( Ψ) + 1 n b dec , C[f (i) ] = Φ f (i) Ψ + 1 n b enc .</formula><p>Next, note that f (i) is contaminated with artifacts, so it can be written by</p><formula xml:id="formula_152">f (i) = f * (i) + h (i) ,</formula><p>where h (i) denotes the noise components and f * (i) refers to the noise-free ground truth. Therefore, the network training (89) using the bypass connection can be equivalently written as min</p><formula xml:id="formula_153">(Ψ, Ψ,benc,b dec ) N i=1 h (i) + Q(f * (i) + h (i) ; Ψ, Ψ, b enc , b dec ) 2 .<label>(93)</label></formula><p>Therefore, if we can find an encoder filter Ψ that approximately annihilates the true signal f * (i) , i.e.,</p><formula xml:id="formula_154">f * (i)</formula><p>Ψ 0, (94) then we have</p><formula xml:id="formula_155">C[f * (i) + h (i) ] = Φ (f * (i) + h (i) ) Ψ + 1 n b enc Φ h (i) Ψ + 1 n b enc = C[h (i) ].</formula><p>With this filter Ψ, the decoder filter Ψ can be designed such that</p><formula xml:id="formula_156">ΦC[h (i) ] ν( Ψ) = ΦΦ (h (i) ) Ψ ν( Ψ) = h (i) Ψ ν( Ψ) -h (i)</formula><p>by minimizing the cost (93). Thus, our deep convolutional framelet with a bypass connection can recover the artifact signal h (i) (hence, it recovers f * (i) by subtracting it from f (i) ). In fact, (94) is equivalent to the filter condition in (80) in ResNet which spans the minimum singular vector subspace.</p><p>Using similar arguments, we can see that if the encoder filter annihilates the artifacts, i.e.,</p><formula xml:id="formula_157">h (i) Ψ 0, (95) then C[f * (i) + h (i) ] C[f * (i)</formula><p>] and our deep convolutional framelet without the bypass connection can recover the ground-truth signal f * (i) , i.e., Q(f (i) ; Ψ, Ψ) f * (i) , by minimizing the cost (93).</p><p>In brief, if the true underlying signal has a lower-dimensional structure than the artifact, then the annihilating filter relationship in (94) is easier to achieve <ref type="bibr" target="#b65">[65]</ref>; thus, the neural network with the bypass connection achieves better performance. On the other hand, if the artifact has the lower-dimensional structure, a neural network with no bypass connection is better. This coincides with many empirical findings. For example, in image denoising <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b75">75]</ref> or streaking artifact problems <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b34">34]</ref>, a residual network works better, since the artifacts are random and have complicated distribution. In contrast, to remove the cupping artifacts in the X-ray interior tomography problem, a CNN without skipped connections is better, since the cupping artifacts are usually smoothly varying <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multiresolution analysis via deep convolutional framelets.</head><p>In deep convolutional framelets, for a given nonlocal basis, the local convolution filters are trained to give the best shrinkage behavior. Thus, nonlocal basis Φ is an important design parameter that controls the performance. In particular, the energy compaction property for the deep convolutional framelets is significantly affected by Φ. Recall that the SVD basis for the Hankel matrix results in the best energy compaction property; however, the SVD basis varies depending on the input signal type, so we cannot use the same basis for various input data.</p><p>Therefore, we should choose an analytic nonlocal basis Φ that can approximate the SVD basis and results in good energy compaction. Thus the wavelet is one of the preferred choices for piecewise continuous signals and images <ref type="bibr" target="#b17">[17]</ref>. Specifically, in a wavelet basis, the standard pooling and unpooling networks are used as low-frequency paths for wavelet transform, but additional high-frequency paths also exist. Another important motivation for multiresolution analysis (MRA) of convolutional framelets is the exponentially large receptive field. For example, Figure <ref type="figure" target="#fig_10">8</ref> compares the network depth-wise effective receptive field of a multiresolutional network with pooling against that of a baseline network without pooling layers. With samesized convolutional filters, the effective receptive field is enlarged in the network with pooling layers. Therefore, our MRA is indeed derived to supplement the enlarged receptive field from pooling layers with fine-detailed processing using high-pass band convolutional framelets.</p><p>4.1. Limitations of U-Net. Before we explain our multiresolution deep convolutional framelets, we first discuss the limitations of the popular multiresolution deep learning architecture called U-Net <ref type="bibr" target="#b58">[58]</ref>, which is composed of an encoder and decoder network with a skipped connection. U-Net utilizes pooling and unpooling as shown in Figure <ref type="figure" target="#fig_15">10</ref>(a) to obtain an exponentially large receptive field. Specifically, the average and max pooling operators Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Φ ave , Φ max ∈ R n× n 2 for f ∈ R n used by U-Net are defined as follows:</p><formula xml:id="formula_158">Φ ave = 1 √ 2             1 0 • • • 0 1 0 • • • 0 0 1 • • • 0 0 1 • • • 0 . . . . . . . . . . . . 0 0 • • • 1 0 0 . . . 1             , Φ max =             b 1 0 • • • 0 1 -b 1 0 • • • 0 0 b 2 • • • 0 0 1 -b 2 • • • 0 . . . . . . . . . . . . 0 0 • • • b n 2 0 0 . . . 1 -b n 2             , (<label>96</label></formula><formula xml:id="formula_159">)</formula><p>where {b i } i in max pooling are random (0, 1) binary numbers that are determined by the signal statistics. We can easily see that the columns of max pooling and average pooling are orthogonal to each other; however, they does not constitute a basis because they does not span R n . What, then, does this network perform?</p><p>Recall that for the case of average pooling, the unpooling layer Φ has the same form as the pooling, i.e., Φ = Φ. In this case, under the frame condition for the local basis Ψ Ψ = I d×d , the signal after pooling and unpooling becomes.  <ref type="figure" target="#fig_15">10</ref>(a). Specifically, by combining the low-pass and bypass connections, the augmented convolutional framelet coefficients C aug can be represented by</p><formula xml:id="formula_160">f = H † d Φ(Φ H d (f ) = ΦΦ f,</formula><formula xml:id="formula_161">C aug = Φ aug H d (f )Ψ = Φ aug (f Ψ) = C S ,<label>(97)</label></formula><p>where</p><formula xml:id="formula_162">Φ aug := I Φ , C := f Ψ, S := Φ (f Ψ). (<label>98</label></formula><formula xml:id="formula_163">)</formula><p>After unpooling, the low-pass branch signal becomes ΦS = ΦΦ (f Ψ), so the signals at the concatenation layer are then given by</p><formula xml:id="formula_164">W = H d (f )Ψ ΦΦ H d (f )Ψ = f Ψ ΦΦ (f Ψ) , (<label>99</label></formula><formula xml:id="formula_165">)</formula><p>where the first element in W comes from the bypass connection. The final step of recovery can then be represented by</p><formula xml:id="formula_166">f = H † d W Ψ 1 Ψ 2 = H † d (H d (f )Ψ Ψ 1 ) + H † d ΦΦ H d (f )Ψ Ψ 2 = 1 d q i=1 f ψ i ψ1 i + ΦΦ (f ψ i ) ψ2 i ,<label>(100)</label></formula><p>where the last equality comes from (A.9) and (A.6) in Lemma 14. Note that this does not guarantee PR because the low-frequency component ΦΦ (f ψ i ) is contained in both terms of (100); thus the low-frequency component is over-emphasized, which is believed to be the main source of smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Proposed MRA.</head><p>To address the limitations of U-Net, here we propose a novel MRA using a wavelet nonlocal basis. As discussed before, at the first layer we are interested in learning Ψ (1) and Ψ(1) such that <ref type="formula" target="#formula_0">1</ref>) , where C (1) := Φ (1) H d (1) (f )Ψ (1) .</p><formula xml:id="formula_167">H d (1) (f ) = Φ (1) C (1) Ψ(</formula><p>For MRA, we decompose the nonlocal orthonormal basis Φ (1) into low-and high-frequency subbands, i.e., Φ (1) = Φ</p><p>(1)</p><p>high .</p><p>For example, if we use the Haar wavelet, the first-layer operators Φ</p><p>low , Φ</p><p>high ∈ R n× n 2 are given by  </p><formula xml:id="formula_171">Φ (1) low = 1 √ 2             1 0 • • • 0 1 0 • • • 0 0 1 • • • 0 0 1 • • • 0 . . . . . . . . . . . . 0 0 • • • 1 0 0 . . . 1             , Φ<label>(1)</label></formula><formula xml:id="formula_172">high = 1 √ 2             1 0 • • • 0 -1 0 • • • 0 0 1 • • • 0 0 -1 • • • 0 . . . . . . . . . . . . 0 0 • • • 1 0 0 . . . -1             . Downloaded 04/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note that Φ (1)</head><p>low is exactly the same as the average pooling operation in (96); however, unlike the pooling in U-Net, Φ (1) = [ Φ (1)   low Φ <ref type="bibr" target="#b0">(1)</ref> high ] now constitutes an orthonormal basis in R n thanks to Φ <ref type="bibr" target="#b0">(1)</ref> high . We also define the approximate signal C </p><formula xml:id="formula_173">low := Φ (1) low H d (1) (f )Ψ (1) = Φ (1) low f Ψ (1) , C<label>(1)</label></formula><p>high := Φ</p><p>(1)</p><formula xml:id="formula_174">high H d (1) (f )Ψ (1) = Φ (1) high f Ψ (1)</formula><p>such that</p><formula xml:id="formula_175">C (1) = Φ (1) H d (1) (f )Ψ (1) = C (1) low C (1) high .</formula><p>Note that this operation corresponds to local filtering followed by nonlocal basis matrix multiplication, as shown by the red blocks of Figure <ref type="figure" target="#fig_11">9</ref>(a). Then, at the first layer, we have the following decomposition:</p><formula xml:id="formula_176">H d (1) (f ) = Φ (1) C (1) Ψ(1) = Φ (1) low C (1) low Ψ(1) + Φ (1)</formula><p>high C</p><p>(1) high</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ψ(1) .</head><p>At the second layer, we proceed in a similar manner using the approximate signal C</p><p>(1)</p><p>low . More specifically, we are interested in using orthonormal nonlocal bases:</p><formula xml:id="formula_177">Φ (2) = [ Φ (2) low Φ (2) high ],</formula><p>where Φ low ∈ R n/2×d (1) to low and high bands, respectively (see Figure <ref type="figure" target="#fig_11">9(a)</ref>): </p><formula xml:id="formula_178">H d (2) |p (2) C (1) low = Φ (2) low C (2) low Ψ(2) + Φ (2) high C (2) high Ψ(2) ,</formula><formula xml:id="formula_179">low := Φ (2) low H d (2) |p (2) C (1) low Ψ (2) = Φ (2) low C (1) low Ψ (2) , C<label>(2)</label></formula><p>high := Φ</p><p>(2)</p><formula xml:id="formula_180">high H d (2) |p (2) C (1) low Ψ (2) = Φ (2)</formula><p>high C</p><p>(1) low Ψ (2) . (101) Again, Φ</p><p>(2) low corresponds to the standard average pooling operation. Note that we need a lifting operation for an extended Hankel matrix with p (2) = d (1) Hankel blocks in (101), because the first layer generates p (2) filtered output which needs to be convolved with d (2) -length filters in the second layer.</p><p>Similarly, the approximate signal needs further processing from the following layers. In general, for l = 1, . . . , L, we have</p><formula xml:id="formula_181">H d (l) |p (l) C (l-1) low = Φ (l) low C (l) low Ψ(l) + Φ (l) high C (l) high Ψ(l) ,</formula><p>where</p><formula xml:id="formula_182">C (l) low := Φ (l) low H d (l) |p (l) C (l-1) low Ψ (l) = Φ (l) low C (l-1) low Ψ (l) , C<label>(l)</label></formula><formula xml:id="formula_183">high := Φ (l) high H d (l) |p (l) C (l-1) low Ψ (l) = Φ (l) high C (l-1) low Ψ (l) ,</formula><p>where p (l) denotes the dimension of the local basis at lth layer. This results in L-layer deep convolutional framelets using Haar wavelets. The multilayer implementation of convolution framelets now results in an interesting encoder-decoder deep network structure as shown in Figure <ref type="figure" target="#fig_11">9(a)</ref>, where the red and blue blocks represent the encoder and decoder blocks, respectively. In addition, Table <ref type="table">2</ref> summarizes the dimensions of the lth layer matrices. More specifically, with ReLU, the encoder parts are given as follows:</p><p>Table <ref type="table">2</ref> The nomenclature and dimensions of the matrices at the lth layer MRA using deep convolutional framelets. Here, d (l) denotes the filter length, and p (l) = p (l-1) d (l-1) with p (0) = d (0) = 1 refers to the number of Hankel blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Symbol Dimension</head><p>Nonlocal basis Φ (l)</p><formula xml:id="formula_184">n 2 l-1 × n 2 l-1 Low-band nonlocal basis Φ (l) low n 2 l-1 × n 2 l High-band nonlocal basis Φ (l) high n 2 l-1 × n 2 l</formula><p>Local basis</p><formula xml:id="formula_185">Ψ (l) p (l) d (l) × p (l) d (l) Dual local basis Ψ(l) p (l) d (l) × p (l) d (l) Signal and its estimate C (l) , Ĉ(l) n 2 l-1 × p (l) d (l) Approximate signal and its estimate C (l) low , Ĉ(l) low n 2 l × p (l) d (l) Detail signal C (l) high n 2 l × p (l) d (l) Hankel lifting of low-band signals H d (l) |p (l) (C (l-1) low ) n 2 l-1 × p (l) d (l)</formula><p>Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><formula xml:id="formula_186">               C (1) low = ρ Φ (1) low f Ψ (1)</formula><p>, C</p><p>(1)</p><formula xml:id="formula_187">high = Φ (1) high f Ψ (1) , C<label>(2)</label></formula><formula xml:id="formula_188">low = ρ Φ (2) low C (1) low Ψ (2)</formula><p>, C</p><p>(2)</p><formula xml:id="formula_189">high = Φ (2)</formula><p>high C</p><p>(1) low</p><formula xml:id="formula_190">Ψ (2) , . . . C (L) low = ρ Φ (L) low C (L-1) low Ψ (L) , C (L) high = Φ (L) high C (L-1) low Ψ (L) .</formula><p>On the other hand, the decoder part is given by</p><formula xml:id="formula_191">               Ĉ(L-1) low = ρ H † d (L) |p (L) Φ (L) Ĉ(L) Ψ(L) = ρ Φ (L) Ĉ(L) ν( Ψ(L) ) , . . . Ĉ(1) low = ρ H † d (2) |p (2) Φ (2) Ĉ(2) Ψ(2) = ρ Φ (2) Ĉ(2) ν( Ψ(2) ) , f = H † d (1) Φ (1) Ĉ(1) Ψ(1) = ρ Φ (1) Ĉ(1) ν( Ψ(1) ) ,<label>(102)</label></formula><p>where ν(Ψ) is defined in ( <ref type="formula" target="#formula_86">54</ref>) and we use</p><formula xml:id="formula_192">Φ (l) Ĉ(l) Ψ(l) = Φ (l) low Ĉ(l) low Ψ(l) + Φ (l) high Ĉ(l) high Ψ(l) ,</formula><p>where we could further process high-frequency components as</p><formula xml:id="formula_193">Ĉ(L) high = C (L) high H (L) (103)</formula><p>for some filter H (L) , and Ĉ(L) low is the decoded low-frequency band from the (L -1)th resolution layer, which can be further processed with additional filters.</p><p>Figure <ref type="figure" target="#fig_11">9</ref>(b) shows the overall structure of MRA with convolutional framelets when length-2 local filters are used. Note that the structure is quite similar to the U-Net structure <ref type="bibr" target="#b58">[58]</ref>, except for the high-pass filter pass. This again confirms a close relationship between the deep convolutional framelets and deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results.</head><p>In this section, we investigate various inverse problem applications of deep convolutional framelets, including image denoising, sparse-view CT reconstruction, and inpainting. In particular, we will focus on our novel multiresolution deep convolutional framelets using Haar wavelets. For these applications, our multiresolution deep convolutional framelets should be extended to a 2-D structure, so the architecture in Figure <ref type="figure" target="#fig_11">9</ref>(b) should be modified. The resulting architectures are illustrated in Figures <ref type="figure" target="#fig_29">10(b</ref>) and 10(c). The 2-D U-Net, which is used as one of our reference networks, is shown in Figure <ref type="figure" target="#fig_29">10(a)</ref>. Note that the U-Net structure in Figure <ref type="figure" target="#fig_29">10(a</ref>) is exactly the same as the reconstruction network for inverse problems in Jin et al. <ref type="bibr" target="#b34">[34]</ref> and Han, Yoo, and Ye <ref type="bibr" target="#b28">[28]</ref>.</p><p>More specifically, the networks are composed of a convolution layer, ReLU, and a skipped connection with concatenation. Each stage consists of four 3 × 3 convolution layers followed by ReLU, except for the final stage and the last layer. The final stage is composed of two 3 × 3 convolution layers for each low-pass and high-pass branch, and the last layer is a 1 × 1 convolution layer. In Figures <ref type="figure" target="#fig_29">10(b</ref> layer, the wavelet transform generates four subbands: HH, HL, LH, and LL. The LL band is processed using convolutional layers followed by another wavelet-based pooling layer. As shown in Figures <ref type="figure" target="#fig_29">10(b</ref>) and 10(c), the channels are doubled after wavelet decomposition. Therefore, the number of convolution kernels increases from 64 in the first layer to 1024 in the final stage. In Figure <ref type="figure" target="#fig_15">10</ref>(b) we have additional filters for the high-pass branches of the highest layer, whereas Figure <ref type="figure" target="#fig_29">10(c</ref>) only has a skipped connection. The reason we do not include an additional filter in Figure <ref type="figure" target="#fig_29">10(c</ref>) is to verify that the improvement over U-Net does not come from the additional filters in the high-pass bands but rather from the wavelet-based nonlocal basis. For a fair comparison with the U-Net structure in Figure <ref type="figure" target="#fig_15">10</ref>(a), our proposed network in Figures <ref type="figure" target="#fig_29">10(b</ref>) and 10(c) also have concatenation layers that stack all subband signals before applying filtering.</p><p>Although Figures <ref type="figure" target="#fig_29">10(b</ref>) and 10(c) appear similar to Figure <ref type="figure" target="#fig_15">10</ref>(a), there exist fundamental differences due to the additional high-pass connections. In particular, in Figures <ref type="figure" target="#fig_29">10(b</ref>) and 10(c), there exist skipped connections at the HH, HL, and LH subbands, whereas the U-Net structure in Figure <ref type="figure" target="#fig_29">10(a)</ref> does not have high-pass filtering before the bypass connection. Accordingly, as explained in section 4.1, U-Net does not satisfy the frame condition, emphasizing the low-pass components. Our networks in Figures <ref type="figure" target="#fig_29">10(b</ref>) and 10(c) do satisfy the frame condition, so we conjecture that high-pass signals can be better recovered by the proposed network. Our numerical results in the following indeed provide empirical evidence of our claim. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b75">75]</ref>. In this section, we will therefore show that our proposed multiresolution deep convolutional framelets outperform the standard U-Net method at the denoising task. Specifically, the proposed network and other networks were trained to learn the noise pattern similarly as in <ref type="bibr" target="#b75">[75]</ref>. The noise-free image can then be obtained by subtracting the estimated noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image denoising. Nowadays, deep CNN-based algorithms have achieved great performance in image denoising</head><p>For training and validation, a dataset of DIVerse 2K resolution images (DIV2K) <ref type="bibr" target="#b1">[2]</ref> was used to train the proposed network in Figure <ref type="figure" target="#fig_29">10(b)</ref>. Specifically, 800 and 200 images from the dataset were used for training and validation, respectively. The noisy input images were generated by adding Gaussian noise of σ = 30. To train the network with various noise patterns, the Gaussian noise was regenerated at every epoch during training.</p><p>The proposed network was trained by Adam optimization <ref type="bibr" target="#b42">[42]</ref> with the momentum β 1 = 0.5. The initial learning rate was set to 0.0001, and it was divided in half every 25 iterations, until it reached around 0.00001. The size of the patch was 256 × 256, with a mini-batch size of 8. The network was trained using 249 epochs. The proposed network was implemented in Python using the TensorFlow library <ref type="bibr" target="#b0">[1]</ref> and trained using a GeForce GTX 1080. The Gaussian denoising network took about two days to be trained.</p><p>The standard U-Net structure in Figure <ref type="figure" target="#fig_15">10</ref>(a) was used as a baseline network for comparison. In addition, RED-Net <ref type="bibr" target="#b51">[51]</ref> was used as another baseline network. For a fair comparison, RED-Net was implemented using identical hyperparameters. More specifically, the number of filter channels at the finest scale was 64 and we used 3 × 3 filters. Furthermore, we used a mini-batch size of 8. These networks were trained under the same conditions. To evaluate the trained network, we used Set12, Set14, and BSD68, and the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) index <ref type="bibr" target="#b67">[67]</ref> were calculated for a quantitative evaluation. The PSNR is used to measure the quality of the reconstructed image, which is defined as</p><formula xml:id="formula_194">PSNR = 20 • log 10   MAX Y MSE( X), Y )   ,<label>(104)</label></formula><p>where X and Y denote the reconstructed and noise-free (ground truth) images, respectively. MAX Y is the maximum value of the noise-free image. SSIM is used to measure the similarity between the original image and the distorted image due to deformation, and it is defined as</p><formula xml:id="formula_195">(105) SSIM = (2µ X µ Y + c 1 )(2σ XY + c 2 ) µ 2 X + µ 2 Y + c 1 σ 2 X + σ 2 Y + c 2</formula><p>, where µ M is an average of M , σ 2 M is a variance of M , and σ M N is a covariance of M and N . To stabilize the division, c 1 = (k 1 R) 2 and c 2 = (k 2 R) 2 are defined in terms of R, which is the dynamic range of the pixel values. We followed the default values of k 1 = 0.01 and k 2 = 0.03.</p><p>Table <ref type="table" target="#tab_5">3</ref> shows a quantitative comparison of the denoising performance. The proposed network was superior to U-Net and RED-Net in terms of PSNR for all test datasets with Gaussian noise σ = 30. Specifically, edge structures were smoothed out by the standard U-Net and RED-Net networks, whereas edge structures were quite accurately recovered by the proposed network, as shown in Figure <ref type="figure" target="#fig_16">11</ref>. This is because of the additional high-pass branches in the proposed network, which enable the image detail to be well recovered. These results confirm that imposing the frame condition for the nonlocal basis is useful in recovering high-resolution images, as predicted by our theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Sparse-view CT reconstruction.</head><p>In X-ray CT, due to the potential risk of radiation exposure, the main research aim is to reduce the radiation dose. Among various approaches for low-dose CT, sparse-view CT is a recent proposal that reduces the radiation dose by reducing the number of projection views <ref type="bibr" target="#b61">[61]</ref>. However, due to the insufficient number of projection views, standard reconstruction using the filtered back-projection (FBP) algorithm exhibits severe streaking artifacts that are globally distributed. Accordingly, researchers have extensively employed compressed sensing approaches, that minimize the total variation <ref type="bibr" target="#b20">[20]</ref> or other sparsity-inducing penalties under the data fidelity <ref type="bibr" target="#b61">[61]</ref>. These approaches are, however, computationally very expensive due to the repeated applications of projection and backprojection during iterative update steps.</p><p>Therefore, the main goal of this experiment is to apply the proposed network to sparse-view CT reconstruction such that it outperforms the existing approaches in terms of computational Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php speed and reconstruction quality. To address this, our network is trained to learn streaking artifacts as suggested in <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b34">34]</ref> using the new network architecture in Figure <ref type="figure" target="#fig_29">10(c)</ref>. For training data, we used the ten-patient dataset provided by the AAPM's Low Dose CT Grand Challenge (http://www.aapm.org/GrandChallenge/LowDoseCT/). The initial images were reconstructed by 3-D CT 2304 projection data. To generate several sparse-view images, the measurements were regenerated by the radon operator in MATLAB. The data was composed of 2-D CT projection data from 720 views. Artifact-free original images were generated by the iradon operator in MATLAB using all 720 projection views. The input images with streaking artifacts were generated using the iradon operator from 60, 120, 240, and 360 projection views, respectively. These sparse-view images correspond to each downsampling factor x12, x6, x3, and x2. Then, the network was trained to remove the artifacts.</p><p>Among the ten-patient dataset, data from eight patients was used for training, one patient's data was used for validation, and a test was conducted using the remaining patient's data. This corresponds to 3720 and 254 slices of 512 × 512 images for the training and validation data, respectively, and 486 slices of 512 × 512 images for the test data. The training data was augmented by conducting horizontal and vertical flipping.</p><p>For the baseline comparison network, we used the U-Net structure in Figure <ref type="figure" target="#fig_15">10</ref>(a) and a single-resolution CNN similar to the experimental setup in Jin et al. <ref type="bibr" target="#b34">[34]</ref> and Han, You, and Ye <ref type="bibr" target="#b28">[28]</ref>. The single-resolution CNN had the same architecture as U-Net in Figure <ref type="figure" target="#fig_15">10</ref>(a), except that pooling and unpooling were not used. All these networks were trained similarly using the same dataset. For quantitative evaluation, we used the normalized mean square error (NMSE) and the PSNR.</p><p>The proposed network was trained by stochastic gradient descent. The regularization parameter was λ = 10 -4 . The learning rate was set from 10 -3 to 10 -5 and was gradually reduced at each epoch. The number of epochs was 150. Mini-batch data using an image patch were used, and the size of the image patch was 256×256. The network was implemented using the MatConvNet toolbox (version 24) <ref type="bibr" target="#b64">[64]</ref> in the MATLAB 2015a environment (MathWorks, Natick, MA). We used a GTX 1080 Ti graphics processor and an i7-7770 CPU (3.60 GHz). The network took about four days for training. Baseline networks were trained similarly.</p><p>Table <ref type="table" target="#tab_6">4</ref> illustrates the average PSNR values for reconstruction results from various numbers of projection views. Due to the high-pass branches of the network, the deep convolutional framelets produced consistently improved images quantitatively across all view downsampling factors. Moreover, visual improvements from the proposed network are more remarkable. For example, Figure <ref type="figure" target="#fig_17">12</ref> shows reconstruction results from 60 projection views. Due to the severe view downsampling, the FBP reconstruction result in Figure <ref type="figure" target="#fig_17">12</ref>(b) produces a severely corrupted image with significant streaking artifacts. Accordingly, all the reconstruction results in Figures <ref type="figure" target="#fig_29">12(c</ref>)-(e) were not compatible with the full-view reconstruction results in Figure <ref type="figure" target="#fig_17">12</ref>(a). In particular, there were significant remaining streaking artifacts for the conventional CNN architecture (Figure <ref type="figure" target="#fig_29">12(c</ref>)), which were reduced using U-Net as shown in Figure <ref type="figure" target="#fig_29">12(d)</ref>. However, as indicated by the arrow, some blurring artifacts were still visible. On the other hand, the proposed network removes the streaking and blurring artifacts as shown in Figure <ref type="figure" target="#fig_17">12</ref>(e). Quantitative evaluation also showed that the proposed deep convolutional framelets have the minimum number of NMSE values.</p><p>As for reconstruction results from larger numbers of projection views, Figure <ref type="figure" target="#fig_18">13</ref> shows reconstruction results from 90 projection views. All the algorithms are significantly improved compared to the 60-view reconstruction. However, in the reconstruction results by singleresolution CNN in Figure <ref type="figure" target="#fig_18">13</ref>(b) and U-Net in Figure <ref type="figure" target="#fig_30">13(c</ref>), the details have disappeared. On the other hand, most of the detailed structures were well reconstructed by the proposed deep convolutional framelets as shown in Figure <ref type="figure" target="#fig_30">13(e)</ref>. Quantitative evaluation also showed that the proposed deep convolutional framelets have the minimum number of NMSE values. The zoomed areas in Figure <ref type="figure" target="#fig_18">13</ref> also confirm these findings. The reconstruction result by the deep convolutional framelets provided a very realistic image, whereas the other results are somewhat blurry. Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php These experimental results clearly confirm that the proposed network is quite universal in the sense that it can be used for various artifact patterns. This is due to the network structure retaining the high-pass subbands, which automatically adapts the resolutions even though various scales of image artifacts are present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image inpainting.</head><p>Image inpainting is a classical image processing problem whose goal is to estimate the missing pixels in an image. Image inpainting has many scientific and engineering applications. Recently, the field of image inpainting has been revolutionized due to advances in CNN-based inpainting algorithms <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b72">72]</ref>. One of the remarkable aspects of these approaches is the superior performance improvement over existing methods in spite of its ultrafast run-time speed. Despite this stellar performance, the link between deep learning and classical inpainting approaches remains poorly understood. In this section, inspired by classical frame-based inpainting algorithms <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b10">10]</ref>, we will show that the CNN-based image inpainting algorithm is indeed the first iteration of deep convolutional framelet inpainting, so the inpainting performance can be improved with multiple iterations of inpainting and image update steps using CNN.</p><p>More specifically, similar to the classical frame-based inpainting algorithms <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b10">10]</ref>, we use the update algorithm in (92) derived from the PR condition, which is written again as follows:</p><p>Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php </p><formula xml:id="formula_196">f n+1 = µP Λ g + (I -µP Λ )Q(f n ), (<label>106</label></formula><formula xml:id="formula_197">)</formula><p>where Q is our deep convolutional framelet output. However, unlike in existing works that use tight frames <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b10">10]</ref>, our deep convolutional framelet does not satisfy the tight frame condition, so we relax the iteration using the Krasnoselskii-Mann (KM) method <ref type="bibr" target="#b6">[6]</ref> as summarized in Algorithm 1.</p><p>Algorithm 1. Pseudocode implementation.</p><p>1: Train a deep network Q using a training dataset.</p><p>2: Set 0 ≤ µ &lt; 1 and 0 &lt; λ n &lt; 1∀n.</p><p>3: Set initial guesses of f 0 and f 1 .</p><p>4: for n = 1, 2, . . . , until convergence do 5:</p><p>q n := Q(f n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>fn+1 := µP Λ g + (I -µP Λ )q n 7:</p><formula xml:id="formula_198">f n+1 := f n + λ n ( fn+1 -f n ) 8: end for</formula><p>Note that the resulting inpainting algorithm assumes the form of the recursive neural network (RNN), because the CNN output is used as the input for the CNN another iteration. The corresponding inference step based on Algorithm 1 is illustrated in Figure <ref type="figure" target="#fig_19">14</ref>. As for the CNN building block of the proposed RNN, the multiresolution deep convolution framelets of Figure <ref type="figure" target="#fig_15">10</ref>(b) are used.</p><p>We performed inpainting experiments using randomly subsampled images. We used the DIV2K dataset <ref type="bibr" target="#b1">[2]</ref> for our experiments. Specifically, 800 images from the database were used for training, and 200 images were used for the validation. In addition, the training data was augmented by conducting horizontal and vertical flipping and rotation. For the task of inpainting randomly subsampled images, 75%, 80%, and 85% of pixels were randomly removed from the images every other epoch during the training. All images were rescaled to have values between 0 and 1. For training, Adam optimization <ref type="bibr" target="#b42">[42]</ref> with the momentum β 1 = 0.5 was used. The learning rate for the generators was set to 0.0001, and it was divided in half every 50 iterations, until it reached around 0.00001. The size of the patch was 128 × 128. We used a mini-batch size of 32 for training of the random missing images.</p><p>Since we also wanted our network to perform inferences on intermediate reconstruction images, the network needed to be trained with intermediate results. This training procedure was implemented using multiple intermediate results as inputs; see Figure <ref type="figure" target="#fig_19">14</ref>. In particular, we trained the network at multiple stages. At the first stage, we trained the network using the initial dataset D init , which is composed of missing images and corresponding label images. Once the network was trained using D init , the input data for network training was replaced with the first inference result f 1 = Q k (f 0 ) and the label images, where Q k is the kth trained network and f k denotes the kth inference result. That is, at the kth stage, the network was trained to fit the kth inference result f k = Q l (f k-1 ) to the corresponding label data.</p><p>The proposed network was implemented in Python using the TensorFlow library <ref type="bibr" target="#b0">[1]</ref> and trained using a GeForce GTX 1080ti. The training time of the inpainting network for randomly missing data was about six days.  To evaluate the trained network, we used the Set5, Set14, and BSD100 datasets for testing. Figure <ref type="figure" target="#fig_20">15</ref> shows a typical image update from the proposed RNN structure. As the number of iterations increases the images are gradually improved, as the CNN works as an image restoration network during the RNN update. The associated PSNR graph in Figure <ref type="figure" target="#fig_21">16</ref> confirms that the algorithm converges after six RNN update steps. We compared our algorithm with RED-Net <ref type="bibr" target="#b51">[51]</ref> as a reference network. For a fair comparison, we trained RED-Net using the same 3 × 3 filter and the same number of channels, 64, which were also used for our proposed method. As shown in Figure <ref type="figure" target="#fig_22">17</ref>, the proposed method is able to restore the details and edges of images much better than RED-Net. In addition, the PSNR results in Table <ref type="table" target="#tab_7">5</ref> for images missing 80% of pixels indicate that the proposed method outperformed RED-Net for the Set5 and Set14 data and was comparable for the BSD100 dataset.</p><p>While most existing inpainting networks are based on feed-forward networks <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b72">72]</ref>, our theory of deep convolutional framelets resulted in an RNN that gradually improved the image with CNN-based image restoration, making the algorithm more accurate for various missing patterns. These results have confirmed that the theoretical framework of deep convolutional framelets is promising in designing new deep learning algorithms for inverse problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussions.</head><p>Here we investigate whether our theory can be used to solve current theoretical issues and explain intriguing empirical findings from the machine learning community. Amazingly, our theoretical framework gives us many useful insights.</p><p>6.1. Low-rankness of the extended Hankel matrix. In our theory, we showed that deep convolutional framelets are closely related to the Hankel matrix decomposition, so the multilayer implementation of the convolutional framelets refines the bases such that maximal Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php  energy compaction can be achieved using a deep convolutional framelet expansion. In addition, we have shown that with insufficient filter channels, the rank structure of the extended Hankel matrix in successive layers is bounded by those of previous layers. This perspective suggests that the energy compaction happens across layers, and this can be investigated by the singular value spectrum of the extended Hankel matrix.</p><p>Here, we provide empirical evidence that the singular value spectrum of the extended Hankel matrix is compressed by going through more convolutional layers. For this experiment, Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php  we used the single-resolution CNN with encoder-decoder architecture as shown in Figure <ref type="figure" target="#fig_23">18</ref> for the sake of simplicity. The hyperparameters and dataset for the training were the same as those introduced in our denoising experiments. Since energy compaction occurs at the convolutional framelet coefficients, we have considered only the encoder part corresponding to the network from the first module to the fifth module. As shown in Figure <ref type="figure" target="#fig_24">19</ref>, the singular value spectrum of the extended Hankel matrix is compressed by going through the layer. This confirms our conjecture that the CNN is closely related to the low-rank Hankel matrix approximation.</p><p>6.2. Insights on classification networks. While our mathematical theory of deep convolutional framelets was derived for inverse problems, our findings have many important implications for deep learning networks in general. For example, we conjecture that the classification network corresponds to the encoder aspect of our deep convolutional framelets. More specifically, encoder can be used for energy compaction, where the classifier attached to the encoder can detect input signals based on compressed energy distributions. This is similar to Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php the classical classifier design, where the feature vector is first obtained by a dimensionality reduction algorithm, after which a support vector machine-type classifier is used. Accordingly, the role of the residual net, redundant channels, etc., are believed to hold for classifier networks as well. It is also important to note that the rank structure of the Hankel matrix, which determines the energy distributions of convolutional framelet coefficients, is translation-and rotation-invariant as shown in <ref type="bibr" target="#b36">[36]</ref>. The invariance property was considered to be the most important property, which gives the theoretical motivation for Mallat's wavelet scattering network <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b7">7]</ref>. Therefore, there may be an important connection between the deep convolutional framelets and wavelet scattering. However, this is beyond the scope of the current paper and will be left for future research.</p><p>6.3. Finite sample expressivity. Another interesting observation is that the PR condition is directly related to the finite sample expressivity of a neural network <ref type="bibr" target="#b74">[74]</ref>. Recently, a very intriguing article appeared providing empirical evidence that the traditional statistical learning theoretical approaches fail to explain why large neural networks generalize well in practice <ref type="bibr" target="#b74">[74]</ref>. To explain this, the authors showed that simple depth-two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points <ref type="bibr" target="#b74">[74]</ref>. We conjecture that perfect finite sample expressivity is closely related to the PR condition, stating that any input of finite sample size can be reproduced perfectly using a neural network. The intriguing link between the PR condition and finite sample expressivity needs further investigation.</p><p>6.4. Relationship to pyramidal residual networks. Another interesting aspect of our convolutional framelet analysis is the increased number of filter channels as shown in <ref type="bibr" target="#b67">(67)</ref>. While this does not appear to follow the conventional implementation of convolutional filter channels, there is a very interesting article that provides strong empirical evidence that supports our theoretical prediction. In the recent paper on pyramidal residual networks <ref type="bibr" target="#b25">[25]</ref>, the authors gradually increase the number of feature channels across layers. This design is proven to be an effective means of improving generalization ability. This coincides with our prediction in <ref type="bibr" target="#b67">(67)</ref>, which is that, in order to guarantee the PR condition, the number of filter channels should increase. This again suggests the theoretical potential of the proposed deep convolutional framelets.</p><p>6.5. Revisit to existing deep networks for inverse problems. Based on our theory for deep convolutional framelets, we now revisit existing deep learning algorithms for inverse problems and discuss their pros and cons.</p><p>By extending the work in <ref type="bibr" target="#b38">[38]</ref>, Kang et al. <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b39">39]</ref> proposed a wavelet domain residual learning (WavResNet) method for low-dose CT reconstruction. The key idea of WavResNet is to apply the directional wavelet transform first, after which a neural network is trained such that it can learn the mapping between noisy input wavelet coefficients and noiseless ones <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b39">39]</ref>. In essence, this can be interpreted as deep convolutional framelets with the nonlocal transform being performed first. The remaining layers are then composed of a CNN with local filters and residual blocks. Thanks to the global transform using directional wavelets, the signal becomes more compressed, which is the main advantage compared to the simple CNN. Another unique feature of WavResNet is the concatenation layer at each end that performs additional filtering by using all the intermediate results. This layer performs a signal boosting <ref type="bibr" target="#b40">[40]</ref>. However, due to the lack of pooling layers, the receptive field size is smaller than that of the multiscale network as shown in Figure <ref type="figure" target="#fig_10">8</ref>. Accordingly, the architecture is better suited for localized artifacts from low-dose CT noise, but is not effective for removing globalized artifact patterns from sparse-view CT.</p><p>AUtomated TransfOrm by Manifold APproximation (AUTOMAP) <ref type="bibr" target="#b76">[76]</ref> is a recently proposed neural network approach for image reconstruction, which is claimed to be general for various imaging modalities including MRI, CT, etc. This is similar to a standard CNN, except that at the first layer the nonlocal basis matrix is trained as a fully connected layer. Moreover, the original signal domain is the measurement domain, so only local filters are followed in successive layers with no additional fully connected layers for inversion. In theory, learning-based nonlocal transforms can be optimally adapted to the signals, so they are believed to be preferential to standard CNNs. However, in order to use the fully connected layer as a nonlocal basis, a network of huge size is required. For example, in order to recover an N × N image, the number of parameters for the fully connected layer needs to be 2N 2 × N 2 (see <ref type="bibr" target="#b76">[76]</ref> for just calculations of the required parameter numbers). Thus, if one attempts to learn a CT image of size 512 × 512 (i.e., N = 2 9 ) using AUTOMAP, the required memory becomes 2N 4 = 2 37 , which is neither possible to store nor to avoid any overfitting during the learning. This is another reason we prefer to use analytic nonlocal bases. However, if the measurement size is sufficiently small, the approach by AUTOMAP may be an interesting direction to investigate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions.</head><p>In this paper, we propose a general deep learning framework called deep convolutional framelets for inverse problems. The proposed network architectures were based on key fundamental theoretical advances that we have achieved. First, we show that deep learning is closely related to the existing theory of annihilating filter-based low-rank Hankel matrix approaches and convolution framelets. In particular, our theory was motivated by the observation that when a signal is lifted to a high-dimensional Hankel matrix, it usually results in a low-rank structure. Furthermore, the lifted Hankel matrix can be decomposed using nonlocal and local bases. We further showed that the convolution framelet expansion can be equivalently represented as an encoder-decoder convolutional layer structure. By extending this idea further, we also derived the multilayer convolution framelet expansion and associated encoder-decoder network. Furthermore, we investigated the perfect reconstruction condition for the deep convolutional framelets. In particular, we showed that the perfect reconstruction is still possible even when the framelet coefficients are processed with ReLU. We also proposed a novel class of deep networks using multiresolution convolutional framelets.</p><p>Our discovery provides a theoretical rationale for many existing deep learning architectures and components. In addition, our theoretical framework can address some of the fundamental questions that we raised in the introduction. More specifically, we showed that the convolutional filters work as local bases and the number of channels can be determined based on the perfect reconstruction condition. Interestingly, by controlling the number of filter channels we can achieve a low-rank-based shrinkage behavior. We further showed that ReLU can disappear when paired filter channels with opposite polarity are available. Another important and novel theoretical contribution is that, thanks to the lifting to the Hankel structured matrix, we can show that the pooling and unpooling layers actually come from nonlocal bases, so they should be augmented with high-pass branches to meet the frame condition. Our deep Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php convolutional framelets can also explain the role of the bypass connection. Finally, we have shown that the depth of the network is determined by considering the intrinsic rank of the signal and the convolution filter length. Our numerical results also showed that the proposed deep convolutional framelets can provide improved reconstruction performance under various conditions.</p><p>One of the limitations of the current work is that our analysis, taking perfect reconstruction as an example, is based on a deterministic framework, while most of the mysteries of deep learning are probabilistic in nature, which apply over complex high-dimensional image distributions. Understanding the link between the deterministic and probabilistic views of deep learning is a very important issue, which should be explored further in the future.</p><p>Appendix A. Basic properties of Hankel matrices. The following basic properties of Hankel matrices, which are mostly novel, will be useful in proving the main theoretical results.</p><p>Lemma 14. For a given f ∈ R n , let H d (f ) ∈ H(n, d) denote the associated Hankel matrix. Let Φ ∈ R n×m and Ψ ∈ R d×q be matrices whose dimensions are chosen such that Φ and Ψ can be multiplied to the left and right of H d (f ). Suppose, furthermore, that Φ ∈ R n×m and Ψ ∈ R d×q are another set of matrices that match the dimensions of Φ and Ψ, respectively. Then, the following statements are true.</p><p>1. Let where e k denotes the standard coordinate vector in R n , where only the kth element is 1. Then, the set {E k } n k=1 is the orthonormal basis for the space of H(n, d). where ξ j i (resp., ξj i ) denotes the ith column of Ξ j (resp., Ξj ). Proof. The proof is a simple application of the definition of the Hankel matrix and convolutional framelets. We prove the claims one by one.</p><formula xml:id="formula_199">E k = 1 √ d H d (e k ) ∈</formula><p>1. The proof can be found in <ref type="bibr" target="#b71">[71]</ref>.</p><p>2. Because {E k } n k=1 constitutes an orthonormal basis, for any F ∈ H(n, d), we have</p><formula xml:id="formula_200">F = n k=1 E k , F E k .</formula><p>Furthermore, the operator H d : f → H(n, d) is linear, so we have</p><formula xml:id="formula_201">H d (f ) = H d n k=1 f [k]e k = k=1 f [k]H d (e k ) = √ d n k=1 f [k]E k ,</formula><p>where the last equality comes from (A.1). </p><formula xml:id="formula_202">E k , uv = 1 √ d e k (u v) = 1 √ d (u v)[k].</formula><p>5. We need to show that Thus, we have</p><formula xml:id="formula_203">H † d (H d (f )) = f for any f = [f [1] • • • f [n]] T ∈ R n : H † d (H d (f )) = 1 √ d      E 1 , H d (f ) E 2 , H d (f ) . . . E n , H d (f )      = 1 √ d      √ df [1] √ df [2] . . . √ df [n]      = f,</formula><formula xml:id="formula_204">H † d ( ΦC Ψ ) = 1 d q j=1</formula><p>Φc j ψj .</p><p>Finally, (A.7) can be readily obtained by noting that Φc j = m i=1 φi c ij .</p><p>7. We need to show that the operator L defined by (A.8) satisfies the left-inverse condition, i.e., we should show that L(H d|p (X)) = X for any X = [x 1 , . </p><formula xml:id="formula_205">)) = x 1 • • • x p = X,</formula><p>where the first equality uses the definition of H d|p (X), the second equality comes from the definition of L, and the last equality is from H † d being the generalized inverse of H d . 8. Since f ∈ R n and Ψ, Ψ ∈ R d×q , we have</p><formula xml:id="formula_206">H † d (H d (f )Ψ Ψ ) = 1 d q i=1 H d (f )ψ i = 1 d q i=1 (f ψ i ψi ) ∈ R n ,</formula><p>where the first equality comes from (A.6) and the last equality comes from (2). 9. Since Ξ = [ Ξ </p><formula xml:id="formula_207">H † d|p (H d|p ([f 1 , . . . , f p ])Ξ Ξ ) = H † d H d|p ([f 1 , . . . , f p ]) Ξ Ξ 1 • • • H † d H d|p ([f 1 , . . . , f p ]) Ξ Ξ p = 1 d q i=1 H d|p ([f 1 , . . . , f p ]) ξ i ξ1 i • • • H d|p ([f 1 , . . . , f p ]) ξ i ξp i = 1 d q i=1 p j=1 f j ξ j i ξ1 i , • • • f j ξ j i ξp i ,</formula><p>where the first equality comes from (A.8), the second equality is from (A.6), and the last equality is due to <ref type="bibr" target="#b5">(5)</ref>. Accordingly, using (A.8) and (A.6), we have</p><formula xml:id="formula_208">H † d|p 1 n a 1 • • • a p = H d (1 n a 1 ) • • • H d (1 n a p ) = 1 n a 1 • • • 1 n a p = 1 n 1 d a 1 • • • 1 d a p ,</formula><p>where the last equality comes from</p><formula xml:id="formula_209">1 n a i = H d (1 n )a i = 1 n (1 d a i ).</formula><p>From (B.1), we therefore have</p><formula xml:id="formula_210">Ẑ = Z + 1 n 1 d a 1 • • • 1 d a p + 1 n b dec .</formula><p>Thus, to satisfy PR, i.e., Ẑ = Z, the decoder bias should be</p><formula xml:id="formula_211">b dec = -1 d a 1 • • • 1 d a p = -1 d Ψ1 b enc • • • 1 d Ψp b enc .</formula><p>This concludes the proof.</p><p>Appendix C. Proof of Proposition 7. From ( <ref type="formula" target="#formula_84">53</ref>) and ( <ref type="formula" target="#formula_83">52</ref> where C n (Ψ (l-1) ) can be constructed using the definition in (E.1). Thus, we have rankH n|p (l) (C (l-1) ) ≤ min rankH n|p (l-1) (C (l-2) ), np (l) ≤ rankH n|p (l-1) (C (l-2) ), (E. where the last equality comes from <ref type="bibr" target="#b75">(75)</ref>. By applying this from l = L to 1 in (70), we can see that f = Q(f ; {Φ (j) , Φ(j) } L j=1 ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Construction of an extended Hankel matrix for 1-D multichannel input patches.</figDesc><graphic coords="6,163.94,522.08,255.36,132.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. 1-D convolutional operations and their Hankel matrix representations: (a) single-input singleoutput convolution y = f ψ, (b) SIMO convolution Y = f Ψ, (c) MIMO convolution Y = Z Ψ, and (d) MISO convolution y = Z Ψ.</figDesc><graphic coords="7,93.19,84.41,396.86,118.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. 2-D CNN convolutional operation. For the first-layer filter, the input and output channel numbers are p = 2 and q = 3, respectively, and the filter dimension is d1 = d2 = 2. Thus, the corresponding convolution operation can be represented by Vec(Y (j) ) = 2 i=1 H d 1 ,d 2 (X (i) )Vec(K</figDesc><graphic coords="9,178.23,96.36,226.77,189.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Spectral components of patches from (a) smooth background, (b) texture, and (c) edge.</figDesc><graphic coords="9,149.89,370.76,283.46,140.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (a) One-layer encoder-decoder, and (b) multilayer encoder-decoder architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>or equivalently, b dec [i] = -1 d Ψi b enc , i = 1, . . . , p. (61) Proof. See Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The exponential increase in the number of output channels. Note that the y-axis is in the log-scale.</figDesc><graphic coords="21,206.58,96.37,170.08,178.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Block diagram of (a) a residual block and (b) a skipped connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>where b enc ∈ R d and b dec ∈ R denote the encoder and decoder biases, respectively. Then, the main goal of the neural network training is to learn (Ψ, Ψ, b enc , b dec ) from the training data {(f (i) , f * (i) )} N i=1 , assuming that {f * (i) } are associated with rank-r Hankel matrices. More specifically, our regression problem under the low-rank Hankel matrix constraint in (85) for the training data can be equivalently represented by min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 7(b) is closely related to ResNet in Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Effective receptive field comparison. (a) Multiresolution network, and (b) CNN without pooling.</figDesc><graphic coords="30,194.22,96.36,198.43,251.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. (a) Proposed MRA of deep convolutional framelets. Here, corresponds to the convolution operation, and the red and blue blocks correspond to the encoder and decoder blocks, respectively. (b) An example of multiresolution deep convolutional framelet decomposition with length-2 local filters.</figDesc><graphic coords="32,123.36,175.96,340.16,111.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. DEEP CONVOLUTIONAL FRAMELETS FOR INVERSE PROBLEMS 1023 where p (2) = d (1) denotes the number of Hankel blocks in (10), d (2) is the second layer convolution filter length, and C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. (a) U-Net structure used in [34], which is used for comparative studies. Proposed multiresolution deep convolutional framelet structures for (b) our denoising and in-painting experiments, and (c) sparse-view CT reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Denoising results for U-Net, RED-Net [51], and the proposed deep convolutional framelets from Gaussian noise with σ = 30.</figDesc><graphic coords="38,72.00,93.37,439.38,311.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Comparison of CT reconstruction results from 60 views. The number in the top-right corner of each panel represents the NMSE values, and the red arrows refer to areas of noticeable difference. The yellow boxes denote the zoomed area. FBP reconstruction results for (a) full projection views and (b) 60 views. Reconstruction results from (c) CNN, (d) U-Net, and (e) the proposed multiresolution deep convolutional framelets.</figDesc><graphic coords="40,83.07,96.36,420.74,276.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Comparison of CT reconstruction results from 90 views. The number in the top-right corner of each panel represents the NMSE values, and the red arrows refer to areas of noticeable difference. The yellow boxes denote the zoomed area. FBP reconstruction results for (a) full projection view and (b) 90 views. Reconstruction results from (c) CNN, (d) U-Net, and (e) the proposed multiresolution deep convolutional framelets.</figDesc><graphic coords="41,83.07,96.36,420.73,277.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Proposed RNN architecture using deep convolutional framelets for training and inference steps.</figDesc><graphic coords="42,192.41,96.36,198.43,211.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Results from the proposed RNN according to number of iterations.</figDesc><graphic coords="43,220.75,229.80,141.73,115.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. PSNR versus number of iterations for the results in Figure 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Image inpainting results for input images missing 80% of pixels in random locations.</figDesc><graphic coords="44,72.00,96.36,439.37,337.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Single-resolution CNN architecture for image denoising.</figDesc><graphic coords="45,192.41,255.75,198.43,184.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Normalized singular value plot of the Hankel matrix constructed using the feature map from the first module to the fifth module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>H(n, d), k = 1, . . . , n, (A.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>2 . 3 . 6 . 7 . 9 . 1 •</head><label>236791</label><figDesc>For a given E k in (A.1), we have F := H d (f ) = n k=1 E k , F E k , where E k , H d (f ) = √ df [k]. (A.2) For any vectors u, v ∈ R n and any Hankel matrix F = H d (f ) ∈ H(n, d), we haveF, uv = u F v = u (f v) = f (u v) = f, u v , (A.3)where v denotes the flipped version of the vector v. 4. For a given E k in (A.1), u ∈ R n , and v ∈ R d , we haveE k , uv = 1 √ d (u v)[k]. (A.4) 5.A generalized inverse of the lifting to the Hankel structure in (A.2) is given by where B is any matrix in R n×d and the E k 's are defined in (A.1). Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php For a given C ∈ R m×q , where φj , ψj , and c j denote the jth column of Φ, Ψ, and C, respectively and c ij represents the (i, j)elements of C. For any Y = [Y 1 • • • Y p ] ∈ R n×dp with Y i ∈ R n×d , i = 1, . . . , p, suppose that an operator L satisfies L(Y ) = H † d (Y 1 ) • • • H † d (Y p ) ∈ R n×p . (A.8)Then, L is a generalized left-inverse of an extended Hankel operator H d|p , i.e., L = H † d|p . 8. We haveH † d (H d (f )Ψ Ψ ) = 1 d q i=1 (f ψ i ψi ) ∈ R n . (A.9) Let Ξ, Ξ ∈R pd×pq denote any matrix with block structure Ξ = Ξ • • Ξ p , where Ξ j ∈ R pq×d . Then, we have H † d|p (H d|p ([f 1 , . . . , f p ])Ξ Ξ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>E 1 ,E 2 ,</head><label>12</label><figDesc>where we useE k , H d (f ) = √ df [k]. 6. For Φ ∈ R n×m , C ∈ R m×q , and Ψ ∈ R d×q , H † d ( ΦC Ψ ) = H † d    Φ c 1 • • • c q Φc j ψ j Φc j ψ j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Appendix B. Proof of Theorem 6 .</head><label>6</label><figDesc>By converting<ref type="bibr" target="#b58">(58)</ref> and<ref type="bibr" target="#b59">(59)</ref> to Hankel operator forms, we haveẐ = H † d|p ( ΦC Ψ ) + 1 n b dec = H † d|p Φ(Φ H d|p (Z)Ψ + Φ 1 n b enc ) Ψ + 1 n b dec = H † d|p H d|p (Z) + H † d|p 1 n b enc Ψ + 1 n b dec = Z + H † d|p 1 n b enc Ψ + 1 n b dec , (B.1)where we use the frame condition<ref type="bibr" target="#b38">(38)</ref> for the third quality. Thus, to satisfy PR, i.e., Ẑ = Z, we have1 n b dec = -H † d|p 1 n b enc Ψ ,Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php which concludes the first part. Next, due to the block structure of Ψ in (48), we have b enc Ψ = a 1 • • • a p , where a i = Ψi b enc , i = 1, . . . , p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>1 )H</head><label>1</label><figDesc>d|p H d|p (Z)Ψ Ψ . Thus, (A.10) informs us thatZ := [z 1 , . . . , z p ] = H † d|p (H d|p ([z 1 , . . . , z p ])Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php because the Fourier transform of the flipped signal is equal to the complex conjugate of the original signal. Finally, (C.1) can be represented by a matrix representation form: n|p (l) (C (l-1) ) = H n|p (l-1) (C (l-2) )C n Ψ (l-1) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>3 )</head><label>3</label><figDesc>since np (l-1) ≤ np(l)  . By recursively applying the inequality (E.3) with (E.2), we haverankH d (l) |p (l) (C (l-1) ) ≤ min{H n (f ), d (l) p (l) }.This concludes the proof.Appendix F. Proof of Proposition 10. We will prove this proposition by construction. LetF (l) := H d (l) |p (l) (C (l) ). Because Φ (l) (F (l) (-Ψ (l) + )-1 n b (l) enc,+ ) = -Φ (l) (F (l) Ψ (l) + +1 n b (l) enc,+ ), the negative part can be retrieved from -ρ(Φ (l) (F (l) (-Ψ (l) + ) -1 n b (l) enc,+ )), while the positive part of Φ (l) (F (l) Ψ (l) + + 1 n b (l) enc,+ ) can be retained from ρ(Φ (l) (F (l) Ψ (l) + + 1 n b (l)enc,+ )). Furthermore, their nonzero parts do not overlap. Thus,Φ (l) F (l) Ψ (l) + + 1 n b (l) enc,+ = ρ Φ (l) F (l) Ψ choosing (73) and (75), we haveΦ(l) ρ Φ (l) F (l) Ψ (l) + 1 n b (l) enc,+ Ψ(l)= Φ(l) ρ Φ (l) F (l) Ψ l) ρ Φ (l) F (l) Ψ we use (F.1) for the third inequality and the last equality comes from<ref type="bibr" target="#b74">(74)</ref>. Finally, we haveĈ(l) = H † d (l) |p (l) F (l) + 1 n b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="35,95.01,220.62,396.85,166.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="35,95.01,404.47,396.85,166.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>)</head><label></label><figDesc>Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Notation and definitions used throughout the paper.</figDesc><table><row><cell cols="2">Notation Definition</cell></row><row><cell>Φ</cell><cell>nonlocal basis matrix at the encoder</cell></row><row><cell>Φ</cell><cell>nonlocal basis matrix at the decoder</cell></row><row><cell>Ψ</cell><cell>local basis matrix at the encoder</cell></row><row><cell>Ψ</cell><cell>local basis matrix at the decoder</cell></row><row><cell cols="2">benc, b dec encoder and decoder biases</cell></row><row><cell>φi</cell><cell>ith nonlocal basis or filter at the encoder</cell></row><row><cell>φi</cell><cell>ith nonlocal basis or filter at the decoder</cell></row><row><cell>ψi</cell><cell>ith local basis or filter at the encoder</cell></row><row><cell>ψi</cell><cell>ith local basis or filter at the decoder</cell></row><row><cell>C</cell><cell>convolutional framelet coefficients at the encoder</cell></row><row><cell>C</cell><cell>convolutional framelet coefficients at the decoder</cell></row><row><cell>n</cell><cell>input dimension</cell></row><row><cell>d</cell><cell>convolutional filter length</cell></row><row><cell>p</cell><cell>number of input channels</cell></row><row><cell>q</cell><cell>number of output channels</cell></row></table><note><p>f single channel input signal, i.e., f ∈ R n Z a p-channel input signal, i.e., Z ∈ R n×p H d (•) Hankel operator, i.e., H d : R n → Y ⊂ R n×d H d|p (•) extended Hankel operator, i.e., H d|p : R n×p → Y ⊂ R n×pd H † d (•) generalized inverse of Hankel operator, i.e., H † d : R n×d → R n H † d|p (•) generalized inverse of an extended Hankel operator, i.e., H † d|p : R n×pd → R n×p U left singular vector matrix of an (extended) Hankel matrix V right singular vector matrix of an (extended) Hankel matrix Σ singular value matrix of an (extended) Hankel matrix</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php whereas the encoder part of the framelet coefficient C (i) ∈ R n×q (i) is given by</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>(a)</cell></row><row><cell>(b)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Performance comparison in the PSNR/SSIM index for different datasets in the noise removal tasks from Gaussian noise with σ = 30.</figDesc><table><row><cell>Dataset (σ)</cell><cell>Input</cell><cell>RED-Net [51]</cell><cell>U-Net</cell><cell>Proposed</cell></row><row><cell>Set12 (30)</cell><cell>18.7805/0.2942</cell><cell>28.7188/0.8194</cell><cell cols="2">28.9395/0.8118 29.5126/0.8280</cell></row><row><cell>Set14 (30)</cell><cell cols="3">18.8264/0.3299 28.4994/0.7966 27.9911/0.7764</cell><cell>28.5978/0.7866</cell></row><row><cell cols="4">BSD68 (30) 18.8082/0.3267 27.8420/0.7787 27.7314/0.7749</cell><cell>27.8836/0.7761</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Average PSNR comparison reconstruction results from various projection views and algorithms. Here, CNN refers to the single-resolution network.</figDesc><table><row><cell>PSNR [dB]</cell><cell cols="6">60 views 90 views 120 views 180 views 240 views 360 views (x12) (x8) (x6) (x4) (x3) (x2)</cell></row><row><cell>FBP</cell><cell>22.2787</cell><cell>25.3070</cell><cell>27.4840</cell><cell>31.8291</cell><cell>35.0178</cell><cell>40.6892</cell></row><row><cell>CNN</cell><cell>36.7422</cell><cell>38.5736</cell><cell>40.8814</cell><cell>42.1607</cell><cell>43.7930</cell><cell>44.8450</cell></row><row><cell>U-Net</cell><cell>38.8122</cell><cell>40.4124</cell><cell>41.9699</cell><cell>43.0939</cell><cell>44.3413</cell><cell>45.2366</cell></row><row><cell>Proposed</cell><cell cols="2">38.9218 40.5091</cell><cell>42.0457</cell><cell>43.1800</cell><cell>44.3952</cell><cell>45.2552</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Performance comparison of the inpainting task in terms of the PSNR/SSIM index for various datasets for images missing 80% of pixels.</figDesc><table><row><cell>Dataset</cell><cell>Input</cell><cell>RED-Net</cell><cell>Proposed</cell></row><row><cell>Set5</cell><cell>7.3861/0.0992</cell><cell>28.0853/0.9329</cell><cell>28.4792/0.9368</cell></row><row><cell>Set14</cell><cell>6.7930/0.0701</cell><cell>24.4556/0.8545</cell><cell>25.7447/0.8649</cell></row><row><cell cols="3">BSD100 7.8462/0.0643 25.5847/0.8510</cell><cell>25.4588/0.8530</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Thus, E k , H d (f ) = √ df [k]. Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 3. The proof can be found in [73]. 4. Using (A.3) and E k = H d (e k )/ √ d, we have</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>. . , x p ] ∈ R n×p . Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php This can be shown because we have L(H d|p (X)) = L H d (x 1 ) • • • H d (x p ) = H † d (H(x 1 )) • • • H † d (H(x p</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>] ∈ R pq×pd with Ξ i ∈ R pq×d , we have H d|p ([f 1 , . . . , f p ]) Ξ Ξ = H d|p ([f 1 , . . . , f p ]) Ξ Ξ</figDesc><table /><note><p><p>1 ••• Ξ p 1 • • • H d|p ([f 1 , . . . , f p ]) Ξ Ξ p ,</p>where H d|p ([f 1 , . . . , f p ])Ξ Ξ i ∈ R n×d for i = 1, . . . , p. Thus, we have</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded 04/24/18 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors would like to thank Dr. Cynthia MaCollough, the Mayo Clinic, the American Association of Physicists in Medicine (AAPM), and grants EB01705 and EB01785 from the National Institute of Biomedical Imaging and Bioengineering for providing the Low-Dose CT Grand Challenge data set.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. http://www.siam.org/journals/siims/11-2/M114177.html</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work of the authors was supported by the National Research Foundation of Korea, grants NRF-2016R1A2B3008104, NRF-2015M3A9A7029734, and NRF-2017M3C7A1047904.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>. . . . . . . . .</p><p>which is equivalent to condition <ref type="bibr" target="#b64">(64)</ref>. For the proof of <ref type="bibr" target="#b65">(65)</ref>, note that the PR conditions <ref type="bibr" target="#b56">(56)</ref> and <ref type="bibr" target="#b55">(55)</ref> are for p = 1. Thus, <ref type="bibr" target="#b64">(64)</ref> is reduced to</p><p>which proves <ref type="bibr" target="#b65">(65)</ref>. Finally, for <ref type="bibr" target="#b66">(66)</ref>, note that ψi = ψ i for the orthonormal basis. Thus, ( <ref type="formula">65</ref>) is reduced to <ref type="bibr" target="#b66">(66)</ref>. This concludes the proof.</p><p>Appendix D. Proof of Proposition 8. We prove this proposition by mathematical induction. At l = 1, the input signal is f ∈ R n , so we need Φ (1) H d (1) (f )Ψ (1) to obtain the filtered signal C (1) . Since H d (1) (f ) ∈ R n×d (1) , the dimension of the local basis matrix should be Ψ (1) ∈ R d (1) ×q (1) with q (1) ≥ d <ref type="bibr" target="#b0">(1)</ref> to satisfy the frame condition <ref type="bibr" target="#b39">(39)</ref>. Next, we assume that ( <ref type="formula">67</ref>) is true at the (l -1)th layer. Then, the number of input channels at the l-layer is p (l) = q (l-1) and the filtering operation can be represented by <ref type="bibr" target="#b9">(9)</ref> or l) . Thus, to guarantee PR, the number of columns for the local basis Ψ (l) should be at least p (l) d (l) to satisfy the frame condition <ref type="bibr" target="#b39">(39)</ref>. This implies that the output channel number at the lth layer should be q (l) ≥ p (l) d (l) = q (l-1) d (l) . This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Proof of Proposition 9.</head><p>Note that the extended Hankel matrix H d (l) |p (l) (C (l-1) ) in <ref type="bibr" target="#b63">(63)</ref> has the following decomposition:</p><p>Here,</p><p>where C d (h) ∈ R n×d is defined in <ref type="bibr" target="#b30">(30)</ref>. Due to the rank inequality rank(AB) ≤ min{rank(A), rank(B)}, we have Appendix G. Proof of Proposition 11. We will prove this proposition by construction. From the proof of Proposition 10, we know that</p><p>where the last equality comes from the orthonormality of Φ. Since m ≥ r, there always exist Ψ + , Ψ+ ∈ R pd×m such that Ψ + Ψ+ = P R(V ) , where V ∈ R pd×r denotes the right singular vectors of H d|p (X). Thus,</p><p>By applying H † d|p to both sides, we conclude the proof. Appendix H. Proof of Proposition 12. Again the proof can be done by construction. Specifically, from the proof of Proposition 10, we know that</p><p>Thus,</p><p>Thus, if we choose Ψ + such that V Ψ + = 0, we have</p><p>where the last equality comes from the nonnegativity of X. Thus, we can guarantee (78). Now, the remaining issue to prove is the existence of Ψ + such that V Ψ + = 0. This is always possible if r &lt; pd. This concludes the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NTIRE 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<title level="m">Neural Network Learning: Theoretical Foundations</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond deep residual learning for image restoration: Persistent homology-guided manifold simplification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 04/24/18 to 131.172.36.29</idno>
		<ptr target="http://www.siam.org/journals/ojsa.phpCopyright©bySIAM" />
		<imprint/>
	</monogr>
	<note>Unauthorized reproduction of this article is prohibited</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bauschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<title level="m">Convex Analysis and Monotone Operator Theory in Hilbert Spaces</title>
		<imprint>
			<publisher>Chan</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D?</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convergence analysis of tight framelet approach for missing data recovery</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="87" to="113" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A framelet-based image inpainting algorithm</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="131" to="149" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image restoration: Total variation, wavelet frames, and beyond</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1033" to="1089" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Split Bregman methods and frame based image restoration</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="337" to="369" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5261" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transformdomain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ten Lectures on Wavelets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>SIAM, Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MRA-based constructions of wavelet frames</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Framelets</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hankel matrix rank minimization with applications to system identification and realization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="946" to="977" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1 Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Highway and residual networks learn unrolled iterative estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07771</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a variational model for compressed sensing MRI reconstruction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hammernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sodickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Society of Magnetic Resonance in Medicine (ISMRM) Annual Meeting &amp; Exhibition</title>
		<meeting>the 24th International Society of Magnetic Resonance in Medicine (ISMRM) Annual Meeting &amp; Exhibition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6307" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.10248</idno>
		<title level="m">Deep Learning Interior Tomography for Region-of-Interest Reconstruction</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08333</idno>
		<title level="m">Framing U-Net via Deep Convolutional Framelets: Application to Sparse-view CT</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Compressed Sensing CT Reconstruction via Persistent Homology Analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06391</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning with domain adaptation for accelerated projectionreconstruction MR</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1002/mrm.27106</idno>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matrix pencil method for estimating parameters of exponentially damped/ undamped sinusoids in noise</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="814" to="824" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A general framework for compressed sensing and parallel MRI using annihilating filter based low-rank Hankel matrix</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="480" to="495" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MRI artifact correction using sparse + low-rank decomposition of annihilating filter-based Hankel matrix</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Um</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Annihilating filter-based low-rank Hankel matrix approach for image inpainting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3498" to="3511" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sparse and low-rank decomposition of a Hankel Structured matrix for impulse noise removal</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1448" to="1461" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network using directional wavelets for low-dose X-ray CT reconstruction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="360" to="e375" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01383</idno>
		<title level="m">Wavelet Domain Residual Network (WavResNet) for Low-Dose X-ray CT Reconstruction</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Wavelet Residual Network for Low-Dose CT via Deep Convolutional Framelets</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09938</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A parallel MR imaging method using multilayer perceptron</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="6209" to="6224" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning-based reconstruction using artificial neural network for higher acceleration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Society of Magnetic Resonance in Medicine (ISMRM) Annual Meeting &amp; Exhibition</title>
		<meeting>the 24th International Society of Magnetic Resonance in Medicine (ISMRM) Annual Meeting &amp; Exhibition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Acceleration of MR parameter mapping using annihilating filter-based low rank Hankel matrix (ALOHA)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="1848" to="1868" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reference-free single-pass EPI Nyquist ghost correction using annihilating filter-based low rank Hankel matrix (ALOHA)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<title level="m">A Wavelet Tour of Signal Processing: The Sparse Way</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Group invariant scattering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1331" to="1398" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast live cell imaging at nanometer scale using annihilating filter-based low-rank Hankel matrix approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9597</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Off-the-grid recovery of piecewise constant images from few Fourier samples</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1004" to="1041" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Convolutional neural networks analyzed via convolutional sparse coding</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="52" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Exponential expressivity in deep neural networks through transient chaos</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3360" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the 18th International Conference on Medical Image Computing and Computer-Assisted Intervention<address><addrLine>Munich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2217" to="2225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Sidky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="4777" to="4807" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Benefits of depth in neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Conference on Learning Theory</title>
		<meeting>the 29th Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1517" to="1539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Blind identification and equalization based on second-order statistics: A time domain approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="340" to="349" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">MatConvNet: Convolutional neural networks for MATLAB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
		<meeting>the 23rd ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sampling signals with finite rate of innovation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1417" to="1428" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Accelerating magnetic resonance imaging via deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th IEEE International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the 13th IEEE International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="514" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A mathematical theory of deep convolutional neural networks for feature extraction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wiatowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bölcskei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1845" to="1866" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Maximal sparsity with deep networks?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4340" to="4348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Compressive sampling using annihilating filter-based low-rank interpolation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="777" to="801" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07539</idno>
		<title level="m">Semantic Image Inpainting with Perceptual and Contextual Losses</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A tale of two bases: Local-nonlocal regularization on image patches with convolution framelets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="711" to="750" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Understanding Deep Learning Requires Rethinking Generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Image Reconstruction by Domain Transform Manifold Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rosen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08841</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
