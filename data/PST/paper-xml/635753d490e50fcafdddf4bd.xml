<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High Fidelity Neural Audio Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-24">24 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
							<email>defossez@meta.com</email>
						</author>
						<author>
							<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
							<email>jadecopet@meta.com</email>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
							<email>adiyoss@meta.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Meta AI</orgName>
								<orgName type="institution" key="instit2">FAIR Team</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Meta AI</orgName>
								<orgName type="institution" key="instit2">FAIR Team</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Meta AI</orgName>
								<orgName type="institution" key="instit2">FAIR Team</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Meta AI</orgName>
								<orgName type="institution" key="instit2">FAIR Team</orgName>
								<address>
									<settlement>Tel-Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High Fidelity Neural Audio Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-24">24 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.13438v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec. * , ? Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent studies suggest that streaming audio and video have accounted for the majority of the internet traffic in 2021 (82% according to <ref type="bibr" target="#b11">(Cisco, 2021)</ref>). With the internet traffic expected to grow, audio compression is an increasingly important problem. In lossy signal compression we aim at minimizing the bitrate of a sample while also minimizing the amount of distortion according to a given metric, ideally correlated with human perception. Audio codecs typically employ a carefully engineered pipeline combining an encoder and a decoder to remove redundancies in the audio content and yield a compact bitstream. Traditionally, this is achieved by decomposing the input with a signal processing transform and trading off the quality of the components that are less likely to influence perception. Leveraging neural networks as trained transforms via an encoder-decoder mechanism has been explored by <ref type="bibr" target="#b49">Morishima et al. (1990)</ref>; <ref type="bibr" target="#b58">Rippel et al. (2019)</ref>; <ref type="bibr" target="#b74">Zeghidour et al. (2021)</ref>. Our research work is in the continuity of this line of work, with a focus on audio signals.</p><p>The problems arising in lossy neural compression models are twofold: first, the model has to represent a wide range of signals, such as not to overfit the training set or produce artifact laden audio outside its comfort zone. We solve this by having a large and diverse training set (described in Section 4.1), as well as discriminator networks (see Section 3.4) that serve as perceptual losses, which we study extensively in Section 4.5.1, Table <ref type="table" target="#tab_1">2</ref>. The other problem is that of compressing efficiently, both in compute time and in size.</p><p>Figure <ref type="figure">1</ref>: EnCodec : an encoder decoder codec architecture which is trained with reconstruction ( f and t ) as well as adversarial losses ( g for the generator and d for the discriminator). The residual vector quantization commitment loss ( w ) applies only to the encoder. Optionally, we train a small Transformer language model for entropy coding over the quantized units with l , which reduces bandwidth even further.</p><p>For the former, we limit ourselves to models that run in real-time on a single CPU core. For the latter, we use residual vector quantization of the neural encoder floating-point output, for which various approaches have been proposed <ref type="bibr" target="#b67">(Van Den Oord et al., 2017;</ref><ref type="bibr" target="#b74">Zeghidour et al., 2021)</ref>.</p><p>Accompanying those technical contributions, we posit that designing end-to-end neural compression models is a set of intertwined choices, among which at least the encoder-decoder architecture, the quantization method, and the perceptual loss play key parts. Objective evaluations exist and we report scores on them in our ablations (Section 4.5.1). But the evaluation of lossy audio codecs necessarily relies on human perception, so we ran extensive human evaluation for multiple points in this design space, both for speech and music. Those evaluations (MUSHRA) consist in having humans listen to, compare, and rate excerpts of speech or music compressed with competitive codecs and variants of our method, and the uncompressed ground truth. This allows to compare variants of the whole pipeline in isolation, as well as their combined effect, in Section 4.5.1 (Figure <ref type="figure" target="#fig_1">3</ref> and Table <ref type="table" target="#tab_0">1</ref>). Finally, our best model, EnCodec , reaches state-of-the-art scores for speech and for music at 1.5, 3, 6, 12 kbps at 24 kHz, and at 6, 12, and 24 kbps for 48 kHz with stereo channels.</p><p>Neural based audio codecs have been recently proposed and demonstrated promising results <ref type="bibr" target="#b35">(Kleijn et al., 2018;</ref><ref type="bibr">Valin &amp; Skoglund, 2019b;</ref><ref type="bibr" target="#b45">Lim et al., 2020;</ref><ref type="bibr" target="#b36">Kleijn et al., 2021;</ref><ref type="bibr" target="#b74">Zeghidour et al., 2021;</ref><ref type="bibr" target="#b52">Omran et al., 2022;</ref><ref type="bibr" target="#b46">Lin et al., 2022;</ref><ref type="bibr" target="#b31">Jayashankar et al., 2022;</ref><ref type="bibr">Li et al.;</ref><ref type="bibr" target="#b32">Jiang et al., 2022)</ref>, where most methods are based on quantizing the latent space before feeding it to the decoder. In <ref type="bibr">Valin &amp; Skoglund (2019b)</ref>, an LPCNet <ref type="bibr">(Valin &amp; Skoglund, 2019a)</ref> vocoder was conditioned on hand-crafted features and a uniform quantizer. <ref type="bibr" target="#b21">G?rbacea et al. (2019)</ref> conditioned a WaveNet based model on discrete units obtained from a VQ-VAE <ref type="bibr" target="#b67">(Van Den Oord et al., 2017;</ref><ref type="bibr" target="#b57">Razavi et al., 2019)</ref> model, while <ref type="bibr">Skoglund &amp; Valin (2019)</ref> tried feeding the Opus codec <ref type="bibr" target="#b66">(Valin et al., 2012)</ref> to a WaveNet to further improve its perceptual quality. <ref type="bibr" target="#b31">Jayashankar et al. (2022)</ref>; <ref type="bibr" target="#b32">Jiang et al. (2022)</ref> propose an auto-encoder with a vector quantization layer applied over the latent representation and minimizing the reconstruction loss, while Li et al. suggested using Gumbel-Softmax (GS) <ref type="bibr" target="#b30">(Jang et al., 2017)</ref> for representation quantization. The most relevant related work to ours is the SoundStream model <ref type="bibr" target="#b74">(Zeghidour et al., 2021)</ref>, in which the authors propose a fully convolutional encoder decoder architecture with a Residual Vector Quantization (RVQ) <ref type="bibr" target="#b24">(Gray, 1984;</ref><ref type="bibr" target="#b68">Vasuki &amp; Vanathi, 2006</ref>) layers. The model was optimized using both reconstruction loss and adversarial perceptual losses.</p><p>Audio Discretization. Representing audio and speech using discrete values was proposed to various tasks recently. <ref type="bibr" target="#b17">Dieleman et al. (2018)</ref>; <ref type="bibr" target="#b16">Dhariwal et al. (2020)</ref> proposed a hierarchical VQ-VAE based model for learning discrete representation of raw audio, next combined with an auto-regressive model, demonstrating the ability to generate high quality music. Similarly, <ref type="bibr" target="#b40">Lakhotia et al. (2021)</ref>; <ref type="bibr" target="#b34">Kharitonov et al. (2021)</ref> demonstrated that self-supervised learning methods for speech (e.g., HuBERT <ref type="bibr" target="#b28">(Hsu et al., 2021)</ref>), can be quantized and used for conditional and unconditional speech generation. Similar methods were applied to speech resynthesis <ref type="bibr" target="#b55">(Polyak et al., 2021)</ref>, speech emotion conversion <ref type="bibr" target="#b38">(Kreuk et al., 2021)</ref>, spoken dialog system <ref type="bibr" target="#b51">(Nguyen et al., 2022)</ref>, and speech-to-speech translation <ref type="bibr">(Lee et al., 2021a;</ref><ref type="bibr" target="#b61">b;</ref><ref type="bibr" target="#b56">Popuri et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>An audio signal of duration d can be represented by a sequence x ? [-1, 1] Ca?T with C a the number of audio channels, T = d ? f sr the number of audio samples at a given sample rate f sr . The EnCodec model is composed of three main components: (i) First, an encoder network E is input an audio extract and outputs a latent representation z; (ii) Next, a quantization layer Q produces a compressed representation z q , using vector quantization; (iii) Lastly, a decoder network G reconstructs the time-domain signal, x, from the compressed latent representation z q . The whole system is trained end-to-end to minimize a reconstruction loss applied over both time and frequency domain, together with a perceptual loss in the form of discriminators operating at different resolutions. A visual description of the proposed method can be seen in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder &amp; Decoder Architecture</head><p>The EnCodec model is a simple streaming, convolutional-based encoder-decoder architecture with sequential modeling component applied over the latent representation, both on the encoder and on the decoder side. Such modeling framework was shown to provide great results in various audio-related tasks, e.g., source separation and enhancement <ref type="bibr" target="#b13">(D?fossez et al., 2019;</ref><ref type="bibr" target="#b14">Defossez et al., 2020)</ref>, neural vocoders <ref type="bibr" target="#b39">(Kumar et al., 2019;</ref><ref type="bibr" target="#b37">Kong et al., 2020)</ref>, audio codec <ref type="bibr" target="#b74">(Zeghidour et al., 2021)</ref>, and artificial bandwidth extension <ref type="bibr" target="#b63">(Tagliasacchi et al., 2020;</ref><ref type="bibr">Li et al., 2021)</ref>. We use the same architecture for 24 kHz and 48 kHz audio.</p><p>Encoder-Decoder. The encoder model E consists in a 1D convolution with C channels and a kernel size of 7 followed by B convolution blocks. Each convolution block is composed of a single residual unit followed by a down-sampling layer consisting in a strided convolution, with a kernel size K of twice the stride S. The residual unit contains two convolutions with kernel size 3 and a skip-connection. The number of channels is doubled whenever down-sampling occurred. The convolution blocks are followed by a two-layer LSTM for sequence modeling and a final 1D convolution layer with a kernel size of 7 and D output channels. Following <ref type="bibr" target="#b74">Zeghidour et al. (2021)</ref>; <ref type="bibr">Li et al. (2021)</ref>, we use C = 32, B = 4 and (2, 4, 5, 8) as strides. We use ELU as a non-linear activation function <ref type="bibr" target="#b12">(Clevert et al., 2015)</ref> either layer normalization <ref type="bibr" target="#b3">(Ba et al., 2016)</ref> or weight normalization <ref type="bibr" target="#b60">(Salimans &amp; Kingma, 2016)</ref>. We use two variants of the model, depending on whether we target the low-latency streamable setup, or a high fidelity non-streamable usage. With this setup, the encoder outputs 75 latent steps per second of audio at 24 kHz, and 150 at 48 kHz. The decoder mirrors the encoder, using transposed convolutions instead of strided convolutions, and with the strides in reverse order as in the encoder, outputting the final mono or stereo audio.</p><p>Non-streamable. In the non-streamable setup, we use for each convolution a total padding of K -S, split equally before the first time step and after the last one (with one more before if K -S is odd). We further split the input into chunks of 1 seconds, with an overlap of 10 ms to avoid clicks, and normalize each chunk before feeding it to the model, applying the inverse operation on the output of the decoder, adding a negligible bandwidth overhead to transmit the scale. We use layer normalization <ref type="bibr" target="#b3">(Ba et al., 2016)</ref>, computing the statistics including also the time dimension in order to keep the relative scale information.</p><p>Streamable. For the streamable setup, all padding is put before the first time step. For a transposed convolution with stride s, we output the s first time steps, and keep the remaining s steps in memory for completion when the next frame is available, or discarding it at the end of a stream. Thanks to this padding scheme, the model can output 320 samples (13 ms) as soon as the first 320 samples (13 ms) are received. We replace the layer normalization with statistics computed over the time dimension with weight normalization <ref type="bibr" target="#b60">(Salimans &amp; Kingma, 2016)</ref>, as the former is ill-suited for a streaming setup. We notice a small gain over the objective metrics by keeping a form of normalization, as demonstrated in Table A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Residual Vector Quantization</head><p>We use Residual Vector Quantization (RVQ) to quantize the output of the encoder as introduced by <ref type="bibr" target="#b74">Zeghidour et al. (2021)</ref>. Vector quantization consists in projecting an input vector onto the closest entry in a codebook of a given size. RVQ refines this process by computing the residual after quantization, and further quantizing it using a second codebook, and so forth.</p><p>We follow the same training procedure as described by <ref type="bibr" target="#b16">Dhariwal et al. (2020)</ref> and <ref type="bibr" target="#b74">Zeghidour et al. (2021)</ref>. The codebook entry selected for each input is updated using an exponential moving average with a decay of 0.99, and entries that are not used are replaced with a candidate sampled from the current batch. We use a straight-through-estimator <ref type="bibr" target="#b6">(Bengio et al., 2013)</ref> to compute the gradient of the encoder, e.g. as if the quantization step was the identity function during the backward phase. Finally, a commitment loss, consisting of the MSE between the input of the quantizer and its output, with gradient only computed with respect to its input, is added to the overall training loss.</p><p>By selecting a variable number of residual steps at train time, a single model can be used to support multiple bandwidth target <ref type="bibr" target="#b74">(Zeghidour et al., 2021)</ref>. For all of our models, we use at most 32 codebooks (16 for the 48 kHz models) with 1024 entries each, e.g. 10 bits per codebook. When doing variable bandwidth training, we select randomly a number of codebooks as a multiple of 4, i.e. corresponding to a bandwidth 1.5, 3, 6, 12 or 24 kbps at 24 kHz. Given a continuous latent represention with shape [B, D, T ] that comes out of the encoder, this procedure turns it into a discrete set of indexes [B, N q , T ] with N q the number of codebooks selected. This discrete representation can changed again to a vector by summing the corresponding codebook entries, which is done just before going into the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Language Modeling and Entropy Coding</head><p>We additionally train a small Transformer based language model <ref type="bibr" target="#b69">(Vaswani et al., 2017)</ref> with the objective of keeping faster than real time end-to-end compression/decompression on a single CPU core. The model consists of 5 layers, 8 heads, 200 channels, a dimension of 800 for the feed-forward blocks, and no dropout. At train time, we select a bandwidth and the corresponding number of codebooks N q . For a time step t, the discrete representation obtained at time t -1 is transformed into a continuous representation using learnt embedding tables, one for each codebook, and which are summed. For t = 0, a special token is used instead. The output of the Transformer is fed into N q linear layers with as many output channels as the cardinality of each codebook (e.g. 1024), giving us the logits of the estimated distribution over each codebook for time t. We thus neglect potential mutual information between the codebooks at a single time step. This allows to speedup inference (as opposed to having one time step per codebook, or a multi-stage prediction) with a limited impact over the final cross entropy. Each attention layer has a causal receptive field of 3.5 seconds, <ref type="table">,</ref><ref type="table" target="#tab_2">h)   Conv2D (C=32,</ref><ref type="table" target="#tab_2">k=3x9,</ref><ref type="table" target="#tab_0">s=(1,</ref><ref type="table" target="#tab_1">2),</ref><ref type="table" target="#tab_0">d=(1,</ref><ref type="table" target="#tab_2">1))   Conv2D (C=32,</ref><ref type="table" target="#tab_2">k=3x9)  Conv2D (C=32,</ref><ref type="table" target="#tab_2">k=3x9,</ref><ref type="table" target="#tab_0">s=(1,</ref><ref type="table" target="#tab_1">2),</ref><ref type="table" target="#tab_1">d=(2,</ref><ref type="table" target="#tab_2">1))  Conv2D (C=32,</ref><ref type="table" target="#tab_2">k=3x9,</ref><ref type="table" target="#tab_0">s=(1,</ref><ref type="table" target="#tab_1">2),</ref><ref type="table" target="#tab_4">d=(4,</ref><ref type="table" target="#tab_0">1</ref> The input to the network is a complex-valued STFT with the real and imaginary parts concatenated. Each discriminator is composed of a 2D convolutional layer, followed by 2D convolutions with increasing dilation rates. Then a final 2D convolution is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STFT(w</head><p>and we offset by a random amount the initial position of the sinusoidal position embedding to emulate being in a longer sequence. We train the model on sequences of 5 seconds.</p><p>Entropy Encoding. We use a range based arithmetic coder <ref type="bibr" target="#b54">(Pasco, 1976;</ref><ref type="bibr" target="#b59">Rissanen &amp; Langdon, 1981)</ref> in order to leverage the estimated probabilities given by the language model. As noted by <ref type="bibr" target="#b5">Ball? et al. (2018)</ref>, evaluation of the same model might lead to different results on different architectures, or with different evaluation procedures due to floating point approximations. This can lead to decoding errors as the encoder and decoder will not use the exact same code. We observe in particular that the difference between batch evaluation (e.g. all time steps at once), and the real-life streaming evaluation that occurs in the decoder can lead to difference larger than 10 -8 . We first round the estimated probabilities with a precision of 10 -6 , although evaluations in more contexts would be needed for practical deployment. We use a total range width of 2 24 , and assign a minimum range width of 2. We discuss the impact on the processing time in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training objective</head><p>We detail the training objective that combines a reconstruction loss term, a perceptual loss term (via discriminators), and the RVQ commitment loss.</p><p>Reconstruction Loss. The reconstruction loss term is comprised of a time and a frequency domain loss term. We minimize the L1 distance between the target and compressed audio over the time domain, i.e.</p><p>t (x, x) = xx 1 . For the frequency domain, we use a linear combination between the L1 and L2 losses over the mel-spectrogram using several time scales <ref type="bibr">(Yamamoto et al., 2020b;</ref><ref type="bibr" target="#b26">Gritsenko et al., 2020)</ref>. Formally,</p><formula xml:id="formula_0">f (x, x) = 1 |?| ? |s| ?i?? i?e S i (x) -S i ( x) 1 + ? i S i (x) -S i ( x) 2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where S i is a 64-bins mel-spectrogram using a normalized STFT with window size of 2 i and hop length of 2 i /4, e = 5, . . . , 11 is the set of scales, and ? represents the set of scalar coefficients balancing between the L1 and L2 terms. Unlike <ref type="bibr" target="#b26">Gritsenko et al. (2020)</ref>, we take ? i = 1.</p><p>Discriminative Loss. To further improve the quality of the generated samples, we introduce a perceptual loss term based on a multi-scale STFT-based (MS-STFT) discriminator, illustrated in Figure <ref type="figure" target="#fig_0">2</ref>. Multi scale discriminators are popular for capturing different structures in audio signals <ref type="bibr" target="#b39">(Kumar et al., 2019;</ref><ref type="bibr" target="#b37">Kong et al., 2020;</ref><ref type="bibr" target="#b73">You et al., 2021)</ref>. The MS-STFT discriminator consists in identically structured networks operating on multi-scaled complex-valued STFT with the real and imaginary parts concatenated. Each sub-network is composed of a 2D convolutional layer (using kernel size 3 x 8 with 32 channels), followed by 2D convolutions with increasing dilation rates in the time dimension of 1, 2 and 4, and a stride of 2 over the frequency axis. A final 2D convolution with kernel size 3 x 3 and stride (1, 1) provide the final prediction. We use 5 different scales with STFT window lengths of <ref type="bibr">[2048,</ref><ref type="bibr">1024,</ref><ref type="bibr">512,</ref><ref type="bibr">256,</ref><ref type="bibr">128]</ref>. For 48 kHz audio, we double the size of each STFT window and train the discriminator every two batches, and for stereophonic audio, we process separately the left and right channels. We use LeakyReLU as a non-linear activation function and apply weight normalization <ref type="bibr" target="#b60">(Salimans &amp; Kingma, 2016)</ref> to our discriminator network. The MS-STFT discriminator model architecture is visually depicted in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>The adversarial loss for the generator is constructed as follows,</p><formula xml:id="formula_2">g ( x) = 1 K k max(0, 1 -D k ( x)))</formula><p>, where K is the number of discriminators. Similarly to previous work on neural vocoders <ref type="bibr" target="#b39">(Kumar et al., 2019;</ref><ref type="bibr" target="#b37">Kong et al., 2020;</ref><ref type="bibr" target="#b73">You et al., 2021)</ref>, we additionally include a relative feature matching loss for the generator. Formally,</p><formula xml:id="formula_3">f eat (x, x) = 1 KL K k=1 L l=1 D l k (x) -D l k ( x) 1 mean D l k (x) 1 , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where the mean is computed over all dimensions, (D k ) are the discriminators, and L is the number of layers in discriminators. The discriminators are trained to minimize the following hinge-loss adversarial loss function:</p><formula xml:id="formula_5">L d (x, x) = 1 K K k=1 max(0, 1 -D k (x)) + max(0, 1 + D k ( x))</formula><p>, where K is the number of discriminators. Given that the discriminator tend to overpower easily the decoder, we update its weight with a probability of 2/3 at 24 kHz, and 0.5 at 48 kHz.</p><p>Multi-bandwidth training. At 24 kHz, we train the model to support the bandwidths 1.5, 3, 6, 12, and 24 kbps by selecting the appropriate number of codebooks to keep in the RVQ step, as explained in Section 3.2. At 48 kHz, we train to support 3, 6, 12 and 24 kbps. We also noticed that using a dedicated discriminator per-bandwidth is beneficial to the audio quality. Thus, we select a given bandwidth for the entire batch, and evaluate and update only the corresponding discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQ commitment loss.</head><p>As mentioned in Section 3.2, we add a commitment loss l w between the output of the encoder, and its quantized value, with no gradient being computed for the quantized value. For each residual step c ? {1, . . . C} (with C depeding on the bandwidth target for the current batch), noting z c the current residual and q c (z c ) the nearest entry in the corresponding codebook, we define l w as</p><formula xml:id="formula_6">l w = C c=1 z c -q c (z c ) 2 2 . (3)</formula><p>Overall, the generator is trained to optimize the following loss, summed over the batch,</p><formula xml:id="formula_7">L G = ? t ? t (x, x) + ? f ? f (x, x) + ? g ? g ( x) + ? f eat ? f eat (x, x) + ? w ? w (w),<label>(4)</label></formula><p>where ? t , ? f , ? g , ? f eat , and ? w the scalar coefficients to balance between the terms.</p><p>Balancer. We introduce a loss balancer in order to stabilize training, in particular the varying scale of the gradients coming from the discriminators. We also find that the balancer makes it easier to reason about the different loss weights, independently of their scale. Let us take a number of losses ( i ) i that depends only on the output of the model x. We define g i = ? i ? x , and g i 2 ? the exponential moving average of g i over the last training batches. Given a set of weights (? i ) and a reference norm R, we define</p><formula xml:id="formula_8">gi = R ? i j ? j ? g i g i 2 ? . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>We then backpropagate into the network i gi , instead of the original i ? i g i . This changes the optimization problem but allows to make the ? i interpretable irrespectively of the natural scale of each loss. If i ? i = 1, then each weight can be interpreted as the fraction of the model gradient that come from the corresponding loss. We take R = 1 and ? = 0.999. All the generator losses from Eq. ( <ref type="formula" target="#formula_7">4</ref>) fit into the balancer, except for the commitment loss, as it is not defined with respect to the output of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We train EnCodec on 24 kHz monophonic across diverse domains, namely: speech, noisy speech, music and general audio while we train the fullband stereo EnCodec on only 48 kHz music. For speech, we use the clean and with entropy coding (hollow). Lyra-v2 is a neural audio codec, while EVS and Opus are competitive standard codecs. The audio samples are from speech and music. The ground truth is 16bits 24kHz wave.</p><p>speech segments from DNS Challenge 4 <ref type="bibr" target="#b19">(Dubey et al., 2022)</ref> and the Common Voice dataset <ref type="bibr" target="#b1">(Ardila et al., 2019)</ref>.</p><p>For general audio, we use on AudioSet (Gemmeke et al., 2017) together with FSD50K <ref type="bibr" target="#b20">(Fonseca et al., 2021)</ref>.</p><p>For music, we rely on the Jamendo dataset <ref type="bibr" target="#b8">(Bogdanov et al., 2019)</ref> for training and evaluation and we further evaluate our models on music using a proprietary music dataset. Data splits are detailed in Appendix A.1.</p><p>For training and validation, we define a mixing strategy which consists in either sampling a single source from a dataset or performing on the fly mixing of two or three sources. Specifically, we have four strategies: (s1) we sample a single source from Jamendo with probability 0.32; (s2) we sample a single source from the other datasets with the same probability; (s3) we mix two sources from all datasets with a probability of 0.24;</p><p>(s4) we mix three sources from all datasets except music with a probability of 0.12.</p><p>The audio is normalized by file and we apply a random gain between -10 and 6 dB. We reject any sample that has been clipped. Finally we add reverberation using room impulse responses provided by the DNS challenge with probability 0.2, and RT60 in the range [0.3, 1.3] except for the single-source music samples.</p><p>For testing, we use four categories: clean speech from DNS alone, clean speech mixed with FSDK50K sample, Jamendo sample alone, proprietary music sample alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Opus <ref type="bibr" target="#b66">(Valin et al., 2012</ref>) is a versatile speech and audio codec standardized by the IETF in 2012. It scales from 6 kbps narrowband monophonic audio to 510 kbps fullband stereophonic audio. EVS <ref type="bibr" target="#b18">(Dietz et al., 2015)</ref> is a codec standardized in 2014 by 3GPP and developed for Voice over LTE (VoLTE). It supports a range of bitrates from 5.9 kbps to 128 kbps, and audio bandwidths from 4 kHz to 20 kHz. It is the successor of AMR-WB <ref type="bibr" target="#b7">(Bhagat et al., 2012)</ref>.We use both codecs to serve as traditional digital signal processing baselines. We also utilize MP3 compression at 64 kbps as an additional baseline for the stereophonic signal compression case. MP3 uses lossy data compression by approximating the accuracy of certain components of sound that are considered to be beyond hearing capabilities of most humans. Finally, we compare EnCodec to the SoundStream model from the official implementation available in Lyra 2<ref type="foot" target="#foot_0">1</ref> at 3.2 kbps and 6 kbps on audio upsampled to 32 kHz. We also reproduced a version of SoundStream <ref type="bibr" target="#b74">(Zeghidour et al., 2021)</ref> with minor improvements. Namely, we use the relative feature loss introduce in Section 3.4, and layer normalization (applied separately for each time step) in the discriminators, except for the first and last layer, which improved the audio quality during our preliminary studies. Results a reported in Table <ref type="table" target="#tab_6">A</ref>.2 in the Appendix A.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Methods</head><p>We consider both subjective and objective evaluation metrics. For the subjective tests we follow the MUSHRA protocol <ref type="bibr" target="#b61">(Series, 2014)</ref>, using both a hidden reference and a low anchor. Annotators were recruited using a crowd-sourcing platform, in which they were asked to rate the perceptual quality of the provided samples in a range between 1 to 100. We randomly select 50 samples of 5 seconds from each category of the the test set and force at least 10 annotations per samples. To filter noisy annotations and outliers we remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time. For objective metrics, we use ViSQOL <ref type="bibr" target="#b27">(Hines et al., 2012;</ref><ref type="bibr">Chinen et al., 2020)</ref> <ref type="foot" target="#foot_1">2</ref> , together with the Scale-Invariant Signal-to-Noise Ration (SI-SNR) <ref type="bibr" target="#b47">(Luo &amp; Mesgarani, 2019;</ref><ref type="bibr" target="#b50">Nachmani et al., 2020;</ref><ref type="bibr" target="#b9">Chazan et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>We train all models for 300 epochs, with one epoch being 2,000 updates with the Adam optimizer with a batch size of 64 examples of 1 second each, a learning rate of 3 ? 10 -4 , ? 1 = 0.5, and ? 2 = 0.9. All the models are traind using 8 A100 GPUs. We use the balancer introduced in Section 3.4 with weights ? t = 0.1, ? f = 1, ? g = 3, ? feat = 3 for the 24 kHz models. For the 48 kHz model, we use instead ? g = 4, ? feat = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>We start with the results for EnCodec with a bandwidth in {1.5, 3, 6, 12} kbps and compare them to the baselines. Results for the streamable setup are reported in Figure <ref type="figure" target="#fig_1">3</ref> and a breakdown per category in Table <ref type="table" target="#tab_0">1</ref>. We additionally explored other quantizers such as Gumbel-Softmax and DiffQ (see details in Appendix A.2), however, we found in preliminary results that they provide similar or worse results, hence we do not report them.</p><p>When considering the same bandwidth, EnCodec is superior to all evaluated baselines considering the MUSHRA score. Notice, EnCodec at 3kbps reaches better performance on average than Lyra-v2 using 6kbps and Opus at 12kbps. When considering the additional language model over the codes, we can reduce the bandwidth by ? 25 -40%. For instance, we can reduce the bandwidth of the 3 kpbs model to 1.9 kbps. We observe that for higher bandwidth, the compression ratio is lower, which could be explained by the small size of the Transformer model used, making hard to model all codebooks together. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Ablation study</head><p>Next, we perform an ablation study to better evaluate the effect of the discriminator setup, streaming, multitarget bandwidth, and balancer. We provide more detailed ablation studies in the Appendix, Section A.3.</p><p>The effect of discriminators setup. Various discriminators were proposed in prior work to improve the perceptual quality of the generated audio. The Multi-Scale Discriminator (MSD) model proposed by <ref type="bibr" target="#b39">Kumar et al. (2019)</ref> and adopted in <ref type="bibr" target="#b37">(Kong et al., 2020;</ref><ref type="bibr" target="#b0">Andreev et al., 2022;</ref><ref type="bibr" target="#b74">Zeghidour et al., 2021)</ref>, operates on the raw waveform at different resolutions. We adopt the same MSD configuration as described in <ref type="bibr" target="#b74">Zeghidour et al. (2021)</ref>. <ref type="bibr" target="#b37">Kong et al. (2020)</ref> additionally propose the Multi-Period Discriminator (MPD) model, which reshapes the waveform to a 2D input with multiple periods. Next, the STFT Discriminator (Mono-STFTD) model was introduced in <ref type="bibr" target="#b74">Zeghidour et al. (2021)</ref>, where a single network operates over the complex-valued STFT.</p><p>We evaluate our MS-STFTD discriminator against three other discriminator configurations: (i) MSD+Mono-STFTD (as in <ref type="bibr" target="#b74">Zeghidour et al. (2021)</ref> The effect of the streamable modeling. We also investigate streamable vs. non-streamable setups and report results in Table <ref type="table" target="#tab_2">3</ref>. Unsurprisingly, we notice a small degradation switching from non-streamable to streamable but the performance remains strong while this setting enables streaming inference.</p><p>The effect of the balancer. Lastly, we present results evaluating the impact of the balancer. We train the EnCodec model considering various values ? t , ? f , ? g , and ? f eat with and without the balancer. Results are reported in Table <ref type="table" target="#tab_6">A</ref>.4 in the Appendix. As expected, results suggest the balancer significantly stabilizes the training process. See Appendix A.3 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Stereo Evaluation</head><p>All previously reported results considered only the monophonic setup. Although it makes sense when considering speech data, however for music data, stereo compression is highly important. We adjust our current setup to stereo by only modifying our discriminator setup as described in Section 3.4.</p><p>Results for EnCodec working at 6 kbps, EnCodec with Residual Vector Quantization (RVQ) at 6 kbps, and Opus at 6 kbps, and MP3 at 64 kbps are reported in Table <ref type="table" target="#tab_4">4</ref>. EnCodec is significantly outperforms Opus at 6kbps and is comparable to MP3 at 64kbps, while EnCodec at 12kpbs achieve comparable performance to EnCodec at 24kbps. Using a language model and entropy coding gives a variable gain between 20% to 30%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Latency and computation time</head><p>We report the initial latency and real time factor on Table <ref type="table" target="#tab_5">5</ref>. The real-time factor is here defined as the ratio between the duration of the audio and the processing time, so that it is greater than one when the method is faster than real time. We profiled all models on a single thread of a MacBook Pro 2019 CPU at 6 kbps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial latency.</head><p>The 24 kHz streaming EnCodec model has an initial latency (i.e., without the computation time) of 13.3 ms. The 48 kHz non-streaming version has an initial latency of 1 second, due to the normalizations used. Note that using entropy coding increases the initial latency, because the stream cannot be "flushed" with each frame, in order to keep the overhead small. Thus decoding the frame at time t, requires for the frame t + 1 to be partially received, increasing the latency by 13ms.</p><p>Real time factor. While our model is worse than Lyra v2 in term of processing speed, it processes the audio 10 times faster than real time, making it a good candidate for real life applications. The gain from the entropy coding comes at a cost, although the processing is still faster than real time and could be used for applications where latency is not essential (e.g. streaming). At 48 kHz, the increased number of step size lead to a slower than real time processing, although a more efficient implementation, or using accelerated hardware would improve the RTF. It could also be used for archiving where real time processing is not required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented EnCodec : a state-of-the-art real-time neural audio compression model, producing high-fidelity audio samples across a range of sample rates and bandwidth. We showed subjective and objective results from 24kHz monophonic at 1.5 kbps (Figure <ref type="figure" target="#fig_1">3</ref>) to 48kHz stereophonic (Table <ref type="table" target="#tab_4">4</ref>). We improved sample quality by developing a simple but potent spectrogram-only adversarial loss which efficiently reduces artifacts and produce high-quality samples. Besides, we stabilized training and improved the interpretability of the weights for losses through a novel gradient balancer. Finally, we also demonstrated that a small Transformer model can be used to further reduce the bandwidth by up to 40% without further degradation of quality, in particular for applications where low latency is not essential (e.g. music streaming). with m (resp. ?) the mean (resp. standard deviation) of z along the time and batch axis, and U[-1, 1] uniform i.i.d noise. The limit L is chosen so that when B goes to 0, the noise covers most of the range of values accessible to a Gaussian variable of variance ?. In order to prevent outlier values, we clamp the input z to this expected range of values. If L is too large, the dynamic range of the quantization will be poorly used. If L is too low, many values will get clamped and lose gradients. In practice we choose L = 3, which verifies that values are not clamped 99.5% of the time (against 99.8% if the input were gaussian). In order to learn B, we approximate at train time the bandwidth used by the model by w diffq = T D i=1 B (i) /d with T the number of latent time steps, d the duration of the sample, and add to the training loss a penalty term of the form ?w diffq , as long as w diffq is over a given target, as used in Section 3.4.</p><p>Test time quantization. At validation and test time, we replace the batch-level statistic with an exponential moving average with a decay of 0.9 computed over the train set, similar to batch norm <ref type="bibr" target="#b29">(Ioffe &amp; Szegedy, 2015)</ref>. We first normalize and clamp z to the segment [0, 1] as u = clamp((L + ? -1 (z -m))/2L, 0, 1) and define the number of quantized levels N B = round(2 B ) and the quantized index as</p><formula xml:id="formula_10">i = min [floor(N B ? u), N B -1] ? [0..N B -1],<label>(7)</label></formula><p>with the minimum taken to avoid the edge case u = 1. We know we can code each entry in i on at most log 2 (N B ) bits. The quantized latent z q is finally defined as z q = m + L? 2 i+0.5 N B -1 . Sparsity. We want to allow B to go to 0, however additive noise fails to remove all information contained in the latent in that case. Indeed, if z &lt; m for instance, then even after adding the largest possible amount of noise, z q,train is still biased towards values smaller than m. In order to remove all information about z from z q,train , the scale of the noise relative to the scale of the signal must go to infinity. This is achieved by scaling down z by a factor min(B, 1) in Eq. ( <ref type="formula">6</ref>), while scaling down the additive noise only by a factor min(B, 1). Thus, the decoder cannot invert the downscaling of z without blowing up the noise. In the limit of B ? 0, we recover a sparse representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Gumbel softmax quantizer</head><p>We introduce a second fully differentiable vector quantizer composed of N C codebooks each with ? entries. The i-th codebook is composed of a set of centroids C i ? R ??D , a logit bias b i ? R ? , and a learnt prior logit l i ? R ? . Assuming for simplicity a latent vector for the j-th time step z j ? R D , we define a probability distribution over each codebook entries as q i (z) = softmax(C i z j +b i ). We then sample from the corresponding gumbel-softmax <ref type="bibr" target="#b30">(Jang et al., 2017)</ref> with a temperature ? = 0.5. This gives us a differentiable approximately 1-hot vector over the codebooks, i.e., noting GS the gumbel-softmax,</p><formula xml:id="formula_11">z q,train = N C i=1 GS(log(q i (z)), ? ) T C i . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>At test time, we replace the gumbel-softmax with a sampling from the distribution q i . We define for all i, p i = softmax(l i ) the prior distribution over the codebooks entries which is used for coding the quantized value z q with an arithmetic coder. We can both train the prior and minimize the bandwidth with a single loss term given by the cross entropy between the prior p i and the posterior q i (z), i.e. the differentiable bandwidth estimate for this method is given by w gs =</p><formula xml:id="formula_13">N C i=1 ? k=1 -q i (z) log(p i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional Results</head><p>Comparing to SoundStream. For fair evaluation, we also compare EnCodec to our reimplementation of SoundStream <ref type="bibr" target="#b74">(Zeghidour et al., 2021)</ref>. For which, the quantizer corresponds to RVQ in and the discriminator corresponds to STFTD+MSD. Results are reported in Table <ref type="table" target="#tab_6">A</ref>.2. The results of our SoundStream implementation are slightly worse than EnCodec using DiffQ quantizer. However, when considering the RVQ as the latent quantizer, EnCodec is superior to the SoundStream model. Both EnCodec and SoundStream methods are significantly better than Opus and EVS at 6.0kbps.</p><p>The effect of the model architecture. We investigate the impact of different architectural decisions of our EnCodec model and we present our results with objective metrics and real-time factor in Table A.3. For all models, we consider the streamable setup and our reference EnCodec base model has the number of channels C set to 32 and a single residual unit. The real-time factor reported here is defined as the ratio between the duration of the input audio and the processing time needed for encoding (respectively decoding) it. We profiled streamable multi-target 24 kHz models during inference at 6 kbps on a single thread of a MacBook Pro 2019 CPU. First, we validate that the selected number of channels provide good trade offs in terms of perceived quality and inference speed. We observe that increasing the capacity of the model only marginally affects the scores on objective metrics while it has a high impact on the real-time factor. The results also demonstrate that presence of LSTM improves the SI-SNR and the final reconstruction quality, at the detriment of the real-time factor. We also experiment with increasing the number of residual units instead of relying on a LSTM in our architecture. To do so, we use 3 Residual Units and we double the dilation used in the first convolutional layer of the residual unit for each subsequent unit. We observe that using Residual Units has more impact on the real-time factor and we note a small degradation of the SI-SNR compared to the LSTM-based version.</p><p>The effect of the balancer. Lastly, we present results evaluating the impact of the balancer. In which, we train the EnCodec model considering various levels of balancing the loss. For this set of experiments we use the DiffQ quantizer other RVQ. All models were trained on Jamando music dataset (see Table <ref type="table" target="#tab_6">A</ref> <ref type="bibr">.4)</ref>. Results suggest the balancer significantly stabilizes the training process. This is especially useful while considering the different terms in the objective together where each term operates at a different scale. Following the balancer approach significantly reduce the effort needed for tuning the objective coefficients (i.e., ? t , ? f , ? g , ? f eat ). We demonstrate that the use of the balancer shows no degradation compared to an identified combination of coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Societal impact</head><p>The majority of the internet traffic is represented by audio and video streams (82% in 2021 according to <ref type="bibr" target="#b11">Cisco (2021)</ref>). This share of content is boosted by user-generated content, phone and video calls and by the development of HD music streaming and video streaming services. Compression methods are used to reduce the storage requirements and network bandwidth used to serve this content. Furthermore, the growing adoption of wearable devices contributes to making efficient compression an increasingly important problem.</p><p>Different audio codecs, including Opus and EVS, have been developed and widely adopted over the past years. Those codecs support audio coding at low latency with high audio quality at low to medium bitrates (in the range of 12 to 24 kbps) but the audio quality deteriorates at very low bitrates (eg. 3 kbps) on non-speech audio. Addressing very low bitrate compression with high fidelity remains an essential challenge to solve as very low bitrate codecs enable communication and improve experiences such as videoconferencing or streaming content even with a poor internet connection and therefore allows the internet services to become more inclusive. While further work needs to be done, we hope that sharing the result of this work to the broader community can further contribute to this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: MS-STFT Discriminator architecture. The input to the network is a complex-valued STFT with the real and imaginary parts concatenated. Each discriminator is composed of a 2D convolutional layer, followed by 2D convolutions with increasing dilation rates. Then a final 2D convolution is applied.</figDesc><graphic url="image-3.png" coords="5,95.40,94.92,51.39,91.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Human evaluations (MUSHRA: comparative scoring of samples) across bandwidths of standard codecs and neural codecs. For EnCodec we report the initial bandwidth without entropy coding (in plain) and with entropy coding (hollow). Lyra-v2 is a neural audio codec, while EVS and Opus are competitive standard codecs. The audio samples are from speech and music. The ground truth is 16bits 24kHz wave.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,55.22,81.86,484.80,197.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MUSHRA scores for Opus, EVS, Lyra-v2, and EnCodec for various bandwidths under the streamable setting. Results are reported across different audio categories (clean speech, noisy speech, and music), sampled at 24 kHz. We report mean scores and 95% confidence intervals. For EnCodec , we also report the average bandwidth after using the entropy coding described in Section 3.3.</figDesc><table><row><cell>Model</cell><cell cols="6">Bandwidth Entropy Coded Clean Speech Noisy Speech Music Set-1 Music Set-2</cell></row><row><cell>Reference</cell><cell>-</cell><cell>-</cell><cell>95.5?1.6</cell><cell>93.9?1.8</cell><cell>93.2?2.5</cell><cell>97.1?1.3</cell></row><row><cell>Opus</cell><cell>6.0 kbps</cell><cell>-</cell><cell>30.1?2.8</cell><cell>19.1?5.9</cell><cell>20.6?5.8</cell><cell>17.9?5.3</cell></row><row><cell>Opus</cell><cell>12.0 kbps</cell><cell>-</cell><cell>76.5?2.3</cell><cell>61.9?2.1</cell><cell>77.8?3.2</cell><cell>65.4?2.7</cell></row><row><cell>EVS</cell><cell>9.6 kbps</cell><cell>-</cell><cell>84.4?2.5</cell><cell>80.0?2.4</cell><cell>89.9?2.3</cell><cell>87.7?2.3</cell></row><row><cell>Lyra-v2</cell><cell>3.0 kbps</cell><cell>-</cell><cell>53.1?1.9</cell><cell>52.0?4.7</cell><cell>69.3?3.3</cell><cell>42.3?3.5</cell></row><row><cell>Lyra-v2</cell><cell>6.0 kbps</cell><cell>-</cell><cell>66.2?2.9</cell><cell>59.9?3.3</cell><cell>75.7?2.6</cell><cell>48.6?2.1</cell></row><row><cell>EnCodec</cell><cell>1.5 kbps</cell><cell>0.9 kbps</cell><cell>49.2?2.4</cell><cell>41.3?3.6</cell><cell>68.2?2.2</cell><cell>66.5?2.3</cell></row><row><cell>EnCodec</cell><cell>3.0 kbps</cell><cell>1.9 kbps</cell><cell>67.0?1.5</cell><cell>62.5?2.3</cell><cell>89.6?3.1</cell><cell>87.8?2.9</cell></row><row><cell>EnCodec</cell><cell>6.0 kbps</cell><cell>4.1 kbps</cell><cell>83.1?2.7</cell><cell>69.4?2.3</cell><cell>92.9?1.8</cell><cell>91.3?2.1</cell></row><row><cell cols="2">EnCodec 12.0 kbps</cell><cell>8.9 kbps</cell><cell>90.6?2.6</cell><cell>80.1?2.5</cell><cell>91.8?2.5</cell><cell>92.9?1.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparing discriminators using objective (ViSQOL, SI-SNR) and subjective metrics (MUSHRA).</figDesc><table><row><cell cols="4">Discriminator setup SI-SNR ViSQOL MUSHRA</cell></row><row><cell>MSD+Mono-STFT</cell><cell>5.99</cell><cell>4.22</cell><cell>62.91?2.62</cell></row><row><cell>MPD</cell><cell>7.35</cell><cell>4.24</cell><cell>60.7?2.8</cell></row><row><cell>MS-STFT+MPD</cell><cell>6.55</cell><cell>4.34</cell><cell>79.0?1.9</cell></row><row><cell>MS-STFT</cell><cell>6.67</cell><cell>4.35</cell><cell>77.5?1.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Streamable vs. Non-streamable evaluations at 6 kbps on an equal mix of speech and music.</figDesc><table><row><cell>Model</cell><cell cols="2">Streamable SI-SNR ViSQOL</cell></row><row><cell>Opus</cell><cell>2.45</cell><cell>2.60</cell></row><row><cell>EVS</cell><cell>1.89</cell><cell>2.74</cell></row><row><cell>EnCodec</cell><cell>6.67</cell><cell>4.35</cell></row><row><cell>EnCodec</cell><cell>7.46</cell><cell>4.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Stereophonic extreme music compression versus MP3 and Opus for music sampled at 48 kHz.</figDesc><table><row><cell>Model</cell><cell cols="4">Bandwidth Entropy Coded Compression MUSHRA</cell></row><row><cell>Reference</cell><cell>-</cell><cell>-</cell><cell>1?</cell><cell>95.1?1.8</cell></row><row><cell>MP3</cell><cell>64 kbps</cell><cell>-</cell><cell>24?</cell><cell>82.7?3.2</cell></row><row><cell>Opus</cell><cell>6 kbps</cell><cell>-</cell><cell>256?</cell><cell>17.7?5.9</cell></row><row><cell>Opus</cell><cell>24 kbps</cell><cell>-</cell><cell>64?</cell><cell>82.9?3.7</cell></row><row><cell>EnCodec</cell><cell>6 kbps</cell><cell>4.2 kbps</cell><cell>256?</cell><cell>82.9?2.4</cell></row><row><cell>EnCodec</cell><cell>12 kbps</cell><cell>8.9 kbps</cell><cell>128?</cell><cell>88.0?2.7</cell></row><row><cell>EnCodec</cell><cell>24 kbps</cell><cell>19.4 kbps</cell><cell>64?</cell><cell>87.5?2.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Initial latency and real time factor (RTF) for Lyra v2, EnCodec at 24 kHz and 48 kHz. A RTF greater than 1 indicates faster than real time processing. We report the RTF for both the encoding (Enc.) and decoding (Dec.), without and with entropy coding (EC). All models are evaluated at 6 kbps.</figDesc><table><row><cell>Real Time Factor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A .</head><label>A</label><figDesc>1: Datasets description. License with asterisk annotation * imply that the specific license varies across the dataset and is specific to each sample.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Audio domain Sampling rate Channels Duration License</cell></row><row><cell>Common Voice 7.0</cell><cell>Speech</cell><cell>48 kHz</cell><cell>1</cell><cell>9,096 h CC-0</cell></row><row><cell cols="2">DNS Challenge 4 (speech) Speech</cell><cell>48 kHz</cell><cell>1</cell><cell>2,425 h Multiples*</cell></row><row><cell>AudioSet</cell><cell>General audio</cell><cell>48 kHz</cell><cell>2</cell><cell>4,989 h CC BY 4.0*</cell></row><row><cell>FSD50K</cell><cell>General audio</cell><cell>44.1 kHz</cell><cell>1</cell><cell>108 h CC*</cell></row><row><cell>Jamendo</cell><cell>Music</cell><cell>multiples</cell><cell>2</cell><cell>919 h CC*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A .</head><label>A</label><figDesc>2: A comparison between EnCodec using either DiffQ or RVQ as latent quantizers against our implementation of the SoundStream model at 3kbps. We additionally include the results of Opus and EVS at 6.0kbps for reference.Table A.3: Model architecture Analysis. We explore variations of our architecture including the impact of the number of Residual Units (ResUnits), the sequence modeling with LSTM, the number of channels (C) on objective metrics and real-time factor (RTF greater than 1 is faster than real time).</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">Bandwidth MUSHRA</cell></row><row><cell>Reference</cell><cell></cell><cell>-</cell><cell cols="2">96.1?1.41</cell></row><row><cell>Opus</cell><cell></cell><cell>6.0</cell><cell cols="2">21.1?2.62</cell></row><row><cell>EVS</cell><cell></cell><cell>6.0</cell><cell cols="2">62.9?2.18</cell></row><row><cell>SoundStream</cell><cell></cell><cell>3.0</cell><cell cols="2">71.8?1.51</cell></row><row><cell cols="2">EnCodec (DiffQ)</cell><cell>3.0</cell><cell cols="2">72.3?1.18</cell></row><row><cell>EnCodec (RVQ)</cell><cell></cell><cell>3.0</cell><cell cols="2">76.8?1.31</cell></row><row><cell>Model</cell><cell cols="5">RTF Enc RTF Dec SI-SNR ViSQOL</cell></row><row><cell>EnCodec base</cell><cell></cell><cell>9.8</cell><cell>10.4</cell><cell>6.67</cell><cell>4.35</cell></row><row><cell>Channels = 16</cell><cell></cell><cell>26.0</cell><cell>25.7</cell><cell>6.40</cell><cell>4.32</cell></row><row><cell>Channels = 64</cell><cell></cell><cell>1.3</cell><cell>3.1</cell><cell>6.70</cell><cell>4.38</cell></row><row><cell>norm = None</cell><cell></cell><cell>10.1</cell><cell>10.4</cell><cell>6.45</cell><cell>4.29</cell></row><row><cell>LSTM = 0</cell><cell></cell><cell>15.0</cell><cell>14.6</cell><cell>6.40</cell><cell>4.35</cell></row><row><cell>Residual Layer = 3, LSTM = 0</cell><cell></cell><cell>6.0</cell><cell>7.3</cell><cell>6.32</cell><cell>4.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A .</head><label>A</label><figDesc>4: ViSQOL and SI-SNR results for EnCodec using DiffQ considering various coefficients to balance the overall objective. All models were trained using Jamando music dataset.? t ? f ? g ? f eat Balancer SI-SNR ViSQOL</figDesc><table><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>10.32</cell><cell>4.16</cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>6.16</cell><cell>3.89</cell></row><row><cell>1</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>10.08</cell><cell>4.12</cell></row><row><cell>1</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>5.01</cell><cell>3.77</cell></row><row><cell>1</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>10.06</cell><cell>4.17</cell></row><row><cell>1</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>3.84</cell><cell>3.67</cell></row><row><cell>1</cell><cell>2</cell><cell>4</cell><cell>1</cell><cell>9.93</cell><cell>4.17</cell></row><row><cell>1</cell><cell>2</cell><cell>4</cell><cell>1</cell><cell>1.72</cell><cell>3.52</cell></row><row><cell>1</cell><cell>2</cell><cell cols="2">100 1</cell><cell>8.41</cell><cell>4.05</cell></row><row><cell>1</cell><cell>2</cell><cell cols="2">100 1</cell><cell>-35.83</cell><cell>2.82</cell></row><row><cell>2</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>10.53</cell><cell>4.06</cell></row><row><cell>2</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>7.66</cell><cell>4.03</cell></row><row><cell>2</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>10.19</cell><cell>4.13</cell></row><row><cell>2</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>7.16</cell><cell>3.98</cell></row><row><cell>2</cell><cell>2</cell><cell>10</cell><cell>4</cell><cell>9.82</cell><cell>4.11</cell></row><row><cell>2</cell><cell>2</cell><cell>10</cell><cell>4</cell><cell>3.98</cell><cell>3.66</cell></row><row><cell>2</cell><cell>2</cell><cell cols="2">100 4</cell><cell>8.52</cell><cell>4.03</cell></row><row><cell>2</cell><cell>2</cell><cell cols="2">100 4</cell><cell>-34.31</cell><cell>2.91</cell></row><row><cell cols="2">10 1</cell><cell>1</cell><cell>1</cell><cell>10.53</cell><cell>3.65</cell></row><row><cell cols="2">10 1</cell><cell>1</cell><cell>1</cell><cell>8.16</cell><cell>4.00</cell></row><row><cell cols="2">10 1</cell><cell>4</cell><cell>1</cell><cell>10.72</cell><cell>3.62</cell></row><row><cell cols="2">10 1</cell><cell>4</cell><cell>1</cell><cell>5.99</cell><cell>3.74</cell></row><row><cell cols="2">10 1</cell><cell>4</cell><cell>2</cell><cell>10.71</cell><cell>3.73</cell></row><row><cell cols="2">10 1</cell><cell>4</cell><cell>2</cell><cell>7.03</cell><cell>3.88</cell></row><row><cell cols="2">10 2</cell><cell cols="2">100 2</cell><cell>9.23</cell><cell>4.07</cell></row><row><cell cols="2">10 2</cell><cell cols="2">100 2</cell><cell>-33.78</cell><cell>2.85</cell></row><row><cell cols="2">10 2</cell><cell cols="2">100 4</cell><cell>9.22</cell><cell>4.09</cell></row><row><cell cols="2">10 2</cell><cell cols="2">100 4</cell><cell>-16.39</cell><cell>2.95</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/google/lyra</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We compute visqol with: https://github.com/google/visqol using the recommended recipes.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Experimental details Datasets details. We present additional details and statistics over the datasets used for training in Table <ref type="table">A</ref>.1. Notice that for some datasets, each sub-source or sample has its own license and we then refer the reader to the respective dataset details for more information. We create our datasets splits as followed. For Common Voice, we randomly sample 99.5% of the dataset for train, 0.25% for valid and the rest for test splits. Similarly, we sample 98% of the clean segments from DNS Challenge 4 for train, 1% for valid and 1% for test. For AudioSet, we use the unbalanced train segments as training data and randomly selected half of the eval segments as validation set and the other half as test set. We follow the same procedure for FSD50K using the dev set for training and splitting the eval set between validation and test. Finally for the Jamendo dataset, we randomly take 96% of the artists and their corresponding tracks for train, 2% for valid and 2% for test, hence there is no artists overlap in the different sets.</p><p>SoundStream model. We additionally re-implemented SoundStream. We follow the implementation details in <ref type="bibr" target="#b74">Zeghidour et al. (2021)</ref> to develop our own SoundStream version as the original implementation is not open sourced. We implement Residual Vector Quantization with k-means initialization, exponential moving average updates and random restart as pointed by the original implementation. For the wave-based discriminator, we follow the details provided in <ref type="bibr" target="#b74">Zeghidour et al. (2021)</ref> and refer to the original Multi-Scale Discriminator implementation proposed in <ref type="bibr" target="#b39">Kumar et al. (2019)</ref> for additional information. We use LeakyReLU as non-linear activation function and following <ref type="bibr" target="#b37">Kong et al. (2020)</ref>, we use spectral normalization for the original resolution and weight normalization for other resolutions. For the STFT-based discriminator, we experimented with multiple normalization methods and found that using LayerNorm <ref type="bibr" target="#b3">(Ba et al., 2016)</ref> was the only one that prevented the discriminator from diverging. We used LeakyReLU as non-linear activation function. Finally, training hyper parameters are not shared either so we use the same parameters as for our EnCodec model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Alternative quantizers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 DiffQ Quantizer</head><p>Pseudo quantization noise. We perform scalar quantization of the latent representation. As quantization is non differentiable, we use properly scaled additive independent noise, a.k.a pseudo quantization noise, at train time to simulate it. This approach was first use for analog-to-digital converters design <ref type="bibr" target="#b70">(Widrow et al., 1996)</ref>, then for image compression <ref type="bibr" target="#b4">(Ball? et al., 2017)</ref>, and finally by the DiffQ model compression method <ref type="bibr" target="#b15">(D?fossez et al., 2021)</ref> with a differentiable bandwidth estimate. We extend the DiffQ approach for latent space quantization, adding support for streamable rescaling, proper sparsity, and improved prior coding. Formally, we introduce a learnt parameter B ? R D (with D the dimension of the latent space) such that B (i) represents the number of bits to use of the i-th dimension. In practice B is parameterized as B = B max ? sigmoid(?v), with B max = 15, and the ? = 5 factor is used for boosting the learning rate of v, the learnt parameter. We then define the pseudo quantized representation z q,train used at train time, <ref type="bibr">(6)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Hifi++: a unified framework for neural vocoding, bandwidth extension and speech enhancement</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Andreev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aibek</forename><surname>Alanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13086</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Rosana</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsay</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06670</idno>
		<title level="m">Common voice: A massively-multilingual speech corpus</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech analysis and synthesis by linear prediction of the speech wave</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bishnu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanne</forename><forename type="middle">L</forename><surname>Atal</surname></persName>
		</author>
		<author>
			<persName><surname>Hanauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of the acoustical society of America</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2B</biblScope>
			<biblScope unit="page" from="637" to="655" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end optimized image compression</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valero</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integer networks for data compression with latent-variable models</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Minnen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive multi-rate wideband speech codec based on celp algorithm: architectural study, implementation &amp; performance analysis</title>
		<author>
			<persName><forename type="first">Dipesh</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninad</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yogeshwar</forename><surname>Kosta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 International Conference on Communication Systems and Network Technologies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="547" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The mtg-jamendo dataset for automatic music tagging</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minz</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Tovstogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/10230/42015" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML 2019)</title>
		<meeting><address><addrLine>Long Beach, CA, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single channel voice separation for unknown number of speakers under reverberant and noisy settings</title>
		<author>
			<persName><forename type="first">Lior</forename><surname>Shlomo E Chazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliya</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName><surname>Adi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3730" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visqol v3: An open source production ready objective speech and audio metric</title>
		<author>
			<persName><forename type="first">Felicia</forename><forename type="middle">Sc</forename><surname>Michael Chinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><surname>Gureev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Feargus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>'gorman</surname></persName>
		</author>
		<author>
			<persName><surname>Hines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 twelfth international conference on quality of multimedia experience (QoMEX)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Global -2021 forecast highlights -cisco</title>
		<author>
			<persName><surname>Cisco</surname></persName>
		</author>
		<ptr target="https://www.cisco.com/c/dam/m/en_us/solutions/service-provider/vni-forecast-highlights/pdf/Global_2021_Forecast_Highlights.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Real time speech enhancement in the waveform domain</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Defossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12847</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09987</idno>
		<title level="m">Differentiable model compression via pseudo quantization noise</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The challenge of realistic music generation: modelling raw audio at scale</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sander Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overview of the evs codec architecture</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Multrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaclav</forename><surname>Eksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Malenovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Norvell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Pobloth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5698" to="5702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Icassp 2022 deep noise suppression challenge</title>
		<author>
			<persName><forename type="first">Harishchandra</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishak</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergiy</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sefik Eskimez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manthan</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Gamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Aichner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fsd50k: an open dataset of human-labeled sound events</title>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="829" to="852" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Low bit-rate speech coding with vq-vae and a wavenet decoder</title>
		<author>
			<persName><forename type="first">Cristina</forename><surname>G?rbacea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felicia</forename><forename type="middle">Sc</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Luebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="735" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">It&apos;s raw! audio generation with state-space models</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09729</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vector quantization</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Assp Magazine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="29" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new model-based speech analysis/synthesis system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A spectral energy distance for parallel speech synthesis</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13062" to="13072" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visqol: The virtual speech quality objective listener</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWAENC 2012; International Workshop on Acoustic Signal Enhancement</title>
		<imprint>
			<publisher>VDE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Yao-Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Hubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3451" to="3460" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>1502.03167</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Architecture for variable bitrate neural speech codec with configurable computation complexity</title>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Jayashankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thilo</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaustubh</forename><surname>Kalgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ju</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="861" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end neural speech coding for real-time communications</title>
		<author>
			<persName><forename type="first">Xue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaying</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="866" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nal Kalchbrenner et al. Efficient Neural Audio Synthesis</title>
		<author>
			<persName><forename type="first">Biing-Hwang</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP&apos;82. IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1982">1982. 2018</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Text-free prosody-aware generative spoken language modeling</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu-Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03264</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wavenet based low rate speech coding</title>
		<author>
			<persName><forename type="first">Felicia</forename><forename type="middle">Sc</forename><surname>W Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Luebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="676" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generative speech coding with predictive variance regularization</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>W Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Storus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felicia</forename><forename type="middle">Sc</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Luebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengchin</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6478" to="6482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaekyoung</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17022" to="17033" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu-Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07402</idno>
		<title level="m">Textless speech emotion conversion using decomposed and discrete representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Melgan: Generative adversarial networks for conditional waveform synthesis</title>
		<author>
			<persName><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Thibault De Boissiere</surname></persName>
		</author>
		<author>
			<persName><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Wei Zhen Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>De Br?bisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On generative spoken language modeling from raw audio</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu-Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1336" to="1354" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Direct speech-to-speech translation with discrete units</title>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05604</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Ambroise</forename><surname>Duquenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sravya</forename><surname>Popuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08352</idno>
		<title level="m">Textless speech-to-speech translation on real data</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Variable bitrate discrete neural representations via causal self-attention</title>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanru</forename><surname>Henry Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Pre-registration workshop</title>
		<imprint>
			<date type="published" when="2021">NeurIPS 2021</date>
		</imprint>
	</monogr>
	<note>Remote</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time speech frequency bandwidth extension</title>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Ungureanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Roblek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="691" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust low rate speech coding based on cloned networks and wavenet</title>
		<author>
			<persName><forename type="first">Felicia Sc</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Skoglund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Speech enhancement for low bit rate speech codec</title>
		<author>
			<persName><forename type="first">Ju</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaustubh</forename><surname>Kalgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7777" to="7781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A 2.4 kbit/s melp coder candidate for the new us federal standard</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwan</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bryan George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Barnwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishu</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Speech coding based on a multi-layer neural network</title>
		<author>
			<persName><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Katayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Communications, Including Supercomm Technical Sessions</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="429" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7164" to="7175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paden</forename><surname>Elkahky</surname></persName>
		</author>
		<author>
			<persName><surname>Tomasello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16502</idno>
		<title level="m">Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al. Generative spoken dialogue language modeling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zal?n</forename><surname>Borsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F?lix</forename><surname>De Chaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Quitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName><surname>Tagliasacchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15578</idno>
		<title level="m">Disentangling speech from surroundings in a neural audio codec</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Source coding algorithms for fast data compression</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasco</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University CA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00355</idno>
		<title level="m">Abdelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled self-supervised representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Enhanced direct speech-to-speech translation using self-supervised pre-training and data augmentation</title>
		<author>
			<persName><forename type="first">Sravya</forename><surname>Popuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02967</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Aaron Van den Oord, and Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learned video compression</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3454" to="3463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Universal modeling and coding</title>
		<author>
			<persName><forename type="first">Jorma</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Langdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Method for the subjective assessment of intermediate quality level of audio systems</title>
		<author>
			<persName><surname>Series</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>International Telecommunication Union Radiocommunication Assembly</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Improving opus low bit rate quality with neural speech synthesis</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04628</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Seanet: A multi-modal speech enhancement network</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolis</forename><surname>Misiunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Roblek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02095</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Lpcnet: Improving neural speech synthesis through linear prediction</title>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Skoglund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5891" to="5895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A real-time wideband neural vocoder at 1.6 kb/s using lpcnet</title>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Skoglund</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12087</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Definition of the opus audio codec</title>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koen</forename><surname>Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Terriberry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IETF</title>
		<imprint>
			<date type="published" when="2012-09-02">September, 2, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A review of vector quantization techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vasuki</surname></persName>
		</author>
		<author>
			<persName><surname>Vanathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Potentials</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Neural Information Processing Systems</title>
		<meeting>of Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Statistical theory of quantization</title>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Istvan</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on instrumentation and measurement</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="361" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Min</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6199" to="6203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Min</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6199" to="6203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Gan vocoder: Multi-resolution discriminator is all you need</title>
		<author>
			<persName><forename type="first">Jaeseong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalhyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyuhyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geumbyeol</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyeongsu</forename><surname>Chae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05236</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Soundstream: An end-to-end neural audio codec</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Luebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
