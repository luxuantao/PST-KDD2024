<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning of Binary Hash Codes for Fast Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huei-Fang</forename><surname>Yang</surname></persName>
							<email>hfyang@citi.sinica.edu.tw</email>
						</author>
						<author>
							<persName><forename type="first">Jen-Hao</forename><surname>Hsiao</surname></persName>
							<email>jenhaoh@yahoo-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Academia</forename><surname>Sinica</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yahoo</forename><forename type="middle">!</forename><surname>Taiwan</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning of Binary Hash Codes for Fast Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels. The utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-ofthe-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Content-based image retrieval aims at searching for similar images through the analysis of image content; hence image representations and similarity measure become critical to such a task. Along this research track, one of the most challenging issues is associating the pixel-level information to the semantics from human perception <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>. Despite several hand-crafted features have been proposed to represent the images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22]</ref>, the performance of these visual descriptors is still limited until the recent breakthrough of deep learning. Recent studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> have shown that deep CNN significantly improves the performance on various vision tasks, such as object detection, image classification, and segmentation. These accomplishments are attributed to the ability of deep CNN to learn the rich midlevel image representations.</p><p>As deep CNNs learn rich mid-level image descriptors, Krizhevsky et al. <ref type="bibr" target="#b13">[14]</ref> used the feature vectors from the 7th layer in image retrieval and demonstrated outstanding performance on ImageNet. However, because the CNN features are high-dimensional and directly computing the similarity between two 4096-dimensional vectors is inefficient, Babenko et al. <ref type="bibr" target="#b0">[1]</ref> proposed to compress the CNN features using PCA and discriminative dimensionality reduction, and obtained a good performance.</p><p>In CBIR, both image representations and computational cost play an essential role. Due to the recent growth of visual contents, rapid search in a large database becomes an emerging need. Many studies aim at answering the question that how to efficiently retrieve the relevant data from the large-scale database. Due to the high-computational cost, traditional linear search (or exhaustive search) is not appropriate for searching in a large corpus. Instead of linear search, a practical strategy is to use the technique of Approximate Nearest Neighbor (ANN) or hashing based method <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref> for speedup. These methods project the high-dimensional features to a lower dimensional space, and then generate the compact binary codes. Benefiting from the produced binary codes, fast image search can be carried out via binary pattern matching or Hamming distance measurement, which dramatically reduces the computational cost and further optimizes the efficiency of the search. Some of these methods belong to the pair-wised method that use similarity matrix (containing the pair-wised similarity of data) to describe the relationship of the image pairs or data pairs, and employ this similarity information to learn hash functions. However, it is demanding to construct the matrix and generate the codes when dealing with a large-scale dataset.</p><p>Inspiring from the advancement of deep learning, we raise a question that can we take the advantage of deep CNN to achieve hashing? Instead of the use of the pairwised learning method, can we generate the binary compact codes directly from the deep CNN? To address these questions, we propose a deep CNN model that can simultaneously learn image representations and binary codes, under the assumption that the data are labeled. That is, our method is designed particularly for supervised learning. Furthermore, we argue that when a powerful learning model such as deep CNN is used and the data labels are available, the binary codes can be learned by employing some hidden layer for representing the latent concepts (with binary activation functions such as sigmoid) that dominate the class labels in the architecture. This is different from other supervised methods (such as <ref type="bibr" target="#b29">[30]</ref>) that take into consideration the data labels but require pair-wised inputs to the prepared learning process. In other words, our approach learns binary hashing codes in a point-wised manner, taking advantage of the incremental learning nature (via stochastic gradient descent) of deep CNN. The employment of deep architecture also allows for efficient-retrieval feature learning. Our method is suitable for large datasets in comparison of conventional approaches.</p><p>Our method is with the following characteristics:</p><p>• We introduce a simple yet effective supervised learning framework for rapid image retrieval.</p><p>• With small modifications to the network model, our deep CNN simultaneously learns domain specific image representations and a set of hashing-like functions for rapid image retrieval.</p><p>• The proposed method outperforms all of the stateof-the-art works on the public dataset MNIST and CIFAR-10. Our model improves the previous best retrieval performance on CIFAR10 dataset by 30% precision, and on MNIST dataset by 1% precision.</p><p>• Our approach learns binary hashing codes in a pointwised manner and is easily scalable to the data size in comparison of conventional pair-wised approaches.</p><p>This paper is organized as follows: We briefly review the related work of hashing algorithms and image retrieval with deep learning in Section 2. We elaborate on the details of our method in Section 3. Finally, experimental results are provided in Section 4, followed by conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several hashing algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref> have been proposed to approximately identify data relevant to the query. These approaches can be classified into two main categories, unsupervised and supervised methods.</p><p>Unsupervised hashing methods use unlabeled data to learn a set of hash functions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8]</ref>. The most representative one is the Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b5">[6]</ref>, which aims at maximizing the probability that similar data are mapped to similar binary codes. LSH generates the binary codes by projecting the data points to a random hyperplane with random threshold. Spectral hashing (SH) <ref type="bibr" target="#b28">[29]</ref> is another representative approach, which produces the compact binary codes via thresholding with non-linear functions along the PCA direction of the given data.</p><p>Recent studies have shown that using supervised information can boost the binary hash codes learning performance. Supervised approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15]</ref> incorporate label information during learning. These supervised hashing methods usually use the pair-wised labels for generating effective hash functions. However, these algorithms generally require a large sparse matrix to describe the similarity between data points in the training set.</p><p>Beside the research track of hashing, image representations also play an essential role in CBIR. CNN-based visual descriptors have been applied on the task of image retrieval recently. Krizhevsky et al. <ref type="bibr" target="#b13">[14]</ref> firstly use the features extracted from seventh layer to retrieve images, and achieve impressive performance on ImageNet. Babenko et al. <ref type="bibr" target="#b0">[1]</ref> focus on dimensional reduction of the CNN features, and improve the retrieval performance with compressed CNN features. Though these recent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1]</ref> present good results on the task of image retrieval, the learned CNN features are employed for retrieval by directly performing pattern matching in the Euclidean space, which is inefficient.</p><p>Deep architectures have been used for hash learning. However, most of them are unsupervised, where deep autoencoders are used for learning the representations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref>. Xia et al. <ref type="bibr" target="#b29">[30]</ref> propose a supervised hashing approach to learn binary hashing codes for fast image retrieval through deep learning and demonstrate state-of-the-art retrieval performance on public datasets. However, in their preprocessing stage, a matrix-decomposition algorithm is used for learning the representation codes for data. It thus requires the input of a pair-wised similarity matrix of the data and is unfavorable for the case when the data size is large (e.g., 1M in our experiment) because it consumes both considerable storage and computational time.</p><p>In contrast, we present a simple but efficient deep learning approach to learn a set of effective hash-like functions, and it achieves more favorable results on the publicly available datasets. We further apply our method to a large-scale dataset of 1 million clothing images to demonstrate the scalability of our approach. We will describe the proposed method in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Figure <ref type="figure">1</ref> shows the proposed framework. Our method includes three main components. The first component is the supervised pre-training on the large-scale ImageNet dataset <ref type="bibr" target="#b13">[14]</ref>. The second component is fine-tuning the network with the latent layer to simultaneously learn domainspecific feature representation and a set of hash-like function. The third retrieves images similar to the query one via the proposed hierarchical deep search. We use the pre- trained CNN model proposed by Krizhevsky et al. <ref type="bibr" target="#b13">[14]</ref> from the Caffe CNN library <ref type="bibr" target="#b10">[11]</ref>, which is trained on the large-scale ImageNet dataset which contains more than 1.2 million images categorized into 1000 object classes. Our method for learning binary codes is described in detail as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Hash-like Binary Codes</head><p>Recent studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b0">1]</ref> have shown that the feature activations of layers F 6−8 induced by the input image can serve as the visual signatures. The use of these mid-level image representations demonstrates impressive improvement on the task of image classification, retrieval, and others. However, these signatures are high-dimensional vectors that are inefficient for image retrieval in a large corpus. To facilitate efficient image retrieval, a practical way to reduce the computational cost is to convert the feature vectors to binary codes. Such binary compact codes can be quickly compared using hashing or Hamming distance.</p><p>In this work, we propose to learn the domain specific image representations and a set of hash-like (or binary coded) functions simultaneously. We assume that the final outputs of the classification layer F 8 rely on a set of h hidden attributes with each attribute on or off. In other points of view, images inducing similar binary activations would have the same label. To fulfill this idea, we embed the latent layer H between F 7 and F 8 as shown in the middle row of Figure <ref type="figure">1</ref>. The latent layer H is a fully connected layer, and its neuron activities are regulated by the succeeding layer F 8 that encodes semantics and achieves classification. The proposed latent layer H not only provides an abstraction of the rich features from F 7 , but also bridges the mid-level features and the high-level semantics. In our design, the neurons in the latent layer H are activated by sigmoid functions so the activations are approximated to {0, 1}.</p><p>To achieve domain adaptation, we fine-tune the proposed network on the target-domain dataset via back propagation. The initial weights of the deep CNN are set as the weights trained from ImageNet dataset. The weights of the latent layer H and the final classification layer F 8 are randomly initialized. The initial random weights of latent layer H acts like LSH <ref type="bibr" target="#b5">[6]</ref> which uses random projections for con-structing the hashing bits. The codes are then adapted from LSH to those that suit the data better from supervised deepnetwork learning. Without dramatic modifications to a deep CNN model, the propose model learns domain specific visual descriptors and a set of hashing-like functions simultaneously for efficient image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Retrieval via Hierarchical Deep Search</head><p>Zeiler and Fergus <ref type="bibr" target="#b31">[32]</ref> analyzed the deep CNN and showed that the shallow layers learn local visual descriptors while the deeper layers of CNN capture the semantic information suitable for recognition. We adopt a coarse-to-fine search strategy for rapid and accurate image retrieval. We firstly retrieve a set of candidates with similar high-level semantics, that is, with similar hidden binary activations from the latent layer. Then, to further filter the images with similar appearance, similarity ranking is performed based on the deepest mid-level image representations.</p><p>Coarse-level Search. Given an image I, we first extract the outputs of the latent layer as the image signature which is denoted by Out(H). The binary codes are then obtained by binarizing the activations by a threshold. For each bit j = 1 • • • h (where h is the number of nodes in the latent layer), we output the binary codes of H by</p><formula xml:id="formula_0">H j = { 1 Out j (H) ≥ 0.5, 0 otherwise. (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Let Γ = {I 1 , I 2 , . . . , I n } denote the dataset consisting of n images for retrieval. The corresponding binary codes of each images are denoted as Γ H = {H 1 , H 2 , . . . , H n } with H i ∈ {0, 1} h . Given a query image I q and its binary codes H q , we identify a pool of m candidates, P = {I c 1 , I c 2 , . . . , I c }, if the Hamming distance between H q and H i ∈ Γ H is lower than a threshold.</p><p>Fine-level Search. Given the query image I q and the candidate pool P , we use the features extracted from the layer F 7 to identify the top k ranked images to form the candidate pool P . Let V q and V P i denote the feature vectors of the query image q and of the image I c i from the pool, respectively. We define the similarity level between I q and the i-th image of P as the Euclidean distance between their corresponding features vectors,</p><formula xml:id="formula_2">s i = ∥V q − V P i ∥. (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>The smaller the Euclidean distance is, the higher level the similarity of the two images is. Each candidate I c i is ranked in ascending order by the similarity; hence, top k ranked images are identified. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we demonstrate the benefits of our approach. We start with introducing the datasets and then present our experimental results with performance comparison to several state-of-the-arts on the public datasets, MNIST and CIFAR-10 datasets. Finally, we verify the scalability and the efficacy of our approach on the large-scale Yahoo-1M dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>MNIST Dataset <ref type="bibr" target="#b15">[16]</ref> consists of 10 categories of the handwritten digits form 0 to 9. There are 60,000 training images, and 10,000 test images. All the digits are normalized to gray-scale images with size 28 × 28.</p><p>CIFAR-10 Dataset <ref type="bibr" target="#b11">[12]</ref> contains 10 object categories and each class consists of 6,000 images, resulting in a total of 60,000 images. The dataset is split into training and test sets, with 50,000 and 10,000 images respectively.</p><p>Yahoo-1M Dataset contains a total of 1,124,087 shopping product images, categorized into 116 clothing-specific classes. The dataset is collected by crawling the images from the Yahoo shopping sites. All the images are labeled with a category, such as Top, Dress, Skirt and so on. Figure <ref type="figure" target="#fig_1">2</ref> shows some examples of the dataset.</p><p>In the experiments of MNIST and CIFAR-10, we retrieve the relevant images using the learned binary codes in order to fairly compare with other hashing algorithms. In the experiments of Yahoo-1M dataset, we retrieve similar images from the entire dataset via the hierarchical search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We use a ranking based criterion <ref type="bibr" target="#b3">[4]</ref> for evaluation. Given a query image q and a similarity measure, a rank can   <ref type="bibr" target="#b30">[31]</ref> 0.47 NIN + Dropout <ref type="bibr" target="#b16">[17]</ref> 0.47 Conv. maxout + Dropout <ref type="bibr" target="#b8">[9]</ref> 0.45</p><p>Ours w/ 48 nodes latent layer 0.47 Ours w/ 128 nodes latent layer 0.50 be assigned for each dataset image. We evaluate the ranking of top k images with respect to a query image q by a precision:</p><formula xml:id="formula_4">P recision@k = ∑ k i=1 Rel(i) k , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where Rel(i) denotes the ground truth relevance between a query q and the i-th ranked image. Here, we consider only the category label in measuring the relevance so Rel(i) ∈ {0, 1} with 1 for the query and the ith image with the same label and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on MNIST Dataset</head><p>Performance of Image Classification. To adapt our deep CNN on the new domain, we modify the layer F 8 to 10way softmax to predict 10 digit classes. In order to measure the effect of latent layer embedded in the deep CNN, we set the number of neurons h in the latent layer to 48 and 128, respectively. Then, we apply stochastic gradient descent (SGD) to train the CNN on the MNIST dataset. The network is trained for 50,000 iterations with a learning rate of 0.001. We compare our results with several state-of-the-arts <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref> in Table <ref type="table" target="#tab_0">1</ref>. Our approach with 48 latent nodes attains 0.47% error rate and performs favorably against most of the alternatives. It it worth noting that our model is designed particularly for image retrieval whereas others are optimizing for a classification task through modification of a network. For example, the work of <ref type="bibr" target="#b30">[31]</ref> proposed the maxout activation function which improves the accuracy of dropout's approximate model averaging technique. Another representative work is Network in Network (NIN) <ref type="bibr" target="#b16">[17]</ref>, which enhances the discriminability of local patches via multilayer perception, and avoids overfitting using the global average pooling instead of the fully connected layers. Also note that our method with 48 latent nodes yields an error rate lower than the model with 128 nodes does. This may be due to that few latent nodes are capable of representing latent concepts for classification and adding more neurons can cause overfitting.  Stochastic Pooling <ref type="bibr" target="#b30">[31]</ref> 84.87 CNN + Spearmint <ref type="bibr" target="#b25">[26]</ref> 85.02 MCDNN <ref type="bibr" target="#b2">[3]</ref> 88.79 AlexNet + Fine-tuning <ref type="bibr" target="#b13">[14]</ref> 89 NIN + Dropout <ref type="bibr" target="#b16">[17]</ref> 89.59 NIN + Dropout + Augmentation <ref type="bibr" target="#b16">[17]</ref> 91.2</p><p>Ours w/ 48 nodes latent layer 89.4 Ours w/ 128 nodes latent layer 89.6</p><p>Performance of Images Retrieval. In this experiment, we unify the retrieval evaluation that retrieve the relevant images using 48 bits binary code and hamming distance measure. The retrieval is performed by randomly selecting 1,000 query images from the testing set for the system to retrieve relevant ones from the training set.</p><p>To evaluate the retrieval performance, we compare the proposed method with several state-of-the-art hashing approaches, including supervised (KSH <ref type="bibr" target="#b17">[18]</ref>, MLH <ref type="bibr" target="#b19">[20]</ref>, BRE <ref type="bibr" target="#b14">[15]</ref>, CNNH <ref type="bibr" target="#b29">[30]</ref>, and CNNH+ <ref type="bibr" target="#b29">[30]</ref>) and unsupervised methods (LSH <ref type="bibr" target="#b5">[6]</ref>, SH <ref type="bibr" target="#b28">[29]</ref>, and ITQ <ref type="bibr" target="#b7">[8]</ref>). Figure <ref type="figure" target="#fig_3">4</ref> shows the retrieval precision of different methods with respect to different number of retrieved images. As can be seen, our method demonstrates stable performance (98.2 + − 0.3% retrieval precision) regardless of the number of images retrieved. Furthermore, our approach improves the precision to 98.5% from 97.5% achieved by CNNH+ <ref type="bibr" target="#b29">[30]</ref>, which learns the hashing functions via decomposition of the pair-wised similarity information. This improvement indicates that our point-wised method that requires only class labels is effective.</p><p>We further analyze the quality of the learned hash-like codes for h = 48 and h = 128, respectively, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. As can be seen, both settings can learn informative binary codes for image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on CIFAR-10 Dataset</head><p>Performance of Image Classification. To transfer the deep CNN to the domain of CIFAR-10, we modify F 8 to 10way softmax to predict 10 object categories, and h is also set as 48 and 128. We then fine-tune our network model on the CIFAR-10 dataset, and finally achieves around 89% testing accuracy after 50, 000 training iterations. As shown in Table <ref type="table" target="#tab_1">2</ref>, the proposed method is more favorable against most approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>  Performance of Image Retrieval. In order for a fair comparison with other hashing algorithms, we unify the evaluation method that retrieves the relevant images by 48 bits binary codes and Hamming distance. Figure <ref type="figure" target="#fig_5">6</ref> shows the precision curves with respect to different number of the top retrieved samples. Our approach achieves better performance than other unsupervised and supervised methods. Moreover, it attains a precision of 89% while varying the number of retrieved images, improving the performance by a margin of 30% compared to CNNH+ <ref type="bibr" target="#b29">[30]</ref>. These results suggest that the use of a latent layer for representing the hidden concepts is a practical approach to learning efficient binary codes.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows our retrieval results. The proposed latent binary codes successfully retrieve images with relevant category, similar appearance, and/or both. Increasing the bit numbers from h = 48 to h = 128 retrieves more appearance-relevant images according to our empirical eyeball checking. For example, in Figure <ref type="figure" target="#fig_4">5</ref>, using h = 128 bits binary code tends to retrieve more relevant horse-head images (instead of entire horses) than that of the h = 48 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on Yahoo-1M Dataset.</head><p>Performance of Image Classification. To show the scalability and efficacy of our method, we further test it on the large-scale Yahoo-1M dataset. This dataset consists of plentiful product images that are heterogeneous and they are variant in person poses with noisy backgrounds.</p><p>We set the number of neurons in the classification layer to 116, and h in the latent layer to 128. We then finetune our network with the entire Yahoo-1M dataset. After 750, 000 training iterations, our proposed approach achieves 83.75% accuracy (obtained by the final layer) on the task of 116 categories clothing classification. As shown in <ref type="bibr">Fig 7,</ref><ref type="bibr"></ref> though the clothing images are backgroundless or of noisy backgrounds, with or without human, the proposed method demonstrates a good classification performance. Note that some of the images are miss-predicted because the products might be ambiguous between some specific categories. For example, it might be difficult to distinguish between the Mary Janes and the Flats as shown in Fig <ref type="figure" target="#fig_6">7</ref>. However, our method can still retrieve the images similar to the query image.</p><p>Performance of Images Retrieval. In this experiment, we demonstrate that our method can learn efficient deep binary codes for the dataset of million data. This is demanding to achieve by using previous pairwised-data approaches due to the large time and storage complexity.</p><p>Because image representations are critical to image retrieval, we compare the retrieval results obtained by features from different network modes: (1) AlexNet: F 7 feature from the pre-trained CNN <ref type="bibr" target="#b13">[14]</ref>; (2) Ours-ES: F 7 features from our network; (3) Ours-BCS: Latent binary codes from our network; and (4) Ours-HDS: F 7 features and latent binary codes from our network.</p><p>We conduct the exhaustive search (or linear search) based on L 2 -norm distance when the F 7 features are used in  retrieval; the hashing is performed based on Hamming distance when the binary codes of the latent layer are used; the coarse-to-fine hierarchical search is performed to retrieve relevant images by using both the laytent layer codes and F 7 . We randomly select 1000 images from the Yahoo-1M dataset, and retrieve the relevant images from the same dataset.</p><p>Figure <ref type="figure" target="#fig_7">8</ref> shows the precision regarding to various number of the top images retrieved using different CNN features. The proposed methods perform more favorably against the original AlexNet feature. Apparently, the procedure of fine-tuning successfully transfers deep CNN to the new domain (clothing images). Among the fine-tuned models, Our-ES and Our-HDS show good retrieval precision at first. However, Ours-BCS outperforms Ours-ES with higher and more stable retrieval precision when more than 12 images are retrieved. This indicates the learned binary codes are informative and with high discriminative power. Ours-HDS complements both Ours-BCS and Ours-ES and achieves the best retrieval precision in overall.</p><p>Figure <ref type="figure" target="#fig_8">9</ref> shows the top 5 images retrieved by different features. As can be seen, AlexNet retrieves the images with great diversity. The fine-tuned models retrieve more images with the same label as the query than AlexNet. Ours-HDS, Ours-BCS, and Ours-ES demonstrate good performance, and successfully retrieve similar products. Nevertheless, benefiting from the binary codes, Ours-BCS achieves the fastest search among the approaches compared. Extracting CNN features takes around 60 milliseconds (ms) on the machine with Geforce GTX 780 GPU and 3 GB memory. The search is carried out on the CPU mode with C/C++ implementation. Performing an Euclidean distance measure between two 4096-dimensional vectors takes 109.767 ms.</p><p>In contrast, computing the hamming distance between two 128 bits binary codes takes 0.113 ms. Thus, Ours-BCS is 971.3x faster than traditional exhaustive search with 4096dimensional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We present a simple yet effective deep learning framework to create the hash-like binary codes for fast image retrieval. We add a latent-attribute layer in the deep CNN to simultaneously learn domain specific image representations and a set of hash-like functions. Our method does not rely on pairwised similarities of data and is highly scalable to the dataset size. Experimental results show that, with only a simple modification of the deep CNN, our method improves the previous best retrieval results with 1% and 30% retrieval precision on the MNIST and CIFAR-10 datasets, respectively. We further demonstrate the scalability and efficacy of the proposed approach on the large-scale dataset of 1 million shopping images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>7 F 8 F 8 F 7 FFigure 1 :</head><label>78871</label><figDesc>Figure1: The proposed image retrieval framework via hierarchical deep search. Our method consists of three main components. The first is the supervised pre-training of a convolutional neural network on the ImageNet to learn rich mid-level image representations. In the second component, we add a latent layer to the network and have neurons in this layer learn hashes-like representations while fine-tuning it on the target domain dataset. The final stage is to retrieve similar images using a coarse-to-fine strategy that utilizes the learn hashes-like binary codes and F 7 features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample images from the Yahoo-1M Shopping Dataset. The heterogeneous product images demonstrate highly variation, and are challenging to image classification and retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top 10 retrieved images from MNIST dataset by vary bit numbers of the latent binary codes. Relevant images with similar appearance are retrieved when the bit numbers increased.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Image retrieval precision with 48 bits of MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Top 10 retrieved images from CIFAR-10 by vary bit numbers of the latent binary codes. Relevant images with similar appearance are retrieved when the bit numbers increased.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Image retrieval precision with 48 bits of CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Image classification results on Yahoo-1M dataset. The first row indicates ground truth label. The bars below depict the prediction scores sorted in ascending order. Red and blue bar represent the correct and incorrect predictions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Image retrieval precision of Yahoo-1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Top 5 retrieved images from Yahoo-1M dataset by different features. The blue check marks indicate the query and retrieved images share the same label; the black crosses indicate otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance Comparison (Error, %) of Classification Error Rates on the MNIST dataset.</figDesc><table><row><cell>Methods</cell><cell>Test Error (%)</cell></row><row><cell>2-Layer CNN + 2-Layer NN [31]</cell><cell>0.53</cell></row><row><cell>Stochastic Pooling</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance Comparison (mAP, %) of Classification Accuracy on the CIFAR-10 dataset.</figDesc><table><row><cell>Methods</cell><cell>Accuracy (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, which indicates that embedding the binary latent layer in the deep CNN does not severely alter the performance.</figDesc><table><row><cell>Dress</cell><cell></cell><cell>Camis</cell><cell></cell><cell>Top</cell><cell></cell><cell>Coat</cell><cell></cell><cell cols="2">Mary Janes</cell><cell>Top</cell><cell></cell></row><row><cell>56%</cell><cell>Dress</cell><cell>76%</cell><cell>Camis</cell><cell>72%</cell><cell>Top</cell><cell>98%</cell><cell>Coat</cell><cell>34%</cell><cell>Flats</cell><cell>99%</cell><cell>Top</cell></row><row><cell></cell><cell>Suit Skirt Top</cell><cell></cell><cell>Top Dress Skirt</cell><cell>21%</cell><cell>Dress Top-XL Skirt</cell><cell cols="2">Shirt Down Jacket Jacket</cell><cell>30% 14%</cell><cell>Mary Janes Heels Casual Shoes</cell><cell></cell><cell>Dress Camis Top-XL</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This work was supported in part by the Ministry of Science and Technology of Taiwan under Contract MOST 103-2221-E-001-010.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical semantic indexing for large scale image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
				<imprint>
			<date type="published" when="1999">1999. 1, 2, 4, 6</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast image search for learned metrics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using very deep autoencoders for content-based image retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007">2012. 1, 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006">2011. 1, 2, 6</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Indexing chromatic and achromatic patterns for content-based colour image retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>PR</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1675" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
				<meeting>CVPRW</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">500</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning for content-based image retrieval: A comprehensive study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
				<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for scalable image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3424" to="3431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2007">2014. 1, 2, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3557</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
