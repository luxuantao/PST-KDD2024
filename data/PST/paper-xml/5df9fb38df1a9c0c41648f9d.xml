<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accuracy Improvement of Memory System Simulation for Modern Shared Memory Processor</title>
				<funder ref="#_jaTrZaW">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuetsu</forename><surname>Kodama</surname></persName>
							<email>yuetsu.kodama@riken.jp</email>
						</author>
						<author>
							<persName><forename type="first">Mitsuhisa</forename><surname>Sato</surname></persName>
							<email>msato@riken.jp</email>
						</author>
						<author>
							<persName><forename type="first">Tetsuya</forename><surname>Odajima</surname></persName>
							<email>tetsuya.odajima@riken.jp</email>
						</author>
						<author>
							<persName><forename type="first">Akira</forename><surname>Asato</surname></persName>
							<email>asato@jp.fujitsu.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">RIKEN Center for Computational Science(R-CCS) Tetsuya Odajima</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>Core0 Core1 Core2 Core3 Core4 Core5 Core6 Core7 Core8 Core9 Core10 Core11</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accuracy Improvement of Memory System Simulation for Modern Shared Memory Processor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3368474.3368483</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the purpose of developing applications for supercomputer Fugaku at an early stage, RIKEN has developed a processor simulator. This simulator is based on the general-purpose processor simulator gem5. It does not simulate the actual hardware of a Fugaku processor. However, we believe that su cient simulation accuracy can be obtained since it simulates the instruction pipeline of out-of-order execution with cycle-level accuracy along with performing detailed parameter tuning of out-of-order resources. In order to estimate the accurate execution time of a program, it is necessary to simulate with accuracy not only the instruction execution time, but also the access time of the cache memory hierarchy. Therefore, in the RIKEN simulator, we expanded gem5 to match the performance of the cache memory hierarchy to that of a Fugaku processor. In this simulator, we aim to estimate the execution cycles of one node application on a Fugaku processor with accuracy that enables relative evaluation and application tuning. In this paper, we show the details of the implementation of this simulator and verify its accuracy compared with that of a Fugaku processor test chip. In the evaluation of the total 46 kernel benchmarks, it was con rmed that the di erence is 13% or less for 85% of the kernels. In the multithreaded execution of Stream Triad benchmark, scalable performance according to the number of threads was con rmed, and achieved over 80% of memory throughput with enough accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fugaku is planned to start operation around 2021, but there is still more time to provide early access whereby users can use preliminarily a part of the actual machines. RIKEN has developed a RIKEN Fugaku processor simulator with the aim of developing applications on the Fugaku as soon as possible.</p><p>The RIKEN simulator is based on the general-purpose processor simulator gem5 <ref type="bibr" target="#b0">[1]</ref>. In gem5, out-of-order execution based on the basic architecture model can be simulated and the correct number of execution cycles can be estimated. This basic architecture model di ers in certain details from the Fujitsu A64FX processor <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> which is the processor of the Fugaku. However, we believe that su cient accuracy can be obtained since it simulates along with performing detailed parameter tuning of out-of-order resources and function expansion of the cache memory hierarchy. In this simulator, we aim to estimate the execution cycles of one node application on a Fugaku processor with accuracy that enables relative evaluation and application tuning.</p><p>In this paper, we rst show an overview of the A64FX, and explain the general purpose processor simulator gem5, as the basis of our simulator. Next, we show the details of the implementation of the RIKEN simulator. Then we compare the execution time of this simulator with that of the A64FX test chip, and verify its accuracy. Finally we summarize this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FUGAKU PROCESSOR, THE A64FX</head><p>Figure <ref type="figure">1</ref> shows the con guration of the A64FX, which is the Fugaku processor developed by Fujitsu. The A64FX consists of four CMGs (Core Memory Group) connected via a ring bus. Each CMG has 13 cores (including an OS support core called an assistant core), a shared L2 cache and a memory controller. The L2 cache has a capacity of 8 MiB per CMG, has a throughput exceeding 900 GB/s, and supports coherence between CMGs on a chip. The memory HBM2 is mounted in the same package with a capacity of 8 GiB per CMG and has a throughput of 256 GB/s per CMG.</p><p>Each core is based on the 64-bit architecture of the Armv8.2-A and supports a new SIMD extension called SVE (Scalable Vector Extension) <ref type="bibr" target="#b3">[4]</ref>. Each core has two SIMD pipelines of 512 bits, and the computing performance of the entire processor exceeds 2.7 TFLOPS. In addition to supporting double precision and single precision oating point numbers, SVE supports half precision and also supports 64 bit, 32 bit, 16 bit, and 8 bit integers. Each core has a 64 KiB data and instruction L1 cache. The data cache load performance exceeds 230 GB/s and the store performance exceeds The A64FX has a network interface in the processor that supports the 6-dimensional torus network (TofuD), the same as the K Computer. Each link speed has been expanded to 6.8 GB/s, and the number of transfer engines has been increased to six.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROCESSOR SIMULATOR GEM5</head><p>The RIKEN simulator is based on the open source processor simulator gem5. The main features of gem5 are as follows. For details, please refer to http://gem5.org.</p><p>? It supports multiple instruction set architectures (ISA), such as Alpha, Arm, SPARC, x86, RISC-V, GPU and so on. The RIKEN simulator only covers the Armv8-A architecture. ? It supports two system modes. One is full system (FS) mode which simulates the OS code with the processor on the simulator. The other is system emulation (SE) mode which simulates the system call by software. The RIKEN simulator supports SE mode only.  After that, Arm Research also developed an O3 model for SVE. We compared both implementations, and found that there was no big di erence. Since the Arm version will be integrated into the main gem5 distribution, we decided to move to the Arm version. The transition began in April 2018, and was completed in October 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RIKEN SIMULATOR</head><p>In the O3 model of gem5, it is possible to exibly specify detailed parameters such as the latency and the throughput of each stage of the pipeline, as well as each resource size for out-of-order execution. We received the detailed parameters of the A64FX processor from Fujitsu. By adjusting these parameters, we can simulate programs with su cient accuracy within the processor. However, since the CPU model of gem5 is based on an out-of-order pipeline based on that of the Alpha 21264, the di erence exists. The main di erences are as follows.</p><p>? While there is one reservation station in gem5, the A64FX is divided into a total of four: one for memory address calculation, two for arithmetic operations, and one for branch operations. The RIKEN simulator makes adjustments with values close to the four total values, so if processing units are used that are biased toward any of them, the di erence between the two may become large. ? In gem5, the memory address calculation is performed in the load/store unit, so the store instruction is executed after both the memory address operand and the write data operand are determined. On the other hand, in the A64FX, memory addresses can be calculated in independent units.</p><p>Since the RIKEN simulator is the same as gem5, there may be di erences in the timing of address calculation. ? In gem5, it is assumed that one instruction uses a single execution unit, while in the A64FX, some instructions may use multiple execution units in parallel or in sequential. In the RIKEN simulator, one instruction is assigned to one main execution unit, assuming that the frequency of such instructions are low. Also, some instructions, for example gather load instructions, are divided into multiple micro instructions in the decode stage in gem5. These instructions will use more out-of-order resources than in the A64FX and may appear as a di erence in execution time.</p><p>The RIKEN simulator currently supports only one CMG simulation, and multithreaded execution of up to 12 cores is possible. In order to estimate the accurate execution time of a program, it is necessary to simulate with accuracy not only the instruction execution time, but also the access time of the cache memory hierarchy. Therefore, in the RIKEN simulator, we expanded gem5 to match the performance of the cache memory hierarchy to that of the A64FX. The main extensions are as follows. These functions were developed for the RIKEN simulator, but many of them are general functions and can be applied to other processors.</p><p>? The L1 cache and L2 cache capacity, associativity, line size and latency were set in the gem5 parameter le according to the actual settings of the A64FX. ? In gem5, it is assumed that load and store operations can access the L1 cache in the same cycle. On the other hand, the A64FX enables two load or single store operations in a cycle. The RIKEN simulator enabled the same controls as those of the A64FX. ? In gem5, the L1 cache access in order to maintain L2 cache such as cache ll from L2 cache to L1 cache and write-back from L1 cache to L2 cache is controlled independently of the L1 cache access from the core. Therefore, the performance of the L1 cache will be enhanced. By performing exclusive control between them, the RIKEN simulator was able to simulate the L1 cache performance of the A64FX accurately. ? In gem5, when the core accesses to the L1 cache exceeds cache alignment, the access is divided into multiple accesses, and the overhead occurs. The A64FX is designed so as not to cause performance degradation even when accessing across cache lines. The RIKEN simulator also supports the unaligned cache access without overhead. ? In gem5 for ARM ISA, software prefetch was supported, but only prefetch for read access targeting L1 cache was implemented. Prefetch for write access is important for optimizing memory access, so the RIKEN simulator supports it. This feature has already been reported to the gem5 developers and accepted. Software prefetch targeting L2 cache is also important for optimizing memory access. The RIKEN simulator also supports it. ? Gem5 has several hardware prefetching capabilities, but there are only simple prefetches such as queued prefetch and stride prefetch. Furthermore they only support prefetch for read access from next level memory hierarchy. The A64FX supports the hardware prefetch which extended the prefetch of K computer <ref type="bibr" target="#b4">[5]</ref>, but the RIKEN simulator implements the following prefetch that is based on the prefetch of K computer with small modi cation matched to the A64FX. When a cache miss is detected, a prefetch entry is generated. When a subsequent memory access matches the prefetch entry, two consecutive prefetch requests are generated both for L1 cache and L2 cache. When the prefetch distance reaches a set value, it is switched to single prefetch. This prefetch distance is set individually for L1 cache and L2 cache. The hardware prefetch supports both read access and write access. ? Gem5 supports shared L2 cache with multiple cores, but since default L2 cache is a single bank, L2 access could be a bottleneck if the number of cores increases. Since the L2 cache is designed as a module in gem5, it is possible for users to set multiple banks of L2 cache by adding descriptions individually. The RIKEN simulator has been expanded so that the number of banks can be changed just by specifying parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? In gem5, L2 cache requests are handled by FCFS (First Come</head><p>First Serve) manner. In the A64FX, the L2 cache has a mechanism to keep fairness between requests from each core. In the RIKEN simulator, L2 cache has been extended to allow FCFS and RR (Round Robin) policies to be selected. ? In gem5, the bus width between L1 cache and L2 cache is one parameter, and it is assumed that the transfer throughput from L1 cache to L2 cache and that from L2 cache to L1 cache are the same. On the other hand, in the A64FX, these two transfer throughputs are di erent. The RIKEN simulator has been extended to specify these two bus widths as di erent parameters. The same applies to the bus width between the L2 cache and memory. ? Gem5 supports HBM1 as main memory, but HBM2 is not yet supported. The RIKEN simulator added HBM2 parameters based on the HBM1 parameters. Although standard features of gem5 could not completely match the memory interleaving method used with the A64FX, the RIKEN simulator achieved almost the same memory performance as that of the A64FX by combining this feature with other parameters, such as burst length.</p><p>We also enhanced gem5 on processor execution in addition to the memory cache hierarchy as follows.</p><p>? In gem5, the instruction group called 'OpClass' can be used to de ne the instruction latency and the execution unit to be used. Normally, OpClass is determined for each opcode eld of the instruction, but the A64FX has an instruction with di erent instruction latency, depending on the operand type (double precision or single precision). We have extended the gem5 to accommodate such instructions. ? Gem5 supports multithreaded execution on only FS mode, but the RIKEN simulator support only SE mode, so we extend the system call support on SE mode such as futex to enable multithreaded execution on SE mode.</p><p>In addition to these, the following function enhancements have been made to improve usability.</p><p>? Gem5 outputs various statistical information of the simulation to a le called stats.txt, which collects information from the start to the end of the simulation. However, since it is useful to obtain the statistical information on a speci c section, a function to achieve this has been developed. However, in order to separate such information from stats.txt, it is necessary to specify the switching time at simulator startup. Therefore, rst, pre-execution is performed in order to obtain the time of the switching timing, and then, the time is speci ed to perform the main execution. We created a python script to do that. ? The Fujitsu supercomputer system can acquire detailed prole information called PA data. We extended the statistical information in gem5 so that similar information could be acquired. In particular, we added a function to classify the cause of the wait instruction at the time of a 0 instruction commit into memory wait, arithmetic operation wait and so on. Furthermore, a python script was created to extract information from stats.txt based on Fujitsu's PA data.</p><p>? We extended the function to count element numbers of operations by an SIMD instruction. Furthermore, a function to count only the number of valid operations according to the activity of the predicate register has been added.</p><p>We are considering to release these extended functions separately from various detailed parameters involving NDA information from Fujitsu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>The RIKEN simulator was evaluated using several programs to nd out how accurate it is with the execution time of the A64FX. The evaluation target is the test chip of the A64FX prototype, and does not indicate the performance of the nal Fugaku processor. Also, the compiler used for generating the program to be executed was a prototype version of the compiler for the Fugaku from Fujitsu, which is the April 2019 version. It is the same version of the compiler used with the test chip evaluation.</p><p>We evaluated it with simple programs to make it easy to analyze the reason if there was a di erence between them. We already opened the preliminary evaluation results in <ref type="bibr" target="#b5">[6]</ref>, but following results are latest results using the latest RIKEN simulator and the latest compiler. 3072 y(i) = log10(x1(i))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation of the kernel program on a single core</head><p>First, we compared the execution times of various kernel programs on a single core with that of the test chip. An example of a kernel program used for evaluation is shown in Listing 1, and a list of kernel programs used for evaluation of double precision oating point number is shown in Table <ref type="table" target="#tab_3">1</ref>. There are four types of kernels: basic arithmetic functions, type conversions, numerical functions, and mathematical functions. The basic arithmetic functions include seven arithmetic operations: addition, subtraction, multiplication, product-sum, division, reciprocal and square root. The type conversions include seven conversions: conversion from double precision to single precision and 32-bit integer and its inverse conversion, and conversion from the double precision of 'aint', 'nint' and 'anint' that are built-in functions of Fujitsu Fortran. The numerical functions include four functions of absolute value, maximum, minimum and sign. The mathematical functions include six functions of 'cos', 'sin', 'exp', 'exp10', 'log' and 'log10'. Evaluation was performed on a total of 24 kernels for double precision.</p><p>We also evaluated for single precision oating point number on almost same type of kernels. There are 22 kernels for single precision because of elimination of duplication in double precision kernels such as double from/to single conversions. The column labeled 'Size' in Table <ref type="table" target="#tab_3">1</ref> represents the size of each array referenced in the kernel part, and the size referring to 3/4 of the L1 cache in total is selected. It corresponds to the number 'n' of iterations of the innermost loop. The number of iterations of the outer loop, 'iter', is 1,000,000 for the test chip, but the execution speed of the RIKEN simulator is about 10,000 times slower than that of the actual machine, so the value of the outer loop is 1000 times of 1/1000 for the RIKEN simulator. The timer accuracy of the RIKEN simulator is several cycles, i.e. nanosecond order, and even with a small number of iterations, it enables measurement of time with su cient accuracy.</p><p>These kernel programs were compiled with the '-Kfast' option. We compared the execution time on the RIKEN simulator with that on the A64FX test chip. In the Fujitsu compiler, all of these kernels are executed by 8 SIMD for double precision and 16 SIMD for single precision, mathematical functions are inlined, and optimization by software pipeline is applied. Divisions and reciprocals are also calculated using reciprocal instructions that are pipelined rather than using non-pipelined division instructions.</p><p>The evaluation results are shown in Figure <ref type="figure">2</ref> for double precision and in Figure <ref type="figure">3</ref> for single precision. The bar graph represents the inverse of operation throughput (the number of cycles required for SIMD-length operation) evaluated by the RIKEN simulator, and corresponds to the left vertical axis. The orange point is the ratio of the di erence in execution time between the test chip result and the RIKEN simulator result, and corresponds to the right vertical axis. An execution time di erence of 10% indicates that the execution time of the RIKEN simulator is 10% longer than that of the test chip, and -10% indicates that the execution time of the test chip is 10% longer than that of the RIKEN simulator.</p><p>For example, the result of add in double precision is 2.4 cycles per 8SIMD and the di erence is 11.6%. The inner-most loop of add kernel requires 2 load and 1 store, so it takes at least 2 cycles. The add kernel is a L1 cache bandwidth bound program, and the e ciency of add kernel is 83% in the RIKEN simulator, while that in test chip is 92%.</p><p>In the evaluation of 24 kernels with double precision, 10 kernels had an execution time di erence of more than 10% compared to that of A64FX test chip, but 6 kernels of them is less than 13%.</p><p>Looking at the add kernel, the di erence is more than 10% for double precision kernels, but less than 10% for single precision kernels. This is because the A64FX has some special tuning for double precision, but the RIKEN simulator has no such tuning. 4 kernels ('rev', 'd2f', 'd2i' and 'exp') had an execution time di erence exceeding 13%, and it was con rmed that the execution time di erence was 13% or less with 83% kernels. The average of 24 execution time di erences is 3.2%, the standard deviation is 9.7%, and the average of absolute values is 8.3%.</p><p>In the evaluation of 22 kernels with single precision, 3 kernels ('sqrt', 'exp' and 'exp10') had an execution time di erence exceeding 13%, and it was con rmed that the execution time di erence was 13% or less with 86% kernels. The average of 22 execution time di erences is -5.0%, the standard deviation is 6.9%, and the average of absolute values is 6.6%.</p><p>In the evaluation of the total 46 kernel benchmarks, it was conrmed that the di erence is 13% or less for 85% of the kernels.</p><p>Details of the reason of the execution time di erences are under consideration. In d2f and d2i, the e ect of the write data in the write bu er can be expected in the A64FX, whereas the RIKEN simulator has no write merge function. Similarly, di erences in gather load implementation can also cause di erences in execution time. In the A64FX, the combined gather load allows up to 2 elements to be accessed in one cycle if the data location meets the condition, while the RIKEN simulator always takes 1 cycle per element. As a result, execution time di erences of up 2 times may occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation of L2 cache and memory performance by multithreading</head><p>Next, in order to evaluate the L2 cache and memory performance multithreaded execution, the performance of Stream Triad for two data sizes was compared by changing the number of threads.</p><p>The rst evaluation is for L2 cache throughput, using Stream Triad for the size within the L2 cache. The results are shown in Figure <ref type="figure">4</ref>. The left graph is results using hardware prefetch, and the right graph is results using software prefetch. To enable software prefetch, the following compiler options are needed. They mean that each loop creates a software prefetch request for 8 line ahead only for L1 cache.</p><p>-Kprefetch_sequential=soft -Kprefetch_cache_level=1 -Kprefetch_line=8</p><p>The bar graph shows the total L2 cache throughput in the RIKEN simulator when the number of threads is changed from 1 to 12 for the same data size. The orange dots show the percentage di erence in execution time between the RIKEN simulator and the A64FX test chip. The horizontal axis corresponds to the number of threads, the left vertical axis corresponds to the throughput, and the right vertical axis corresponds to the percentage di erence.</p><p>Using hardware prefetch, although the result is relatively scalable with the increase in the number of threads, some of the di erence are over 10%, and the variation is large. The A64FX results are more scalable up to 12 threads. When the number of thread is small, the RIKEN simulator is faster than the A64FX. This may be because the L1 hardware prefetch requests the L2 cache without checking the L1 cache in the RIKEN simulator, and the load on the L1 cache may be smaller. On the other hand, there is another problem on implementation of hardware prefetch for L2 cache in the RIKEN simulator. In the RIKEN simulator, the occupancy rate of crossbar between L1 cache and L2 cache is very high due to hardware prefetching to L2 cache, so we are continuing to develop to reduce it.</p><p>Using software prefetch, although the result is scalable with the increase in the number of threads, the throughput is saturated around 10 threads in the RIKEN simulator. On the other hand, the A64FX shows scalable performance up to 12 threads. As a result, the execution time di erence for 12 threads is slightly larger at 8%, but otherwise it is 2% or less.</p><p>The second evaluation is for memory throughput, using Stream Triad for thirty times the size of L2 cache. The result is shown in Figure <ref type="figure" target="#fig_1">5</ref>. The gure legends and axes are the same as in Figure <ref type="figure">4</ref>. The left graph is the results using hardware prefetch and right graph is the results using ZFILL optimization (detailed below) and software prefetch.</p><p>Using hardware prefetch, the memory throughput is saturated in 4 threads and it is slightly decreased when the number of threads are increased in the RIKEN simulator. In the A64FX, the throughput is saturated in 4 threads but the throughput is kept in large number of threads. Therefor the ratio of the di erences is larger after 4 threads. We think that the di erences are caused mainly by overhead of prefetch to L2 cache same as L2 size evaluation with hardware prefetch.</p><p>While the memory throughput in one CMG is 256 GB/s, the maximum throughput using hardware prefetch is only 160 GB/s. This is because the throughput in this result is the application throughput. In the Stream triad, since innermost loop includes 2 load and 1 store of double precision oating point data, it is calculated as memory access of 24 bytes per loop. On the other hand, a store instruction needs to read a cache line from memory before writing data in order to maintain cache line consistency. Thus, the actual memory access is 3 load and 1 store. Therefore, when achieving 160 GB/s in the Stream throughput, the memory throughput is more than 210 GB/s, which is over 80% of peak throughput.</p><p>However, when writing to all cache lines as in the Stream benchmark, it is not necessary to pre-load from memory. Therefore, the Fujitsu compiler provides the ZFILL option. ZFILL uses a 'DC ZVA' Armv8 instruction to zero-ll a cache line without memory access. The right graph in Figure <ref type="figure" target="#fig_1">5</ref> is results using ZFILL optimization and software prefetch. For the optimization, the following compiler options are needed. They mean that each loop creates a 'DC ZVA' for 18 line ahead, L1 cache software prefetch for 9 line ahead and L2 cache software prefetch for 40 line ahead.</p><p>-Kzfill=18 -Kprefetch_sequential=soft -Kprefetch_line=9 -Kprefetch_line_L2=40</p><p>Using ZFILL optimization, the performance is almost saturated with 6 threads in the RIKEN simulator. In the A64FX, the performance is saturated with 4 threads. Therefore, a di erence of slightly over 10% was con rmed when the number of threads was small. Although this performance di erence is under investigation, there is a high possibility of overhead of software prefetch to L2 cache. It was con rmed that the throughput exceeded 200 GB/s and exceeded 80% of the peak performance.  Compared to the previous performance evaluation in <ref type="bibr" target="#b5">[6]</ref>, the accuracy of the cache and memory performance has been improved by implementing fully one-port L1 cache, implementing software and hardware prefetch targeting L2 cache, and implementing hardware prefetch for store data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation of simulation speed</head><p>The RIKEN simulator is a software cycle accurate simulator, so it is very slow. We evaluated the simulation speed.</p><p>We evaluated simulation speed for single thread execution of 46 kernel benchmarks in section 5.1. The average simulation speed rate indicating how many times the simulator is slower than the actual machine was 20,293. The minimum ratio was 10,294, the maximum ratio was 33,931, and the standard deviation was 5,063.</p><p>We also evaluated simulation speed for multithread execution of stream benchmarks. The results are shown in Figure <ref type="figure" target="#fig_3">6</ref>. Since the RIKEN simulator itself is a single thread program, when the number of thread increases, the simulation becomes slower even if the thread execution has good scalability. For example, in the stream benchmark on L2 cache with hardware prefetch that is speci ed by L2-hwp in the gure, simulation speed ratio for 10 threads was about 250,000 while that for 1 thread was about 20,000. In other words, 10 thread execution reduced simulated time to  11% of 1 thread execution, but simulation time increased 37%, so the simulation speed rate increased 12 times. Between hardware prefetch and software prefetch, there was no large di erence. For the stream benchmark for memory, the increase of simulation speed ratio was moderated. This is because it was slower than that for L2 cache due to small memory bandwidth and long memory latency, but simulator itself was not so busy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>RIKEN has developed a Fugaku processor simulator based on the general-purpose processor simulator gem5. The simulator can estimate the accurate number of execution cycles by simulating out-oforder execution based on the basic architecture model with detailed parameter tuning and function extension. We aimed to estimate the execution cycles of applications on a single CMG of the A64FX with such an accuracy that enables relative evaluations and tuning of applications. In order to estimate the accurate execution time of a program, it is necessary to simulate with accuracy not only the instruction execution time but also the access time of the cache memory hierarchy. Therefore, in the RIKEN simulator, we expanded gem5 to match the performance of the cache memory hierarchy to that of the A64FX.</p><p>The accuracy of this simulator was con rmed by comparing it with the number of execution cycles of the A64FX test chip. For the 24 kernels for double precision and 22 kernels for single precision, total 46 kernels, it was con rmed that the di erence is 13% or less for 85% of the kernel programs. In Stream Triad multithreaded execution, scalable performance according to the number of threads was con rmed, and achieved over 80% of memory throughput with enough accuracy. However, it was con rmed that there is still room for improvement in prefetch to L2 cache.</p><p>Increasing the accuracy of the simulator will not only increase the accuracy of the pre-evaluation of application performance, but also increase the accuracy of the performance of the base of architecture evaluation, which is the original purpose of gem5. To that goal, we will continue to improve the accuracy of RIKEN simulator.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Execution throughput of kernel benchmark for double in the RIKEN simulator and the execution time di erence ratio of the test chip</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Memory throughput of Stream Triad in the RIKEN simulator and the di erence in execution time with the test chip</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>L2-swp Mem-hwp Mem-zfill</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Simulation speed ratio on stream benchmark</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>?</head><label></label><figDesc>It supports multiple CPU models. The Atomic model simulates a CPU with 1 cycle execution of one instruction. The Timing model simulates a CPU with the timing of a memory reference added to the Atomic model. The In-order model simulates a CPU with an in-order pipeline. The Out-of-order (O3) model simulates a CPU with an out-of-order pipeline. In the RIKEN simulator, only the Atomic model and the O3 model are supported. ? It has two memory systems. One is the simple Classic model and the other is the Ruby model that can exibly con gure a coherent memory system. Currently, in the RIKEN simulator, the Classic model is used.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Kernel program list</figDesc><table><row><cell cols="2">Listing 1: Example source code of the kernel program (dou-</cell></row><row><cell cols="2">ble addition)</cell></row><row><cell cols="2">1 subroutine calc01_add_r8(n,iter,dist,y,x1,x2)</cell></row><row><cell>2</cell><cell>real*8 y(n), x1(n), x2(n)</cell></row><row><cell>3</cell><cell>integer n, iter, i, j, dist</cell></row><row><cell>4</cell><cell></cell></row><row><cell>5</cell><cell>do j = 1, iter</cell></row><row><cell>6</cell><cell>do i = 1, n</cell></row><row><cell>7</cell><cell>y(i) = x1(i) + x2(i)</cell></row><row><cell>8</cell><cell>end do</cell></row><row><cell>9</cell><cell>enddo</cell></row><row><cell>10</cell><cell></cell></row><row><cell cols="2">11 end subroutine calc01_add_r8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure 4: L2 cache throughput of Stream Triad in the RIKEN simulator and the di erence in execution time with the test chip</figDesc><table><row><cell></cell><cell cols="8">Stream triad on L2 : hardware prefetch</cell><cell></cell><cell></cell><cell cols="4">Stream triad on L2 : software prefetch</cell></row><row><cell></cell><cell>1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell>1024</cell><cell></cell><cell></cell><cell>40%</cell></row><row><cell>Throughput (GB/s)</cell><cell>128 256 384 512 640 768 896</cell><cell></cell><cell cols="4">RIKEN simulator</cell><cell>diff</cell><cell>-30% -20% -10% 0% 10% 20% 30%</cell><cell>ratio of exec time diff</cell><cell>Throughput (GB/s)</cell><cell>128 256 384 512 640 768 896</cell><cell cols="3">RIKEN simulator</cell><cell>diff</cell><cell>-30% -20% -10% 0% 10% 20% 30%</cell><cell>ratio of exec time diff</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell cols="2">8 10 12</cell><cell>-40%</cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8 10 12</cell><cell>-40%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3"># of threads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell># of threads</cell></row><row><cell></cell><cell cols="7">Stream triad : hardware prefetch</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Stream triad : ZFILL+software prefetch</cell></row><row><cell>Trhoughput (GB/s)</cell><cell>32 64 96 128 160 192 224 256</cell><cell></cell><cell></cell><cell cols="3">RIKEN simulator</cell><cell>diff</cell><cell>-30% -20% -10% 0% 10% 20% 30% 40%</cell><cell>ratio of exec time diff</cell><cell>Throughput (GB/s)</cell><cell>32 64 96 128 160 192 224 256</cell><cell></cell><cell></cell><cell>RIKEN simulator</cell><cell>diff</cell><cell>-30% -20% -10% 0% 10% 20% 30% 40%</cell><cell>ratio of exec time diff</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-40%</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>-40%</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell></cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10 12</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell></cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10 12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3"># of threads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell># of threads</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is partially funded by <rs type="programName">MEXT's program</rs> for the Development and Improvement for the <rs type="projectName">Next Generation Ultra High-Speed Computer System</rs>, under its Subsidies for Operating the <rs type="institution">Speci c Advanced Large Research Facilities</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_jaTrZaW">
					<orgName type="project" subtype="full">Next Generation Ultra High-Speed Computer System</orgName>
					<orgName type="program" subtype="full">MEXT&apos;s program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The gem5 Simulator</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fujitsu High Performance CPU for the Post-K Computer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips 30 Symposium</title>
		<imprint>
			<date type="published" when="2018-08">2018. Aug. 2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hiramoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inoue</surname></persName>
		</author>
		<title level="m">The Tofu Interconnect D, IEEE International Conference on Cluster Computing</title>
		<imprint>
			<date type="published" when="2018-09">Sep. 2018</date>
			<biblScope unit="page" from="646" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ARM Scalable Vector Extension</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boettcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eapen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eyole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gabrielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horsnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Magklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Premillieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="26" to="29" />
			<date type="published" when="2017-04">March/April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://www.fujitsu.com/downloads/JP/archive/imgjp/jhpc/sparc64viiifx-extensions.pdf" />
		<title level="m">SPARC64 VIIIfx Extensions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Kodama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Odajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.06451" />
		<title level="m">Evaluation of the RIKEN Post-K Processor Simulator</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
