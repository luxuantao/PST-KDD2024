<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
							<email>†nponomareva@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been a recent surge of interest in designing Graph Neural Networks (GNNs) for semi-supervised learning tasks. Unfortunately this work has assumed that the nodes labeled for use in training were selected uniformly at random (i.e. are an IID sample). However in many real world scenarios gathering labels for graph nodes is both expensive and inherently biased -so this assumption can not be met. GNNs can suffer poor generalization when this occurs, by overfitting to superfluous regularities present in the training data. In this work we present a method, Shift-Robust GNN (SR-GNN), designed to account for distributional differences between biased training data and a graph's true inference distribution. SR-GNN adapts GNN models to the presence of distributional shift between the nodes labeled for training and the rest of the dataset. We illustrate the effectiveness of SR-GNN in a variety of experiments with biased training datasets on common GNN benchmark datasets for semi-supervised learning, where we see that SR-GNN outperforms other GNN baselines in accuracy, addressing at least ∼40% of the negative effects introduced by biased training data. On the largest dataset we consider, ogb-arxiv, we observe a 2% absolute improvement over the baseline and are able to mitigate 30% of the negative effects from training data bias 1 . Specifically, our contributions are the following:</p><p>1. We provide the first focused discussion on the distributional shift problem in GNNs. 2. We propose generalized framework, Shift-Robust GNN (SR-GNN), which can address shift in both shallow and deep GNNs. 3. We create an experimental framework which allows for creating biased train/test sets for graph learning datasets. 4. We run extensive experiments and analyze the results, proving that our methods can mitigate distributional shift.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributional shift and Domain adaption work</head><p>Standard learning theory (i.e. PAC, empirical risk minimization, etc.) assumes that training and inference data is drawn from the same distribution, but there are many practical cases where this</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of graph-based semi-supervised learning (SSL) is to use relationships between data (its graph inductive bias), along with a small set of labeled items, to predict the labels for the rest of a dataset. Unsurprisingly, varying exactly which nodes are labeled can have a profound effect on the generalization capability of a SSL classifier. Any bias in the sampling process to select nodes for training can create distributional differences between the training set and the rest of the graph. During inference any portion of the graph can be used, so any uneven labeling for training data can cause training and test data to have different distributions. An SSL classifier may then overfit to training data irregularities, thus hurting the performance at inference time.</p><p>Recently, GNNs have emerged as a way to combine graph structure with deep neural networks. Surprisingly, most work on semi-supervised learning using GNNs for node classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1]</ref> have ignored this critical problem, and even the most recently proposed GNN benchmarks <ref type="bibr" target="#b11">[12]</ref> assume that an independent and identically distributed (IID) sample is possible for training labels. This problem of biased training labels can be quite pronounced when GNNs are applied for semisupervised learning in practice. It commonly happens when the size of the dataset is so large that only a subset of it can afford to be labeled -the exact situation where graph-based SSL is supposed to have a value proposition! While the specific source of bias can vary, we have encountered it in many different settings. For example, sometimes fixed heuristics are used to select a subset of data (which shares some characteristics) for labeling. Other times, human analysts individually choose data items for labeling, using complex domain knowledge. However, even this can be rooted in shared characteristics of data. In yet another scenario, a label source may have some latency, causing a temporal mismatch between the distribution of data at time of labeling and at the time of inference. In all of these cases, the core problem is that the GNN overfits to spurious regularities as the subset of labeled data could not be created in an IID manner.</p><p>One particular area where this can apply is in the spam and abuse domain, a common area of application for GNNs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33]</ref>. However, the labels in these problems usually come from explicit human annotations, which are both sparse (as human labelling is expensive), and also frequently biased. Since spam and abuse problems typically have very imbalanced label distributions (e.g. in many problems there are relatively few abusers -typically less than 1:100), labeling nodes IID results in discovering very few abusive labels. In this case choosing the points to request labels for in an IID manner is simply not a feasible option if one wants to have a reasonable number of data items from the rare class.</p><p>In this paper, we seek to quantify and address the problem of localized training data in Graph Neural Networks. We frame the problem as that of transfer learning -seeking to transfer the model's performance from a small biased portion of the graph to the entire graph itself. Our proposed framework for addressing this problem, Shift-Robust GNN (SR-GNN), strives to adapt a biased sample of labeled nodes to more closely conform to the distributional characteristics present in an IID sample of the graph. It can handle two kinds of bias that occur in both deeper GNNs and more recent linearized (shallow) versions of these models.</p><p>First we consider the case of addressing distributional shift for standard GNN models such as GCNs <ref type="bibr" target="#b14">[15]</ref>, MPNNs <ref type="bibr" target="#b6">[7]</ref>, and many more <ref type="bibr" target="#b4">[5]</ref>. These models create deep networks which iteratively convolve information over graph structure. SR-GNN addresses this variety of distributional shift via a regularization over the hidden layers of the network. Second, we consider a class of linearized models (APPNP <ref type="bibr" target="#b15">[16]</ref>, SimpleGCN <ref type="bibr" target="#b33">[34]</ref>, etc) which decouple GNNs into non-linear feature encoding and linear message passing. These models present an interesting challenge for debiasing, as the graph can introduce bias over the features after all learnable layers. In cases like this, SR-GNN can use an instance reweighting paradigm to ensure that the training examples are as representative as possible over the graph data.</p><p>We illustrate the effectiveness of our proposed method on both paradigms with an experimental framework that introduces bias to the train/test split in a GNN, which lets us simulate the 'localized discovery' pattern observed in real applications on fully-labeled academic datasets. With these experiments we show that that our method SR-GNN can recover at least 40% of the performance lost when training a GCN on the same biased input.</p><p>does not hold. The question of dealing with different distributions has been widely explored as a part of the transfer learning literature. In transfer learning, the domain adaptation problem deals with transferring knowledge from the source domain (used for learning) to the target domain (the ultimate inference distribution).</p><p>One of the first theoretical works on domain adaptation <ref type="bibr" target="#b2">[3]</ref> developed a distance function between a model's performance on the source and target domains to describe how similar they are. To obtain a final model, training then happens on a reweighed combination of source and target data, where weights are a function of the domain's distance. Much additional theoretical (e.g. <ref type="bibr" target="#b21">[22]</ref>) and practical work expanded this idea and explored models which are co-trained on both source and target data. These models seek to optimize utility while minimizing the distance between extracted features distributions on both domains; this in turn led to the field of Domain Invariant Representation Learning (DIR) <ref type="bibr" target="#b5">[6]</ref>. DIR is commonly achieved via co-training on labeled source and (unlabeled) target data. A modification to the loss either uses an adversarial head or adds additional regularizations.</p><p>More recently, various regularizations using discrepancy measures have been shown to be more stable to hyperparameters and result in better performance than adversarial heads, faster loss convergence and easier training. Maximum mean discrepancy (MMD) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> is a metric that measures difference between means of distributions in some rich Hilbert kernel space. Central moment discrepancy (CMD) <ref type="bibr" target="#b37">[38]</ref> extends this idea and matches means and higher order moments in the original space (without the projection into the kernel). CMD has been shown to produce superior results and is less susceptible to the weight with which CMD regularization is added to the loss <ref type="bibr" target="#b19">[20]</ref>.</p><p>It is important to point out that MMD and CMD regularizations are commonly used with non-linear networks on some hidden layer (e.g. on extracted features). For linear models or non-differentiable models, prior work for domain adaptation often employed importance-reweighting instead. To find the appropriate weights, the same MMD distance was often used: in kernel mean matching (KMM) to find the appropriate weights one essentially minimizes MMD distance w.r.t the instance weights <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GNNs</head><p>Given a graph G = {V, E, X}, the nodes V are associated with their features X (X ∈ R |V |×F ) and the set of edges E (i.e. adjacency matrix A, A ∈ R |V |×|V | ) which form connections between them. Graph neural networks <ref type="bibr" target="#b14">[15]</ref> are neural networks that operate on both node features and graph structures. The core assumption of GNNs is that the structure of data (A) can provide a useful inductive bias for many modeling problems. GNNs output node representations Z which are used for unsupervised <ref type="bibr" target="#b29">[30]</ref> or semi-supervised <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref> learning. We denote the labels for SSL as {y i }.</p><p>The general architecture Φ of a GNN consists of K neural network layers which encode the nodes and their neighborhood information using some learnable weights θ. More specifically the output of layer k of a GNN contains a row (h k i ) which can be used as a representation for each node i, i.e. z k i = h k i . Successive layers mix the node representations using graph information, for example:</p><formula xml:id="formula_0">H k = σ( ÃH k−1 θ k ) (1)</formula><p>where Ã is an appropriately normalized adjacency matrix<ref type="foot" target="#foot_2">2</ref> , σ is the activation function, and H 0 = X. For brevity's sake, we refer to the final latent representations Z K simply as Z throughout the work.</p><p>Although no existing work studies the distributional shift problem in GNNs, transfer learning of GNNs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref> has explored different node-level and graph-level pre-training tasks across different graphs. Alternatively, domain adaption methods have been used to optimize a domain classifier between source and target graphs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>. In addition, distribution discrepancy minimization (MMD) has been adopted to train network embedding across domains <ref type="bibr" target="#b27">[28]</ref>. Other regularizations for the latent state of GNNs have been proposed for domains like fairness <ref type="bibr" target="#b22">[23]</ref>. We are the first to notice the importance of distributional shift on the same graph in a realistic setting and analyze its influence on different kinds of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distributional shift in GNNs</head><p>To learn an SSL classifier, a cross-entropy loss function l is commonly used,</p><formula xml:id="formula_1">L = 1 M M i=1 l(y i , z i ),<label>(2)</label></formula><p>where z i is the node representation for the node i learned from a graph neural network, and M is the number of training examples. When the training and testing data come from the same domain (i.e. Pr train (X, Y ) = Pr test (X, Y )), optimizing the cross entropy loss over the training data ensures that a classifier is well-calibrated for performing inference on the testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data shift as representation shift</head><p>However, a mismatch between the training and testing distributions (i.e. Pr train (X, Y ) = Pr test (X, Y )) is a common challenge for machine learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>In To measure such a shift, discrepancy metrics such as MMD <ref type="bibr" target="#b7">[8]</ref> or CMD <ref type="bibr" target="#b36">[37]</ref> can be used. CMD measures the direct distance between distributions p and q as the following <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_2">CMD = 1 |b − a| E (p) − E (q) 2 + ∞ k=2 1 |b − a| k c k (p) − c k (q) 2 ,<label>(3)</label></formula><p>where c k is k-th order moment and a, b denotes the joint distribution support of the distributions. In practice, only a limited number of moments is usually included (e.g. k=5). In this work we focus on the use of CMD <ref type="bibr" target="#b36">[37]</ref> as a distance metric to measure distributions discrepancy for efficiency. We note that GNNs (Eq (1)) are different from traditional neural networks, where the output for a layer K is defined as Formally, we start the analysis of distributional shift as follows. Definition 3.1 (Distribution shift in GNNs). Assume node representations Z = {z 1 , z 2 , . . . , z n } are given as an output of the last hidden layer of a graph neural network on graph G with n nodes. Given labeled data {(x i , y i )} of size M, the labeled node representation Z l = (z 1 , . . . , z m ) is a subset of the nodes that are labeled, Z l ⊂ Z. Assume Z and Z l are drawn from two probability distributions p and q. The distribution shift in GNNs is then measured via a distance metric d(Z, Z l ).</p><formula xml:id="formula_3">H k = σ(H k−1 θ k ). Instead,</formula><p>Interestingly, it can be empirically shown that the effects of distribution shift due to sample bias directly lower the performance of models. To illustrate this, we plot the distribution shift distance values (x-axis) and corresponding model accuracy (y-axis) for three common GNN benchmarks using the classic GCN model <ref type="bibr" target="#b14">[15]</ref> in Figure <ref type="figure" target="#fig_1">1</ref>. The results demonstrate that the performance of GNNs for</p><formula xml:id="formula_4">X F ! (Θ, %, &amp;) A F " (Θ, ( "#! , &amp;) A $ ! $" head A F ! (&amp;) F % (F ! , Θ, %) X $" head (e.g., F 1 = PPR) Traditional GNN Linearized GNN X F ! (Θ, %, &amp;) A F " (Θ, ( "#! , &amp;) A $ ! $" head SR-GNN (Deep)</formula><p>) *(+, ( " ) *(+, ( " ) *(+, ( " )</p><formula xml:id="formula_5">+ . ⋅ 0 # 1(* $ , * %%&amp; ) A F ! (&amp;) F % (F ! , Θ, %) X $"</formula><p>head SR-GNN (Linearized)</p><p>) *(+, ( " ) (e.g., F 1 = PPR) node classification on these datasets is inversely related to the magnitude of distributional shift and motivates our investigation into distribution shift.</p><note type="other">Feedforward Backpropagation</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Shift-Robust Graph Neural Networks</head><p>In this section, we will address the distributional shift problem (Pr train (Z) = Pr test (Z)) in GNNs by proposing ways to mitigate the shift for two different GNN models (Section 4.1 and 4.2, respectively). Subsequently, we introduce SR-GNN (Fig. <ref type="figure" target="#fig_3">2</ref>) as a general framework that reduces distributional shifts for both differentiable and non-differentiable (e.g. graph inductive bias) sources simultaneously in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scenario 1: Traditional GNN models</head><p>We begin by considering a traditional GNN model Φ, a learnable function F with parameters Θ, over some adjacency matrix A: Φ = F(Θ, Z, A).</p><p>(4) In the original GCN <ref type="bibr" target="#b14">[15]</ref>, the graph inductive bias is multiplicative at each layer and gradients are back propagated through all of the layers. In the last activated hidden layers, we denote a bounded node representation <ref type="foot" target="#foot_3">3</ref> </p><formula xml:id="formula_6">as Z ≡ Z k = Φ(Θ, Z k−1 , A), Z k ∈ [a, b] n , Z 0 = X.</formula><p>Let us denote the training samples as {x i } M i=1 , the node representations are Z train = {z i } M i=1 . For the test samples, we sample an unbiased IID sample from unlabeled data X IID = {x i } M i=1 and denote the output representations as Z IID = {z i } M i=1 . In order to mitigate the distributional shift between training and testing, we propose a regularizer</p><formula xml:id="formula_7">d : [a, b] n × [a, b] n → R + that</formula><p>is added to the cross entropy loss. Since Φ is fully differentiable, we can use a distributional shift metric as a regularization to directly minimize the discrepancy between a biased and unbiased IID sample like so:</p><formula xml:id="formula_8">L = 1 M i l(y i , z i ) + λ • d(Z train , Z IID ).<label>(5)</label></formula><p>Here we consider the central moment discrepancy regularizer, d CMD :</p><formula xml:id="formula_9">d CMD (Z train , Z IID ) = 1 b − a E(Z train ) − E(Z IID ) + ∞ k=2 1 |b − a| k c k (Z train ) − c k (Z IID ) , (6)</formula><p>where</p><formula xml:id="formula_10">E(Z) = 1 M z i and c k (Z) = E(Z − E(Z)</formula><p>) k is the k-th order moment. In practice, we use moments up to the 5th order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scenario 2: Linearized GNN Models</head><p>Another family of recently proposed models for GNNs uses two distinct different functions -one for non-linear feature transformation, and another for a linear graph spreading stage,</p><formula xml:id="formula_11">Φ = F 2 ( F 1 (A) linear function , Θ, X).<label>(7)</label></formula><p>In such a linearized GNN model, the graph inductive bias is combined with node features by a linear function F 1 , which is decoupled from multi-layer neural network feature encoder F 2 . SimpleGCN <ref type="bibr" target="#b33">[34]</ref> is an example of linearized model when F 1 (A) = A k X. Another branch of linearized models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref> employs personalized pagerank to pre-compute the information diffusion in a graph (i.e.</p><formula xml:id="formula_12">F 1 (A) = α(I − (1 − α) Ã) −1</formula><p>) and apply it on encoded node features F (Θ, X).</p><p>In both models, the graph inductive bias is provided as an input feature to a linear F 1 . Unfortunately, as there are no learnable layers at this stage in these models, one can not simply apply the distributional regularizer proposed in the previous section. In this case, we can view training and testing samples as row-wise samples h i from F 1 (A). The problem of distribution shift Pr train (Z) = Pr test (Z) can then be transformed into matching the training and testing graph inductive bias feature space h i ∈ R n (where n is the number of the nodes in the graph). Then to generalize from training data to testing, we can adopt an instance reweighting scheme to correct the bias, such that biased training sample {h i } M i=1 will be similar to an IID sample {h i } M i=1 . The resulting cross entropy loss is then</p><formula xml:id="formula_13">L = 1 M β i l(y i , Φ(h i )),<label>(8)</label></formula><p>where β i be the weight for each training instance, and l is the cross-entropy loss. We can then compute the optimal β via kernel mean matching (KMM) <ref type="bibr" target="#b8">[9]</ref> by solving a quadratic problem,</p><formula xml:id="formula_14">min βi 1 M M i=1 β i ψ(h i ) − 1 M M i=1 ψ(h i ) 2 , s.t. B l ≤ β &lt; B u<label>(9)</label></formula><p>It tries to match the mean elements in a kernel space k(•, •) on the domain R n × R n . Specifically, ψ : R n → H denotes the feature map to the reproducing kernel Hilbert space(RKHS) introduced by kernel k. In our experiment, we use a mixture of gaussian kernel k(x, y) = αi exp(α i x − y 2 ), α i = 1, 0.1, 0.01. The lower B l and upper bound B u constraints are there to make sure that most of the instances get some reasonable weight, as opposed to only a few instances getting non zero weight. In practice, we have multiple classes in the label space. To prevent label imbalance introduced by β, we further require that the sum of β for a specific class c remains the same before and after the correction,</p><formula xml:id="formula_15">M i β i • I(l i = c) = M i I(l i = c), ∀c.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Shift-Robust GNN Framework</head><p>Now we propose Shift-Robust GNN (SR-GNN) -our general training objective for addressing distributional shift in GNNs:</p><formula xml:id="formula_16">L SR-GNN = 1 M β i l(y i , Φ(x i , A)) + λ • d(Z train , Z IID ).<label>(10)</label></formula><p>The framework consists of both a regularization for addressing distributional shift in learnable layers (Section 4.1) and an instance reweighting component which is capable of handling situations where a graph inductive bias is added after feature encoding (Section 4.2).</p><p>We will now discuss a concrete instance of our framework, by applying it to the APPNP <ref type="bibr" target="#b15">[16]</ref> model. The APPNP model is defined as:</p><formula xml:id="formula_17">Φ APPNP = (1 − α) k Ãk + α k−1 i=0 (1 − α) i Ãi approximated personalized page rank F(Θ, X).</formula><p>feature encoder <ref type="bibr" target="#b10">(11)</ref> It first applies a feature encoder F on node features X and approximated personalized pagerank matrix linearly. Thereby, we have h i = π ppr i , where π ppr i is the personalized pagerank vector. For this, we mitigate distributional shifts from graph inductive bias via instance weighting. Moreover, let Z = F(Θ, X) and we can further reduce the distributional shifts from non-linear networks by the proposed discrepancy regularizer d. In our experiments, we show the application of SR-GNN on two other representative GNN models: GCN <ref type="bibr" target="#b14">[15]</ref> and DGI <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we first describe how we create training set with a controllable amount of bias, then discuss our experiment design, demonstrate the efficacy our proposed framework for handling bias as well as its advantages over domain adaptation baselines, and finally, present a study on sensitivity to the hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Biased Training Set Creation</head><p>In order to study distribution shift in GNNs, we require a repeatable process which can generate graphbiased training sets. The core aspect of creating a biased sample for graph learning tasks requires an efficient method for finding 'nearby' nodes in the graph for a particular seed node. In this work, we use the Personalized PageRank (PPR) vectors to find such nearby nodes, Π ppr = (I − (1 − α) Ã) −1 . PPR vectors are well suited for this case for a number of reasons. First, several previous studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16]</ref> have shown strong correlations between the information diffusion in GNNs and PPR vectors. Second, a PPR vector can be computed for an individual node in time sublinear to the size of the graph <ref type="bibr" target="#b1">[2]</ref> -so biased training samples can be easily generated, even for large datasets. This local algorithm provides a sparse approximation Π ppr ( ) with guaranteed truncation error , such that we can efficiently compute the top-γ entries of a ppr vector with controllable residuals. Therefore, using PPR we can generate stable localized training data that can effectively challenge GNN models.</p><p>We obtain a biased sample from our scalable personalized pagerank sampler (PPR-S) as follows. For a certain label ratio τ , we compute the number of training nodes needed per label in advance. Then we repeatedly randomly select nodes that have enough neighbors in their sparse personalized pagerank vector π ppr i ( ). We add both the seed nodes and their neighbors with the same label into the training data until we have enough number of nodes for each label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental settings</head><p>Datasets. In our experiments, we perform semi-supervised node classification tasks on five popular benchmark datasets: Cora, Citeseer, Pubmed <ref type="bibr" target="#b26">[27]</ref>, ogb-arxiv <ref type="bibr" target="#b25">[26]</ref> and Reddit <ref type="bibr" target="#b10">[11]</ref>. We use the same validation and test splits as in the original GCN paper <ref type="bibr" target="#b14">[15]</ref> and OGB benchmark. We use the remaining nodes for training. For the unbiased baseline's performance numbers, we use a random sample from this training data. Similarly, for a biased training sample, we apply our biased sampler PPR-S on the training nodes to obtain a biased training sample and report its performance. The dataset statistics can be found in Appendix A.1.</p><p>Baselines. Following the two scenarios outlined in Section 4, we consider the following methods to investigate their performance under distributional shifts: (1) Traditional GNN Models: GCN <ref type="bibr" target="#b14">[15]</ref>, GAT <ref type="bibr" target="#b30">[31]</ref>, (2) Linearized GNNs: SGC <ref type="bibr" target="#b33">[34]</ref> and APPNP <ref type="bibr" target="#b15">[16]</ref>. We also include methods based on unsupervised node representation learning (DeepWalk <ref type="bibr" target="#b23">[24]</ref> and DGI <ref type="bibr" target="#b31">[32]</ref>) as a third category (3). For these methods, we use a linear classifier learned on top of pretrained node embeddings.</p><p>Scalable biased sampler. Our scalable biased sampler uses Personalized PageRank to efficiently create biased training samples in large graphs. Details are omitted for brevity here, but a full description can be found in Appendix A.2.</p><p>Our Method. If not otherwise specified, we consider the APPNP <ref type="bibr" target="#b15">[16]</ref> instance of Shift-Robust as our base model, and also provide two ablations of it. These ablations independently use the shift-robust techniques introduced in Section 4.1 and 4.2 to validate the effectiveness of SR-GNN.</p><p>Hyperparameters. The main hyper parameters in our sampler PPR-S are α = 0.1,γ = 100. When the graph is large, we set = 0.001 in the local algorithm for sparse PPR approximation. In SR-GNN, λ = 1.0 is the penalty parameter for the discrepancy regularizer d, the lower bound for the instance weight B l is 0.2. For all of the GNN methods except DGI, we set the hidden dimension as 32 for Cora, Citeseer, Pubmed and 256 for ogb-arxiv, with a dropout of 0.5. In order to learn effective representations, DGI <ref type="bibr" target="#b31">[32]</ref> usually needs a higher dimensional hidden space and so, following the DGI paper we set it as 512 across all of our experiments. We use Adam <ref type="bibr" target="#b13">[14]</ref> as an optimizer, and set the learning rate to 0.01 and L 2 regularization to 5e-4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalability</head><formula xml:id="formula_18">h i ∈ R n . The total complexity of SR-GNN is therefore O(M Φ + M 2 n).</formula><p>Our experiments were run on a single machine with 8 CPU and 1 Nvidia T4 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment results</head><p>We first show the performance comparison of SR-GNN (ours) and other baselines on three wellknown citation GNN benchmarks in Table <ref type="table">6</ref>. We report the Micro-F1, and Macro-F1 for each method.</p><p>We compare each method trained on a biased sample to a GCN trained on an unbiased sample, and report its performance drop in Micro-F1 (∆F1). We begin by noting that when the training sample is biased, every method suffers a substantial performance drop (as indicated by the column ∆F1). However, SR-GNN consistently reduces the influence of distributional shift and decreases the performance drop (∆F1) relative to a GCN model trained on biased input by at least 40%. We note that on these three datasets, the largest decrease in performance occurs on PubMed, where all of the existing methods experience more than a 10% absolute drop in their performance due to the biased samples. However we note that in this challenging case, the improvements of SR-GNN against APPNP (base model) also grow when the shift is larger. Finally, we see from the ablation models that the combination of both the regularization and instance reweighting appears to work better than either bias correction on its own. Our results demonstrate that our shift-robust framework is effective at minimizing the effects of distributional shift in GNN training data.</p><p>On two large benchmarks in Table <ref type="table" target="#tab_2">2</ref> we see that the performance loss from biased sample is smaller but still significant. Even in a dense network like reddit, the localized training data still affect the GNN model performance. Compared with baselines, SR-GNN can effectively mitigate the 30% of the negative effect (∆) relative to an unbiased GCN. When more training data is provided (5%) we can further minimize this performance gap.</p><p>Finally, we also study how our Shift-Robust framework can be applied to two other representative models -GCNs <ref type="bibr" target="#b14">[15]</ref>, and DGI <ref type="bibr" target="#b31">[32]</ref>. For the GCN model, we apply the regularized loss from the Equation (4) to the final node representations. For DGI, the embeddings are first trained via unsupervised learning over the entire graph. In this case, as we are optimizing a simple logistic regression over the DGI representations we can use only the instance reweighting regularization from Equation <ref type="bibr" target="#b8">(9)</ref>. Table <ref type="table" target="#tab_3">3</ref> confirms that the the Shift-Robust framework successfully improves task performance in the face of biased training labels, and that it can be easily applied to a variety of different GNN models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with other domain invariant learning methods</head><p>In SR-GNN, we utilize the unlabeled data Z IID sampled from the unifying set of training and test dataset. In domain invariant learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>, unlabeled data from target domain is also used to regularize latent space between source and target domain. DANN <ref type="bibr" target="#b5">[6]</ref> is a method that uses an adversarial domain classifier to encourage similar feature distributions between different domains. In our case, the domains are a biased training data (source) and IID unlabeled data (target). We compare the CMD regularizer and DANN regularizer using the same GNN architectures (i.e. GCN <ref type="bibr" target="#b14">[15]</ref>, APPNP <ref type="bibr" target="#b15">[16]</ref>). The hyper parameter λ is used to weight the domain loss and the CMD regularization respectively. We tune this hyper parameter on the validation set and report models' performance under the best λ (1 for Cora and Citeseer, 0.1 for PubMed). In Table <ref type="table" target="#tab_4">4</ref>, as discussed in the related work, the discrepancy measures generally perform better than adversarial domain invariant learning (and CMD has been shown to be less sensitive to the regularizer weight). Especially under semi-supervised setting, the performance of DANN is more sensitive to the domain loss. Notice that CMD (Ours) is the ablation (SR-GNN w.o. IR) in the main paper -we note that we have already shown its performance can be further boosted using instance reweighting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Parameter sensitivity of SR-GNN</head><p>In this section we study how varying some of our hyper-parameters affects the performance. We perform 20 runs for each parameter and fix the initialization for all the models per run. Studies on SR-GNN with more layers (deeper models) and different biased samples are in Appendix A.3.</p><p>Performance with different α in PPR-S. Previously, we set α = 0.1 in the biased sampler PPR-S.</p><p>In Figure <ref type="figure" target="#fig_5">3</ref>  Performance with different B l , λ in SR-GNN. In our framework, there are three major hyper parameters: the regularization strength λ, the number of moments used in CMD k, and the bounds of instance weights: B l , B u . In Figure <ref type="figure">4a</ref>, the performance of SR-GNN with different k and λ is reported. Even when only one moment is used for the CMD estimation, the method can still perform well with a reasonable penalty λ. We study different choices of B l (for a constant B u ) in Figure <ref type="figure">4b</ref>.</p><p>The result shows a smaller B l is better for training since larger values limit the expressive range. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we were the first to demonstrate that unbiased training data is very important for performance of GNNs. We argued that biased training data is extremely common in real world scenarios and can arise due to a variety of reasons including: difficulties of labelling large amount of data, the various heuristics or inconsistent techniques that are used to choose nodes for labelling, delayed label assignment, and other constraints from real world problems. We presented a general framework (SR-GNN) that is able to reduce influence of biased training data and can be applied to various types of GNNs, including both deeper GNNs and more recent linearized (shallow) versions of these models. With a number of experiments, we demonstrated both GNNs susceptibility to biased data and the success of our method in mitigating performance drops due to this bias: our method outperforms other GNN baselines on biased data and eliminates between (30 − 50%) of the negative effects introduced by training a GCN on biased training data.</p><p>However, there is still much to do. For instance, while we have considered the general problem of distributional shift in GNNs, there is much specific work that can (and should) be done in specific domains! Future work in this area should include regularizations to maximize performance for particular kinds of distribution shift (e.g. in spam &amp; abuse detection) and to ensure constraints (e.g. fairness) in the presence of imbalanced training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix A.1 Data Statistics</head><p>Nodes in the graph are anonymized and do not contain any personally identifiable data.   In the above example of our scalable personalized pagerank sampler (PPR-S), we set = 0.005 and compute the top-γ approximated personalized page rank vector π ppr i ( ) for a randomly selected node i. If a seed node i has enough PPR-neighbors (non-zero entries), we add the top-γ PPR neighbors into the training set (γ=20). In Figures <ref type="figure" target="#fig_8">5a and 5b</ref>, we visualize a sub-network of a specific class in Cora <ref type="bibr" target="#b26">[27]</ref>; training samples are colored orange. Specifically, we visualize the nodes in biased sample and their PPR-score w.r.t. the seed node in color red in Figure <ref type="figure" target="#fig_8">5c</ref>. The algorithm for biased training set creation is described in Algorithm 1. In Section 3.1 of the main paper, we highlighted that the graph inductive bias can amplify the 'normal' shift of non-IID samples. Therefore, in a deeper GNN, the negative influence of biased training data is expected to be larger. We present the accuracy (Micro-F1) and distribution shifts (CMD) of a deeper GCN <ref type="bibr" target="#b14">[15]</ref> model and our Shift-Robust GCN using regularization proposed in Section 4.1 of the main paper. In Figure <ref type="figure" target="#fig_11">6</ref>, the distribution shift dramatically increases as the depth of the model grows. On Cora and Citeseer, SR-GCN can effectively improve the performance regardless of the depth of the model. On PubMed, we observe that the performance of GCN increases when there are fewer than 4 hidden layers. In most cases, SR-GCN still outperforms its base model with a smaller variance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Performance of SR-GNN in wider models</head><p>Besides the depth of the model, increasing the width of the model is another way to increase the complexity and capacity of the model. We vary the number of heads in GAT <ref type="bibr" target="#b30">[31]</ref> while keeping the number of the GAT layers (i.e. 2) and hidden dimension of each attention head fixed (i.e. 32) and report back the performance of GAT and SR-GAT in Figure <ref type="figure" target="#fig_13">7</ref>. In general, more attention heads lead to better performance and smaller distribution shifts (see lower figures). SR-GNN provides robust improvements across various number of the attention heads.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>this paper, we care about the distributional shift between training and test datasets present in Z, the output of the last activated hidden layer. Given that the foundation of standard learning theory assumes Pr train (Y |Z) = Pr test (Y |Z), the main cause of distribution shift is the representation shift, i.e. Pr train (Z, Y ) = Pr test (Z, Y ) → Pr train (Z) = Pr test (Z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution shift lowers performance on GNN datasets. For each dataset, we show the performance (F1:y-axis) vs their distribution shift (CMD:x-axis) for 100 biased training set samples .</figDesc><graphic url="image-2.png" coords="4,243.88,382.01,126.72,94.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the multiplication of the normalized adjacency matrix (H k = σ( ÃH k−1 θ k )) essentially changes the output distribution of the hidden representation via the graph's inductive bias. Hence, in a semi-supervised GNN, a biased training sample can lead to large representation shift due to both the graph's inductive bias in addition to 'normal' shift between non-IID sampled feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A comparison between a traditional GNN, a linearized GNN and our framework (SR-GNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, we vary α between [0.05, 0.3] to evaluate the performance of APPNP and SR-GNN as the bias of the sample changes. While the absolute accuracy varies with different α, SR-GNN consistently beats its base model by a clear margin. The different patterns across three datasets are expected, since as α grows, the PPR-neighbors can have different topological properties and structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Varying α of biased sampler on three benchmarks.</figDesc><graphic url="image-4.png" coords="10,112.18,136.80,126.72,94.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )Figure 4 :</head><label>a4</label><figDesc>Figure 4: Parameter sensitivity of SR-GNN.</figDesc><graphic url="image-7.png" coords="10,132.69,351.96,136.86,101.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) IID sample (b) Biased sample (c) PPR-score on biased sample</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A biased sample on Cora dataset for one class, orange indicates the training data, red indicates the initial seed used in our PPR-S sampler. The PPR-score is presented in figure (c).</figDesc><graphic url="image-9.png" coords="14,112.18,254.12,126.72,95.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 1 : 5 if π ppr i ( ) has γ non-zero entries then 6 X</head><label>156</label><figDesc>Biased Training Set Creation PPR-S(γ, , α) 1 Given a class c, label ratio τ , graph size N; 2 Initialize the biased training set X = {} ; 3 while len(X)&lt; N • τ do 4 Sample node i of class c, compute its top-γ entries in π ppr i ( ) via [2]; -GNN with increasing model complexity A.3.1 Performance of SR-GNN in deeper models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Cora (b) Citeseer (c) Pubmed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of GCN vs. SR-GCN model performance with the the same parameters. Our shift-robust algorithm boosts the performance (top) consistently by reducing the distribution shifts (bottom).</figDesc><graphic url="image-15.png" coords="15,111.62,201.53,126.72,99.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Cora (b) Citeseer (c) Pubmed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of GAT vs. SR-GAT model performance under increasing attention heads. Our shiftrobust algorithm boosts the performance (upper) consistently by reducing the distribution shifts (lower).</figDesc><graphic url="image-21.png" coords="15,111.62,573.44,126.72,93.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Semi-supervised classification on three different citation networks using biased training samples. Our proposed framework (SR-GNN) outperforms all baselines on biased training input.</figDesc><table><row><cell>Method</cell><cell cols="8">Cora Micro-F1↑ Macro-F1↑ ∆F1 ↓ Micro-F1↑ Macro-F1↑ ∆F1 ↓ Micro-F1↑ Macro-F1↑ ∆F1 ↓ Citeseer PubMed</cell></row><row><cell>GCN (IID)</cell><cell cols="2">80.8 ± 1.6 80.1 ± 1.3</cell><cell>0</cell><cell>70.3 ± 1.9 66.8 ± 1.3</cell><cell>0</cell><cell cols="2">79.8 ± 1.4 78.8 ± 1.4</cell><cell>0</cell></row><row><cell>Feat.+MLP</cell><cell cols="2">49.7 ± 2.5 48.3 ± 2.2</cell><cell>31.1</cell><cell>55.1 ± 1.3 52.7 ± 1.3</cell><cell>25.2</cell><cell cols="2">51.3 ± 2.8 41.8 ± 6.2</cell><cell>28.5</cell></row><row><cell>Emb.+MLP</cell><cell cols="2">57.6 ± 3.0 56.2 ± 3.0</cell><cell>23.2</cell><cell>38.5 ± 1.2 38.6 ± 1.1</cell><cell>31.8</cell><cell cols="2">60.4 ± 2.1 56.6 ± 2.0</cell><cell>19.4</cell></row><row><cell>DGI</cell><cell cols="2">71.7 ± 4.2 69.2 ± 3.7</cell><cell>9.1</cell><cell>62.6 ± 1.6 60.0 ± 1.6</cell><cell>7.6</cell><cell cols="2">58.0 ± 5.3 52.4 ± 8.3</cell><cell>21.8</cell></row><row><cell>GCN</cell><cell cols="2">67.6 ± 3.5 66.4 ± 3.0</cell><cell>13.2</cell><cell>62.7 ± 1.8 60.4 ± 1.6</cell><cell>7.6</cell><cell cols="2">60.6 ± 3.8 56.0 ± 6.0</cell><cell>19.2</cell></row><row><cell>GAT</cell><cell cols="2">58.4 ± 5.7 58.5 ± 5.0</cell><cell>22.4</cell><cell>58.0 ± 3.5 55.0 ± 2.7</cell><cell>12.3</cell><cell cols="2">55.2 ± 3.7 46.0 ± 6.4</cell><cell>14.6</cell></row><row><cell>SGC</cell><cell cols="2">70.2 ± 3.0 68.0 ± 3.8</cell><cell>10.6</cell><cell>65.4 ± 0.8 62.5 ± 0.8</cell><cell>4.9</cell><cell cols="2">61.8 ± 4.5 57.4 ± 7.2</cell><cell>18.0</cell></row><row><cell>APPNP</cell><cell cols="2">71.3 ± 4.1 69.2 ± 3.4</cell><cell>9.5</cell><cell>63.4 ± 1.8 61.2 ± 1.6</cell><cell>6.9</cell><cell cols="2">63.4 ± 4.2 58.7 ± 7.0</cell><cell>16.4</cell></row><row><cell>SR-GNN w.o. IR</cell><cell cols="2">72.1 ± 4.4 69.8 ± 3.7</cell><cell>8.7</cell><cell>63.9 ± 0.7 61.8 ± 0.6</cell><cell>6.4</cell><cell>69.4± 3.4</cell><cell>67.6 ± 4.0</cell><cell>10.4</cell></row><row><cell cols="3">SR-GNN w.o. Reg. 72.0 ± 3.2 69.5 ± 3.7</cell><cell>8.8</cell><cell>66.1 ± 0.9 63.4 ± 0.9</cell><cell>4.2</cell><cell cols="2">66.4 ± 4.0 64.0 ± 5.5</cell><cell>13.4</cell></row><row><cell>SR-GNN (Ours)</cell><cell>73.5 ± 3.3</cell><cell>71.4± 3.5</cell><cell>7.3</cell><cell>67.1 ± 0.9 64.0 ± 0.9</cell><cell>3.2</cell><cell cols="2">71.3 ± 2.2 70.2 ± 2.4</cell><cell>8.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semi-supervised classification on ogb-arxiv and reddit varying label ratio.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ogb-arxiv</cell><cell></cell><cell></cell><cell cols="2">reddit</cell><cell></cell></row><row><cell>label(%)</cell><cell>1 %</cell><cell></cell><cell>5 %</cell><cell></cell><cell>1 %</cell><cell></cell><cell>5 %</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Accuracy ∆ ↓</cell><cell>Accuracy</cell><cell>∆ ↓</cell><cell>Accuracy</cell><cell>∆ ↓</cell><cell>Accuracy</cell><cell>∆ ↓</cell></row><row><cell>GCN (IID)</cell><cell>66.0± 0.6</cell><cell>0</cell><cell>69.1± 0.6</cell><cell>0</cell><cell>93.8 ± 0.3</cell><cell>0</cell><cell>94.0 ± 0.1</cell><cell>0</cell></row><row><cell>Feat.+MLP</cell><cell cols="8">45.5± 0.6 21.5 43.7± 0.3 25.4 46.6±0.6 47.2 57.2±0.2 36.8</cell></row><row><cell>Emb.+MLP</cell><cell cols="8">51.1± 1.3 14.9 56.9± 0.8 13.2 89.6 ± 0.8 4.2 90.9 ± 0.3 3.1</cell></row><row><cell>DGI</cell><cell cols="7">44.8± 3.0 21.2 49.7± 3.3 19.4 83.7±1.2 10.1 85.4±0.6</cell><cell>8.6</cell></row><row><cell>GCN</cell><cell cols="4">59.3± 1.2 6.7 65.3 ± 0.6 3.8</cell><cell>89.7±1.0</cell><cell>4.1</cell><cell>90.9±0.3</cell><cell>3.1</cell></row><row><cell>GAT</cell><cell cols="4">58.6± 1.0 7.4 63.4 ± 1.0 5.7</cell><cell cols="4">80.5±5.4 13.3 82.0±3.6 12.0</cell></row><row><cell>SGC</cell><cell cols="4">59.0± 0.7 7.0 64.2 ± 1.3 4.9</cell><cell>88.6±1.0</cell><cell>5.2</cell><cell>90.6±0.2</cell><cell>3.4</cell></row><row><cell>APPNP</cell><cell cols="4">59.8± 1.1 6.2 65.1 ± 2.6 4.0</cell><cell>88.4±1.0</cell><cell>5.4</cell><cell>88.9±0.8</cell><cell>5.1</cell></row><row><cell>SR-GNN w.o. IR</cell><cell cols="2">60.6± 0.2 5.4</cell><cell>65.1±1.8</cell><cell>4.0</cell><cell>90.4± 0.6</cell><cell>3.4</cell><cell>91.2± 0.2</cell><cell>2.8</cell></row><row><cell cols="3">SR-GNN w.o. Reg. 61.0± 0.3 5.0</cell><cell>65.8±2.0</cell><cell>3.3</cell><cell>89.4± 0.8</cell><cell>4.4</cell><cell>91.9± 0.1</cell><cell>2.1</cell></row><row><cell>SR-GNN (Ours)</cell><cell>61.6±0.6</cell><cell>4.4</cell><cell>66.5±0.6</cell><cell>2.6</cell><cell>91.5± 0.5</cell><cell>2.3</cell><cell>92.1± 0.3</cell><cell>1.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of baseline and our SR(Shift-Robust) version (∆(%) -relative loss with biased sample) .</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>PubMed</cell><cell></cell></row><row><cell>Method</cell><cell cols="9">Micro-F1↑ Macro-F1↑ ∆(%) Micro-F1↑ Macro-F1↑ ∆(%) Micro-F1↑ Macro-F1↑ ∆(%)</cell></row><row><cell>GCN (IID)</cell><cell>80.8</cell><cell>80.1</cell><cell>0%</cell><cell>70.3</cell><cell>66.8</cell><cell>0%</cell><cell>79.8</cell><cell>78.8</cell><cell>0%</cell></row><row><cell>GCN</cell><cell>67.6</cell><cell>66.4</cell><cell>-12%</cell><cell>62.7</cell><cell>60.4</cell><cell>-8%</cell><cell>60.6</cell><cell>56.0</cell><cell>-19%</cell></row><row><cell>SR-GCN</cell><cell>69.6</cell><cell>68.2</cell><cell>-10%</cell><cell>64.7</cell><cell>62.0</cell><cell>-6%</cell><cell>67.0</cell><cell>65.2</cell><cell>-13%</cell></row><row><cell>DGI (IID)</cell><cell>80.6</cell><cell>79.3</cell><cell>0%</cell><cell>70.8</cell><cell>66.7</cell><cell>0%</cell><cell>77.6</cell><cell>77.0</cell><cell>0%</cell></row><row><cell>DGI</cell><cell>71.7</cell><cell>69.2</cell><cell>-9%</cell><cell>62.6</cell><cell>60.0</cell><cell>-8%</cell><cell>58.0</cell><cell>52.4</cell><cell>-20%</cell></row><row><cell>SR-DGI</cell><cell>74.3</cell><cell>72.6</cell><cell>-6%</cell><cell>65.8</cell><cell>62.6</cell><cell>-6%</cell><cell>62.0</cell><cell>57.8</cell><cell>-16%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of Domain-Adversarial Neural Network (DANN) and CMD regularizer used in SR-GNN with biased training data.</figDesc><table><row><cell></cell><cell>Cora</cell><cell></cell><cell>Citeseer</cell><cell></cell><cell>PubMed</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Micro-F1↑ Macro-F1↑ Micro-F1↑ Macro-F1↑ Micro-F1↑ Macro-F1↑</cell></row><row><cell>GCN</cell><cell>68.3</cell><cell>67.2</cell><cell>62.4</cell><cell>60.2</cell><cell>59.2</cell><cell>53.8</cell></row><row><cell>DANN</cell><cell>69.8</cell><cell>68.5</cell><cell>63.8</cell><cell>61.0</cell><cell>64.8</cell><cell>61.8</cell></row><row><cell>CMD (Ours)</cell><cell>71.0</cell><cell>69.4</cell><cell>65.0</cell><cell>62.3</cell><cell>67.5</cell><cell>66.2</cell></row><row><cell>APPNP</cell><cell>71.3</cell><cell>69.2</cell><cell>63.9</cell><cell>61.6</cell><cell>64.8</cell><cell>60.4</cell></row><row><cell>DANN</cell><cell>71.6</cell><cell>69.5</cell><cell>64.3</cell><cell>61.8</cell><cell>67.8</cell><cell>65.4</cell></row><row><cell>CMD (Ours)</cell><cell>72.4</cell><cell>70.1</cell><cell>65.0</cell><cell>62.4</cell><cell>70.4</cell><cell>68.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Overall Dataset Statistics</figDesc><table><row><cell>Dataset</cell><cell># Nodes</cell><cell># Edges</cell><cell cols="2"># Features # Classes</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,278</cell><cell>1,433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,614</cell><cell>3,703</cell><cell>6</cell></row><row><cell>PubMed</cell><cell>19,717</cell><cell>44,325</cell><cell>500</cell><cell>3</cell></row><row><cell cols="2">ogb-arxiv 169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 114,615,892</cell><cell>602</cell><cell>41</cell></row><row><cell cols="2">A.2 Scalable biased sampler details</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code and processed data are available at https://github.com/GentleZhu/Shift-Robust-GNNs.35th Conference on Neural Information Processing Systems (NeurIPS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2021" xml:id="foot_1">), Online.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">The exact use of the adjacency matrix varies with specific GNN methods. For instance, the GCN<ref type="bibr" target="#b14">[15]</ref> performs mean-pooling using matrix multiplication:Ã = D − 1 2 (A + I)D − 1 2where I is the identity matrix, and D is the degree matrix of (A + I), Dii = j (A + I)ij.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">We use a bounded activation function here, e.g. tanh or sigmoid.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>Research was supported in part by US DARPA KAIROS Program No. FA8750-19-2-1004, SocialSim Program No. W911NF-17-C-0099, and INCAS Program No. HR001121C0165, National Science Foundation IIS-19-56151, IIS-17-41317, and IIS 17-04532, and the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of DARPA or the U.S. Government. We would like to thank AWS Machine Learning Research Awards program for providing computational resources for the experiments in this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. <ref type="bibr" target="#b3">4</ref> Performance study with best hyper parameters In Section 5.4 of the main paper, we presented the parameter study of SR-GNN regarding the distribution discrepancy regularizer and instance weighting. We tune these parameters to obtain the best performance of SR-GNN w.o. IR, SR-GNN w.o. Reg. and SR-GNN in Table <ref type="table">6</ref>. Both SR-GNN and its variants show improvement performance and minimize the performance gap with the IID trained model even further.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MixHop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Annual IEEE Symposium on Foundations of Computer Science (FOCS&apos;06)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rózemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine learning on graphs: A model and comprehensive taxonomy</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franccois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Covariate shift by kernel mean matching</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Schmittfull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dataset shift in machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Grale: Designing networks for graph learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Mosoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2523" to="2532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An introduction to domain adaptation and transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wouter</surname></persName>
		</author>
		<author>
			<persName><surname>Kouw</surname></persName>
		</author>
		<idno>CoRR, abs/1812.11806</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural networks for malicious account detection</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2077" to="2085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
				<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gcan: Graph convolutional adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Xinhong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8266" to="8276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<idno>CoRR, abs/0902.3430</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Debiasing graph representations via metadata-orthogonal training</title>
		<author>
			<persName><forename type="first">John</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Network together: Node classification via cross-network deep network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Lai</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kup-Sze</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph clustering with graph neural networks</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16904</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A semi-supervised graph attentive network for financial fraud detection</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive graph convolutional networks</title>
		<author>
			<persName><forename type="first">Man</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Continuous graph neural networks</title>
		<author>
			<persName><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10432" to="10441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08811</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust unsupervised domain adaptation for neural networks via moment alignment</title>
		<author>
			<persName><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><forename type="middle">A</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">483</biblScope>
			<biblScope unit="page" from="174" to="191" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Transfer learning of graph neural networks with ego-graph information maximization</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
