<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Event-based Shape from Polarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-11">11 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manasi</forename><surname>Muglikar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Perception Group</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonard</forename><surname>Bauersfeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Perception Group</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Diederik</forename><forename type="middle">Paul</forename><surname>Moeys</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Advanced Sensors and Modelling Group</orgName>
								<orgName type="institution">SONY R&amp;D Center Europe</orgName>
								<address>
									<postCode>SL1</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Perception Group</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Event-based Shape from Polarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-11">11 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2301.06855v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art solutions for Shape-from-Polarization (SfP) suffer from a speed-resolution tradeoff: they either sacrifice the number of polarization angles measured or necessitate lengthy acquisition times due to framerate constraints, thus compromising either accuracy or latency. We tackle this tradeoff using event cameras. Event cameras operate at microseconds resolution with negligible motion blur, and output a continuous stream of events that precisely measures how light changes over time asynchronously. We propose a setup that consists of a linear polarizer rotating at high speeds in front of an event camera. Our method uses the continuous event stream caused by the rotation to reconstruct relative intensities at multiple polarizer angles. Experiments demonstrate that our method outperforms physics-based baselines using frames, reducing the MAE by 25% in synthetic and real-world datasets. In the real world, we observe, however, that the challenging conditions (i.e., when few events are generated) harm the performance of physics-based solutions. To overcome this, we propose a learning-based approach that learns to estimate surface normals even at low event-rates, improving the physics-based approach by 52% on the real world dataset. The proposed system achieves an acquisition speed equivalent to 50 fps (&gt;twice the framerate of the commercial polarization sensor) while retaining the spatial resolution of 1MP. Our evaluation is based on the first large-scale dataset for event-based SfP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Polarization cues have been used in many applications across computer vision, including image dehazing <ref type="bibr" target="#b43">[43]</ref>, panorama stitching and mosaicing <ref type="bibr" target="#b44">[44]</ref>, reflection removal <ref type="bibr" target="#b21">[21]</ref>, image segmentation <ref type="bibr" target="#b26">[26]</ref>, optical flow gyroscope, <ref type="bibr" target="#b48">[49]</ref> and material classification <ref type="bibr" target="#b4">[5]</ref>. Among these, Shape-from-Polarization (SfP) methods exploit changes in polariza- tion information to infer geometric properties of an object <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr">51,</ref><ref type="bibr" target="#b51">53]</ref>. It uses variations in radiance under different polarizer angles to estimate the 3D surface of a given object. In particular, when unpolarized light is reflected from a surface, it becomes partially polarized depending on the geometry and material of the surface. Surface normals, and thus 3D shape, can then be estimated by orienting a polarizing filter in front of a camera sensor and studying the relationship between the polarizer angle and the magnitude of light transmission. SfP has a number of advantages over both active and passive depth sensing methods. Unlike active depth sensors that use structured light (SL) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">50]</ref> or time-of-flight (ToF), SfP is not limited by material type and can be applied to non-Lambertian surfaces like transparent glass and reflective, metallic surfaces. Despite these advantages, however, estimating highquality surface normals from polarization images is still an open challenge. Division of Focal Plane (DoFP) methods <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b51">53]</ref> trade-off spatial resolution for latency and allow for the capture of four polarizations in the same image. This is achieved through a complex manufacturing process that requires precisely placing a micro-array of four polarization filters on the image sensor <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35]</ref>, as shown in Fig. <ref type="figure" target="#fig_1">2a</ref> Despite the reduced latency, this system constrains the maximum number of polarization angles that can be captured, potentially impacting the accuracy of the estimates as we show in our results.Additionally, the spatial resolution of the sensor is also reduced, requiring further mosaicing-based algorithms for high-resolution reconstruction <ref type="bibr" target="#b50">[52]</ref>. On the other hand, Division of Time (DoT) methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr">51]</ref> provide full-resolution images and are not limited in the number of polarization angles they can capture thanks to a rotating polarizing filter put in front of the image sensor. The frame rate of the sensing camera, however, effectively limits the rate at which the filter can rotate, increasing the acquisition time significantly (acquisition time = N/f , where N is the number of polarizer angles and f is the framerate of the camera). For this reason, commercial solutions, such as the Lucid Polarisens <ref type="bibr" target="#b35">[35]</ref>, favor DoFP, despite the lower resolution of both polarization angles and image pixels. To overcome this shortcoming, recently, significant progress has been made with data-driven priors <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b51">53]</ref>. However, these solutions still fall short in terms of computational complexity when compared to DoT methods. A solution able to bridge the accuracy of DoT with the speed of DoFP is thus still lacking in the field.</p><p>In this paper, we tackle the speed-resolution trade-off using event cameras. Event cameras are efficient highspeed vision sensors that asynchronously measure changes in brightness intensity with microsecond resolution. We exploit these characteristics to design a DoT approach able to operate at high acquisition speeds (up to 5, 000 fps vs. 22 fps of standard frame-based devices) and full-resolution (1280 ? 720) s shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Thanks to the working principles of event-cameras, our sensing device provides a continuous stream of information for estimating the surface normal as compared to the discrete intensities captured at fixed polarization angles of traditional approaches. We present two algorithms to estimate surface normals from events, one using geometry and the other based on a learning-based approach. Our geometry-based method takes advantage of the continuous event stream to reconstruct relative intensities at multiple polarizer angles, which are then used to estimate the surface normal using traditional methods. Since events provide a temporally rich information, this results in better reconstruction of intermediate intensities. This leads to an improvement of upto 25% in surface normal estimation, both on the synthetic dataset and on the real-world dataset.On the real dataset, however, the non-idealities of the event camera introduce a lower fill-rate (percentage of pixels triggering events) of 3.6% in average (refer Section 3.1). To overcome this, we propose a deep learning framework which uses a simple U-Net network to predict the dense surface normals from events. Our datadriven approach improves the accuracy over the geometrybased method by 52%. Our contributions can be summarized as follows:</p><p>? A novel approach for shape-from-polarization using an event camera. Our approach utilizes the rich temporal information of events to reconstruct event intensities at multiple polarization angles. These event intensities are then used to estimate the surface normal. Our method outperforms previous state-of-the-art physics-based approaches using images by 25% in terms of accuracy. ? A learning-based framework which predicts surface normals using events to solve the issue of low fill-rate common in the real-world. This framework improves the estimation over physics-based approach by 52% in terms of angular error. ? Lastly, we present the first large scale dataset containing over 90 challenging scenes for SfP with events and images. Our dataset consists of events captured by rotating a polarizer in front of an event camera, as well as images captured using the Lucid Polarisens <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Frame-based SfP methods Shape-from-polarization estimates the normal of each point on an object's surface through Fresnel equations <ref type="bibr" target="#b7">[8]</ref> by measuring the azimuthal and zenithal angles at each pixel. However, since a lin-ear polarizer can only distinguish polarized light modulus 2?, shape-from-polarization is often regarded as an underdetermined problem and additional constraints are typically required to solve ambiguities in the measurements.</p><p>Prior SfP systems that addressed the problem with traditional cameras are thoroughly discussed in <ref type="bibr" target="#b10">[11]</ref>. Early methods relied on assumptions about object surface and environment lighting to constrain the problem, such as pure specular <ref type="bibr" target="#b39">[39]</ref> or pure diffusion reflections <ref type="bibr" target="#b0">[1]</ref> and surface convexity <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">15]</ref>, but they typically fail when the theoretical model is violated. In order to ensure uniqueness in the solution, several methods exploit depth and geometric information to guide the surface reconstruction operation. A line of research <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref> measures polarization information from multiple view points, while other authors exploit coarse depth maps from low-cost depth sensors, such as Kinect <ref type="bibr" target="#b19">[19]</ref> or RGBD cameras <ref type="bibr" target="#b53">[55]</ref>, to obtain additional geometric cues. In single-view, spectral information measured by multi-band cameras can also be used to solve the ?-ambiguity, as well as estimating refractive properties of the material useful for 3D reconstruction <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b46">46]</ref>. Another class of methods considers photometric information to disambiguate SfP normal estimates by combining photometric stereo <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">33]</ref> and photometric constraints <ref type="bibr" target="#b25">[25]</ref> to impose shading constraints from multiple light directions.</p><p>More recently, <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b51">53]</ref> presented a data-driven approach to solve the ambiguity, resulting in state-of-the-art performance. Both these approaches, however, use a Polarisens sensor <ref type="bibr" target="#b35">[35]</ref> which compromises spatial resolution for faster acquisition time, thus limiting the potential of the learningbased approach. We propose to address the shortcoming arsing form the spatial-temporal resolution trade-off using an event camera and take advantage of deep learning methods to tackle non-idealities of the event camera. Event-based SfP methods Since event cameras are novel sensors, there are very few papers that combine polarization information with events. Recently, <ref type="bibr" target="#b14">[14]</ref> combined the DoFP approach with an event camera by manufacturing a micro array of polarizers and placing it on top of the event camera sensor. This requires a very precise manufacturing process in which the polarizers are manually aligned with the pixels by inducing polarized motion for the pixels to see. Due to the low yield resulting from this process and to low-spatial resolution issues analogous to standard cameras, such as mosaicing, commercial solution are currently not available. Moreover, this approach also requires relative motion between the camera and the scene to capture event information, which further limits its application. In contrast, our approach, making use of a rotating polarizing filter, exploits the full spatial resolution of the event camera and does not require camera motion. Since event cameras have a high temporal resolution, this approach enables us to rotate the filter at very high speeds (upto 1500 RPM), thus enabling high-speed and high spatial resolution SfP.</p><p>SfP Datasets Prior SfP datasets are summarized in Table 1. The datasets were collected using standard cameras with either DoT <ref type="bibr" target="#b18">[18]</ref> or DoFP <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b51">53]</ref>. Kadambi et al. <ref type="bibr" target="#b18">[18]</ref> presented a small dataset which was collected using Canon Rebel T3i DSLR camera and a linear polarizer and images were collected at 6 discrete polarizer angles. The advent of the Lucid Polarsens <ref type="bibr" target="#b35">[35]</ref> camera enabled the generation of large scale dataset proposed by Ba et al. <ref type="bibr" target="#b51">[53]</ref> and Lei et al. <ref type="bibr" target="#b22">[22]</ref>. These datasets capture only 4 polarization angles as is the case with DoFP-based methods, and do not contain events. To push the limits of event-based SfP, in this paper, we propose two datasets, synthetic and real-world, which contain events, images and accurate groundtruth in challenging scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Event-based Shape from Polarization</head><p>Event-cameras are novel, bio-inspired sensors that asynchronously measure changes (i.e., temporal contrast) in illumination at every pixel, at the time they occur <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr">47]</ref>. In particular, an event camera generates an event e k = (x k , t k , p k ) at time t k when the difference of logarithmic brightness at the same pixel x k = (x k , y k ) reaches a predefined threshold C:</p><formula xml:id="formula_0">L(x k , t k ) -L(x k , t k -?t k ) = p k C,<label>(1)</label></formula><p>where p k ? {-1, +1} is the sign (or polarity) of the brightness change, and ?t k is the time since the last event at the pixel x k . The result is a sparse sequence of events which are asynchronously triggered by illumination changes. We consider the task of surface normal estimation using a polarizer and an event camera, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. The polarizer, rotating in front of the event camera at speed ? using a motor, changes the illumination of the incoming light. Given the orientation ? pol of the polarizer, we can express the light intensity I(? pol ) passing through it as:</p><formula xml:id="formula_1">I(? pol ) = I un (1 + ? ? cos(2(? pol -?)),<label>(2)</label></formula><p>where I un is the unpolarized intensity, ? is the degree of polarization and ? is the angle of polarization of the surface. When rotating the polarizer, the sinusoidal intensity changes trigger events in the event camera. For instance, when observing two surfaces which are perpendicular to each other with our setup, the corresponding events have opposite polarity, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. This is because the underlying sinusoids are phase shifted by 90 ? . We take advantage of the close relationship between surface orientation and event generation to develop a principled procedure for estimating surface normals. By continuously capturing how light is affected by the polarizer as it rotates, events offer increased information than what is provided by regular frame-based devices.</p><p>Based on these considerations, we start in Section 3.1 by describing how events can directly be used to estimate the normals in a physics-based fashion. This approach, however, assumes events are generated following an ideal model, which might limit its performance when edge-cases are met in the real world due to nonidealities. Leveraging these findings, we also propose a deep learning method that improves predictions in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Physics-based ESfP</head><p>Over one rotation of the polarizer, the intensity at each pixel x p follows a sine wave, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. By combining this behavior with the event generation model in Eq. ( <ref type="formula" target="#formula_0">1</ref>), we can estimate the intensity I e (x p ) at any time ? as:</p><formula xml:id="formula_2">L(? ) = L(? -?t) + p t C = L(0) + ? t=0 p t C<label>(3)</label></formula><formula xml:id="formula_3">I e (? ) = exp (L(0)) ? exp ? t=0 p t C ,<label>(4)</label></formula><p>where we dropped the pixel location x p from the logarithmic brightness for readability. Since events only capture relative intensity changes and not absolute ones, there is no way to recover the constant e L(0) without direct intensity measurements. However, we show below that for the computation of the surface normal, the knowledge of absolute intensity is not required, and we can thus avoid estimating e L(0) .</p><p>Since ? pol = ? ? t, we can use the above formulation to reconstruct "event-intensities" (I e ) at multiple angles ? pol , rather than time instants. These "event-intensities" are then used to estimate the surface normal using traditional algorithms. Below, we show the estimation of ? and ? using the event intensities computed at 4 polarizer angles as an example, where we change the notation from from I e (?) to I e [?] to highlight the change in variable from time to angles. Notice that, in practice, we estimate event intensities at 12 polarizer angles. Nonetheless, in both cases, we can demonstrate that estimating surface normal does not require the knowledge of absolute intensity. The SfP equations <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b51">53]</ref> can be substituted with our event intensities as follows:</p><formula xml:id="formula_4">? = 0.5 ? arctan I e [ ? /4] -I e [ 3? /4] I e [0] -I e [ ? /2] (5) = 0.5 ? arctan e ?/4? t=0 ptC- 3?/4? t=0 ptC - ?/2? t=0 p t C<label>(6)</label></formula><p>Similarly, we can estimate ? as, Using the above ? and ?, the azimuth (?) and zenith angle (?) of the surface normal can be estimated. We refer the reader to the supplementary materials for a complete derivation, which takes into account all the 12 angles we consider in our approach and the estimation of surface normal from ? and ?. Since we reconstruct event intensities, any traditional algorithm developed for frame-based approaches can be utilized along with our approach. This opens the door to build upon the past research and develop better phyiscsbased algorithms from events.</p><formula xml:id="formula_5">? = 0.5 ? arctan (I e [ ? /2] 2 + (I e [ ? /4] 2 -I e [ 3? /4] 2 ) I e [ ? /2] + I e [ ? /4] + I e [ 3? /4] ,<label>(7)</label></formula><p>Event non-idealities An event camera, like any physical sensor, is affected by noise caused by nonidealities of its individual electric circuits and the manufacturing process. The most important parameter that affects the noise generation model is the contrast sensitivity threshold C. If C is too high, this results in less number events triggered, thus increasing the signal to noise ratio; however, if it C is set too low, a higher number of noisy events are generated. In our application, it is favourable to have a low contrast threshold, in the order of 1-5% as the polarization signal is often weak (especially in low light conditions). Commercial available event sensors <ref type="bibr" target="#b38">[38]</ref> however, are manufactured for a general purpose use of event cameras, and therefore have a very high threshold, typically ranging between 30% and 50%<ref type="foot" target="#foot_0">1</ref> . This directly affects the number of events that are triggered (or the fill-rate), resulting in a sparse estimation. Since the polarization signal is especially weak for front-parallel surfaces in real world scenes, we find that these surfaces rarely trigger any event. To address this sensor issue, we propose to tackle the problem using a data-driven approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning-based ESfP</head><p>We now describe our approach to solve the problem of shape-from-polarization using deep learning. As described above, events can be used to reconstruct event intensities at multiple polarization angles. Inspired by the frame-based solution of Ba et al. <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b51">53]</ref>, we propose to tackle the problem of estimating the surface normals using a U-Net <ref type="bibr" target="#b42">[42]</ref> architecture. The network predicts the surface normal N as a 3 channel tensor N = (sin ? cos ?, sin ? sin ?, cos ?), where ? and ? are the zenith and azimuth angle respectively.</p><p>To adopt this network to event data, we need to convert the sparse event stream into a frame-like event representation that is compatible with the CNN layers. A straightforward approach, which demonstrated remarkable performance on several tasks <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b41">41]</ref>, is to use the voxel grid <ref type="bibr" target="#b52">[54]</ref> representation as input to the encoder network. This representation splits the temporal domain into bins, and aggregates events in each pixel as the sum of their polarity. However, doing so, empirically results in a higher mean absolute error, most likely due to events with opposite polarity in the same bin nullifying each other, and thus resulting in an empty feature. Please refer the supplementary material for more details. We therefore design a novel event representation specifically suited for the SfP task. In Equation. 3, we observe that the accumulated sum of event polarities is directly related to the image intensity at a given time instant. Based on this consideration, we propose the Cumulative Voxel Grid Representation (CVGR), a variation of the voxel grid <ref type="bibr" target="#b52">[54]</ref> that preserves polarity information even across long time intervals. Similar to previous works on learning with events <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b47">48]</ref>, we first build a voxel grid <ref type="bibr" target="#b52">[54]</ref> by partitioning the time domain into b = 0, ..., B -1 equallysized temporal windows, each aggregating the sequence of events</p><formula xml:id="formula_6">E b = e k | t k ? b?T B , (b+1)?T B .</formula><p>We then compute the cumulative sum over the bins and multiply this quantity with the contrast threshold.</p><formula xml:id="formula_7">E(x, y, b) = i=b i=0 C ? V (x, y, i) = i=b i=0 C e k ?Ei: x k =x,y k =y p k . (8)</formula><p>This final representation E is used as an input to the network. In our experiments, we use a voxel grid with 8 bins, thus our CVGR has the dimension H ? W ? 8.</p><p>This novel event-representations is then fed to a 8level U-Net encoder-decoder architecture which densely regresses the normals. We apply cosine similarity loss function on the unit normalized predictions for training. We show this simple architecture is enough to recover highquality normal vectors, even in locations where no event has been triggered, thus directly addressing the weakness of purely model-based solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>To the best of our knowledge, there exists no dataset on which the proposed approach can be evaluated. We, therefore, first evaluate our approach on a synthetic dataset, rendered using an accurate polarimetric renderer <ref type="bibr" target="#b16">[16]</ref> to perform controlled experiments. To evaluate our approach in the real world, we also build a prototype of the proposed setup using an event camera and a polarizing filter. We provide more details in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ESfP-Synthetic Dataset</head><p>We use the Mitsuba renderer <ref type="bibr" target="#b16">[16]</ref> to render a scene consisting of a textured mesh illuminated with a point light source. A polarizer lens was rotated in front of the camera between 0 and 180 degrees with 15 degrees intervals, resulting in 12 rendered images. Images thus collected were then used to simulate events using ESIM <ref type="bibr" target="#b40">[40]</ref> with a contrast sensitivity threshold of 5%. The dataset consists of 89 training and 15 test sequences, each containing the images captured with the polarizer, together with the events generated from these, as well as the groundtruth surface normal provided by the renderer. Meshes used from data generation were obtained from the Google Scanned Objects dataset <ref type="bibr" target="#b8">[9]</ref> and the textures derived from <ref type="bibr" target="#b5">[6]</ref>. More details are provided in the supplementary material. To increase data variability, the position of the light source was randomized and the mesh rotated for each scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ESfP-Real Dataset</head><p>To evaluate our approach in the real world, we built a prototype using a Prophesee Gen 4 event camera <ref type="bibr" target="#b12">[12]</ref> and a Breakthrough Photography X4 CPL <ref type="bibr" target="#b36">[36]</ref> linear polarizer with a quarter-wave plate, commonly referred to as a circular polarizer (CPL). The prototype is shown in Fig. <ref type="figure" target="#fig_3">3</ref>. The polarizer can rotate at speeds of up to 1500 rpm using a brushless DC motor. The groundtruth is generated using Event-based Structured Light (ESL) <ref type="bibr" target="#b31">[31]</ref>, which consists of a laser point projector combined with an event camera. It provides accurate groundtruth thanks to laser scanner, while also having a small acquisition time (16 ms). Another advantage of using ESL is that the groundtruth is always in the frame of the event camera and therefore no additional alignment is required.</p><p>We use this setup to collect the first large scale dataset consisting of several objects with different textures and shapes, and featuring multiple illumination and scene depths, for a total of 90 scenes. Additionally, we use a Lucid Polarisens camera <ref type="bibr" target="#b35">[35]</ref> to collect polarization images of the same scene. The images are obtained at 4 polarization angles {0, 45, 90, 135} with a resolution of 1224 ? 1220. We rectify the images and aligned them with the event camera to enable fair comparison with available image-based only methods. The dataset was recorded with the rotation speeds of 150RPM with illumination of 200lux.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section evaluates the performance of our eventbased SfP system for surface normal estimation. We begin by introducing the baselines and performance metrics which will be used for evaluation on our dataset. Then, we perform experiments on the ESfP-Synthetic dataset to quantify the accuracy of our proposed physics-based and Image Events <ref type="bibr" target="#b25">[25]</ref> [45] Ours GT learning-based methods. Finally, we evaluate these approach on our ESfP-Real dataset and study the proposed approach on real scenes under challenging conditions.</p><p>Implementation details We implement our physicsbased approach in Python. As noted in Section 3.1, we reconstruct event intensities at 12 polarizer angles. Our physics-based approach does not require any hyperparameter tuning. The proposed learning-based approach was implemented in PyTorch. We train our network for 1000 epochs with a batch size of 4 on NVIDIA Tesla V100. We used a learning rate of 1e -4 and Adam <ref type="bibr" target="#b20">[20]</ref> as optimizer.</p><p>Evaluation Metrics We use 4 metrics to quantify the predicted surface normals, namely Mean Angular Error (MAE), % Angular Error under 11.25 ? (AE&lt;11.25), % Angular Error under 22.5 ? (AE&lt;22.5) and % Angular Error under 30 ? (AE&lt;30). The first is a widely used metric for computing the angular error of the predicted surface normal (lower is better) <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b51">53]</ref>, while the last three, which we also refer to as angular accuracy, measure the percentage of pixels under 11.25 ? , 22.5 ? and 30 ? of angular error (higher is better). We introduce another metric called fill-rate to compare events and images. The fill-rate measures the percentage of pixels that are triggered by SfP, i.e., by the polarizing filter's rotation. For image-based approaches, this always equals to 1, since images capture intensity for every pixel. However, for events this becomes an important parameter, since the number of triggered pixels depends on the contrast sensitivity of the camera as explained in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Ba et al. [53]</head><p>Ours (L) GT Baselines We evaluate the proposed methods against frame-based SfP solutions. As we propose to tackle the problem both in a model-based and a data-driven manner, we include state-of-the-art methods from both categories as baselines in our evaluation. Smith et al. <ref type="bibr" target="#b45">[45]</ref> and Mahmoud et al. <ref type="bibr" target="#b25">[25]</ref> recover the surface normal using physics-based SfP methods<ref type="foot" target="#foot_1">2</ref> , whereas Ba et al. <ref type="bibr" target="#b51">[53]</ref> uses a learning-based SfP method. We re-implemented the last approach and retrained it on our datasets using only the images as input. Note, while Lei et al. <ref type="bibr" target="#b22">[22]</ref> improved upon the works of Ba et al., by introducing view-encoding as an input to the network to generalize better to natural scenes, the goal of this paper is to show the performance of our learning-based framework on object-level scenes used in Ba et al. We therefore, do not compare our approach against Lei et al.</p><p>The event-based SfP solution proposed in <ref type="bibr" target="#b14">[14]</ref> is not commercially available and requires specific manufacturing in order to be replicated. Moreover, this sensor requires relative motion between the scene and the camera and would not result in any surface normal estimation in our experiments. We, therefore, do not include comparison to this method in our experiments. In the following sections, we will first evaluate our methods in a simulated setting where precise ground truth allows for an accurate evaluation, and then proceed by studying their performance on the real world dataset we collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on ESfP-Synthetic</head><p>Table <ref type="table" target="#tab_1">2</ref> shows the performance of our event-based approaches (both geometry-based and learning-based SfP) on the synthetic dataset. As it can be observed, frames-based solutions leveraging a physics-based approach are limited by the number of polarization images (and thus angles) which can be used for inference. When the same approach is used, i.e., in the case of Smith et al. <ref type="bibr" target="#b45">[45]</ref>, simply increasing the number of images from 4 to 12 decreases the angular error by nearly 8%. This gain in performance, however, comes at the cost of either spatial resolution or acquisition speed. In the case of a DoT approach, for instance, because the acquisition time is linearly proportional to the number of frames acquired, increasing the angles from 4 to 12 triples the acquisition time at a given frame-rate.</p><p>On the other hand, our approach is not limited by <ref type="bibr" target="#b25">[25]</ref> [45] Ours (P) GT this tradeoff thanks to the high-temporal resolution of the event camera. Exploiting the continuous stream of events acquired when observing the light being polarized, our geometry-based approach outperforms other physics-based methods using 4 images by 14% in angular error. In Fig. <ref type="figure">4</ref>, we show qualitative results of our approach against the baselines. While our learning-based approach boosts the performance compared to physics-based methods significantly, it falls short of the image-based counterpart. The reason is because the learning-based method tries to hallucinate the surface normal prediction in the absence of events, whereas the image counterpart still has a dense spatial information to guide the network to interpolate the surface normals. In future, it would be worth investigating a combination of events and images to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on ESfP-Real</head><p>We also compare these methods on the real dataset in Table <ref type="table" target="#tab_2">3</ref> and report qualitative results in Fig. <ref type="figure">6</ref>. We observe similar conclusions as with the synthetic dataset. The proposed geometry-based approach outperforms previous physics-based solutions by 25.8% in angular error.As mentioned in Section 3.1, a real event camera introduces several non-idealities, one of them being the contrast sensitivity. With a typical event camera the contrast sensitivity cannot be usually set lower than 30%, resulting in the inability to capture small intensity changes. An example of this phenomenon is given in Fig. <ref type="figure" target="#fig_5">7</ref>, where the presence Scene Ba <ref type="bibr" target="#b51">[53]</ref> Ours P Ours L GT  of fronto-parallel geometries causes a polarization signal and therefore a small fill-rate metric. On average, for this dataset, the fill-rate is around 3.6%. This, however, is exactly the condition where the proposed learning based approach excel. By leveraging learned data-priors, it can recover accurate normals even in the presence of very few events. The improvements in both angular error and accuracy, show the proposed method outperforms all baselines, despite falling short by a small margin to Ba et al. <ref type="bibr" target="#b51">[53]</ref>. We emphasize, however, that these results were obtained under ideal lighting conditions, where data-priors of learningbased approaches are enough to compensate for the limited polarization information that standard sensors can provide. We now show the performance of our methods when these perfect conditions are not satisfied against baselines.</p><p>Effect of illumination Fig. <ref type="figure" target="#fig_6">8</ref> highlights the high dynamic range (HDR) advantage of using an event camera for SfP under different illumination conditions. We increased the illumination from 528 Lux to 1442 Lux and evaluated the performance our proposed approach and compared it to the image-baselines. The corresponding images and events are shown. As it can be noticed, the performance of our methods remains consistent across different illumination conditions. This is case for both the model-based and learningbased variants, highlighting that events provide more information than images, and are thus more suited for these challenging conditions.</p><p>Effect of speed In this section, we assess the performance of our method under different rotation speeds of the motor.</p><p>In Table <ref type="table" target="#tab_5">4</ref>, we find the performance of both our methods improves slightly with an increase in the speed. This can be explained as at higher rotation speeds, the rate of intensity change is higher, which makes non-linearities and non-  idealities in the event-camera's less prominent, resulting in a better quality of events. Please refer to the suppl. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Generalization to outdoor scenes</head><p>In Fig. <ref type="figure">9</ref>, we show that our method generalizes to outdoor scenes as well, despite being trained on single-object and near-depth scenes. Fine tuning on this dataset is not possible due to the unavailability of any groundtruth. In the future, the proposed work would benefit by adopting a view-encoding proposed in <ref type="bibr" target="#b22">[22]</ref> here. Scene Events Ours L Figure <ref type="figure">9</ref>. Performance of our method on outdoor scenes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We introduce a novel method for surface normal estimation using a rotating linear polarizer and an event camera. To the best of our knowledge, we are the first to propose a principled way of estimating the surface normals from polarization events using a physics-based solution. Our method takes advantage of the rich temporal event information to reconstruct relative event intensities at multiple polarizer angles. We also propose a learning-based method to overcome the non-idealities of the physical sensor which improves the performance by 52% in terms of MAE. Despite reduced performance in the presence of event camera non-idealities, we believe our derivations could pave the way to future hybrid solutions to increased robustness and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Surface Normal from Events</head><p>In this section, we describe the details for surface normal estimation from polarizer images. We then extend this knowledge to estimate surface normals from events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Basics of Shape-from-Polarization (SfP)</head><p>Intensity change at ? pol can be expressed as:</p><formula xml:id="formula_8">I(? pol ) = I max + I min 2 + I max -I min 2 ?cos(2(? pol -?)),<label>(9)</label></formula><p>where I min and I max represent the minimum and maximum magnitude seen through the polarizer respectively <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b30">30]</ref>. This equation can be expressed in terms of the magnitude of the light I un and the proportion of polarized component ? (also known as degree of polarizer) as follows:</p><formula xml:id="formula_9">I = I max + I min<label>(10)</label></formula><formula xml:id="formula_10">? = I max -I min I max + I min<label>(11)</label></formula><p>Lastly, ? is the angle of the linearly polarized component which corresponds to the phase shift of the sinosoid. <ref type="bibr" target="#b30">[30]</ref>. Estimating these three parameters forms the crux of shapefrom-polarization techniques <ref type="bibr">[51]</ref>. These quantities can be estimated from images captured at 4 different polarization angles as follows:</p><formula xml:id="formula_11">I un = I[0] + I[ ? /4] + I[ ? /2] + I[ 3? /4] 2<label>(12)</label></formula><formula xml:id="formula_12">? = (I[0] -I[ ? /2]) 2 + (I[ ? /4] -I[ 3? /4]) 2 I un<label>(13)</label></formula><formula xml:id="formula_13">? = 1 2 ? arctan I[ ? /4] -I[ 3? /4] (I[0] -I[ ? /2]<label>(14)</label></formula><p>To estimate these quantities, minimum 3observations of the intensity are required. However, increasing the observations, improves the accuracy of surface normals. To use 12 polarization angles the above quantities can be derived as follows:</p><formula xml:id="formula_14">I un = i=? i=0 I[i]<label>(15)</label></formula><formula xml:id="formula_15">Q1 = (I[0] -I[ ? /2])<label>(16)</label></formula><formula xml:id="formula_16">Q2 = (I[ ? /12] -I[ 7? /12])<label>(17)</label></formula><formula xml:id="formula_17">Q3 = (I[ ? /6] -I[ 3? /2])<label>(18)</label></formula><formula xml:id="formula_18">U 1 = (I[ ? /4] -I[ 3? /4])<label>(19)</label></formula><formula xml:id="formula_19">U 2 = (I[ ? /3] -I[ 5? /6])<label>(20)</label></formula><formula xml:id="formula_20">U 3 = (I[ 5? /12] -I[ 11? /12])<label>(21) (22)</label></formula><formula xml:id="formula_21">? = Q1 2 + U 1 2 + Q2 2 + U 2 2 + Q3 2 + U 3 2 3 * I un<label>(23)</label></formula><formula xml:id="formula_22">? = 1.5 * (arctan(U 1/Q1)<label>(24)</label></formula><p>+ arctan(U 2/Q2) -?/6 (25)</p><formula xml:id="formula_23">+ arctan(U 3/Q3) -?/3)<label>(26)</label></formula><p>Estimating the surface normals from ? and ? is a matter of estimating the zenith angle ? and azimuth angle ? as shown in the equations below:</p><formula xml:id="formula_24">? dif f use = (n -1 n ) 2 sin 2 ? 2 + 2n 2 -(n + 1 n ) 2 sin 2 ? + 4 cos ? ? n 2 -sin 2 ?<label>(27)</label></formula><formula xml:id="formula_25">? spec = 2n tan ? tan 2 ? sin 2 ? + n * 2<label>(28)</label></formula><p>where n denotes the refractive index and ? is the zenith angle. Depending on the type of reflection (diffuse or specular), the ? is computed differently. Similarly depending the type of reflection, the azimuth angle ? is ? if diffuse reflection dominates otherwise it is ? -?/2:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">SfP from Events</head><p>When estimating surface normals from events, we reconstruct event intensities (I e ) as explained in the main paper. Using the above equations, we estimate ? and ? by first estimating event intensities at 12 polarizer angles, The use of event intensities enables us to use the traditional SfP algorithms to estimation surface normals. Depending on the type of polarization (specular or diffuse), this can result in multiple solutions. We observed using the specular solution results in the lowest angular error. We also used the Smith et al. <ref type="bibr" target="#b45">[45]</ref> baseline with our event intensities. However, this results in a lower performance as shown in Table <ref type="table" target="#tab_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Event Representation</head><p>When learning surface normals from events, the event representation have a significant effect on the performance of the network. In this section, we describe the performance of 3 kinds of input representations namely: event intensities (I e ), voxel grid <ref type="bibr" target="#b52">[54]</ref>, CVGR representation and CVGR-I representation on the ESfP-synthetic dataset. The event intensity representation concatenates I e at polarizer angles of 15, 60, 105, 150 as input the network. (note, we cannot use the intensity at 0 angle, since it will always be zero for all pixels). The CVGR representation builds on top of voxel grid representation as follows:  Lastly, the CVGR-I representation combines a single image with events and is expressed follows:</p><formula xml:id="formula_26">E(x, y, b) = i=b i=0 C ? V (x, y, i) = i=b i=0 C ? ? ? e k ?Ei: x k =x,y k =y p k ? ? ? . (<label>29</label></formula><formula xml:id="formula_27">)</formula><formula xml:id="formula_28">E(x, y, b) = I[0] + i=b C ? (x, y, i)<label>(30)</label></formula><p>As can be seen from Table <ref type="table" target="#tab_6">5</ref>, the best performing representation is CVGR-I. The main reason for improvement is because the image gives more context to the network to estimate surface normals in the areas where the event information is insufficient. Qualitative results on real dataset are shown in Fig. <ref type="figure" target="#fig_7">10</ref>. As can be seen, the events are triggered prominently on the edge of the vase and are missing from the front-parallel surface of the vase. The network using only events has a difficult time to estimate normals on these front-parallel surfaces. On the other hand, using CVGR-I representation, the network performs better resulting in a lower MAE score. Additionally, we also evaluate the effect of number of bins on the performance of the network.</p><p>For the same representation, increasing the number of bins from 4 to 8 improves the performance by 6% in terms of angular error. Higher number of bins preserves the temporal information of events better. However, further increasing the bins to 12 results in a a decrease in performance. This is because not all bins add new information due to limitation of contrast threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Dynamic scenes</head><p>An advantage of using event camera is the high temporal resolution as compared to the frame-based sensor. To highlight this, we record a dynamic scene. The scene consists of a rectangular block which is rotating about it's diagonal Low Speed Med Speed High Speed axis with a drill. The speed of the drill could be adjusted to three increasing levels. As seen in Fig. <ref type="figure" target="#fig_8">11</ref>, the images corresponding to three different speeds are shown in the first row.</p><p>Increasing the speed introduces motion blur for the standard camera, which is also reflected in the surface normals (second row, high speed). In contrast, event-based SfP methods (last row) are better than the image-based counterpart, as can be seen by the sharpness of the edge of the rectangular block. This is primarily due the high rotation speeds of the polarizer enabled by the high temporal resolution of the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Dataset</head><p>ESfP Synthetic Dataset In this section, we provide details on the ESfP-Synthetic dataset which we use for evaluation. The dataset was generated using publicly available meshes <ref type="bibr" target="#b8">[9]</ref> which consists of over 1000 3D scanned common household objects. These meshes were textured using the 25 textures available in this dataset <ref type="bibr" target="#b5">[6]</ref>. These textures provide polarimetric BRDF of real-world materials which provide accurate polarimetric state information when used with physically-based simulation such as Mitsuba.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Limitations</head><p>Our real-world dataset only considers specular objects such as reflective metallic surfaces. We specifically chose Figure <ref type="figure" target="#fig_1">12</ref>. At low speeds more positive than negative events are triggered. Towards higher rotation speeds this trend reverses but the overall number of events per filter rotation decreases visibly. specular objects as they are the most challenging to obtain surface normals for. The geometry of objects with diffuse reflection can be captured easily by methods such as structured light (SL). Additionally, the intensity changes of diffuse materials when observed with a rotating polarizer is low compared to specular objects, which the real event camera cannot capture due to a high contrast threshold. Therefore, capturing events for diffuse materials was not possible with the current version of event cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Effect of speed</head><p>Conducting experiments at different rotation speeds of the polarizer, we observed a slight increase in the performance for our method and linked this to the decreasing relevance of nonidealities in the event-camera pixel circuits. In this section, we provide more details on why an increase in rotational speed improves the performance. Our analysis is based on additional experiments, general considerations on event-camera circuitry <ref type="bibr" target="#b24">[24]</ref> and the technical details of the Prophesee Gen 4 event camera <ref type="bibr" target="#b12">[12]</ref>. The additional data we recorded cover rotation speeds between 53 RPM up to 1,734 RPM and two illumination conditions, 200 lux and 800 lux. In this section, we will use the number of events triggered on the object per revolution of the polarizer as a proxy-measure for the quality of the resulting surface normals.</p><p>Ideal Event Camera A key observation stated in the main paper is that the illumination intensity cancels out in an idealized event camera model. For a given surface on the object degree of polarization ? and polarization angle ?, the event camera observes a sinusoidal intensity profile of the form</p><formula xml:id="formula_29">I(t) = I un (1 + ? cos(2?t -?))<label>(31)</label></formula><p>The ideal event-sensor triggers and event when the temporal contrast T (logarithmic intensity change) exceeds some threshold C <ref type="bibr" target="#b24">[24]</ref>. Combining this with (31) yields an expression which, for an object with illumination independent polarization characteristics, only depends on the rotational speed.</p><formula xml:id="formula_30">T = d (ln I(t)) dt = -2?? sin(2?t -?) 1 + ? cos(2?t -?)<label>(32)</label></formula><p>Based on <ref type="bibr" target="#b32">(32)</ref>, we can see that the number of events triggered per unit time linearly depends on the rotational speed. By considering only the number of events per rotation, this dependency is also cancelled out and an ideal event camera would not show any dependency on the illumination condition or rotational speed.</p><p>Real Event Camera In practice however, we observe that illumination and rotation speed have an effect on the quality of the surface normal estimation. To better understand this, we look at the number of events triggered per rotation of the polarizer and observe that 1. at low rotation speeds, more positive events are triggered (Fig. <ref type="figure" target="#fig_1">12</ref>, Fig. <ref type="figure" target="#fig_10">13</ref>), 2. the difference in fraction of positive and negative events at low RPM becomes more pronounced at lower illumination conditions (Fig. <ref type="figure" target="#fig_10">13</ref>), 3. the number of events per rotation decreases at high rotation speeds (Fig. <ref type="figure" target="#fig_11">14</ref>), and 4. at low illumnation conditions, less events are triggered at a set rotation speed (Fig. <ref type="figure" target="#fig_11">14</ref>).</p><p>While the ideal event camera model fails to explain those observations, a more realistic model takes the non-idealities of the circuitry into account. In <ref type="bibr" target="#b23">[23]</ref> the leakage of the resettransistor is described as a major source of non-ideality as it leads to the spurious positive events, thus increasing the fraction of positive events. Because we consider the number of events per rotation, slower rotation speeds correspond to a longer accumulation times and the BG (background rate) rate corrupts such low-speed measurements stronger as shown in Fig. <ref type="figure" target="#fig_1">12</ref>. This so-called BG rate (background rate) is illumination dependent <ref type="bibr" target="#b12">[12]</ref>. Together with the increase in BG rate at lower light levels <ref type="bibr" target="#b12">[12]</ref> (for bright scenes) this explains observations 1 and 2.</p><p>At high rotation speeds the BG rate has negligible influence. However, a second non-ideality becomes visible: after triggering an event the pixel needs a certain dead time until it can trigger the next event. This is typically done to avoid bus-saturation by a small group of pixels <ref type="bibr" target="#b23">[23]</ref>. This effect is clearly visible when looking at the highest event frequency of pixels on the object (Fig. <ref type="figure" target="#fig_4">15</ref>). For an ideal camera the event-frequency would depend linearly on the rotation speed as derived in <ref type="bibr" target="#b32">(32)</ref>. However, the data clearly  Figure <ref type="figure" target="#fig_4">15</ref>. The frequency at which events are triggered on the object shows a saturation effect due to pixel dead time <ref type="bibr" target="#b23">[23]</ref>. This explains the decrease in event count at high polarzier speeds.</p><p>shows a saturation effect because pixels can only be triggered with a limited frequency, around 1 kHz at 800 lux illumination. In accordance with literature, this maximum frequency decreases with decreasing illumination <ref type="bibr" target="#b23">[23]</ref>, explaining the remaining two observations. In contrast to the ideal event camera model, the output of a real event-camera is sensitive to illumination and rotational speed of the polarizer. Low speeds increase the BG rate noise significantly and only medium polarizer speeds lead to a more even distribution of positive and negative events. If the speed is increased greatly, the pixel dead time may start to degrade the result again. This is in accordance with the results shown in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">Advantage of event camera</head><p>Fig. <ref type="figure" target="#fig_12">16</ref> illustrates the advantage of using an event-based SfP (blue) against frame-based SfP methods (red) using as metrics the framerate and the Mean Angular Error (MAE). Image-based approaches focus on maximizing the performance; however, they are restricted by the camera's framerate to 22 fps while reducing the effective resolution from 4MP to 1MP (DoFP approach). On the other hand, our event-based approach is 3 times faster and pushes the SfP methods toward higher framerates, without sacrificing the resolution. This enables the capture of surface normals of high-speed motion. Unlike high-framerate cameras, event cameras present a fundamentally new approach to visual information processing. While a high framerate camera would capture redundant information resulting in data bus saturation, an event camera only triggers events when there is contrast change, resulting in lower bandwidth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Surface normal estimation using event-based SfP. (a) Rotating a polarizer in front of an event camera creates sinosoidal changes in intensities, triggering events. (b) The proposed eventbased method uses the continuous event stream to reconstruct relative intensities at multiple polarizer angles which is used to estimate surface normals using physics-based and learning-based method. (c) Our approach outperforms image-based baselines [25, 53].</figDesc><graphic url="image-8.png" coords="1,470.86,433.01,58.27,58.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of SfP methods.</figDesc><graphic url="image-9.png" coords="2,57.30,65.30,224.29,83.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Hardware setup of event camera coupled with a rotating linear polarizer.</figDesc><graphic url="image-11.png" coords="5,308.86,72.00,162.14,108.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of our learning-based method against image-counterpart on ESfP-Synthetic scene with low-contrast.</figDesc><graphic url="image-32.png" coords="6,370.12,84.15,55.07,55.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparison of best image-baseline (Ba et al.) against ours event-based methods on the ESfP-Real dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Varying the ambient illumination has a drastic effect on the image-based baselines (bar plot: green colors), whereas both our methods (bar plot: blue colors), perform consistently.</figDesc><graphic url="image-89.png" coords="8,68.24,440.60,51.09,51.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Qualitative comparison of event representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Comparison on dynamic scenes: The object is rotating with increasing speeds from left to right. top row shows the images captured by the camera. The second row shows the surface normals estimated by image-based SfP baseline Ba et al. [53] and last row shows the surface normals estimation by our learningbased SfP baseline.</figDesc><graphic url="image-116.png" coords="10,311.05,217.20,74.77,74.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. At low rotational speeds and low light conditions the number of positive events drastically exceeds 0.5 due to the background rate<ref type="bibr" target="#b23">[23]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. The total number of events (given in thousands) per rotation of the polarizer decreases at higher speeds and at lower illumination levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Acquisition time versus Mean Angular Error (MAE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of publicly available datasets for SfP.</figDesc><table><row><cell>Dataset</cell><cell>Modality</cell><cell>(Resolution) Size</cell></row><row><cell>Polar3D [18] DeepSfP [53] SPW [22]</cell><cell>6 Images(DoT) 4 Images(DoFP)</cell><cell>18 MP 1224 ? 1024 236 3</cell></row></table><note><p>4 Images(DoFP) 1224 ? 1024 522 ESfP-Synthetic (Ours) Events (DoT) + 12 Images(DoT) 512 ? 512 104 ESfP-Real (Ours) Events (DoT) + 4 Images (DoFP) 1280 ? 720 90</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Figure 4. Comparison of physics-based methods on ESfP-Synthetic.We report the angular error on the top-left corner of each picture. State-of-the-art comparison on ESfP-Synthetic in terms of accuracy and angular error. The Input column report whether the method uses events (E) or images (I), in which case the number of frames is also reported. We underline best results among the physics-based methods, and use bold text for learning-based ones.</figDesc><table><row><cell></cell><cell></cell><cell>71.9</cell><cell>64.0</cell><cell>39.4</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>74.7</cell><cell>70.8</cell><cell>60.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>89.9</cell><cell>78.3</cell><cell>28.3</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Input</cell><cell>Task</cell><cell>Angular Error ?</cell><cell cols="2">Accuracy ?</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mean</cell><cell cols="3">AE&lt;11.25 AE&lt;22.5 AE&lt;30</cell></row><row><cell cols="2">Mahmoud et al. [25] 4 I</cell><cell>Physics-based</cell><cell>80.923</cell><cell>0.034</cell><cell>0.065</cell><cell>0.085</cell></row><row><cell>Smith et al. [45]</cell><cell>4 I</cell><cell>Physics-based</cell><cell>67.684</cell><cell>0.010</cell><cell>0.047</cell><cell>0.106</cell></row><row><cell>Smith et al. [45] Ours (P)</cell><cell cols="2">12 I Physics-based E Physics-based</cell><cell>62.476 58.196</cell><cell>0.007 0.007</cell><cell>0.043 0.046</cell><cell>0.097 0.095</cell></row><row><cell>Ba et al. [53] Ours (L)</cell><cell cols="2">4 I Learning-based E Learning-based</cell><cell>24.509 27.953</cell><cell>0.357 0.263</cell><cell>0.623 0.527</cell><cell>0.718 0.655</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of physics-based methods using images<ref type="bibr" target="#b25">[25]</ref> and<ref type="bibr" target="#b45">[45]</ref> on the ESfP-Real. Our physics-based method results in sparse estimation as explained in Section 3.1</figDesc><table><row><cell></cell><cell>55.96</cell><cell>73.85</cell><cell>37.38</cell><cell></cell><cell></cell></row><row><cell></cell><cell>55.62</cell><cell>66.68</cell><cell>25.94</cell><cell></cell><cell></cell></row><row><cell></cell><cell>50.54</cell><cell>66.73</cell><cell>30.61</cell><cell></cell><cell></cell></row><row><cell>Figure 6. Method</cell><cell cols="2">Input Angular Error ?</cell><cell cols="2">Accuracy ?</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell cols="3">AE&lt;11.25 AE&lt;22.5 AE&lt;30</cell></row><row><cell>Smith et al. [45]</cell><cell>I</cell><cell>72.525</cell><cell>0.009</cell><cell>0.034</cell><cell>0.058</cell></row><row><cell>Mahmoud et al. [25] Ours (P)</cell><cell>I E</cell><cell>56.278 38.786</cell><cell>0.032 0.087</cell><cell>0.091 0.22</cell><cell>0.163 0.452</cell></row><row><cell>Ba et al. [53] Ours (L)</cell><cell>I E</cell><cell>26.157 26.677</cell><cell>0.103 0.105</cell><cell>0.467 0.452</cell><cell>0.710 0.691</cell></row></table><note><p>State-of-the-art comparison on ESfP-Real in terms of angular error and accuracy. We use underlined text to mark the best results within the physics-based category, and use bold text for best results among learning-based ones.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on the effect of using different angular velocities for the rotation of the polarizing lens on the proposed ESfP-Real dataset.</figDesc><table><row><cell>Speed (RPM)</cell><cell>Task</cell><cell>Angular Error ?</cell><cell cols="2">Accuracy ?</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell cols="3">AE&lt;11.25 AE&lt;22.5 AE&lt;30</cell></row><row><cell>57.5</cell><cell>Physics-based</cell><cell>44.29</cell><cell>0.01026</cell><cell cols="2">0.1294 0.1998</cell></row><row><cell>171.25</cell><cell>Physics-based</cell><cell>44.13</cell><cell>0.01028</cell><cell cols="2">0.1297 0.2000</cell></row><row><cell>308.75</cell><cell>Physics-based</cell><cell>44.02</cell><cell>0.01043</cell><cell cols="2">0.1302 0.2008</cell></row><row><cell>57.5 171.25 308.75</cell><cell>Learning-based Learning-based Learning-based</cell><cell>29.47 27.10 27.47</cell><cell>0.147 0.177 0.172</cell><cell>0.441 0.498 0.491</cell><cell>0.613 0.667 0.656</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison of event representations: The first two rows correspond to physics-based baseline. Rest of the rows correspond to learning-based approaches with different event representations.</figDesc><table><row><cell>Method</cell><cell>Dimension</cell><cell>Angular Error ?</cell><cell cols="2">Accuracy ?</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell cols="3">AE&lt;11.25 AE&lt;22.5 AE&lt;30</cell></row><row><cell>Events (P) [45]</cell><cell>12 ? H ? W</cell><cell>69.722</cell><cell>0.028</cell><cell>0.067</cell><cell>0.098</cell></row><row><cell>Events (P, specular)</cell><cell>12 ? H ? W</cell><cell>58.196</cell><cell>0.007</cell><cell>0.046</cell><cell>0.095</cell></row><row><cell>Event intensities</cell><cell>4 ? H ? W</cell><cell>39.316</cell><cell>0.147</cell><cell>0.321</cell><cell>0.402</cell></row><row><cell cols="2">VoxelGrid [54] -8Bins 8 ? H ? W</cell><cell>34.232</cell><cell>0.230</cell><cell>0.465</cell><cell>0.556</cell></row><row><cell>CVGR -4Bins</cell><cell>4 ? H ? W</cell><cell>34.053</cell><cell>0.220</cell><cell>0.494</cell><cell>0.579</cell></row><row><cell>CVGR -8Bins</cell><cell>8 ? H ? W</cell><cell>32.010</cell><cell>0.248</cell><cell>0.515</cell><cell>0.594</cell></row><row><cell>CVGR -12Bins CVGR-I</cell><cell>12 ? H ? W 8 ? H ? W</cell><cell>34.655 27.953</cell><cell>0.227 0.263</cell><cell>0.510 0.527</cell><cell>0.596 0.655</cell></row><row><cell>Scene</cell><cell>[53]</cell><cell cols="2">Events E (CVGR)</cell><cell cols="2">E&amp;I (CVGR-I)</cell></row><row><cell>18.06</cell><cell></cell><cell cols="2">25.48</cell><cell>19.64</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The contrast threshold can be quantified as the percentage of intensity change with respect to the previous intensity value.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/waps101/depth-from-polarisation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>This research was supported by SONY R&amp;D Center Europe and the National Centre of Competence in Research (NCCR) Robotics (grant agreement No. 51NF40-185543) through the Swiss National Science Foundation (SNSF).</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code, dataset and video are available under: https://rpg.ifi.uzh.ch/esfp.html https://youtu.be/sF3Ue2Zkpec</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polarisation photometric stereo</title>
		<author>
			<persName><surname>Gary A Atkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="158" to="167" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High-sensitivity analysis of polarization by surface reflection. Machine Vision and Applications</title>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">A</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-08">Aug. 2018</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view surface reconstruction using polarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><forename type="middle">R</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recovery of surface orientation from diffuse polarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><forename type="middle">R</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1653" to="1664" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simultaneous acquisition of polarimetric svbrdf and normals</title>
		<author>
			<persName><forename type="first">Seung-Hwan</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Daniel S Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="268" to="269" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagebased acquisition and modeling of polarimetric reflectance</title>
		<author>
			<persName><forename type="first">Seung-Hwan</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tizian</forename><surname>Zeltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inseung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 2020)</title>
		<meeting>SIGGRAPH 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="https://www.intelrealsense.com/coded-light/.2" />
		<title level="m">Intel RealSense Depth cameras</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Field Guide to Polarization</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Collett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE</title>
		<imprint>
			<date type="published" when="2002">Sept. 2005. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Google scanned objects: A highquality dataset of 3d scanned household items</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Kinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Hickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krista</forename><surname>Reymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unambiguous determination of shape from photometric stereo with unknown light sources</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Drbohlav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radim</forename><surname>Sara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jean-Denis</forename><surname>Durou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Falcone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvain</forename><surname>Qu?au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Tozza</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Advances in photometric 3d-reconstruction</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-03">2020. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A 1280x720 back-illuminated stacked temporal contrast event-based vision sensor with 4.86?m pixels, 1.066geps readout, programmable eventrate controller and compressive data-formatting pipeline</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Finateu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsumi</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Matolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koya</forename><surname>Tsuchimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Mascheroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Reynaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooria</forename><surname>Mostafalu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Chotard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Legoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirotsugu</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayato</forename><surname>Wakabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Oike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Posch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Solid-State Circuits Conf. (ISSCC), 2020. 3, 5</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">E-raft: Dense optical flow from event cameras</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Millh?usler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Division of focal plane asynchronous polarization imager</title>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Gruev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germain</forename><surname>Haessig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Joubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingkai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Polarization: Measurement, Analysis, and Remote Sensing XV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">12112</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape and refractive index from single-view spectro-polarimetric images</title>
		<author>
			<persName><forename type="first">Cong Phuoc</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Robles-Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="94" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?bastien</forename><surname>Speierer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merlin</forename><surname>Nimier-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delio</forename><surname>Vicini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tizian</forename><surname>Zeltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Nicolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://mitsuba-renderer.org.5" />
		<title level="m">Mitsuba 3 renderer</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning monocular dense depth from events</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Hidalgo-Carrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision.(3DV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polarized 3d: High-quality depth sensing with polarization cues</title>
		<author>
			<persName><forename type="first">Achuta</forename><surname>Kadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vage</forename><surname>Taamazyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2015. 1, 2, 3, 4, 9</date>
			<biblScope unit="page" from="3370" to="3378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Depth sensing using geometrically constrained po-larization normals</title>
		<author>
			<persName><forename type="first">Achuta</forename><surname>Kadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vage</forename><surname>Taamazyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="51" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">L</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. Int. Conf. Learn. Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Polarized reflection removal with perfect alignment in the wild</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001">June 2020. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shape from polarization for complex scenes in the wild</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Vladlen Koltun</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">June 2022. 1, 2, 3, 4, 6, 8</date>
			<biblScope unit="page" from="12632" to="12641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A 128x128 120dB 30mW asynchronous vision sensor that responds to relative intensity change</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Solid-State Circuits Conf. (ISSCC)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A 128?128 120 dB 15 ?s latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Direct method for shape recovery from polarization and shading</title>
		<author>
			<persName><surname>Ali H Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moumen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aly A</forename><surname>El-Melegy</surname></persName>
		</author>
		<author>
			<persName><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 19th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2012. 1, 3, 6, 7, 8</date>
			<biblScope unit="page" from="1769" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glass segmentation using intensity and spectral polarization cues</title>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Hwan</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001">June 2022. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Polarization-based inverse rendering from a single view</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Computer Vision</title>
		<meeting>Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transparent surface modeling from a pair of polarization images</title>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masataka</forename><surname>Kagesawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsushi</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Surface normal estimation of black specular objects from multiview polarization images</title>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Shigetomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Baba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryo</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsaku</forename><surname>Hiura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Asada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41303</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Polarization imaging applied to 3D reconstruction of specular metallic surfaces</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Stolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gorria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision Applications in Industrial Inspection XIII</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5679</biblScope>
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ESL: Event-based structure light</title>
		<author>
			<persName><forename type="first">Manasi</forename><surname>Muglikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision.(3DV)</title>
		<imprint>
			<date type="published" when="2021-12">December 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Diederik Paul Moeys, and Davide Scaramuzza. Event guided depth sensing</title>
		<author>
			<persName><forename type="first">Manasi</forename><surname>Muglikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-12">Dec. 2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shape and light directions from shading and polarization</title>
		<author>
			<persName><forename type="first">Trung</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hajime</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rin-Ichiro</forename><surname>Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="http://www.4dtechnology.com/products/polarimeters/polarcam/.2" />
		<title level="m">PolarM polarization camera</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Lucid Vision Phoenix polarization camera</title>
		<ptr target="https://thinklucid.com/product/phoenix-5-0-mp-polarized-model/.2" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="https://breakthrough.photography/products/x4-circular-polarizer.5" />
		<title level="m">Breakthrough Photography X4 Polarizer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A QVGA 143 dB dynamic range frame-free PWM image sensor with lossless pixel-level video compression and timedomain CDS</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Matolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Wohlgenannt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2003">Jan. 2011. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<ptr target="https://www.prophesee.ai/event-based-evk/,2020.4" />
		<title level="m">Prophesee Evaluation Kits</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reconstruction of specular surfaces using polarization imaging</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Rahmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Canterakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ESIM: an open event camera simulator</title>
		<author>
			<persName><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Robotics Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">High speed and high dynamic range video with an event camera</title>
		<author>
			<persName><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Instant dehazing of images using polarization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalized mosaicing: polarization panorama</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="631" to="636" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Height-from-polarisation with unknown lighting or albedo</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><surname>Tozza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009">2019. 6, 7, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hyunsurk Ryu, and Yongin Park. A 1280x960 Dynamic Vision Sensor with a 4.95-?m pixel pitch and motion artifact minimization</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Stolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Ferraton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Yunjae</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungnam</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masamichi</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongseok</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heejae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Hee</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seol</forename><surname>Namgung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwoo</forename><surname>Bong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun Seok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Circuits Syst. (ISCAS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="4218" to="4220" />
			<date type="published" when="2012">2012. 2020</date>
		</imprint>
	</monogr>
	<note>Optics letters</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Time lens++: Event-based frame interpolation with parametric non-linear flow and multi-scale fusion</title>
		<author>
			<persName><forename type="first">Stepan</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Bochicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Polarized opticalflow gyroscope</title>
		<author>
			<persName><forename type="first">Masada</forename><surname>Tzabari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Polarization vision: a new sensory approach to image understanding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><surname>Wolff</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="81" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Polarization image demosaicking using polarization channel difference prior</title>
		<author>
			<persName><forename type="first">Rongyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Express</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="22066" to="22079" />
			<date type="published" when="2002">Jul 2021. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deep shape from polarization</title>
		<author>
			<persName><forename type="first">Ba</forename><surname>Yunhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Jinfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yiqin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kadambi</forename><surname>Shi Boxin</surname></persName>
		</author>
		<author>
			<persName><surname>Achuta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 1, 2, 3, 4, 6, 7, 8, 10</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised event-based optical flow using motion compensation</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. Workshops (ECCVW)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Depth from a polarisa-tion+ rgb stereo pair</title>
		<author>
			<persName><forename type="first">Dizhong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William Ap</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7586" to="7595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
