<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning Hierarchical Representations for Image Steganalysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jian</forename><surname>Ye</surname></persName>
							<email>yejian2@mail2.sysu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jiangqun</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yang</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Tech-nology</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Xinhua College of Sun Yat-Sen University</orgName>
								<address>
									<postCode>510520</postCode>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Transactions on Information Forensics and Security</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning Hierarchical Representations for Image Steganalysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">98F22D332AD9E285502492B95E13982D</idno>
					<idno type="DOI">10.1109/TIFS.2017.2710946</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2017.2710946, IEEE Transactions on Information Forensics and Security 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Steganalysis</term>
					<term>Convolutional Neural Networks</term>
					<term>Feature Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays the prevailing detectors of steganographic communication in digital images mainly consist of three steps, i.e., residual computation, feature extraction and binary classification. In this paper, we present an alternative approach to steganalysis of digital images based on convolutional neural network (CNN), which is shown to be able to well replicate and optimize these key steps in a unified framework and learn hierarchical representations directly from raw images. The proposed CNN has a quite different structure from the ones used in conventional computer vision tasks (CVs). Rather than a random strategy, the weights in first layer of the proposed CNN are initialized with the basic high-pass filter set used in calculation of residual maps in Spatial Rich Model (SRM), which acts as a regularizer to suppress the image content effectively. To better capture the structure of embedding signals, which usually have extremely low SNR (stego signal to image content), a new activation function called truncated linear unit (TLU) is adopted in our CNN model. Finally, we further boost the performance of the proposed CNN based steganalyzer by incorporating the knowledge of selection channel. Three state-of-the-art steganographic algorithms in spatial domain, e.g., WOW, S-UNIWARD and HILL are used to evaluate the effectiveness of our model. Compared to SRM and its selection-channel-aware variant maxSRMd2, our model achieves superior performance across all tested algorithms for a wide variety of payloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the aim of revealing the presence of the hidden message in images. The state-of-the-art steganalyzers in spatial domain are the Spatial Rich Model (SRM) <ref type="bibr" target="#b3">[4]</ref> and its several variants <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. These steganalysis tools are constructed by assembling a rich model as a union of many diverse submodels formed by joint distributions of neighboring samples from quantized image noise residuals obtained using linear and non-linear high-pass filters. Recently, to better cope with the emerging content-adaptive steganographic schemes with ever-increasing security, some selection-channel-aware steganalysis feature sets <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> were proposed, among which, based on the rich media model and by utilizing the selection channel, the maxSRM <ref type="bibr" target="#b5">[6]</ref> improved the detection of all content-adaptive steganographic schemes in spatial domain to a varying degree.</p><p>Currently the best image steganalyzers are built using feature-based steganalysis and machine learning and share the same pipeline: namely, noise residual computation, feature construction and binary classification. The success of the feature-based steganalysis depends heavily on the process of feature engineering, i.e., using the domain knowledge, e.g., the cover source model and the behavior of its opponent, to create features that make machine learning algorithms work. For the pipeline described above, the residuals help to improve the SNR of stego signal, and the state-of-the-art feature sets are unions of co-occurrences of different filter residuals, so-called rich models, which tend to be high-dimensional (e.g., <ref type="bibr" target="#b29">30</ref>,000 or more). From the perspective of steganalysis, to obtain a more complete description of cover source, high-dimensional representation is an inevitable trend, indicating that the features for steganalysis become increasingly complicated. In addition, note that current state-of-the-art steganalysis features are heuristically designed and the optimization of classifier is independent of the feature extraction step. In other words, the pipeline of steganalysis has barely been optimized in a unified framework.</p><p>In this paper, we show that the pipeline of steganalysis can be alternately implemented by a deep convolutional neural network (CNN) <ref type="bibr" target="#b7">[8]</ref> to learn the optimized deep hierarchical representations for image steganalysis. An important property of CNN is that it can extract complex statistical dependencies from high-dimensional sensory input and efficiently learn deep (hierarchical) representations by re-using and combining intermediate concepts, allowing it to generalize well across a wide variety of computer vision (CV) tasks, including image classification <ref type="bibr" target="#b8">[9]</ref>, face recognition <ref type="bibr" target="#b9">[10]</ref>, and many others. It naturally motivates us to consider training a CNN to distinguish covers from stegos. In this way, the raw image to be detected can be directly mapped to a binary label (cover or stego) using the trained CNN. Furthermore, the feature extraction can be optimized together with the classifier, which helps to relieve us from the complicated feature-design step.</p><p>The first attempt of using CNNs to steganalysis is Qian et al.'s work <ref type="bibr" target="#b10">[11]</ref>, where a Gaussian-Neuron Convolutional Neural Network (GNCNN) was proposed for image steganalysis in spatial domain. By using the Gaussian function instead of the ReLU or sigmoid in conventional CNNs as the activation function, the GNCNN achieves a comparable performance to SRM on BOSSbase <ref type="bibr" target="#b11">[12]</ref>. Recently, Xu et al. <ref type="bibr" target="#b12">[13]</ref> investigated the design of CNN structure specific for image steganalysis applications, which is featured in <ref type="bibr" target="#b0">(1)</ref> embedding an absolute activation (ABS) layer in the first convolutional layer to improve the statistical modeling in the subsequent layers; (2) applying the TanH activation function at early stages of networks to prevent overfitting; (3) performing batch normalization (BN) immediately before each nonlinear activation layer. Their results show that a well-designed CNN has the potential to provide better detection performance in steganalysis. The authors extended their works later in <ref type="bibr" target="#b13">[14]</ref> by employing the network in <ref type="bibr" target="#b12">[13]</ref> as base learners for ensemble classifier and obtained the results that can rival the SRM.</p><p>In this paper, we develop a supervised CNN model specific to steganalysis applications. The proposed CNN shows several prominent characteristics different from other CNNs, which are summarized as follows: <ref type="bibr" target="#b0">(1)</ref> The first layer in the proposed CNN serves as the pre-processing module for noise residuals computation. Instead of the random strategy, the weights of the first layer are initialized with all the 30 basic filters used in the computation of residual maps in SRM <ref type="bibr" target="#b3">[4]</ref>, which corresponds to 30 output feature maps of the first layer and helps to accelerate the convergence of the network. ( <ref type="formula" target="#formula_1">2</ref>  <ref type="bibr" target="#b10">[11]</ref>, and outperforms the current state-of-the-art handcrafted feature sets, e.g., SRM <ref type="bibr" target="#b3">[4]</ref> and maxSRMd2 <ref type="bibr" target="#b5">[6]</ref>, by a clear margin.</p><p>The rest of the paper is organized as follows. In Section II, we present a brief review of the framework of the prevailing image steganalysis methods in spatial domain and the convolutional neural networks (CNNs). The structure of the proposed CNN is described in Section III, which is followed by the experimental results and analysis in Section IV. Finally, the concluding remarks are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Framework of Prevailing Image Steganalysis Methods</head><p>The well-established paradigm <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> of image steganalysis consists of three major steps, i.e., noise residual computation, feature extraction and binary classification as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>1) Noise residual computation: The embedding operation in steganography can be viewed as adding extremely low amplitude noise to the cover. Therefore, it is wiser to model the noise residuals instead of raw pixels in steganalysis. Such an idea was initially proposed in <ref type="bibr" target="#b16">[17]</ref> and was later adopted and developed in several subsequent methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b3">[4]</ref>. For a test image X = (x ij ), a popular strategy in steganalysis is to compute the noise residuals R = (r ij ) from a pixel predictor:</p><formula xml:id="formula_0">r ij = P red(N (x ij )) -lx ij ,<label>(1)</label></formula><p>where N (x ij ) is a set of neighboring pixels of x ij , l ∈ N is the residual order, and P red(•) is the adopted predictor.</p><p>In practice, many steganalysis schemes <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref> implement the predictor by convolving a finite impulse response filter K with image X:</p><formula xml:id="formula_1">R = X * K -lX = (r ij ) = ( r,c x rc ij k rc -lx ij ),<label>(2)</label></formula><p>where * denotes the convolution operator, and r, c are the index of the kernel K . According to the distributive law, the residuals above can be reformulated as:</p><formula xml:id="formula_2">R = X * K = (r ij ) = ( r,c</formula><p>x r,c i,j k rc ).</p><p>There are plenty of choices for the filters (linear or nonlinear) in steganalysis, which can be used to generate different residuals and capture different dependencies among neighboring pixels. The diversity of residuals is fundamental to the success of the so-call rich media models (RM).</p><p>2) Feature extraction: This is critical in steganalysis. With more discriminative features, it would be much easier to distinguish cover images from stego ones. In this step, the joint or conditional probability distributions of neighboring residuals are modeled through histograms or co-occurrences. For SRM and its several variants, the features are built on the basis of fourth order co-occurrence matrixes. Take horizontal co-occurrence for example, we have:</p><formula xml:id="formula_4">c h d0d1d2d3 = n1,n2-3 i,j=1 [r i,j+k = d k , ∀k = 0, 1, 2, 3] • ϕ(β i,j ) d k ∈ {-T q, (-T + 1)q, • • • , T q} ,<label>(4)</label></formula><p>where [•] is Iverson bracket whose result is 1 when statement inside is true and 0 otherwise. And ϕ(β i,j ) is a statistical measure of the corresponding embedding probability. For different versions of SRM, there are different values for ϕ(β i,j ) :</p><formula xml:id="formula_5">ϕ(β i,j ) =      1, SRM max(2β i,j+k ), k = 0, 1, 2, 3, maxSRM [β ij ≥ β threshold ], tSRM .<label>(5)</label></formula><p>Note that the ϕ(β i,j )s in maxSRM and tSRM vary from one place to another, indicating that both of them are selectionchannel-aware.</p><p>3) Binary classification: The final step of steganalysis is to classify an image as a cover or a stego using an elaborately designed classifier (support vector machine (SVM) or ensemble classifier), which needs to be trained through supervised learning prior to practical application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional Neural Network Architecture</head><p>A convolutional neural network consists of one or several convolutional layers, followed by some fully-connected layers of neurons. The input and output of a convolutional layer are sets of arrays called feature maps, while each convolutional layer usually produces feature maps by a three-step process, i.e., convolution, non-linear activation and pooling. The first step performs some filtering using k kernels leading to k new feature maps. Therefore, each kernel is applied on the existing feature maps resulting from the previous layer. Let us denote by F n (X) the output feature map in layer n with the kernel (filter) and bias defined by W n and B n , respectively, we have:</p><formula xml:id="formula_6">F n (X) = pooling(f n (F n-1 (X) * W n + B n )),<label>(6)</label></formula><p>where F 0 (X) = X is the input data, f n (•) is a non-linear activation function that applies to each element of its input, e.g., TanH or ReLU function, and pooling(•) represents the pooling operation, including mean-pooling or max-pooling, etc. Generally speaking, the non-linear activation and pooling operation are optional in a specific layer. For a classification problem, a complete network usually contains several cascaded convolutional layers and ends with one fully-connected layer followed by a softmax classifier. Obviously, the three key steps in the framework of modern steganalysis can be well simulated by a CNN model described above. According to (3), the residual computation is actually implemented through convolution which can be achieved by a convolutional layer. The cascade of multiple convolutional layers in a CNN can be trained to learn or extract high-level and discriminative representation or features of the original data, which explains the success of CNNs in many image and video recognition problems, and that also coincides with the objective of the feature extraction in steganalysis. As for the classification step, the softmax classifier in CNN acts like the SVM or ensemble classifier. In fact, a CNN based steganalyzer would allow to automatically unify residual computation, feature extraction and classification steps in one unique architecture without any a prior feature selection and to be optimized simultaneously as a whole framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED CONVOLUTIONAL NEURAL NETWORK</head><p>FOR STEGANALYSIS From the analysis in Section II, a CNN model is shown to be able to well simulate the three key steps in the framework of modern steganalysis. Therefore, it is not only realistic but also attainable to develop image steganalyzer via convolutional neural networks. However, the steganalysis task is quite different from the ones in computer vision, where the CNNs have made great success. The stego noise to deal with in steganalysis usually cannot be perceived by human perceptual system. In fact, with elaborately designed steganographic schemes, the stego usually closely resembles the cover not only visually but also statistically. As a result, the feature representations in CNN based steganalyzer should be a lot different than the ones in conventional CV tasks. In light of this, it is not surprising to find that a CNN with random initialized weights usually cannot converge when it is trained as a steganalyzer (see Table <ref type="table" target="#tab_0">I</ref>). Therefore, some customized designs specific to steganalysis are required in order to incorporate the domain knowledge into the learning of CNN based steganlyzer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Architecture</head><p>As illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>, the proposed CNN consists of 10 layers and ends with a fully-connected layer with a 2way softmax, which produces the distribution over 2 class labels. Non-linear activation is applied after every convolution operation. And the pooling operations from 1st to 3th layers are suppressed. Different from other conventional CNN architectures that employ two or more fully-connected layers, we use only one necessary 2-way fully-connected layer at the end of our network. This is because the fully-connected layer usually involves too many parameters to be trained, and this could easily lead to overfitting, especially when the training set is not big enough, which is the case for our task. Besides, except the layers illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>, there is no other layer, such as Local Response Normalization (LRN) <ref type="bibr" target="#b8">[9]</ref>, dropout <ref type="bibr" target="#b8">[9]</ref>, Batch Normalization (BN) <ref type="bibr" target="#b19">[20]</ref> or Local Contrast Normalization (LCN) <ref type="bibr" target="#b20">[21]</ref>, used in the network. The components inside the dotted box show the first two operations when the knowledge of selection channel is explored, which will be detailed in Section III-D. The depth and width of the network and the size of filters are determined by experiments based on the tradeoff between performance and model complexity.</p><p>We then proceed to discuss the naming conventions for the CNN and the way we repeat our experiments on the proposed models, which will be adopted throughout this paper. For the network whose non-linear activation functions are ReLUs, we call it ReLU-CNN, while if some of the activation functions are replaced by the new introduced truncated linear unit (TLU), which will be later elaborated in Section III-B, it is referred as TLU-CNN. In addition, the TLU-CNN becomes its Selection-Channel-Aware version SCA-TLU-CNN, when the statistical measure of embedding probabilities is incorporated in the network design. For all of the experiments on the CNN models, we repeat the training and testing procedure for three times using three different training/validation/test sets. The final experimental results are obtained by averaging three test results. Unless otherwise specified, the involved networks for the experiments in this paper are trained with resampled images on BOSS+BOWS2+AUG dataset and tested with resampled images on BOSS test (see Section IV-B for details).</p><p>For the rest of this Section, we elaborate several key techniques employed in the design of our proposed convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Initialization with High-Pass Filters in SRM</head><p>As mentioned earlier in Section II, the computation of residuals can be well simulated by a convolutional layer. Inspired by this, we initialize the weights of the first convolutional layer in our convolutional neural network with high-pass filter kernels used in SRM instead of random values. Although such strategy was also utilized in Qian et al.'s work <ref type="bibr" target="#b10">[11]</ref>, only the "square 5 × 5" filter in SRM was used to initialize their first layer in the GNCNN model. According to our understanding, the residuals in SRM help to improve the SNR (stego signal to image content) and it is the combination of different filter residual models that makes the success of the rich models (RM) in steganalysis. Consequently, we propose to increase the width of the first convolutional layer and initialize the weights with the kernels of all the 30 basic linear filters (the "spam" filters and their rotated counterparts) used in calculating residual maps in SRM.</p><p>The basic filters above correspond to 7 residual classes in SRM, which include 8 filters in class "1st", 4 in class "2nd", 8 in class "3rd", 1 in class "SQUARE 3 × 3", 1 in class "SQUARE 5 × 5", 4 in class "EDGE 3 × 3" and 4 in class "EDGE 5 × 5", for a total of 30 basic filters with maximum kernel size of 5 × 5. Therefore, we set the kernel size of weighting matrix in the first convolutional layer of our CNN all to 5 × 5 as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. Let W 5×5 CN N and W m×n SRM be the weight matrix and filter kernel in SRM, respectively, we initialize the central part of W CN N with W SRM and leave the remaining elements of W CN N to be zeros. In another word, we pad W m×n SRM to be the W 5×5 CN N with zeros. It is worth noting that for all the SRM filters, we do not normalize them by dividing the residual orders l in formula <ref type="bibr" target="#b0">(1)</ref>.</p><p>The above initialization strategy acts as a regularization term in machine learning, which dramatically narrows down the feasible parameter space and helps to facilitate the convergence of the network. Besides, these high-pass filters make our network concentrate on the embedding artifacts introduced by steganography rather than the complex image content. To the best of our knowledge, all of the CNN models that are trained for steganalysis adopt a similar initialization strategy <ref type="bibr" target="#b10">[11]</ref> [13] <ref type="bibr" target="#b21">[22]</ref>.</p><p>However, the initialization with the 30 SRM filters in the first layer serves as a good starting point for the network training but not the best end point. According to our experiments (see Table <ref type="table" target="#tab_0">I</ref>), keeping these filters unchanged during training usually leads to worse results than updating them. As a result, all the filters in the first convolutional layer should be optimized through training together with other parameters in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Truncated Linear Unit 1) Motivation:</head><p>The activation function f (•) : R → R introduces non-linearity to neural networks, which can significantly increase the capability of feature representation. Various choices are possible for f (•), such as the conventional sigmoid and hyperbolic tangent function, or the recently emerged ReLU (Rectified Linear Unit) function. Among them, ReLU is a notable choice for the convolutional layer in CNN and it can be formulated as</p><formula xml:id="formula_7">f (x) = 0, x &lt; 0 x, x ≥ 0 . (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>The ReLU function has been successfully applied to a wide variety of tasks in CV. For CV tasks, e.g., object classification, the target object can usually be distinguished easily from the background. In another word, the signals in such tasks are of high SNR. Under this circumstance, applying ReLU to neurons can make them selectively respond to useful signals in the input, resulting the so-called sparse features. Both theoretical and empirical arguments show that sparse representations are more likely to be linearly separable and have better generalization ability <ref type="bibr" target="#b22">[23]</ref>. However, the picture is completely different for our task in steganalysis. The steganographic embedding procedure can be viewed as adding low-amplitude additive noises to cover images. And the embedding signals have much low amplitude compared to the image content, which means extremely low SNR. In contrast to CV tasks or some high SNR applications, where the ReLU function can well adapt to the distributions of the object signals, the activation function adopted in steganalysis should take into account the structure of the embedding signals, especially in the first several convolutional layers. Notice that in image steganography, the embedding signals are usually in the range of [-1,1], a new activation function known as truncated linear unit (TLU), which is slightly modified from ReLU, is introduced in our proposed CNN and defined as follows:</p><formula xml:id="formula_9">f (x) =      -T, x &lt; -T x, -T ≤ x ≤ T T, x &gt; T ,<label>(8)</label></formula><p>where T &gt; 0 is the parameter determined by experiments.</p><p>In the proposed CNN, the weighting kernels in the first convolutional layer are initialized with the basic high-pass filters in SRM, which contributes to the suppression of image content and extraction of embedding signals. The utilization of TLU in the outputs of the first convolution operation helps to:</p><p>(1) adapt to the distribution of embedding signals (with low SNR); and (2) enforce the CNN to learn more effective highpass filters in the first layer. According to our experiments, for other layers of the CNN based steganalyzer, the distributions of the input signals tend to be more consistent with the ones in conventional CV tasks. Therefore, the ReLU function is more preferable in those layers.</p><p>2) Experimental Verification and Analysis: We then proceed to verify the effectiveness of TLU in steganalysis and determine the proper T through experiments. In addition, we also try to explain the function of TLU with the help of visualization tools.</p><p>We conducted the comparisons based on the deep convolutional model shown in Fig. <ref type="figure" target="#fig_2">2</ref>. The CNN with ReLU as activation function in all its layers, i.e., ReLU-CNN, is trained as the baseline model. The TLU-CNN with ReLU replaced by TLU in its first convolutional layer is also adopted for comparison.</p><p>Both ReLU-CNN and TLU-CNN with different T are trained for HILL, S-UNIWARD and WOW, at the payload of 0.2 bpp. The experimental results are summarized in Table <ref type="table" target="#tab_1">II</ref>.</p><p>It is observed that, for all the three involved steganographic schemes, the TLU-CNN achieves consistently better performance in terms of detection error than the baseline ReLU-  CNN for most of the tested parameter values, with the best performance obtained when T = 3 or T = 7. With increasing value of T , the effect of TLU to suppress image content gradually decreases, leading to the loss in performance. Note that in the extreme case when T = ∞, TLU becomes an identity function (linear activation function). It is interesting to see that, even with the linear activation function which will decrease the non-linearity, the TLU-CNN can still have comparable performance to ReLU-CNN. This may result from the effect of ReLU which sets all the negative inputs to be zeros, leading to about 50% information loss in the embedding signals.</p><p>As an added bonus, it is also observed that the TLU-CNN can be trained much faster than its counterparts with ReLU and TanH unit. The convergence performance when training the three involved networks (TLU with T = 3, ReLU and TanH) is illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>, which shows the evolution of training error versus the number of epochs on the training images, obtained for S-UNIWARD at 0.2 bpp. The TLU is specifically designed according to the distribution of the embedding signals. Therefore, it is able to make better use of the information to train the network more efficiently.</p><p>To better illustrate the superiority of the TLU-CNN for steganalysis, we then visualize the filters in the first convolutional layer trained with TLU and ReLU, respectively. Fig. <ref type="figure" target="#fig_4">4</ref> (a) and (b) show the visualizations of filters (30 filters in total) in the first convolutional layers with ReLU and TLU (T = 3). It is clear to see that the adoption of TLU results in more distinctive features and fewer "dead" filters (filters corresponding to the feature boxes that are almost pure white). Note that although the learnt filters have similar shapes with the original SRM filters, they actually have different values. From the results in Table <ref type="table" target="#tab_0">I</ref>, it can be easily verified that the network can indeed fine-tune the SRM filters in the first layer.</p><p>It is also interesting to evaluate the performance of the proposed CNN based steganalyzer when the ReLUs in first couple of layers are replaced by TLUs. The network is named as TLU n if the ReLUs in its first n convolutional layers are replaced by TLUs and the ReLUs in other layers remain unchanged. As shown in Table <ref type="table" target="#tab_2">III</ref>, despite of the similar results from TLU 1 to TLU 3, the detection accuracy becomes worse from TLU 4 and beyond. This is because when the network becomes deeper, the distribution of TLU output tends to be consistent with the one of ReLU. Therefore it is preferable to use ReLUs in relatively deep convolutional layers. Considering that the computation of ReLU can be accelerated by CuDNN library and TLU 3 needs more training epochs, we choose TLU 1 as the TLU-CNN model in our implementation so as to achieve a good compromise between training time and detection performance.  <ref type="bibr" target="#b23">[24]</ref> from cryptography, to evaluate the security performance of a steganographic scheme, each element of the scheme (embedding, detection, etc) should be declared public except for the secret key. For image steganography, the probability of each pixel being modified, i.e., the so-called selection channel, when executing embedding could also be known by the steganalyst. By incorporating the knowledge of selection channel, the performance of steganalyzers against modern content-adaptive steganographic schemes is expected to be improved. For those recently proposed selection-channelaware SRM feature sets, e.g., tSRM, maxSRM and σSRM <ref type="bibr" target="#b24">[25]</ref>, some statistical measures of the embedding probabilities are accumulated when calculating the co-occurrences or histograms of the corresponding residuals. For our CNN model, however, we do not explicitly compute the co-occurrences or histograms. Therefore, we have to find another way to exploit the embedding probabilities in the design of CNN based steganalyzer.</p><p>Inspired by the work in <ref type="bibr" target="#b24">[25]</ref>, we choose to take the upper bound of the expectation of L 1 norm of the residual distortion as the statistical measure of selection channel. For a cover image X = (x ij ), and the corresponding stego Y = (y ij ), we denote the difference between stego and cover by</p><formula xml:id="formula_10">N = Y -X = (y ij -x ij ) = (n ij ).<label>(9)</label></formula><p>For most of the existing steganographic schemes, a pixel x ij is modified into x ij + 1 and x ij -1 with the same probability β ij , we then have</p><formula xml:id="formula_11">n ij ∈ {-1, 0, 1} with probability {β ij , 1 - 2β ij , β ij }, therefore, E[n ij ] = 0, E[|n ij |] = 2β ij .<label>(10)</label></formula><p>Recall that residuals can be computed by convolutions with high-pass filters. For filter kernel K, the residual distortion can be formulated as:</p><formula xml:id="formula_12">D = K * Y -K * X = K * (Y -X) = K * N = r,c k rc n rc ij = (d ij ) ,<label>(11)</label></formula><p>where r, c are the index of the filter kernel. And it is easy to verify that:</p><formula xml:id="formula_13">E[d ij ] = r,c k rc • E[n rc ij ] = 0.<label>(12)</label></formula><p>To make use of the selection channel, the standard deviation Std[d ij ] or the expectation of the L 1 norm of d ij seems to be a natural choice:</p><formula xml:id="formula_14">Std[d ij ] = 2 r,c (k rc ) 2 β rc ij E[|d ij |] = E[| r,c k rc n rc ij |] ,<label>(13)</label></formula><p>where β ij is the embedding probability for x ij . Note the fact that the computation of both Std[d ij ] and E[|d ij |] is not computationally efficient on a GPU platform, we then turn to resort to the upper bound of ij |] for the statistical measure of embedding probabilities:</p><formula xml:id="formula_15">ϕ(β ij ) = 2 r,c |k rc | • β rc ij = E[ r,c |k rc | • |n rc ij |] ≥ E[| r,c k rc n rc ij |] = E[|d ij |] . (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>The ϕ(β ij ) above can be easily obtained by convolving the probability map P = (p ij ) = (2β ij ) with the absolute value of the residual filter K. Therefore, the upper bound map for the L 1 norm of the residual distortion d ij , i.e., ϕ(P ) is obtained:</p><formula xml:id="formula_17">ϕ(P ) = P * |K|.<label>(15)</label></formula><p>We then try to take advantage of the selection channel in the design of a convolutional neural network. For maxSRM <ref type="bibr" target="#b5">[6]</ref> and σSRM <ref type="bibr" target="#b24">[25]</ref>, the statistical measure of probabilities are accumulated in the bins of co-occurrences as the final features. Therefore, we should also try to propagate the computed ϕ(P ) through the whole network and make it contribute to the final features extracted by the CNN. There are two simple ways that can achieve this. One is applying an elementwise summation between ϕ(P ) and the output feature maps of the first convolutional layer, and the other is applying an elementwise multiplication. Our experimental results indicated that the elementwise summation approach always performs much better than the elementwise multiplication. It is most likely that with elementwise multiplication, the distribution of the output feature maps in the first layer (act as the residuals in SRM) will be changed too much, which inhibits to a great extent the feature extraction in the subsequent layers. Therefore, the elementwise summation is adopted in our network and the output of the second convolutional layer then becomes (refer to (6)):</p><formula xml:id="formula_18">F 2 (Z) = f 2 ((F 1 (Z) + ϕ(P )) * W 2 + B 2 ) = f 2 (F 1 (Z) * W 2 + B 2 + ϕ(P ) * W 2 ) .<label>(16)</label></formula><p>Note that, for our proposed CNN, except for the first convolutional layer, the non-linear activation functions in other layers are ReLUs, which means that for neurons that are activated the second layer, their outputs can be expressed as:</p><formula xml:id="formula_19">F 2 (Z) activated = (F 1 (Z) * W 2 + B 2 ) + ϕ(P ) * W 2 , (<label>17</label></formula><formula xml:id="formula_20">)</formula><p>where the first term above is the original output of the neuron and the second one can be regarded as a weighted sum of the statistical measure of selection channel. Recall that, for the activated neurons, the ReLU is a linear operator, thus the outputs F n (Z) activated of the n th layer can also be factorized into two terms as the one in <ref type="bibr" target="#b16">(17)</ref>, and the effect of the statistical measure is accumulated hierarchically through the forward propagation along the network. As a result, the obtained features, or the input of the fully-connected softmax classifier (see Fig. <ref type="figure" target="#fig_2">2</ref>), are the combination of features from two separate sources, one of which is extracted from the test image and the other from the selection channel of the same image. Note that according to <ref type="bibr" target="#b14">(15)</ref>, the statistical measure ϕ(P ) is computed based on the learnt filter K, and therefore, during training, ϕ(P ) does not participate in back propagation to update filter K.</p><p>2) Experimental Verification and Analysis: We then proceed to conduct several experiments to verify the effectiveness of the CNN based steganalyzer when the knowledge of selection channel is properly utilized. In our experiments, the SCA-TLU-CNN stands for the network that makes use of selection channel. First, the SCA-TLU-CNN can be trained more efficiently when incorporated with the information of selection channel as illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>. It is also observed that the SCA-TLU-CNN can learn more effective filters as shown in Fig. <ref type="figure" target="#fig_4">4 (c)</ref>, where the filters in the first convolutional layer of SCA-TLU-CNN are more distinct than the ones of TLU-CNN and ReLU-CNN, and there are no "dead" filters any more.</p><p>Moreover, compared to the TLU-CNN, the detection error can also be further decreased as expected. Table <ref type="table" target="#tab_3">IV</ref> shows the performance comparison between these two CNN models against three state-of-the-art content-adaptive steganographic schemes, i.e, S-UNIWARD, HILL, and WOW at 0.2 bpp. It is clear to see that there is about 3% decrease in terms of detection error for all the involved embedding schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Curriculum Learning for Low Payload Steganalysis</head><p>Nowadays it is still quite challenging to distinguish cover images from stego ones that are generated by some stateof-the-art steganographic schemes at very low payload, e.g., 0.1 bpp or below. In fact, we found that the proposed CNN described in Fig. <ref type="figure" target="#fig_2">2</ref> usually cannot converge if we train the network from scratch on those images with very low embedding rate. In our work, however, the issue above can be well solved by adopting a curriculum learning <ref type="bibr" target="#b25">[26]</ref> or transfer learning strategy <ref type="bibr" target="#b26">[27]</ref>. It benefits from the observation that humans can learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more complex concepts. To put the strategy into practice, we train the network from easy aspects of the steganalysis task, and gradually increase the difficult level. In another word, we first train a network on a dataset generated at a higher embedding rate and then fine-tune it on another dataset generated at a relatively low embedding rate, and so on. Unless otherwise specified, the results on low payload steganalysis (0.1 bpp and 0.05 bpp), which are presented later in Section IV, are all based on the CNN models trained with the strategy of curriculum learning. For instance, to train a steganalyzer at 0.1 bpp, we first train a model from scratch on dataset at 0.2 bpp, then fine-tune the final model on dataset at 0.1 bpp. And similarly, the model at 0.05 bpp is obtained based on the final model at 0.1 bpp and so on. Note that this curriculum learning strategy also applies to the training of networks with images generated by subsampling (detailed in Section IV-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND ANALYSIS</head><p>In this Section, extensive experiments are carried out to demonstrate the feasibility and effectiveness of our proposed CNN model. We compare our model with the state-of-the-art hand-crafted feature set SRM and its selection-channel-aware variant maxSRMd2. For fair comparison, all the involved steganalysis methods are tested on the same datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Steganographic Schemes</head><p>In our experiments, several state-of-the-art content-adaptive steganographic methods in the spatial domain, e.g., S-UNIWARD, WOW and HILL, are employed to evaluate the performance of the involved steganalyzers. And all the embedding algorithms are implemented with STC simulator based on the publicly available codes. Note that in our implementation, instead of the C++ code version of the simulator tools (S-UNIWARD, WOW) with fixed embedding key, we use the ones in Matlab code with random embedding key. This is because we found in our experiments that, although the CNN based steganalyzer could achieve extraordinary detection performance (say, detection error is less than 0.1 for WOW in 0.2 bpp) if the CNN was trained on dataset generated by simulator with a fixed embedding key, its performance would decrease dramatically (detection error is close to 0.5) when the test dataset was generated using another embedding key.In another word, the trained CNN was overfitted to the specific embedding key in training set and had no generalization capability at all. The similar problem was also reported in <ref type="bibr" target="#b21">[22]</ref>, where the authors made use of the same embedding key to create stego images for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Datasets and Data Augmentation</head><p>In this paper, the involved experiments are carried out on two image sources. The first comes from the BOSSbase 1.01 <ref type="bibr" target="#b11">[12]</ref>, which contains 10,000 512×512×8-bit grayscale images with different texture characteristics and is widely used in steganalysis. The other one is BOWS2 <ref type="bibr" target="#b27">[28]</ref>, which is used for BOWS2 contest and consists of downsampled and cropped natural and grayscale images of size 512 × 512 × 8-bit. Constrained by our available GPU computing platform, conducting experiments on full resolution images of 512 × 512 pixels can be extremely time consuming. As a result, we decide to evaluate the performance of our CNN based steganalyzer on test images of 256 × 256 pixels. To this end, we generated 3 image datasets from both image databases above in different ways as described below:</p><p>a) resample all the images into the size of 256 × 256 pixels (using "imresize()" in Matlab with default settings); b) crop the central part of the original images into size of 256 × 256 pixels; c) subsample the original images to 256 × 256 by skipping every other pixels. For each image dataset, we then prepared 3 training sets and 1 testing set separately as follows:</p><p>1) training set BOSS: it contains 5,000 images randomly selected from BOSSbase (including 1,000 randomly selected images as validation set); 2) training set BOSS+BOWS2: it contains the images in training set BOSS and another 10,000 images from BOWS2;  3) training set BOSS+BOWS2+AUG: it is obtained by performing some label-preserving transformations, such as transposing and rotating, on the images in BOSS+BOWS2, which increases the size of BOSS+BOWS2 training set by a factor of 8; 4) testing set BOSS test: it contains the remaining 5,000 images in BOSSbase other than the ones in training set BOSS. For CNN based steganalysis, it is preferable to adopt a larger training set to avoid overfitting. Tables V -VII summarizes the performance of our CNN based steganalyzer and other two competing steganalysis schemes trained on different training sets and tested on BOSS test of resampled, cropped and subsampled images respectively, for WOW at 0.2 or 0.8 bpp. It is observed that, the proposed TLU-CNN suffers from substantial overfitting when it is trained on BOSS. Its performance is improved to a certain degree when the training set is replaced with BOSS+BOWS2. And the best performance is achieved if we train the network using BOSS+BOWS2+AUG. However, things are different for the involved hand-crafted feature sets, e.g., SRM and maxSRMd2. For resampled images, the better choice is BOSS+BOWS2. But for cropped and subsampled images, there is no clear difference in performance on BOSS and BOSS+BOWS2. The experiments for SRM and maxSRMd2 on BOSS+BOWS2+AUG are pointless because these features are already symmetrized. The rotated or mirrored images will cause duplicated features and singular matrices in the base FLD learners of the ensemble classifer. Therefore, for the image dataset adopted in this paper and in the interest of fairness, the training set for SRM and maxSRMd2 is BOSS+BOWS2, while our CNN models use the BOSS+BOWS2+AUG. And the performance of all involved schemes is evaluated on BOSS test described above. For repeating the experiments, we create three different training sets and test sets. Every CNN model will be trained independently on the three training sets using the same hyper-parameters and tested on their associated test sets. Then the test results are averaged as the final performance of this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We implemented the proposed CNN models using Caffe <ref type="bibr" target="#b28">[29]</ref> with necessary modifications. It is worth noting that, instead of SGD, we use AdaDelta <ref type="bibr" target="#b29">[30]</ref> to train our networks as we found in our early experiments that with AdaDelta the networks can learn much faster and achieve better results. Accordingly, all the following parameters we described are based on AdaDelta: the mini-batch size is 32, which contains 16 pairs of cover and stego images; the momentum value is 0.95 and the weight decay is 5 × 10 -4 ; the "delta" parameter for AdaDelta is 1 × 10 -8 . Data augmentation is conducted during training and the same rotated or mirrored operation is applied to a pair of images within a mini-batch. "Xavier" initialization <ref type="bibr" target="#b30">[31]</ref> is used to initialize the weights from 2nd to 9th layers and their initial biases are set to be 0.2. The last fully-connected layer is initialized with random values obtained from a Gaussian source of zero mean and standard deviation 0.01 and the initial bias is set to be zero. Based on the above settings, the networks are then trained to minimize the cross-entropy loss.</p><p>During training, we use the "multistep" policy in Caffe to adjust the learning rate. When the training iteration is equal to one of the specified step values, the learning rate will be divided by 5. Take TLU-CNN for WOW at 0.2 bpp on resampled images as an example, with an initial value of 0.4, the learning rate will be decreased to 0.08, 0.016 and 0.0032 at iterations 500,000, 600,000 and 650,000 respectively <ref type="foot" target="#foot_0">1</ref> . Note that with different embedding schemes at different payloads, we are actually training the CNNs for tasks of varying difficulties, which means that different configurations should be applied to control the learning rate. Owing to space constraints, it is impractical for us to give all the step values for each involved CNN models. As an alternative, we turn to elaborate the rules on how to determine those step values. The key to tackling the problem lies in monitoring the error and accuracy on the validation set during training. When neither the error decreases nor the accuracy increases, the learning rate should be changed. A similar policy was also adopted in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b31">[32]</ref>. Note that for each model, we create three different training/validation/test sets using the way described in Section IV-B and choose the step values for this model from the first trining/validation set under the rules described above. The same step values are used when training on the other two training sets.</p><p>For TLU-CNN models corresponding to resampled and cropped images at payloads from 0.2 to 0.5 bpp and subsampled images at 0.8 bpp, the network parameters are trained from scratch and are stopped at 100 epochs with an initial learning rate of 0.4. The strategy of curriculum learning is applied to train the models corresponding to other stego images at lower embedding rates, to be specific, the resampled and cropped images at payloads from 0.05 to 0.1 bpp, and the subsampled images at payloads from 0.05 to 0.5 bpp, where the networks are fine-tuned from their previous trained ones with an initial learning rate of 0.05. All the fine-tuning procedures will be stopped at 35 epochs except for the training of subsampled images at 0.5 bpp, which will be stopped at 70 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with the state-of-the-art Steganalyzers in Spatial Domain</head><p>In this subsection, we compare the performance of the proposed CNN-based models with two state-of-the-art steganalyzers in spatial domain, i.e., SRM and maxSRMd2, for a wide variety of payloads. Tables VIII-X show the performance comparison in terms of detection error (P E ) for all the tested schemes on resampled, cropped and subsampled images. In Fig. <ref type="figure" target="#fig_5">5</ref> to Fig. <ref type="figure">7</ref>, we further illustrate the detection errors of three state-of-the-art steganographic schemes in spatial domain, i.e., WOW, S-UNIWARD and HILL, as a function of payload (ranging from 0.05 bpp to 0.5 bpp), for all the involved steganalyzers on all the 3 image datasets.</p><p>It is observed in Fig. <ref type="figure" target="#fig_5">5</ref> to Fig. <ref type="figure">7</ref> that, the proposed TLU-CNN and SCA-TLU-CNN consistently outperform the other two hand-crafted rich models by a clear margin, irrespective of the embedding method, payload and image dataset (resampled, cropped and subsampled). On one hand, when the knowledge of selection channel is not incorporated, our TLU-CNN model can achieve significant performance gains over SRM for the involved embedding schemes, tested payloads and image datasets. This is particularly evident for resampled and subsampled images. For instance, in contrast to SRM, the TLU-CNN decreases the detection error of WOW by 12.46% on the resampled images when payload is 0.2 bpp as shown in Fig. <ref type="figure" target="#fig_5">5 (a)</ref>. It is also shown in Fig. <ref type="figure">6</ref> that the performance gain of TLU-CNN decreases somewhat for cropped images. This is because the cropped central images are usually the most complex regions of the original images, which makes the detection of stego images more difficult for both CNN based and hand-crafted steganalyzers. On the other hand, for those selection-channel-aware schemes, our SCA-TLU-CNN model also convincingly outperforms the maxSRMd2 as shown in Fig. <ref type="figure" target="#fig_5">5</ref> to Fig. <ref type="figure">7</ref>, and the performance gap becomes most pronounced for S-UNIWARD at 0.3 bpp on the resampled images, where the detection error is decreased by 10.4%. It is believed that the regularization for initialization with the high-pass filters in SRM, the use of TLU non-linearity and the unified optimization framework of the CNN model contribute much to the superior performance of CNN based staganalyzers over the conventional heuristic feature sets.</p><p>It is worth noticing that, although the TLU-CNN does not explicitly take advantage of the selection channel, it still defeats the selection-channel-aware maxSRMd2 algorithm in most cases. This surprising result shows that the proposed TLU-CNN is able to learn the distribution of selection channel for a specific embedding scheme implicitly, if it is trained on a sufficient large and diverse training set. It may explain the reason why the SCA-TLU-CNN does not outperform much its non-selection-channel-aware version TLU-CNN. It is also observed that, although the SCA-TLU-CNN consistently works better than TLU-CNN, the performance tends to be more and more similar at high payloads, especially for WOW and S-UNIWARD on resampled images as shown in Fig. <ref type="figure" target="#fig_5">5 (a)-(b</ref>). This is because, with the increase of data payload, both the WOW and S-UNIWARD become less adaptive, and therefore, the SCA-TLU-CNN could not exemplify its advantage in knowledge of selection channel. For HILL, however, the adoption of a series of filtering operations allows it to exhibit some "adaptivity" at different payload, which contributes to the superior performance of SCA-TLU-CNN over TLU-CNN at high data payload, especially on resampled and subsampled images.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>The paradigm of modern steganalyzer mainly consists of three steps, i.e., residual computation, feature extraction and binary classification. In this paper, we propose a CNN based steganalyzer, which is shown to be able to well simulate and optimize these key steps in a unified network architecture. The proposed CNN has a quite different structure compared to the ones designed for CV tasks, and is capable of detecting several state-of-the-art steganographic schemes in spatial domain for a wide variety of payloads with high accuracy. Instead of a random strategy, the weights in the first layer of the proposed CNN are initialized with the basic high-pass filters used in computation of residual maps in SRM, which helps to find a better local minima as a regularizer. Considering that the embedding signals usually have an extremely low SNR, a set of hybrid activation functions is adopted in our CNN model, where, in addition to the conventional ReLU function, a new function called truncated linear unit (TLU) is introduced to the first few layers of our network to well adapt to the distribution of the embedding signals. And finally, the performance of the proposed CNN is further boosted by incorporating the knowledge of selection channel. Extensive experiments have been carried out, which demonstrates the superior performance of the proposed CNN based steganalyzer over other state-ofthe-art steganalysis methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The framework of image steganalysis methods and its similarity with the convolutional neural networks.</figDesc><graphic coords="2,134.17,53.14,340.15,197.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) We employ a set of hybrid activation functions in the proposed CNN, where, in addition to the conventional ReLU function, a new function called truncated linear unit (TLU) is introduced to the first few layers of the network. Actually, different from the conventional CV tasks, the process of steganography can be regarded as the one of adding extremely low SNR embedding signals to the cover. The adoption of the TLU in the first few layers contributes to the adaptation to the distribution of the embedding signals and enforces the CNN to learn the high-pass filter in a more efficient manner. (3) We finally further boost the steganalysis performance by making use of the selection channel in training of the proposed CNN. The effectiveness of the proposed CNN is verified with evidence from thorough experiments using several state-of-the-art steganographic tools for a wide variety of payloads. The proposed CNN achieves considerable performance improvement in terms of detection accuracy when compared with previous CNN based steganalyzers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of the proposed 10-layer convolutional neural network. For each convolutional layer, the input feature maps are the output of its previous layer. The layers in the dotted box only exist in the selection-channel-aware version of the proposed network.</figDesc><graphic coords="4,77.48,53.14,453.55,199.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3.The convergence performance for training the four involved 10-layer CNN models against S-UNIWARD at 0.2 bpp on resampled BOSS+BOWS2+AUG images. Normalization of the initial high-pass filters is necessary for training with TanH unit. Except for the first layer, all the activation functions are ReLUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualizations of 1st convolutional layer filters in 3 different models. (a) Filters in ReLU-CNN. (b) Filters in TLU-CNN (T = 3). (c) Filters in SCA-TLU-CNN. Using TLU non-linearity and incorporating the knowledge of selection channel can result in more distinctive filters and fewer "dead" filters.</figDesc><graphic coords="7,65.94,53.14,154.23,129.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Detection errors P E of 3 state-of-the-art steganographic schemes as a function of payload for the involved steganalysis methods.Images are resized to 256 × 256. (a) WOW. (b) S-UNIWARD. (c) HILL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Detection errors P E of 3 state-of-the-art steganographic schemes as a function of payload for the involved steganalysis methods.Images are cropped into 256 × 256. (a) WOW. (b) S-UNIWARD. (c) HILL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>PERFORMANCE OF DIFFERENT INITIALIZATION STRATEGIES OF THE FIRST CONVOLUTIONAL LAYER IN TERMS OF DETECTION ERROR (P E ) FOR RELU-CNN AND TLU-CNN(T = 3) ON THREE STEGANOGRAPIC SCHEMES AT A PAYLOAD OF 0.2 BPP ON RESAMPLED IMAGES. THE INVOLVED NETWORKS ARE TRAINED ON BOSS+BOWS2+AUG AND TESTED ON BOSS TEST.</figDesc><table><row><cell>Algorithm</cell><cell>Model</cell><cell>Random</cell><cell>Fixed</cell><cell>Learned</cell></row><row><cell>WOW</cell><cell>ReLU-CNN TLU-CNN</cell><cell>0.5 0.5</cell><cell>0.2259 0.2261</cell><cell>0.2136 0.1982</cell></row><row><cell>S-UNIWARD</cell><cell>ReLU-CNN TLU-CNN</cell><cell>0.5 0.5</cell><cell>0.2968 0.2807</cell><cell>0.2937 0.2540</cell></row><row><cell>HILL</cell><cell>ReLU-CNN TLU-CNN</cell><cell>0.5 0.5</cell><cell>0.2980 0.3068</cell><cell>0.2971 0.2761</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>PERFORMANCE OF RELU AND TLU ON RESAMPLED IMAGES IN TERMS OF DETECTION ERROR (P E ) WITH DIFFERENT T SETTINGS. INVOLVED NETWORKS ARE TRAINED ON BOSS+BOWS2+AUG AND TESTED ON BOSS TEST. THE EMBEDDING PAYLOAD IS 0.2 BPP.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Algorithm</cell><cell></cell><cell>ReLU</cell><cell>TLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>T = 3</cell><cell>T = 7</cell><cell>T = 15</cell><cell>T = 63</cell><cell>T = ∞</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WOW</cell><cell></cell><cell>0.2136</cell><cell>0.1982</cell><cell>0.1966</cell><cell>0.2142</cell><cell>0.2139</cell><cell>0.2170</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">S-UNIWARD</cell><cell></cell><cell>0.2937</cell><cell>0.2540</cell><cell>0.2624</cell><cell>0.2653</cell><cell>0.2921</cell><cell>0.2990</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HILL</cell><cell></cell><cell>0.2971</cell><cell>0.2761</cell><cell>0.2812</cell><cell>0.2894</cell><cell>0.2956</cell><cell>0.2955</cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TanH</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TLU(T=3) SCA-TLU</cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training Loss</cell><cell>0.55 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III THE</head><label>III</label><figDesc>PERFORMANCE ON RESAMPLED IMAGES IN TERMS OF DETECTION ERROR (P E ) OF OUR CNN MODEL WHEN THE FIRST SEVERAL RELUS ARE REPLACED BY TLUS(T = 3). INVOLVED NETWORKS ARE TRAINED ON BOSS+BOWS2+AUG AND TESTED ON BOSS TEST. THE EMBEDDING PAYLOAD IS 0.2 BPP.</figDesc><table><row><cell></cell><cell>TLU 1</cell><cell>TLU 2</cell><cell>TLU 3</cell><cell>TLU 4</cell><cell>TLU 5</cell></row><row><cell>S-UNIWARD</cell><cell>0.2540</cell><cell>0.2409</cell><cell>0.2389</cell><cell>0.2660</cell><cell>0.2851</cell></row><row><cell cols="6">D. Incorporating the Knowledge of Selection Channel</cell></row><row><cell cols="6">1) Problem Formulation: According to Kerckhoffs's prin-</cell></row><row><cell>ciple</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV THE</head><label>IV</label><figDesc>DETECTION ERROR (P E ) OF TLU-CNN AND ITS SELECTION-CHANNEL-AWARE COUNTERPART SCA-TLU-CNN. THE EMBEDDING PAYLOAD IS 0.2 BPP. IMAGES (BOSS+BOWS2+AUG) ARE RESIZED TO 256 × 256.</figDesc><table><row><cell>Algorithms</cell><cell>TLU-CNN</cell><cell>SCA-TLU-CNN</cell></row><row><cell>WOW</cell><cell>0.1982</cell><cell>0.1691</cell></row><row><cell>S-UNIWARD</cell><cell>0.2540</cell><cell>0.2224</cell></row><row><cell>HILL</cell><cell>0.2761</cell><cell>0.2538</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V DETECTION</head><label>V</label><figDesc>ERROR (P E ) OF THREE STEGANALYSIS SCHEMES TRAINED ON DIFFERENT DATASETS AND TESTED ON BOSS TEST OF RESAMPLED IMAGES, FOR WOW AT 0.2 BPP. E ) OF THREE STEGANALYSIS SCHEMES TRAINED ON DIFFERENT DATASETS AND TESTED ON BOSS TEST OF CROPPED IMAGES, FOR WOW AT 0.2 BPP.</figDesc><table><row><cell>Algorithms</cell><cell>BOSS</cell><cell>BOSS+BOWS2</cell><cell>BOSS+BOWS2+AUG</cell></row><row><cell>SRM</cell><cell>0.3266</cell><cell>0.3228</cell><cell>N/A</cell></row><row><cell>maxSRMd2</cell><cell>0.2424</cell><cell>0.2325</cell><cell>N/A</cell></row><row><cell>TLU-CNN</cell><cell>0.3364</cell><cell>0.2693</cell><cell>0.1982</cell></row><row><cell></cell><cell></cell><cell>TABLE VI</cell><cell></cell></row><row><cell cols="2">DETECTION ERROR (P Algorithms BOSS</cell><cell>BOSS+BOWS2</cell><cell>BOSS+BOWS2+AUG</cell></row><row><cell>SRM</cell><cell>0.3865</cell><cell>0.3853</cell><cell>N/A</cell></row><row><cell>maxSRMd2</cell><cell>0.3075</cell><cell>0.3092</cell><cell>N/A</cell></row><row><cell>TLU-CNN</cell><cell>0.4205</cell><cell>0.3512</cell><cell>0.2808</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII DETECTION</head><label>VII</label><figDesc>ERROR (P E ) OF THREE STEGANALYSIS SCHEMES TRAINED ON DIFFERENT DATASETS AND TESTED ON BOSS TEST OF SUBSAMPLEDIMAGES FOR WOW AT 0.8 BPP.</figDesc><table><row><cell>Algorithms</cell><cell>BOSS</cell><cell>BOSS+BOWS2</cell><cell>BOSS+BOWS2+AUG</cell></row><row><cell>SRM</cell><cell>0.2300</cell><cell>0.2332</cell><cell>N/A</cell></row><row><cell>maxSRMd2</cell><cell>0.1813</cell><cell>0.1807</cell><cell>N/A</cell></row><row><cell>TLU-CNN</cell><cell>0.1991</cell><cell>0.1569</cell><cell>0.1182</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII PERFORMANCE</head><label>VIII</label><figDesc>COMPARISON OF THE INVOLVED STEGANALYZERS IN TERMS OF DETECTION ERROR (P E ) FOR 3 STATE-OF-THE-ART STEGANOGRAPHIC SCHEMES AT DIFFERENT PAYLOADS ON RESAMPLED IMAGES.</figDesc><table><row><cell>Algorithm</cell><cell>Payload (bpp)</cell><cell>SRM (P E )</cell><cell>TLU-CNN (P E )</cell><cell>maxSRMd2 (P E )</cell><cell>SCA-TLU-CNN (P E )</cell></row><row><cell></cell><cell>0.05</cell><cell>0.4551</cell><cell>0.3850</cell><cell>0.3810</cell><cell>0.3450</cell></row><row><cell></cell><cell>0.1</cell><cell>0.4066</cell><cell>0.3000</cell><cell>0.3163</cell><cell>0.2442</cell></row><row><cell>WOW</cell><cell>0.2 0.3</cell><cell>0.3228 0.2633</cell><cell>0.1982 0.1394</cell><cell>0.2325 0.1918</cell><cell>0.1691 0.1229</cell></row><row><cell></cell><cell>0.4</cell><cell>0.2127</cell><cell>0.1109</cell><cell>0.1536</cell><cell>0.0959</cell></row><row><cell></cell><cell>0.5</cell><cell>0.1800</cell><cell>0.0938</cell><cell>0.1331</cell><cell>0.0906</cell></row><row><cell></cell><cell>0.05</cell><cell>0.4641</cell><cell>0.4200</cell><cell>0.4316</cell><cell>0.4000</cell></row><row><cell></cell><cell>0.1</cell><cell>0.4232</cell><cell>0.3350</cell><cell>0.3806</cell><cell>0.3220</cell></row><row><cell>S-UNIWARD</cell><cell>0.2 0.3</cell><cell>0.3437 0.2798</cell><cell>0.2540 0.1772</cell><cell>0.2999 0.2542</cell><cell>0.2224 0.1502</cell></row><row><cell></cell><cell>0.4</cell><cell>0.2260</cell><cell>0.1410</cell><cell>0.2136</cell><cell>0.1281</cell></row><row><cell></cell><cell>0.5</cell><cell>0.1848</cell><cell>0.1003</cell><cell>0.1732</cell><cell>0.1000</cell></row><row><cell></cell><cell>0.05</cell><cell>0.4765</cell><cell>0.4150</cell><cell>0.4409</cell><cell>0.4000</cell></row><row><cell></cell><cell>0.1</cell><cell>0.453</cell><cell>0.3560</cell><cell>0.3894</cell><cell>0.3380</cell></row><row><cell>HILL</cell><cell>0.2 0.3</cell><cell>0.3811 0.3236</cell><cell>0.2761 0.2145</cell><cell>0.3226 0.2804</cell><cell>0.2538 0.1949</cell></row><row><cell></cell><cell>0.4</cell><cell>0.2818</cell><cell>0.1782</cell><cell>0.2410</cell><cell>0.1708</cell></row><row><cell></cell><cell>0.5</cell><cell>0.2363</cell><cell>0.1561</cell><cell>0.2115</cell><cell>0.1305</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IX PERFORMANCE</head><label>IX</label><figDesc>COMPARISON OF THE INVOLVED STEGANALYZERS IN TERMS OF DETECTION ERROR (P E ) FOR 3 STATE-OF-THE-ART STEGANOGRAPHIC SCHEMES AT DIFFERENT PAYLOADS ON CROPPED IMAGES.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Algorithm</cell><cell>Payload (bpp)</cell><cell>SRM (P E )</cell><cell>TLU-CNN (P E )</cell><cell>maxSRMd2 (P E )</cell><cell>SCA-TLU-CNN (P E )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell>0.4772</cell><cell>0.4139</cell><cell>0.4199</cell><cell>0.3874</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.4460</cell><cell>0.3488</cell><cell>0.3730</cell><cell>0.3240</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>WOW</cell><cell></cell><cell>0.2 0.3</cell><cell>0.3853 0.3337</cell><cell>0.2808 0.2450</cell><cell>0.3092 0.2686</cell><cell>0.2435 0.2036</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell>0.2887</cell><cell>0.2044</cell><cell>0.2361</cell><cell>0.1707</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell>0.2496</cell><cell>0.1680</cell><cell>0.2041</cell><cell>0.1445</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell>0.4750</cell><cell>0.4460</cell><cell>0.4571</cell><cell>0.4390</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.4439</cell><cell>0.4040</cell><cell>0.4206</cell><cell>0.3938</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">S-UNIWARD</cell><cell>0.2 0.3</cell><cell>0.3823 0.3287</cell><cell>0.3318 0.2850</cell><cell>0.3614 0.3132</cell><cell>0.3218 0.2571</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell>0.2805</cell><cell>0.2374</cell><cell>0.2721</cell><cell>0.1955</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell>0.2411</cell><cell>0.1959</cell><cell>0.2355</cell><cell>0.1660</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell>0.4845</cell><cell>0.4540</cell><cell>0.4536</cell><cell>0.4325</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.4618</cell><cell>0.4129</cell><cell>0.4211</cell><cell>0.3806</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>HILL</cell><cell></cell><cell>0.2 0.3</cell><cell>0.4129 0.3645</cell><cell>0.3494 0.3018</cell><cell>0.3638 0.3253</cell><cell>0.3288 0.2885</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell>0.3236</cell><cell>0.2470</cell><cell>0.2874</cell><cell>0.2291</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell>0.2810</cell><cell>0.2100</cell><cell>0.2520</cell><cell>0.1977</cell></row><row><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE X PERFORMANCE</head><label>X</label><figDesc>COMPARISON OF THE INVOLVED STEGANALYZERS IN TERMS OF DETECTION ERROR (P E ) FOR 3 STATE-OF-THE-ART STEGANOGRAPHICSCHEMES AT DIFFERENT PAYLOADS ON SUBSAMPLED IMAGES.</figDesc><table><row><cell>Algorithm</cell><cell>Payload (bpp)</cell><cell>SRM (P E )</cell><cell>TLU-CNN (P E )</cell><cell>maxSRMd2 (P E )</cell><cell>SCA-TLU-CNN (P E )</cell></row><row><cell></cell><cell>0.05</cell><cell>0.4831</cell><cell>0.4176</cell><cell>0.4254</cell><cell>0.3916</cell></row><row><cell></cell><cell>0.1</cell><cell>0.4592</cell><cell>0.3622</cell><cell>0.3788</cell><cell>0.3333</cell></row><row><cell>WOW</cell><cell>0.2 0.3</cell><cell>0.4171 0.3797</cell><cell>0.2900 0.2391</cell><cell>0.3176 0.2796</cell><cell>0.2585 0.2070</cell></row><row><cell></cell><cell>0.4</cell><cell>0.3443</cell><cell>0.2077</cell><cell>0.2523</cell><cell>0.1691</cell></row><row><cell></cell><cell>0.5</cell><cell>0.3132</cell><cell>0.1812</cell><cell>0.2335</cell><cell>0.1547</cell></row><row><cell></cell><cell>0.05</cell><cell>0.4893</cell><cell>0.4541</cell><cell>0.4662</cell><cell>0.4452</cell></row><row><cell></cell><cell>0.1</cell><cell>0.4722</cell><cell>0.4283</cell><cell>0.4347</cell><cell>0.4020</cell></row><row><cell>S-UNIWARD</cell><cell>0.2 0.3</cell><cell>0.4323 0.3949</cell><cell>0.3618 0.3137</cell><cell>0.3842 0.3416</cell><cell>0.3307 0.2814</cell></row><row><cell></cell><cell>0.4</cell><cell>0.3544</cell><cell>0.2872</cell><cell>0.3120</cell><cell>0.2387</cell></row><row><cell></cell><cell>0.5</cell><cell>0.3213</cell><cell>0.2226</cell><cell>0.2881</cell><cell>0.1988</cell></row><row><cell></cell><cell>0.05</cell><cell>0.4948</cell><cell>0.4697</cell><cell>0.4761</cell><cell>0.4551</cell></row><row><cell></cell><cell>0.1</cell><cell>0.4840</cell><cell>0.4430</cell><cell>0.4592</cell><cell>0.4140</cell></row><row><cell>HILL</cell><cell>0.2 0.3</cell><cell>0.4629 0.4416</cell><cell>0.3940 0.3490</cell><cell>0.4269 0.3998</cell><cell>0.3632 0.3216</cell></row><row><cell></cell><cell>0.4</cell><cell>0.4146</cell><cell>0.3245</cell><cell>0.3747</cell><cell>0.2877</cell></row><row><cell></cell><cell>0.5</cell><cell>0.3859</cell><cell>0.2950</cell><cell>0.3541</cell><cell>0.2596</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>These values correspond to the repeated "stepvalue" in Caffe when using "multistep" policy.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers and associate editor for their comments that greatly improved the paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported by National Natural Science Foundation of China (No. 61379156 and 60970145), the National Research Foundation for the Doctoral Program of Higher Education of China (No. 20120171110037) and the Key Program of Natural Science Foundation of Guangdong (No.S2012020011114).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using high-dimensional image models to perform highly undetectable steganography</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Information Hiding</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="161" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Designing steganographic distortion using directional filters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Information Forensics and Security</title>
		<imprint>
			<publisher>WIFS</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal distortion function for steganography in an arbitrary domain</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Information Security</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rich models for steganalysis of digital images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kodovský</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="868" to="882" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random projections of residuals for digital image steganalysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1996" to="2006" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Selection-channel-aware rich model for steganalysis of digital images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sedighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cogranne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Information Forensics and Security</title>
		<imprint>
			<publisher>WIF-S</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive steganalysis against wow embedding algorithm</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM workshop on Information hiding and multimedia security</title>
		<meeting>the 2nd ACM workshop on Information hiding and multimedia security</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for steganalysis via convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE/IS&amp;T Electronic Imaging</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">90</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">break our steganographic system: The ins and outs of organizing boss</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Information Hiding</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structural design of convolutional neural networks for steganalysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="708" to="712" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ensemble of cnns for steganalysis: An empirical study</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security</title>
		<meeting>the 4th ACM Workshop on Information Hiding and Multimedia Security</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="103" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Steganalysis by subtractive pixel adjacency matrix</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="224" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Steganalysis based on Markov model of thresholded prediction-error image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1365" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting hidden messages using higher-order statistics and support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Information Hiding</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="340" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Steganalysis using image quality metrics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Avcibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="229" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">New blind steganalysis and its implications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Holotyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE/IS&amp;T Electronic Imaging</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="607" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning is a good steganalysis tool when embedding key is reused for different images, even if there is a cover sourcemismatch</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pibre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pasquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ienco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaumont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">La cryptographie militaire</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kerckhoffs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Des Sciences Militaires</title>
		<imprint>
			<biblScope unit="volume">IX</biblScope>
			<biblScope unit="page" from="5" to="83" />
			<date type="published" when="1883">1883</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving selectionchannel-aware steganalysis features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Comesaña-Alfaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and transferring representations for image steganalysis using convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2752" to="2756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bows-2</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<ptr target="http://bows2.gipsa-lab.inpg.fr" />
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">He is currently pursuing the M.S. degree with the School of Electronics and Information Technology</title>
	</analytic>
	<monogr>
		<title level="m">His current research interests include image steganalysis and machine learning</title>
		<title level="s">Jian Ye receved the B.S. degree in School of Information Science and Technology</title>
		<meeting><address><addrLine>Guangzhou, China; Guangzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Sun Yat-Sen University ; Sun Yat-Sen University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">He was a Post-doctoral Fellow for a joint program between the Sun Yat-Sen University and the Guangdong Institute of Telecommunication Research from 1998 to 2000. Since 2001, he has been with the School of Data and Computer Science</title>
		<author>
			<persName><forename type="first">Jiangqun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Hong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Engineering from Fudan University, and the M.Sc. and Ph.D. degrees in Machine Learning and Optimization from Northeastern University, respectively</title>
		<meeting><address><addrLine>Guangzhou, China; Eastern Washington University, USA; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-09">1998. September 2005 to August 2006</date>
		</imprint>
		<respStmt>
			<orgName>Sun Yat-Sen University</orgName>
		</respStmt>
	</monogr>
	<note>She has published over 80 papers in international conferences or journals. And her current research interests include computer vision and digital image processing by machine learning and pattern recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
