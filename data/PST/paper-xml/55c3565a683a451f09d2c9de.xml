<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shadow Remover: Image Shadow Removal Based on Illumination Recovering Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ling</forename><surname>Zhang</surname></persName>
							<email>lingzhang@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Chunxia</forename><surname>Xiao</surname></persName>
							<email>cxxiao@whu.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shadow Remover: Image Shadow Removal Based on Illumination Recovering Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F9292ADE5BECC84FA37FC8236BBED056</idno>
					<idno type="DOI">10.1109/TIP.2015.2465159</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2465159, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING, 2014 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2465159, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING, 2014 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2465159, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2465159, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>shadow detection</term>
					<term>shadow removal</term>
					<term>shadow matting</term>
					<term>patch matching</term>
					<term>aerial images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel shadow removal system for single natural images as well as color aerial images using an illumination recovering optimization method. We first adaptively decompose the input image into overlapped patches according to the shadow distribution. Then by building the correspondence between the shadow patch and the lit patch based on texture similarity, we construct an optimized illumination recovering operator which effectively removes the shadows and recovers the texture detail under the shadow patches. Based on coherent optimization processing among the neighboring patches, we finally produce high-quality shadow-free results with consistent illumination. Our shadow removal system is simple and effective, and can process shadow images with rich texture types and nonuniform shadows. The illumination of shadow-free results is consistent with that of surrounding environment. We further present several shadow editing applications to illustrate the versatility of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>first problem to be addressed is shadow detection. For image with complex shadows, accurately shadow detecting is a difficult problem. For example, sometimes it is even difficult for human to distinguish little dark objects from the scattered shadow points. The second problem is, when the illumination conditions, object materials, and scene shapes are complex, the shadows in the image are usually nonuniform, which makes it difficult to obtain consistent shadow removal results. Finally, as the illumination usually changes dramatically in the boundary regions, effectively recovering the illumination on the shadow boundaries is also a challenging task.</p><p>In this paper, we present a novel shadow removal approach using an illumination recovering optimization method.</p><p>We first detect the shadows in the input image, and compute the shadow alpha for the shadows. Then we adaptively decompose the input image into overlapped patches according to the shadow distribution. Denser patches are put on the shadow boundaries and the regions with dramatically changed illumination. Finally, by building the correspondence between the shadow patch and the lit patch based on illumination independent texture similarity, we develop an optimized illumination recovering operator which can effectively remove the shadows and recover the texture detail under the shadow patches. By using coherent illumination optimization processing among the neighboring patches, we produce high-quality shadow-free results. The texture details under the shadow regions are effectively recovered, and the recovered illumination is consistent with that of surrounding environment. Fig. <ref type="figure" target="#fig_0">1</ref> shows the overview of the proposed shadow removal system. The proposed method is simple and effective. With the adaptively image patch decomposition and illumination independent patch matching, our method can process nonuniform shadows and shadow regions with rich texture types. While most existing shadow removal methods do not work well in processing images with complex shadows and various texture materials. Furthermore, with the adaptively image decomposition, our shadow removal system also works well on the shadow boundaries. In addition, we present several applications for the proposed method, such as illumination and color transfer, shadow edge softening. In our applications, shadow sharpness, position, and intensity can be freely adjusted, which illustrate the versatility of the proposed method.</p><p>The main contributions and advantages of our work are as follows:</p><p>• Introduce a novel local illumination recovering optimization, which produces illumination consistent results on images with nonuniform shadows and multi-texture types.</p><p>• Develop an adaptive image decomposition strategy for patch matching used in shadow removal, which generates compelling results on both shadow interior and shadow boundaries.</p><p>• Present several image editing applications using the proposed shadow removal operator, which illustrates the versatility of the proposed approach.</p><p>The remainder of this paper is organized as follows. In Section II, we introduce the related work. In section III, we perform the shadow detection and shadow alpha matting, which is a preprocessing step. Section IV introduces the local illumination recovering operator. Section V presents the shadow removal approach. In Section VI, we present several applications of our method. Section VII shows the experimental results, and Section VIII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Shadow removal involves two basic stages: shadow detection and shadow removal. So far, many shadow detection methods have been proposed, including automatic shadow detection methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and user-assisted shadow detection methods <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Many video shadow detection methods also have been proposed <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. A variety of shadow removal methods have been developed in computer vision, including the shadow removal from single images <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and shadow removal from multiple images and video sequences <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. A comprehensive survey is beyond the scope of this paper, we only focus on shadow removal methods from a single image and review the most related work to our paper.</p><p>Methods based on gradient domain: Finlayson et al. proposed a series of shadow removal methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> based on gradient domain manipulation. The basic idea in these methods is to reconstruct the shadow-free image based on the gradient information in shadow regions by nullifying the gradients on shadow boundaries. Mohan et al.</p><p>[3] proposed a shadow removal method by fitting a gradient domain shadow edge model. This method also simulates a variety of lighting conditions, such as ambient lighting, for shadow editing. Liu et al. <ref type="bibr" target="#b10">[11]</ref> removed shadow by assigning gradients for the shadow and lit area according to illumination and gradient change in penumbra area.</p><p>Although impressive results have been presented, the gradient domain based shadow removal methods may fail to produce high-quality shadow-free results for the nonuniform shadows since they only modify the gradients in shadow edges or penumbra regions. To receive satisfied results, the gradient domain based method needs to detect accurate shadow edges <ref type="bibr" target="#b0">[1]</ref>, or find best function fit for the shadow edge intensity <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Since shadow edge detection is also a challenging work, this limits the practicality of these methods.</p><p>Methods based on illumination transfer: Inspired by color transfer technique <ref type="bibr" target="#b19">[20]</ref>, several shadow removal methods have been proposed. The basic idea of color transfer is applying statistical analysis to transfer one image's color characteristics to another. Shor et al. <ref type="bibr" target="#b9">[10]</ref> built linear mapping models from the shadow and nonshadow areas based on illumination transfer. However, the shadow and nonshadow samples used for estimating the illuminated recovering parameters should have similar texture materials, which means this method can only handle shadow regions with uniform texture. Xiao et al. <ref type="bibr" target="#b20">[21]</ref> proposed an adaptive multi-scale illumination transfer technique considering the material reflectance variation, and improved to recover the texture details of the shadow regions.</p><p>The global color transfer methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref> applied a global transformation on the shadow regions to match color statistics of the nonshadow regions. As they do not take texture variety into account, they work well when the shadow and nonshadow sample regions exhibit similar texture.</p><p>More recently, Xiao et al. <ref type="bibr" target="#b4">[5]</ref> further improved <ref type="bibr" target="#b20">[21]</ref> by performing illumination transfer on the matched subregion pairs between the shadow regions and the nonshadow regions. This method can process complex images with different kinds of shadow texture regions and illumination conditions. But it fails to ensure smooth transition between subregions, and may produce illumination inconsistent results. Li et al. <ref type="bibr" target="#b5">[6]</ref> applied color transfer results</p><p>as the predicted shadow-free image used in the fidelity item. They also introduced an adaptive NL (nonlocal) regularized shadow removal method for aerial images by regularizing the shadow scale and the updated shadowfree image. The adaptive behavior regionally smooths the shadow scale, which can preserve the edges and the textures in the shadow regions. The NL Laplacian prior can reduce noises in the shadow regions, but introduces texture detail blurring artifacts.</p><p>Methods based on Shadow matting: Chuang et al. <ref type="bibr" target="#b16">[17]</ref> proposed a shadow matting method which considered the input image as a linear combination of a shadow-free image and a shadow matte image. Although this method can be used to remove shadow, it focuses on shadow extracting and compositing, and does not pay much effort for shadow removal. Instead of considering shadow extraction as the conventional matting equation, Wu et al. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref> supposed shadow effect as a light attenuation problem. Wu and Tang <ref type="bibr" target="#b21">[22]</ref> proposed a Bayesian framework for shadow extraction and produced shadowless image. Later, Wu et al. <ref type="bibr" target="#b8">[9]</ref> first estimated an approximate shadowless image using color transfer techniques <ref type="bibr" target="#b19">[20]</ref>, and then defined an optimization function incorporating the approximate shadowless image for shadow extracting. Both <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b21">[22]</ref> applied user-supplied hints to identify shadow and nonshadow regions. Although <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref> tried to preserve the texture appearance under the extracted shadow, to process complex nonuniform shadows, both methods still may not work well in recovering the image detail in the shadow areas.</p><p>Other shadow removal methods: Guo et al. <ref type="bibr" target="#b7">[8]</ref> presented a regions-based approach for shadow detection and removal. By segmenting the input shadow image using mean shift algorithm, Guo et al. explored the pairwise relationship between regions, and provided information about illumination condition of regions for shadow detection.</p><p>They estimated the ratio between direct light and environment light, and recovered the illumination of shadow regions by relighting each pixel. This approach can produce high-quality shadow-free results for simple shadow images.</p><p>However, for image with complex shadows and nonuniform shadows, this method cannot recover satisfactory texture detail since it does not take the reflectance variation into account. More recently, Yao et al. <ref type="bibr" target="#b22">[23]</ref> transformed the shadow removal problem into nonlocal feature matching between unshadowed samples and shadow pixels, and recovered the illumination from a single RGB-D image by applying an energy minimization method. They implemented the feature matching by measuring similarities between nonlocal pixels using normals, chromaticity and spatial locations. However, the performance of this algorithm relies on accurate depth map provided by the depth sensor such as MS Kinect.</p><p>Arbel and Hel-Or <ref type="bibr" target="#b1">[2]</ref> considered each image channel as an intensity surface, and approximated the surface shape of shadow regions using a smooth thin-plate with the constraint on the shadow edges and the texture anchor points. This method can remove some nonuniform shadows. However, smooth thin-plate approximation is difficult to accurately estimate the shadow scale factors on the textured and highly structured images. Although texture anchor points have been used to alleviate this problem, there is still much room for improvement. Assuming a single flat texture shadow surface, Baba et al. <ref type="bibr" target="#b23">[24]</ref> presented a shadow removal method based on color and variance adjustment of shadow pixels in RGB space. The method in <ref type="bibr" target="#b24">[25]</ref> is based on the estimation of shadow scale factors assuming uniform shadow intensities and hard shadows. Both methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> used inpainting techniques for completion of missing information in shadow boundary regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SHADOW DETECTION AND ALPHA MATTING</head><p>To perform shadow removal, we first have to detect the shadows. The current shadow detection methods can be divided into two categories: automatic shadow detection <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and interactive shadow detection <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>As automatic shadow detection is an extremely difficult task, we incorporate user interaction and shadow alpha matting for shadow detection. Similar to interactive image matting, we first specify some shadow samples and lit samples, and construct a trimap for the input image. With the tripmap, we can extract the shadow alpha matte using the specified samples as the constraints. More specially, we set α = 0 for the specified shadow samples, and α = 1 for the specified lit samples. Then we apply the closed-form matting method <ref type="bibr" target="#b27">[28]</ref> and minimize the following energy equation for computing the shadow alpha:</p><formula xml:id="formula_0">α = arg min α T Lα + λ(α T -b T S )D S (α -b S )<label>(1)</label></formula><p>where L is the Laplacian matrix, D S is a diagonal matrix whose diagonal elements are one for constrained pixels and zero for all other pixels, and the vector b S contains specified alpha values for the constrained nonshadow pixels and zero for all other pixels. Minimizing above energy equation, we can get the shadow alpha α.</p><p>With the computed shadow alpha matte α, we can identify the shadow regions of the input image. Specially, given threshold δ 1 and δ 2 , the pixels with α &lt; δ 1 can be considered as the pixels in the shadow regions (umbra regions). The pixels with α &gt; δ 2 can be considered as pixels in the nonshadow regions. The pixels with δ 1 ≤ α ≤ δ 2 can be considered as the pixels in the penumbra regions, for hard shadow, these pixels can be considered as the shadow boundaries. In the shadow boundaries, the α value usually changes dramatically. In our experiments, we set δ 1 = 0.2 and δ 2 = 0.9. Fig. <ref type="figure" target="#fig_1">2</ref> shows a shadow matte result. Note that several methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref> also used closed-form matting method <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> for shadow detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LOCAL ILLUMINATION RECOVERING OPERATOR</head><p>In image formation equation <ref type="bibr" target="#b29">[30]</ref>, an image is the pixel-wise product of illumination and reflectance:</p><formula xml:id="formula_1">I x = R x L x ,</formula><p>where I x is the observed RGB color at pixel x. L x and R x are the illumination and the reflectance (albedo) at pixel x, respectively. We assume shadows in the scene are cast due to the single primary source of illumination is blocked. If the pixel x is in the lit regions, the illumination can be described as a sum of the direct illumination L d and the indirect (ambient) illumination L a : L x = L d + L a . In the umbra regions, since the object occludes the primary light source, it will cast a shadow at pixel x, and the cast illumination is only the ambient illumination:</p><formula xml:id="formula_2">L x = L a .</formula><p>In the penumbra regions or shadow boundaries, as the occluder blocks some of the direct illumination, the practical illumination including the ambient illumination and some of the direct illumination, which can be described as:</p><formula xml:id="formula_3">L x = α x L d + L a</formula><p>, where α x is the shadow matting alpha value of pixel x (introduced in Section III), which can be regarded as the attenuation factor of the direction illumination. Thus, the shadow color at pixel x and its shadow-free value I f ree x can be expressed as:</p><formula xml:id="formula_4">   I x = (αL d + L a )R x I f ree x = (L d + L a )R x<label>(2)</label></formula><p>Shor et al. <ref type="bibr" target="#b9">[10]</ref> claimed that there is a linear relationship between the observed value of pixel in a shadow region and its shadow-free value. The shadow-free value at pixel x can be represented as</p><formula xml:id="formula_5">I x = kI x + b, where k = σ(L) σ(S)</formula><p>and b = µ(L) -kµ(S). µ(S) is the average value of the shadow sample regions, and µ(L) is the average value of the lit sample regions which have similar textures to the shadow sample regions. σ(L) and σ(S) are the standard deviation corresponding to the regions. Note that, this shadow removal model is only used in images with single texture, while can not work well for images with complex textures. The use of local patch in our method is just to meet this demand. We employ the results of Shor et al. to estimate the initial value in our shadow removal system. Specifically, we replace the shadow sample regions and the lit sample regions with the shadow patch and its matched lit patch, and the shadow-free value at pixel x in a shadow patch can be estimated as:</p><formula xml:id="formula_6">I x = σ(L) σ(S) (I x -µ(S)) + µ(L)<label>(3)</label></formula><p>Then Equation ( <ref type="formula" target="#formula_4">2</ref>) can be reformulated as:</p><formula xml:id="formula_7">L d L a = I x -I x α x I x -I x (4) Let t = Ix-I x αxI x -Ix , we have L d = tL a<label>(5)</label></formula><p>Based on the above analysis, the shadow-free result of pixel x in shadow regions (umbra regions or penumbra regions) can be estimated as:</p><formula xml:id="formula_8">I f ree x = (t + 1)L a R x = t + 1 α x t + 1 I x<label>(6)</label></formula><p>Our illumination recovering operator (Equation ( <ref type="formula" target="#formula_8">6</ref>)) is based on the assumption that, in a local patch, the illumination and reflectance variation are small. Using this operator, we can efficiently remove the shadows in the shadow patch using information of the matched patch with similar texture. To obtain a smooth shadow alpha α on the shadow boundaries, we estimate the alpha α for pixel on shadow boundaries using the average α value of the neighboring pixels.</p><p>V. SHADOW REMOVAL From Equation ( <ref type="formula" target="#formula_8">6</ref>) we know that, with the computed shadow matting alpha and the local reflectance constant assumption, for one patch in the shadow regions, if we find a lit patch in the nearby lit regions with similar material or texture, the shadow of this patch can be removed using our illumination recovering operator. To remove the whole shadows of the input image, we can transform this problem into the following solver: for each patch in the shadow regions, we find a corresponding patch with similar texture in the lit regions, and then by using the illumination recovering method on each corresponding patch pair, the shadows in the image can be removed.</p><p>However, to receive satisfactory results, the following several issues need to be addressed. The first issue is that if we remove the shadow of each patch separately, the results may be inconsistent and visually unnatural, thus, we should ensure that the shadow-free results have natural transitions between neighboring patches. The second important issue is that as the dramatically change of illumination may lead to texture detail loss on shadow boundaries, these boundaries should be elaborately processed to recover the texture information. Finally, for each patch in the shadow regions, to make the shadow removal processing efficient, the corresponding matched patch should be found efficiently in the lit regions.</p><p>To address above issues, we incorporate the illumination recovering operator into coherent optimization process.</p><p>We first decompose the input image into adaptive overlapped patches. For each patches in the shadow regions, we find a corresponding patch in the lit regions for shadow removal. The shadow-free value for the pixel in the overlapped region is computed as the weighted average of all the shadow-removed values at its position from patches containing this pixel. By using this technique, we can receive shadow removal result with satisfied transition between patches. Finally, we exploit the image synthesis technology to recover the illumination and texture information on the shadow boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image decomposition</head><p>We decompose the input image into adaptive patches according to the illumination distribution. Formally, let I denote the input image, S and L are the shadow regions and lit regions of I, respectively. We first decompose the input image I into uniformly overlapped patches with patch size of w × w (w &gt; 10). Instead of moving patch one pixel at a time, we shift patch five pixels at a time. Thus, we set patches that sufficiently overlap with each other which ensures that a given pixels can be affected by multiple patches, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. This makes the shadow removal result exhibit natural and smooth transition between adjacent patches, and also alleviates the artifacts when a certain patch is matched inaccurately. Since the illumination in the shadow boundaries usually changes dramatically, to receive satisfied shadow removal results, we further subdivide patches on the shadow boundaries with smaller patch size. In our experiments, the pixels with δ 1 ≤ α ≤ δ 2 are considered as shadow boundary pixels, where δ 1 = 0.2 and δ 2 = 0.9. We subdivide the patch containing boundary pixels into four smaller patches. Using this method, we can receive adaptive patch decomposition for the input image. Note that we consider the patch containing pixels with α ≤ δ 2 as the shadow patch. The remaining patches are lit patches.</p><p>Using adaptive decomposition, the content of the input image can be expressed using the shadow patches</p><formula xml:id="formula_9">{S k } k=1,••• ,Ns and the lit patches {L k } k=1,••• ,N l ,</formula><p>where N s is the number of shadow patches and N l is the number of lit patches. The lit patches present guided samples for shadow patches used in shadow removal. For each patch S i in {S k }, we find a matching patch L j in {L k } which has similar texture with S i . For the patch pair (S i , L j ),</p><p>we remove the shadow on patch S i by applying our local illumination recovering operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fast nearest patch matching</head><p>For each shadow patch S i , we develop an effective texture matching metric to find a nearest patch in the lit regions. Our patch matching method is based on the following observation: the texture and color of two patches with the same reflectance are similar only when the illumination is the same; the color and intensity of these two patches may differ when the illumination is different. Thus, the patch matching metric should be illumination independently, that is, it should be robust in the presence of illumination changes. As the illumination difference between shadow regions and lit regions is usually large, we perform the effective patch search from two aspects.</p><p>For finding the matched patch pair (S i , L j ), each aspect acts as a constraint condition in the searching processes.</p><p>Covariance matrices: region covariance descriptor <ref type="bibr" target="#b30">[31]</ref> is an efficient feature description for a region in an image, which provides strong discriminative power in distinguishing local texture and image structures <ref type="bibr" target="#b31">[32]</ref>. It represents a region R with a covariance matrix of the feature points:</p><formula xml:id="formula_10">C R = 1 n -1 n k=1 (z k -µ)(z k -µ) T (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where n is the pixel number of region R, z k is the d-dimensional feature vector of the k-th pixel, and µ is the mean feature vector of the pixels in region R. The d-dimensional feature vector can be chosen freely depending on different applications. In our method, we choose a 6-dimensional vector (intensity, chromaticity, first derivatives of the intensity in x and y directions, second derivatives of the intensity in x and y directions) as the feature vector of each pixel, and C R is a 6 × 6 covariance matrix. As subtracting the mean feature vectors, covariance matrices are less sensitive to illumination, which is preferred for patch matching between shadow regions and lit regions.</p><p>Since covariance matrices do not live on Euclidean space, we use the Cholesky decomposition <ref type="bibr" target="#b32">[33]</ref> to transform covariance matrices into Euclidean space. We use the following vector <ref type="bibr" target="#b32">[33]</ref> to represent the covariance matrix:</p><formula xml:id="formula_12">f (C R ) = {µ, √ 6L 1 , • • • , √ 6L 6 , - √ 6L 1 , • • • , - √ 6L 6 } T<label>(8)</label></formula><p>where L i is the ith column of the lower triangular matrix L computed from the Cholesky decomposition of C R :</p><formula xml:id="formula_13">C R = LL T .</formula><p>To accelerate this search, we construct a KD-tree for the vector f (C R ) of all lit patches. We query the KD-tree with the vector f (C R ) of the shadow patches, and extract some nearest patches as the candidate lit patches for each patch S i . In fact, a larger number of candidates contribute to more accurate patch matching while it requires higher computational cost. To achieve a good tradeoff between efficiency and accuracy during the patch matching, we choose five candidate lit patches in our experiments.</p><p>Spatial distance: Spatial distance between two patches is also an important cue to find the corresponding patch.</p><p>As spatially neighboring patches usually have similar illumination information, thus, for shadow removal, finding a lit patch close to shadow patch with similar texture is more appropriate to produce illumination coherent results.</p><p>In our method, spatial distance metric is used to select the final matched patch from the five candidate lit patches.</p><p>The spatial distance is computed as the square sum of the difference between the lit patch center and the shadow patch center. For the five candidate lit patches, we choose the patch which has the smallest spatial distance to S i .</p><p>Xiao et al. <ref type="bibr" target="#b4">[5]</ref> used Gabor wavelet descriptor to extract the texture feature, however this method is influenced by the illumination. That is, this descriptor is illumination dependently. In contrast, the texture descriptor we use to extract the texture information is illumination independently. In Fig. <ref type="figure" target="#fig_3">4</ref>, we compare patch matching results based on our patch matching strategy by using region covariance descriptor and the Gabor wavelet descriptor <ref type="bibr" target="#b4">[5]</ref>, respectively.</p><p>The results show that our texture descriptor outperforms Gabor wavelet descriptor using for texture matching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Coherence recovering optimization</head><p>Once we have found the nearest patch in the lit regions for each shadow patch, we can remove the shadow using Equation ( <ref type="formula" target="#formula_8">6</ref>). Because of independent processing for each shadow patch, illumination between patches may be inconsistent, as illustrated in Fig. <ref type="figure">5</ref>(b). To get coherent results and eliminate the potential blurring artifacts, we introduce illumination consistency optimization and texture detail enhancement in our method. Texture detail enhancement is an optional step which is required when texture blurring exists in shadow regions after the illumination consistency optimization. Intuitively, to receive consistent result between adjacent patches, each pixel should take all possible shadow-free values into consideration.</p><p>Let S(x) be a patch set containing pixel x. The weight w i = 1 -dis(x,center Si)</p><formula xml:id="formula_14">max S k ∈S(x) [dis(x,center S k )]</formula><p>is the optimized weighting factor for pixel x in patch S i ∈ S(x), where dis(x, center S k ) is the spatial distance between pixel</p><p>x and the patch center of S i . Let I f ree x∈Si be the shadow-free result of pixel x by performing local illumination recovering operator using the matched patch pair (S i , L j ). The illumination consistency optimization result for pixel x is computed as weighted average of all the possible shadow-free values I f ree x∈Si in each S i ∈ S(x):</p><formula xml:id="formula_15">I f ree x = Si∈S(x) w i I f ree x∈Si Si∈S(x) w i<label>(9)</label></formula><p>After the procedure of illumination consistency optimization, the inconsistent artifacts of the neighboring patches can be avoided or greatly alleviated, and we receive high-quality shadow-free result, as illustrated in Fig. <ref type="figure">5(c</ref>). In our experiment, we set denser and smaller patches on the shadow boundaries, which helps to receive satisfactory result on the boundaries.</p><p>2) Texture detail enhancement: The illumination consistency optimization makes consistent transition between shadow removed patches. However, due to the weighted averaging, sometimes it will leads to some texture blurring artifacts, especially for image with weak texture structure, as illustrated in Fig. <ref type="figure" target="#fig_6">6</ref>(b). Furthermore, to recover the illumination information of the heavy shadow regions, the texture detail may not be recovered thoroughly.</p><p>In order to recover texture details, we apply the image gradient of the original shadow regions as the guidance to enhance the received shadow-free result with the purpose to maintain gradient information of shadow regions.</p><p>We define a optimization function as follows:</p><formula xml:id="formula_16">x∈S (I detail x -I f ree x ) 2 + λ 1 x∈S (∇I detail x -∇I x ) 2<label>(10)</label></formula><p>The first term of this energy function is the data term. I detail x is the target value at pixel x. I f ree x is the shadow removed value generated from illumination consistency optimization. The second term is the gradient constraint term, whose purpose is to maintain the gradient information of the shadow regions. ∇ is the gradient operator. λ 1 is a user controlled parameter used for balancing the contribution of the gradient constraint term. A large λ 1 is set when the shadow regions show significant blurring artifacts.</p><p>Because of the influence of shadow, especially heavy shadow, the gradient may be weakened in shadow regions.</p><p>Hence, we set a weight coefficient λ 2 for the original gradient in Equation ( <ref type="formula" target="#formula_16">10</ref>), whose objective is to compensate for the gradient weakening derived from light occluding. The value of λ 2 is controlled by user, which is fixed for one image. But for different images, a large λ 2 can be set when the color differences between adjacent pixels in shadow regions are small. The optimization function is rewritten as:</p><formula xml:id="formula_17">x∈S (I detail x -I f ree x ) 2 + λ 1 x∈S (∇I detail x -λ 2 ∇I x ) 2<label>(11)</label></formula><p>We solve the above linear system using gradient decent method. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, with the texture detail recovering enhancement, the texture details in shadow regions can be efficiently recovered. The step for texture detail enhancement is optional, and it is required only when texture blurring occurs after illumination consistency optimization. This blurring is a relatively rare event. In all the results presented in this paper, only Fig. <ref type="figure" target="#fig_6">6(d</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Shadow boundary processing</head><p>Our method can effectively recover the illumination around the shadow boundaries where there is smooth transition between shadow regions and lit regions, as illustrated in the boxes of the first row in Fig. <ref type="figure" target="#fig_7">7(c</ref>). But for some complex and sharp shadow boundaries, as shown in the second row in Fig. <ref type="figure" target="#fig_7">7</ref>(c), our current method may not work well.</p><p>The main difficulty in shadow boundaries is that some of the detail information is lost at shadow boundaries due to the dramatic changes of illumination on shadow boundaries, which leads to some defects for some existing shadow removal methods. Let I be the input image, and I f ilter is the filtered map by bilateral filter <ref type="bibr" target="#b33">[34]</ref>, we compute the image detail by D = I -I f ilter . We can observe from Fig. <ref type="figure" target="#fig_7">7(b</ref>) that, the detail information on the shadow boundaries is sometimes seriously destroyed or missing. Thus, to receive satisfied shadow-free results on sharp shadow boundaries, different from the linear interpolation method such as <ref type="bibr" target="#b4">[5]</ref>, we need to develop more effective shadow boundary processing techniques. Compared with the whole image, the target shadow boundary regions are relatively small. The examplebased texture synthesis method can handle it very well. Inspired by the texture synthesis method <ref type="bibr" target="#b34">[35]</ref>, we present the constrained texture synthesis to recover the texture and illumination information on the shadow boundaries.</p><p>Let φ be the target shadow boundary regions. For each pixel in φ, T x is a window centered at pixel x with size of r × r. To remove transitional difference between lit regions and shadow removed regions, we minimize the following objective function to recover boundary texture and illumination:</p><formula xml:id="formula_18">E(I edge x , {T x }) = x∈φ (||T x -M || + ω ||T x -M || 2 d(x) 2 + c(W ) 2 ) (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>where I edge x is the target value at pixel x. The first term measures the appearance difference between T x and M in a L 1 norm fashion. M is a r × r window in the sample regions. In our experiments, we set r = 7. The second term is the proximity term constraining the search space. ω is the balance parameter. d(x) is the distance between pixel x and the boundary of the lit region and c(W ) = W 8 is the strength parameter for adjusting the proximity constraint. W is the largest image dimension (image width or height).</p><p>We constrain the sample regions near the target shadow boundary regions, and obtain the sample regions by dilating the mask of φ, such as the pink regions in Fig. <ref type="figure" target="#fig_7">7</ref>(e). We apply a two-step iterative method to get the optimized results. For each iteration, we first find T x by minimizing Equation ( <ref type="formula" target="#formula_18">12</ref>), then evaluate the value for each pixel x in φ. We repeat the two steps iteratively until satisfying the convergence value. Based on these two terms, for each patch in target shadow boundary regions, we can find a nearest patch in sample regions, which will be used for synthesizing the textures in target shadow boundary regions. As illustrated in Fig. <ref type="figure" target="#fig_7">7</ref> (f), with the controllable texture synthesis on shadow boundaries, the illumination and texture details are effectively recovered, and the results are also consistent with the surrounding regions.</p><p>In algorithm 1, we outline the main steps of the proposed shadow removal algorithm. Each step has been detailed in the previous sections. In our shadow removal systemwe only need to manually set three parameters: ω, λ 1 and λ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPLICATIONS</head><p>Our illumination recovering operator can be easily extended to image editing applications, such as shadow editing and color transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Shadow editing:</head><p>According to the image formation model <ref type="bibr" target="#b29">[30]</ref>, one pixel in an image can be represented as: I x = (ηL d + νL a )R x , the parameter η, ν depend on the light conditions and can be considered as the light attenuation factor or the object occluding factor. In the lit areas, η and ν are defined as 1. In the umbra areas, η is 0 and ν is 1. To remove the shadow, we add the direct illumination to the shadow areas, that is, setting η = 1 in the shadow areas. We can also modify the value of η and ν in specified areas for producing new shadow editing results, which corresponds to change the direct illumination or indirect illumination for the specified area.  Our illumination recovering operator can also be applied to soften the sharp shadow edges, as illustrated in Fig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8(e)</head><p>. Softening shadow is generated with illumination transition from the shadow areas to the lit areas. This process can be simulated using our method. As shown in Fig. <ref type="figure" target="#fig_9">8</ref>(d), we first specify a transition region around the shadow edges. For the transition region, we define a direct illumination attenuation function f x which varies depending on the distance between pixel x and the shadow edges. Then the shadow edge softening can be achieved by performing the following illumination recovering:</p><formula xml:id="formula_20">I edite x = (f x L d + L a )R x = fxt+υ αt+1 I x</formula><p>, where f x is the normalized light attenuation function, and 0 ≤ f x ≤ 1. The user can develop more sophisticated attenuation function to produce more physically realistic results.</p><p>2) Color transfer: Color and tone transfer is a popular topic in computer graphics <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Our illumination recovering operator can also work on color transfer. Given the sample image, we can transfer its color to the target image. More specially, to transfer the color information of the sample image to the target areas, we apply our illumination recovering operator on these areas. The color transfer result for the target result can be written as:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS AND DISCUSSIONS</head><p>In this section, we perform our shadow removal method on a variety of shadow images to illustrate the effectiveness of the proposed approach. We also present comparisons with the most related shadow removal methods.</p><p>The limitations of the proposed method are also given. All our results are implemented using C++ on a machine equipped with Pentium(R) Dual-Core CPU E5200@2.50GHz with 2GB RAM.</p><p>In Fig. <ref type="figure" target="#fig_1">12</ref>, we compare our method with the approach <ref type="bibr" target="#b7">[8]</ref>. Guo et al. <ref type="bibr" target="#b7">[8]</ref> segment the input image using texton histogram and SVM, but the divided regions are irregular. Large regions may contain several different kinds of colors and textures, which leads to calculation error for the ratio between direct light and environment light. This may result in an unsatisfactory shadow-free result, as shown in Fig. <ref type="figure" target="#fig_1">12</ref>(b). Our method introduces adaptive overlapped patches which alleviate the problem of inaccurate matching. Meanwhile, we calculate different ratio values between direct illumination and environment illumination for different patches, but Guo et al. <ref type="bibr" target="#b7">[8]</ref> uses the paired regions to compute a fixed ratio value for an image. The fixed ratio is impropriate for the shadow images with complex textures, which makes the shadow-free result unnatural, such as the result in Fig. <ref type="figure" target="#fig_1">12(b)</ref>.</p><p>In Fig. <ref type="figure" target="#fig_2">13</ref>, we present shadow removal results for boundary treatment. As illustrated in Fig. <ref type="figure" target="#fig_7">7</ref>, the texture details under the shadow boundaries may be blurred. We apply the texture and illumination optimization to recover the information of the shadow boundaries, and also present comparison results with previous methods. Xiao et al. <ref type="bibr" target="#b4">[5]</ref> processes the shadow boundary using alpha matte interpolation. As the interpolation process depends on the original texture details at the shadow boundary regions, this method cannot process complex shadow boundaries with loss of details. Shor et al. <ref type="bibr" target="#b9">[10]</ref> utilizes graph-cut based texture technique to repair the shadow boundaries. This method works well for image with rich texture, otherwise, the results will not be satisfied. The method of <ref type="bibr" target="#b0">[1]</ref> needs to locate the boundary precisely, and inaccurate shadow edges lead to fuzzy transition from interior shadow regions to nonshadow regions. Compared with aforementioned methods, our results are more visually natural and consistent with the surrounding content. The results in Fig. <ref type="figure" target="#fig_14">14</ref> demonstrate that our method is capable of dealing with nonuniform shadows. Note that in these images, the shadow intensity vary among the umbra, penumbra, and lit regions. In Fig. <ref type="figure" target="#fig_14">14</ref>, we compare our method with the related works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref> for handling nonuniform shadows. Since the relatively wide soft shadow on the shadow boundaries, the accurate shadow edges are difficult to be detected out. This makes Finlayson et al. <ref type="bibr" target="#b0">[1]</ref> impossible to recover the illumination in shadow regions. Shor et al. <ref type="bibr" target="#b9">[10]</ref> use fixed parameters for whole shadow regions. But the illumination is variable for soft shadows, it make this method failed to deal with nonuniform shadows. The unsatisfied results in Fig. <ref type="figure" target="#fig_14">14(d</ref>) demonstrate Xiao et al. <ref type="bibr" target="#b20">[21]</ref> also cannot treat nonuniform shadows.</p><p>By using adaptive texture patch matching, our method processes each small patch individually and remove the nonuniform shadows completely. In addition, the recovered illumination of nonuniform shadow regions is consistent with the surrounding environment. In Fig. <ref type="figure" target="#fig_0">15</ref>, we give shadow removal results for shadow images with complex textures, and compare our method with <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>. These images contain multiple textures or materials. Shor et al. <ref type="bibr" target="#b9">[10]</ref> fails to take texture types into account in shadow regions. This make the wrong recovered color in shadow regions, especially for images with large color variation between differen textures. The shadow removal method <ref type="bibr" target="#b9">[10]</ref> for simple texture image is not applicable to complex texture images. Xiao et al. <ref type="bibr" target="#b4">[5]</ref> take the texture classification into account and divides the shadow areas into several parts. The independent processing for each part induces inconsistency between subregions, as shown in the fifth row in Fig. <ref type="figure" target="#fig_0">15</ref>. Neither <ref type="bibr" target="#b0">[1]</ref> nor <ref type="bibr" target="#b7">[8]</ref> works well in removing shadows with complex texture or material content. Note that for the results of the sixth row in Fig. <ref type="figure" target="#fig_0">15</ref>, even the shadow regions have a variety of texture types, our method can effectively recover the texture information in the shadow regions. In addition, the illumination on the shadow boundaries is also effectively recovered, and the results are consistent with the surrounding regions.</p><p>Our method can also be applied to aerial remote sensing shadow images. In Fig. <ref type="figure" target="#fig_6">16</ref>, we compare our method to <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref> on aerial remote sensing images. The texture descriptor used in <ref type="bibr" target="#b4">[5]</ref> is not illumination independent which is not suited for subregion matching between shadow regions and nonshadow regions. Meanwhile, as demonstrated by <ref type="bibr" target="#b37">[38]</ref>, high resolution aerial images often contain heavy noises. The linear shadow-free algorithm <ref type="bibr" target="#b4">[5]</ref> does not  shadow free results, and can effectively recover the texture details in shadow regions which benefit the further image analysis.</p><p>In Fig. <ref type="figure" target="#fig_17">17</ref>, we compare our shadow removal results with ground truth images. We fix the camera on a tripod in the sun. Thereafter, we capture the shadow image and ground truth image with an occluder coming in and out. As illustrated in Fig. <ref type="figure" target="#fig_17">17</ref>, both the illumination and texture of our results are very close to those of the ground truth images.</p><p>The time consumption of the proposed method depends on the size of the shadow regions and the number of the sample patches in the lit regions. The time is mainly spent in patch matching, and the shadow removal processing.</p><p>Typically, for an image with size of 704 × 647 (for example, the third row in Fig. <ref type="figure" target="#fig_0">15</ref>), we decompose the shadows into 231 patches, and there are 236 lit patches. It takes about 0.9 seconds for performing the patch matching, and takes about 8.2 seconds for performing the shadow removal.</p><p>Limitations For some special shadow images, the pixels in the shadow patch are zero. In this case, it is difficult to find a corresponding patch in the lit regions. Furthermore, for the shadow images with heavy noise, the illumination recovered results may also contain some noises. Another limitation of our method is that when the textures of shadow regions and lit regions are quite different, there is no correctly matched lit patch for shadow patch. In this situation, if we apply the lit patch as the sample to remove shadows, the texture and illumination in the shadow regions cannot be accurately estimated, which induces to unnatural shadow removal results. As shown in Fig. <ref type="figure" target="#fig_18">18</ref>, the shadow regions are mountain and road, but the lit regions are sky and clouds, in this case, an unsatisfied image is obtained using the inappropriate matched information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we have presented a novel shadow removal approach using an illumination recovering optimization operator. We adaptively decompose the input image into overlapped patches according to the shadow distribution.</p><p>By building the correspondence between the shadow patch and the lit patch, we construct an illumination recovering operator which effectively remove the shadows and obtain natural transition result between patches through coherence optimization process. Our shadow removal system is simple and effective, and can handle a variety of complex shadows, including images with both sharp shadows and soft shadows.</p><p>In the current version, we perform image decomposition using adaptive method. In the future, we would like to apply the Poisson disk <ref type="bibr" target="#b38">[39]</ref> to generate adaptive patches, which is edge-aware, thus, our method may produce more accurate and pleasing shadow removal results. When we consider video sequence as video cube, our adaptive image decomposition, local illumination recovering optimization and nearest patch search can be easily extended to video data. In the future, we would like to extend our method to video shadow removal and editing. Another potential research topic is that, we can apply the histogram matching method used in the texture optimization to eliminate the possible texture blurring occurred in our illumination optimization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overview of our shadow removal system. (a) Input image, (b) nearest patch search for the shadow patches, (c) shadow removal result based on the patch pairs, (d) shadow-free result with coherence optimization, (e) final shadow-free result with boundary processing.</figDesc><graphic coords="2,76.72,373.39,88.92,90.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Shadow matte result. (a) Input image, (b) brushes for sample regions, (c) shadow matte map, where the white regions represent shadow regions, the black regions are lit regions and the rest is the shadow boundary, and the bottom left is close-up for the red box.</figDesc><graphic coords="6,127.01,75.59,117.00,119.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Overlap region between patches. Both the shadow region and lit regions are decomposed using overlapped patches. For each patch in the shadow regions, we can find its nearest patch in the lit regions for shadow removing.</figDesc><graphic coords="8,200.70,176.67,210.59,149.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Evaluation of the proposed texture descriptor. (a) The two source shadow patches are shown in white box, the target patches found by Gabor filter texture descriptor [5] are shown in red box, and our results are shown in green box, (b) close-ups for sample patches in (a).</figDesc><graphic coords="10,237.54,75.59,257.40,124.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 . 1 )</head><label>51</label><figDesc>Fig. 5. Local illumination recovering optimization. (a) Input image, (b) shadow removal result without using overlapped patches, (c) shadow removal result using overlapped patches and illumination consistency optimization.</figDesc><graphic coords="10,127.01,425.30,116.99,105.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) and the fourth column in Fig.15need such optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Texture detail enhancement. (a) Input images, (b) shadow removal results without texture detail covering enhancement, (c) shadow removal results with texture detail recovering, where λ 1 = 1, λ 2 = 2.0 in the first row and λ 1 = 0.5, λ 2 = 0.8 in the second row.</figDesc><graphic coords="12,364.24,144.07,109.46,111.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Shadow boundary processing. (a) Input images with shadows, (b) the image details, (c) shadow removal results without shadow boundary processing, (d) close-ups for the red boxes and the blue boxes, (e) the trimaps for shadow boundary regions and sample regions, white regions are the target shadow boundaries and the pink regions are the sample regions, (f) shadow removal results with shadow boundary processing, (g) close-ups for white box regions, where the bottom and top close-ups correspond to the results with and without shadow boundary processing, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 1 4 : 5 :</head><label>145</label><figDesc>Shadow removal algorithm Input: RGB image I Output: Shadow-free RGB image 1: Detect the shadow regions (Section III) 2: Decompose the input image into adaptive patches (Section V-A) 3: for each patch S i ∈ S do Patch matching: Find an optimal matching patch L j in L (Section V-B) Patch shadow removal: Remove the shadow in patch S i applying the Local illumination recovering operator on patch pair (S i , L j ) (Section IV) 6: end for 7: Illumination optimization: perform weighted average for the overlapped pixels (Section V-C) 8: Texture details recovering (optional) (Section V-C) 9: Boundary processing (optional) (Section V-D) We first specify a shadow sample and a nonshadow sample with similar texture. Then we utilize the illumination recovering operator and the relationship between L d and L a : L d = tL a for shadow editing, where t is computed based on the shadow sample and nonshadow sample in the input image. By setting the direct illumination and the indirect illumination, the intensity of the specified areas can be expressed as: I edit x = ηt+υ αt+1 I x . In Fig. 8 and Fig. 9, we give several shadow editing results by setting different values for η and ν. Compared with the shadow editing method [3] based on Poisson shadow interpolation and the illumination editing method [36], our method is easier to simulate a variety of lighting conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Shadow editing. (a) Input image, (b) the direction illumination of the shadow areas are tuned by setting η = 1/2, υ = 1, (c) shadow editing result, where η = 1, υ = 1 in the shadow areas and η = 1/5, υ = 1 in the lit areas, (d) the specified transition region for shadow edge softening, (e) shadow edges softening result with the attenuation function fx = √ 1 -dx, where dx is the normalization distance between the pixel Ix and the shadow boundary.</figDesc><graphic coords="14,353.95,467.14,88.92,112.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Image shadow removal and editing based on illumination recovering optimization. (a) Input image, (b) shadow removal result, (c) shadow editing for specific area with η = 0.1, υ = 0.4.</figDesc><graphic coords="15,127.01,75.59,117.00,62.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>+ 1)I x , where I x denotes the RGB components of the original image. Fig.10shows two examples for color transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Color transfer. (a) Sample images, (b) input target images, (c) color transfer results, where color of the red box in the sample image is transferred to the target image.</figDesc><graphic coords="15,127.01,539.33,117.00,79.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Shadow removal results. (a) Input images, (b) results of [8], (c) our results.</figDesc><graphic coords="17,76.72,334.45,88.92,55.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Nonuniform shadow removal result. (a) Input image, (b) result of [1], (c) result of [10], (d)results of [5], (e) our result.</figDesc><graphic coords="18,76.72,161.04,88.92,119.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>work well on aerial shadow image, as illustrated inFig 16(b). Li et al.<ref type="bibr" target="#b5">[6]</ref> removed the shadows of image using spatially adaptive NL operators. To reduce the influences of abundant texture and noise existing in aerial remote sensing images, they use NL Laplacian prior to the regularization terms in the optimizing function. This introduces texture detail blurring for the shadow-free result. As shown in Fig.16(c), the results are over smoothed. Furthermore, some shadows are not removed completely. Our method has a better matching accuracy which produces natural</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 .Fig. 16 .</head><label>1516</label><figDesc>Fig. 15. Shadow removal results for complex textures. The first row: input images, the second row: results of [1], the third row: results of [10], the fourth row: results of [8], the fifth row: results of [5], the sixth row: our results, the patch size is 15 × 15, 20 × 20, 30 × 30, 21 × 21, respectively.</figDesc><graphic coords="19,78.20,586.93,93.12,98.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Shadow removal results. (a) Input images, (b) ground truth, (c) our shadow removal results.</figDesc><graphic coords="21,127.01,180.77,116.99,98.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Limitation. (a) Input image, (b) our shadow removal result.</figDesc><graphic coords="21,140.45,433.35,163.80,119.25" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON IMAGE PROCESSING, 2014</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We can also simulate the color transfer processing in animated style. We add a color transfer parameter g x to 1057-7149 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2465159, IEEE Transactions on Image Processing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for their valuable comments and insightful suggestions.</p><p>We also thank H. Li, L. Zhang, and H. Shen for presenting aerial remote sensing shadow images used in Fig. <ref type="figure">16</ref>.</p><p>work was partly supported by the National Basic Research Program of China (No. 2012CB725303), the NSFC (No.61472288), NCET (NCET-13-0441) and the Key Grant Project of Hubei province (2013AAA02). Chunxia Xiao is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the removal of shadows from images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shadow removal using intensity surfaces and texture anchor points</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hel-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1202" to="1216" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Editing soft shadows in a digital photograph</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2007">2007</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">User-assisted image compositing for photographic lighting</title>
		<author>
			<persName><forename type="first">I</forename><surname>Boyadzhiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient shadow removal using subregion matching illumination transfer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="421" to="430" />
			<date type="published" when="2013">2013</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An adaptive nonlocal regularized shadow removal method for aerial remote sensing images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="120" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Geoscience and Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to recognize shadows in monochromatic natural images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single-image shadow detection and removal using paired regions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2033" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural shadow matting</title>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The shadow meets the mask: Pyramid-based shadow removal</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="577" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Texture-consistent shadow removal</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2008</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="437" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting moving shadows: algorithms and evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mikic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="918" to="923" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moving cast shadow detection using physics-based features</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="2310" to="2317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shadow removal using bilateral filtering</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4361" to="4368" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactive Shadow Removal from a Single Image Using Hierarchical Graph Cut</title>
		<author>
			<persName><forename type="first">D</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision C ACCV 2009</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deriving intrinsic images from image sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001. 2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="68" to="75" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shadow matting and compositing</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="494" to="500" />
			<date type="published" when="2003">2003</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Removing shadows from images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (4)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2353</biblScope>
			<biblScope unit="page" from="823" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intrinsic images by entropy minimization</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2004</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="582" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications, IEEE</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast shadow removal using adaptive multi-scale illumination transfer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2013">2013</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A bayesian approach for shadow extraction from a single image</title>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="480" to="487" />
		</imprint>
	</monogr>
	<note>Computer Vision, 2005. ICCV 2005</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shadow removal from single rgb-d images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsougenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3011" to="3018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shadow removal from a real image based on shadow density</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mukunoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Asada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2004 Posters</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple shadow remova</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fredembach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="832" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparative study on shadow compensation of color aerial images in invariant color models</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1661" to="1671" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Geoscience and Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive shadow detection using a blackbody radiator model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makarau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2049" to="2059" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Geoscience and Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast closed-form matting using a hierarchical data structure</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="49" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recovering intrinsic scene</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Barrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision systems</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Region covariance: A fast descriptor for detection and classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="589" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structure-preserving image smoothing via region covariances</title>
		<author>
			<persName><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">176</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sigma set: A small second order statistical region descriptor</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="1802" to="1809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 1998. Sixth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Texture optimization for example-based synthesis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="795" to="802" />
			<date type="published" when="2005">2005</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Illumination recovery from image with cast shadows via sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2366" to="2377" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-scale tone management for photographic look</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="637" to="645" />
			<date type="published" when="2006">2006</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cloud detection of rgb color aerial photographs by progressive refinement scheme</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="7264" to="7275" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Geoscience and Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ling Zhang Ling Zhang received the BSc degree in computer science and technology from Wuhan Donghu University in 2009, and received MSc degrees in Instructional Technology from Central China Normal University in 2012. Currently, she is working toward the PhD degree at the School of Computer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Wuhan University</publisher>
			<pubPlace>China</pubPlace>
		</imprint>
	</monogr>
	<note>Slic superpixels compared to state-of-the-art superpixel methods. and computational photography</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">His research interests include image and video processing Chunxia Xiao Chunxia Xiao received the BSc and MSc degrees from the Mathematics Department of Hunan Normal University in 1999 and 2002, respectively, and the PhD degree from the State Key Lab of CAD &amp; CG of Zhejiang University in 2006. Currently, he is a professor at the School of Computer</title>
		<imprint>
			<date type="published" when="2006-04">2011 and 2013. October 2006 to April 2007. February 2012 to February 2013</date>
			<pubPlace>Wuhan, China; Wuhan University, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Qing Zhang Qing Zhang received the BSc and MSc degrees in computer science and technology from Wuhan University ; Department of Computer Science and Engineering, Hong Kong University of Science and Technology ; University of California-Davis for one year</orgName>
		</respStmt>
	</monogr>
	<note>His main interests include image and video processing. digital geometry processing, and computational photography. He is a member of IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
