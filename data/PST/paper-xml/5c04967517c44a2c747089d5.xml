<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEVERAGING WEAKLY SUPERVISED DATA TO IMPROVE END-TO-END SPEECH-TO-TEXT TRANSLATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
							<email>jiaye@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
							<email>melvinp@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Naveen</forename><surname>Ari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stella</forename><surname>Laurenzo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">LEVERAGING WEAKLY SUPERVISED DATA TO IMPROVE END-TO-END SPEECH-TO-TEXT TRANSLATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech translation</term>
					<term>sequence-to-sequence model</term>
					<term>weakly supervised learning</term>
					<term>synthetic training data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-totranscript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recent advances in deep learning and more specifically in sequenceto-sequence modeling have led to dramatic improvements in ASR <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and MT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> tasks. These successes naturally led to attempts to construct end-to-end speech-to-text translation systems as a single neural network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Such end-to-end systems have advantages over a traditional cascaded system that performs ASR and MT consecutively in that they 1) naturally avoid compounding errors between the two systems; 2) can directly utilize prosodic cues from speech to improve translation; 3) have lower latency by avoiding inference with two models; and 4) lower memory and computational resource usage.</p><p>However, training such an end-to-end ST model typically requires a large set of parallel speech-to-translation training data. Obtaining such a large dataset is significantly more expensive than acquiring data for ASR and MT tasks. This is often a limiting factor for the performance of such end-to-end systems. Recently explored techniques to mitigate this issue include multi-task learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and pre-trained components <ref type="bibr" target="#b10">[11]</ref> in order to utilize weakly supervised data, i.e. speech-to-transcript or text-to-translation pairs, in contrast to fully supervised speech-to-translation pairs.</p><p>Although multi-task learning has been shown to bring significant quality improvements to end-to-end ST systems, it has two constraints which limit the performance of the trained ST model: 1) the shared components have to compromise between multiple tasks, which can limit their performance on individual tasks; 2) for each training example, the gradients are calculated for a single task, parameters are therefore updated independently for each task, which may lead to sub-optimal solution for the entire multi-task optimization problem.</p><p>In this paper, we train end-to-end ST models on much larger datasets than previous work, spanning up to 100 million training examples, including 1.3K hours of translated speech and 49K hours of transcribed speech. We confirm that multi-task learning and pretraining are still beneficial at such a large scale. We demonstrate that performance of our end-to-end ST system can be significantly improved, even outperforming multi-task learning, by using a large amount of data synthesized from weakly supervised data such as typical ASR or MT training sets. Similarly, we show that it is possible to train a high-quality end-to-end ST model without any fully supervised training data by leveraging pre-trained components and data synthesized from weakly supervised datasets. Finally, we demonstrate that data synthesized from fully unsupervised monolingual datasets can be used to improve end-to-end ST performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Early work on speech translation typically used a cascade of an ASR model and an MT model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, giving the MT model access to the predicted probabilities and uncertainties from the ASR. Recent work has focused on training end-to-end ST in a single model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In order to utilize both fully supervised data and also weakly supervised data, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> use multi-task learning to train the ST model jointly with the ASR and/or the MT model. By doing so, both of them achieved better performance with the end-to-end model than the cascaded model. <ref type="bibr" target="#b10">[11]</ref> conducts experiments on a larger 236 hour English-to-French dataset and pre-trains the encoder and decoder prior to multi-task learning, which further improves performance. However, the end-to-end model performs worse than the cascaded model in that work. <ref type="bibr" target="#b14">[15]</ref> shows that pre-training a speech encoder on one language can improve ST quality on a different source language.</p><p>Using TTS synthetic data for training speech translation was a requirement when no direct parallel training data is available, such as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. In contrast, we show that even when a large fully supervised training set is available, using synthetic training data from a high quality multi-speaker TTS system can further improve the performance of an end-to-end ST model. Synthetic data has also been used to improve ASR performance. <ref type="bibr" target="#b16">[17]</ref> builds a cycle chain between TTS and ASR models, in which the output from one model is used to help the training of the other. Instead of using TTS, <ref type="bibr" target="#b17">[18]</ref>  The MT synthetic data in this work helps the system in a manner similar to knowledge distillation <ref type="bibr" target="#b19">[20]</ref>, since the network is trained to predict outputs from a pretrained MT model. In contrast, synthesizing speech inputs using TTS is more similar to MT back-translation <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODELS</head><p>Similar to <ref type="bibr" target="#b8">[9]</ref>, we make use of three sequence-to-sequence models. Each one is composed of an encoder, a decoder, and an attention module. Besides the end-to-end ST model which is the major focus of this paper, we also build an ASR model and an MT model, which are used for building the baseline cascaded ST model, as well as for multi-task learning and encoder / decoder pre-training for ST. All three models represent text using the same shared English/Spanish Word Piece Model (WPM) <ref type="bibr" target="#b21">[22]</ref> containing 16K tokens.</p><p>ASR model: Our ASR model follows the architecture of <ref type="bibr" target="#b1">[2]</ref>. We use a 5 layer bidirectional LSTM encoder, with cell size 1024. The decoder is a 2 layer unidirectional LSTM with cell size 1024. The attention is 4-head additive attention. The model takes 80-channel log mel spectrogram features as input. MT model: Our MT model follows the architecture of <ref type="bibr" target="#b22">[23]</ref>. We use a 6 layer bidirectional LSTM encoder, with cell size of 1024. The decoder is an 8 layer unidirectional LSTM with cell size 1024, with residual connection across layers. We use 8-head additive attention. ST model: The encoder has similar architecture to the ASR encoder, and the decoder has similar architecture to the MT decoder. Throughout this work we experiment with varying the number of encoder layers. The model with the best performance is visualized in Figure <ref type="figure">1</ref>. It uses an 8 layer bidirectional LSTM for the encoder and an 8 layer unidirectional LSTM with residual connections for the decoder. The attention is 8-head additive attention, following the MT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SYNTHETIC TRAINING DATA</head><p>Acquiring large-scale parallel speech-to-translation training data is extremely expensive. The scarcity of such data is often a limiting factor on the quality of an end-to-end ST model. To overcome this issue, we use two forms of weakly supervised data by: synthesizing input speech corresponding to the input text in a parallel text MT training corpus, and synthesizing translated text targets from the output transcript in an ASR training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthesis with TTS model</head><p>Recent TTS systems are able to synthesize speech with close to human naturalness <ref type="bibr" target="#b23">[24]</ref>, in varied speakers' voices <ref type="bibr" target="#b24">[25]</ref>, create novel voices by sampling from a continuous speaker embedding space <ref type="bibr" target="#b25">[26]</ref>.</p><p>In this work, we use the TTS model trained on LibriSpeech <ref type="bibr" target="#b26">[27]</ref> from <ref type="bibr" target="#b25">[26]</ref>, except that we use a Griffin-Lim <ref type="bibr" target="#b27">[28]</ref> vocoder as in <ref type="bibr" target="#b28">[29]</ref> which has significantly lower cost, but results in reduced audio quality <ref type="foot" target="#foot_0">1</ref> . We randomly sample from the continuous speaker embedding space for each synthesized example, resulting in wide diversity in speaker voices in the synthetic data. This avoids unintentional bias toward a few synthetic speakers when using it to train an ST model and encourages generalization to speakers outside the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Synthesis with MT model</head><p>Another way to synthesize training data is to use an MT model to translate the transcripts in an ASR training set into the target language. In this work, we use the Google Translate service to obtain such translations. This procedure is similar to knowledge distillation <ref type="bibr" target="#b19">[20]</ref>, except that it uses the final predictions as training targets rather than the predicted probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and metrics</head><p>We focus on an English speech to Spanish text conversational speech translation task. Our experiments make use of three proprietary datasets, all consisting of conversational language, including: (1) an MT training set of 70M English-Spanish parallel sentences; (2) an ASR training set of 29M hand-transcribed English utterances, collected from anonymized voice search log; and (3) a substantially smaller English-to-Spanish speech-to-translation set of 1M utterances obtained by sampling a subset from the 70M MT set, and crowdsourcing humans to read the English sentences. The final dataset can be directly used to train the end-to-end ST model. We use data augmentation on both speech corpora by adding varying degrees of background noise and reverberation in the same manner as <ref type="bibr" target="#b1">[2]</ref>. The WPM shared among all models is trained with the 70M MT set.</p><p>We use two datasets for evaluation: a held out subset of 10.8K examples from the 1M ST set, which contains read speech, and another 8.9K recordings of natural conversational speech in a domain different from both the 70M MT and 29M ASR sets. Both eval sets contain English speech, English transcript and Spanish translation triples, so they can be used for evaluating either ASR, MT, or ST. ASR performance is measured in terms of Word Error Rate (WER) and translation performance is measured in terms of BLEU <ref type="bibr" target="#b29">[30]</ref> scores, both on case and punctuation sensitive reference text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baseline cascaded model</head><p>We build a baseline system by training an English ASR model and an English-to-Spanish MT model with the architectures described  <ref type="table" target="#tab_1">1</ref>, the ST BLEU is significantly lower than the MT BLEU, which is the result of cascading errors from the ASR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Baseline end-to-end models</head><p>We train a vanilla end-to-end ST model with a 5-layer encoder and an 8-layer decoder directly on the 1M ST set. We then adopt pretraining and multi-task learning as proposed in previous literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref> in order to improve its performance. We pre-train the encoder on the ASR task, and the decoder on the MT task as described in Section 5.2. After initialization with pre-trained components (or random values for components not being pre-trained), the ST model is fine-tuned on the 1M ST set. Finally, we make use of multi-task learning by jointly training a combined network on the ST, ASR, and MT tasks, using the 1M, 29M, 70M datasets, respectively. The ST sub-network shares the encoder with the ASR network, and shares the decoder with the MT network. For each training step, one task is sampled and trained with equal probability. Performance of these baseline models is shown in Table <ref type="table" target="#tab_2">2</ref>. Consistent with previous literature, we find that pre-training and multitask learning both significantly improve ST performance, because they increase the amount of data seen during training by two orders of magnitude. When both pre-training and multi-task learning are applied, the end-to-end ST model slightly outperforms the cascaded model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Using synthetic training data</head><p>We explore the effect of using synthetic training data as described in Section 4. To avoid overfitting to TTS synthesized audio (especially since audio synthesized using the Griffin-Lim algorithm contains obvious and unnatural artifacts), we freeze the pre-trained encoder 2 We report WER based on references which are case and punctuation sensitive in order to be consistent with the way BLEU is evaluated. The same ASR model obtains a WER of 6.9% (in-domain) and 14.1% (out-of-domain) if trained and evaluated on lower-cased transcripts without punctuation.  <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># additional encoder</head><p>but stack a few additional layers on top of it. The impact of adding different number of additional layers when fine-tuning with the 1M ST set is shown in Table <ref type="table" target="#tab_3">3</ref>. Even with no additional layer, this approach outperforms the fully trainable model (Table <ref type="table" target="#tab_2">2</ref> row 3) on the out-of-domain eval set, which indicates that the frozen pre-trained encoder helps the ST model generalize better. The benefit of adding extra layers saturates after around 3 layers. Following this result, we use 3 extra layers for the following experiments, as visualized in Figure <ref type="figure">1</ref>.</p><p>We analyze the effect of using different synthetic datasets in Table <ref type="table" target="#tab_4">4</ref> for fine-tuning. The TTS synthetic data is sourced from the 70M MT training set, by synthesizing English speech as described in Section 4.1. The synthesized speech is augmented with noise and reverberation following the same procedure as is used for real speech. The MT synthetic data is sourced from the 29M ASR training set, by synthesizing translation to Spanish as described in Section 4.2. The encoder and decoder are both pre-trained on ASR and MT tasks, respectively, but multi-task learning is not used.</p><p>The middle group in Table <ref type="table" target="#tab_4">4</ref> presents the result of fine-tuning with both synthetic datasets and the 1M real dataset, sampled with equal probability. As expected, adding a large amount of synthetic training data (increasing the total number of training examples by 1 -2 orders of magnitude), significantly improves performance on both indomain and out-of-domain eval sets. The MT synthetic data improves performance on the out-of-domain eval set more than it does on the in-domain set, partially because it contains natural speech instead of read speech, which is better matched to the out-of-domain eval set, and partially because it introduces more diversity to the training set and thus generalizes better. Fine-tuning on the mixture of the three datasets results in dramatic gains on both eval sets, demonstrating that the two synthetic sources have complementary effects. It also significantly outperforms the cascaded model.</p><p>The bottom group in Table <ref type="table" target="#tab_4">4</ref> fine-tunes using only synthetic data. Surprisingly, they achieve very good performance and even outperform training with both synthetic and real collected data on the out-of-domain eval set. This can be attributed to the increased Table <ref type="table">5</ref>. BLEU scores using fully trainable encoder, which performs worse than freezing lower encoder layers as in Table <ref type="table" target="#tab_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning set</head><p>In-domain Out-of-domain Real + one-speaker TTS synthetic 59. <ref type="bibr" target="#b4">5</ref> 19.5 Only one-speaker TTS synthetic 38.5 13.8</p><p>Table <ref type="table">6</ref>. BLEU scores when fine-tuning with synthetic speech data synthesized using a single-speaker TTS system, which performs worse than using the multi-speaker TTS as in Table <ref type="table" target="#tab_4">4</ref>.</p><p>sampling weight of training data with natural speech (instead of read speech). This result demonstrates the possibility of training a highquality end-to-end ST system with only weakly supervised data, by using such data for components pre-training and generating synthetic parallel training data from them by leveraging on high quality TTS and MT models or services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Importance of frozen encoder and multi-speaker TTS</head><p>To validate the importance of freezing the pre-trained encoder, we compare to a model where the encoder is fully fine-tuned on the ST task. As shown in Table <ref type="table">5</ref>, full encoder fine-tuning hurts ST performance, by overfitting to the synthetic speech. The ASR encoder learns a high quality latent representation of the speech content when pre-training on a large quantity of real speech with data augmentation. Additional fine-tuning on synthetic speech only hurts performance since the TTS data are not as realistic nor as diverse as real speech.</p><p>Similarly, to validate the importance of using a high quality multispeaker TTS system to synthesize training data with wide speaker variation, we train models using data synthesized with the single speaker TTS model from <ref type="bibr" target="#b23">[24]</ref>. This model generates more natural speech than the multi-speaker model used in Sec. 5.4 <ref type="bibr" target="#b25">[26]</ref>. To ensure a fair comparison, we use a Griffin-Lim vocoder and a 16 kHz sampling rate. We use the same data augmentation procedure described above.</p><p>Results are shown in Table <ref type="table">6</ref>. Even though a frozen pre-trained encoder is used, fine-tuning on only single-speaker TTS synthetic data still performs much worse than fine-tuning with multi-speaker TTS data, especially on the out-of-domain eval set. However, when trained on the combination of real and synthetic speech, performance on the in-domain eval set is not affected. We conjecture that this is because the in-domain eval set consists of read speech, which is better matched to the prosodic quality of the single-speaker TTS model. The large performance degradation on the out-of-domain eval set again indicates worse generalization.</p><p>Incorporating recent advances in TTS to introduce more natural prosody and style variation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> to the synthetic speech might further improve performance when training on synthetic speech. We leave such investigations as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Utilizing unlabelled monolingual text or speech</head><p>In this section, we go further and show that unlabeled monolingual speech and text can be leveraged to improve performance of an end-to-end ST model, by using them to synthesize parallel speechto-translation examples using available ASR, MT, and TTS systems. We take the English text from the 70M MT set as an unlabeled text set, synthesize English speech for it using a multi-speaker TTS model as in Section 5.4, and translate it to Spanish using the Google Translate service. Similarly, we take the English speech from the 29M ASR set as an unlabeled speech set, synthesize translation targets for it by using the cascaded model we build in Section 5.2. We use this cascaded model only to enable comparison to its own performance. Replacing it with a cascade of other ASR and MT models or services should not change the conclusion. Since there is no parallel training data for ASR or MT in this case, pre-training does not apply. We use the vanilla model with a 5-layer encoder as in Section 5.3.</p><p>Results are presented in Table <ref type="table" target="#tab_5">7</ref>. Even though these datasets are highly synthetic, they still significantly improve performance over the vanilla model. Because the unlabeled speech is processed with weaker models, it doesn't bring as much gain as the synthetic set from unlabeled text. Since it essentially distills knowledge from the cascaded model, it is also understandable that it does not outperform it. Performance is far behind our best results in Section 5.4 since pre-training is not used. Nevertheless, this result demonstrates that with access to high-quality ASR, MT, and/or TTS systems, one can leverage large sets of unlabeled monolingual data to improve the quality of an end-to-end ST system, even if a small amount of direct parallel training data are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>We propose a weakly supervised learning procedure that leverages synthetic training data to fine-tune an end-to-end sequence-tosequence ST model, whose encoder and decoder networks have been separately pre-trained on ASR and MT tasks, respectively. We demonstrate that this approach outperforms multi-task learning in experiments on a large scale English speech to Spanish text translation task. When utilizing synthetic speech inputs, we find that it is important to use a high quality multispeaker TTS model, and to freeze the pre-trained encoder to avoid overfitting to synthetic audio. We explore even more impoverished data scenarios, and show that it is possible to train a high quality end-to-end ST model by fine-tuning only on synthetic data from readily available ASR or MT training sets. Finally, we demonstrate that a large quantity of unlabeled speech or text can be leveraged to improve an end-to-end ST model when a small fully supervised training corpus is available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fine</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance of the baseline cascaded ST system and the underlying ASR and MT components on both test sets.</figDesc><table><row><cell cols="5">Task Metric In-domain Out-of-domain</cell></row><row><cell cols="2">ASR WER 2</cell><cell>13.7%</cell><cell>30.7%</cell></row><row><cell>MT</cell><cell>BLEU</cell><cell>78.8</cell><cell>35.6</cell></row><row><cell>ST</cell><cell>BLEU</cell><cell>56.9</cell><cell>21.1</cell></row><row><cell></cell><cell></cell><cell cols="3">In-domain Out-of-domain</cell></row><row><cell>Cascaded</cell><cell></cell><cell></cell><cell>56.9</cell><cell>21.1</cell></row><row><cell>Vanilla</cell><cell></cell><cell></cell><cell>49.1</cell><cell>12.1</cell></row><row><cell>+ Pre-training</cell><cell></cell><cell></cell><cell>54.6</cell><cell>18.2</cell></row><row><cell cols="3">+ Pre-training + Multi-task</cell><cell>57.1</cell><cell>21.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>BLEU scores of baseline end-to-end ST and cascaded models. All end-to-end models use 5 encoder and 8 decoder layers.in Section 3, and cascading them together by feeding the predicted transcript from the ASR model as the input to the MT model. The ASR model is trained on a mixture of the 29M ASR set and the 1M ST set with 8:1 per-dataset sampling probabilities in order to better adapt to the domain of the ST set. The MT model is trained on the 70M MT set, which is a superset of the 1M ST set. As shown in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>BLEU scores of the extended ST model, varying the number of additional encoder layers on top of the frozen pre-trained ASR encoder. The decoder is pre-trained but kept trainable. Multi-task learning is not used.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>layers</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>In-domain</cell><cell cols="5">54.5 55.7 56.1 55.9 56.1</cell></row><row><cell cols="6">Out-of-domain 19.5 18.8 19.3 19.5 19.6</cell></row><row><cell>Fine-tuning set</cell><cell></cell><cell cols="4">In-domain Out-of-domain</cell></row><row><cell>Real</cell><cell></cell><cell>55.9</cell><cell></cell><cell>19.5</cell></row><row><cell cols="2">Real + TTS synthetic</cell><cell>59.5</cell><cell></cell><cell>22.7</cell></row><row><cell cols="2">Real + MT synthetic</cell><cell>57.9</cell><cell></cell><cell>26.2</cell></row><row><cell cols="2">Real + both synthetic</cell><cell>59.5</cell><cell></cell><cell>26.7</cell></row><row><cell cols="2">Only TTS synthetic</cell><cell>53.9</cell><cell></cell><cell>20.8</cell></row><row><cell cols="2">Only MT synthetic</cell><cell>42.7</cell><cell></cell><cell>26.9</cell></row><row><cell cols="2">Only both synthetic</cell><cell>55.6</cell><cell></cell><cell>27.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>BLEU scores of ST trained with synthetic data. All rows use the same model architecture as Figure</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>BLEU scores for the vanilla model trained on synthetic data generated from unlabeled monolingual data, without pre-training. Even though such datasets are highly synthetic, they can still benefit an ST model trained with as many as 1M real training examples.</figDesc><table><row><cell>Training set</cell><cell cols="2">In-domain Out-of-domain</cell></row><row><cell>Real</cell><cell>49.1</cell><cell>12.1</cell></row><row><cell>Real + Synthetic from text</cell><cell>55.9</cell><cell>19.4</cell></row><row><cell>Real + Synthetic from speech</cell><cell>52.4</cell><cell>15.3</cell></row><row><cell>Real + Synthetic from both</cell><cell>55.8</cell><cell>16.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Synthetic waveforms are needed only for audio data augmentation. Otherwise, the mel-spectrogram predicted by the TTS model can be directly fed as input to the ST or ASR models, bypassing the vocoder.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>The authors thank Patrick Nguyen, Orhan Firat, the Google Brain team, the Google Translate team, and the Google Speech Research team for their helpful discussions and feedback, as well as Mengmeng Niu for her operational support on data collection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NeurIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Listen and translate: A proof of concept for end-to-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on End-to-end Learning for Speech and Audio Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models can directly translate foreign speech</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tied multitask learning for neural speech translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end automatic speech translation of audiobooks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech translation: Coupling of recognition and translation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the integration of speech recognition and statistical machine translation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanthak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Speech Communication and Technology</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved speech-to-text translation with the Fisher and Callhome Spanish-English speech translation corpus</title>
		<author>
			<persName><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IWSLT</title>
				<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pre-training on high-resource speech recognition improves low-resource speech-to-text translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01431</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured-based curriculum learning for end-to-end english-japanese speech translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine speech chain with one-shot speaker adaptation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-modal data augmentation for end-to-end ASR</title>
		<author>
			<persName><forename type="first">A</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Back-translation-style data augmentation for end-to-end ASR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10893</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Deep Learning and Representation Learning Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Japanese and Korean voice search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Association for Computational Linguistics (ACL)</title>
				<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep voice 3: 2000-speaker neural text-to-speech</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Transfer learning from speaker verification to multispeaker text-to-speech synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>in Advances in NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lib-riSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Signal estimation from modified shorttime Fourier transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tacotron: Towards end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Association for Computational Linguistics (ACL)</title>
				<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards endto-end prosody transfer for expressive speech synthesis with Tacotron</title>
		<author>
			<persName><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical generative modeling for controllable speech synthesis</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
