<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is ChatGPT a Good Recommender? A Preliminary Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-20">20 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junling</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Zhou</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Renjie</forename><surname>Lv</surname></persName>
							<email>lvrenjie.lrj@antgroup.com</email>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><forename type="middle">2023</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><surname>Is</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<country>Alibaba Group China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group China Yan Zhang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<postCode>03-05, 2018</postCode>
									<settlement>June, Woodstock</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Is ChatGPT a Good Recommender? A Preliminary Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-20">20 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2304.10149v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large-Language Model</term>
					<term>ChatGPT</term>
					<term>Recommendation System</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Further, we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests. Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT has achieved promising results in certain tasks and is capable of reaching the baseline level in others. We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models. And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results. We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As a crucial technique for addressing information overload and enhancing user experience, recommendation systems have witnessed significant advancements over the past decade and have been widely used in various web applications such as product recommendation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59]</ref>, video recommendation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b65">66]</ref>, news recommendation <ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref>, music recommendation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref> and so on. In the meanwhile, with the development of deep learning, recommendation systems have gone through several stages. In early ages, collaborative filtering-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b61">62]</ref> are primarily used to model the user's behavior patterns from the user-item interactions. Later on, with the introduction of user and item side information into recommendation systems, content-based recommendation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref> and knowledge-based recommendation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> have gained attention due to their ability to provide personalized recommendations.</p><p>However, most traditional recommendation methods are taskspecific. Therefore, specific data is required to train specific models for different tasks or application scenarios, which lack efficient generalization ability. To address this issue, researchers have shifted their focus towards implementing Pretrained Language Models (PLMs) in recommendation scenarios since PLMs have demonstrated impressive adaptability to improve the performance of downstream NLP tasks significantly. To effectively convert user interaction data into text sequences, a variety of prompts <ref type="bibr" target="#b63">[64]</ref> is designed to convert user interaction data into text sequences. Furthermore, P5 <ref type="bibr" target="#b18">[19]</ref> and M6-Rec <ref type="bibr" target="#b10">[11]</ref> focus on building a foundation model to support a wide range of recommendation tasks.</p><p>Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models, making it a valuable tool for businesses and organizations. Chataug et al. <ref type="bibr" target="#b11">[12]</ref> leverages ChatGPT to rephrase sentences for text data augmentation. Jiao et al. <ref type="bibr" target="#b22">[23]</ref> finds the translation ability of Chat-GPT performs competitively with commercial translation products on high-resource and low-resource languages. Bang et al. <ref type="bibr" target="#b2">[3]</ref> finds ChatGPT outperforms the previous state-of-the-art zero-shot model by a large margin in the sentiment analysis task. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated, and whether ChatGPT can perform well on classical recommendation tasks remains an open question. Therefore, it is necessary to establish a benchmark to preliminarily evaluate and compare ChatGPT with traditional recommendation models, thereby providing valuable insights and facilitating further exploration of the potential of large-scale language models in recommendation systems.</p><p>To bridge this research gap, in this paper, we directly employ ChatGPT as a general-purpose recommendation model that can handle various recommendation tasks, and attempt to explore whether the extensive linguistic and world knowledge acquired from largescale corpora can be effectively transferred to recommendation scenarios. Our main contribution is the construction of a benchmark to track ChatGPT's performance in recommendation scenarios, and a comprehensive analysis and discussion of its strengths and limitations. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Furthermore, we explore the use of few-shot prompting to inject interaction information that contains user potential interests to help ChatGPT better understand user needs and preferences.</p><p>Comprehensive experimental results on Amazon Beauty dataset reveal that, from the perspective of accuracy, ChatGPT performs well in rating prediction but poorly in sequential and direct recommendation tasks, achieving only similar performance levels to early baseline methods on certain metrics. On the other hand, while ChatGPT demonstrates poor performance in terms of objective evaluation metrics for explainable recommendation tasks such as explanation generation and review summarization, our additional human evaluations show that ChatGPT outperforms state-of-theart methods. This highlights the limitations of using an objective evaluation approach to accurately reflect ChatGPT's true explainable recommendation capabilities. Furthermore, despite ChatGPT's unsatisfactory performance in accuracy-based recommendation tasks, it is worth noting that ChatGPT has not been specifically trained on any recommendation data. Thus, there is still significant potential for improvement in future research by incorporating more relevant training data and techniques. We believe that our benchmark not only sheds light on ChatGPT's recommendation capabilities but also provides a valuable starting point for researchers to better understand the advantages and shortcomings of Chat-GPT in recommendation tasks. Moreover, we hope that our study can inspire researchers to design new methods that leverage the strengths of language models like ChatGPT to improve recommendation performance, and contribute to the advancement of the recommendation systems field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Large Language Models and ChatGPT</head><p>Language Models (LMs) are a fundamental component of natural language processing (NLP) and have been the focus of research for several decades. Recently, the emergence of large-scale LMs has led to significant progress in NLP tasks such as machine translation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b60">61]</ref>, summarization <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b45">46]</ref>, and dialogue generation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Large Language Models (LLMs) are a subclass of LMs that leverage massive amounts of data and computational resources to achieve state-of-the-art performance on a wide range of NLP tasks. The history of LLMs can be traced back to the early work on neural networks and language modeling. <ref type="bibr" target="#b3">[4]</ref> introduced neural language models that learned to predict the next word in a sentence given the previous words. Later, the development of recurrent neural networks (RNNs) and long short-term memory (LSTM) networks further improved the ability of models to capture long-term dependencies in language <ref type="bibr" target="#b21">[22]</ref>. However, traditional neural language models still struggled with capturing the rich semantic and contextual relationships present in natural language. The introduction of the Transformer architecture by <ref type="bibr" target="#b51">[52]</ref> was a major breakthrough in this area. The Transformer model utilizes self-attention mechanisms to capture the relationships between all elements in a sequence simultaneously, allowing for more comprehensive contextual understanding. This architecture has been used as the backbone of many successful LLMs, including BERT <ref type="bibr" target="#b12">[13]</ref>, GPT-2 <ref type="bibr" target="#b40">[41]</ref>, and XLNet <ref type="bibr" target="#b59">[60]</ref>.</p><p>ChatGPT <ref type="bibr" target="#b37">[38]</ref> is a state-of-the-art dialogue system developed by OpenAI in 2022. It is a state-of-the-art natural language processing (NLP) model that has been widely used in various vertical domains, such as text generation and dialogue systems. In text generation, ChatGPT has shown impressive results in generating coherent and diverse text, surpassing the performance of previous models <ref type="bibr" target="#b6">[7]</ref>. In dialogue systems, it has been used for task-oriented and open-domain conversations, achieving state-of-the-art performance in both settings <ref type="bibr" target="#b64">[65]</ref>. Although the value of ChatGPT has been validated in various fields, whether it can still be effective in the recommendation domain remains an under-explored topic, which motivates us to construct such a benchmark to gain insights into the potential of large language models for recommendation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Model for Recommendation</head><p>Language Models (LMs), such as BERT <ref type="bibr" target="#b12">[13]</ref> and GPT <ref type="bibr" target="#b37">[38]</ref>, have demonstrated impressive adaptability to improve the performance of downstream NLP tasks significantly, thanks to extensive linguistic and world knowledge learned from large-scale corpora. Inspired by these achievements, an increasing amount of attention is being paid for the application of LMs in recommender scenarios, yielding several recent breakthroughs in this field. For instance, LMRec-Sys <ref type="bibr" target="#b63">[64]</ref> utilizes prompts to reconstitute some recommendation tasks as multi-token cloze tasks, aiming to address zero-shot and data efficiency issues. P5 <ref type="bibr" target="#b18">[19]</ref> is the first attempt to integrate different recommendation tasks within a shared conditional language generation framework (i.e., T5 <ref type="bibr" target="#b41">[42]</ref>). To effectively convert user interaction data into text sequences, a variety of prompts are desgined to accomodate the specific characteristics of each recommendation task. Similarly, M6-Rec <ref type="bibr" target="#b10">[11]</ref> focuses on building a foundation model to support a wide range of recommendation tasks, including retrieval, ranking, and explanation generation, etc. Notably, the authors also provide practical solutions for model deployment in real-world settings. Chat-REC <ref type="bibr" target="#b16">[17]</ref>, a concurrent work closely related to our study, leverages ChatGPT as an interface for conversational recommendations, thereby augmenting the performance of existing recommender models and rendering the recommendation process more interactive and explainable.</p><p>Different from Chat-REC, our work is inspired by P5 and treats ChatGPT as a self-contained recommendation system that does not rely on any external systems. Based on this, we conduct a thorough evaluation and comparison of its performance on classic recommendation tasks including sequential recommendation, rating prediction, etc. By doing so, we hope our analysis can offer valuable insights for researchers to delve deeper into the potential of large-scale language models in the domain of recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RECOMMENDATION WITH CHATGPT</head><p>The workflow of using ChatGPT to complete recommendation tasks is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, which consists of three steps. First, different prompts are constructed based on the specific characteristics of the recommendation tasks (Section 3.1). Second, these prompts are used as inputs for ChatGPT, which generates the recommendation results according to the requirements specified in the prompts. Finally, the output from ChatGPT is checked and refined by the refinement module, and the refined results are returned to the user as the final recommendation results (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task-specific Prompt Construction</head><p>In this section, we investigate the recommendation capability of ChatGPT by designing prompts tailored to different tasks. Each prompt comprises three parts: task description, behavior injection, and format indicator. The task description is utilized to adapt recommendation tasks to natural language processing tasks. The behavior injection is designed to assess the impact of few-shot prompting, which incorporates user-item interaction to aid ChatGPT in capturing user preferences and needs more effectively. The format indicator serves to constrain the output format, making the recommendation results more comprehensible and assessable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Rating Prediction.</head><p>Rating prediction is a crucial task in recommendation systems that aims to predict the ratings that a user would give to a particular item. This task is essential in personalizing recommendations for users and improving the overall user experience. Some recent advancements in this field include the use of deep learning models <ref type="bibr" target="#b19">[20]</ref>, and the use of matrix factorization techniques <ref type="bibr" target="#b25">[26]</ref>, which are effective in dealing with the sparsity problem in recommendation systems. In line with the innovative recommendation paradigm of the LLM, we conducted experiments on a rating task that involved formulating two unique prompt types to elicit the results. We provide some sample prompts in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Sequential</head><p>Recommendation. Sequential recommendation is a subfield of recommender systems that aims to predict a user's next item or action based on their past sequential behavior. It has received increasing attention in recent years due to its potential applications in various domains, such as e-commerce, online advertising, and music recommendation. In sequential recommendation, researchers have proposed various methods, including recurrent neural networks <ref type="bibr" target="#b30">[31]</ref>, contrastive learning <ref type="bibr" target="#b67">[68]</ref>, and attention-based models <ref type="bibr" target="#b51">[52]</ref>, for capturing the temporal dependencies and patterns in user-item interactions. We have devised three distinct prompt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rating Prediction</head><p>Here is user rating history:  formats for the sequential recommendation task family. These include: 1) direct prediction of the user's next item based on their interaction history, 2) selection of a possible next item from a list of candidates, where only one item is positive and based on the user's interaction history, and 3) prediction of whether a specific item will be the next one interacted with by the user, using their previous interaction history as a basis. These prompt formats have been designed to enhance the accuracy and effectiveness of sequential recommendations, and are grounded in rigorous academic principles. Examples of these prompts can be seen in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Direct Recommendation.</head><p>Direct Recommendation, also known as explicit feedback recommendation or rating-based recommendation, is a type of recommendation system that relies on explicit feedback from users in the form of ratings or reviews. Unlike other recommendation systems that rely on implicit feedback, such as user behavior or purchase history, direct recommendation systems are able to provide more personalized and accurate recommendations by taking into account the explicit preferences of users. For this task, we develop the item selection prompt that selects the most appropriate item from a list of potential candidates. These prompt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explanation Generation</head><p>Here are some recommended products and their corresponding explanations for user: 1. product "TIGI Catwalk Curl Collection Curlesque Curls Rock Amplifier, 5.07 Ounce, Packaging May Vary" and its explanation "One of the few things I have found that work for white people with curly hair" 2. product "DevaCurl Mist-er Right Lavender Curl Revitalizer 12.0 oz" and its explanation "it makes my hair greasy and gross" Help user generate a 5.0-star explanation about this product "SHANY Nail Art Set (24 Famouse Colors Nail Art Polish, Nail Art Decoration)" with around 10 words?</p><p>Here is user's interaction history: 1. Prolab Caffeine-Maximum Potency 200 mg 100 Tablets 2. DevaCurl Mist-er Right Lavender Curl Revitalizer 12.0 oz 3. TIGI Catwalk Curl Collection Curlesque Leave-In Conditioner, 7.27 Ounce 4. e.l.f. Pigment Eyeshadow, Naturally Nude, 0.05 Ounce Help user generate a 5.0-star explanation about this product "SHANY Nail Art Set (24 Famouse Colors Nail Art Polish, Nail Art Decoration)" with around 10 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>zero-shot few-shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review Sumarization</head><p>Here are some reviews and their corresponding summaries of user: 1. Review:"After watching kardashian episode back in 2009 kim mentioned OPI my private jet.. and i was like what is that? i looked it up online and LOVED it and i just got it 1 week shipping.. awesomee just love this color its sparkley brown and its turns black sometimes cool!! well for mee looolll LOVE this color &amp;lt;3 on my toes and fingers lol". Summary:"Loving this sooo muchh" 2. Review:"I love this and im glad im adding this to my collection! (: a nice top coat or alone very shimmery and very pretty, especially the top brush cap thing its silver than the original black! (: i recommend this". Summary:"Amazing color" Write a short sentence to summarize the following product review from user: "So i was pretty excited that i got this in the mail, but seriously.....i think its just the color of mine, i don't know, not good cover stick... Received it sticking to the top...so basically it was broken when i opened it because of the air mail looks very pasty...very white i shall say...im never buying this product ever..". The sentence should be around 4 words.</p><p>Here are some summaries of user: 1. "Loving this sooo muchh" 2. "Amazing color" 3. "..Hong Kong Collection &amp;lt;3 OPI" Write a short sentence to summarize the following product review from user: "So i was pretty excited that i got this in the mail, but seriously.....i think its just the color of mine, i don't know, not good cover stick... Received it sticking to the top...so basically it was broken when i opened it because of the air mail looks very pasty...very white i shall say...im never buying this product ever..". The sentence should be around 4 words. formats are based on rigorous academic principles and aim to optimize the accuracy and relevance of recommendations. Examples of these prompts can be seen in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>zero-shot few-shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Explanation Generation.</head><p>Explanation generation refers to providing users or system designers with explanations to clarify why such items are recommended. In this way, it enhances the transparency, persuasiveness, effectiveness, trustworthiness, and user satisfaction of recommendation systems. Furthermore, it facilitates system designers in diagnosing, debugging, and refining the recommendation algorithm. Large language models such as ChatGPT can use the vast amount of knowledge they contain to learn the user's interests through their historical interaction records and provide reasonable explanations for their behavior. Specifically, We ask ChatGPT model to generate a textual explanation to justify a user's preference towards a selected item as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. For each category, additional auxiliary information such as the hint word and the star rating could be included.</p><p>3.1.5 Review Summarization. Automatic generation of summaries is becoming increasingly important in Natural Language Processing, as the demand for concise and easily comprehensible content continues to grow. Similar to the explanation generation task, we create two types of prompts: zero/few-shot prompts, and provide some example prompts in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Output Refinement</head><p>To ensure the diversity of generated results, ChatGPT incorporates a degree of randomness into its response generation process, which may result in different responses for the same input. However, when using ChatGPT for recommendation, this randomness can sometimes cause difficulties in evaluating the recommended items. While the format indicator in the prompt construction can partially alleviate this issue, in practical usage, it still cannot guarantee the anticipated output format. Therefore, we devise output refinement module to check the format of ChatGPT's output. If the output passes the format check, it is directly used as the final output. If not, it is modified based on pre-defined rules. If the format correction is successful, the corrected result is used as the final output. If not, the corresponding prompt is fed into ChatGPT for a re-recommendation until the format requirements are met. It is worth noting that different tasks have different output format requirements when evaluating ChatGPT. For example, for rating prediction, only a specific score is needed, whereas for sequential or direct recommendation, a list of recommended items is required. Particularly for sequence recommendation, it is challenging to feed all the items in the dataset to ChatGPT at once. As a result, Chat-GPT's output may not correctly match the item set in the dataset. To address this issue, we introduce a text matching method based on similarity in the correction process to map ChatGPT's predictions back to the original dataset. Although this method may not perfectly reflect ChatGPT's ability, it can still indirectly demonstrate its potential in sequential recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>To evaluate ChatGPT, we conduct extensive experiments on the realworld Amazon dataset. Through the performance comparison with various representative methods and ablation studies on different tasks, we aim to answer the following research questions:</p><p>? RQ1: How does ChatGPT perform as compared with the state-of-the-art baseline models? ? RQ2: What is the impact of few-shot prompting on performance? ? RQ3: How do we design the human evaluation to assess explanation generation and summarization tasks?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>4.1.1 Datasets. We conduct numerical and human evaluations on the real-world Amazon recommendation dataset. The Amazon dataset contains the customer review text with accompanying metadata on 29 categories of products. This paper focuses on evaluating the Beauty category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Metrics.</head><p>In numerical evaluations, we employ Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) for rating prediction. And we adopt top-k Hit Ratio (HR@k), top-k Normalized Discounted Cumulative Gain (NDCG@k) for sequential recommendation and direct recommendation which are widely used in related works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b66">67]</ref>. Specifically, we report results on HR@{1,5,10}, NCGG@{5,10} for evaluation. Besides, n-gram Bilingual Evaluation Understudy (BLEU-n) and n-gram Recall-Roiented Understudy for Gising Evaluation (ROUGE-n) are used to evaluate the explanation generation and review summarization tasks. In human evaluations, we have designed and deployed a crowdsourcing task to assess the qualities of the generated explanations and review summaries. Through this task, we aim to accurately evaluate the effectiveness of the content by gathering feedback from a diverse range of human evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details.</head><p>In order to verify that we can directly apply the knowledge learned by ChatGPT to recommendation scenarios without the need for a large amount of task-specific data for training, we apply gpt-3.5-turbo to conduct few-shot and zeroshot experiments for the five tasks mentioned above. We collect n items that users have interacted with and k shots of historical records to enable ChatGPT to learn users' interests implicitly. In this experiment, we use the titles of the items as meta information, and set ? = 10 and ? = 3 due to the limitation of a maximum context length of 4096 tokens in ChatGPT. We ramdomly sample 100 records from the test set proposed by P5 <ref type="bibr" target="#b18">[19]</ref> for evaluation.</p><p>For direct recommendation, we set the number of negative samples to 99, thus forming a candidate list of length 100 with one positive item. Also, due to the addition of the candidate pool in the request, we set the number of shots to 1. For sequential recommendation, we input the user's historical interacted items in order and let ChatGPT predict the title of the next item that the user might interact with, and use BERT <ref type="bibr" target="#b12">[13]</ref> to calculate the vector of the predicted title and compute the similarity between the predicted title vector and the title vectors of all items, and select the item with the highest similarity as the predicted item. For human evaluation on explanation generation and review summarization, we sample some results of different methods for each task, and each result will be scored and ranked by three human evaluators. After obtaining the manually annotated results, we will calculate the average top1 ratio and average ranking position of different methods to measure their generation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines for multiple tasks</head><p>Following P5 <ref type="bibr" target="#b18">[19]</ref>, we gather a range of approaches that are representative of various tasks. For rating prediction, we employ MF <ref type="bibr" target="#b24">[25]</ref> and MLP <ref type="bibr" target="#b9">[10]</ref> as our baselines, both evaluated using mean square root loss. For direct recommendation, we use BPR-MF <ref type="bibr" target="#b42">[43]</ref>, BPR-MLP <ref type="bibr" target="#b9">[10]</ref> and SimpleX <ref type="bibr" target="#b34">[35]</ref> as baselines. For sequential recommendation, we adopt Caser <ref type="bibr" target="#b49">[50]</ref>, HGN <ref type="bibr" target="#b33">[34]</ref>, GRU4Rec <ref type="bibr" target="#b20">[21]</ref>, BERT4Rec <ref type="bibr" target="#b47">[48]</ref>, FDSA <ref type="bibr" target="#b62">[63]</ref>, SASRec <ref type="bibr" target="#b23">[24]</ref> and S 3 -Rec <ref type="bibr" target="#b66">[67]</ref> as baselines for comparison. For explanation generation, we utilize Attn2Seq <ref type="bibr" target="#b14">[15]</ref>, NRT <ref type="bibr" target="#b29">[30]</ref> and PETER <ref type="bibr" target="#b28">[29]</ref> as baselines. For review summarization, we adopt pretrained T0 <ref type="bibr" target="#b44">[45]</ref> and GPT-2 <ref type="bibr" target="#b40">[41]</ref> as baselines. For more details, you can refer to P5 <ref type="bibr" target="#b18">[19]</ref> or relevant articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Comparison on 5 Tasks (RQ1&amp;2)</head><p>4.3.1 Rating prediction. To evaluate the rating prediction performance of ChatGPT, zero-shot and few-shot prompts were employed, and the results obtained from the Beauty dataset were summarized in Tab.1. The results indicate that, for the seen category on the Beauty dataset, few-shot prompts outperform MF and MLP in terms of both MAE and RMSE. These results provide evidence supporting the feasibility of utilizing a conditional text generation framework for rating prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Sequential recommendation.</head><p>To assess the sequential recommendation capability of ChatGPT, we conducted both zero-shot and We found that, compared to the baselines, ChatGPT's performance in the zero-shot prompting setup is considerably inferior, with all metrics being significantly lower than the baselines. However, under the few-shot prompting setup, while there is a relative improvement in performance, such as NDCG@5 surpassing GRU4Rec, ChatGPT is still generally outperformed by classical sequential recommendation methods in most cases. There are possibly two main reasons contributing to this outcome: First, during the prompting design process, all items are represented by their titles. Although this approach can alleviate the cold-start problem to some extent, it may cause ChatGPT to focus more on semantic similarity rather than the transition relationships between items, which are crucial for effective recommendations. Second, due to the length constraint of the prompts, it is not possible to input all items from the item set into ChatGPT. This leads to ChatGPT lacking constraints in predicting the title of the next item, resulting in generating item titles that do not exist in the dataset. Although it is possible to map these predicted titles to existing titles in the dataset through semantic similarity matching, our experiments show that this mapping does not result in significant gains. Therefore, for sequential recommendation tasks, merely employing ChatGPT is not a suitable choice. Further exploration is needed to introduce more guidance and constraints to help ChatGPT accurately capture historical interests and make reasonable recommendations within a limited scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Direct recommendation. Tab.3 illustrates the performance of</head><p>ChatGPT on the direct recommendation task. Unlike the sequential recommendation setup, direct recommendation requires the recommendation model to select the most relevant item for the user from a limited-sized item pool. We observed that, when using zero-shot prompting, the recommendation performance is significantly inferior to supervised recommendation models. This can be attributed to the insufficient information provided to ChatGPT, resulting in an inability to capture user interests and generating more random recommendations. While few-shot prompting can improve ChatGPT's recommendation performance by providing some of the user's historical preferences, it still fails to surpass the baseline performance.</p><p>Table <ref type="table">3</ref>: Performance comparison on direct recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beauty</head><p>HR@5 NDCG@5 HR@10 NDCG@10 It is worth noting that we discovered during the experiments that the construction of the item pool, specifically whether the item pool is shuffled or not, has a considerable impact on the direct recommendation performance. In an extreme scenario where the ground truth item is placed at the first position in the item pool, we found that the evaluation metrics were approximately ten times higher than when the item pool was shuffled. This finding suggests that ChatGPT exhibits a positional bias for the input item pool within the prompt, tending to consider items towards the beginning of the pool as more important, and thus more likely to be recommended. This additional bias introduced by the language model renders using ChatGPT for direct recommendation a challenging endeavor. 4.3.4 Explanation Generation. In Tab.4, both zero-shot and fewshot prompts are used to evaluate ChatGPT's performance on explanation generation. From the metrics perspective, the P5 model has a better performance. As language models, P5 and ChatGPT have different design goals and application scenarios. P5 aims to generate explanatory language similar to known texts. Therefore, P5 focuses on learning text structure and grammar rules during training, making the generated results more standardized, as shown in Fig. <ref type="figure">4</ref>. In contrast, ChatGPT focuses more on language interaction and diversity. Its application scenario is usually to simulate human conversation, so it needs to consider multiple factors such as context, emotion, and logic when generating text to better express human thinking and language habits. This design is bound to make the text generated by ChatGPT more diverse and creative. Besides, P5 is fine-tuned on Beauty dataset while ChatGPT is utilized in a zero-shot or few-shot experimental seting. Therefore, it is understandable that ChatGPT may not perform as well as P5 in metrics. Hence, we introduce human evaluation to better measure the performance of different models in generating content. 4.3.5 Review summarization. We conduct zero-shot and few-shot experiments to evaluate ChatGPT's ability on review summarization, as shown in Tab.5. Similar to the explanation generation task, ChatGPT does not have an advantage in metrics evaluation. However, although the summary result of P5 has extracted some keywords, it has ignored relevant information from the entire review. In contrast, ChatGPT can generate more effective and meaningful summaries by deeply understanding and summarizing the reviews. As shown in Fig. <ref type="figure">5</ref>. Hence, we also conduct human evaluation in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explanation Generation Results</head><p>Ground truth: "this is the best deal I've seen on nail polish in a long time" P5's output: "great price and great quality and great price" ChatGPT's output: "SHANY's Nail Art Set is a must-have for stunning manicures." Wow, this is the best deal I've seen on nail polish in a long time. You get so many vibrant beautiful colors to choose from. These are nail art brushes for fine detail. I love that you can get a whole kit for this price! Love the colors. Didn't get any doubles. 1 bottle was not fully closed and the bottle chipped on the neck of the bottle. But being where the break was I just closed it and it is still usable. I wouldn't recommend this for painting your full nail (It is for art), but I would for stamping and nail art. Small brushes great for that. Not all work for stamping though, like the metallic ones.</p><p>Ground truth: "I wouldn't recommend this for painting your full nail (It is for art)" P5's output: "great price and great price and great price" ChatGPT's output: "SHANY's Nail Art Set is a must-have for creative nails."</p><p>Absolutely great product. I bought this for my fourteen year old niece for Christmas and of course I had to try it out, then I tried another one, and another one and another one. So much fun! I even contemplated keeping a few for myself! Ground truth: "Absolutely great product" P5's output: "great colors and great price for the price" ChatGPT's output: "Love this nail art set -perfect colors and variety!"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reviews Results</head><p>Figure <ref type="figure">4</ref>: Example explanation results of different models on Beauty dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review Summarization Results</head><p>Ground truth: "works!" P5's output: "works" ChatGPT's output: "Effective and user-friendly." </p><formula xml:id="formula_0">"</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Human Evaluation (RQ3)</head><p>As shown in the experiments above, we conducted numerical evaluations on the explanation generation and review summarization tasks using the test set constructed by P5. However, the groundtruth explanations generated by P5 are not truly accurate because P5 extracts sentences from views commenting on one or more item feature words as users' explanations about their preferences. In that case, we designed human evaluations to better assess the performance of ChatGPT. Specifically, we randomly sample 20 prompts for explanation generation and 97 prompts for review summarization  from the Beauty dataset and display every generated explanation or summary to several evaluators. The evaluators rank the results generated by ChatGPT, baseline, and ground truth for assessment. avg_top1_ration represents the proportion in which the prompt ranked first among the prompts. avg_position denotes the average position of sorting for each prompt.</p><p>For explanation generation task, as shown in Tab.6, the results of the four manual annotators have a certain degree of subjectivity, but the score distribution is relatively consistent, with a general consensus that the explanations generated by ChatGPT are clearer and more reasonable, even better than the ground truth. Meanwhile, P5's performance is the worst, with explanations tending towards a generic style and sentences that are not fluent. We can also draw the same conclusion from the examples in Tab.4. For review summarization task, we can find in Fig. <ref type="figure">5</ref> that the contents summarized in P5 are too general and do not extract useful information. However, ChatGPT can truly understand the reviews and provide accurate summaries, rather than simply extracting a few keywords from the reviews. As shown in Tab.7, all annotators unanimously agree that ChatGPT has the best performance, surpassing ground truth and P5 by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this paper, we construct a benchmark to evaluate ChatGPT's performance in recommendation tasks and compare it with traditional recommendation models. The experimental results show that Chat-GPT performs well in rating prediction but poorly in sequential and direct recommendation tasks, indicating the need for further exploration and improvement. Despite its limitations, ChatGPT outperforms state-of-the-art methods in terms of human evaluation for explainable recommendation tasks, highlighting its potential in generating explanations and summaries. We believe that our study provides valuable insights into the strengths and limitations of ChatGPT in recommendation systems, and we hope that it can inspire future research to explore the use of large language models to enhance recommendation performance. Moving forward, we plan to investigate better ways to incorporate user interaction data into large language models and bridge the semantic gap between language and user interests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Workflow of utilizing ChatGPT to perform five recommendation tasks and evaluating its recommendation performance.</figDesc><graphic url="image-1.png" coords="3,65.06,83.69,481.90,245.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example prompts of accuracy-based tasks on Beauty dataset. The black texts represent the description of the task, the red texts indicate the format requirements, the blue texts represent user historical information or few-shot information, and the gray texts indicate the current input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example prompts of explainability-oriented tasks on Beauty dataset. The black texts represent the description of the task, the red texts indicate the format requirements, the blue texts represent user historical information or few-shot information, and the gray texts indicate the current input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1. Bundle Monster 100 PC 3D Designs Nail Art Nailart Manicure Fimo Canes Sticks Rods Stickers Gel Tips, 5.0; 2. Winstonia's Double Ended Nail Art Marbling Dotting Tool Pen Set w/ 10 Different Sizes 5 Colors -Manicure Pedicure, 5.0; 3. Nail Art Jumbo Stamp Stamping Manicure Image Plate 2 Tropical Holiday by Cheeky&amp;reg, 5.0 ; 4.Nail Art Jumbo Stamp Stamping Manicure Image Plate 6 Happy Holidays by Cheeky&amp;reg, 5.0; Based on above rating history, please predict user's rating for the product: "SHANY Nail Art Set (24 Famouse Colors Nail Art Polish, Nail Art Decoration)", (1 being lowest and5 being highest,The output should be like: (x stars, xx%), do not explain the reason.) How will user rate this product_title: "SHANY Nail Art Set (24 Famous Colors Nail Art Polish, Nail Art Decoration)" , and product_category: Beauty? ( 1 being lowest and 5 being highest ) Attention! Just give me back the exact number a result , and you don't need a lot of text. : you must choose 10 items for recommendation and sort them in order of priority, from highest to lowest. Output format: a python list. Do not explain the reason or include any other words. Given the user's interaction history in chronological order: ['Avalon Biotin B-Complex Thickening Conditioner, 14 Ounce', 'Conair 1600 Watt Folding Handle Hair Dryer', ?..., 'RoC Multi-Correxion 4-Zone Daily Moisturizer, SPF 30, 1.7 Ounce'], the next interacted item is ['Le Edge Full Body Exfoliator -Pink']. Now, if the interaction history is updated to ['Avalon Biotin B-Complex Thickening Conditioner, 14 Ounce', 'Conair 1600 Watt Folding Handle Hair Dryer',?..., 'RoC Multi-Correxion 4-Zone Daily Moisturizer, SPF 30, 1.7 Ounce', 'Le Edge Full Body Exfoliator -Pink'] and the user is likely to interact again, recommend the next item. Requirements: you must choose 10 items for recommendation and sort them in order of priority, from highest to lowest. Output format: a python list. Do not explain the reason or include any other words. The user has interacted with the following items in chronological order: ['Better Living Classic Two Chamber Dispenser, White', 'Andre Silhouettes Shampoo Cape, Metallic Black', ?..., 'John Frieda JFHA5 Hot Air Brush, 1.5 inch'].Please recommend the next item that the user might interact with. : you must choose 10 items for recommendation and sort them in order of priority, from highest to lowest. Output format: a python list. Do not explain the reason or include any other words. The user has interacted with the following items (in no particular order): ['Maybelline New York Eye Studio Lasting Drama Gel Eyeliner, Eggplant 956, 0.106 Ounce', ""L'Oreal Paris Healthy Look Hair Color, 8.5 Blonde/White Chocolate"", ??, 'Duo Lash Adhesive, Clear, 0.25 Ounce']. Given that the user has interacted with 'WAWO 15 Color Professionl Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette' from a pool of candidates: ['MASH Bamboo Reusable Cuticle Pushers Remover / Manicure Pedicure Stick', 'Urban Decay All Nighter Long-Lasting Makeup Setting Spray 4 oz', ......,'Classic Cotton Balls Jumbo Size, 100 Count'], please recommend the best item from a new candidate pool, ['Neutrogena Ultra Sheer Sunscreen SPF 45 Twin Pack 6.0 Ounce', 'Blinc Eyeliner Pencil -Black', ......,'Skin MD Natural + SPF15 combines the benefits of a shielding lotion and a sunscreen lotion']. Note that the candidates in the new pool are not ordered in any particular way.Requirements: you must choose 10 items for recommendation and sort them in order of priority, from highest to lowest. Output format: a python list. Do not explain the reason or include any other words. The user has interacted with the following items (in no particular order): [""Skin Obsession Jessner's Chemical Peel Kit Anti-aging and Antiacne Skin Care Treatment"", 'Xtreme Brite Brightening Gel 1oz.',?..., 'Reviva -Light Skin Peel, 1.5 oz cream']. From the candidates listed below, choose the top 10 items to recommend to the user and rank them in order of priority from highest to lowest. Candidates: ['Rogaine for Women Hair Regrowth Treatment 3-2 ounce bottles', 'Best Age Spot Remover', ?...""L'Oreal Kids Extra Gentle 2-in-1 Shampoo With a Burst of Cherry Almond, 9.0 Fluid Ounce""].</figDesc><table><row><cell>zero-shot</cell></row><row><cell>few-shot</cell></row><row><cell>Sequential Recommendation</cell></row><row><cell>zero-shot</cell></row><row><cell>few-shot</cell></row><row><cell>Direct Recommendation</cell></row><row><cell>zero-shot</cell></row><row><cell>few-shot</cell></row></table><note><p><p>Requirements</p>Requirements</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on rating prediction.</figDesc><table><row><cell>Methods</cell><cell cols="2">Beauty</cell></row><row><cell></cell><cell>RMSE</cell><cell>MAE</cell></row><row><cell>MF</cell><cell>1.1973</cell><cell>0.9461</cell></row><row><cell>MLP</cell><cell>1.3078</cell><cell>0.9597</cell></row><row><cell>ChatGPT(zero-shot)</cell><cell>1.4059</cell><cell>1.1861</cell></row><row><cell>ChatGPT(few-shot)</cell><cell cols="2">1.0751 0.6977</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on sequential recommendation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Beauty</cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">HR@5 NDCG@5 HR@10 NDCG@10</cell></row><row><cell>Caser</cell><cell>0.0205</cell><cell>0.0131</cell><cell>0.0347</cell><cell>0.0176</cell></row><row><cell>HGN</cell><cell>0.0325</cell><cell>0.0206</cell><cell>0.0512</cell><cell>0.0266</cell></row><row><cell>GRU4Rec</cell><cell>0.0164</cell><cell>0.0099</cell><cell>0.0283</cell><cell>0.0137</cell></row><row><cell>BERT4Rec</cell><cell>0.0203</cell><cell>0.0124</cell><cell>0.0347</cell><cell>0.0170</cell></row><row><cell>FDSA</cell><cell>0.0267</cell><cell>0.0163</cell><cell>0.0407</cell><cell>0.0208</cell></row><row><cell>SASRec</cell><cell>0.0387</cell><cell>0.0249</cell><cell>0.0605</cell><cell>0.0318</cell></row><row><cell>S 3 -Rec</cell><cell>0.0387</cell><cell>0.0244</cell><cell>0.0647</cell><cell>0.0327</cell></row><row><cell>P5-B</cell><cell>0.0493</cell><cell>0.0367</cell><cell>0.0645</cell><cell>0.0416</cell></row><row><cell cols="2">ChatGPT(zero-shot) 0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell cols="2">ChatGPT(few-shot) 0.0135</cell><cell>0.0135</cell><cell>0.0135</cell><cell>0.0135</cell></row><row><cell cols="5">few-shot experiments, the results of which are shown in Tab.2.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>you can see and feel that it's working. easy to use too. after a few times you'll get the hang of it." "Great to use after the microdermabrasion roller needle process. Skin absorbs it quickly. Face looks more rejuvenated in the morning." Ground truth</head><label></label><figDesc></figDesc><table /><note><p>: "Works Well" P5's output: "Great" ChatGPT's output: "Quickly absorbed rejuvenating serum." "These brushes are okay. I don't think they're anything special, but for the price their quality is okay. I don't know that I would buy them again though.". Ground truth: "It's okay" P5's output: "Okay" ChatGPT's output: "Average brushes for price." "I truly love this soap. I</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>have very sensitive skin and this is one of the few soaps that doesn't dry out or break out my skin. Would recommend to others. It smells soft as well" Ground truth</head><label></label><figDesc></figDesc><table /><note><p><p><p><p><p>: "Soap" P5's output: "Great soap" ChatGPT's output: "Gentle, effective soap recommended."</p>Reviews Results</p>Figure</p>5</p>: Example summarization results of different models on Beauty dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison on explanation generation (%).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Beauty</cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">BLUE4 ROUGE1 ROUGE2 ROUGEL</cell></row><row><cell>Attn2Seq</cell><cell>0.7889</cell><cell>12.6590</cell><cell>1.6820</cell><cell>9.7481</cell></row><row><cell>NRT</cell><cell>0.8295</cell><cell>12.7815</cell><cell>1.8543</cell><cell>9.9477</cell></row><row><cell>PETER</cell><cell>1.1541</cell><cell>14.8497</cell><cell>2.1413</cell><cell>11.4143</cell></row><row><cell>P5-B</cell><cell>0.9742</cell><cell>16.4530</cell><cell>1.8858</cell><cell>11.8765</cell></row><row><cell>PETER+</cell><cell cols="2">3.2606 25.5541</cell><cell>5.9668</cell><cell>19.7168</cell></row><row><cell cols="2">ChatGPT(zero-shot) 0.0000</cell><cell>8.5992</cell><cell>0.6995</cell><cell>4.7564</cell></row><row><cell cols="2">ChatGPT(few-shot) 1.1967</cell><cell>11.4103</cell><cell>2.5675</cell><cell>5.9119</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison on review summarization (%).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Beauty</cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">BLUE4 ROUGE1 ROUGE2 ROUGEL</cell></row><row><cell>T0</cell><cell>1.2871</cell><cell>1.2750</cell><cell>0.3904</cell><cell>0.9592</cell></row><row><cell>GPT-2</cell><cell>0.5879</cell><cell>3.3844</cell><cell>0.6756</cell><cell>1.3956</cell></row><row><cell>P5-B</cell><cell>2.1225</cell><cell>8.4205</cell><cell>1.6676</cell><cell>7.5476</cell></row><row><cell cols="2">ChatGPT(zero-shot) 0.0000</cell><cell>3.8246</cell><cell>0.2857</cell><cell>3.1344</cell></row><row><cell cols="2">ChatGPT(few-shot) 0.0000</cell><cell>2.7822</cell><cell>0.0000</cell><cell>2.4328</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Human evaluation for explanation generation on Beauty dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Evaluators</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">Eva_1 Eva_2 Eva_3 Eva_4</cell><cell cols="2">avg_top1_ration avg_position</cell></row><row><cell>Ground truth</cell><cell cols="4">25.0% 45.0% 45.0% 50.0%</cell><cell>38.0%</cell><cell>1.83</cell></row><row><cell>P5</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>2.71</cell></row><row><cell cols="5">ChatGPT(zero-shot) 75.0% 55.0% 55.0% 50.0%</cell><cell>62.0%</cell><cell>1.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Human evaluation for review summarization on Beauty dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Evaluators</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">Eva_1 Eva_2 Eva_3 Eva_4 Eva_5</cell><cell cols="2">avg_top1_ration avg_position</cell></row><row><cell>Ground truth</cell><cell cols="2">12.5% 10.6%</cell><cell>8.7%</cell><cell>17.3% 22.1%</cell><cell>14.2%</cell><cell>2.91</cell></row><row><cell>P5</cell><cell>5.8%</cell><cell>0.0%</cell><cell>5.7%</cell><cell>11.5% 19.2%</cell><cell>8.5%</cell><cell>3.16</cell></row><row><cell cols="5">ChatGPT(zero-shot) 46.2% 37.5% 36.5% 45.2% 23.1%</cell><cell>37.7%</cell><cell>1.90</cell></row><row><cell cols="5">ChatGPT(few-shot) 35.6% 51.9% 49.0% 26.0% 35.6%</cell><cell>39.6%</cell><cell>2.01</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>ChatGPT a Good Recommender? A Preliminary Study. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). <rs type="institution">ACM, New York, NY, USA</rs>, 10 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00089</idno>
		<title level="m">Massively multilingual neural machine translation</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A systematic study on the recommender systems in the E-commerce</title>
		<author>
			<persName><forename type="first">Nima</forename><surname>Pegah Malekpour Alamdari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Jafari Navimipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">Asghar</forename><surname>Hosseinzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aso</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName><surname>Darwesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ieee Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="115694" to="115716" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Wilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holy</forename><surname>Lovenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willy</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04023</idno>
		<title level="m">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">03 A Neural Probabilistic Language Model</title>
		<imprint>
			<date type="published" when="2003">2003. 2003. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning architecture for collaborative filtering recommender systems</title>
		<author>
			<persName><forename type="first">Jesus</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Hernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2441</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Classification-based deep neural network architecture for collaborative filtering recommender systems</title>
		<author>
			<persName><forename type="first">Jes?s</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Guti?rrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alonso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<title level="m">Language Models are Few-Shot Learners</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Logical foundations of knowledge-based recommender systems: A unifying spectrum of alternatives</title>
		<author>
			<persName><forename type="first">Federica</forename><surname>Cena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Console</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabiana</forename><surname>Vernero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">546</biblScope>
			<biblScope unit="page" from="60" to="73" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09849</idno>
		<title level="m">The best of both worlds: Combining recent advances in neural machine translation</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR abs/2205.08084</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Haixing</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoke</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dajiang</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13007</idno>
		<title level="m">Chataug: Leveraging chatgpt for text data augmentation</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to generate product reviews from attributes</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An interactive knowledge-based recommender system for fashion product design in the big data environment</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Koehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">540</biblScope>
			<biblScope unit="page" from="469" to="488" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlin</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.14524</idno>
		<title level="m">Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new similarity measure for collaborative filtering based recommender systems</title>
		<author>
			<persName><forename type="first">Achraf</forename><surname>Gazdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lotfi</forename><surname>Hidri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page">105058</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Proceedings of the 16th ACM Conference on Recommender Systems</title>
		<meeting>the 16th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="299" to="315" />
		</imprint>
	</monogr>
	<note>Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Collaborative Filtering. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<title level="m">Session-based recommendations with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jen-Tse</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08745</idno>
		<title level="m">Is ChatGPT a good translator? A preliminary study</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on data mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Journal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>Computer</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The unfairness of popularity bias in music recommendation: A reproducibility study</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Kowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Lex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-04-14">2020. April 14-17, 2020</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 42</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">A Persona-Based Neural Conversation Model</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Personalized transformer for explainable recommendation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11601</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural rating regression with abstractive tips generation for recommendation</title>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Critical Review of Recurrent Neural Networks for Sequence Learning</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An ecommerce recommendation algorithm based on link prediction</title>
		<author>
			<persName><forename type="first">Guoguang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alexandria Engineering Journal</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="905" to="910" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fine-tune BERT for Extractive Summarization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical gating networks for sequential recommendation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="825" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SimpleX: A simple and strong baseline for collaborative filtering</title>
		<author>
			<persName><forename type="first">Kelong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Smart billing using content-based recommender systems based on fingerprint</title>
		<author>
			<persName><forename type="first">Darshita</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanyukta</forename><surname>Shandilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Khirwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archana</forename><surname>Bhise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICT Analysis and Applications: Proceedings of ICT4SD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2020. 2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning word embeddings from wikipedia for content-based recommender systems</title>
		<author>
			<persName><forename type="first">Cataldo</forename><surname>Musto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>De Gemmis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Lops</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval: 38th European Conference on IR Research</title>
		<title level="s">Proceedings</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-03-20">2016. 2016. March 20-23, 2016</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno>. CoRR abs/2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">GPT-4 Technical Report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">It is just a flu&quot;: Assessing the Effect of Watch History on YouTube&apos;s Pseudoscientific Video Recommendations</title>
		<author>
			<persName><forename type="first">Kostantinos</forename><surname>Papadamou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savvas</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiliano</forename><surname>De Cristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Stringhini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sirivianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international AAAI conference on web and social media</title>
		<meeting>the international AAAI conference on web and social media</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="723" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Content-based group recommender systems: A general taxonomy and further improvements</title>
		<author>
			<persName><forename type="first">Yilena</forename><surname>P?rez-Almaguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raciel</forename><surname>Yera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><forename type="middle">A</forename><surname>Alzahrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page">115444</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2618</idno>
		<title level="m">Bayesian personalized ranking from implicit feedback</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey of attack detection approaches in collaborative filtering recommender systems</title>
		<author>
			<persName><forename type="first">Fatemeh</forename><surname>Rezaimehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitra</forename><surname>Dadkhah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="2011" to="2066" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08207</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Novel Deep Neural-based Music Recommendation Method considering User and Song Data</title>
		<author>
			<persName><forename type="first">Jagendra</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Shekhar Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank Sheshar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manthan</forename><surname>Saini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 6th International Conference on Trends in Electronics and Informatics (ICOEI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on information and knowledge management</title>
		<meeting>the 28th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting Bundle Recommendation: Datasets, Tasks, Challenges and Opportunities for Intent-aware Product Bundling</title>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaidong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2900" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Personalized top-n sequential recommendation via convolutional sequence embedding</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on web search and data mining</title>
		<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="565" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Challenges and research opportunities in ecommerce search and recommendations</title>
		<author>
			<persName><forename type="first">Manos</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tracy</forename><forename type="middle">Holloway</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Kallumadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigir Forum</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Attention Is All You Need</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Content-based neighbor models for cold start in recommender systems</title>
		<author>
			<persName><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Poutanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Recommender Systems Challenge</title>
		<meeting>the Recommender Systems Challenge</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on multimedia</title>
		<meeting>the 27th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">NPA: neural news recommendation with personalized attention</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxiao</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2576" to="2584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Feedrec: News feed recommendation with various user feedbacks</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2088" to="2097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mind: A large-scale dataset for news recommendation</title>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiun-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winnie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3597" to="3606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Rethinking Multi-Interest Learning for Candidate Matching in Recommender Systems</title>
		<author>
			<persName><forename type="first">Yueqi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qichen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeboum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.14532</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Decoupled side information fusion for sequential recommendation</title>
		<author>
			<persName><forename type="first">Yueqi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1611" to="1621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Qingcheng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Garay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dading</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiageng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.06993</idno>
		<title level="m">GreenPLM: Cross-lingual pre-trained language models conversion with (almost) no cost</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Privacy-aware smart city: A case study in collaborative filtering recommender systems</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Victor E Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim-Kwang Raymond</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Maasberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="145" to="159" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Feature-level Deeper Self-Attention Network for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4320" to="4326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeren</forename><surname>Hao Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hFx3fY7-m9b" />
		<title level="m">Language Models as Recommender Systems: Evaluations and Limitations. In I (Still) Can&apos;t Believe It&apos;s Not Better! NeurIPS 2021 Workshop</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Recommending what video to watch next: a multitask ranking system</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddh</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheswaran</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM international conference on information &amp; knowledge management</title>
		<meeting>the 29th ACM international conference on information &amp; knowledge management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueqi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qichen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05290</idno>
		<title level="m">Equivariant Contrastive Learning for Sequential Recommendation</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
