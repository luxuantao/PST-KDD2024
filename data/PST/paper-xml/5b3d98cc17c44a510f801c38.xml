<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforced Imitation: Sample Efficient Deep Reinforcement Learning for Mapless Navigation by Leveraging Prior Demonstrations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mark</forename><surname>Pfeiffer</surname></persName>
							<idno type="ORCID">0000-0003-0276-324X</idno>
						</author>
						<author>
							<persName><forename type="first">Samarth</forename><surname>Shukla</surname></persName>
							<email>shuklas@ethz.ch</email>
							<idno type="ORCID">0000-0002-1670-5211</idno>
						</author>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Turchet</surname></persName>
							<email>matteotu@ethz.ch</email>
							<idno type="ORCID">0000-0003-4808-0831</idno>
						</author>
						<author>
							<persName><forename type="first">Cesar</forename><surname>Caden</surname></persName>
							<email>cesarc@ethz.ch</email>
							<idno type="ORCID">0000-0002-2972-6011</idno>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
							<email>krausea@ethz.ch</email>
						</author>
						<author>
							<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
							<email>rsiegwart@ethz.ch</email>
							<idno type="ORCID">0000-0002-2760-7983</idno>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><surname>Nieto</surname></persName>
							<email>nietoj@ethz.ch</email>
						</author>
						<author>
							<affiliation>
								<orgName>Autonomous Systems Lab, Computer Vision Lab, Learning and Adaptive Systems Group, and Max Planck ETH Center for Learning Systems,</orgName>
								<address><addrLine>ETH Zurich, Zurich 8057, Switzerland</addrLine></address>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforced Imitation: Sample Efficient Deep Reinforcement Learning for Mapless Navigation by Leveraging Prior Demonstrations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5823C5A2192144C2998D9A2A76B3C734</idno>
					<idno type="DOI">10.1109/LRA.2018.2869644</idno>
					<note type="submission">received May 3, 2018; accepted August 23</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Navigation</term>
					<term>deep reinforcement learning</term>
					<term>end-toend planning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This letter presents a case study of a learning-based approach for target-driven mapless navigation. The underlying navigation model is an end-to-end neural network, which is trained using a combination of expert demonstrations, imitation learning (IL) and reinforcement learning (RL). While RL and IL suffer from a large sample complexity and the distribution mismatch problem, respectively, we show that leveraging prior expert demonstrations for pretraining can reduce the training time to reach at least the same level of the performance compared to plain RL by a factor of 5. We present a thorough evaluation of different combinations of expert demonstrations, different RL algorithms, and reward functions, both in simulation and on a real robotic platform. Our results show that the final model outperforms both standalone approaches in the amount of successful navigation tasks. In addition, the RL reward function can be significantly simplified when using pretraining, e.g., by using a sparse reward only. The learned navigation policy is able to generalize to unseen and real-world environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. An end-to-end navigation policy is learned from a combination of imitation and reinforcement learning. The resulting policy is tested thoroughly in simulation and on a real robotic platform.</p><p>neural networks leading the way <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>-have gained importance allowing for the application of end-to-end motion planning approaches. Instead of splitting the navigation task into multiple sub-modules like, e.g., sensor fusion, obstacle detection, global and local motion planning, end-to-end approaches use a direct mapping from sensor data to robot motion commands which can reduce the complexity during deployment significantly.</p><p>Current state-of-the-art end-to-end planning approaches can be split in two major groups: (i) imitation learning (IL) based ones use supervised learning techniques to imitate expert demonstrations as close as possible, 1 and (ii) approaches based on Reinforcement learning (RL) where the agents learn their navigation policy by trial and error exploration combined with reward signals. imitation learning (IL) is sample efficient and can achieve accurate imitation of the expert demonstrations. Given the training data, satisfactory navigation models can be found within a few hours of training <ref type="bibr" target="#b1">[2]</ref>. However, it is likely to overfit to the environment and situations presented at training time. This limits the potential for generalization and the robustness of the policy (distribution mismatch). Reinforcement learning (RL) is conceptually more robust-also in unseen scenarios-as the agent learns from its own mistakes during training <ref type="bibr" target="#b2">[3]</ref>. The disadvantage of RL is its sample inefficiency and missing safety during training, limiting the current utilization to applications where training can be conducted using extremely fast simulators <ref type="bibr" target="#b4">[5]</ref>. As for RL training, episodes need to be forward simulated (on-or off-policy), training iterations are significantly more time consuming than in IL, which reduces the number of training iterations in a given time. However, RL allows to encode desired behavior-such as reaching the target and avoiding collisions -specifically in a reward function and does not only rely on suitable expert demonstrations. In addition, RL maximizes the overall expected return on a full trajectory, while IL treats every observation independently <ref type="bibr" target="#b5">[6]</ref>, which conceptually makes RL superior to IL.</p><p>In this work, we present and analyze an approach that combines the advantages of both IL and RL. It is inspired by human learning, which typically combines the observation of other people and self-exploration <ref type="bibr" target="#b6">[7]</ref>. Our approach, in the following called Reinforced imitation learning (R-IL), combines supervised IL based on expert demonstrations to pre-train the navigation policy with subsequent RL (see Fig. <ref type="figure">1</ref>). For RL, we use Constrained Policy Optimization (CPO) <ref type="bibr" target="#b7">[8]</ref> due to its ability to incorporate constraints during training. This allows for safer training and navigation, which is especially important for real-world mobile robotics.</p><p>We hypothesize that the combination of the two learning approaches yields a more robust policy than pure IL, and that it is also easier and faster to train than pure RL. In addition, by enforcing the collision avoidance by constraint instead of a fixed penalty in the reward function, the amount of collisions during training and testing should be decreased. To the best of our knowledge, this is the first work to explore this combination for robot navigation and also to apply constraint-based RL to map-less navigation. We provide an extensive evaluation of the training and navigation performance in simulation and on a robotic platform. Our main contributions are: r a case study for combining IL and RL<ref type="foot" target="#foot_0">2</ref> for map-less navigation r a model for map-less end-to-end motion planning that gen- eralizes to unseen environments r an extensive evaluation of training and generalization per- formance to unseen environments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning by Demonstration</head><p>Learning by demonstration can be split in two main areas: (i) Inverse reinforcement learning (IRL), where a reward function is inferred from expert demonstrations and a policy is derived by optimizing this reward with optimal control techniques and (ii) IL, where expert demonstrations are used to directly infer a policy. Abbeel et al. <ref type="bibr" target="#b8">[9]</ref> present an IRL-based approach where they teach an autonomous car to navigate in parking lots by observing human demonstrations. Similarly, Pfeiffer et al. <ref type="bibr" target="#b9">[10]</ref> and Kretzschmar et al. <ref type="bibr" target="#b10">[11]</ref> present approaches for navigation in dynamic environments based on IRL. By observing pedestrian motion, a probability distribution over pedestrian trajectories is found. For path planning, the trajectory with the highest probability according to the learned model is chosen with the goal of a close imitation of pedestrian motion. Wulfmeier et al. <ref type="bibr" target="#b11">[12]</ref> present a similar approach using deep IRL instead of a combination of classical features in order to learn how to drive an autonomous car through static environments.</p><p>In the following, we give an overview of the literature on mapless navigation using IL. Muller et al. <ref type="bibr" target="#b3">[4]</ref> present an image-based approach for end-to-end collision avoidance using imitation learning. In their work, the focus is on feature extraction and on generalization to new situations. The overall navigation performance of such approaches is not analyzed. Another approach focused on perception is presented by Chen et al. <ref type="bibr" target="#b12">[13]</ref>. They combine learning-based feature extraction using Convolutional neural networks (CNNs) with a classical driving controller for an autonomous car. However, they focus on a lane-following application and do not deal with target-driven navigation. Kim et al. <ref type="bibr" target="#b13">[14]</ref> present an IL approach for hallway navigation and collision avoidance for an unmanned aerial vehicle (UAV). They show a working model on a real-world platform, yet the environmental setup is relatively easy and no real navigation capabilities are required. Sergeant et al. <ref type="bibr" target="#b14">[15]</ref> present an end-to-end approach for laser-based collision avoidance for ground vehicles demonstrated in simulation and real-world tests. However, the approach is limited to collision avoidance and cannot be used for target-driven navigation. Ross et al. <ref type="bibr" target="#b15">[16]</ref> present the Dataset Aggregation (DAGGER) method which collects demonstrations according to the currently best policy but can also query additional expert demonstrations in order to alleviate the distribution mismatch problem. One application of the DAGGER algorithm is presented in <ref type="bibr" target="#b16">[17]</ref>, where directional commands for forest navigation and collision avoidance are learned from expert demonstrations. In addition, Kuefler et al. <ref type="bibr" target="#b5">[6]</ref> presented an approach based on Generative Adversarial Imitation Learning (GAIL) <ref type="bibr" target="#b17">[18]</ref>, where they learn driver models for an autonomous car based on expert demonstrations. Tai et al. <ref type="bibr" target="#b18">[19]</ref> recently applied GAIL to model interaction-aware navigation behavior. Although conceptually GAIL generalizes better than standard behavioral cloning techniques, it is still constrained by the provided expert demonstrations.</p><p>The method we introduce builds upon prior work presented in <ref type="bibr" target="#b1">[2]</ref>, where a global planner is used to generate expert demonstrations in simulation. Given demonstrations, an end-to-end navigation policy mapping from 2D laser measurements and a relative goal position to motion commands is found. The main drawbacks of this approach are the generalization to new environments-also due to the specific CNN model structureand the behavior in situations which were not covered in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reinforcement Learning</head><p>Bischoff et al. <ref type="bibr" target="#b19">[20]</ref> use ideas from hierarchical RL to decompose the navigation task in motion planning and movement execution and thus are able to improve the sample efficiency of plain RL. Yet global map information is always assumed to be known. Zuo et al. <ref type="bibr" target="#b20">[21]</ref> use a popular model-free RL algorithm, Q-learning, to teach a robot a policy to navigate through a simple spiral maze from sonar inputs only. Mirowski et al. <ref type="bibr" target="#b21">[22]</ref> use auxiliary tasks such as depth prediction and loop closure assessment to improve the learning rate of A3C <ref type="bibr" target="#b4">[5]</ref> for simulated maze navigation from RGB images. Bruce et al. <ref type="bibr" target="#b22">[23]</ref> use interactive experience replay to learn how to navigate in a known environment to a fixed goal from images by traversing it only once. The method presented in <ref type="bibr" target="#b23">[24]</ref> focuses on efficient knowledge transfer across maps and conditions for an autonomous navigation task. To this end, it uses a particular parametrization of the Q-function, known as successor representation, that decouples task specific knowledge from transferable knowledge. Zhu et al. <ref type="bibr" target="#b24">[25]</ref> present an end-to-end vision-based navigation algorithm that uses the target as an additional input to the policy to learn to achieve proper target-driven navigation.</p><p>Chen et al. <ref type="bibr" target="#b25">[26]</ref> presented a RL approach for collision avoidance in dynamic environments. Similar to our work, prior demonstrations are used for pre-training, yet their focus lies on learning interactions between multiple agents and the algorithm is not designed for navigation scenarios. The method presented by Tai et al. <ref type="bibr" target="#b2">[3]</ref> is the most closely related to ours. In their work, the Asynchronous Deep Deterministic Policy Gradients (ADDPG) algorithm is used to learn a policy from range findings to continuous steering commands for both simulated and real-world map-less navigation tasks. However, using AD-DPG, no collision constraints can be enforced and the models are trained from scratch. When moving towards real world applications and eventually RL training on real platforms, safety and training speed become decisive factors. Therefore, compared to <ref type="bibr" target="#b2">[3]</ref>, we use prior demonstrations for pre-training and CPO during RL training, targeting the real-world applicability of RL approaches.</p><p>As experiments in robotics usually require large amounts of time, the problem of reducing the sample complexity of RL based approaches has received increasing attention recently. Using a combination of IL and RL to obtain a sample efficient and robust learning algorithm has previously been explored in robotics in the context of manipulation tasks <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. In this context, the main challenge consists in using human demonstrations that may not be replicable by the robot due its dynamics. In the case of navigation, this is usually not a concern. However, navigation tasks present challenges in terms of safety. Even small deviations from the expert policy may lead to a crash. To the best of our knowledge, our method is the first to use expert demonstrations to boost RL learning performance in the context of map-less autonomous navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>Classical path planning techniques <ref type="bibr" target="#b0">[1]</ref> require prior knowledge of the environment for navigation. In case of unknown or constantly changing and dynamic environments, obtaining and maintaining an accurate map representation becomes increasingly difficult or even unfeasible. Therefore, map-less navigation skills based solely on local information available to the robot through its sensors are required.</p><p>Given the sensor measurements y and a relative target position g, we want to find a policy π θ parametrized by θ which maps the inputs to suitable control commands, u, i.e. u = π θ (y, g).</p><p>(</p><formula xml:id="formula_0">)<label>1</label></formula><p>The required control commands are comprised of the translational and rotational velocity. As the mapping from local sensor and target data to control commands can be arbitrarily complex, learning how to plan from experience in an end-to-end fashion using powerful non-linear function approximators, such as neural networks, has become more prominent.</p><p>In this work, we aim at combining IL and RL to obtain a sample efficient and robust learning based navigation algorithm. We do this in a sequential fashion by using the result from IL to initialize our RL method. In the remainder of this section we introduce separately the underlying neural network model, the IL and RL components of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Network Model</head><p>The neural network model which represents π θ , is shown in Fig. <ref type="figure" target="#fig_0">2</ref>. In this work, the inputs to the model are 2D laser range findings and a relative target position in polar coordinates w.r.t. the local robot coordinate frame. In contrast to <ref type="bibr" target="#b1">[2]</ref>, where a CNN was used to extract environmental features, this model is simplified and only relies on three fully connected layers. While the CNN allows to find relevant environmental features, we found that it tends to overfit to the shapes of the obstacles presented during training. Instead, we use minimum pooling of the laser data and compress the full range of 1080 measurements into 36 values, where each pooled value y p,i is computed as:</p><formula xml:id="formula_1">y p,i = min y i•k , . . . , y (i+1)•k -1 , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where i is the value index and k is the kernel size for 1D pooling.</p><p>In our case, we chose k = 30. Using min-pooling, safety can be assured, yet detailed environmental features may get lost.</p><p>The resulting simplified neural network model can be trained more efficiently and is less likely to overfit to specific obstacle shapes. Furthermore, the inputs are normalized before being fed to the neural network model. The pooled laser measurements are cropped and then mapped to lie in the interval [-1, 1] by applying the normalization 2 • 1 -</p><formula xml:id="formula_3">min(y p , i ,r max ) r max -1</formula><p>, where r max is the maximum laser range. The same normalization is applied to the relative target position. The outputs of the neural network, which also lie in the interval [-1, 1], are de-normalized and mapped to translational and rotational velocities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supervised Pre-Training Via Behavior Cloning</head><p>In order to improve the performance and sample complexity of the succeeding RL, the policy is pre-trained using supervised IL based on expert demonstrations similar to <ref type="bibr" target="#b1">[2]</ref>. The goal is to imitate the expert as closely as possible, given the representation limitations of the neural network model. Compared to plain IL, where the performance of the final model is limited by the performance of the expert demonstrations, R-IL can overcome this limitation through self-improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Reinforcement Learning</head><p>1) Background Information: Given a Markov Decision Process (MDP), M = S, A, P, R, γ , where S is the state space, A is the action space, P(•|s t , a t ) : S × S × A :→ R + is the transition probability distribution, R(•, •) : S × A → R is the reward function and γ ∈ [0, 1] is the discount factor, RL aims to find a policy π θ , mapping states to actions and parametrized by θ, that maximizes the expected sum of discounted rewards,</p><formula xml:id="formula_4">J(θ) = E T t=0 γ t • r(s t , π θ (s t )) , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where T is the time horizon of a navigation episode. In our case, s t consists of laser measurements and the target information, a t of the control commands. Policy gradient methods <ref type="bibr" target="#b28">[29]</ref> are model-free RL algorithms that use modifications of stochastic gradient descent to optimize J(θ) with respect to the policy parameters θ. However, they suffer from a high variance in gradients, resulting in undesirably large updates to the policy. A popular technique to reduce model variance and ensure stability between updates is Trust Region Policy Optimization (TRPO) <ref type="bibr" target="#b29">[30]</ref>. To this end, it restricts the change in policy at each update by imposing a constraint on the average Kullback-Leibler (KL) divergence between the new and old policy.</p><p>Enforcing safety is crucial when dealing with mobile robotics applications. Often, safety in RL is encouraged by imposing high cost on unsafe states. However, this requires tuning such cost. If it is too low, the agent may decide to experience unsafe states for short amounts of time as this will not severely impact the overall performance (Eq. 3). Conversely, if the cost is too high, the agent may avoid exploring entire portions of the state space to avoid the risk of experiencing unsafe states. A more elegant and increasingly popular way of ensuring safety in RL is to treat it as a constraint <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[31]</ref>. In particular, in this work, we use a safety constrained extension of TRPO known as Constrained Policy Optimization (CPO) <ref type="bibr" target="#b7">[8]</ref> to ensure safety. Given a cost function C : S × A :→ R, let J C (θ) indicate the expected discounted return of π θ with respect to this cost</p><formula xml:id="formula_6">J C (θ) = E T t=0 γ t • C(s t , π θ (s t )) . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>CPO finds an approximate solution to the following problem,</p><formula xml:id="formula_8">θ * = arg maxJ(θ), s.t. J C (θ) ≤ α. (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>2) Training Process: For training, the neural network model is first initialized either randomly (pure RL) or using IL (R-IL). We use a stochastic policy where the actions are sampled from a 2D Gaussian distribution having the de-normalized values of the output of the neural network as mean, and a 2D standard deviation which is a separate learn-able parameter. Using a supervised IL model thus only influences the initialization of the RL policy. During training we randomly select a start and target position and collect robot experience samples by running an episode using the current policy π θ for a fixed number of time steps or until the robot reaches the target. At each policy update, we use a batch of samples collected from multiple episodes. The agent's objective is to learn to reach the target in the shortest possible number of time-steps while avoiding collisions with surrounding obstacles. The reward function provides the required feedback to the robot during the learning process. In this work, we investigate different choices for the reward function encoding various degree of information about the task. These rewards can be expressed by:</p><formula xml:id="formula_10">r(s t ) = 10, if success -(d(s t ) -d(s t-1 )), otherwise.</formula><p>Setting d(s) = 0, ∀s ∈ S we encode the minimum information required to carry out the task. This sparse reward makes the learning process difficult due to the credit assignment problem, i.e. the fact that all the actions taken in an episode get credit for its outcome regardless of whether they contributed to it or not. An alternative to such choice is to set d(s) to the Euclidean distance between s and the target. This reward provides continuous feedback for each action by rewarding/penalizing the agent for getting closer/further to/from the goal in Euclidean space. However, it does not consider the placement of obstacles in the environment. The last option we investigate consists in setting d(s) to the distance between s and the goal along the shortest feasible path that can be computed using the Dijkstra algorithm. Note, the agent does not have any knowledge about d(•). This distance is only used to compute the reward which the agent receives from the environment during training. Using a negative reward for collisions makes the policy highly sensitive to this reward's magnitude, resulting in a delicate tradeoff between two different objectives -reaching the target and avoiding crashes. However, in constrained MDPs, we can encode collision avoidance through a constraint on the expected number of crashes allowed per episode. Let S c ⊂ S denote the set of states that correspond to a crash. We define a state dependend cost function as follows:</p><formula xml:id="formula_11">c(s t ) = I(s t ∈ S c ),<label>(6)</label></formula><p>where I is the indicator function. In our experiments, we noticed the robot stays in a crash state for four consecutive timesteps on average. By setting the discount factor for the cost -which does not have to be equal to the one for the reward -close to 1 and introducing the constraint value α, we can constrain the total number of expected crashes per episode to be approximately less or equal to α 4 . In our model we set α = 0.4. This value was found empirically by testing values between 0.0 and 0.6 in a simple environment. While training, we allow for multiple crashes in each episode. This leads to more crash samples in the training set and makes it easier to reach the target, thus making the training process more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>This section presents the experiments conducted in simulation and on the real robotic platform. The goal of the experiments is to investigate the influence of pre-training the RL policy, to compare constraint-based to fixed penalty methods and analyze the influence of the reward functions presented in Section III. We also compare to models presented in prior work <ref type="bibr" target="#b2">[3]</ref>. Furthermore, we investigate the generalization performance of the navigation policies to unseen scenarios and the real world, which is also shown in our video. <ref type="foot" target="#foot_1">3</ref> Our work does not intend to show that we can outperform a global graph-based planner in known environments, where graph-based solutions are fast and can achieve optimal behavior. The goal of our experiments is to investigate the limits of motion planning with local information only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>The models are purely trained in simulation since it is a safe, fast and efficient way of training and evaluating the model. Additionally, there are no physical space constraints and the environment structure can be changed almost arbitrarily. Models trained in simulation have previously been shown to successfully transfer to the real-world <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>The experiments are based on a differential drive Kobuki TurtleBot2<ref type="foot" target="#foot_2">4</ref> platform equipped with a front-facing Hokuyo UTM laser range finder with a field of view of 270 • , maximum range of 30 m and 1080 range measurements per revolution. For on-board computations we resort to an Intel NUC with an i7-5557U processor and without any GPU, running Ubuntu 14.04 and ROS <ref type="bibr" target="#b31">[32]</ref> as a middleware. The motion commands are published with a frequency of 5 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Training</head><p>Different procedures for model training are applied: (i) pure IL, (ii) pure RL and (iii) R-IL, which is a combination of both. In order to test the influence of the complexity and the diversity of the training environments on test performance, we train the models on five maps (or subsets of them) as shown in Fig. <ref type="figure" target="#fig_1">3</ref>. The pure IL models are trained in the simple and complex maps, the RL part is conducted on all three TM maps. Similarly, for R-IL, IL is conducted on the simple and complex maps and the RL part takes place on the TM maps. We do this separation in order to investigate how demonstrations from a different environment can be transferred to the RL training.</p><p>The expert demonstrations used for IL are generated using the ROS move_base<ref type="foot" target="#foot_3">5</ref> navigation stack to navigate between random start and target positions, as presented in <ref type="bibr" target="#b1">[2]</ref>. We use an expert planner instead of a human to make the demonstrations more consistent and time efficient. We note that the demonstrations are suboptimal for RL, as they are generated based on a different cost function and also in a different environment. After recording the demonstrations, one IL training iteration takes around 7 ms on an Intel i7-7700K processor and a Nvidia GeForce GTX 1070 GPU. Therefore, IL model training takes between one hour (s 10 , 500 k iterations) and around 2.5 hours (c 1000 , 1.5 M iterations).</p><p>Table <ref type="table" target="#tab_0">I</ref> summarizes all the models we trained. Our case study presents constraint based R-IL yet compares to a broad range of different models: We vary the number of demonstrations (from 10 to 1000), the RL training procedure (CPO, TRPO) and reward signals (sparse, Euclidean and shortest distance) in order to provide insights into how those factors influence map-less navigation. The TRPO training procedure is the fixed collision penalty version of CPO with a collision constraint, as described in Section III.</p><p>During RL, the training environment is uniformly sampled among the three TM maps (see Fig. <ref type="figure" target="#fig_1">3</ref>). One training iterationfor which we consider a batch consisting of 60 k time stepstakes around 180 s using the accelerated Stage <ref type="bibr" target="#b32">[33]</ref> simulation. Therefore, 1000 iterations require around 50 hours of training time using the simulation, which is a real-time equivalent of around 100 days. This further motivates the need to find a good policy initialization by IL in order to reduce the training time significantly.</p><p>Fig. <ref type="figure" target="#fig_2">4</ref> shows the success and crash rates of a broad range of models during RL training alongside the performance of pure IL trained on all TM maps. This IL model only serves as a baseline to evaluate the progress of the RL and R-IL methods during training. CPO1 differs from all the other models during training as it is exclusively trained on the simplest TM map (TM1). However, it will be shown that this model does not generalize well to more complex test environments. From Fig. <ref type="figure" target="#fig_2">4</ref> the following can be shown:</p><p>1) Difference Between the Models Which Were Pre-Trained Using IL and the Ones Based on Pure RL Using CPO / TRPO: While the pre-trained models already start at a certain success rate (depending on the performance of the IL model), it takes a significant amount of iterations for the RL models to reach the target in the majority of the cases. Comparing the TRPO and CPO versions of the different models also shows the potential problems of constraint-based methods. Initially, the cost that defines the safety constraint (Eq. 6), used in CPO has very high values and the agent learns to satisfy it. This also explains the drop in success rate early during training, which all R-IL models trained with CPO have in common. Therefore, in this phase, the agent learns to avoid crashes and unlearns the behavior of reaching the target, which is also supported by the crash rate curves. Both models (with high and low cost of collision) trained with TRPO do not show this behavior as no constraint needs to be satisfied. Therefore TRPO allows for more "risky" exploration initially. This further motivates to use pre-training when using constraint-based RL, as it provides enough intuition to reach the target while the agent can learn how to satisfy the safety constraint. This would be hard otherwise, as exploration through Gaussian perturbation of a nominal motion command is inherently local in the policy space. The difference becomes even more pronounced for the simpler reward structures, such as sparse target reward. While the agent is stuck with a low success rate for CPO123 spar se and mostly learns collision avoidance, pre-training with only 10 demonstrations allows the agent to successfully reach the goal in the vast majority of the cases (s 10 +CPO spar se ). With pre-training, sparse and full (shortest path) reward reach about the same final performance.</p><p>2) Problem of Fixed Penalty Methods: While, e.g., s 1000 +CPO reaches a high final success rate and a low crash rate, s 1000 +TRPO c0.1 reaches similar success rates, yet struggles with significantly more crashes. On the other side, s 1000 +TRPO c1.0 reaches a similar crash rate yet does not achieve the same final success rate. This difficulty of fixed penalty parameter tuning was already raised in <ref type="bibr" target="#b7">[8]</ref>.</p><p>3) Final Performance is Affected by the Initial Starting State: Models initialized using more complex maps and/or more trajectories not only perform better but also learn faster. Even a very small amount of demonstrations can significantly improve the overall performance. The R-IL models reach the final </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Simulation Results</head><p>In the following, the performance of the navigation policies is analyzed when deployed in unseen environments in simulation. We constructed two 10 m × 10 m evaluation maps as shown in Fig. <ref type="figure" target="#fig_3">5</ref>: (i) A test maze and (ii) an environment with thin walls and clutter. Then, we conducted the following experiment: 100 random start and target positions were sampled for each of the two environments and consistently used for the evaluation of all models. Possible outcomes for each run are a success, a timeout or a crash. The timeout is triggered, if the target cannot be reached within 5 min. This time would allow the robot to travel 60 m with an average speed of 0.2 ms -1 and should suffice to reach the target on a 10 m × 10 m map. Each episode is aborted after a collision. The resulting trajectories of the evaluation with model c 1000 +CPO on both maps are visualized in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>Based on the 200 evaluation trajectories per model, Fig. <ref type="figure" target="#fig_4">6</ref> presents the resulting statistics. For comparison, first, we trained the model presented in <ref type="bibr" target="#b2">[3]</ref> in our environments, which in the following will be referred to as the V2R (virtual-to-real) model. Second, we used their policy architecture to train our R-IL policy (pretrained in c 1000 ) in order to test the generalization to other model structures (c 1000 +CPO V 2R ). The robot's velocity was removed from the inputs (resulting in 12 inputs) as supervised learning approaches (as for pre-training) tend to predict the prior velocity values instead of focussing on the perception <ref type="bibr" target="#b3">[4]</ref>.</p><p>Fig. <ref type="figure" target="#fig_4">6</ref> shows that more reward information during training and more pre-training samples not only benefit the training but also the generalization performance. c 1000 +CPO, the model with shortest distance reward and complex pre-training, shows the best generalization performance to unseen environments (using the model structure shown in Figure <ref type="figure" target="#fig_0">2</ref>), with a success rate of 79%. Interestingly, even the model with only sparse reward and 10 demonstration trajectories in the simple environment shows similar performance to the fixed collision penalty TRPO methods, which were pre-trained with 1000 samples and use the full reward. Both R-IL TRPO methods show a lower success rate  <ref type="figure" target="#fig_3">5</ref>. The outcome of each trajectory can be a success, a timeout (not reaching the target after 5 min), or a crash. The models are split in five categories: R-IL, where IL is combined with 1000 RL iterations; R-IL 200 ; with 200 RL iterations only; pure IL; pure RL and the comp. approaches for comparison, comprising the method presented in <ref type="bibr" target="#b2">[3]</ref> (V2R) and our method based on the model presented in <ref type="bibr" target="#b2">[3]</ref> (c 1000 +CPO V 2 R ). More details of the analyzed models can be found in Table <ref type="table" target="#tab_0">I</ref>.</p><p>than the corresponding CPO model (s 1000 +CPO), which also shows that encoding both collision avoidance and reaching the target in one reward is inferior to encoding the collision avoidance as a constraint. Furthermore, the R-IL 200 models show that early stopping of the training (at 200 RL iterations) still leads to similar performance as training pure RL from scratch. Therefore, pre-training allows for a RL training time reduction of around 80% in order to achieve the same performance. CPO1 model, which reached a high success rate during training, does not generalize properly to unseen and more complex environments.</p><p>The V2R method <ref type="bibr" target="#b2">[3]</ref> (second-to-right bar) shows a similar success rate as the CPO123 model, while the crash rate is about 50% higher although a collision penalty of 1.0 was used. However, it uses the Euclidean distance reward which is a slight disadvantage compared to CPO123. With V2R, the same problems as with other fixed collision penalty methods can be observed, which is the difficult tuning between exploration and collision avoidance. Our approach also generalizes well to other model structures as the one presented in <ref type="bibr" target="#b2">[3]</ref>, as shown by the rightmost bar of Fig. <ref type="figure" target="#fig_4">6</ref>. Using this simpler architecture, the success rate can even be further improved in our test scenarios, which leaves more room for further graph optimization, which is not covered in this letter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Real-World Experiments</head><p>Moving to the real world scenarios further shows the generalization capabilities of the models and also their robustness against sensor noise and actuation delays. The models are purely trained in simulation and the real-world test environment (see Fig. <ref type="figure" target="#fig_5">7</ref>) is unknown to the agents.</p><p>A quantitative analysis of the trajectories is provided in Table <ref type="table" target="#tab_1">II</ref>, where the number of crashes, the amount of manual joystick interference and the comparison of the learningbased trajectories compared to the ones taken by the grid-based move_base planning module (which uses global map information) are listed. Table II both lists the average and maximum values observed during five runs per model. The human joystick interference was triggered, if no motion command was sent by the autonomous agent for 10 seconds.</p><p>The pure RL model tends to be more cautious, which results in a larger factor λt MB , which is the relative time compared to a global planner. The pure IL model collides more often as there is no collision constraint or penalty during training. Also the R-IL  models generalize well to the unseen real-world environment and show similar performance. As expected, c 1000 +CPO shows the best performance. However, s 10 +CPO spar se performs surprisingly well. This can be explained by the fact that the sparse reward structure allows for the best generalization performance to unseen environments, since no information about the shortest path to the goal has to be inferred. This is a promising result, as for this model no environment information and reward shaping is required. By combining sparse reward with pre-training and constraint-based RL, even real-world training might be feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we presented a case study for a learning-based approach for map-less target driven navigation. It is based on an end-to-end neural network model which maps from raw sensor measurements and a relative target location to motion commands of a robotic platform and is trained using a combination of imitation (IL) and reinforcement learning (RL). We compare different combinations of prior demonstrations for IL, different RL algorithms and analyze the influence of different reward structures.</p><p>Our simulation and real-world experiments show that target-driven demonstrations through IL significantly improve the exploration during RL. The RL training time in R-IL can be reduced by around 80% while still achieving similar final performance in terms of success rate and collision avoidance. While pure RL does achieve the same collision avoidance capabilities as R-IL, there are significant differences in the target reaching success. Pre-training with supervised IL provides a good intuition for more efficient exploration during RL, even if only 10 demonstrations are provided. This becomes even more pronounced when using low information reward structures, like sparse target reward.</p><p>Furthermore, our experiments show that constraint-based methods focus on enforcing the collision constraint early during training. This makes exploration harder yet allows for safer training and deployment which becomes important when moving towards real-world applications. Therefore, especially in combination with IL, to achieve safe navigation capabilities, we recommend to enforce collision avoidance by constraint instead of a fixed penalty in the reward signal.</p><p>Our trained navigation models are able to reliably navigate in unseen environments, both in simulation and the real world. We do not recommend to replace global planning if a map is available, yet this work shows the current state of what is possible using only local information for navigation scenarios, where no environment map is available.</p><p>While in this work, training was purely conducted in simulation, in future work we will investigate how real-world human demonstrations can be leveraged and how this navigation method can be extended to dynamic environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The neural network model for π θ . The normalized input data is fed through three fully connected layers with tanh activation functions. Between layer one and two, dropout is added during IL training. The outputs are denormalized to obtain physical control commands from the neural network.</figDesc><graphic coords="3,304.43,79.73,243.86,55.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Training maps for IL and RL. The TM vary significantly in difficulty. Maps can be better viewed by zooming in on a computer screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The evolution of navigation success and crash rates throughout the RL training process of various models. The curves indicate the rolling mean of success and crash rates over 20 steps. The models contain pure RL models, pure IL models and R-IL which differ in the amount and complexitiy of pre-training, the reward structure and the RL training procedure. The black line indicates the performance of IL on the training maps (TM-123) as a reference. For the RL training, where multiple runs were conducted, only the best one is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Evaluation runs between 100 randomly sampled start and target positions on the two unknown test maps (both 10 m × 10 m). The model used for visualization is c 1000 +CPO. The trajectories are shown in blue, the starting positions in green, the set targets in red, the trajectory end points in yellow and crashes as magenta crosses.</figDesc><graphic coords="6,308.15,67.25,244.82,95.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Evaluation results of 200 trajectories on the previously unseen test maps (100 each) as shown in Fig.5. The outcome of each trajectory can be a success, a timeout (not reaching the target after 5 min), or a crash. The models are split in five categories: R-IL, where IL is combined with 1000 RL iterations; R-IL 200 ; with 200 RL iterations only; pure IL; pure RL and the comp. approaches for comparison, comprising the method presented in<ref type="bibr" target="#b2">[3]</ref> (V2R) and our method based on the model presented in<ref type="bibr" target="#b2">[3]</ref> (c 1000 +CPO V 2 R ). More details of the analyzed models can be found in TableI.</figDesc><graphic coords="7,327.11,237.05,214.94,174.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Trajectories driven with the real robotic platform for a subset of the models analyzed in Fig. 6. Red dots depict the numbered target positions, crosses in trajectory colors show crashes of the corresponding agents. For clarity reasons, only the first out of 5 runs with each model is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MODEL</head><label>I</label><figDesc>DETAILS, INCLUDING THE MAPS AND NUMBER OF TRAJECTORIES USED FOR IL AND THE REWARD SIGNAL USED FOR RL. BESIDES CPO1, ALL MODELS ARE TRAINED ON ALL THREE T M MAPS. CPO AND TRPO IN THE MODEL NAME SPECIFY THE RL TRAINING PROCEDURE, THE SUBSCRIPT OF TRPO INDICATES THE FIXED PENALTY WEIGHT FOR COLLISIONS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II AVERAGE</head><label>II</label><figDesc>RESULTS (5 RUNS) FROM THE REAL-WORLD EXPERIMENTS, AS SHOWN IN FIG. 7. THE CORRESPONDING MAXIMUM VALUES ARE LISTED IN PARENTHESIS. D R C STANDS FOR THE REMOTE CONTROLLED (JOYSTICK) DISTANCE, λd MB FOR THE RELATIVE DISTANCE COMPARED TO Move_base AND λt MB FOR THE RELATIVE TIME COMPARED TO move_base</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Our source code is available here: https://github.com/ethz-asl/rl-navigation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://youtu.be/uc386uZCgEU</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://kobuki.yujinrobot.com/about2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://wiki.ros.org/move_base</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This letter was recommended for publication by Associate Editor J. Kober and Editor T. Asfour upon evaluation of the reviewers' comments. This work was supported by the European Union Horizon 2020 project CROWDBOT under Grant 779942. (Mark Pfeiffer and Samarth Shukla contributed equally to this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Planning Algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lavalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From perception to decision: A data-driven approach to end-to-end motion planning for autonomous ground robots</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaeuble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Automat</title>
		<meeting>IEEE Int. Conf. Robot. Automat</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1527" to="1533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Off-road obstacle avoidance through end-to-end learning</title>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imitating driver behavior with generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intell. Veh. Symp</title>
		<meeting>Intell. Veh. Symp</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="204" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning actions through imitation and exploration: Towards humanoid robots that learn from humans</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Creating Brain-Like Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="103" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Constrained policy optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Apprenticeship learning for motion planning with application to parking lot navigation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dolgov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting actions to act predictably: Cooperative partial motion planning with maximum entropy models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schwesinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Galceran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst</meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="page" from="2096" to="2101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Socially compliant mobile robot navigation via inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sprunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1289" to="1307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Watch this: Scalable costfunction learning for path planning in urban environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2089" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep neural network for real-time autonomous indoor navigation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04668</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal deep autoencoders for control of a mobile robot</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sergeant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Australas. Conf. Robot. Automat</title>
		<meeting>Australas. Conf. Robot. Automat</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf</title>
		<meeting>14th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning monocular reactive UAV control in cluttered natural environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Automat</title>
		<meeting>IEEE Int. Conf. Robot. Automat</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1765" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Socially-compliant navigation through raw depth inputs with generative adversarial imitation learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>presented at ICRA</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical reinforcement learning for robot navigation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen-Tuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Streichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int</title>
		<meeting>11th Int</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="149" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A reinforcement learning based robotic navigation system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Syst., Man, Cybern</title>
		<meeting>IEEE Int. Conf. Syst., Man, Cybern</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3452" to="3457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03673</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One-shot reinforcement learning for robot navigation with interactive replay</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<idno>abs/1711.10137</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with successor features for navigation across similar environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2371" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Automat</title>
		<meeting>IEEE Int. Conf. Robot. Automat</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3357" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Socially aware motion planning with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1343" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining imitation and reinforcement learning to fold deformable planar objects</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balaguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1405" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reinforcement and imitation learning for diverse visuomotor skills</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://www.roboticsproceedings.org/rss14/p09.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. of Robotics: Science and Syst</title>
		<meeting>of Robotics: Science and Syst</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Safe modelbased reinforcement learning with stability guarantees</title>
		<author>
			<persName><forename type="first">F</forename><surname>Berkenkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schoellig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="908" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ROS: An open-source robot operating system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Quigley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICRA Workshop Open Source Softw</title>
		<meeting>ICRA Workshop Open Source Softw<address><addrLine>Kobe, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Massively multi-robot simulation in stage</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
