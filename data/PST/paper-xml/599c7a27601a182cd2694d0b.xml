<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">X</forename><surname>Lagorce</surname></persName>
							<email>xavier.lagorce@ens-cachan.fr</email>
						</author>
						<author>
							<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
							<email>ryad.benosman@upmc.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computation Group</orgName>
								<orgName type="institution">Institut National de la Santé et de la Recherche Médicale</orgName>
								<address>
									<postCode>F-75012</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institut de la Vision</orgName>
								<orgName type="department" key="dep2">Centre National de la Recherche Scientifique</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">Université</orgName>
								<address>
									<addrLine>Paris 06</addrLine>
									<postCode>F-75012, F-75012</postCode>
									<settlement>Paris, Paris</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute for Neurotechnology (SINAPSE)</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DEB24289F3852A1CE7BAC34D270DA72F</idno>
					<idno type="DOI">10.1109/TPAMI.2016.2574707</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-This paper describes novel event-based spatiotemporal features called time-surfaces and how they can be used to create a hierarchical event-based pattern recognition architecture. Unlike existing hierarchical architectures for pattern recognition, the presented model relies on a time oriented approach to extract spatio-temporal features from the asynchronously acquired dynamics of a visual scene. These dynamics are acquired using biologically inspired frameless asynchronous event-driven vision sensors. Similarly to cortical structures, subsequent layers in our hierarchy extract increasingly abstract features using increasingly large spatio-temporal windows. The central concept is to use the rich temporal information provided by events to create contexts in the form of time-surfaces which represent the recent temporal activity within a local spatial neighborhood. We demonstrate that this concept can robustly be used at all stages of an event-based hierarchical model. First layer feature units operate on groups of pixels, while subsequent layer feature units operate on the output of lower level feature units. We report results on a previously published 36 class character recognition task and a 4 class canonical dynamic card pip task, achieving near 100% accuracy on each. We introduce a new 7 class moving face recognition task, achieving 79% accuracy.</p><p>Index Terms-Neuromorphic sensing, Event-based vision, Feature extraction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F EATURE selection for object recognition is a fundamen- tal problem in the study of visual processing. The open issue is always to determine how features of an image should be extracted and characterized. In traditional computer vision, visual features are typically defined as a function of the static luminance information within a local spatial neighborhood of an image <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>. Different feature types differ only in the function they apply to the image. The temporal content of features has rarely been explored or tackled, mainly due to the three underlying hypotheses on which machine vision is based. The first hypothesis is that scenes are observed using a stroboscopic acquisition which produces a collection of static images (frames). Images are currently at the core of the whole field of artificial vision. So far, everything that has been developed has been designed to acquire, operate on, and display frames. A major drawback of frame-based acquisition is that it acquires information in a way that is independent of the dynamics of the underlying scene <ref type="bibr" target="#b2">[3]</ref>. Scene illumination is measured at unnatural fixed time periods (frame-rate), resulting in acquisition of huge amounts of redundant data because most pixels will not change from one frame to the next. Massive redundancy in the acquired data is what allows video compression algorithms to achieve such impressive compression ratios (often around 50:1 <ref type="bibr" target="#b3">[4]</ref>). However, before compression, this redundant data is still unnecessarily sampled, digitized, and transmitted, inducing a waste of resources, before expending even more resources to implement compression. This process sets important limitations on artificial perception that might one day be surmounted by using faster and more powerful computing devices (e.g. GPUs, clusters, etc.), but always at the cost of increasing power consumption. Nevertheless, the lack of dynamic content and the acquisition of both relevant and non-relevant data will always be the fundamental limit of images.</p><p>The second hypothesis of machine vision is that absolute pixel illumination (gray levels or colors) is the main source of information. However illumination is not an invariant property of a scene <ref type="bibr" target="#b0">[1]</ref>. Most current algorithms fail to operate in uncontrolled lighting conditions. The ability to accurately measure luminance is also limited by the low dynamic range of conventional cameras <ref type="bibr" target="#b4">[5]</ref>.</p><p>The third hypothesis is that real-time operation implies a minimum of 24 images per second (the frame rate of common video formats <ref type="bibr" target="#b3">[4]</ref>). There is currently a widespread belief in the field of artificial vision that high visual acquisition rates are only useful for cases where a fast changing stimulus must be observed. It is true that sensations of dynamic motion can be observed at 24 fps. However, it has been recently shown that biological retinas operate at temporal precision of 1kHz (see <ref type="bibr" target="#b5">[6]</ref>) because that is where most of the information of everyday scenes are <ref type="bibr" target="#b6">[7]</ref>. If conventional scenes are processed at low temporal acquisition rates , it has been shown that there is a loss of 75% of valuable information leading to a poor separability between classes of objects <ref type="bibr" target="#b6">[7]</ref>. Currently it is computationally and energetically expensive to process visual input in real time using conventional cameras at above 100Hz. This because the amount of data which must be processed grows linearly with the frame rate, while the amount of information captured only grows sublinearly <ref type="bibr" target="#b6">[7]</ref>.</p><p>However, the field of Neuromorphic Engineering <ref type="bibr" target="#b7">[8]</ref> has been developing bio-inspired event-driven, time-based vision sensors which operate on a very different principle <ref type="bibr" target="#b8">[9]</ref>. Instead of capturing static images of the scene, these sensors record pixel intensity changes with high temporal precision. This high temporal precision provides information about scene dynamics which can aid in recognition and increase class separability <ref type="bibr" target="#b6">[7]</ref>. In the last decade these sensors have matured to a point where they are now commercially available and can be operated by laymen. Event-driven time-based vision sensors promise to allow for power efficient low latency visual sensing in real world moving scenes, which has the potential for major impact in robotics, as well as mobile and wearable devices.</p><p>Event-driven time-based vision sensors provide data in the Address Event Representation (AER) format <ref type="bibr" target="#b9">[10]</ref> which differs significantly from frames, and therefore conventional Machine Vision algorithms cannot be directly applied. For the task of object recognition, accuracy using event-driven timebased vision sensors still lags behind traditional approaches. Previous notable works on object recognition using eventdriven time-based vision sensors include the Convolution AER Vision Architecture for Real-time (CAVIAR) project <ref type="bibr" target="#b10">[11]</ref> which recognizes and tracks circles of different sizes using a hierarchical spiking network running on custom hardware. Later work progressed to differentiation between different shapes (circle, square, triangle) using an HMAX inspired algorithm also on custom silicon hardware <ref type="bibr" target="#b11">[12]</ref>. Targeting more complex shapes, Perez-Carrasco et al. introduced a card pip recognition task which has been tackled in real-time using FPGAs running different hierarchical spiking models inspired by Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b12">[13]</ref> and HFirst <ref type="bibr" target="#b13">[14]</ref>.</p><p>Inspired by the popularity of the MNIST database (Mixed National Institute of Standards and Technology database) <ref type="bibr" target="#b1">[2]</ref> in traditional machine vision, there has been a recent focus on character recognition using event-driven time-based vision sensors, which has been tackled using CNNs <ref type="bibr" target="#b12">[13]</ref>, Hierarchical like models <ref type="bibr" target="#b13">[14]</ref>, and Deep Belief Networks (DBNs) <ref type="bibr" target="#b14">[15]</ref>. Perez Carrasco et al. showed how recent advances in training frame based CNNs can be leveraged by converting frame based CNNs to spiking CNNs for recognition <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Moving past shape and character recognition, similar hierarchical models have been developed for application to human posture detection <ref type="bibr" target="#b16">[17]</ref>. Frame-based CNNs have also been applied for discriminating between vehicles and pedestrians in a traffic scene, and recognizing household objects <ref type="bibr" target="#b17">[18]</ref>. Other methods started to explore the integration of dynamical information into recognition by using motion-direction sensitive units <ref type="bibr" target="#b18">[19]</ref> or dynamical networks (like Echo-state networks) <ref type="bibr" target="#b19">[20]</ref>.</p><p>This paper serves to advance the state of the art for performing recognition using event-driven time-based vision sensors. To provide comparison to previous works, we tackle the previously published card pip dataset <ref type="bibr" target="#b20">[21]</ref> and character recognition tasks <ref type="bibr" target="#b13">[14]</ref>, achieving near perfect accuracy on both. As a first step towards performing human user recognition using eventdriven time-based vision sensors, we introduce a new, more challenging, facial recognition task on which we achieve 79% accuracy, providing room for improvement in future work.</p><p>This paper begins with an introduction to the operation of event-driven time-based vision sensors in Section II, followed by a description of the hierachical time-surface feature extraction technique in Section III. We then describe performance of the technique in Section IV, before wrapping up with a discussion of results and a conclusion in Section V and Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EVENT-DRIVEN TIME-BASED VISION SENSORS</head><p>Biomimetic event-driven time-based vision sensors are a novel class of vision device that -like the biological retinaare driven by "events" happening within the visual scene. They are not like conventional vision sensors, which are driven by artificially created timing and control signals (e.g. frame clock) that have no relation whatsoever to the source of the visual information <ref type="bibr" target="#b22">[23]</ref>. Over the past few years, a variety of these event-based devices has been developed, including temporal contrast vision sensors that are sensitive to relative luminance change <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b22">[23]</ref>, gradient-based sensors sensitive to static edges <ref type="bibr" target="#b24">[25]</ref>, and optical-flow sensors <ref type="bibr" target="#b25">[26]</ref>. Most of these vision sensors output visual information about the scene in the form of asynchronous address events using the Address Event Representation (AER) protocol <ref type="bibr" target="#b9">[10]</ref> and encode the visual information in the time dimension rather than as a voltage, charge, or current. The novel features we propose in this paper are designed to take advantage of the high temporal resolution data representation provided by event-based cameras, which is not provided by frame-based sensors.</p><p>This work uses datasets recorded by different event-based sensors <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b22">[23]</ref> and previously used in other publications <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Due to the high accuracy we achieve on these dataset, we introduce a new set of recordings acquired using the Asynchronous Time-based Image Sensor (ATIS) <ref type="bibr" target="#b27">[28]</ref>. This sensor is also a time-domain encoding vision sensor with 304x240 pixel resolution. The sensor contains an array of fully autonomous pixels that combine an illuminance relative change detector circuit and a conditional exposure measurement block.</p><p>The working principle of the ATIS sensor is shown Fig. <ref type="figure" target="#fig_0">1</ref>. In this paper, we rely only on the output of a circuit contained in the ATIS pixels (see <ref type="bibr" target="#b21">[22]</ref>) that detects relative changes in pixel log luminance over time. As soon as a change is detected, the process of communicating this change event offchip is initiated. Off-chip communication executes with low latency (on the order of microseconds), ensuring that the time at which a change event is readout from the ATIS inherently represents the time at which the change was detected. This ON events OFF events Fig. <ref type="figure">2</ref>. Definition of a time-surface from the spatio-temporal cloud of events. A time-surface describes the recent time history of events in the spatial neighborhood of an event. This figure shows how the time-surface for an event happening at pixel x 0 = [x 0 , y 0 ] T at time t 0 is computed. The event-driven time-based vision sensor (a) is filming a scene and outputs events shown in (b) where ON events are represented on the left hand picture and OFF events on the right hand one. For clarity, we continue by only showing values associated to OFF events. When an OFF event ev i = [x 0 , t i , -1] arrives, we consider the times of most recent OFF events in the spatial neighborhood (c) where brighter pixels represent more recent events. Extracting a spatial receptive field allows to build the event-context T i (x, p) (d) associated with that event. Exponential decay kernels are then applyed to the obtained values (e) and their values at t i constitute the time-surface itself. (f) shows these values as a surface. This representation will be used in the following figures and the label of the axes will be removed for better clarity.</p><p>asynchronous low-latency readout scheme provides the high temporal resolution change detection data our features rely on. We discuss two types of change detection events: "ON" events and "OFF" events, which respectively indicate that an increase or decrease in log pixel intensity has been detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL DESCRIPTION</head><p>In this section we describe the construction of our architecture for object recognition. We begin by formally defining time-surfaces (Section III-A), and how time-surface prototypes can be learnt from input data (Section III-B). Then we show how these time-surface prototypes can be arranged to form a hierarchical model (Section III-C). Finally, in Section III-D we describe how classification is performed on the model output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Time-surface</head><p>The process of building a time-surface from the output of an event-driven time-based vision sensor is illustrated in Fig. <ref type="figure">2</ref> and described hereafter.</p><p>Consider a stream of visual events (Fig. <ref type="figure">2(b</ref>)) which can be mathematically defined as</p><formula xml:id="formula_0">ev i = [x i , t i , p i ] T , i ∈ N (1)</formula><p>where ev i is the i th event and consists of a location (x i = [x i , y i ] T ), time (t i ) and polarity (p i ), with p i ∈ {-1, 1}, where -1 and 1 represent OFF and ON events respectively. When an object (or the camera) moves, the pixels asynchronously generate events which form a spatio-temporal point cloud representing the object's spatial distribution and dynamical behavior. Fig. <ref type="figure">2</ref>(b) shows such events generated by an object rotating in front of the sensor ((Fig. <ref type="figure">2(a)</ref>) where ON and OFF events are represented respectively by white and black dots.</p><p>Because the structure of this point cloud contains information about the object and its movement, we introduce the timesurface S i of the i th event ev i to keep track of the activity surrounding the spatial location x i just before time t i . We can then define T i (u, p) a time-context around an incoming event ev i as the array of most recent events times at t i for the pixels in the (2R + 1) × (2R + 1) square neighborhood centered at</p><formula xml:id="formula_1">x i = [x i , y i ] T as: T i (u, p) = max j≤i {t j | x j = (x i + u), p j = p} ,<label>(2)</label></formula><p>where Let S i (u, p), be the time-surface around an event ev i (shown in Fig. <ref type="figure">2</ref>(e)), it is defined by applying an exponential decay kernel with time constant τ on the values of T i (u, p).</p><formula xml:id="formula_2">u = [u x , u y ] T is such that u x ∈ {-R, . . . , R}, u y ∈ {-R, . . . , R} and p ∈ {-1, 1} T i (x, p) is</formula><formula xml:id="formula_3">S i (u, p) = e -(ti-Ti(u,p))/τ .<label>(3)</label></formula><p>S i provides a dynamic spatiotemporal context around an event, the exponential decay expands the activity of passed events and provides information about the history of the activity in the neighborhood. The resulting surface S i (u, p) is shown in Fig. <ref type="figure">2</ref>(f) for the OFF events represented all along Fig. <ref type="figure">2</ref>. In the following sections S i (u, p) will be referred to directly as S i to simplify notations. In the figures it will be represented as a surface showing the values of each of its element at their corresponding spatial positions. Fig. <ref type="figure" target="#fig_4">3</ref> shows examples of time surfaces for simple moving edges. One can see, that a time surface is composed of two halves corresponding to the two polarity of incoming events. The first half has positive values, showing points corresponding to the ON events (p = 1) and the second half has negative values showing points corresponding to the OFF events (p = -1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Time-surface prototypes</head><p>Time-surface prototypes take the form of time-surfaces themselves, but whereas each incoming event will have a different surface, the time-surface of each prototype remains constant (except during an initial learning phase). Time-surface prototypes are the set of elementary surfaces that are encountered in the observed scenes. The process of learning a timesurface for each prototype is described below, it relies on an incremental clustering process <ref type="bibr" target="#b28">[29]</ref>. When an input event arrives at a bank of time-surface prototypes, the time-surface associated to the incoming event is calculated and compared to the time-surface of each prototype. The prototype with the time-surface most closely matching the surface of the input event will then generate an output event. We begin with a set of N inital time-surface prototypes, C n , n ∈ 1, N , where C n takes the same form as S i in (3). For initialization we simply use the first N time-surfaces as our initial values for the N prototypes. More formally:</p><formula xml:id="formula_4">C n = S n n ∈ 1, N<label>(4)</label></formula><p>We then implement learning using the online clustering algorithm described in <ref type="bibr" target="#b28">[29]</ref>.</p><p>For each input event, ev i , we calculate the time-surface, S i , and find C k , the time-surface prototype closest to S i according to the Euclidian distance. C k is then updated using:</p><formula xml:id="formula_5">C k ← C k + α(S i -βC k )<label>(5)</label></formula><p>with</p><formula xml:id="formula_6">β = cos(C k , S i ) = C k • S i ||C k || • ||S i || (6) α = 0.01 1 + p k 20000 (7)</formula><p>where p k is the number of time-surfaces which have already been assigned to C k .</p><p>The full clustering process is summarized in Algorithm 1. After this learning process, each time-surface can be associated to a particular prototype C k . In this manner, the stream of input events is transformed into a stream of prototype activations: </p><formula xml:id="formula_7">f eat i = [x i , y i , t i , k i ] T (8)</formula><formula xml:id="formula_8">+ p k /20000) β ← C k • S i /(||C k || • ||S i ||) C k ← C k + α(S -βC k ) p k ← p k + 1 end for</formula><p>where k i is the index of the cluster center C ki .</p><p>At this stage, when comparing an event's time-surface to a bank of time-surface prototypes one can also refine the process by adding some noise filtering. If an event is isolated (meaning that its time-surfaces is only made of a peak at the center), it can be dropped. It is also possible to add a maximum distance from the prototypes over which the time-surface is considered not to match any of the prototypes in the bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Creating a Hierarchical Model</head><p>Fig. <ref type="figure" target="#fig_6">4</ref> illustrates the hierarchical model we introduce in this paper. Steps (a) to (g) sum up the process described in the previous sections. As shown in Fig. <ref type="figure" target="#fig_6">4</ref>, a moving digit (a) is presented to the ATIS camera (b) which produces ON and OFF events (c). Time-surfaces are built by convolving them with an exponential kernel of time constant τ 1 (d) and considering spatial receptive fields of sidelength (2R 1 + 1). These timesurfaces are then clustered into N 1 prototypes represented as surfaces (e) in the Layer 1 box. When a cluster center is matched, an event is produced, resulting in the activations shown in (f). These events constitute the output of Layer 1 (g). One can see that each incoming event from the observed pattern is associated with the most representative prototype surface. The nature of the output of Layer 1 is exactly the same as its input: Layer 1 outputs timed events. Once a prototype matches the temporal surface around the incoming event it immediately emits an event. Thus, the same steps used in Layer 1 (from (d) to (g)) can be applied in Layer 2. However the emitted event is now representing the temporal activity of a prototype surface, it thus carries more meaning than the initial camera event. The prototype surfaces of Layer 2 represent the temporal signature of the activity of complex features. Layer 2 uses different constants for space-time integration of features (R 2 , N 2 and τ 2 ). The goal is to introduce stability of the perceptual representation and sensitivity to the accumulation of sensory evidence over time. This integration over longer and longer time period will thus be able to accumulate evidence in favor of alternative propositions in a recognition process. When alternatives with a barely discernible difference in their sensory inputs are presented over an extended period of time, longer time and spatial integration scales can accumulate the small These activations constitute the output of the layer. The output (i) of the last layer is then fed to the classifier (j) which will recognize the object.</p><p>differences over time until it becomes eventually possible to discriminate the alternatives through its ever growing output. This accumulation dynamics is at the heart of the HOTS model, the difference between time scales can be substantial and can start from 50ms for Layer 1 to 250ms for Layer 2 to finally reach 1.25 s for Layer 3. Layer 3 receives input from Layer 2, it is the last layer of the system and it provides the highest level information integration, as shown in Fig. <ref type="figure" target="#fig_6">4</ref>(i) time-surface prototypes are also larger both spatially and temporally. The output of the temporal activity of Layer 3 can finally be used for object recognition by being fed to a classifier (shown in Fig. <ref type="figure" target="#fig_6">4(j)</ref>).</p><p>As stated above, each layer is then defined by only a few parameters (we add an index l for the lth layer of the system):</p><p>• R l , which defines the size of the time-surface neighborhood • τ l , the time constant of the exponential kernel applied to events • N l , the number of cluster centers (prototypes) learnt by the clustering algorithm. To increase the information extracted by each subsequent layer, we make these parameters evolve between subsequent layer. For each layer, we define the parameters K R , K τ , K N so that:</p><formula xml:id="formula_9">R l+1 = K R • R l (9) τ l+1 = K τ • τ l (10) N l+1 = K N • N l<label>(11)</label></formula><p>The obtained architecture consists in a Hierarchy Of Time-Surfaces (HOTS) which is building and extracting a set of features (the prototypes from the final layer) out of a stream of input events. The time-surface prototypes will then be called time-surface features in the rest of the paper.</p><p>Fig. <ref type="figure" target="#fig_4">3</ref> shows what these features could be for the first layer of the achitecture where its input basis is constituted of only two vectors: ON events and OFF events. The other layers have input bases constituted of more vectors (as many as the number of features extracted by their previous layer), thus we could represent their features by a serie of surfaces each corresponding to one feature of the previous layer. Because this representation is harder to relate to the actual input from the camera activating the feature, we chose to fuse these surfaces into their corresponding activity of ON and OFF events. The features of every layer of the architecture will then be represented as a set of two surfaces such as in Fig. <ref type="figure" target="#fig_4">3</ref>, showing an image of the activity of ON and OFF events associated to the feature, this what is represented Fig. <ref type="figure" target="#fig_6">4</ref> in the gray boxes representing the different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Classification</head><p>In this section we describe how the output of Layer 3 can be used as features for object recognition. Training of the recognition algorithm consists of two main steps. In the first step, different stimuli are presented to the model to learn the time-surface prototypes (referred to in the next sections as features) computed as described in the previous section. This is the training phase of the algorithm (model). In a second step, the same learning stimuli are presented to the trained model and a histogram of the time-surface feature activations in the final layer is built for each object class. This is the training phases referred to as the classifier. A similar histogram is built for each test stimulus, it can then be compared to trained histograms to determine which object is present in the scene. The choice of the histogram is to show the robustness of the method, historams of activities as we will show are sufficient to provide reliable recognition scores. More complex classifier could be used specially time oriented ones such as Echo State Networks <ref type="bibr" target="#b29">[30]</ref> or reccurent networks <ref type="bibr" target="#b30">[31]</ref>, these would allow the learning of the temporeal dynamics of activated features. However, as we will show in the experiment section, this is not necessary as the mean activity of features activation is sufficient to achieve high recognition scores. When an object is presented to the camera between instants t start and t end , the time-surface feature activations form the set:</p><formula xml:id="formula_10">F(t start , t end ) = {f eat i | t i ∈ [t start , t end ]}<label>(12)</label></formula><p>From this set, it is possible to build a histogram H counting how many times each feature has been activated, independently of its spatial position. This will constitute the signature of the observed objects.</p><p>To estimate the distance between two histograms, we use three different distances. We will refer to the standard distance when we will use the euclidian norm of the difference between two histograms (by looking at histograms as vectors in which the k th coordinate is the number of times feature k was matched):</p><formula xml:id="formula_11">d(H 1 , H 2 ) = ||H 1 -H 2 ||<label>(13)</label></formula><p>We will refer to the normalized distance when we will use the euclidian norm of the difference between two histograms which have each been normalized by the number of generated features:</p><formula xml:id="formula_12">d N (H 1 , H 2 ) = H 1 card(H 1 ) - H 2 card(H 2 )<label>(14)</label></formula><p>where card(H k ) is the total number of features counted in H k . We will also use the Bhattacharyya distance <ref type="bibr" target="#b31">[32]</ref> defined as:</p><formula xml:id="formula_13">d B (H 1 , H 2 ) = -ln i H 1 (i) card(H 1 ) • H 2 (i) card(H 2 ) . (<label>15</label></formula><formula xml:id="formula_14">)</formula><p>Because these histograms are characteristics of the classes to recognize, we will refer to them as signatures in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TESTING</head><p>The proposed method has been tested on three different tasks. The first consists of recognizing pips on poker cards as they are shuffled in front of the sensor to identify their suit. This task, which will be referred to as the flipped card deck recognition task (Section IV-A), has already been tackled by <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b13">[14]</ref>. The second task is a simulated reading task in which characters are recognized as they move across the field The database for this experiment consists of the four suits (spades, hearts, clubs, and diamonds) found in a card deck. They are captured by a sensitive DVS sensor as the cards are flipped in front of it (white dots represent ON events and black dots OFF events). Histograms of feature activation numbers for the four suits moving in front of the camera. X axis is the index of the feature shown in the supplemental material, Y axis is the number of activations of the feature during the stimulus presentation. Each column corresponds to one suit. The snapshots show how the pips evolve during one particular presentation (each snapshot is taken at a regular time interval). Each pattern outputs a different signature that allows its recognition.</p><p>of view of the sensor. This task, which will be referred as the letters &amp; digits recognition task (Section IV-B), has already been tackled by <ref type="bibr" target="#b13">[14]</ref>. These first two tasks have been chosen to provide comparison to previously published work. The third task is a face recognition task, which will be referred as the Face recognition task (Section IV-C). The data used for these different tasks are illustrated in Figs. <ref type="figure" target="#fig_7">5,</ref><ref type="figure" target="#fig_10">8</ref>, and 12 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Flipped card deck recognition task</head><p>The first experiment is run on the card dataset provided by Teresa Serrano-Gotarredona and Bernabe Linares-Barranco <ref type="bibr" target="#b20">[21]</ref> who captured the data using the sensitive DVS <ref type="bibr" target="#b26">[27]</ref>. It represents a set of playing cards which are being flipped in front of the sensor (see Fig. <ref type="figure" target="#fig_7">5</ref> and Fig. <ref type="figure" target="#fig_8">6</ref>). The pip representing the suit of the card has been isolated from the recording and the classifier has to determine the suit of the presented card. The data consists of ten presentations of each of the four suits, in the following, one presentation will be used for learning and the other nine for testing. Because the cards are being flipped by hand at high speed during the recording, an important deformation of the symbols occurs.</p><p>We use the hierarchical system described in the previous section with 3 layers. The parameters for the first layer are:</p><formula xml:id="formula_15">• R 1 = 2, • τ 1 = 20 ms, • N 1 = 4.</formula><p>To go from one layer to the next, we use the following parameters:</p><formula xml:id="formula_16">• K R = 2, • K τ = 10, • K N = 2.</formula><p>The sensor feeds spiking data into the first layer of the hierarchical model and histograms are built from the third layer's output. All four suits are used to train the model and each layer is trained sequentially. The features extracted in each layer are presented in the supplemental material.</p><p>Training stimuli (a single presentation for each suit) are then presented to the system again to train the classifier. Examples of the classifier histograms are shown in Fig. <ref type="figure" target="#fig_8">6</ref>. The rest of the stimulus examples (nine presentations) for each object are used for testing. The results can be seen in Fig. <ref type="figure" target="#fig_9">7</ref>. In Fig. <ref type="figure" target="#fig_9">7</ref> the model and classifier have both been trained with only one presentation of each of the four suits. The nine other stimulus examples for each suit are used for testing.</p><p>The four rows along the vertical axis each show results for a different suit during testing. Each section between dashed lines shows histogram distances for one card flipped in front of the sensor. The different bars encode the histogram distances where white, gray, and black bars code for the standard, normalized, and Bhattacharyya distances respectively. The recognized class is the one with the smallest bar in each column, and is marked with a star. This particular experiment leads to performances of 94%, 100% and 97% with the standard, normalized, and Bhattacharyya distances respectively.</p><p>Running some cross validation tests on the data gave us performances of 95% -100% with all three distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Letters &amp; Digits recognition task</head><p>The second experiment is run on a dataset provided by Orchard <ref type="bibr" target="#b13">[14]</ref>. A DVS camera <ref type="bibr" target="#b22">[23]</ref> is presented with the 10 digits from 0 to 9 and the 26 letters from the roman alphabet (see Fig. <ref type="figure" target="#fig_10">8</ref>) printed on a barrel which was rotated at 40rpm. The goal is to be able to classify correctly these 36 objects. This dataset consists of 2 presentations for each of the 36 patterns.</p><p>We use the same hierarchical system described previously with the same parameters as in the previous section.</p><p>The camera is feeding data into the first layer and the third layer's output is used by the classifier to recognize the letters and digits. Only the objects F, V, O, 8 and B are used to train the model. Each layer is trained sequentially. The trained features are available in the supplemental material. Then, for each digit or character, we use one example to train the classifier. Some examples of the classifier histograms are provided in Fig. <ref type="figure" target="#fig_11">9</ref>. All the signatures are also available in the supplemental material. One testing presentation is then used The system is presented with nine different presentations of each learnt pattern. Each line presents data related to a particular trained pattern. Each section in between dashed vertical lines corresponds to the presentation of a test stimulus. Histograms show normalized distances obtained during the experiment so that the recognized objects is the smallest bar in each column (marked with a star). White bars code for standard distance, grey bars for normalized distance and black bars for distance. These three distances lead to respective performances of 94%, 100% and 97%. for each object to test the classifier. The results can be seen Fig. <ref type="figure" target="#fig_12">10</ref>. Each row along the vertical axis shows results for a different class during testing. Each section between dashed lines shows histogram distances for one character presented to the sensor. The different bars encode the histogram distances where white, gray, and black bars code for the standard, normalized, and Bhattacharyya distances respectively. The recognized class is the one with the smallest bar in each column, and is marked with a star. Each character is presented once to the sensor in the order we gave to the classes so that perfect results correspond to stars only on the main diagonal. In this experiment, all distances lead to a recognition performance of 100%. We ran a cross validation test by randomly choosing for each pattern which presentation would be used for learning (both the model and classifier), and the other is then used for testing. Every trial amongst the hundred we ran gave a recognition accuracy of 100%.</p><p>This experiment is run on a dataset composed of objects with very distinct spatial characteristics. Because of that, it is the best one to try to interpret the features learnt by the architecture. Fig. <ref type="figure" target="#fig_13">11</ref> presents the activation of all three layers' features for three different characters. We can observe on panels (a), (b) and (c) how the information available in different layers allow us to discriminate between three similar characters: E, B and 8. Each column shows the response of a given feature (its associated surface representation is shown at the top) when the characters are presented (each line), with the last column showing all these data at once. We can see the difference in activation of the features corresponding to the differences in the input stimuli.</p><p>Panel (d) shows the accumulated feature activations for a set of objects used in this task. We can clearly see that the information encoded by each feature is becoming more and more abstract as we go from one layer to the next. In the second layer, features seem to respond to particular orientation of edges constituted of either ON or OFF events. In the third layer however, it seems that these features were pooled in order to recognize the line drawing the characters with features being tuned to its curvature. We can also see that the feature activations are very reproductible from one character to another containing the same inner shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face recognition task</head><p>The results obtained in the previous sections encouraged us to run the proposed method on more complex data. For our last experiment, we use a dataset consisting of the faces of seven subjects. Each subject is moving his or her head to follow the same visual stimulus tracing out a square path on a computer monitor (see Fig. <ref type="figure" target="#fig_15">12</ref>). The data is acquired by an ATIS camera <ref type="bibr" target="#b21">[22]</ref>. Each subject has 20 presentations in the dataset of which one is used for training and the others for testing.</p><p>We again use the same hierarchical system described previously with the following parameters: For each letter and digit the trained histogram used as a signature by the classifier is shown. The snapshot shows an accumulation of events from the sensor (White dots for ON events and black dots for OFF events). The histograms present the signatures: X axis is the index of the feature shown in Fig. <ref type="figure">??,</ref><ref type="figure">Y</ref> axis is the number of activations of the feature during the stimulus presentation. The signatures of all the letters &amp; digits are presented in the supplemental material.</p><formula xml:id="formula_17">• R 1 = 4, • τ 1 = 50 ms,</formula><p>• N 1 = 8.</p><formula xml:id="formula_18">• K R = 2, • K τ = 5, • K N = 2.</formula><p>Because the faces are bigger and more complex objects, we use bigger receptive fields to define the time-surfaces and we set the layers to cluster twice as many features as in the previous experiments. These parameters lead to recognition performances of 37%, 78% and 79% when using the standard, normalized and Bhattacharyya distances respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>In this paper we have described a hierarchical architecture for object recognition using a new type of feature relying on time-surfaces. These time-surfaces use the high temporal resolution of event-driven time-based vision sensors to associate a descriptor to every event based on its relative timings to recent activity in its spatial neighborhood. The model then clusters this space to extract features. Successive layers perform this operation again and again, incorporating larger and larger spatial and temporal scales in the process. This allows for features of these successive layers to acquire more information, from bigger spatial receptive fields and from longer timescales.</p><p>Every layer also automatically learns its own features from its predecessor's output, removing the need for supervision. The process of training the model is thus completely unsupervised. Supervision only takes place when training the classifier, which inherently requires knowledge of the object class.</p><p>Other object recognition methods such as HMAX <ref type="bibr" target="#b32">[33]</ref> or HFirst <ref type="bibr" target="#b13">[14]</ref> extract orientations as a first step using oriented Gabor filters, thus only looking at the spatial repartition of the input data or events to build features. HFirst does make use of time in computation, but it uses time to encode signal strength of spatial features rather than capturing scene dynamics. In contrast, time-surfaces use both spatial and temporal information to build features which are then not only spatial, but spatio-temporal features. Time-surfaces are, by design, encoding spatial information such as shapes and temporal information such as motion in the feature space. This makes them interesting features for recognition in dynamic environments. More than recognizing moving objects, these features should be able to extract the intrinsic relative inner movements of objects to help the recognition process. In the architecture described in this work, the extracted features are still very dependent from the direction and speed of motion of the objects we want to recognize. Thus for an object to be recognized independently from its motion, a lot of different movement patterns have to be presented to the system during learning. This could be overcome using basis function decomposition similarly to what has been introduced in <ref type="bibr" target="#b33">[34]</ref>. This concept will be studied in an extension of this work.</p><p>Masquelier et al. <ref type="bibr" target="#b34">[35]</ref> used STDP to learn features as event patterns. But STDP in its basic and most used form can only extract the co-activation of different input neurons and thus cannot extract the exact order of activation. Because time-surfaces encodes relative timings, they can distinguish between different orders of activation which is important for real spatio-temporal features. Moreover because time-surfaces are generated around every event, we also do not need to stabilize or track objects for their recognition and features will be recognized whatever their position in the field of view of the sensor.</p><p>The letters &amp; digits experiment demonstrates the generalization strength of the feature extraction process. To recognize the 36 characters constituting the dataset, we only used a subset of 5 characters representative of the whole. When the different layers were learning features, they were only shown the characters F, V, O, 8 and B. Then, signatures were obtained for all of the 36 characters of the dataset to train the  Examples of outputs of the three layers for a set of recognized objects. We can see how the information becomes more abstract when increasing layers, going from orientation of contours of a given polarity (ON or OFF events) to contours of a given curvature.</p><p>classifier stage. This still leads to a recognition performance of 100% which is showing us than the system can recognize objects without the need for its features to be specifically trained on those objects.</p><p>One can note that the parameters of the system still had to be tuned differently for the different experiments presented in the paper. Coping with the varying nature of the input stimulus can be achieved in different ways. In the architecture  The database is constituted of 7 faces moving their gaze by following a dot in a square movement. The snapshots show the obtained stimulus with successive positions in time of the faces (white dots represent ON events and black dots represent OFF events; snapshots are taken every 350 ms.). The last column presents the signature learnt by the classifier for each face using the 32 features of the third layer of the system: X axis is the index of the feature shown in the supplemental material, Y axis is the number of activations of the feature during the stimulus presentation (in logarithmic scaling).</p><p>presented in this work, we use only one classifier fed by the last layer of the system, thus only operating recognition on the higher level features which are also the slowest (because they integrate information over the longest period of time). It is possible to add intermediate classifiers at the output of each layer in order to get different classifications results, each operating on features built over longer and longer period of time. Thus the first layers' classifier would give the fastest but most inaccurate results whereas the following layers would give more and more reliable results with a larger and larger detection time. Increasing the numbers of layers would then allow us to get classification results computed from different timescales without further tuning of the system. Each layer could also compute time-surfaces with different time constants for their exponential kernels by adding an internal competition system (such as lateral inhibition or a winner-take-all circuit).</p><p>Recognizing a time-surface essentially consists of performing coincidence detection on input event arriving in a specific spatial and temporal pattern. For instance, this could be implemented by using leaky integrate and fire neurons and delay lines. Thus, the model described in this paper can be seen as a Spiking Neural Network and could be implemented on neuromorphic hardware for neural simulation. Moreover, every pixel is considering its own receptive field to build its associated time-surface. This can allow most of the processing to be parallelized to be implemented on platforms such as FPGAs or new, highly parallel, computer architectures such as SpiNNaker <ref type="bibr" target="#b35">[36]</ref>.</p><p>Several improvements can also be considered to improve the architecture proposed in this paper and are considered as possible futur work. In the current method, every incoming event generates an output event. But events both close in time and space generate similar time-surfaces. Inhibition and Winner-take-all mechanisms could be added to reduce the amount of redundancy introduced by these similarities as have been done in <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Object recognition is also undertaken after the whole object has finished passing by. We chose to operate that way because of the final classifier that we used, which we wanted to keep as simple as possible to show that information from the network are valuable compared to other architectures. If another online classifier is used, such as an ELM <ref type="bibr" target="#b37">[38]</ref> then we could get intermediate classification results as time-surface are arriving.</p><p>One can also note that even if our method is inspired by biology, the timescales that increase geometrically across the successive layers of the system are fairly large in the latest layers. This not compatible anymore with biological time constants. Our method is a proof of concept of a hierarchy of time scales that does not aim at explaining the brain. In principle though, information could be integrated over even longer timescales. We could then imagine large networks such as the ones used currently by Google <ref type="bibr" target="#b38">[39]</ref> where the last layer could code for integration over months. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have presented a hierarchical recognition architecture using a new way of representing features in the spatio-temporal output of asynchronous change detection vision sensors. These features are using relative timings of events, enabled by the high temporal precision of these sensors' output, to give contextual information to events, which we have called timesurfaces. The proposed architecture matches or improves the current state of the art on two previously published recognition tasks and results for a third, more difficult task have been presented. This comparison is summed-up in Table <ref type="table">I</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 HOTS:</head><label>1</label><figDesc>A Hierarchy Of event-based Time-Surfaces for pattern recognition X. Lagorce, G. Orchard, F. Galluppi, B. E. Shi, R. Benosman</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of event-based encoding of visual signals. (a) ATIS camera [22]. (b) Log of the luminance of a pixel located at [x, y] T . (c) Asynchronous temporal contrast events generated with respect to the predefined threshold ∆ log I.</figDesc><graphic coords="2,303.13,47.74,96.25,128.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>shown in Fig. 2(d) where intensity is coding for time values: bright pixels show recent activity whereas dark pixels received events further away in the past (only time values corresponding to OFF events are represented in the figure for clarity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Example of some time-surfaces for simple movements of objects. First column shows a representation of the stimulus. The second column shows corresponding data from the ATIS sensor where white dots are ON events and black dots are OFF events. The third column shows the time-surface obtained from these events and computed for the event located in the center of the circle in the second column: the first, positive, half is obtained from the ON events and the second, negative, half is obtained from the OFF events. (a) A horizontal bar moving downwards. (b) A vertical bar moving rightward. (c) Corner moving to the top-right.</figDesc><graphic coords="4,120.14,179.47,57.85,57.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>Online clustering of time-surfaces Ensure: N cluster centers C n , n ∈ 1, N Use the first N events' time-surfaces as initial values for C n , n ∈ 1, N Initialize p n ← 1, n ∈ 1, N for every incoming event ev i do Compute time-surface S i Find closest cluster center C k α ← 0.01/(1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. View of the proposed hierarchical model. From left to right, a moving digit (a) is presented to the ATIS camera (b) which produces ON and OFF events (c) which are fed into Layer 1. The events are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Flipped cards experiment: Pattern database.The database for this experiment consists of the four suits (spades, hearts, clubs, and diamonds) found in a card deck. They are captured by a sensitive DVS sensor as the cards are flipped in front of it (white dots represent ON events and black dots OFF events).</figDesc><graphic coords="7,48.96,56.07,251.07,59.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Flipped cards experiment: Patterns' signatures.Histograms of feature activation numbers for the four suits moving in front of the camera. X axis is the index of the feature shown in the supplemental material, Y axis is the number of activations of the feature during the stimulus presentation. Each column corresponds to one suit. The snapshots show how the pips evolve during one particular presentation (each snapshot is taken at a regular time interval). Each pattern outputs a different signature that allows its recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Flipped cards experiment: Distance measurements between learning and testing presentations of patterns.The system is presented with nine different presentations of each learnt pattern. Each line presents data related to a particular trained pattern. Each section in between dashed vertical lines corresponds to the presentation of a test stimulus. Histograms show normalized distances obtained during the experiment so that the recognized objects is the smallest bar in each column (marked with a star). White bars code for standard distance, grey bars for normalized distance and black bars for distance. These three distances lead to respective performances of 94%, 100% and 97%.</figDesc><graphic coords="8,48.96,308.08,251.06,102.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Letters &amp; Digits experiment: Pattern database.The database for this experiment consists of the 26 letters of the roman alphabet and the digits 0 to 9. They are captured by a DVS sensor as the characters are moving in front of it (white dots represent ON events and black dots OFF events).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Letters &amp; Digits experiment: Pattern signatures for some of the input classes.For each letter and digit the trained histogram used as a signature by the classifier is shown. The snapshot shows an accumulation of events from the sensor (White dots for ON events and black dots for OFF events). The histograms present the signatures: X axis is the index of the feature shown in Fig.??, Yaxis is the number of activations of the feature during the stimulus presentation. The signatures of all the letters &amp; digits are presented in the supplemental material.</figDesc><graphic coords="9,362.03,192.21,100.02,57.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Letters &amp; Digits experiment: Distance measurements between learning and testing presentations of patterns. one presentation of each learnt pattern is shown to the system. Each line presents data related to a particular trained pattern. Each section in between dashed vertical lines corresponds to the presentation of a test stimulus. Histograms show normalized distances obtained during the experiment so that the recognized objects is the smallest bar in each column (marked with a star). White bars code for standard distance, grey bars for normalized distance and black bars for Bhattacharyya distance. These three distances all lead to a 100% recognition rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Letters &amp; Digits experiment: Activation of features in the different layers. (a), (b) and (c) activation of the different features extracted by each of the three layers for some chosen characters: E, B and 8. The last column shows the superposition of the different features (each color corresponds to a particular feature). The other columns show the independent activation of the different features with their associated surface representation. Each of the three line of activations presents a different character.(a) shows the output of the first layer, (b) of the second and (c) of the third. Differences in the information offered by each layer to differentiate between these similar classes can be observed. (d)Examples of outputs of the three layers for a set of recognized objects. We can see how the information becomes more abstract when increasing layers, going from orientation of contours of a given polarity (ON or OFF events) to contours of a given curvature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Faces recognition experiment: Pattern database and signatures.The database is constituted of 7 faces moving their gaze by following a dot in a square movement. The snapshots show the obtained stimulus with successive positions in time of the faces (white dots represent ON events and black dots represent OFF events; snapshots are taken every 350 ms.). The last column presents the signature learnt by the classifier for each face using the 32 features of the third layer of the system: X axis is the index of the feature shown in the supplemental material, Y axis is the number of activations of the feature during the stimulus presentation (in logarithmic scaling).</figDesc><graphic coords="12,476.89,386.06,86.16,52.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Ryad B. Benosman is a full Professor at Université Pierre et Marie Curie, Paris, France. He leads the Neuromorphic Vision and Natural Computation group. He focuses mainly on neuromorphic engineering, visual computation and sensing. He is a pioneer of omnidirectional vision systems, complex perception systems, variant scale sensors, and noncentral sensors. His current research interests include the understanding of the computation operated by the visual system with the goal to establish the link between computational and biological vision. Prof Benosman is also invested in applying neuromorphic computation to retina prosthetics and is a cofounder of the startup Pixium Vision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Face recognition experiment: Recognition results. Subfigure show the recognition results for the three different distances used. (a) Standard distance: 37% accuracy. (b) Normalized distance: 78% accuracy. (c) Bhattacharyya distance: 79% accuracy. The system is presented with 19 testing presentations of each learnt class. These 19 presentations are merged into the columns and the numbers are indicating how many matches are obtained for the different learnt patterns. Perferct results would fill the diagonal (gray background cells) with values 19, numbers in other cells correspond to classification errors.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was performed in the frame of the LABEX LIFE-SENSES [ANR-10-LABX-65] and was supported by French state funds managed by the ANR within the Investissements d'Avenir programme [ANR-11-IDEX-0004-02]. XL has been supported by the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n • 604102 (HBP). Franco-Singaporean collaboration on this project was supported by the Merlion Programme of the Institut Franais de Singapour, under administrative supervision of the French Ministry of Foreign Affairs and the National University of Singapore. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressly or implied, of the French Ministry of Foreign Affairs. The authors would also like to acknowledge discussion at the NSF Telluride Neuromorphic Cognition Engineering Workshop.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Xavier Lagorce received the B.Sc. degree, the Agrégation degree in electrical engineering, and the M.Sc. degree in advanced systems and robotics from the École Normale Supérieure de Cachan, Cachan, France, in 2008, 2010 and 2011 respectively. He is currently pursuing the Ph.D. degree in neurally inspired hardware systems and sensors with the Vision Institute, Paris. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Garrick</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:VISI.0000029664.99615.94</idno>
		<ptr target="http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=726791" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retinomorphic event-based vision sensors: Bioinspired cameras with spiking output</title>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1470" to="1484" />
			<date type="published" when="2014-10">Oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Bhaskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konstantinides</surname></persName>
		</author>
		<title level="m">Image and Video Compression Standards: Algorithms and Architectures</title>
		<meeting><address><addrLine>Norwell, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">High dynamic range imaging: acquisition, display, and image-based lighting</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eye smarter than scientists believed: neural computations in circuits of the retina</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gollisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="164" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What can neuromorphic event-driven precise timing add to spike-based pattern recognition?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akolkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Clady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Marre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panzeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neuromorphic electronic systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1629" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The silicon retina</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahowald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Point-to-point connectivity between neuromorphic chips using address-events</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="416" to="434" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CAVIAR: a 45k neuron, 5M synapse, 12G connects/s AER hardware sensory-processinglearning-actuating system for high-speed visual object recognition and tracking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paz-Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Camunas-Mesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivas-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hafliger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jimenez-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Civit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ballcels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Acosta-Jimenez</surname></persName>
		</author>
		<author>
			<persName><surname>Linares-Barranco</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/19635693" />
	</analytic>
	<monogr>
		<title level="j">IEEE Neural Networks Council</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1417" to="1438" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards a Cortical Prosthesis: Implementing A Spike-Based HMAX Model of Visual Object Recognition in Silico</title>
		<author>
			<persName><forename type="first">F</forename><surname>Folowosele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Etienne-Cummings</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6133300" />
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="516" to="525" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mapping from framedriven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing-application to feedforward ConvNets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Pérez-Carrasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2706" to="2719" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hfirst: A temporal approach to object recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Etienne-Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thakor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Realtime classification and sensor fusion with a spiking deep belief network</title>
		<author>
			<persName><forename type="first">P</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">178</biblScope>
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spiking deep convolutional neural networks for energy-efficient object recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="66" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient feedforward categorization of objects and human postures with address-event image sensors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Akselrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A P</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="302" to="314" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time object recognition and orientation estimation using an event-based camera and cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Thakor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Biomedical Circuits and Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A biologically inspired system for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatiotemporal features for asynchronous event-based data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Clady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Poker-dvs and mnist-dvs. their history, how they were made, and other details</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2015.00481/abstract</idno>
		<ptr target="http://www.frontiersin.org/neuromorphicengineering/10.3389/fnins.2015.00481/abstract" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">481</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A qvga 143 db dynamic range frame-free pwm image sensor with lossless pixel-level video compression and time-domain cds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wohlgenannt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2011-01">jan. 2011</date>
		</imprint>
	</monogr>
	<note>Solid-State Circuits</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A 128x128 120db 15us latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008-02">February 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A 3.6 us Latency Asynchronous Frame-Free Event-Driven Dynamic-Vision-Sensor</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lenero-Bardallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5746543" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1443" to="1455" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Contrast Sensitive Silicon Retina with Reciprocal Synapses</title>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">G</forename><surname>Andreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwabena</forename><forename type="middle">A</forename><surname>Boahen</surname></persName>
		</author>
		<ptr target="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.8246" />
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="764" to="772" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analog Integrated 2-{D} Optical Flow Sensor</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Stocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analog Integr. Circuits Signal Process</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="138" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A 128 x 128 1.5% contrast sensitivity 0.9% fpn 3 µs latency 4 mw asynchronous frame-free dynamic vision sensor using transimpedance preamplifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="827" to="838" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Solid-State Circuits</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An asynchronous timebased image sensor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wohlgenannt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCAS 2008. IEEE International Symposium on</title>
		<imprint>
			<date type="published" when="2008-05">2008. may 2008</date>
			<biblScope unit="page" from="2130" to="2133" />
		</imprint>
	</monogr>
	<note>Circuits and Systems</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic coding of signed quantities in cortical feedback circuits</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jehee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">254</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The &quot;echo state&quot; approach to analysing and training recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GMD Report</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>GMD -German National Research Institute for Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A novel connectionist system for improved unconstrained handwriting recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On a measure of divergence between two statistical populations defined by probability distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Calcutta Math. Soc</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="411" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6583</biblScope>
			<biblScope unit="page" from="607" to="609" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features through spike timing dependent plasticity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">e31</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The SpiNNaker project</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plana</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6750072" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="652" to="665" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Event-based features for robotic vision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Synthesis of neural networks for spatio-temporal spike pattern recognition and processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stiefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Buskila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.7118</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
