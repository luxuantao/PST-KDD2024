<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable? This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks. We show that, for certain classes of problems, adversarial examples are inescapable. Using experiments, we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier's robustness against adversarial examples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A number of adversarial attacks on neural networks have been recently proposed. To counter these attacks, a number of authors have proposed a range of defenses. However, these defenses are often quickly broken by new and revised attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable?</p><p>In this paper, we identify a broad class of problems for which adversarial examples cannot be avoided. We also derive fundamental limits on the susceptibility of a classifier to adversarial attacks that depend on properties of the data distribution as well as the dimensionality of the dataset.</p><p>Adversarial examples occur when a small perturbation to an image changes its class label. There are different ways of measuring what it means for a perturbation to be "small"; as such, our analysis considers a range of different norms. While the ∞ -norm is commonly used, adversarial examples can be crafted in any p -norm (see Figure <ref type="figure">1</ref>). We will see that the choice of norm can have a dramatic effect on the strength of theoretical guarantees for the existence of adversarial examples. Our analysis also extends to the 0 -norm, which yields "sparse" adversarial examples that only perturb a small subset of image pixels (Figure <ref type="figure" target="#fig_7">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original egyptian cat (28%)</head><p>2 -norm=10 traffic light (97%) ∞ -norm=0.05 traffic light (96%) 0 -norm=5000 (sparse) traffic light (80%) Figure <ref type="figure">1</ref>: Adversarial examples with different norm constraints formed via the projected gradient method <ref type="bibr" target="#b18">(Madry et al., 2017)</ref> on Resnet50, along with the distance between the base image and the adversarial example, and the top class label.</p><p>As a simple example result, consider a classification problem with n-dimensional images with pixels scaled between 0 and 1 (in this case images live inside the unit hypercube). If the image classes each occupy a fraction of the cube greater than 1 2 exp(−π 2 ), then images exist that are susceptible to adversarial perturbations of 2 -norm at most . Note that = 10 was used in Figure <ref type="figure">1</ref>, and larger values are typical for larger images.</p><p>Finally, in Section 8, we explore the causes of adversarial susceptibility in real datasets, and the effect of dimensionality. We present an example image class for which there is no fundamental link between dimensionality and robustness, and argue that the data distribution, and not dimensionality, is the primary cause of adversarial susceptibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">BACKGROUND: A BRIEF HISTORY OF ADVERSARIAL EXAMPLES</head><p>Adversarial examples, first demonstrated in <ref type="bibr" target="#b34">Szegedy et al. (2013)</ref> and <ref type="bibr" target="#b3">Biggio et al. (2013)</ref>, change the label of an image using small and often imperceptible perturbations to its pixels. A number of defenses have been proposed to harden networks against attacks, but historically, these defenses have been quickly broken.</p><p>Adversarial training, one of the earliest defenses, successfully thwarted the fast gradient sign method (FGSM) <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref>, one of the earliest and simplest attacks. However, adversarial training with FGSM examples was quickly shown to be vulnerable to more sophisticated multi-stage attacks <ref type="bibr" target="#b16">(Kurakin et al., 2016;</ref><ref type="bibr" target="#b37">Tramèr et al., 2017a)</ref>. More sophisticated defenses that rely on network distillation <ref type="bibr" target="#b25">(Papernot et al., 2016b)</ref> and specialized activation functions <ref type="bibr" target="#b42">(Zantedeschi et al., 2017)</ref> were also toppled by strong attacks <ref type="bibr" target="#b24">(Papernot et al., 2016a;</ref><ref type="bibr" target="#b38">Tramèr et al., 2017b;</ref><ref type="bibr" target="#b6">Carlini &amp; Wagner, 2016;</ref><ref type="bibr">2017a)</ref>.</p><p>The ongoing vulnerability of classifiers was highlighted in recent work by <ref type="bibr" target="#b2">Athalye et al. (2018)</ref> and <ref type="bibr" target="#b1">Athalye &amp; Sutskever (2017)</ref> that broke an entire suite of defenses presented in ICLR 2018 including thermometer encoding <ref type="bibr" target="#b5">(Buckman et al., 2018)</ref>, detection using local intrinsic dimensionality <ref type="bibr" target="#b17">(Ma et al., 2018)</ref>, input transformations such as compression and image quilting <ref type="bibr" target="#b14">(Guo et al., 2017)</ref>, stochastic activation pruning <ref type="bibr" target="#b10">(Dhillon et al., 2018)</ref>, adding randomization at inference time <ref type="bibr" target="#b40">(Xie et al., 2017)</ref>, enhancing the confidence of image labels <ref type="bibr" target="#b31">(Song et al., 2017)</ref>, and using a generative model as a defense <ref type="bibr">(Samangouei et al., 2018)</ref>.</p><p>Rather than hardening classifiers to attacks, some authors have proposed sanitizing datasets to remove adversarial perturbations before classification. Approaches based on auto-encoders <ref type="bibr" target="#b21">(Meng &amp; Chen, 2017)</ref> and GANs <ref type="bibr" target="#b28">(Shen et al., 2017)</ref> were broken using optimization-based attacks <ref type="bibr" target="#b8">(Carlini &amp; Wagner, 2017b;</ref><ref type="bibr">a)</ref>. A number of "certifiable" defense mechanisms have been developed for certain classifiers. <ref type="bibr" target="#b26">Raghunathan et al. (2018)</ref> harden a two-layer classifier using semidefinite programming, and <ref type="bibr" target="#b30">Sinha et al. (2018)</ref> propose a convex duality-based approach to adversarial training that works on sufficiently small adversarial perturbations with a quadratic adversarial loss. <ref type="bibr" target="#b15">Kolter &amp; Wong (2017)</ref> consider training a robust classifier using the convex outer adversarial polytope. All of these methods only consider robustness of the classifier on the training set, and robustness properties often fail to generalize reliably to test examples.</p><p>One place where researchers have enjoyed success is at training classifiers on low-dimensional datasets like MNIST <ref type="bibr" target="#b18">(Madry et al., 2017;</ref><ref type="bibr" target="#b30">Sinha et al., 2018)</ref>. The robustness achieved on more complicated datasets such original ∞ -norm 0 -norm (sparse) sparse perturbation</p><p>Figure <ref type="figure" target="#fig_7">2</ref>: Sparse adversarial examples perturb a small subset of pixels and can hide adversarial "fuzz" inside highfrequency image regions. The original image (left) is classified as an "ox." Under ∞-norm perturbations, it is classified as "traffic light", but the perturbations visibly distort smooth regions of the image (the sky). These effects are hidden in the grass using 0-norm (sparse) perturbations limited to a small subset of pixels.</p><p>as CIFAR-10 and ImageNet are nowhere near that of MNIST, which leads some researchers to speculate that adversarial defense is fundamentally harder in higher dimensions -an issue we address in Section 8.</p><p>This paper uses well-known results from high-dimensional geometry, specifically isoperimetric inequalities, to provide bounds on the robustness of classifiers. Several other authors have investigated adversarial susceptibility through the lens of geometry. <ref type="bibr" target="#b11">Fawzi et al. (2018)</ref> study adversarial susceptibility of datasets under the assumption that they are produced by a generative model that maps random Gaussian vectors onto images. <ref type="bibr" target="#b12">Gilmer et al. (2018)</ref> do a detailed case study, including empirical and theoretical results, of classifiers for a synthetic dataset that lies on two concentric spheres. <ref type="bibr" target="#b29">Simon-Gabriel et al. (2018)</ref> show that the Lipschitz constant of untrained networks with random weights gets large in high dimensions. Shortly after the original appearance of our work, <ref type="bibr" target="#b19">Mahloujifar et al. (2018)</ref> presented a study of adversarial susceptibility that included both evasion and poisoning attacks. Our work is distinct in that it studies adversarial robustness for arbitrary data distributions, and also that it rigorously looks at the effect of dimensionality on robustness limits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">NOTATION</head><p>We use [0, 1] n to denote the unit hypercube in n dimensions, and vol(A) to denote the volume (i.e., ndimensional Lebesgue measure) of a subset A ⊂ [0, 1] n . We use</p><formula xml:id="formula_0">S n−1 = {x ∈ R n | x 2 = 1}</formula><p>to denote the unit sphere embedded in R n , and s n−1 to denote its surface area. The size of a subset A ∈ S n−1 can be quantified by its (n − 1 dimensional) measure µ[A], which is simply the surface area the set covers.</p><p>Because the surface area of the unit sphere varies greatly with n, it is much easier in practice to work with the normalized measure, which we denote µ 1 [A] = µ[A]/s n−1 . This normalized measure has the property that µ 1 [S n−1 ] = 1, and so we can interpret µ 1 [A] as the probability of a uniform random point from the sphere lying in A. When working with points on a sphere, we often use geodesic distance, which is always somewhat larger than (but comparable to) the Euclidean distance. In the cube, we measure distance between points using p -norms, which are denoted</p><formula xml:id="formula_1">z p = i |z i | p 1/p if p &gt; 0, and z 0 = card{z i | z i = 0}.</formula><p>Note that • p is not truly a norm for p &lt; 1, but rather a semi-norm. Such metrics are still commonly used, particularly the " 0 -norm" which counts the number of non-zero entries in a vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM SETUP</head><p>We consider the problem of classifying data points that lie in a space Ω (either a sphere or a hypercube) into m different object classes. The m object classes are defined by probability density functions {ρ c } m c=1 , where ρ c : Ω → R. A "random" point from class c is a random variable with density ρ c . We assume ρ c to be bounded (i.e., we don't allow delta functions or other generalized functions), and denote its upper bound by</p><formula xml:id="formula_2">U c = sup x ρ c (x).</formula><p>We also consider a "classifier" function C : Ω → {1, 2, . . . , m} that partitions Ω into disjoint measurable subsets, one for each class label. The classifier we consider is discrete valued -it provides a label for each data point but not a confidence level.</p><p>With this setup, we can give a formal definition of an adversarial example. Definition 1. Consider a point x ∈ Ω drawn from class c, a scalar &gt; 0, and a metric d. We say that x admits an -adversarial example in the metric d if there exists a point x ∈ Ω with C(x) = c, and d(x, x) ≤ .</p><p>In plain words, a point has an -adversarial example if we can sneak it into a different class by moving it at most units in the distance d.</p><p>We consider adversarial examples with respect to different p -norm metrics. These metrics are written d p (x, x) = x − x p . A common choice is p = ∞, which limits the absolute change that can be made to any one pixel. However, 2 -norm and 1 -norm adversarial examples are also used, as it is frequently easier to create adversarial examples in these less restrictive metrics. We also consider sparse adversarial examples in which only a small subset of pixels are manipulated. This corresponds to the metric d 0 , in which case the constraint x − x 0 ≤ means that an adversarial example was crafted by changing at most pixels, and leaving the others alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE SIMPLE CASE: ADVERSARIAL EXAMPLES ON THE UNIT SPHERE</head><p>We begin by looking at the case of classifiers for data on the sphere. While this data model may be less relevant than the other models studied below, it provides a straightforward case where results can be proven using simple, geometric lemmas. The more realistic case of images with pixels in [0, 1] will be studied in Section 4.</p><p>The idea is to show that, provided a class of data points takes up enough space, nearly every point in the class lies close to the class boundary. To show this, we begin with a simple definition. Definition 2. The -expansion of a subset A ⊂ Ω with respect to distance metric d, denoted A( , d), contains all points that are at most units away from A. To be precise</p><formula xml:id="formula_3">A( , d) = {x ∈ Ω | d(x, y) ≤ for some y ∈ A}.</formula><p>We sometimes simply write A( ) when the distance metric is clear from context.</p><p>Our result provides bounds on the probability of adversarial examples that are independent of the shape of the class boundary. This independence is a simple consequence of an isoperimetric inequality. The classical isoperimetric inequality states that, of all closed surfaces that enclose a unit volume, the sphere has the smallest surface area. This simple fact is intuitive but famously difficult to prove. For a historical review of the isoperimetric inequality and its variants, see <ref type="bibr" target="#b23">Osserman et al. (1978)</ref>. We will use a special variant of the isoperimetric inequality first proved by <ref type="bibr">Lévy &amp; Pellegrino (1951)</ref> and simplified by <ref type="bibr" target="#b35">Talagrand (1995)</ref>.</p><p>Lemma 1 (Isoperimetric inequality). Consider a subset of the sphere A ⊂ S n−1 ⊂ R n with normalized measure µ 1 (A) ≥ 1/2. When using the geodesic metric, the -expansion A( ) is at least as large as the -expansion of a half sphere.</p><p>The classical isoperimetric inequality is a simple geometric statement, and frequently appears without absolute bounds on the size of the -expansion of a half-sphere, or with bounds that involve unspecified constants <ref type="bibr" target="#b39">(Vershynin, 2017)</ref>. A tight bound derived by Milman &amp; Schechtman ( <ref type="formula">1986</ref>) is given below. The asymptotic blow-up of the -expansion of a half sphere predicted by this bound is shown in Figure <ref type="figure" target="#fig_0">3</ref>. Lemma 2 ( -expansion of half sphere). The geodesic -expansion of a half sphere has normalized measure at least</p><formula xml:id="formula_4">1 − π 8 1 2 exp − n − 1 2 2 .</formula><p>Lemmas 1 and 2 together can be taken to mean that, if a set is not too small, then in high dimensions almost all points on the sphere are reachable within a short jump from that set. These lemmas have immediate implications for adversarial examples, which are formed by mapping one class into another using small perturbations. Despite its complex appearance, the result below is a consequence of the (relatively simple) isoperimetric inequality.</p><p>Theorem 1 (Existence of Adversarial Examples). Consider a classification problem with m object classes, each distributed over the unit sphere S n−1 ⊂ R n with density functions {ρ c } m c=1 . Choose a classifier function C : S n−1 → {1, 2, . . . , m} that partitions the sphere into disjoint measurable subsets. Define the following scalar constants:</p><p>• Let V c denote the magnitude of the supremum of ρ c relative to the uniform density. This can be written Choose some class c with f c ≤ 1 2 . Sample a random data point x from ρ c . Then with probability at least</p><formula xml:id="formula_5">V c := s n−1 • sup x ρ c (x). • Let f c = µ 1 {x|C(x) = c}</formula><formula xml:id="formula_6">1 − V c π 8 1 2 exp − n − 1 2 2 (1)</formula><p>one of the following conditions holds: 1.</p><p>x is misclassified by C, or 2.</p><p>x admits an -adversarial example in the geodesic distance.</p><p>Proof. Choose a class c with f c ≤ 1 2 . Let R = {x|C(x) = c} denote the region of the sphere labeled as class c by C, and let R be its complement. R( ) is the -expansion of R in the geodesic metric. Because R covers at least half the sphere, the isoperimetric inequality (Lemma 1) tells us that the epsilon expansion is at least as great as the epsilon expansion of a half sphere. We thus have</p><formula xml:id="formula_7">µ 1 [R( )] ≥ 1 − π 8 1 2 exp − n − 1 2 2 .</formula><p>Now, consider the set S c of "safe" points from class c that are correctly classified and do not admit adversarial perturbations. A point is correctly classified only if it lies inside R, and therefore outside of R. To be safe from adversarial perturbations, a point cannot lie within distance from the class boundary, and so it cannot lie within R( ). It is clear that the set S c of safe points is exactly the complement of R( ). This set has normalized measure</p><formula xml:id="formula_8">µ 1 [S c ] ≤ π 8 1 2 exp − n − 1 2 2 .</formula><p>The probability of a random point lying in S c is bounded above by the normalized supremum of ρ c times the normalized measure µ 1 [S c ]. This product is given by</p><formula xml:id="formula_9">V c π 8 1 2 exp − n − 1 2 2 .</formula><p>We then subtract this probability from 1 to obtain the probability of a point lying outside the safe region, and arrive at equation 1.</p><p>In the above result, we measure the size of adversarial perturbations using the geodesic distance. Most studies of adversarial examples measure the size of perturbation in either the 2 (Euclidean) norm or the ∞ (max) norm, and so it is natural to wonder whether Theorem 1 depends strongly on the distance metric. Fortunately (or, rather unfortunately) it does not.</p><p>It is easily observed that, for any two points x and y on a sphere,</p><formula xml:id="formula_10">d ∞ (x, y) ≤ d 2 (x, y) ≤ d g (x, y),</formula><p>where d ∞ (x, y), d 2 (x, y), and d g (x, y) denote the l ∞ , Euclidean, and geodesic distance, respectively. From this, we see that Theorem 1 is actually fairly conservative; any -adversarial example in the geodesic metric would also be adversarial in the other two metrics, and the bound in Theorem 1 holds regardless of which of the three metrics we choose (although different values of will be appropriate depending on the norm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WHAT ABOUT THE UNIT CUBE?</head><p>The above result about the sphere is simple and easy to prove using classical results. However, real world images do not lie on the sphere. In a more typical situation, images will be scaled so that their pixels lie in [0, 1], and data lies inside a high-dimensional hypercube (but, unlike the sphere, data is not confined to its surface). The proof of Theorem 1 makes extensive use of properties that are exclusive to the sphere, and is not applicable to this more realistic setting. Are there still problem classes on the cube where adversarial examples are inevitable?</p><p>This question is complicated by the fact that geometric isoperimetric inequalities do not exist for the cube, as the shapes that achieve minimal -expansion (if they exist) depend on the volume they enclose and the choice of <ref type="bibr" target="#b27">(Ros, 2001)</ref>. Fortunately, researchers have been able to derive "algebraic" isoperimetric inequalities that provide lower bounds on the size of the -expansion of sets without identifying the shape that achieves this minimum <ref type="bibr" target="#b36">(Talagrand, 1996;</ref><ref type="bibr" target="#b22">Milman &amp; Schechtman, 1986)</ref>. The result below about the unit cube is analogous to Proposition 2.8 in Ledoux ( <ref type="formula">2001</ref>), except with tighter constants. For completeness, a proof (which utilizes methods from Ledoux) is provided in Appendix A.</p><p>Lemma 3 (Isoperimetric inequality on a cube). Consider a measurable subset of the cube A ⊂ [0, 1] n , and a p-norm distance metric</p><formula xml:id="formula_11">d p (x, y) = x − y p for p &gt; 0. Let Φ(z) = (2π) − 1 2 z</formula><p>−∞ e −t 2 /2 dt, and let α be the scalar that satisfies</p><formula xml:id="formula_12">Φ(α) = vol[A]. Then vol[A( , d p )] ≥ Φ α + √ 2πn n 1/p * (2)</formula><p>where p * = min(p, 2). In particular, if vol(A) ≥ 1/2, then we simply have Choose some class c with f c ≤ 1 2 , and select an p -norm with p &gt; 0. Define p * = min(p, 2). Sample a random data point x from the class distribution ρ c . Then with probability at least</p><formula xml:id="formula_13">vol[A( , d p )] ≥ 1 − exp(−πn 1−2/p * 2 ) 2πn 1/2−1/p * .<label>(3)</label></formula><formula xml:id="formula_14">1 − U c exp(−πn 1−2/p * 2 ) 2πn 1/2−1/p * . (<label>4</label></formula><formula xml:id="formula_15">)</formula><p>one of the following conditions holds:</p><p>1.</p><p>x is misclassified by C, or 2. x has an adversarial example x, with x − x p ≤ .</p><p>When adversarial examples are defined in the 2 -norm (or for any p ≥ 2), the bound in equation 4 becomes</p><formula xml:id="formula_16">1 − U c exp(−π 2 )/(2π).<label>(5)</label></formula><p>Provided the class distribution is not overly concentrated, equation 5 guarantees adversarial examples with relatively "small" relative to a typical vector. In n dimensions, the 2 diameter of the cube is √ n, and so it is reasonable to choose = O( √ n) in equation 5. In Figure <ref type="figure">1</ref>, we chose = 10. A similarly strong bound of</p><formula xml:id="formula_17">1 − U c √ n exp(−π 2 /n)/(2π)</formula><p>holds for the case of the 1 -norm, in which case the diameter is n.</p><p>Oddly, equation 4 seems particularly weak when the ∞ norm is used. In this case, the bound on the right side of equation 4 becomes equation 5 just like in the 2 case. However, ∞ adversarial examples are only interesting if we take &lt; 1, in which case equation 5 becomes vacuous for large n. This bound can be tightened up in certain situations. If we prove Theorem 2 using the tighter (but messier) bound of equation 2 instead of equation 3, we can replace equation 4 with</p><formula xml:id="formula_18">1 − U c Φ α + √ 2π for p ≥ 2, where Φ(z) = 1 √ 2π ∞ z e −t 2 /2 dt ≥ 1 √ 2πz e −z 2 /2 (for z &gt; 0), and α = Φ −1 (1 − f c ).</formula><p>For this bound to be meaningful with &lt; 1, we need f c to be relatively small, and to be roughly f c or smaller. This is realistic for some problems; ImageNet has 1000 classes, and so f c &lt; 10 −3 for at least one class.</p><p>Interestingly, under ∞ -norm attacks, guarantees of adversarial examples are much stronger on the sphere (Section 3) than on the cube. One might wonder whether the weakness of Theorem 4 in the ∞ case is fundamental, or if this is a failure of our approach. One can construct examples of sets with ∞ expansions that nearly match the behavior of equation 5, and so our theorems in this case are actually quite tight. It seems to be inherently more difficult to prove the existence of adversarial examples in the cube using the ∞ -norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">WHAT ABOUT SPARSE ADVERSARIAL EXAMPLES?</head><p>A number of papers have looked at sparse adversarial examples, in which a small number of image pixels, in some cases only one <ref type="bibr" target="#b32">(Su et al., 2017)</ref>, are changed to manipulate the class label. To study this case, we would like to investigate adversarial examples under the 0 metric. The 0 distance is defined as</p><formula xml:id="formula_19">d(x, y) = x − y 0 = card{i | x i = y i }.</formula><p>If a point x has an -adversarial example in this norm, then it can be perturbed into a different class by modifying at most pixels (in this case is taken to be a positive integer).</p><p>Theorem 2 is fairly tight for p = 1 or 2. However, the bound becomes quite loose for small p, and in particular it fails completely for the important case of p = 0. For this reason, we present a different bound that is considerably tighter for small p (although slightly looser for large p).</p><p>The case p = 0 was studied by Milman &amp; Schechtman (1986) (Section 6.2) and <ref type="bibr" target="#b20">McDiarmid (1989)</ref>, and later by <ref type="bibr" target="#b35">Talagrand (1995;</ref><ref type="bibr" target="#b36">1996)</ref>. The proof of the following theorem (appendix B) follows the method used in Section 5 of <ref type="bibr" target="#b36">Talagrand (1996)</ref>, with modifications made to extend the proof to arbitrary p. Lemma 4 (Isoperimetric inequality on the cube: small p). Consider a measurable subset of the cube A ⊂ [0, 1] n , and a p-norm distance metric d(x, y) = x − y p for any p ≥ 0. We have</p><formula xml:id="formula_20">vol[A( , d p )] ≥ 1 − exp − 2p /n vol[A]</formula><p>, for p &gt; 0 and (6)</p><formula xml:id="formula_21">vol[A( , d 0 )] ≥ 1 − exp − 2 /n vol[A] , for p = 0.<label>(7)</label></formula><p>Using this result, we can prove a statement analogous to Theorem 2, but for sparse adversarial examples. We present only the case of p = 0, but the generalization to the case of other small p using Lemma 4 is straightforward.</p><p>Theorem 3 (Sparse adversarial examples). Consider the problem setup of Theorem 2. Choose some class c with f c ≤ 1 2 , and sample a random data point x from the class distribution ρ c . Then with probability at least</p><formula xml:id="formula_22">1 − 2U c exp(− 2 /n) (8)</formula><p>one of the following conditions holds: 1. x is misclassified by C, or 2. x can be adversarially perturbed by modifying at most pixels, while still remaining in the unit hypercube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">WHAT IF WE JUST SHOW THAT ADVERSARIAL EXAMPLES EXIST?</head><p>Tighter bounds can be obtained if we only guarantee that adversarial examples exist for some data points in a class, without bounding the probability of this event.</p><p>Theorem 4 (Condition for existence of adversarial examples). Consider the setup of Theorem 2. Choose a class c that occupies a fraction of the cube f c &lt; 1 2 . Pick an p norm and set p * = min(p, 2). Let supp(ρ c ) denote the support of ρ c . Then there is a point x with ρ c (x) &gt; 0 that admits an -adversarial example if</p><formula xml:id="formula_23">vol[supp(ρ c )] ≥      1 2 exp(−π 2 n 1−2/p * ), for p &gt; 0 or exp −2 − n log 2 2 2 /n , for p = 0. (<label>9</label></formula><formula xml:id="formula_24">)</formula><p>The bound for the case p = 0 is valid only if ≥ n log 2/2.</p><p>It is interesting to consider when Theorem 4 produces non-vacuous bounds. When the 2 -norm is used, the bound becomes vol[supp(ρ c )] ≥ exp(−π 2 )/2. The diameter of the cube is √ n, and so the bound becomes active for = √ n. Plugging this in, we see that the bound is active whenever the size of the support satisfies vol[supp(ρ c )] &gt; 1 2e πn . Remarkably, this holds for large n whenever the support of class c is larger than (or contains) a hypercube of side length at least e −π ≈ 0.043. Note, however, that the bound being "active" does not guarantee adversarial examples with a "small" .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION: CAN WE ESCAPE FUNDAMENTAL BOUNDS?</head><p>There are a number of ways to escape the guarantees of adversarial examples made by Theorems 1-4. One potential escape is for the class density functions to take on extremely large values (i.e., exponentially large U c ); the dependence of U c on n is addressed separately in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unbounded density functions and low-dimensional data manifolds</head><p>In practice, image datasets might lie on low-dimensional manifolds within the cube, and the support of these distributions could have measure zero, making the density function infinite (i.e., U c = ∞). The arguments above are still relevant (at least in theory) in this case; we can expand the data manifold by adding a uniform random noise to each image pixel of magnitude at most 1 . The expanded dataset has positive volume. Then, adversarial examples of this expanded dataset can be crafted with perturbations of size 2 . This method of expanding the manifold before crafting adversarial examples is often used in practice. <ref type="bibr" target="#b37">Tramèr et al. (2017a)</ref> proposed adding a small perturbation to step off the image manifold before crafting adversarial examples. This strategy is also used during adversarial training <ref type="bibr" target="#b18">(Madry et al., 2017)</ref>.</p><p>Adding a "don't know" class The analysis above assumes the classifier assigns a label to every point in the cube. If a classifier has the ability to say "I don't know," rather than assign a label to every input, then the region of the cube that is assigned class labels might be very small, and adversarial examples could be escaped even if the other assumptions of Theorem 4 are satisfied. In this case, it would still be easy for the adversary to degrade classifier performance by perturbing images into the "don't know" class.</p><p>Feature squeezing If decreasing the dimensionality of data does not lead to substantially increased values for U c (we see in Section 8 that this is a reasonable assumption) or loss in accuracy (a stronger assumption), measuring data in lower dimensions could increase robustness. This can be done via an auto-encoder <ref type="bibr" target="#b21">(Meng &amp; Chen, 2017;</ref><ref type="bibr" target="#b28">Shen et al., 2017)</ref>, JPEG encoding <ref type="bibr" target="#b9">(Das et al., 2018)</ref>, or quantization <ref type="bibr" target="#b41">(Xu et al., 2017)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational hardness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EXPERIMENTS &amp; EFFECT OF DIMENSIONALITY</head><p>In this section, we discuss the relationship between dimensionality and adversarial robustness, and explore how the predictions made by the theorems above are reflected in experiments.</p><p>It is commonly thought that high-dimensional classifiers are more susceptible to adversarial examples than low-dimensional classifiers. This perception is partially motivated by the observation that classifiers on highresolution image distributions like ImageNet are more easily fooled than low resolution classifiers on MNIST <ref type="bibr" target="#b37">(Tramèr et al., 2017a)</ref>. Indeed, Theorem 2 predicts that high-dimensional classifiers should be much easier to fool than low-dimensional classifiers, assuming the datasets they classify have comparable probability density limits U c . However, this is not a reasonable assumption; we will see below that high dimensional distributions may be more concentrated than their low-dimensional counterparts.</p><p>We study the effects of dimensionality with a thought experiment involving a "big MNIST" image distribution. Given an integer expansion factor b, we can make a big MNIST distribution, denoted b-MNIST, by replacing each pixel in an MNIST image with a b × b array of identical pixels. This expands an original 28 × 28 image into a 28b × 28b image. Figure <ref type="figure" target="#fig_5">4a</ref> shows that, without adversarial training, a classifier on big MNIST is far more susceptible to attacks than a classifier trained on the original MNIST<ref type="foot" target="#foot_0">1</ref> . However, each curve in Figure <ref type="figure" target="#fig_5">4a</ref> only shows the attack susceptibility of one particular classifier. In contrast, Theorems 1-4 describe the fundamental limits of susceptibility for all classifiers. These limits are an inherent property of the data distribution. The theorem below shows that these fundamental limits do not depend in a non-trivial way on the dimensionality of the images in big MNIST, and so the relationship between dimensionality and susceptibility in Figure <ref type="figure" target="#fig_5">4a</ref> results from the weakness of the training process. Theorem 5 predicts that the perturbation needed to fool all 56 × 56 classifiers is twice that needed to fool all 28 × 28 classifiers. This is reasonable since the 2 -norm of a 56 × 56 image is twice that of its 28 × 28 counterpart. Put simply, fooling big MNIST is just as hard/easy as fooling the original MNIST regardless of resolution. This also shows that for big MNIST, as the expansion factor b gets larger and is expanded to match, the concentration bound U c grows at exactly the same rate as the exponential term in equation 2 shrinks, and there is no net effect on fundamental susceptibility. Also note that an analogous result could be based on any image classification problem (we chose MNIST only for illustration), and any p ≥ 0.</p><p>We get a better picture of the fundamental limits of MNIST by considering classifiers that are hardened by adversarial training<ref type="foot" target="#foot_1">2</ref> (Figure <ref type="figure" target="#fig_5">4b</ref>). These curves display several properties of fundamental limits predicted by our theorems. As predicted by Theorem 5, the 112 × 112 classifer curve is twice as wide as the 56 × 56 curve, which in turn is twice as wide as the 28×28 curve. In addition, we see the kind of "phase transition" behavior predicted by Theorem 2, in which the classifier suddenly changes from being highly robust to being highly susceptible as passes a critical threshold. For these reasons, it is reasonable to suspect that the adversarially trained classifiers in Figure <ref type="figure" target="#fig_5">4b</ref> are operating near the fundamental limit predicted by Theorem 2.</p><p>Theorem 5 shows that increased dimensionality does not increase adversarial susceptibility in a fundamental way. But then why are high-dimensional classifiers so easy to fool? To answer this question, we look at the concentration bound U c for object classes. The smallest possible value of U c is 1, which only occurs when images are "spread out" with uniform, uncorrelated pixels. In contrast, adjacent pixels in MNIST (and especially big MNIST) are very highly correlated, and images are concentrated near simple, low-dimensional manifolds, resulting in highly concentrated image classes with large U c . Theory predicts that such highly concentrated datasets can be relatively safe from adversarial examples.</p><p>Under review as a conference paper at ICLR 2019 We can reduce U c and dramatically increase susceptibility by choosing a more "spread out" dataset, like CIFAR-10, in which adjacent pixels are less strongly correlated and images appear to concentrate near complex, higher-dimensional manifolds. We observe the effect of decreasing U c by plotting the susceptibility of a 56 × 56 MNIST classifier against a classifier for CIFAR-10 (Figure <ref type="figure" target="#fig_5">4</ref>, right). The former problem lives in 3136 dimensions, while the latter lives in 3072, and both have 10 classes. Despite the structural similarities between these problems, the decreased concentration of CIFAR-10 results in vastly more susceptibility to attacks, regardless of whether adversarial training is used. The theory above suggests that this increased susceptibility is caused at least in part by a shift in the fundamental limits for CIFAR-10, rather than the weakness of the particular classifiers we chose.</p><p>Informally, the concentration limit U c can be interpreted as a measure of image complexity. Image classes with smaller U c are likely concentrated near high-dimensional complex manifolds, have more intra-class variation, and thus more apparent complexity. An informal interpretation of Theorem 2 is that "high complexity" image classes are fundamentally more susceptible to adversarial examples, and Figure <ref type="figure" target="#fig_5">4</ref> suggests that complexity (rather than dimensionality) is largely responsible for differences we observe in the effectiveness of adversarial training for different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">SO...ARE ADVERSARIAL EXAMPLES INEVITABLE?</head><p>The question of whether adversarial examples are inevitable is an ill-posed one. Clearly, any classification problem has a fundamental limit on robustness to adversarial attacks that cannot be escaped by any classifier. However, we have seen that these limits depend not only on fundamental properties of the dataset, but also on the strength of the adversary and the metric used to measure perturbations. This paper provides a characterization of these limits and how they depend on properties of the data distribution. Unfortunately, it is impossible to know the exact properties of real-world image distributions or the resulting fundamental limits of adversarial training for specific datasets. However, the analysis and experiments in this paper suggest that, especially for complex image classes in high-dimensional spaces, these limits may be far worse than our intuition tells us.</p><p>Under review as a conference paper at ICLR 2019 for any z, w ∈ R n . From this, we see that for p * = min(p, 2)</p><formula xml:id="formula_25">Φ(z) − Φ(w) p ≤ n 1/p * −1/2 Φ(z) − Φ(w) 2 ≤ n 1/p * √ 2πn z − w 2 (10)</formula><p>where we have used the identity u p ≤ n 1/ min(p,2)−1/2 u 2 . Now, consider any set A in the cube, and let B = Φ −1 (A). From equation 10, we see that</p><formula xml:id="formula_26">ΦB √ 2πn n 1/p * , d 2 ⊂ A( , d p ).</formula><p>It follows from equation 10 that</p><formula xml:id="formula_27">σ[A( , d p )] ≥ µ B √ 2πn n 1/p * , d 2 .</formula><p>Applying Lemma 5, we see that</p><formula xml:id="formula_28">σ[A( , d p )] ≥ Φ α + √ 2πn n 1/p *<label>(11)</label></formula><p>where α = Φ −1 (σ[A]).</p><p>To obtain the simplified formula in the theorem, we use the identity</p><formula xml:id="formula_29">1 √ 2π ∞ x e −t 2 dt &lt; e −x 2 √ 2πx</formula><p>which is valid for x &gt; 0, and can be found in <ref type="bibr" target="#b0">Abramowitz &amp; Stegun (1965)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF OF LEMMA 4</head><p>Our proof emulates the method of Talagrand, with minor modifications that extend the result to other p norms. We need the following standard inequality. Proof can be found in <ref type="bibr" target="#b35">Talagrand (1995;</ref><ref type="bibr" target="#b36">1996)</ref>. Lemma 6 <ref type="bibr">(Talagrand)</ref>. Consider a probability space Ω with measure µ. For g : Ω → [0, 1], we have</p><formula xml:id="formula_30">Ω min e t , 1 g α dµ × Ω gdµ α ≤ exp t 2 (α + 1) 8α .</formula><p>Our proof of Lemma 3 follows the three-step process of Talagrand illustrated in <ref type="bibr" target="#b35">Talagrand (1995)</ref>. We begin by proving the bound <ref type="figure">A</ref>) is a measure of distance from A to x, and α, t are arbitrary positive constants. Once this bound is established, a Markov bound can be used to obtain the final result. Finally, constants are tuned in order to optimize the tightness of the bound.</p><formula xml:id="formula_31">e tf (x,A) dx ≤ 1 σ α [A] exp nt 2 (α + 1) 8α (12) where f (x, A) = min y∈A i |x i − y i | p = d p p (x,</formula><p>We start by proving the bound in equation 12 using induction on the dimension. The base case for the induction is n = 1, and we have</p><formula xml:id="formula_32">e tf (x,A) dx ≤ σ[A]+ A c e tf (x,A) dx ≤ σ[A]+ A c 1dx ≤ σ[A]+(1−σ[A])e t ≤ 1 σ α [A] exp t 2 (α + 1) 8α .</formula><p>We now prove the result for n dimensions using the inductive hypothesis. We can upper bound the integral by integrating over "slices" along one dimension. Let A ⊂ [0, 1] n . Define</p><formula xml:id="formula_33">A ω = {z ∈ R n−1 | (ω, z) ∈ A} and B = {z ∈ R n−1 | (ω, z) ∈ A for some ω}.</formula><p>Clearly, the distance from (ω, z) to A is at most the distance from z to A ω , and so</p><formula xml:id="formula_34">e tf (x,A) dx ≤ ω∈[0,1] z∈[0,1] n−1 e tf (z,Aω) dz dx ≤ ω∈[0,1] 1 σ α [A ω ] exp (n − 1)t 2 (α + 1) 8α .</formula><p>We also have that the distance from x to A is at most one unit greater than the distance from x to B. This gives us</p><formula xml:id="formula_35">e tf (x,A) dx ≤ (ω,z)∈[0,1] n e t(f (x,B)+1) ≤ e t (ω,z)∈[0,1] n e tf (x,B) ≤ e t σ α [B] exp (n − 1)t 2 (α + 1) 8α .</formula><p>Applying equation 6 gives us</p><formula xml:id="formula_36">e tf (x,A) dx ≤ ω∈[0,1] min e t σ α [B] exp (n − 1)t 2 (α + 1) 8α , 1 σ α [A ω ] exp (n − 1)t 2 (α + 1) 8α = exp (n − 1)t 2 (α + 1) 8α 1 σ α [B] ω∈[0,1] min e t , σ α [B] σ α [A ω ] .</formula><p>Now, we apply lemma 6 to equation 13 with g(ω) = α[A ω ]/α[B] to arrive at equation 12.</p><p>The second step of the proof is to produce a Markov inequality from equation 12. For the bound in equation 12 to hold, we need</p><formula xml:id="formula_37">1 − σ[A( , d p )] = σ{x | f (x) &gt; p } ≤ e tf (x,A) dx e t p ≤ exp nt 2 (α+1) 8α σ α [A]e t p .<label>(14)</label></formula><p>The third step is to optimize the bound by choosing constants. We minimize the right hand side by choosing t = 4α p n(α+1) to get</p><formula xml:id="formula_38">1 − σ[A( , d p )] ≤ exp − 2α 2p n(α+1) σ α [A] .<label>(15)</label></formula><p>Now, we can simply choose α = 1 to get the simple bound</p><formula xml:id="formula_39">1 − σ[A( , d p )] ≤ exp − 2p /n σ[A] ,<label>(16)</label></formula><p>or we can choose the optimal value of α = 2 2p</p><p>n log(1/σ) − 1, which optimizes the bound in the case 2p ≥ n 2 log(1/σ(A)). We arrive at</p><formula xml:id="formula_40">1 − σ[A( , d p )] ≤ exp − 2 n p − n log(σ −1 [A])/2 2 . (<label>17</label></formula><formula xml:id="formula_41">)</formula><p>This latter bound is stronger than we need to prove Lemma 3, but it will come in handy later to prove Theorem 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOF OF THEOREMS 2 AND 3</head><p>We combine the proofs of these results since their proofs are nearly identical. The proofs closely follow the argument of Theorem 1.</p><p>Choose a class c with f c ≤ 1 2 and let R = {x|C(x) = c} denote the subset of the cube lying in class c according to the classifier C. Let R be the complement, who's p expansion is denoted R( ; d p ). Because R covers at least half the cube, we can invoke Lemma 3. We have that vol[R( ; h)] ≥ 1 − δ, where δ = exp(−πn 1−2/p * 2 ) 2πn 1/2−1/p * , for Theorem 2 and 2U c exp(− 2 /n), for Theorem 3.</p><p>(18)</p><p>The set R( ; h) contains all points that are correctly classified and safe from adversarial perturbations. This region has volume at most δ, and the probability of a sample from the class distribution ρ c lying in this region is at most U c δ. We then subtract this from 1 to obtain the mass of the class distribution lying in the "unsafe" region R c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROOF OF THEOREM 4</head><p>Let A denote the support of p c , and suppose that this support has measure vol[A] = η. We want to show that, for large enough , the expansion A( , d p ) is larger than half the cube. Since class c occupies less than half the cube, this would imply that A( , d p ) overlaps with other classes, and so there must be data points in A with -adversarial examples.</p><p>We start with the case p &gt; 0, where we bound A( , d p ) using equation 2 of Lemma 3. To do this, we need to approximate Φ −1 (η). This can be done using the inequality The quantity on the left will be greater than This can be re-arranged to obtain the desired result.</p><formula xml:id="formula_42">Φ(α) = 1 2π</formula><p>In the case p = 0, we need to use equation 17 from the proof of Lemma 3 in Appendix B, which we restate here </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E PROOF OF THEOREM 5</head><p>Assume that any MNIST classifier can be fooled by perturbations of size at most with probability at least p.</p><p>To begin, we put a bound on the susceptibility of any b-MNIST classifier (for b ≥ 1) under this assumption. We can classify MNIST images by upsampling them to resolution 28b × 28b and feeding them into a highresolution "back-end" classifier. After upsampling, an MNIST image with perturbation of norm becomes a 28b × 28b image with perturbation of norm b . The classifier we have constructed takes low-resolution images as inputs, and so by assumption it is fooled with probability at least p. However, the low-resolution classifier is fooled only when the high-resolution "back-end" classifier is fooled, and so the high-resolution classifier is fooled with probability at least p at well. Note that we can build this two-scale classifier using any high-resolution classifier as a back-end, and so this bound holds uniformly over all high-resolution classifiers.</p><p>Likewise, suppose we classify b-MNIST images (for integer b ≥ 1) by downsampling them to the original 28 × 28 resolution (by averaging pixel blocks) and feeding them into a "back-end" low-resolution classifier.</p><p>After downsampling, a 28b × 28b image with perturbation of norm b becomes a 28 × 28 image with perturbation of norm at most . Whenever the high-resolution classifier is fooled, it is only because the back-end classifier is fooled by a perturbation of size at most , and this happens with probability at least p.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The -expansion of a half sphere nearly covers the whole sphere for small and large n. Visualizations show the fraction of the sphere captured within units of a half sphere in different dimensions. Results from a near-exact experimental method are compared to the theoretical lower bound in Lemma 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>be the fraction of the sphere labeled as c by classifier C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Using this result, we can show that most data samples in a cube admit adversarial examples, provided the data distribution is not excessively concentrated. Theorem 2 (Adversarial examples on the cube). Consider a classification problem with m classes, each distributed over the unit hypercube [0, 1] n with density functions {ρ c } m c=1 . Choose a classifier function C : [0, 1] n → {1, 2, . . . , m} that partitions the hypercube into disjoint measurable subsets. Define the following scalar constants: • Let U c denote the supremum of ρ c . • Let f c be the fraction of hypercube partitioned into class c by C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>It may be computationally hard to craft adversarial examples because of local flatness of the classification function, obscurity of the classifier function, or other computational difficulties. Computational hardness could prevent adversarial attacks in practice, even if adversarial examples still exist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 5 .</head><label>5</label><figDesc>Suppose and p are such that, for all MNIST classifiers, a random image from class c has an -adversarial example (in the 2 -norm) with probability at least p. Then for all classifiers on b-MNIST, with integer b ≥ 1, a random image from c has a b -adversarial example with probability at least p. Likewise, if all b-MNIST classifiers have b -adversarial examples with probability p for some b ≥ 1, then all classifiers on the original MNIST distribution have -adversarial examples with probability p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Robustness of MNIST and "big" MNIST classifiers as a function of . Naturally trained classifiers are less robust with increased dimensionality. (b) With adversarial training, susceptibility curves behave as predicted by Theorems 2 and 5. (c) The susceptibility of CIFAR-10 is compared to big MNIST. Both datasets have similar dimension, but the higher complexity of CIFAR-10 results in far worse susceptibility. Perturbations are measured in the 2-norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>α = Φ −1 (η), then Φ(α) = η, and equation 19 gives us α ≥ − log 1 4η 2 . Plugging this into equation 2 of Lemma 3, we get vol[A( , d p )] ≥ Φ(α + ) ≥ Φ − log 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 .</head><label>2</label><figDesc>vol[A( , d 0 )] ≥ 1 − exp − 2 n − n log(1/η)/2This bound is valid, and produces a non-vacuous guarantee of adversarial examples, if</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Only the fully-connected layer is modified to handle the difference in dimensionality between datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Adversarial examples for MNIST/CIFAR-10 were produced as in<ref type="bibr" target="#b18">Madry et al. (2017)</ref> using 100-step/20-step PGD.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOF OF LEMMA 3</head><p>We now prove Lemma 3. To do this, we begin with a classical isoperimetric inequality for random Gaussian variables. Unlike the case of a cube, tight geometric isoperimetric inequalities exist in this case. We then prove results about the cube by creating a mapping between uniform random variables on the cube and random Gaussian vectors.</p><p>In the lemma below, we consider the standard Gaussian density in R n given by p(x) = 1 (2π) n/2 e −nx 2 /2 and corresponding Gaussian measure µ. We also define</p><p>which is the cumulative density of a Gaussian curve.</p><p>The following Lemma was first proved in <ref type="bibr" target="#b33">Sudakov &amp; Tsirelson (1974)</ref>, and an elementary proof was given in <ref type="bibr" target="#b4">Bobkov et al. (1997)</ref>.</p><p>Lemma 5 (Gaussian Isoperimetric Inequality). Of all sets with the same Gaussian measure, the set with 2 -expansion of smallest measure is a half space. Furthermore, for any measurable set A ⊂ R n , and scalar constant a such that</p><p>Using this result we can now give a proof of Lemma 3.</p><p>This function Φ maps a random Guassian vector z ∈ N (0, I) onto a random uniform vector in the unit cube.</p><p>To see why, consider a measurable subset B ⊂ R n . If µ is the Gaussian measure on R n and σ is the uniform measure on the cube, then</p><p>Since ∂ ∂zi Φ(z) ≤ 1 √ 2π , we also have</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Handbook of mathematical functions: with formulas, graphs, and mathematical tables</title>
		<author>
			<persName><forename type="first">Milton</forename><surname>Abramowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><forename type="middle">A</forename><surname>Stegun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Courier Corporation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07397</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Synthesizing robust adversarial examples. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An isoperimetric inequality on the discrete cube, and an elementary proof of the isoperimetric inequality in gauss space</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName><surname>Bobkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="214" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OpenReview</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Defensive distillation is not robust to adversarial examples</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04311</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Magnet and &quot;efficient defenses against adversarial attacks&quot; are not robust to adversarial examples</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08478</idno>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Nilaksh</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhuri</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Shang-Tse Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duen</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Horng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06816</idno>
		<title level="m">Shield: Fast, practical defense and vaccination for deep learning using JPEG compression</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01442</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarial vulnerability for any classifier</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08686</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02774</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Adversarial spheres. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<title level="m">Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images using input transformations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00851</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The concentration of measure phenomenon</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Ledoux ; Xingjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudanthi</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02613</idno>
	</analytic>
	<monogr>
		<title level="m">Paul Lévy and Franco Pellegrino. Problémes concrets d&apos;analyse fonctionnelle. Gauthier-Villars</title>
				<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1951">2001. 1951. 2018</date>
			<biblScope unit="volume">89</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Characterizing adversarial subspaces using local intrinsic dimensionality</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure</title>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Mahloujifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><forename type="middle">I</forename><surname>Diochnos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mahmoody</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03063</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the method of bounded differences</title>
		<author>
			<persName><surname>Mcdiarmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">London Mathematical Society Lecture Notes</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="148" to="188" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MagNet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Asymptotic Theory of Finite Dimensional Normed Spaces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vitali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Milman</surname></persName>
		</author>
		<author>
			<persName><surname>Schechtman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The isoperimetric inequality</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Osserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1182" to="1238" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09344</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The isoperimetric problem</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Ros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06605</idno>
	</analytic>
	<monogr>
		<title level="m">Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models</title>
				<imprint>
			<date type="published" when="2001">2001. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="175" to="209" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Global theory of minimal surfaces</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">APE-GAN: Adversarial perturbation elimination with GAN. ICLR Submission</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Carl-Johann</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01421</idno>
		<title level="m">Adversarial vulnerability of neural networks increases with input dimension</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Certifying some distributional robustness with principled adversarial training</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OpenReview</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">PixelDefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10766</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">Vasconcellos</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakurai</forename><surname>Kouichi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08864</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extremal properties of half-spaces for spherically invariant measures</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Sudakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Tsirelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Soviet Math</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Concentration of measure and isoperimetric inequalities in product spaces</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Talagrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publications Mathématiques de l&apos;Institut des Hautes Etudes Scientifiques</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="205" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A new look at independence. The Annals of probability</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Talagrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The space of transferable adversarial examples</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03453</idno>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">High-Dimensional Probability</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01991</idno>
		<title level="m">Mitigating adversarial effects through randomization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01155</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient defenses against adversarial attacks</title>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria-Irina</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Rawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="49" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
