<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder ‡</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-06">6 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guanlin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shandong Computer Science Center (National Supercomputer Center in Jinan</orgName>
								<orgName type="laboratory">Shandong Provincial Key Laboratory of Computer Networks</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuya</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Luo</surname></persName>
							<email>junluo@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<email>chang015@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Network</forename><surname>Name</surname></persName>
						</author>
						<title level="a" type="main">Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder ‡</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-06">6 May 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2005.02552v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Whereas adversarial training is employed as the main defence strategy against specific adversarial samples, it has limited generalization capability and incurs excessive time complexity. In this paper, we propose an attack-agnostic defence framework to enhance the intrinsic robustness of neural networks, without jeopardizing the ability of generalizing clean samples. Our Feature Pyramid Decoder (FPD) framework applies to all block-based convolutional neural networks (CNNs). It implants denoising and image restoration modules into a targeted CNN, and it also constraints the Lipschitz constant of the classification layer. Moreover, we propose a two-phase strategy to train the FPD-enhanced CNN, utilizing -neighbourhood noisy images with multi-task and self-supervised learning. Evaluated against a variety of white-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain sufficient robustness against general adversarial samples on MNIST, SVHN and CALTECH. In addition, if we further conduct adversarial training, the FPD-enhanced CNNs perform better than their non-enhanced versions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ever-growing ability of deep learning has found numerous applications mainly in image classification, object detection, and natural language processing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref>. While deep learning has brought great convenience to our lives, its weakness is also catching researchers' attention. Recently, researchers have started to pay more attention to investigating the weakness of neural networks, especially in its application to image classification. Since the seminal work by <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref>, many follow-up works have demonstrated a great variety of methods in generating adversarial sam-ples: though easily distinguishable by human eyes, they are often misclassified by neural networks. More specifically, most convolutional layers are very sensitive to perturbations brought by adversarial samples (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>), resulting in misclassifications. These so-called adversarial attacks may adopt either white-box or black-box approaches, depending on the knowledge of the target network, and they mostly use gradient-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> or score-based methods <ref type="bibr" target="#b3">[4]</ref> to generate adversarial samples.</p><p>To thwart these attacks, many defence methods have been proposed. Most of them use adversarial training to increase the network robustness, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. However, as training often targets a specific attack, the resulting defense method can hardly be generalized, as hinted in <ref type="bibr" target="#b26">[27]</ref>. In order to defend against various attacks, a large amount and variety of adversarial samples are required to retrain the classifier, leading to a high time-complexity. In the meantime, little attention has been given to the direct design of robust frameworks in an attack-agnostic manner, except a few touches on denoising <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> and obfuscating gradients <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref> that aim to directly enhance a target network in order to cope with any potential attacks.</p><p>To enhance the intrinsic robustness of neural networks, we propose an attack-agnostic defence framework, applicable to enhance all types of block-based CNNs. We aim to thwart both white-box and black-box attacks without crafting any specific adversarial attacks. Our Feature Pyramid Decoder (FPD) framework implants a target CNN with both denoising and image restoration modules to filter an input image at multiple levels; it also deploys a Lipschitz Constant Constraint at the classification layer to limit the output variation in the face of attack perturbation. In order to train an FPD-enhanced CNN, we propose a two-phase strategy; it utilizes -neighbourhood noisy images to drive multi-task and self-supervised learning.</p><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, FPD employs a front denoising module, an image restoration module, a middle denoising layer, and a back denoising module. Both front and back denoising modules consist of the original CNN blocks interleaved with inner denoising layers, and the inner denoising layers are empirically implanted only to the shallow blocks of the CNN. Enabled by the image restoration module, the whole enhanced CNN exhibits a multi-scale pyramid structure. The multi-task learning concentrates on improving both the quality of the regenerate images x clean and the performance of final classification. Aided by the supervision target x clean , the enhanced CNN could be trained to denoise images and abstract the features from the denoised images. In summary, we make the following major contributions:</p><p>• Through a series of exploration experiments, we propose a novel defence framework. Our FPD framework aims to enhance the intrinsic robustness of all types of block-based CNN. Owing to unavoidable limitations of evaluating robustness, we release our network in github ‡ to invite researchers to conduct extended evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adversarial attack and training White-box attacks are typically constructed based on the gradients of the target network such as Fast-Gradient Sign Method (FGSM), Pro-jected Gradient Descent (PGD) and Basic Iterative Method (BIM) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>. Some approaches focus on optimizing attack objective function like Carlini &amp; Wagner attack (C&amp;W) and DeepFool <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>, while others utilize the decision boundary to attack the network <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. Black-box attacks mainly rely on transfer-attack. Attackers substitute the target network with a network, trained with the same dataset. Subsequently, white-box attacks are applied to the substituted network for generating the adversarial samples.</p><p>Adversarial training, proposed by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, is an approach to improve the robustness of the target network. Normally, it augments the adversarial samples to the training set in the process of retraining phase. Adversarial training could achieve good results on defending against whitebox and black-box attacks. However, it requires to involve a sufficient amount and variety of adversarial samples, leading to a high time-complexity.</p><p>Denoising Most denoising methods improve the intrinsic robustness of the target network, contributed by obfuscating gradients: non-differentiable operations, gradient vanishing (exploding). Various non-differentiable operations are proposed such as image quilting, total variance minimization and quantization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11]</ref>. Pixel denoising approach utilizes gradient vanishing (exploding) to thwart the attack, widely developed based on Generative-Adversarial-Network (GAN) such as <ref type="bibr" target="#b18">[19]</ref>. However, the aforementioned approaches cannot thwart structure-replaced white-box attacks easily <ref type="bibr" target="#b0">[1]</ref>. Attackers could still conduct attacks by approximating gradients of their non-differentiable computations. Instead of relying on obfuscating gradients, our differentiable FPD can circumvent the structure-replaced white-box attack. Our proposal is partially related to <ref type="bibr" target="#b28">[29]</ref>, as the denoising layers in our FPD are inspired by their feature denoising approach. Nevertheless, different from <ref type="bibr" target="#b28">[29]</ref>, the principle behind our FPD is to improve the intrinsic robustness, regardless of conducting adversarial training or not. Consequently, FPD includes not only two denoising modules, but also image restoration module and the Lipschitz constant constrained classification layer as well, establishing a multi-task and self-supervised training environment. Moreover, we employ denoising layers in a much more effective way: instead of implanting them to all blocks of the enhanced CNN, only shallow blocks are enhanced for maintaining high-level abstract semantic information. We will compare the performance between FPD-enhanced CNN and the CNN enhanced by <ref type="bibr" target="#b28">[29]</ref> in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Pyramid Decoder</head><p>In this section, we introduce each component of our Feature Pyramid Decoder, shown in Figure <ref type="figure" target="#fig_0">1</ref>. Firstly, we introduce the structure of the front denoising module FPD FD and back denoising module FPD BD . Next, the structure of the image restoration module FPD R is depicted. Then, we modify the classification layer of the CNN by applying Lipschitz constant constraint (FPD LCC ). Finally, our two-phase training strategy is introduced, utilizing -neighbourhood noisy images with multi-task and selfsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Front and Back Denoising Module</head><p>A denoising module is a CNN implanted by certain inner denoising layers. Specifically, a group of inner denoising layers is only implanted into the shallow blocks of a block-based CNN. Consequently, the shallow features are processed to alleviate noise, whereas the deep features are directly decoded, helping to keep the abstract semantic in-formation. Meanwhile, we employ a residual connection between denoised features and original features. In the light of it, most of original features could be kept and it helps to amend gradient update.</p><p>Moreover, we modify non-local means algorithm <ref type="bibr" target="#b2">[3]</ref> by replacing the Gaussian filtering operator with a dot product operator. It could be regarded as a self-attention mechanism interpreting the relationship between pixels. Compared with the Gaussian filtering operator, the dot product operator helps improve the adversarial robustness <ref type="bibr" target="#b28">[29]</ref>. Meanwhile, as the dot product operator does not involve extra parameters, it contributes to relatively lower computational complexity. We explore two inner denoising structures shown in Figure <ref type="figure">2a</ref> and Figure <ref type="figure">2b</ref>. The corresponding performance comparison is conducted in Section 4.1. In our framework, the parameters of FPD FD and FPD BD are shared for shrinking the network size. The motivation of exploiting weight sharing mechanism has been explained in <ref type="bibr" target="#b14">[15]</ref>: weight sharing mechanism not only reduces Memory Access Cost (MAC) but also provides more gradient updates to the reused layers from multiple parts of the network, leading to more diverse feature representations and helping FPD to generalize better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Restoration Module</head><p>To build the restoration module, we firstly upsample feature maps from each block of FPD FD (except the first block) for the image dimension consistency and then the upsampled feature maps are fused. Finally, a group of the transposed convolutions transforms the fused feature maps into an image that has the same resolution as the input. On the other hand, we especially find that particular noise is brought by the x clean . To minimize its influence, another middle denoising layer is applied to the x clean , depicted in Figure <ref type="figure">2c</ref>. Contributed by the image restoration module and the denoising module, it helps establish a two-phase training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Lipschitz Constant Constrained Classification</head><p>The influence of employing Lipschitz constant on defending against the adversarial samples have been analyzed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. As stated in our following Theorem 1, the network could be sensitive to some perturbations if Softmax is directly used as the last layer's activation function. However, no network has ever adopted another output-layer activation function before Softmax in defending against adversarial samples so far. Theorem 1 (the constraint on Lipschitz constant for fullyconnected network). Let NN FC be a K-way-L-layer-fullyconnected network, NN FC (x) k be the k-th component of the network output given input x, w i be the weight matrix of the i-th layer of the network, and b i be a bias matrix of the same layer. Given a noise vector ξ, we can bound the variation V component-wisely from above by:</p><formula xml:id="formula_0">V k = |NN FC (x) k − NN FC (x + ξ) k | ≤ e θ k |x (e η − e −η ) p e θp| x+ξ</formula><p>, where θ k | x is the k-th component of the input to Softmax given input x. Given Softmax function as the activation function of the output layer, we denote the activation function of earlier layers by f , f 's Lipschitz constant by C, and let</p><formula xml:id="formula_1">η = max k=1,••• ,K {[w L C L−1 |w L−1 w L−2 . . . w 1 ξ|+b L ] k }.</formula><p>We postpone the proof of Theorem 1 to the supplementary material. The theorem clearly shows that w L and b L may have more prominent influence than C L−1 |w L−1 w L−2 . . . w 1 ξ| on the variation of the output V k , when we have 0 ≤ C (L−1) |w L−1 w L−2 . . . w 1 ξ| ≤ 1 achieved by using regularization to restrict the weights getting close to zero. Therefore, we want to restrict w L and b L by utilizing a squeezing function f s with a small Lipschitz constant C s before Softmax in the output layer. Consequently, this reduces η to max k=1,...,K {C s C L−1 [|w L w L−1 w L−2 . . . w 1 ξ|] k }, potentially leading to a smaller V k . Therefore, the output of NN FC could be more stable in the face of attack perturbation. x noisy := x clean + noise 7:</p><formula xml:id="formula_2">ŷ(1) , x<label>(1)</label></formula><p>clean := FPD(x noisy ) 8:</p><formula xml:id="formula_3">l 2 := L 2 (x clean , x<label>(1) clean ) 9:</label></formula><p>if l 2 &gt; T then end if 19: end for 20: return FPD To thwart various attacks, we let f s = Tanh(x) as our squeezing function, shown in Figure <ref type="figure">3</ref>. Moreover, we empirically replace all the activation functions from ReLU to ELU; this leads to a smoother classification boundary, thus adapting to more complex distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Strategy</head><p>We carefully devise our training strategy and involve uniformly sampled random noise to the clean images for further improving the enhanced CNN. Let us define the enhanced CNN FPD, in which FPD R refers to the image restoration module; FPD FD stands for the front denois- ing module; FPD BD stands for the back denoising module; FPD LCC refers to the modified classification layer.</p><p>To further improve the denoising and generalization capability, we suppose that the samples in theneighbourhood of each image x clean constitute adversarial samples candidate sets. We add uniformly sampled random noise to the clean images by using a sampler. It is impossible to use all samples in candidate sets, but the enhanced CNN will have more stable performance on classifying images in a smaller δ-neighbourhood ([x clean − δ, x clean + δ], 0 ≤ δ ∞ ≤ ) after training on noisy images. The detail training procedures are described in Algorithm 1.</p><p>We propose the two-phase training to drive the selfsupervised and multi-task learning for jointly optimizing the enhanced CNN. It helps the enhanced CNN to learn how to denoise images and abstract features from them with low cost and helps the enhanced CNN to learn a much more accurate mapping between images and labels. As shown in Figure <ref type="figure">4</ref>, the first phase mainly focuses on regenerating images, optimized by L 2 (x clean , x clean ) loss. To guarantee the quality of x clean used in the later training procedures, we set a threshold T . If L 2 loss &gt; T , only the parameters of FPD R and FPD FD is updated for generating the higher quality x clean . Once the L 2 loss reaches the T , the crossentropy (CE) loss with L 2 loss jointly trains the enhanced CNN. Then, the second phase focus on using the good qual-ity x clean to train the enhanced CNN further, jointly optimized by CE loss and L 2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we firstly investigate the best framework structure through the exploration study. Moreover, we compare with the most related work <ref type="bibr" target="#b28">[29]</ref> as well. In the comparison experiments, we focus on comparing the robustness between the enhanced CNN and the original one, conducting adversarial training and normal training, respectively. Owing to the unavoidable limitations of evaluating robustness, we apply various attacks to evaluate our performance. However, we cannot avoid that more effective attacks exist and the trained network will be released for future evaluation.</p><p>We employ MNIST, the Street View House Numbers (SVHN), CALTECH-101 and CALTECH-256 datasets in the following experiments. MNIST consists of a training set of 60,000 samples and a testing dataset of 10,000 samples. SVHN is a real-world colored digits image dataset. We use one of its format which includes 73,257 MNIST-like 32-by-32 images centered around a single character for training and 10,000 images for testing. For both MNIST and SVHN, we resize them to image size 64. Besides, we repeat the channel three times on MNIST for network consistency. For both CALTECH-101 and CALTECH-256, we randomly choose 866 and 1,422 images as test images  <ref type="bibr" target="#b11">[12]</ref> as well as ResNeXt-50 <ref type="bibr" target="#b29">[30]</ref> are enhanced in the following experiments. We use Pytorch to implement the whole experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Exploration Experiments</head><p>In this section, we conduct the exploration experiments of the FPD-enhanced CNN which is based on ResNet-101 F on MNIST. In Table <ref type="table" target="#tab_5">1</ref>, we use L ∞ -PGD attack with parameters: = 0.3, step = 40, step size = 0.01 for both whitebox and black-box attacks. Under the black-box condition, we separately train a simple three layers fully-connected network as the substitute network <ref type="bibr" target="#b22">[23]</ref> for each network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner Denoising Layers Implanted Positions Selection</head><p>We firstly explore the position to implant the inner denoising layers. In Table <ref type="table" target="#tab_5">1</ref>, 'Shallow' means that the denoising layers are implanted into the first two residual blocks. Likewise, 'Deep' means that the layers are implanted into the third and fourth residual blocks. We observe that the 'Shallow' outperforms 'Deep' on average. It may be contributed by the high-level abstract semantic information generated from the directly decoded deep features. In the following experiments, we always implant the inner denoising layers to the shallower blocks.</p><p>Denoising Approaches Selection Next, we explore the best denoising operation. In Table <ref type="table" target="#tab_5">1</ref>, no denoising layers are implanted in both the front and back denoising modules in 'Average', 'Flip' and 'Mid' denoising approaches. In these three approaches, we only focus on cleaning the x clean before passing to F BD . Specifically, 'Average': x clean and x clean are averaged; 'Flip': x clean are flipped; 'Mid': the noise in x clean are alleviated by the middle denoising layer as depicted in Figure <ref type="figure">2c</ref>. Finally, 'Mid + Inner' means that we implant the two inner denoising layers to both the front and back denoising modules respectively. Meanwhile, the middle denoising layer is also utilized. Distinctly, 'Mid + Inner' is all-sided robust among them to defend against both the black-box and white-box attacks, attributing to the stronger denoising capability.</p><p>Ablation Study To validate the effectiveness of F, we perform the ablation experiments on investigating the effectiveness of each module. As shown in Table <ref type="table" target="#tab_5">1</ref>, F performs far better than both F FD+R and F FD in thwarting both white-box and black-box attacks. This overall robustness is owing to the increase of data diversity and the supervision signal brought by F. Furthermore, F BD can further clean the x clean to enhance the robustness in defending against the well-produced perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation Functions Selection</head><p>We explore the activation functions selection. Table <ref type="table" target="#tab_5">1</ref> indicates that ELU activation function outperforms ReLU. Furthermore, as shown in Figure <ref type="figure">3</ref>, ELU with Tanh achieves better performance than ELU one with 3.92%. It demonstrates that ELU with Tanh is the suggested activation function selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner Denoising Layers Selection</head><p>We also investigate the optimal number of the inner denoising layers and whether to use the bottleneck in these inner layers. In Table 1, F kIB−Mid : k inner denoising layers with the bottleneck as depicted in Figure <ref type="figure">2a</ref> are implanted to each denoise module F FD and F BD respectively. Meanwhile, the middle denoising layer is used as depicted above; F kIB is similar to F kIB−Mid except that no middle denoising layer is involved in the framework. F kI−Mid means that the bottleneck is not used in the inner denoising layers as depicted in Fig- <ref type="figure">ure 2b</ref>. We observe that the bottleneck reduces the performance around 10%. Moreover, although F 4IB outperforms F 2IB , the enhancement is not worthy if we consider the time complexity brought by the additional denoising layers. Therefore, we use F 2I−Mid as our proposed framework in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Strategy Selection</head><p>We further demonstrate the efficacy of our two-phase training strategy F Two Phase as depicts in Figure <ref type="figure">4</ref>. We mainly compare F Two Phase with one-phase training strategy F One Phase .i.e the first L ∞ ( = 0.3) training phase (describes in Section 3.4). Results show that F Two Phase could achieve higher performance than F One Phase with 14.3%.</p><formula xml:id="formula_4">L 2 ( = 1.5) L 2 Network Name FGSM PGD C&amp;W FGSM PGD C&amp;W DeepFool Average Acc T(m) Acc T(m) Acc T(m) Acc T(m) Acc T(m) Acc T(m) Acc T(m) Acc T(m) O 4% 0.</formula><p>Comparison with the Related Work As mentioned in Section 2, the denoising approach proposed in <ref type="bibr" target="#b28">[29]</ref> is similar to our denoising layers in FPD. Therefore, we conduct a comparison experiment with <ref type="bibr" target="#b28">[29]</ref> as well. In Table <ref type="table" target="#tab_5">1</ref>, X represents the enhanced CNN by <ref type="bibr" target="#b28">[29]</ref>. We observe that our F 2I−Mid outperforms X . Especially, the performance of thwarting the white-box attack is about 20% higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison Experiments</head><p>We conduct a series of comparison experiments * to further evaluate FPD-enhanced CNN performance on MNIST, SVHN, CALTECH-101 and CALTECH-256.</p><p>Notation and Implementation Details Firstly, let us define the following notations for accurate description: F represents the enhanced CNN; O is the original CNN; F PGD and O PGD is adversarial trained by L ∞ -PGD (on MNIST: =0.3, step=100 and step length=0.01; on SVHN: =8/256.0, step=40 and step length=2/256.0); F FGSM and O FGSM is adversarial trained by L ∞ -FGSM (on MNIST: =0.3). All results are achieved with the batch size 100, running on the RTX Titan.</p><p>On MNIST For sufficient evaluation, we firstly focus on applying FPD to ResNet-101 on MNIST. We mainly concentrate on two performance metrics: classification accuracy and attack time. Longer attack time can be a result of a monetary limit. In this perspective, we believe that longer attacking time may result in the excess of time and monetary limit. The attacker may surrender the attack. Therefore, we state that attackers spend more time attacking networks, which may protect the networks from another perspective. We employ various white-box attacks to attack F, O, F PGD , O PGD , F FGSM and O FGSM . We consider following attacks, including L 2 -PGD, L 2 -FGSM, L ∞ -PGD and L ∞ -FGSM. We set =1.5 and 0.3 to bound the permutations for L 2 and L ∞ norm. Both L 2 -PGD and L ∞ -PGD are set to attack for 100 iterations and each step length is 0.1.</p><p>We have the following remarks on our results as shown in Table <ref type="table" target="#tab_6">2</ref>. Generally, F and its adversarial trained F FGSM outperform O and O FGSM in accuracy around 32% and 10%, respectively. O PGD seems slightly more robust than F PGD . However, as revealed by the average attack time, more computational time (around 89 min) is spent on attacking F PGD . In particular, the overall time spent on attacking F, its adversarial trained networks F FGSM , F PGD are longer than O, O FGSM and O PGD around 104 min, 94 min and 89 min. Above results have demonstrated that F and its adversarial trained networks are harder to be attacked.</p><p>On SVHN We mainly assess the ability of FPD to enhance various block-based CNNs on colored samples: ResNet-101, ResNet-50, ResNeXt-50. We employ a series of white-box and black-box attacks to attack F, O, F PGD and O PGD for each block-based CNNs. Initially, we evaluate FPD performance in thwarting black-box attacks. As shown in Table <ref type="table" target="#tab_8">3</ref>, O and F of each block-based CNNs are employed as substitutes. We adopt L ∞ -FGSM and L ∞ -PGD to attack them. Besides, we observe that O is hard to defend against a L ∞ -C&amp;W attack, depicted in Table <ref type="table" target="#tab_9">4</ref>. Therefore, we additionally adopt L ∞ -C&amp;W to attack substitute O, to further evaluate FPD. As for white-box attacks, we adopt following attacks: L ∞ -FGSM, L ∞ -PGD, L ∞ -C&amp;W, L 2 -DeepFool and L 2 -C&amp;W. We set =8/256.0 for above-mentioned attacks and PGD is set to attack for 40 iterations with step length 2/256.0.</p><p>We have the following remarks on our results as shown in Table <ref type="table" target="#tab_8">3</ref>       O PGD on clean samples for ResNet-101 and ResNeXt-50 (as depicted in Table <ref type="table" target="#tab_8">3</ref>), F-based networks achieve this biased performance under black-box attacks. We also show the output of image restoration module in Figure <ref type="figure" target="#fig_5">5</ref>. Adversarial images are well "denoised" by comparing Figure <ref type="figure" target="#fig_5">5a</ref> with 5b. Figure <ref type="figure" target="#fig_5">5b</ref> and 5c illustrate that the module output generated by adversarial and clean images are quite similar. It guarantees that restoration module could generate similar images from both adversarial and clean images for FPD BD , leading to more robust performance in defending against attacks. In summary, above results have demonstrated that the FPD-enhanced CNN is much more robust than nonenhanced versions on MNIST and high dimensional dataset CALTECH-101 and CALTECH-256. On colored dataset SVHN, the performance under black-box attacks is not exactly satisfactory. However, considering the performance in thwarting white-box attacks, FPD-enhanced CNN performs far better than non-enhanced versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a novel Feature Pyramid Decoder (FPD) to enhance the intrinsic robustness of the block-based CNN. Besides, we have devised a novel two-phase training strategy. Through the exploration experiments, we have investigated the best structure of our FPD. Moreover, we go through a series of comparison experiments to demonstrate the effectiveness of the FPD. Attacking these models by a variety of white-box and black-box attacks, we have shown that the proposed FPD can enhance the robustness of the CNNs. We are planning to design a more powerful decoder to improve desnoising capability. Also, we will exploit a hard threshold to filter relatively bad restored images, further improving classification accuracy. Finally, we will transplant FPD to non-block CNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The structure of the block-based CNN, enhanced with the proposed framework named FPD: it consists of the Lipschitz constant constrained classification layer FPD LCC ; the front denoising module FPD FD , the image restoration module FPD R , a middle denoising layer and the back denoising module FPD BD . -neighbourhood noisy samples x nosiy and original samples x clean are used to train the FPD. Orange, blue and green blocks represent the original components of the CNN, the proposed components that implanted to the CNN, the modified components of the CNN, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>opt(θ = [FPD FD , FPD R ], loss = α 2 * l 2 , lr, wd) opt(θ = [FPD], loss = α 1 * l 1 + α 2 * l 2 , lr, wd) opt(θ = [FPD], loss = α 1 * l 1 + α 2 * l 2 ,lr, wd) 18:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 )( 2 )( 4 )Figure 4 :</head><label>2244</label><figDesc>Figure 4: Implementation details of two-phase training strategy utilizing self-supervised and multi-task learning: the enhanced CNN FPD, in which FPD R refers to the image restoration module; FPD FD stands for the front denoising module; FPD BD stands for the back denoising module; FPD LCC refers to the modified classification layer; x noisy are the samples in the -neighbourhood of each image. The first phase training is optimized by L 2 (x clean , x clean ) loss. If L 2 loss &gt; T , only the parameters of FPD R and FPD FD is updated. Once the L 2 loss reaches the T , the cross-entropy (CE) loss with L 2 loss jointly trains the enhanced CNN. Then, the second phase train the enhanced CNN further, jointly optimized by CE loss and L 2 loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Adversarial images (a) vs. the output of image restoration module from adversarial images (b) and clean images (c). Images are reproduced from the data in Table 4 (enhanced ResNet-101 attacked by PGD).</figDesc><graphic url="image-6.png" coords="8,54.42,404.13,69.10,69.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>On CALTECH-101 &amp; CALTECH-256 We further demonstrate the efficacy of FPD on ResNet-101 on high dimensional dataset CALTECH-101 and CALTECH-256, attacked by L ∞ -PGD attack for 40 iterations. For this attack, we set to 8/256.0 and step length to 2/256.0. To be specific, on CALTECH-101, F achieves 61.78% under PGD attack. It outperforms O around 34.64%. On CALTECH-256, our ResNet-101 model F achieve 49.79% accuracy against 0.00% of the original one O.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>This paper is supported by the Fundamental Research Fund of Shandong Academy of Sciences (NO. 2018:12-16), Major Scientific and Technological Innovation Projects of Shandong Province, China (No. 2019JZZY020128), as well as AcRF Tier 2 Grant MOE2016-T2-2-022 and AcRF Tier 1 Grant RG17/19, Singapore.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Residual Block Residual Block Denoising Layer</head><label></label><figDesc></figDesc><table><row><cell>×</cell></row><row><cell>×</cell></row><row><cell>BottleNeck</cell></row><row><cell>+</cell></row><row><cell>(a) inner denoising layer with bottleneck</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Residual Block Residual Block Denoising Layer</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Image</cell></row><row><cell></cell><cell></cell><cell>Restoration</cell></row><row><cell></cell><cell>𝑥𝑥 𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛</cell><cell>Network</cell></row><row><cell>×</cell><cell>×</cell><cell></cell></row><row><cell>×</cell><cell>×</cell><cell></cell></row><row><cell></cell><cell>Denoising</cell><cell></cell></row><row><cell>+</cell><cell>Layer</cell><cell>+</cell></row><row><cell></cell><cell cols="2">Residual Block</cell></row><row><cell>(b) inner denoising layer without bottleneck</cell><cell cols="2">(c) middle denoising layer without bottleneck</cell></row><row><cell cols="3">Figure 2: Three types of denoising layers which we have experimented on. (a) an inner denoising layer that linking two</cell></row><row><cell cols="3">residual blocks with bottleneck, (b) an inner denoising layer that linking two residual blocks without bottleneck and (c) a</cell></row><row><cell>middle denoising layer that denoising the input of the last part without bottleneck.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Algorithm 1 Detail Training Procedures Input: Clean images x clean , regenerate image x clean , noisy image x noisy , label y, predict label ŷ, optimizer opt, updated parameters θ, learning rate lr, weights decay wd, seed s, other hyperparameters α 1 , α 2 , , the enhanced CNN FPD (including the image restoration module FPD R , the front denoising module FPD FD , the back denoising module FPD BD and the modified classification layer FPD LCC ), loss functions L 2 and crossentropy CE, random sampler RS, threshold T , epoch N Output: FPD 1: Normalize each pixel of x clean into [0, 1] 2: for i = 1 to N do</figDesc><table><row><cell>3:</cell><cell>noise := RS(s)</cell></row><row><cell>4:</cell><cell>UPDATE s</cell></row><row><cell>5:</cell><cell>CLIP noise BETWEEN [− , ]</cell></row></table><note>6:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Overall results of the exploration experiments withResNet-101 on MNIST.</figDesc><table><row><cell cols="4">Inner Denoising Layer Implanted Position Selection</cell></row><row><cell>Accuracy</cell><cell cols="3">WhiteBox BlackBox Average</cell></row><row><cell>Shallow</cell><cell>1.67%</cell><cell>31.10%</cell><cell>16.39%</cell></row><row><cell>Deep</cell><cell>2.08%</cell><cell>27.02%</cell><cell>14.55%</cell></row><row><cell></cell><cell cols="2">Denoising Approaches</cell><cell></cell></row><row><cell>Average</cell><cell>11.04%</cell><cell>15.99%</cell><cell>13.51%</cell></row><row><cell>Flip</cell><cell>1.22%</cell><cell>17.34%</cell><cell>9.28%</cell></row><row><cell>Mid</cell><cell>0.32%</cell><cell>53.77%</cell><cell>27.05%</cell></row><row><cell>Mid + Inner</cell><cell>7.44%</cell><cell>42.41%</cell><cell>24.93%</cell></row><row><cell></cell><cell cols="2">Ablation Study</cell><cell></cell></row><row><cell>F FD</cell><cell>1.67%</cell><cell>31.10%</cell><cell>16.34%</cell></row><row><cell>F FD+R</cell><cell>7.44%</cell><cell>42.41%</cell><cell>24.93%</cell></row><row><cell>F</cell><cell>25.55%</cell><cell>62.72%</cell><cell>44.14%</cell></row><row><cell cols="3">Activation Functions Selection</cell><cell></cell></row><row><cell>ReLU</cell><cell>0.29%</cell><cell>49.28%</cell><cell>24.79%</cell></row><row><cell>ELU</cell><cell>0.28%</cell><cell>61.24%</cell><cell>30.76%</cell></row><row><cell>ELU + Tanh</cell><cell>0.25%</cell><cell>69.11%</cell><cell>34.68%</cell></row><row><cell></cell><cell cols="2">Bottleneck Selection</cell><cell></cell></row><row><cell>F 2IB−Mid</cell><cell>22.21%</cell><cell>46.65%</cell><cell>34.43%</cell></row><row><cell>F 2I−Mid</cell><cell>25.55%</cell><cell>62.72%</cell><cell>44.14%</cell></row><row><cell cols="4">No. Inner Denoising Layers Selection</cell></row><row><cell>F 2IB</cell><cell>0.04%</cell><cell>13.26%</cell><cell>6.65%</cell></row><row><cell>F 4IB</cell><cell>1.97%</cell><cell>15.90%</cell><cell>8.94%</cell></row><row><cell cols="3">Training Strategy Selection</cell><cell></cell></row><row><cell>F One Phase</cell><cell>8.60%</cell><cell>51.08%</cell><cell>29.84%</cell></row><row><cell>F Two Phase</cell><cell>25.55%</cell><cell>62.72%</cell><cell>44.14%</cell></row><row><cell cols="3">ResNet-101 Enhanced by [29]</cell><cell></cell></row><row><cell>X</cell><cell>5.72%</cell><cell>62.39%</cell><cell>32.56%</cell></row></table><note>resized into 224-by-224, respectively. We normalize image pixel value into [0, 1]. ResNet-101, ResNet-50</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Robustness evaluation results (Accuracy %, Attack Time (min)) in thwarting the white-box attacks with ResNet-101 on MNIST.</figDesc><table><row><cell></cell><cell></cell><cell>85</cell><cell>0%</cell><cell>46.42</cell><cell>0%</cell><cell cols="2">56.75 94%</cell><cell>0.95 82% 53.25</cell><cell cols="2">15% 1183.67</cell><cell>6%</cell><cell>364.13 27.57% 243.72</cell></row><row><cell>O FGSM</cell><cell cols="2">43% 0.95</cell><cell>0%</cell><cell cols="5">66.43 36.63% 39.57 100% 0.95 80% 64.22</cell><cell cols="2">74% 1177.27</cell><cell>6%</cell><cell>365.17 48.52% 244.94</cell></row><row><cell>O PGD</cell><cell cols="2">92% 1.85</cell><cell>76%</cell><cell cols="4">68.37 89.88% 42.92 98%</cell><cell>1.83 92% 68.32</cell><cell>93%</cell><cell>1193.5</cell><cell>9%</cell><cell>344.38 78.56% 245.88</cell></row><row><cell>F</cell><cell>31%</cell><cell>1.5</cell><cell>0%</cell><cell>72.3</cell><cell>88%</cell><cell>212</cell><cell>98%</cell><cell>1.73 95% 98.58</cell><cell cols="3">95% 1134.38 9.62%</cell><cell>911.9 59.51% 347.48</cell></row><row><cell>F FGSM</cell><cell cols="2">42% 0.95</cell><cell>0.87%</cell><cell>119</cell><cell cols="3">46.12% 181.6 97%</cell><cell cols="4">2.25 95% 113.97 97% 1047.83 33.16% 907.3 58.74% 338.99</cell></row><row><cell>F PGD</cell><cell>87%</cell><cell>1.6</cell><cell cols="2">64.03% 115.85</cell><cell>78%</cell><cell>160</cell><cell cols="5">100% 3.17 97% 108.5 100% 1043.25 11.87% 912.37 76.84% 334.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>and Table4. Firstly, in defending against white-box attacks, F and the adversarial trained F PGD far outperform O and O PGD in accuracy for all the block-based CNNs, especially in ResNet-101 and ResNeXt-50. We notice that the performance of F PGD is not exactly satisfactory under black-box attacks, yet the outcome is not very surprising. As shown in Table4, F-based networks achieve a high accuracy under white-box attacks. Therefore, when these attacks are applied to F substitute, some attacks effectively fail, returning a large number of clean samples as adversarial examples. Given that F PGD has a lower accuracy than</figDesc><table><row><cell>ResNet-101</cell><cell>OPGD FPGD</cell><cell>89% 84%</cell><cell>87% 82%</cell><cell>87% 83%</cell><cell>86% 84%</cell><cell>80% 64%</cell><cell>86% 78%</cell><cell>86% 82%</cell><cell>87% 83%</cell><cell>86% 84%</cell><cell>81% 76%</cell><cell>88.01% 83%</cell><cell>84% 80%</cell><cell>84% 82%</cell><cell>86% 84%</cell><cell>92% 82%</cell><cell cols="2">85.12% 85.68% 82% 80.6%</cell></row><row><cell>ResNet-50</cell><cell>OPGD FPGD</cell><cell>85% 89%</cell><cell>81% 87%</cell><cell>83% 88%</cell><cell>88% 89%</cell><cell>78% 77%</cell><cell>81% 87%</cell><cell>80% 87%</cell><cell>82% 88%</cell><cell>88% 89%</cell><cell>76% 62%</cell><cell>80% 71%</cell><cell>92% 86%</cell><cell>92% 90%</cell><cell>88% 88%</cell><cell>92% 92%</cell><cell>92% 92%</cell><cell>84.87% 84.87%</cell></row><row><cell>ResNeXt-50</cell><cell>OPGD FPGD</cell><cell>96% 86%</cell><cell>92% 80%</cell><cell>93.48% 84%</cell><cell>96% 86%</cell><cell>86% 74%</cell><cell>93.45% 82%</cell><cell>92% 84%</cell><cell>94.97% 84.81%</cell><cell>96% 86%</cell><cell>90% 80%</cell><cell>92% 84%</cell><cell>84% 66%</cell><cell>86% 62%</cell><cell>92% 86%</cell><cell>92% 80%</cell><cell>94% 82%</cell><cell>91.59% 80.05%</cell></row><row><cell cols="2">Average Acc</cell><cell>88.17%</cell><cell cols="9">84.83% 86.41% 88.17% 76.5% 84.58% 85.17% 86.63% 88.17% 77.5%</cell><cell>83%</cell><cell>82%</cell><cell cols="5">82.67% 87.33% 88.33% 87.85% 84.61%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>L ∞ Metrics: Robustness evaluation results (Accuracy %) in thwarting the black-box attacks with ResNet-101, ResNet-50 and ResNeXt-50 on SVHN.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>WhiteBox</cell><cell></cell><cell></cell></row><row><cell cols="2">Network Name</cell><cell cols="3">L ∞ ( = 8/256.0) FGSM PGD C&amp;W</cell><cell>C&amp;W</cell><cell cols="2">L 2 DeepFool Average</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell></row><row><cell></cell><cell>O</cell><cell>1%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>28%</cell><cell>5.8%</cell></row><row><cell>ResNet-101</cell><cell>O PGD F</cell><cell>57% 44%</cell><cell>36% 44%</cell><cell>39% 71%</cell><cell>1% 62.22%</cell><cell>3% 53.25%</cell><cell>27.2% 54.89%</cell></row><row><cell></cell><cell>F PGD</cell><cell>48%</cell><cell cols="3">47% 72.57% 77.7%</cell><cell>57%</cell><cell>60.45%</cell></row><row><cell></cell><cell>O</cell><cell>4%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>34%</cell><cell>7.6%</cell></row><row><cell>ResNet-50</cell><cell>O PGD F</cell><cell>55% 33%</cell><cell>26% 30%</cell><cell>28% 61%</cell><cell>0% 52.03%</cell><cell>11% 36.78%</cell><cell>24% 42.56%</cell></row><row><cell></cell><cell>F PGD</cell><cell>39%</cell><cell>35%</cell><cell>70%</cell><cell>73.3%</cell><cell>45.38%</cell><cell>52.54%</cell></row><row><cell></cell><cell>O</cell><cell>13%</cell><cell>0%</cell><cell>0%</cell><cell>0.4%</cell><cell>51.24%</cell><cell>12.93%</cell></row><row><cell>ResNeXt-50</cell><cell>O PGD F</cell><cell>58% 80%</cell><cell>36% 80%</cell><cell>46% 86%</cell><cell>4.5% 86%</cell><cell>24.27% 83.17%</cell><cell>33.75% 83.03%</cell></row><row><cell></cell><cell>F PGD</cell><cell>80%</cell><cell>78%</cell><cell>86%</cell><cell>84.15%</cell><cell>84%</cell><cell>82.43%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Robustness evaluation results (Accuracy %) in thwarting the white-box attacks with ResNet-101, ResNet-50 and ResNeXt-50 on SVHN.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">†  The author is also affiliated with Shandong Computer Science Center, Shandong Academy of Sciences, School of Cyber Security, Qilu University of Technology, China ‡ https://github.com/GuanlinLee/FPD-for-Adversarial-Robustness</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Substitute:O Substitute:F Substitute:O Substitute:F Substitute:O Substitute:F FGSM PGD C&amp;W FGSM PGD FGSM PGD C&amp;W FGSM PGD FGSM PGD C&amp;W FGSM PGD Average Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<idno>CoRR, abs/1802.00420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Decision-Based Adversarial Attacks: Reliable Attacks against Black-Box Machine Learning Models</title>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICLR</title>
				<meeting>of the ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Non-Local Algorithm for Image Denoising</title>
		<author>
			<persName><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the CVPR</title>
				<meeting>of the CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards Evaluating the Robustness of Neural Networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Security and Privacy (SP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<idno>CoRR, abs/1705.07263</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suiyi</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Callet</forename></persName>
		</author>
		<idno>CoRR, abs/1904.01231</idno>
		<title level="m">Adversarial Attacks against Deep Saliency Models</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Boundary Attack++: Query-efficient Decision-based Adversarial Attack</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>CoRR, abs/1904.02144</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image Quilting for Texture Synthesis and Transfer</title>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGGRAPH</title>
				<meeting>of the SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved Robustness to Adversarial Examples using Lipschitz Regularization of the Loss</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Finlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">M</forename><surname>Oberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Abbasi</surname></persName>
		</author>
		<idno>CoRR, abs/1810.00953</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Countering Adversarial Images using Input Transformations</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICLR</title>
				<meeting>of the ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the CVPR</title>
				<meeting>of the CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Limitations of the Lipschitz Constant as a Defense against Adversarial Examples</title>
		<author>
			<persName><forename type="first">Todd</forename><surname>Huster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cho-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritu</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><surname>Chadha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial Logit Pairing</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Harini Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
		<idno>CoRR, abs/1803.06373</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks with Layer Reuse</title>
		<author>
			<persName><forename type="first">Okan</forename><surname>Köpüklü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Hörmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICIP</title>
				<meeting>of ICIP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial Examples in the Physical World</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the CVPR</title>
				<meeting>of the CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards Deep Learning Models Resistant to Adversarial Attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno>CoRR, abs/1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Magnet: a Two-pronged Defense against Adversarial Examples</title>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGSAC</title>
				<meeting>of the SIGSAC</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepfool: a Simple and Accurate Method to Fool Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the CVPR</title>
				<meeting>of the CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the CVPR</title>
				<meeting>of the CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Maria-Irina</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Sinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Ngoc Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beat</forename><surname>Buesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryant</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Edwards</surname></persName>
		</author>
		<idno>v0.10.0. CoRR, abs/1807.01069</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Adversarial Robustness Toolbox</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Practical Black-Box Attacks against Machine Learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Berkay</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AsiaCCS</title>
				<meeting>of the AsiaCCS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonlinear Total Variation Based Noise Removal Algorithms</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Leonid I Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: nonlinear phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICLR</title>
				<meeting>of the ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ian Goodfellow, and Rob Fergus. Intriguing Properties of Neural Networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ensemble Adversarial Training: Attacks and Defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICLR</title>
				<meeting>of the ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NIPS</title>
				<meeting>of the NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature Denoising for Improving Adversarial Robustness</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the CVPR</title>
				<meeting>of the CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the CVPR</title>
				<meeting>of the CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Interpreting Adversarial Examples by Activation Promotion and Suppression</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaoyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/1904.02057</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Defense: Training DNNs with Improved Adversarial Robustness</title>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NIPS</title>
				<meeting>of the NIPS</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
