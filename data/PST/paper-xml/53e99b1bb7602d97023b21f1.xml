<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spectral Domain-Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<addrLine>800 Dongchuan Road</addrLine>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<addrLine>800 Dongchuan Road</addrLine>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
							<email>grxue@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<addrLine>800 Dongchuan Road</addrLine>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<email>qyang@cse.ust.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<addrLine>800 Dongchuan Road</addrLine>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>Las Vegas</addrLine>
									<postCode>2008</postCode>
									<settlement>Nevada</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spectral Domain-Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F8EE0806F6471226A6C649A70DB4A47E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I</term>
					<term>2</term>
					<term>6 [Artificial Intelligence]: Learning Algorithms, Experimentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional spectral classification has been proved to be effective in dealing with both labeled and unlabeled data when these data are from the same domain. In many real world applications, however, we wish to make use of the labeled data from one domain (called in-domain) to classify the unlabeled data in a different domain (out-of-domain). This problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain. In general, this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain. In this paper, we formulate this domain-transfer learning problem under a novel spectral classification framework, where the objective function is introduced to seek consistency between the in-domain supervision and the out-of-domain intrinsic structure. Through optimization of the cost function, the label information from the in-domain data is effectively transferred to help classify the unlabeled data from the out-of-domain. We conduct extensive experiments to evaluate our method and show that our algorithm achieves significant improvements on classification performance over many state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Spectral learning methods such as normalized cut <ref type="bibr" target="#b28">[28]</ref> are increasingly being applied to many learning tasks such as document clustering and image segmentation. Exploiting the information in the eigenvectors of a data similarity matrix to find the intrinsic structure, spectral methods have been extended from unsupervised learning to supervised/semisupervised learning <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b19">19]</ref>, where a unified framework is used for spectral classification (SC). The SC algorithm has been shown to be effective when the data consist of both labeled and unlabeled data.</p><p>However, a limitation of these traditional SC methods is that they only focus on the scenario that the labeled and unlabeled data are drawn from the same domain, i.e., with the same bias or feature space. Unfortunately, many scenarios in the real world do not follow this requirement. In contrast to these methods, in this paper, we aim at extending traditional spectral methods to tackle the classification problem when labeled and unlabeled data come from different domains. There are several reasons for why it is important to consider this domain-transfer learning problem, which is an instance of transfer learning <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b7">7]</ref>. First, the labeled information is often scarce in a target domain, while a lot of available labeled data may exist from a different but related domain. In this case, it would be desired to make maximal use of the labeled information, though their domains are different. For example, suppose that our task is to categorize some text articles, where the labeled data are Web pages and the unlabeled data are Blog entries. This task is important in practice, since there are much fewer labeled Blog articles than Web pages. These two kinds of articles may share many common terms, but the statistical observations of words may be quite different, as blog articles tend to use informal words. Second, the data distribution in many domains changes with time. Thus, classifiers trained during one time period may not be applicable to another time period again. Take spam email filtering for an example. The topics of spam/ham emails often evolve with time. Therefore the labeled data may fall into one set of topics whereas the unlabeled data other topics. Because traditional SC algorithms often fail to generalize across different domains, we must design new ways to deal with the cross-domain classification problem.</p><p>This paper focuses on transferring spectral classification models across different domains. Formally speaking, the training data are from a domain Din and the test data are from another domain Dout. Din is called in-domain and Dout out-of-domain in order to highlight the crossing of the domains where the label set is the same. In addition, it is assumed that in-domain Din and out-of-domain Dout are related to make the domain-transfer learning feasible. Our objective is to classify the test data from out-of-domain Dout as accurately as possible using the training data from indomain Din.</p><p>Although several cross-domain classification algorithms have been proposed, e.g., <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b14">14]</ref>, they are all based on local optimization. When the labeled and unlabeled data are not sufficiently large, their optimization function may have a lot of local minima and bring much difficulty for classification. In this paper, a spectral domain-transfer learning method is proposed, where we design a novel cost function from normalized cut, so that the in-domain supervision is regularized by out-of-domain structural constraints. By optimizing this cost function, two objectives are simultaneously being followed. On one hand, we seek an optimal partition of the data that respect the label information, where the labels are considered in the form of must-link constraints <ref type="bibr" target="#b31">[31]</ref>; that is, the corresponding data points with respect to each constraint must be with the same label. On the other hand, the test data are split as separately as possible in terms of the cut size within the test set, which will facilitate the classification process. To sum up, the supervisory knowledge is used to ensure the correctness when searching for the optimal cut of all data points. At the same time, the data points in the test set are also separated with small cut sizes. To achieve this aim, a regularization form is introduced to combine both considerations, resulting in an effective transferring of the labeled knowledge towards out-of-domain Dout.</p><p>We set out to test our proposed algorithm for domaintransfer learning empirically, where our algorithm is referred as the Cross-Domain Spectral Classifier (abbreviated by CDSC).</p><p>In our experiments, we set up eleven domain-transfer problems to evaluate our method. Compared against several state-of-the-art algorithms, our method achieves great improvements on the competent methods.</p><p>The rest of this paper is organized as follows. Spectral methods are reviewed in Section 2. Section 3 dives into details of our method. In Section 4, our method is evaluated compared with other classifiers. Following related work discussed in Section 5, Section 6 concludes this paper with some future work discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES ON SPECTRAL METHODS</head><p>Spectral clustering is aimed at minimizing the inter-cluster similarity and maximizing the intra-cluster connection. Several criteria were proposed to quantify the objective function, such as Ratio Cut <ref type="bibr">[8]</ref>, Normalized Cut (NCut) <ref type="bibr" target="#b28">[28]</ref>, Min-Max Cut (MCut) <ref type="bibr" target="#b15">[15]</ref>. Using graph theory terminology, the data are modeled as vertices and the edges are valued using the similarity of the endpoints. We denote V as the universe of all examples and V = A ∪ B where {A, B} is a partition of V . The goal is to find a partition that optimizes the cost function as follows:</p><formula xml:id="formula_0">FRatioCut = cut(A, B) |A| + cut(A, B) |B| , FNCut = cut(A, B) assoc(A, V ) + cut(A, B) assoc(B, V ) , FMCut = cut(A, B) assoc(A) + cut(A, B) assoc(B) .</formula><p>Here, assoc(A, V ) = P i∈A,j∈V wij, assoc(A) = assoc(A, A), cut(A, B) = P i∈A,j∈B,A∩B=∅ wij , where wij represents the similarity between data points i and j. Take normalized cut as an example. The numerator cut(A, B) measures how loosely the set A and B are connected, while the denominator assoc(A, V ) measures how compact the entire data set is. <ref type="bibr" target="#b28">[28]</ref> presents its equivalent objective in matrix representation as</p><formula xml:id="formula_1">FNCut = y T (D -W )y y T Dy ,</formula><p>where W is the similarity matrix, D = diag(W e) (e is a vector with all coordinates 1) and y is the indicator vector of the partition. Since solving the discrete-valued problem is NP-hard, y is relaxed to be continuous. Minimization of this cost function can be done via Rayleigh quotient <ref type="bibr" target="#b16">[16]</ref>. Given a Laplacian (L = D -W ) of a graph, the second smallest eigenvector y1 meets the optimization constraint <ref type="bibr" target="#b9">[9]</ref>. As to the discretization, linear order search <ref type="bibr" target="#b28">[28]</ref> and other variant search methods (e.g. linkage differential order <ref type="bibr" target="#b15">[15]</ref>) are commonly used to derive the cluster membership.</p><p>Another approach was proposed in <ref type="bibr" target="#b25">[25]</ref> which first normalizes the eigenvectors and then applies the K-Means clustering method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CROSS-DOMAIN SPECTRAL CLASSIFICATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>For conciseness and clarity, in this paper we mainly focus on binary classification on textual data across different domains. Extensions can be easily done for more classes and other domains. Two document sets Sin and Sout are collected from domains Din and Dout, respectively. We also denote S = Sin ∪ Sout. In the binary classification setting, the label set is {+1, -1}, meaning that c(di) equals +1 (positive) or -1 (negative) where c(di) is di's true class label. The objective is to find the hypothesis h which satisfies h(di) = c(di) for as many di ∈ Sout as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Objective Function</head><p>In our approach, the main idea is to regularize two objectives, namely, minimizing the cut size on all the data with the least inconsistency of the in-domain data, and at the same time maximizing the separation of the out-of-domain data. Intuitively, the regularization is regarded as the balance between the in-domain supervision and the out-of-domain structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Supervision from In-domain</head><p>Let n = |S| be the size of the whole sample. A similarity matrix Wn×n is calculated according to a certain similarity measure. Then, the supervisory information is incorporated in the form of must-link constraints by building a constraint matrix U , described in more details in the next subsection. In order to measure the quality of a partition, the cost function for all the data is defined as</p><formula xml:id="formula_2">F1 = x T (D -W )x x T Dx + β||U T x|| 2 , (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where D = diag(W e) is defined as previously mentioned and x is the indicator vector of the partition. In Equation (1), the normalized cut is adopted for the first term and a penalty term β||U T x|| 2 is used to guarantee a good partition on the training data. The first term represents the association between two classes. The second term β||U T x|| 2 will constrain the partition of training data since any violation of constraints results in penalty regarding F1 in Equation <ref type="bibr" target="#b1">(1)</ref>.</p><p>The parameter β controls the enforcement of constraints. This cost function is similar to that proposed in <ref type="bibr" target="#b19">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Structure of Out-of-domain Data</head><p>In Equation ( <ref type="formula" target="#formula_2">1</ref>), F1 mainly focuses on the labeled data. However, we wish to classify the out-of-domain test data correctly. Thus, it is important to find the optimal partition for the test data as well. The cost function for the test data alone is defined as</p><formula xml:id="formula_4">F2 = x T (Ds -Ws)x x T Dsx , (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where Ds = diag(Wse), and Ws is the similarity matrix for test data only. Note that the dimension of Ws is n, similarity entries only within test data are kept, i.e. if node i and j are both in the test data then W s (ij) = W (ij) ; other entries are set to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Integrating In-domain and Out-of-domain via Regularization</head><p>Now a regularization parameter is introduced, incorporating Equation (1) and Equation ( <ref type="formula" target="#formula_4">2</ref>) to get the unified cost function for cross-domain classification:</p><formula xml:id="formula_6">FCDSC = F1 + λF2 (3) = x T (D -W )x x T Dx + β||U T x|| 2 + λ x T (Ds -Ws)x x T Dsx ,</formula><p>where λ is a tradeoff parameter for balancing the supervisory information (Equation ( <ref type="formula" target="#formula_2">1</ref>)) and the cut size of the test data (Equation ( <ref type="formula" target="#formula_4">2</ref>)). The first term F1 ensures a good classification model should maximize the correctness of labeled data. In the domain-transfer setting, we cannot completely rely on the in-domain data. The second term F2 can be understood as the domain-transfer fitting constraint, which means a good classification model should also keep the test data with adequately good separation. The tradeoff between these competing conditions is captured by the parameter λ, which interestingly, allows the classification model to be balanced between in-domain Din and out-ofdomain Dout. In Equation (3), when λ = 0, the overall cost function degenerates into a spectral cost function over all the data in a semi-supervised manner; when λ is large enough, the overall objective is biased towards optimizing only the spectral cost function for the test data without any supervisory knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Incorporating Constraints</head><p>In Equation ( <ref type="formula">3</ref>), a penalty for violations <ref type="bibr" target="#b31">[31]</ref> of the supervisory constraints is introduced. In the binary classification setting, assume there are n1 positive data and n2 negative data in the training set. The constraint matrix U is constructed as follows:</p><formula xml:id="formula_7">U = [u1, u2, . . . , um] , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where each ui is an n-dimentional vector (same row index as W ) with two non-zero entries. Each column u k has an entry of +1 in the ith row, -1 in the jth row and the rest are all zero, which represents a pairwise constraint (data i and data j must be with the same label). Therefore U has m = n1 ×(n1 -1)/2+n2 ×(n2 -1)/2 columns (constraints).</p><p>The detailed construction of the constraint matrix U is presented in Algorithm 1. It is easily seen with the indicator</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 FormConstraintMatrix</head><p>Input : the size of positive data n1, the size of negative data n2 and n = n1 + n2; here, without loss of generality, we assume the first n1 examples are positive, and the next n2 examples are negative.</p><formula xml:id="formula_9">Output : Constraint Matrix U Let U be a n × n 1 (n 1 -1)+n 2 (n 2 -1) 2 matrix. Let colN um = 1.</formula><p>Construct the matrix column by column.</p><formula xml:id="formula_10">for i ← 1 to n1 do for j ← i + 1 to n1 do U (i, colN um) = 1 U (j, colN um) = -1 colN um = colN um + 1 end for end for for i ← n1 + 1 to n1 + n2 do for j ← i + 1 to n1 + n2 do U (i, colN um) = 1 U (j, colN um) = -1 colN um = colN um + 1 end for end for return U vector x that U T x = 0 ,<label>(5)</label></formula><p>when x satisfies all the constraints. Adding this constraint component into the Normalized Cut criterion <ref type="bibr" target="#b28">[28]</ref>, the cost function becomes Equation <ref type="bibr" target="#b1">(1)</ref>. One problem of the constraint matrix U is that the matrix U (with m rows) is greatly oversized, which makes it hard to compute</p><formula xml:id="formula_11">U = UU T in Equation (3) (||U T x|| 2 = x UU x).</formula><p>To alleviate this oversize problem, U can be directly built by considering the pairwise property of the constraints. Notice that U ij is the inner product of ith row and jth row of U . Then U ij has four cases:</p><formula xml:id="formula_12">U ij = 8 &gt; &gt; &lt; &gt; &gt; :</formula><p>n1 -1, i = j and they are both in positive class; n2 -1, i = j and they are both in negative class; -1, i = j and i, j are in the same class; 0, otherwise.</p><p>where n1 is the size of positive data and n2 is the size of negative data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>In this section, the optimization of the overall function (Equation (3)) is addressed.</p><p>Since Equation ( <ref type="formula">3</ref>) is difficult to optimize, we have to seek an approximation. In this work, we use x T (Ds-Ws)x</p><p>x T Dx instead of x T (Ds-Ws)x</p><p>x T Dsx in FCDSC. Usually, x T (Ds-Ws)x</p><p>x T Dx might mislead the normalized cut on Sout. However, in FCDSC, when F1 is sufficiently optimized, the partition of in-domain training data will be more or less balanced due to the constraint β||U T x|| 2 , and thus the balancing functionality of the denominator x T Dx is reduced on only out-of-domain test data (refer to x T Dsx). Then, we have</p><formula xml:id="formula_13">FCDSC ≈ x T (D -W )x x T Dx + β||U T x|| 2 + λ x T (Ds -Ws)x x T Dx = x T [(D -W ) + λ(Ds -Ws)]x x T Dx + β||U T x|| 2 . (6)</formula><p>The similarity matrix is thus modified by amplifying the similarity inside the test data submatrix. In the interpretation through random walk <ref type="bibr" target="#b24">[24]</ref>, this modification can be seen as increasing the transition probability inside the test data.</p><p>Replacing</p><formula xml:id="formula_14">y T = x T D 1/2 /||x T D 1/2 ||, x T (D -W )x x T Dx = y T D -1/2 (D -W )D -1/2 y . (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>Similarly,</p><formula xml:id="formula_16">x T (Ds -Ws)x x T Dx = y T D -1/2 (Ds -Ws)D -1/2 y . (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>With Equation ( <ref type="formula" target="#formula_10">5</ref>),</p><formula xml:id="formula_18">U T D -1/2 y = 0 . (<label>9</label></formula><formula xml:id="formula_19">)</formula><p>Combining Equations ( <ref type="formula" target="#formula_14">7</ref>), ( <ref type="formula" target="#formula_16">8</ref>) and ( <ref type="formula" target="#formula_18">9</ref>), we obtain</p><formula xml:id="formula_20">FCDSC = x T [(D -W ) + λ(Ds -Ws)]x x T Dx + β||U T x|| 2 = y T D -1/2 [(D -W + λ(Ds -Ws)]D -1/2 y + β||U T D -1/2 y|| 2 = y T D -1/2 [(D -W ) + βUU T + λ(Ds -Ws)]D -1/2 y = x T [(D -W ) + βUU T + λ(Ds -Ws)]x x T Dx = x T T x x T Dx , (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>where T = (D -W ) + βUU T + λ(Ds -Ws). Then, FCDSC can be minimized by solving an eigen-system:</p><formula xml:id="formula_22">T x = dDx , (<label>11</label></formula><formula xml:id="formula_23">)</formula><p>where d is the eigenvalue. Moreover, Equation <ref type="bibr" target="#b11">(11)</ref> can also be rewritten into</p><formula xml:id="formula_24">D -1/2 T D -1/2 y = dy . (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>Similar to other spectral methods, y is relaxed to be a realvalued vector. To this end, our problem has been transformed into the minimization of</p><formula xml:id="formula_26">x T (D -1/2 T D -1/2 )x x T x</formula><p>, which is called Rayleigh Quotient. In <ref type="bibr" target="#b16">[16]</ref>, we have Lemma 1 (Rayleigh Quotient). Let A be a real symmetric matrix. Under the constraint that x is orthogonal to the j -1 smallest eigenvectors x1, . . . , xj-1, the quotient</p><formula xml:id="formula_27">x T Ax</formula><p>x T x is minimized by the next smallest eigenvector xj and its minimum value is the corresponding eigenvalue dj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Furthermore, we can prove</head><p>Lemma 2. T = D -1/2 T D -1/2 is symmetric and its eigenvectors are orthogonal.</p><p>Proof. Since D -W , Ds -Ws and UU T are all symmetric, T = (D -W ) + βUU T + λ(Ds -Ws) is therefore symmetric. With the diagonal matrix D -1/2 , T is also symmetric.</p><p>Specifically, let v, w be arbitrarily two different eigenvectors of T and dv, dw be corresponding eigenvalues which are thus different.</p><formula xml:id="formula_28">dvv T w = (T v) T w = v T (T w) = dwv T w .</formula><p>Since dv = dw, v T w should be equal to 0. This implies that v and w are orthogonal.</p><p>By Lemma 1 and Lemma 2, the k smallest orthogonal eigenvectors of T = D -1/2 T D -1/2 are used after row normalization. Each data point is represented by the corresponding row. </p><formula xml:id="formula_29">U = FormConstraintMatrix(n1, n2, n). 3: Find the k smallest eigenvectors x1, x2, • • • , x k of T = D -1/2 (D-W +βUU T +λ(Ds-Ws))D -1/2 and construct a matrix X = D -1/2 (x1, x2, • • • , x k ). 4: Normalize X by row into Y where Yij = Xij / q P k l=1 X 2 il . 5:</formula><p>Call F with input of the eigenvectors to obtain the classification result.</p><p>In Algorithm 2, we firstly prepare the data matrix and constraint matrix. Then the cost function is optimized by solving an eigen-system (Equation ( <ref type="formula" target="#formula_22">11</ref>)). Finally, we use a traditional classifier for the final prediction, which is similar to the procedure in <ref type="bibr" target="#b22">[22]</ref>.Empirically, our algorithm improves several other state-of-the-art classifiers as will be shown in the experiment part (Section 4).</p><p>The major computational cost of the above algorithm is for computing the eigenvectors. The eigenvectors can be obtained by Lanczos method, whose computational cost is proportional to the number of nonzero elements of the target matrix. Thus the cost of our algorithm is O(kNLnnz(T )), where k denotes the number of eigenvectors desired, NL is the number of Lanczos iteration steps and nnz(T ) is the number of non-zero entries in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Study</head><p>Figure <ref type="figure" target="#fig_0">1</ref> plots the rec vs talk data (data details will be presented in Section 4.1) represented by the two smallast eigenvectors using our algorithm CDSC. The data points in the figure are sufficiently separated for classification since the eigenvectors contain the needed structural information. Moreover, the training and test data are similar in terms of Euclidean distance. In this way, the approximate decision boundary can be easily detected (the dashed line) and, as a result, good performance is obtained using our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1:</head><p>The composition of all the data sets. Since there are too many subcategories in Reuters-21578, we omit the composition details of last three data sets here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>Our method is evaluated extensively on several data sets with the training and test data from different domains. As we will show later, our method outperforms several state-ofthe-art classifiers in all the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>The cross-domain data sets are generated in specific strategies using 20 Newsgroups 1 , Reuters-21578 2 and SRAA 3 . The basic idea of our design is utilizing the hierarchy of the data sets to distinguish domains. Specifically, the task is defined as top-category classification. Each top category is split into two disjoint parts with different sub-categories, one for training and the other for test. Because the training and test data are in different subcategories, they are across domains as a result. To reduce the computational burden, we sampled 500 training and 500 test examples for each task.</p><p>1 http://people.csail.mit.edu/jrennie/20Newsgroups/ 2 http://www.daviddlewis.com/resources/testcollections/ 3 http://www.cs.umass.edu/˜mccallum/data/sraa.tar.gz</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">20 Newsgroups</head><p>The 20 Newsgroups is a text collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups nearly evenly. Six different data sets are generated for evaluating cross-domain classification algorithms. For each data set, two top categories<ref type="foot" target="#foot_0">4</ref> are chosen, one as positive and the other as negative. Then, the data are split based on sub-categories. Different sub-categories can be considered as different domains, while the task is defined as top category classification. The splitting strategy ensures the domains of labeled and unlabeled data related, since they are under the same top categories. Besides, the domains are also ensured to be different, since they are drawn from different sub-categories. Table <ref type="table">1</ref> shows how the data sets are generated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Reuters-21578</head><p>Reuters-21578 is one of the most famous test collections for evaluation of automatic text categorization techniques. It contains 5 top categories. Among these categories, orgs, people and places are three big ones. For the category places, all the documents about the USA are removed to make the three categories nearly even, because more than a half of the documents in the corpus are in the USA sub-categories. Reuters-21578 corpus also has hierarchical structure. We generated three data sets orgs vs people, orgs vs places and people vs places for cross-domain classification in a similar way as what have been done on the 20 Newsgroups. Since there are too many sub-categories, the detailed description cannot be listed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">SRAA</head><p>SRAA is a Simulated/Real/Aviation/Auto UseNet data set for document classification. 73,218 UseNet articles are collected from four discussion groups about simulated autos (sim-auto), simulated aviation (sim-aviation), real autos (real-auto) and real aviation (real-aviation). Consider the task that aims to predict labels of instances between real and simulated. The documents in real-auto and sim-  auto are used as in-domain data, while real-aviation and sim-aviation as out-of-domain data. Then, the data set real vs sim is generated as shown in Table <ref type="table">1</ref>. Therefore all the data in the in-domain data set are about auto, while all the data in the out-of-domain set are about aviation. The auto vs aviation data set is generated in the similar way as shown in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Verification of Data Sets</head><p>To verify our data design, the error rates are recorded using the SVM classifier in the scenario of domain-transfer learning (Din-Dout) as well as the single-domain classification case within the out-of-domain and within the indomain, respectively. Under the column "SVM" in Table <ref type="table" target="#tab_0">2</ref>, the three groups of classification results are displayed in the sub-columns. The column "Din-Dout" means that the classifier is trained on in-domain data and tested on out-of-domain data. The next two columns "Dout-CV" and "Din-CV" show the best results by the SVM classifier obtained during 10-fold cross validation. In these two experiments, the training and test data are extracted from the same domain, out-of-domain Dout and in-domain Din respectively. Note that the error rates under the Din-Dout column is much worse than the ones under Dout-CV and Din-CV. This implies that our data sets are not applicable for traditional classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Methods</head><p>To verify the effectiveness of our classifier, the supervised learner SVM is set as the baseline method. Our method is also compared to several semi-supervised classifiers, including Transductive SVM (TSVM) <ref type="bibr" target="#b20">[20]</ref>, Spectral Graph Transducer (SGT) <ref type="bibr" target="#b21">[21]</ref> and Spectral Classifier (SC) <ref type="bibr" target="#b22">[22]</ref>. Note that <ref type="bibr" target="#b22">[22]</ref> is approximately a special case CDSC with λ = 0. We also compare to the co-clustering based classification (CoCC) <ref type="bibr" target="#b10">[10]</ref> as the state-of-the-art domain-transfer learning algorithm and one representative selection bias correction (KDE) <ref type="bibr" target="#b29">[29]</ref>. CoCC builds connection between in-domain and out-of-domain through feature clustering, and is formulated under the co-clustering framework. KDE corrects the domain bias in the in-domain, and then adapts the indomain classification model to out-of-domain. We use test error rate as the evaluation measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>On the textual data designed in Section 4.1, we have con-ducted preprocessing procedures including tokenizing text into bag-of-words, converting text into low-case words, stopword removal and stemming using the Porter stemmer <ref type="bibr" target="#b26">[26]</ref>. Each document di in S is represented by a feature vector using Vector Space Model. Each feature represents a term, which is weighted by its tf-idf value. Feature selection is carried out by thresholding Document Frequency <ref type="bibr" target="#b34">[34]</ref>. In our experiments, Document Frequency threshold is set to 3, and the final result is not sensitive to it. The cosine similarity measure</p><formula xml:id="formula_30">x i •x j |x i ||x j |</formula><p>is adopted when constructing the similarity matrix.</p><p>The comparison methods are implemented by SVM light5 and SGT light<ref type="foot" target="#foot_2">6</ref> . All parameters are set default by the software. The Spectral Classifier (SC) is implemented according to <ref type="bibr" target="#b22">[22]</ref>. CoCC uses the same initialization and parameters in <ref type="bibr" target="#b10">[10]</ref>. KDE is implemented according to <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b35">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Performance</head><p>By comparing with the traditional supervised classifier, it is observed that the cross-domain data present much difficulty in classification, where SVM (training on in-domain Din and testing on out-of-domain Dout) made more than 20% average prediction errors. In Table <ref type="table" target="#tab_0">2</ref>, we observe that the TSVM and SGT always outperformed the supervised classifier SVM. The semi-supervised classifiers worked better since they used the unlabeled data in the classification process, so that they captured more information in the out-ofdomain. However, semi-supervised learning still works under the identical-domain assumption, and thus its improvement is limited. The situations are similar in SC. CoCC improves a lot over the traditional classification algorithm, since CoCC is a cross-domain classification algorithm, and it effectively transfers knowledge across different domains. KDE shows few improvement against SVM in our experiments, although it can effectively correct selection bias between two different domains. In our opinion, KDE fails to improve much in domain-transfer learning because the domain difference may be affected by the selection bias very few. In general, our algorithm CDSC is a spectral domaintransfer learning method, and achieves the best performance against all the comparison methods. Compared to the state-     of-the-art domain-transfer learning algorithm CoCC, CDSC also shows superiority in this experiments. We believe, it is because the data size in our experiment is not so large, and spectral learning is much more superior in learning with small data than many other learning methods.</p><p>However, in some data sets the performance is not satisfactory. For example, this can be observed in orgs vs places. This can be attributed to less common knowledge between in-domain and out-of-domain data. Our method requires that the in-domain and out-of-domain should be related, namely that they share some knowledge. If this condition cannot be satisfied, the quality of transferred knowledge will not be guaranteed. As to the tasks derived from the 20 Newsgroups, the in-domain and out-of-domain data may share a large amount of common knowledge which leads to better performance, despite the fact that other methods failed in most cases. In general, our algorithm can alleviate the classification difficulty better when the in-domain and out-of-domain are not the same albeit related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Parameter Tuning</head><p>There are two parameters in our method: β adjusts the enforcement of supervisory constraints; λ represents the tradeoff of transferring knowledge into the target domain. We tested 5 different values of β when λ is fixed. λ is enumerated from 0.0125 to 0.2 with 5 log-scale values with fixing β. We use the average error rate through 11 tasks for evaluation. From Figure <ref type="figure" target="#fig_3">2</ref>, it can be seen that, empirically the best λ is between [0.0125, 0.05], and we set λ = 0.025 in our experiments. From Figure <ref type="figure" target="#fig_5">3</ref>, the performance of CDSC is not very sensitive to β, and we set β = 15 in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Eigenvectors</head><p>The eigenvectors obtained in the classification process represent the original information approximately in a different  feature space. In this work, the optimal number is found by enumerating the number of used eigenvectors empirically. Figure <ref type="figure">4</ref> illustrates the error rates of several data sets against different numbers of eigenvectors used for classification. From the figure, it can be seen that, generally, the classification on 6 eigenvectors shows the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Varying the Size of the Training Data</head><p>We have also investigated the influence by the size of training examples. Take comp vs sci data set for example (Figure <ref type="figure">5</ref>). We chose a portion of examples in the training data randomly ranging from 100 examples to all of the samples (500). We observe that SVM, TSVM and SC often performed, in general, increasingly worse when the number of training examples decreases. In contrast to these baselines, the error rate curve of our algorithm is generally stable. This indicates our algorithm CDSC can better deal with the data sparsity problem. More importantly, CDSC tops the performance over almost all trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Similarity Pattern</head><p>Spectral methods promise to draw the similar data points nearer by representing the original data in the eigen-space. But how does this projection work on cross-domain data? To answer this question, we illustrate the similarity pattern of the original data, the projected data in Spectral Classifier (SC) <ref type="bibr" target="#b22">[22]</ref> and the projected data in our method (CDSC). Take the data set rec vs talk for example. The data are indexed firstly by category and secondly by training and test, namely positive training, positive test, negative training and negative test in order. Figure <ref type="figure" target="#fig_8">6</ref>(a) displays the documentdocument similarity matrix of the original data valued by the cosine measure, which has a threshold by the mean of this matrix. The latter two patterns are similarly thresholded. In Figure <ref type="figure" target="#fig_8">6</ref>(b), it is shown that SC fails to draw the data within the same category more similar. Figure <ref type="figure" target="#fig_8">6</ref>(c) plots the similarity matrix of the projected data using our method. The projected data show more obvious block-like behavior within the same category. On the contrary to the SC pattern, the data from same category become similar while the data from different categories become dissimilar although the whole data are across domains. It is mainly attributed to our novel objective function, which also considers the out-of-domain separation. This block-like behavior indicates that the supervisory knowledge from another domain can be used directly and effectively. In this way, the classifier will find the decision boundary more easily and more accurately and hence perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Spectral Methods</head><p>In addition to these unsupervised learning methods (described in Section 2), <ref type="bibr" target="#b19">[19]</ref> developed a semi-supervised spectral clustering algorithm by incorporating the prior knowledge. In a supervised manner, <ref type="bibr" target="#b22">[22]</ref> designs a spectral learning framework for classification. To represent the supervisory knowledge, the similarity between two same-label data points is set to 1. An one-nearest-neighbor classifier is applied to the data represented by eigenvectors. <ref type="bibr" target="#b22">[22]</ref> also showed how to make spectral classification to achieve better performance by adding more labeled and unlabeled data. Compared to these methods in <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b19">19]</ref>, our method is derived from spectral clustering, but the eigenvectors are used for classification instead of clustering. A difference from <ref type="bibr" target="#b22">[22]</ref> is that our method classifies the unlabeled data based on the label information from a different domain, while <ref type="bibr" target="#b22">[22]</ref> focuses on learning within a single domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transfer Learning</head><p>Transfer learning has been introduced to handle the learning problem where learning and prediction are in different scenarios. The idea of transfer learning is inspired by the intuition that humans often learn better when they have learned well in related domains. For instance, a good checker player may find it easier to learn to play chess. Previous works in transfer learning include "learning how to learn" <ref type="bibr" target="#b27">[27]</ref> ,"learning one more thing" <ref type="bibr" target="#b30">[30]</ref> and "multi-task learning" <ref type="bibr" target="#b7">[7]</ref>, which laid the initial foundations. <ref type="bibr" target="#b2">[2]</ref> presented the notion of relatedness between learning tasks, which provided theoretical justifications for transfer learning. In our problem setting, we aim to accomplish the same task (i.e. learn with the same label set) in different domains, which is called multi-domain or domain-transfer learning -a special case of transfer learning.</p><p>The domain-transfer learning can be classified into two categories according to whether the out-of-domain supervision is given. <ref type="bibr" target="#b32">[32]</ref> investigated how to exploiting auxiliary data in k-Nearest-Neighbors and SVM algorithm. They used the term "auxiliary data" to refer to the in-domain data and their experiments have demonstrated that the learning performance can be significantly improved with the help of auxiliary data. <ref type="bibr" target="#b14">[14]</ref> utilized additional "in-domain" labeled data to train a statistical classifier under the Conditional Expectation-Maximization framework. Those "in-domain" data play a role as auxiliary data in tackling the scarcity of "out-of-domain" training data. In these works <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b12">12]</ref>, auxiliary data serve as a supplement to the ordinary training data. In contrast to these works, our work focuses on the second category of domain-transfer learning, where the problem is classification without any training examples in the out-of-domain. Note that, in our problem, the in-domain and out-of-domain data are assumed to be relevant, in order to make the domain-transfer learning feasible. In the past, <ref type="bibr" target="#b10">[10]</ref> proposed a co-clustering based algorithm to overcome the domain difference. In this paper, we use both in-domain supervision and out-of-domain structural information to handle the domain-transfer problem through spectral learning. As we showed in the experiments, our algorithm shows superiority over <ref type="bibr" target="#b10">[10]</ref>, when the data size is not sufficiently large. Other work includes <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b5">5]</ref>.</p><p>Covariate shift <ref type="bibr" target="#b29">[29]</ref> (or sample selection bias <ref type="bibr" target="#b35">[35]</ref>) is a similar problem which occurs when samples are selected nonrandomly. Originated from the Nobel-prize work in 2000, <ref type="bibr" target="#b17">[17]</ref> made his contributions on correction of sample selection bias in econometrics. Recent researches on covariate shift include <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b3">3]</ref>. They used the instance weighting method to correct the bias. Although correcting sample selection bias <ref type="bibr" target="#b35">[35]</ref> can solve the classification when training and test data are governed by different selection bias, it still mainly focuses on learning within a single domain. Our experiments in Section 4 show that correcting sample selection bias can only improve very little in domain-transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>In this paper, a novel spectral classification based method CDSC is presented where an objective function is proposed for domain-transfer learning. In the domain-transfer setting, the labeled data from the in-domains are available for training and the unlabeled data from out-of-domains are to be classified. Based on the normalized cut cost function, supervisory knowledge is transferred through a constraint matrix, and the regularized objective function (see Equation <ref type="bibr" target="#b10">(10)</ref>) finds the consistency between the in-domain supervision and the out-of-domain intrinsic structure. The original data are then represented by a set of eigenvectors, to which a linear classifier is applied to get the final predictions. Several domain-transfer learning tasks are used to evaluate our learning method, where experimental results justify that our method is effective on handling this cross-domain classification problem.</p><p>There are several directions for future work. The CDSC is given in batch style in this paper. In the future, we would like to extend CDSC to an online cross-domain classifier. It is also important to investigate when negative transfer (domains are sufficiently dissimilar) would happen in domaintransfer learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Projected data of rec vs talk in 2dimensional eigen-space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The average error rate curve of λ when fixing β at 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The average error rate curve of β when fixing λ at 0.025.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: The error rates against the number of eigenvectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The similarity pattern on the data set rec vs talk. The data are indexed firstly by category and secondly by training and test, namely positive training, positive test, negative training and negative test in order. The document-document similarity matrix of the original data valued by the cosine measure, which has a threshold by the mean of this matrix.</figDesc><graphic coords="8,370.56,53.97,179.75,126.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 2</head><label>2</label><figDesc>Cross-Domain Spectral Classification Input : training data (n1 positive instances, n2 negative instances and n = n1 + n2) and test data, parameters {λ, β, k} and a reasonable classifier F. Output : class predictions for test data 1: Construct the similarity matrix Wn×n given both training and test data and Ws for only test data, where n to be the number of all the data. 2: Let D = diag(W e), Ds = diag(Wse) and</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : The error rate given by each classifier. Under the column "Verification of Data Set", "Din-Dout" means training on in-domain Din and testing on out-of-domain Dout; "Dout-CV" and "Din-CV" means 10-fold cross-validation on out-of-domain Dout and in-domain Din. Note that, the experimental results given by CoCC here are somewhat different from those presented in the original paper, since we sampled only 500 examples from each original data set.</head><label>2</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Three top categories, misc, soc and alt are removed, because they are too small.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>Software available at http://svmlight.joachims.org.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>Software available at http://sgt.joachims.org.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>Qiang Yang would like to thank the support of Hong Kong RGC Grant 621307. Gui-Rong Xue would like to thank Microsoft Research Asia for their support to the MSRA-SJTU joint lab project "Transfer Learning and its application on the Web". We also thank the anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting task relatedness for multiple task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative learning for differing training and test distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brückner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dirichlet-enhanced spam filtering based on biased samples</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning bounds for domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitask Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An improved two-way partitioning algorithm with stable performance [VLSI]</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1502" to="1511" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spectral Graph Theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Co-clustering based classification for out-of-domain documents</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transferring naive bayes classifiers for text classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boosting for transfer learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spectral min-max cut for graph partitioning and data clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Matrix Computation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>The Johns Hopkins University Press</publisher>
			<pubPlace>Baltimore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sample selection bias as a specification error</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="162" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Document clustering with prior knowledge</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transductive learning via spectral graph partitioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spectral learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kamvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Logistic regression with an auxiliary data source</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A random walks view of spectral segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the 8th International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping program</title>
		<author>
			<persName><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On learning how to learn learning strategies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>FKI-198-94</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Fakultat fur Informatik</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning one more thing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Clustering with instance-level constraints</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving SVM accuracy by training on auxiliary data sources</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bridged refinement for transfer learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning and evaluating classifiers under sample selection bias</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
