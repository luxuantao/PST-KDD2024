<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse, collaborative, or nonnegative representation: Which helps pattern classification?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-19">19 December 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wangpeng</forename><surname>An</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School at Shenzhen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse, collaborative, or nonnegative representation: Which helps pattern classification?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-19">19 December 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">87462C88E87947E1B48CA1FDF3BDE049</idno>
					<idno type="DOI">10.1016/j.patcog.2018.12.023</idno>
					<note type="submission">Received 12 June 2018 Revised 2 December 2018 Accepted 18 December 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Pattern classification Nonnegative representation Collaborative representation Sparse representation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use of sparse representation (SR) and collaborative representation (CR) for pattern classification has been widely studied in tasks such as face recognition and object categorization. Despite the success of SR/CR based classifiers, it is still arguable whether it is the 1 -norm sparsity or the 2 -norm collaborative property that brings the success of SR/CR based classification. In this paper, we investigate the use of nonnegative representation (NR) for pattern classification, which is largely ignored by previous work. Our analyses reveal that NR can boost the representation power of homogeneous samples while limiting the representation power of heterogeneous samples, making the representation sparse and discriminative simultaneously and thus providing a more effective solution to representation based classification than SR/CR. Our experiments demonstrate that the proposed NR based classifier (NRC) outperforms previous representation based classifiers. With deep features as inputs, it also achieves state-of-the-art performance on various visual classification tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pattern classification aims to find the correct class to which a query sample y belongs, given the training samples X from K classes. In the past decades, many pattern classification algorithms <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> have been proposed. Among different types of pattern classification methods, one major category is the representation based methods <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b16">17]</ref> . These methods first encode the query sample as a linear combination of the given training samples, and then assign the query sample to the corresponding class with the minimal distance or approximation error. One seminal work in this category is the Sparse Representation (SR) based Classifier (SRC) <ref type="bibr" target="#b10">[11]</ref> , which enforces the 1 -norm sparsity <ref type="bibr" target="#b14">[15]</ref> on the coding vector of the query sample over all the training samples. Despite its success on face recognition, it is questioned that whether it is indeed the 1 -norm sparsity that makes SRC effective <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> . To this end, the Collaborative Representation (CR) based Classifier (CRC) <ref type="bibr" target="#b11">[12]</ref> has been introduced to the pattern classification community, which reveals that it is the collaborative property induced by the 2 -norm that works for representation based pattern classification.</p><p>In SR/CR scheme based classifiers, the query sample is approximated by a linear combination of the training samples from all classes. Despite their success in face recognition, the representation based classifiers such as SRC <ref type="bibr" target="#b10">[11]</ref> , CRC <ref type="bibr" target="#b11">[12]</ref> , and their extensions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> cannot explicitly avoid negative coding coefficients within complex optimization solutions. This is because there is no explicitly positive constraint on the coding coefficients of the query sample y over the training sample matrix X . The negative coding coefficients indicate negative data correlations between the query sample and the training samples. This is to some extent counter-intuitive because the query sample y should be better approximated from the homogeneous samples to it with nonnegative representation coefficients. The negative correlations are largely from the heterogeneity samples, which would bring little benefit to the prediction of the query sample. According to the viewpoint in <ref type="bibr" target="#b23">[24]</ref> , though mathematically feasible, it is not suitable to approximate the query sample by allowing the training samples to "cancel each other out" with additions and subtractions. Recently, researchers have attempted to explain the mechanism of SRC/CRC from the perspective of probabilistic subspace <ref type="bibr" target="#b16">[17]</ref> . However, its coding scheme still cannot avoid the negative coding coefficients on the approximation of the query sample.</p><p>As validated by Belhumeur et al. <ref type="bibr" target="#b24">[25]</ref> , effective representation of the query sample should be dense over homogeneous data samples. This dense representation would be non-negative due to the homogeneity on these samples, reminiscent of non-negative matrix factorization (NMF) problem in machine learning commu- nity <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> , which aims to approximate a data matrix via multiplication of two non-negative factorial matrices. Several NMF methods are proposed in the past few years <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> , trying to get more accurate approximation. However, NMF cannot be directly utilized for pattern classification. The key reason is that in this problem, the training samples need not to be fully non-negative. Hence, we only restrict the coding coefficients to be non-negative for representation based classification, which results in Non-negative Representation (NR). In fact, NR often leads to better performance over SR/CR for data representation <ref type="bibr" target="#b23">[24]</ref> . The main reason is that nonnegative constraint in NR only allows non-negative combination of multiple training samples to reconstruct the query sample, which is compatible with the intuitive of combining parts into a whole <ref type="bibr" target="#b23">[24]</ref> .</p><p>In this work, we investigate the power of Non-negative Representation (NR) for pattern classification tasks, such as face recognition, object recognition, and action recognition, etc. Motivated from the NMF problem, our major idea is that any given query sample y can be accurately coded by the non-negative coefficients over the homogeneous samples (i.e., samples from the same class with y ), which can determine the class label of y . Constraining the coding coefficients to be non-negative can automatically boost the representational power of homogeneous samples while limiting the representation power of heterogeneous samples, making the representation sparse while discriminative. Based on above analysis, we propose a simple yet effective NR based Classifier (NRC) for pattern classification. Specifically, we utilize the non-negative constrained least square model <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> , which restricts the coding vector to be non-negative under the simple least square framework, for query sample encoding, and use the approximation residual from each class for classification. We solve the proposed NR model can be reformulated as a linear equality-constrained problem with two variables, and solved under the alternating direction method of multipliers framework <ref type="bibr" target="#b33">[34]</ref> . Each variable can be solved efficiently in closed-form, and the convergence to the global optimum can be guaranteed ( Fig. <ref type="figure" target="#fig_0">1</ref> ).</p><p>To validate our idea, we show an illustrative example in Fig. <ref type="figure" target="#fig_0">1</ref> . It can be seen that a query face sample y is encoded over 9 samples from 3 classes ( S 1 , S 2 , and S 3 ) in the YaleB dataset, 3 samples for each class. The query sample y is from class S 1 . By encoding y over the 9 samples, one can see that CRC generates dense coefficients, while SRC generates sparse coefficients but from all the 3 classes. In contrast, the coefficients generated by NRC are not only sparse but also from the same correct class. We also perform extensive comparison experiments on several benchmark datasets on face recognition, handwritten digit recognition, action recognition, object recognition, and fine grained visual classification tasks. The results demonstrate that the proposed NRC is very efficient, achieves higher accuracy than other SR/CR scheme based classifiers, and competing performance with state-of-the-art classifiers.</p><p>In summary, our contributions are as follows:</p><p>• We introduce the non-negative representation (NR) as a novel representation scheme; The rest of this paper is organized as follows: We review the related work in Section 2 . We present the proposed NRC based classifier in Section 3 . In Section 4 , we perform a comprehensive comparison on the proposed NRC classifier as well as several competing classifiers on various visual classification datasets. We conclude this paper in Section 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Representation based pattern classification</head><p>The sparse representation based classifier (SRC) <ref type="bibr" target="#b10">[11]</ref> and collaborative representation based classifier (CRC) <ref type="bibr" target="#b11">[12]</ref> , and their several extensions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> have been widely studied for various pattern classification tasks such as face recognition, handwritten digit recognition, object recognition, and action recognition, etc. Assume that we have K classes of samples, denoted by</p><formula xml:id="formula_0">{ X k } , k = 1 , . . . , K,</formula><p>where X k is the sample matrix of class k . Each column of the matrix X k is a training sample from the k -th class. The whole training sample matrix can be denoted as X = [ X 1 , . . . , X K ] ∈ R D ×N . Given a query sample y ∈ R D , both SRC and CRC compute the coding vector c of y over X by solving the following minimization problem:</p><formula xml:id="formula_1">min c y -X c 2 2 + λ c q p ,<label>(1)</label></formula><p>where p = 0 or 1, q = 1 for SRC and p = q = 2 for CRC, and λ is the regularization parameter. The coding vector c can be written as c = [ c 1 , ., c K ] , where c k , k = 1 , . . . , K is the coding sub-vector of y over the sample sub-matrix X k . Assume that the query sample y belongs to the k -th class, then it is highly possible that X k c k can give a good approximation of y , i.e., y ≈ X k c k . Therefore, SRC and CRC perform classification by computing the approximation residual of y in each class as:</p><formula xml:id="formula_2">label (y) = arg min k { y -X k c k 2 } . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>The class (here k ) with minimal residual would be the predicted class for y . The major difference between SRC and CRC lies in the coding vector c . For SRC, the coding vector c is very sparse induced by the 1 norm, and the significant coefficients are mostly fall into c k . While for CRC, c is generally very dense over all classes, this is the collaborative property induced by the 2 norm. The overall classification framework of the SRC/CRC classifiers is summarized in Algorithm 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1:</head><p>The SRC/CRC Algorithms.</p><p>Input : Training sample matrix X = [ X 1 , . . . , X K ] , query sample y; 1. Normalize the columns of X to have unit 2 norm; 2. Compute the coding vector ˆ c of y over X via ˆ c = arg min c y -Xc 2 2 + λ c q p ; 3. Compute the approximation residuals r k = y -X k ˆ c k 2 ;</p><p>Output : Label of y: label (y) = arg min k { r k } .</p><p>Due to efficient performance and elegant mathematical theory, CRC <ref type="bibr" target="#b11">[12]</ref> has been extended in several directions. In <ref type="bibr" target="#b34">[35]</ref> , the authors solved the conventional multi-class classification problem via a two-step framework, first finding the collaborative representation (CR) and then applying it to a multi-class classifier. Later, Timofte et al. <ref type="bibr" target="#b35">[36]</ref> introduced a weighted CR classifier, which sets adaptive weights for different samples or features. In <ref type="bibr" target="#b36">[37]</ref> , the authors recovered that it is still unclear that how to choose the weights and weights optimization in the method of Timofte et al. <ref type="bibr" target="#b35">[36]</ref> . Thus, they proposed a learned collaborative representation based classifier. In <ref type="bibr" target="#b16">[17]</ref> , the authors proposed a probabilistic CR based classifier, which explains the working mechanism of SRC and CRC from the perspective of probability.</p><p>In this work, we introduce a non-negative representation (NR) based scheme as an alternative to the widely studied SR/CR framework for pattern classification. The introduced NR scheme is inspired from the non-negative matrix factorization techniques <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> , which is very different from the 1 induced SR or the 2 induced CR schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Non-negative matrix factorization</head><p>Non-negative matrix factorization (NMF) is introduced by Daniel D. Lee and H. Sebastian Seung in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> for face representation, which consists in finding two non-negative factorial matrices whose multiplication is an accurate approximation of the original sample matrix. Specifically, given the sample matrix X , NMF aims to find two non-negative matrices U and V such that X ≈ UV . Here, U can be regarded as the base matrix while the V can be regarded as the encoding matrix. This is usually performed via min</p><formula xml:id="formula_4">U &gt; 0 , V &gt; 0 X -U V 2 F ,<label>(3)</label></formula><p>where • F denotes the Frobenius norm. This problem is usually solved by alternating non-negative least squares method <ref type="bibr" target="#b25">[26]</ref> , in which U and V are iteratively updated by fixing the other one. Systematic study of the solution is not the theme of this paper, please read <ref type="bibr" target="#b37">[38]</ref> for comprehensive study.</p><p>The NMF representation should be able to discover the characteristics in local parts of the original sample matrix in an additive way. The non-negative constraint in NMF only allows additive operation (without subtraction), and leads to the part-based representation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> . The two factorized non-negative matrices are called basis matrix and coefficient matrix, respectively. NMF has recently been receiving increasing attention as a framework to serve numerous tasks such as face recognition <ref type="bibr" target="#b38">[39]</ref> , graph clustering <ref type="bibr" target="#b39">[40]</ref> , and document clustering <ref type="bibr" target="#b40">[41]</ref> , etc. However, the NMF framework <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> also has its limitation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> . One unavoidable issue of NMF is that it cannot explicitly produce sparse solutions <ref type="bibr" target="#b37">[38]</ref> . In <ref type="bibr" target="#b41">[42]</ref> , the authors proposed to incorporate sparseness property into NMF and extended it to learn more specific features. Sev-eral other work have also been proposed <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> to extend NMF to broader applications. Different from these methods, in this work we only constraints the coefficients to be non-negative but allow the basis matrix to be the sample matrix, in which the samples could contain negative elements. The introduced non-negative representation based model can be solved very efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Non-negative representation based classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The non-negative representation model</head><p>The core idea of Sparse Representation based Classifier (SRC) and Collaborative Representation based Classifier (CRC) is to encode the query sample y over the whole training sample matrix X instead of each class-specific sample matrix X k , while the difference lies in that whether the constraint of 1 norm or 2 norm is imposed on the coding vector. However, SRC and CRC cannot avoid to generate negative coefficients in the coding vector. Though this is mathematically feasible, it is physically not suitable to reconstruct a sample by applying both additions and subtractions to training samples in real-world applications <ref type="bibr" target="#b23">[24]</ref> .</p><p>For representation based pattern classification task, the key problem is how to obtain a discriminative coding vector upon the training samples, which can be used to approximate the query sample from homogeneous training samples. Given a query sample y and the training sample matrix X , the discriminative coding entries of y over X should be positive upon those homogeneous training samples of y from the same class, while be zero upon those heterogeneous training samples from different classes. This is based on the fact that the homogeneous samples are usually more similar to the query sample y with positive correlations. On the contrary, the heterogeneous samples should better contribute nothing to the approximation of the query sample. In SRC/CRC, since there is no restriction on the sign of the coding coefficients, the query sample y will be approximated by complex additions and subtractions of the training samples in X from different classes, which makes the physical explanation difficult.</p><p>Based on the above discussion, we impose the non-negative constraint <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> on the coding coefficients instead of the 1norm or 2 -norm regularizations. Given the query sample y ∈ R D and the training sample matrix X consisting of samples from several classes, i.e., X = [ X 1 , . . . , X K ] ∈ R D ×N , we employ the following non-negative representation (NR) based model to find the discriminative coding vector:</p><formula xml:id="formula_5">min c y -X c 2 2 s.t. c ≥ 0 . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Here, we assume that there are K classes of samples, denoted by</p><formula xml:id="formula_7">{ X k } , k = 1 , . . . , K, where X k is the sample matrix of class k . Each column of the matrix X k is a training sample from the k -th class.</formula><p>Due to the non-negative constraint exposed on the coding vector c , the NR model in Eq. ( <ref type="formula" target="#formula_5">4</ref>) will select a few samples in X to approximate the query sample y , naturally resulting in sparsity on c . Meanwhile, it tends to find the homogeneous samples to represent y in order for accurate approximation, resulting in a discriminative representation. The ability to achieve sparsity and discriminability simultaneously makes NR a better choice than SR and CR for pattern classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model optimization</head><p>The proposed NR model is basically the non-negative least square (NNLS) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> problem, which does not have a closed-form solution. To this end, we employ the variable splitting method <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> to solve the NR model <ref type="bibr" target="#b3">(4)</ref> . By introducing an auxiliary variable z , we can reformulate the NR model into a linear (5)</p><p>The problem ( <ref type="formula">5</ref>) can be solved under the alternating direction method of multipliers (ADMM) <ref type="bibr" target="#b33">[34]</ref> framework. The Lagrangian function of the problem ( <ref type="formula">5</ref>) is</p><formula xml:id="formula_8">L (c, z, δ, ρ) = y -X c 2 2 + δ, z -c + ρ 2 z -c 2 2 , (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where δ is the augmented Lagrangian multiplier and ρ &gt; 0 is the penalty parameter. We initialize the vector variables c 0 , z 0 , and δ 0 to be zero vectors and set ρ &gt; 0 with a suitable value. Denote by ( c t , z t ) and δ t the optimization variables and the Lagrange multiplier at iteration t ( t = 0 , 1 , 2 , . . . ), respectively. The variables can be updated by taking derivatives of the Lagrangian function ( <ref type="formula" target="#formula_8">6</ref>) w.r.t. the variables c and z and setting the derivative function to be zero.</p><p>(1) Updating c while fixing z and δ:</p><formula xml:id="formula_10">min c y -X c 2 2 + ρ 2 c -(z t + ρ -1 δ t ) 2 2 . (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>This is a standard least squares regression problem with closed form solution:</p><formula xml:id="formula_12">c t+1 = X X + ρ 2 I -1 X y + ρ 2 z t + 1 2 δ t (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>(2) Updating z while fixing c and δ:</p><formula xml:id="formula_14">min z z -(c t+1 -ρ -1 δ t ) 2 2 s.t. z ≥ 0 . (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>The solution of z is</p><formula xml:id="formula_16">z t+1 = max (0 , c t+1 -ρ -1 δ t ) ,<label>(10)</label></formula><p>where the "max " operates element-wisely.</p><p>(3) Updating the Lagrangian multiplier δ:</p><formula xml:id="formula_17">δ t+1 = δ t + ρ(z t+1 -c t+1 ) . (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>The above alternative updating steps are repeated until the convergence condition is satisfied or the number of iterations exceeds a preset threshold T . The convergence condition of the ADMM algorithm is: c tz t 2 ≤ Tol , c t+1c t 2 ≤ Tol , and z t+1z t 2 ≤ Tol are simultaneously satisfied, where Tol &gt; 0 is a small tolerance value. Since the least square objective function and the linear equality and non-negative constraints are all strictly convex, the problem (5) solved by the ADMM algorithm is guaranteed to converge to a global optimal solution. We summarize the overall updating procedures in Algorithm 2 .  Convergence Analysis . The Algorithm 2 can be guaranteed to converge since the overall objective function ( <ref type="formula">5</ref>) is convex with a global optimal solution. In Fig. <ref type="figure" target="#fig_2">2</ref> , we show the convergence curve of the proposed NNLS algorithm by using the well-known MNIST </p><formula xml:id="formula_19">ρ &gt; 0 ; Initialization: c 0 = z 0 = δ 0 = 0 , t = 0 ; for t = 0 : T -1 do 1. Update c t+1 as c t+1 = (X X + ρ 2 I) -1 (X y + ρ 2 z t + 1 2 δ t ) ; 2. Update z t+1 as z t+1 = max (0 , c t+1 -ρ -1 δ t ) ; 3. Update δ t+1 as δ t+1 = δ t + ρ(z t+1 -c t+1 ) ; if (Convergence</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification</head><p>Given the query sample y ∈ R D and the training sample matrix Output : Label (y) = arg min k { r k } .</p><formula xml:id="formula_20">X = [ X 1 , . . . , X K ] ∈ R D ×N ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Relation to other problems . This work is inspired by the advantages of non-negative matrix factorization for parts-based facial analysis <ref type="bibr" target="#b23">[24]</ref> . The non-negativity property allows only the nonnegative combination of multiple training samples in X to additively reconstruct the query sample y , which is in accordance with the intuitive notion of "combining parts into a whole" introduced in <ref type="bibr" target="#b23">[24]</ref> . Non-negativity is also in accordance with the biological modeling of visual data and often leads to better performance for data representation <ref type="bibr" target="#b23">[24]</ref> . In many real-world applications, the underlying signals represented by quantities can only take nonnegative values by nature. Examples validating this point include pixel intensities in images, amounts of materials, chemical concentrations, and the compounds of end-members in hyper-spectral images, just to name a few. Even for realistic signals which may contain negative values, the representational coefficients are further required to be non-negative to avoid contradicting physical realities.</p><p>Relation with NNLS . Our NR model (4) shares the same mathematical formula with NNLS, which has been studied in <ref type="bibr" target="#b32">[33]</ref> for sparse recovery without regularization. The authors compared NNLS with the non-negative LASSO <ref type="bibr" target="#b47">[48]</ref> , and found that NNLS can achieve similar or even better performance on sparse recovery problems. Besides, the non-negative garrote has competitive performance with the subset selection method <ref type="bibr" target="#b42">[43]</ref> . The sparsity of NR can also be explained from the perspective of convex geometry. The sparsity of the non-negative constraints can be analyzed by studying the face lattice of the polyhedral cone generated by the columns of the training sample matrix <ref type="bibr" target="#b48">[49]</ref> .</p><p>Sparsity and discriminative . Like in SRC/CRC <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> , in NRC the query sample y is also coded over all the training samples. However, an evident distinction of NRC from SRC/CRC is that NRC restricts the coding coefficients to be non-negative and hence only the samples share similar structures (largely from the homogeneous class) to the query sample y can have positive coding coefficients and contribute positively to the approximation of y . Seen in this light, the NRC based classifier can be guaranteed to perform well on representation based classification tasks. Overall, we argue that NRC brings discriminative property and sparsity simultaneously, and provides an ideal model for representation based pattern classification.</p><p>Perspective of convex hull . In Eq. ( <ref type="formula" target="#formula_5">4</ref>) , the query sample y ∈ R D is consisted of non-negative combination of the homogeneous samples from X = [ X 1 , . . . , X K ] ∈ R D ×N , where X k is the sample matrix including all the samples from the k -th class ( k = 1 , ., K). Each sample in X k can be viewed as a dictionary atom from the k -th class. Since the samples in X k is very similar to each other, the linear combination of these dictionary atoms in X k can be regarded as a convex hull. Different sample matrix form different convex hulls. Given the query sample y , the non-negative constraint of y upon X will require all the reconstructed samples (or dictionary atoms) are in the interior of the correct convex hull. Hence, the non-negative constraint can induce correct representational coefficients from the correct sample matrix X k for the query sample y . Of course we can learn some discriminative yet compact dictionary atoms for each class as <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> , in this work, we employ the data samples directly as the dictionary atoms for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Complexity analysis</head><p>The Algorithm 2 and 3 may become very slow when the number of samples in X is much larger than the feature dimension, i.e., N D . In order to make the proposed NRC scalable to large scale visual datasets, we employ the well-known Woodbury Identity Theorem <ref type="bibr" target="#b51">[52]</ref> to reduce the computational cost for the inversion of the solution in Eq. <ref type="bibr" target="#b6">(7)</ref> . By this way, the update of c in (7) can be computed as follows:</p><formula xml:id="formula_21">c t+1 = 2 ρ I - 2 ρ 2 X I + 2 ρ X X -1 X X y + ρ 2 z t + 1 2 δ t ,<label>(12)</label></formula><p>which will save a lot of computational costs.</p><p>In Algorithm 2 , the cost for updating c is O( DN 2 ) due to the employment of Woodbury Identity Theorem. The cost for updating z is O(D ) . The costs for updating δ is also O(D ) . So the overall complexity for Algorithm 2 is O( DN 2 T ) , where T is the number of iterations. In Algorithm 3 for pattern classification, the inversion in Eq. ( <ref type="formula" target="#formula_10">7</ref>) is pre-stored and can be avoided in the testing stage. For each query sample y , the costs for computing the residuals can be ignored. The overall cost for Algorithm 3 is O(N 2 T ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we compare the proposed NRC with state-ofthe-art classifiers on various pattern classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>The proposed NRC algorithm has two parameters: the iteration number T and the penalty parameter ρ. With the increase of T , the accuracy will in general increase and finally converge to some rate. To balance accuracy and complexity, in all experiments we empirically set T = 5 and determine ρ by 5-fold cross validation on the training set. For the comparison methods, we use the source codes provided by the original authors, and tune their parameters to achieve their corresponding highest classification accuracies on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison methods and datasets</head><p>In Section 4.3 , we first compare NRC with other representative and state-of-the-art representation based classifiers, including NSC <ref type="bibr" target="#b9">[10]</ref> , SRC <ref type="bibr" target="#b10">[11]</ref> , CRC <ref type="bibr" target="#b11">[12]</ref> , CROC <ref type="bibr" target="#b12">[13]</ref> , and ProCRC <ref type="bibr" target="#b16">[17]</ref> , on the classical face recognition datasets including the AR dataset <ref type="bibr" target="#b52">[53]</ref> and the Extended Yale B dataset <ref type="bibr" target="#b53">[54]</ref> , and handwritten digit recognition datasets including the USPS dataset <ref type="bibr" target="#b54">[55]</ref> and the MNIST dataset <ref type="bibr" target="#b46">[47]</ref> . The linear support vector machine (SVM) classifier <ref type="bibr" target="#b55">[56]</ref> is also compared. In Section 4.4 , we compare these competing methods for action recognition task on the Stanford 40 Actions dataset <ref type="bibr" target="#b56">[57]</ref> , for object recognition task on the Caltech 256 dataset <ref type="bibr" target="#b57">[58]</ref> , and for several challenging fine grained visual classification tasks on the Caltech-UCSD Birds-200-2011 dataset <ref type="bibr" target="#b58">[59]</ref> , the Oxford 102 Flowers dataset <ref type="bibr" target="#b59">[60]</ref> , the Aircraft dataset <ref type="bibr" target="#b60">[61]</ref> , and the Cars dataset <ref type="bibr" target="#b61">[62]</ref> datasets. We also compare the proposed method with stateof-the-art methods (such as FV-CNN <ref type="bibr" target="#b62">[63]</ref> , B-CNN <ref type="bibr" target="#b63">[64]</ref> , and NAC <ref type="bibr" target="#b64">[65]</ref> , etc.) on these fine grained visual classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on face and handwritten digit classification 4.3.1. Face recognition</head><p>The AR dataset <ref type="bibr" target="#b52">[53]</ref> contains over 40 0 0 color images corresponding to 126 people's faces (70 men and 56 women) with different facial expressions, illumination conditions and occlusions. Following the settings in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> , we choose a subset with only illumination and expression changes that contains 50 male subjects and 50 female subjects from the AR dataset in our experiments. For each subject, the seven images from Session 1 were used for training, with the other seven images from Session 2 for testing. The images are cropped to 60 × 43 and normalized to have unit 2 norm. We project the images to a subspace of dimension d = 54 , 120 , 300 by using PCA. The results of classification accuracy (%) by the competing methods are listed in Table <ref type="table" target="#tab_2">1</ref> . It can be seen that the proposed NRC achieves the highest accuracy on all dimensionalities.</p><p>The Extended Yale B dataset <ref type="bibr" target="#b53">[54]</ref> has 2432 face images from 38 human subjects, each having around 64 nearly frontal images taken under different illumination conditions. The original images are of 192 × 168 pixels. We resize the images to 54 × 48 pixels and normalize the images to have unit 2 norm. As the experimental settings in <ref type="bibr" target="#b11">[12]</ref> , we randomly split the dataset into two halves. Each half contains 32 images for each person. We use one half as the training samples, and the other half as the testing samples. We project the images to a subspace of dimension d = 84 , 150 , 300 by PCA. The classification accuracies (%) of the comparison methods are listed in Table <ref type="table">2</ref> . One can see that the proposed NRC achieves almost the same performance as CROC, better than the other competing representation based classifiers. The highest accuracies are highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Classification accuracy (%) of different algorithms on the Extended Yale B dataset <ref type="bibr" target="#b53">[54]</ref> . The results are averaged on 10 independent trials. The projected dimension by PCA is d .</p><p>d NSC <ref type="bibr" target="#b9">[10]</ref> SVM <ref type="bibr" target="#b55">[56]</ref> SRC <ref type="bibr" target="#b10">[11]</ref> CRC <ref type="bibr" target="#b11">[12]</ref> CROC <ref type="bibr" target="#b12">[13]</ref> ProCRC <ref type="bibr" target="#b16">[17]</ref>  The highest accuracies are highlighted in bold. The highest accuracies are highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Handwritten digit recognition</head><p>The USPS dataset <ref type="bibr" target="#b54">[55]</ref> contains 9298 images for digit numbers from 0 to 9. The training set contains 7291 images while the testing set contains 2007 images. Each of the images in the USPS dataset is resized into 16 × 16 pixels. Similar to <ref type="bibr" target="#b16">[17]</ref> , we randomly select 50, 100, 200, and 300 images from each digit class of the training set as the training samples, and use all the samples in the testing set as the testing samples. We repeat the experiments on 10 independent trials and report the averaged results. We list the results of classification accuracy (%) by different methods in Table <ref type="table" target="#tab_4">3</ref> . It can be seen that the proposed NRC outperforms all the other competing methods on all cases. With the increasing of the number of training samples, the classification accuracy of all the comparison methods increases consistently, including the proposed NRC. However, one can see that the accuracy of NSC and ProCRC will not be further improved when the number of training samples increases from 200 to 300, while the proposed NRC can still increase from 94.6% to 95.1%.</p><p>The MNIST dataset <ref type="bibr" target="#b46">[47]</ref> contains 60,0 0 0 training images and 10,0 0 0 testing images for digit numbers from 0 to 9. The original images are of size 28 × 28. As in <ref type="bibr" target="#b16">[17]</ref> , we resize each image into 16 × 16 pixels. We randomly selected 50, 10 0, 30 0, and 60 0 samples from each class of the training set for training, and use all the samples in the testing set for testing. Different from the experimental settings in <ref type="bibr" target="#b16">[17]</ref> , for each image in the MNIST dataset, we extract its feature vector by using the scattering convolution network (SCN) <ref type="bibr" target="#b65">[66]</ref> . The feature vector is a concatenation of coefficients in each layer of the network, and is translation invariant and deformation stable. Each feature vector is of length 3472. The feature vectors for all images are then projected into a subspace of dimension 500 using PCA. We run the experiments for 10 independent trials and report the averaged classification accuracy (%). The results are listed in Table <ref type="table">4</ref> . It can be seen that the proposed NRC outperforms the comparison methods no matter how many images (50, 100, 200, or 600) from each class are chosen as the training samples. With the increase of the number of training samples, the classification accuracy of all the competing methods increases consistently. Since stronger features are extracted by using SCN <ref type="bibr" target="#b65">[66]</ref> , the classification accuracies of all comparison methods are higher than the corresponding results reported in <ref type="bibr" target="#b16">[17]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Comparison on speed</head><p>We compare the running time (in second) of the proposed NRC and the competing representation based classifiers by processing one test image on the MNIST dataset <ref type="bibr" target="#b46">[47]</ref> as described in Section 4.3.2 . We randomly selected 300 samples from each class of the training set for training, and use all the samples in the testing set for testing. The features extracted by SCN <ref type="bibr" target="#b65">[66]</ref> are projected onto a 500-dimension subspace by PCA. All experiments are run under the Matlab environment and on a machine with 3.50 GHz CPU and 32GB RAM. Table <ref type="table">5</ref> lists the running time of different methods. The ProCRC and CRC methods have closed-form solutions and have the same speed, which is faster than CROC and much faster than SRC. The proposed NRC algorithm need several iterations in the ADMM algorithm, and hence is a little slower than CRC and ProCRC, but still faster than SRC and CROC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on large scale pattern classification</head><p>To take more comprehensive evaluation of NRC on other pattern classification tasks, we compare it with the state-of-the-art methods on six challenging pattern classification datasets: the Stanford 40 Actions dataset <ref type="bibr" target="#b56">[57]</ref> for action recognition, the Caltech-256 dataset <ref type="bibr" target="#b57">[58]</ref> for large scale object recognition, and the Caltech-UCSD Birds (CUB200-2011) dataset <ref type="bibr" target="#b58">[59]</ref> , the Oxford 102 Flowers dataset <ref type="bibr" target="#b59">[60]</ref> , the Aircraft dataset <ref type="bibr" target="#b60">[61]</ref> , the Cars dataset <ref type="bibr" target="#b61">[62]</ref> for finegrained object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Datasets and settings</head><p>The Stanford 40 Actions dataset <ref type="bibr" target="#b56">[57]</ref> contains 9352 images of 40 different classes of human actions with 180 ∼ 300 images per action, e.g., brushing teeth, reading book, and cleaning the Table <ref type="table">4</ref> Classification accuracy (%) of different algorithms as a function of the number ( N ) of selected samples from each class for training on the MNIST dataset <ref type="bibr" target="#b46">[47]</ref> . The results are averaged on 10 independent trials. The samples are projected onto a 500-dimensional space by PCA. Image # NSC <ref type="bibr" target="#b9">[10]</ref> SVM <ref type="bibr" target="#b55">[56]</ref> SRC <ref type="bibr" target="#b10">[11]</ref> CRC <ref type="bibr" target="#b11">[12]</ref> CROC <ref type="bibr" target="#b12">[13]</ref> ProCRC <ref type="bibr">[</ref> The highest accuracies are highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Running time (second) of different methods on the MNIST dataset <ref type="bibr" target="#b46">[47]</ref> . The samples are projected onto a 500-dimensional space by PCA. floor, etc. Similar to the experimental settings in <ref type="bibr" target="#b16">[17]</ref> , we follow the training-testing split settings suggested in <ref type="bibr" target="#b56">[57]</ref> , and randomly choose 100 images from each class as the training samples and use the remaining images as the testing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>The Caltech-256 dataset <ref type="bibr" target="#b57">[58]</ref> is consisted of 256 categories of objects, in which there are at least 80 images for each category. This large dataset has a total number of 30,608 images. We randomly choose 30 images from each object category as the training samples, and use the remaining images as the testing samples. For fair comparison, we run the proposed NRC and the comparison methods for 10 independent trials for each partition and report the averaged classification accuracy results (%).</p><p>The Caltech-UCSD Birds (CUB200-2011) dataset <ref type="bibr" target="#b58">[59]</ref> contains 11,788 bird images, which is a widely-used benchmark for finegrained visual classification. There are totally 200 bird species and the number of images per specie is around 60. The significant variations in pose, viewpoint, and illumination inside each class make this dataset very challenging for visual classification. We adopt the publicly available split <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59]</ref> , which use nearly half of the images in this dataset as the training samples and the other half as the testing samples.</p><p>The Oxford 102 Flowers dataset <ref type="bibr" target="#b59">[60]</ref> is another fine-grained visual classification benchmark which contains 8189 images from 102 flower classes. Each class contains at least 40 images, in which the flowers appear at different scales, pose, and lighting conditions. This dataset is very challenging since there exist large variations within the same class but small variations across different classes <ref type="bibr" target="#b16">[17]</ref> .</p><p>The Aircraft dataset <ref type="bibr" target="#b60">[61]</ref> contains images of 100 different aircraft model variants, and there are 100 images for each model. The aircrafts appear at different scales, design structures, and appearances, making this dataset very difficult for visual classification task. We adopt the training-testing split protocol provided by Maji et al. <ref type="bibr" target="#b60">[61]</ref> to design our experiments.</p><p>The Cars dataset <ref type="bibr" target="#b61">[62]</ref> is consisted of 16,185 images from 196 car classes. Each class has around 80 images with different sizes and heavy clutter background, which makes this dataset challenging for pattern classification. We use the same split scheme provided by Krause et al. <ref type="bibr" target="#b61">[62]</ref> , in which 8144 images are employed as the training samples and the other 8041 images are employed as the testing samples.</p><p>Following the experimental setting in <ref type="bibr" target="#b16">[17]</ref> , on the Stanford 40 Actions dataset, the Caltech-256 dataset, the Caltech-UCSD Birds (CUB200-2011) dataset and the Oxford 102 Flowers dataset, we employ two different types of features to demonstrate the effectiveness of the proposed NRC. For the first type features, we employ the VLFeat library <ref type="bibr" target="#b66">[67]</ref> to extract the Bag-of-Words feature based on SIFT <ref type="bibr" target="#b67">[68]</ref> (refer to as BOW-SIFT feature). The size of patch and stride are set as 16 × 16 and 8 pixels, respectively. The codebook is trained by the k-means algorithm, and we use a 2level spatial pyramid representation <ref type="bibr" target="#b0">[1]</ref> . The final feature dimension of each image is 5120. For the second type features, we use VGG-verydeep-19 <ref type="bibr" target="#b68">[69]</ref> to extract CNN features (refer to VGG19 features). We use the activations of the penultimate layer as local features, which are extracted on 5 scales { 2 s , s = -1 , -0 . 5 , 0 , 0 . 5 , 1 } . We pool all local features together regardless of scales and locations. The final feature dimension of each image is 4096 for all datasets. The BOW-SIFT and VGG19 features are both 2 normalized. On the Aircraft and Cars datasets, we use the CNN features extracted by a VGG-16 network <ref type="bibr" target="#b68">[69]</ref> in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Evaluation of representation based classifiers with BOW-SIFT and VGG features</head><p>The accuracies (%) by different methods on the six datasets are listed in Table <ref type="table" target="#tab_7">6</ref> . As we can see, by using the same BoW-SIFT or VGG features, the proposed NRC achieves consistently higher accuracies than the previous representation based classifiers, including SRC <ref type="bibr" target="#b10">[11]</ref> , CRC <ref type="bibr" target="#b11">[12]</ref> , and their extensions CROC and ProCRC <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref> . Specifically, with the VGG19 features, NRC achieves performance gains of at least 1.0%, 1.1%, 0.8% and 0.5% over the other classifiers on the Stanford 40 Actions, the Caltech-256, the CUB200-2011, and the Flower 102 datasets, respectively. With the BOW-SIFT features, the corresponding performance gains of NRC over the other classifiers are at least 0.6%, 0.9%, 0.3%, and 3.2%, respectively. With the VGG16 features, the performance gains of NRC over the other classifiers are at least 0.6% and 0.5% on the Aircraft and the Cars datasets, respectively. This demonstrates that NR is indeed more effective than previous SR and CR based classification methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Comparison with state-of-the-art methods</head><p>In this section, we compare NRC, by employing the VGG19 or VGG16 features, with other representative and state-of-the-art methods on the six datasets described in Section 4.4.1 . Note that the compared CNN based methods use even stronger network architectures or features than our employed VGG19 features.</p><p>On the Stanford 40 Actions dataset, we compare the proposed NRC with the state-of-the-art methods such as AlexNet network <ref type="bibr" target="#b5">[6]</ref> , VGG19 network <ref type="bibr" target="#b68">[69]</ref> , EPM <ref type="bibr" target="#b69">[70]</ref> , and ASPD <ref type="bibr" target="#b70">[71]</ref> . Note that EPM and ASPD are the leading methods on action recognition in still images. The classification accuracies are listed in Table <ref type="table" target="#tab_8">7</ref> . One can see that NRC outperforms over the other methods by an improvement of at least 4.7%.</p><p>On the Caltech-256 dataset, we compare the proposed NRC with the state-of-the-art methods such as AlexNet network <ref type="bibr" target="#b5">[6]</ref> , The highest accuracies are highlighted in bold. The highest accuracies are highlighted in bold. The highest accuracies are highlighted in bold.  The highest accuracies are highlighted in bold.</p><p>VGG19 network <ref type="bibr" target="#b68">[69]</ref> , M-HMP <ref type="bibr" target="#b71">[72]</ref> , CNN-S <ref type="bibr" target="#b72">[73]</ref> , ZF <ref type="bibr" target="#b73">[74]</ref> , NAC <ref type="bibr" target="#b64">[65]</ref> , and SJFT <ref type="bibr" target="#b74">[75]</ref> . These methods are the leading players among the others on large scale pattern classification tasks. Note that SJFT <ref type="bibr" target="#b74">[75]</ref> is a CNN based method employing transfering learning scheme to borrow treasures from abundant training data of other domains. The results are listed in Table <ref type="table" target="#tab_9">8</ref> , one can see that NRC outperforms over other methods by an improvement of at least 0.6%. On the CUB200-2011 dataset, we compare the proposed NRC with the state-of-the-art methods such as AlexNet network <ref type="bibr" target="#b5">[6]</ref> , VGG19 network <ref type="bibr" target="#b68">[69]</ref> , POOF <ref type="bibr" target="#b75">[76]</ref> , FV-CNN <ref type="bibr" target="#b62">[63]</ref> , PN-CNN <ref type="bibr" target="#b76">[77]</ref> , and NAC <ref type="bibr" target="#b64">[65]</ref> . Note that NAC is a part based method which employ data augmentation techniques, and achieve state-of-the-art accuracy on fine-grained recognition tasks. The accuracy results are listed in Table <ref type="table" target="#tab_10">9</ref> . One can see that NRC is only slightly inferior to NAC but still outperforms the other methods.</p><p>On the Flower 102 dataset, we compare the proposed NRC with the state-of-the-art methods such as AlexNet network <ref type="bibr" target="#b5">[6]</ref> , VGG19 network <ref type="bibr" target="#b68">[69]</ref> , BiCoS <ref type="bibr" target="#b77">[78]</ref> , DAS <ref type="bibr" target="#b78">[79]</ref> , GMP <ref type="bibr" target="#b79">[80]</ref> , OverFeat <ref type="bibr" target="#b80">[81]</ref> , and NAC <ref type="bibr" target="#b64">[65]</ref> . The results are listed in Table <ref type="table" target="#tab_11">10</ref> . One can see that NRC achieves similar accuracy with NAC and at least 2.2% performance gain than the other methods. The highest accuracies are highlighted in bold.</p><p>For the Aircraft dataset and Cars dataset, we compare the proposed NRC with state-of-the-art VGG19 network <ref type="bibr" target="#b68">[69]</ref> , Symbiotic <ref type="bibr" target="#b81">[82]</ref> , FV-FGC <ref type="bibr" target="#b82">[83]</ref> , B-CNN method <ref type="bibr" target="#b63">[64]</ref> . Note that B-CNN achieves state-of-the-art performance on these two datasets. The results on classification accuracy (%) are listed in Table <ref type="table" target="#tab_12">11</ref> . One can see that, on the Aircraft dataset, the proposed NRC achieves an improvement of 3.4% over the B-CNN. On the Cars dataset, the proposed NRC achieves an improvement of 0.2% over the B-CNN.</p><p>All the above results demonstrate that, with deep CNN features, the proposed NRC can achieve comparable or even better performance than state-of-the-art methods on various visual classification datasets. It is a general and effective visual classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Sparse representation based classifier (SRC) and collaborative representation based classifier (CRC) have been well-studied for face recognition. In this work, we provided an alternative classifier for pattern classification. Specifically, we investigated the attractive property of non-negative representation (NR) for pattern classification. We revealed that NR can enhance the representational power of homogeneous samples while limiting the representational power of heterogeneous samples, naturally resulting a sparse while discriminative encoding for predicting the query sample. Based on the NR scheme, we proposed a NR based model for classification. We solve the model via variable splitting techniques under the alternating direction method of multipliers framework. Based on the NR based model, we proposed a non-negative rep-resentation based classifier (NRC). Extensive experiments on various visual classification datasets validated the effectiveness of the proposed NRC classifier, and demonstrated that NRC outperforms previous representation based classifiers such as SRC/CRC. Besides, with deep features as inputs, NRC also achieves state-ofthe-art performance on various large scale pattern classification tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustrative comparison of SRC, CRC, and NRC. (a) A query sample y and 3 classes ( S 1 , S 2 , and S 3 ), each class having 3 training samples. Note that y is from class S 1 . (b)-(c) show the coding coefficients of y over the 9 training samples by SRC, CRC and NRC, respectively.</figDesc><graphic coords="2,41.03,57.41,504.00,121.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>equality-constrained problem with two variables c and z : min c y -X c 2 2 s.t. c = z, z ≥ 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 :</head><label>2</label><figDesc>Solve the NR model (3) via ADMM. Input: Query sample y, training sample matrix X, Tol &gt; 0 , T ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The convergence curves of maximal values in entries of | c t -z t | (blue line), | c t+1 -c t | (red line), and | z t+1 -z t | (red line, roughly the same with | c t+1 -c t | ). The test set is the well-known MNIST dataset [47] . (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="4,307.52,57.17,240.00,151.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 3 :</head><label>3</label><figDesc>we first normalize y and each column of X to have unit 2 norm, and then compute the coding vector ˆ c of the query sample y over X via solving the problem (4) . The classification of y by ˆ c is similar to that of SRC/CRC (please refer to the Algorithm 1 ). We compute the class-specific representation residual y -X k ˆ c k 2 , where ˆ c k corresponds to the coding sub-vector associated with class k . The proposed Non-negative Representation based Classifier (NRC) is summarized in Algorithm 3 . The NRC Algorithm. Input : Training sample matrix X = [ X 1 , . . . , X K ] , query sample y; 1. Normalize the columns of X to have unit 2 norm; 2. Codes y over X via solving the NR model (3): ˆ c = arg min c y -Xc 2 2 s.t. c ≥ 0 ; 3. Compute the residuals r k = y -X k ˆ c k 2 ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Classification accuracy (%) of different algorithms on the AR dataset<ref type="bibr" target="#b52">[53]</ref> . The projected dimension by PCA is d .</figDesc><table><row><cell>d</cell><cell>NSC [10]</cell><cell>SVM [56]</cell><cell>SRC [11]</cell><cell>CRC [12]</cell><cell>CROC [13]</cell><cell>ProCRC [17]</cell><cell>NRC</cell></row><row><cell>54</cell><cell>70.7</cell><cell>81.6</cell><cell>82.1</cell><cell>80.3</cell><cell>82.0</cell><cell>81.4</cell><cell>86.0</cell></row><row><cell>120</cell><cell>75.5</cell><cell>89.3</cell><cell>88.3</cell><cell>90.0</cell><cell>90.8</cell><cell>90.7</cell><cell>91.3</cell></row><row><cell>300</cell><cell>76.1</cell><cell>91.6</cell><cell>90.3</cell><cell>93.7</cell><cell>93.7</cell><cell>93.7</cell><cell>94.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Classification accuracy (%) of different algorithms as a function of the number ( N ) of selected samples from each class for training on the USPS dataset<ref type="bibr" target="#b54">[55]</ref> . The results are averaged on 10 independent trials.</figDesc><table><row><cell>Image #</cell><cell>NSC [10]</cell><cell>SVM [56]</cell><cell>SRC [11]</cell><cell>CRC [12]</cell><cell>CROC [13]</cell><cell>ProCRC [17]</cell><cell>NRC</cell></row><row><cell>50</cell><cell>91.2</cell><cell>91.6</cell><cell>91.4</cell><cell>89.2</cell><cell>91.9</cell><cell>90.9</cell><cell>92.3</cell></row><row><cell>100</cell><cell>92.2</cell><cell>92.5</cell><cell>93.1</cell><cell>90.6</cell><cell>91.3</cell><cell>91.9</cell><cell>93.7</cell></row><row><cell>200</cell><cell>92.8</cell><cell>93.1</cell><cell>94.2</cell><cell>91.4</cell><cell>91.7</cell><cell>92.2</cell><cell>94.6</cell></row><row><cell>300</cell><cell>92.8</cell><cell>93.2</cell><cell>94.8</cell><cell>91.5</cell><cell>91.8</cell><cell>92.2</cell><cell>95.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Classification accuracy (%) of different classifiers on the above mentioned six datasets with BoW-SIFT or VGG features.</figDesc><table><row><cell>Methods</cell><cell cols="2">Standford 40</cell><cell cols="2">Caltech 256</cell><cell cols="2">CUB200-2011</cell><cell cols="2">Flower 102</cell><cell>Aircraft</cell><cell>Cars</cell></row><row><cell></cell><cell>SIFT</cell><cell>VGG19</cell><cell>SIFT</cell><cell>VGG19</cell><cell>SIFT</cell><cell>VGG19</cell><cell>SIFT</cell><cell>VGG19</cell><cell>VGG16</cell><cell>VGG16</cell></row><row><cell>Softmax</cell><cell>21.1</cell><cell>77.2</cell><cell>25.8</cell><cell>75.3</cell><cell>8.2</cell><cell>72.1</cell><cell>46.5</cell><cell>87.3</cell><cell>85.6</cell><cell>88.7</cell></row><row><cell>NSC [10]</cell><cell>22.1</cell><cell>74.7</cell><cell>25.8</cell><cell>80.2</cell><cell>8.4</cell><cell>74.5</cell><cell>46.7</cell><cell>90.1</cell><cell>85.5</cell><cell>88.3</cell></row><row><cell>SRC [11]</cell><cell>24.2</cell><cell>78.7</cell><cell>26.9</cell><cell>81.3</cell><cell>7.7</cell><cell>76.0</cell><cell>47.2</cell><cell>93.2</cell><cell>86.1</cell><cell>89.2</cell></row><row><cell>CRC [12]</cell><cell>24.6</cell><cell>78.2</cell><cell>27.4</cell><cell>81.1</cell><cell>9.4</cell><cell>76.2</cell><cell>49.9</cell><cell>93.0</cell><cell>86.7</cell><cell>90.0</cell></row><row><cell>CROC [13]</cell><cell>24.5</cell><cell>79.2</cell><cell>27.9</cell><cell>81.7</cell><cell>9.1</cell><cell>76.2</cell><cell>49.4</cell><cell>93.1</cell><cell>86.9</cell><cell>90.3</cell></row><row><cell>ProCRC [17]</cell><cell>28.4</cell><cell>80.9</cell><cell>29.6</cell><cell>83.3</cell><cell>9.9</cell><cell>78.3</cell><cell>51.2</cell><cell>94.8</cell><cell>86.8</cell><cell>90.1</cell></row><row><cell>NRC</cell><cell>29.0</cell><cell>81.9</cell><cell>30.5</cell><cell>84.4</cell><cell>10.2</cell><cell>79.1</cell><cell>54.4</cell><cell>95.3</cell><cell>87.5</cell><cell>90.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>Classification accuracy (%) of different methods on the Standford 40 datasets.</figDesc><table><row><cell>Methods</cell><cell>AlexNet [6]</cell><cell>EPM [70]</cell><cell>ASPD [71]</cell><cell>VGG19 [69]</cell><cell>NRC</cell></row><row><cell>Accuracy</cell><cell>68.6</cell><cell>72.3</cell><cell>75.4</cell><cell>77.2</cell><cell>81.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>Classification accuracy (%) of different methods on the Caltech-256 datasets.</figDesc><table><row><cell>Methods</cell><cell>M-HMP</cell><cell>ZF</cell><cell>AlexNet</cell><cell>CNN-S</cell><cell>NAC</cell><cell>VGG19</cell><cell>SJFT</cell><cell>NRC</cell></row><row><cell>Accuracy</cell><cell>50.7</cell><cell>70.6</cell><cell>71.1</cell><cell>74.2</cell><cell>81.4</cell><cell>82.3</cell><cell>83.8</cell><cell>84.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc>Classification accuracy (%) of different methods on the CUB200-2011 datasets.</figDesc><table><row><cell>Methods</cell><cell>AlexNet</cell><cell>POOF</cell><cell>FV-CNN</cell><cell>VGG19</cell><cell>PN-CNN</cell><cell>NAC</cell><cell>NRC</cell></row><row><cell>Accuracy</cell><cell>52.2</cell><cell>56.9</cell><cell>66.7</cell><cell>71.9</cell><cell>75.7</cell><cell>81.0</cell><cell>79.1</cell></row><row><cell cols="4">The highest accuracies are highlighted in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc>Classification accuracy (%) of different methods on the Flower 102 datasets with VGG19 features.</figDesc><table><row><cell>Methods</cell><cell cols="2">BiCoS DAS</cell><cell cols="4">GMP OverFeat AlexNet VGG19 NAC</cell><cell>NRC</cell></row><row><cell cols="2">Accuracy 79.4</cell><cell cols="2">80.7 84.6</cell><cell>86.8</cell><cell>90.4</cell><cell>93.1</cell><cell>95.3 95.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11</head><label>11</label><figDesc>Classification accuracy (%) of different methods on the Aircraft and Cars datasets with VGG16 features.</figDesc><table><row><cell>Dataset</cell><cell>VGG16 [69]</cell><cell>Symbiotic [82]</cell><cell>FV-FGC [83]</cell><cell>B-CNN [64]</cell><cell>NRC</cell></row><row><cell>Aircraft</cell><cell>85.6</cell><cell>72.5</cell><cell>80.7</cell><cell>84.1</cell><cell>87.5</cell></row><row><cell>Cars</cell><cell>88.7</cell><cell>78.0</cell><cell>82.7</cell><cell>90.6</cell><cell>90.8</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond bags of features: spatial pyramid matching for recognizing natural scene categories, in: Computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE computer society conference on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
			<date type="published" when="2006">2006. 2006</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local features are not lonely-laplacian sparse coding for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3555" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative k-svd for dictionary learning in face recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2691" to="2698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Label consistent k-svd: learning a discriminative dictionary for recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2651" to="2664" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NIPS</publisher>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pcanet: a simple deep learning baseline for image classification?</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<title level="m">Sparse representation or collaborative representation: which helps face recognition? in: Computer vision (ICCV), 2011 IEEE international conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classification and boosting with multiple collaborative representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1519" to="1531" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Color space normalization: enhancing the discriminating power of color spaces for face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1454" to="1466" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond sparsity: the role of l1-optimizer in pattern classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1104" to="1118" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and robust face recognition via coding residual map learning based adaptive masking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="535" to="543" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A probabilistic collaborative representation based approach for pattern classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2950" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint discriminative dimensionality reduction and dictionary learning for face recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2134" to="2143" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective texture classification by texton encoding induced statistical features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="447" to="457" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust, discriminative and comprehensive dictionary learning for face recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="341" to="356" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Why is facial occlusion a challenging problem</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ekenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Biometrics: Third International Conference</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Are sparse representations really relevant for image classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1545" to="1552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative collaborative representation for classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mukunoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="205" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
	<note>in nips</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convex and semi-nonnegative matrix factorizations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Subspace clustering guided convex nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page" from="38" to="48" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with quadratic programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2309" to="2320" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative nonnegative matrix factorization for dimensionality reduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsoukalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="212" to="223" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A fast non-negativity-constrained least squares algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Jong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chemom</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="393" to="401" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Slawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<idno type="DOI">10.1214/13-EJS868</idno>
	</analytic>
	<monogr>
		<title level="j">Electron. J. Statist</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3004" to="3056" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Classification and boosting with multiple collaborative representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1519" to="1531" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive and weighted collaborative representations for image classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="127" to="135" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learned collaborative representations for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="456" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization: a comprehensive review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1336" to="1353" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatially localized, parts-based representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Document clustering by concept factorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="202" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image classification by non-negative sparse coding, low-rank and sparse decomposition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1673" to="1680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Better subset regression using the nonnegative garrote</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="373" to="384" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dissimilarity-based sparse subset selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2182" to="2197" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Variational methods for the solution of problems of equilibrium and vibrations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Courant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="293" to="318" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sparse recovery by thresholded non-negative least squares</title>
		<author>
			<persName><forename type="first">M</forename><surname>Slawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1926" to="1934" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fisher discrimination dictionary learning for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sparse representation based fisher discrimination dictionary learning for image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Accuracy and stability of numerical algorithms</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">80</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The ar face database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
		<idno>No. 24</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">CVC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">From few to many: illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Liblinear: a library for large linear classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Vlfeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1469" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Expanded parts model for human attribute and action recognition in still images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="652" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Recognizing actions through action-specific person detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="422" to="424" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multipath sparse coding using hierarchical matching pursuit</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="660" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="955" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Bicos: A bi-level co-segmentation method for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2579" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Efficient object detection and segmentation for finegrained recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="811" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Generalized max pooling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2473" to="2480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Symbiotic segmentation and part localization for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Revisiting the fisher vector for fine-grained classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="92" to="98" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">His research interests include image restoration, subspace clustering, sparse, and low rank models. Wangpeng An received the B.E. degree from Kunming University of Science and Technology in 2012. He received the M.Sc. degree in 2018 from the Tsinghua University</title>
	</analytic>
	<monogr>
		<title level="s">Jun Xu received the B.Sc. degree in pure mathematics and the M.Sc. degree in Information and Probability from the School of Mathematics Science</title>
		<editor>
			<persName><forename type="first">Qionghai</forename><surname>Professor</surname></persName>
		</editor>
		<editor>
			<persName><surname>Dai</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011 and 2014</date>
			<pubPlace>China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Nankai University ; The Hong Kong Polytechnic University</orgName>
		</respStmt>
	</monogr>
	<note>He received the Ph.D. degree in 2018 fromthe Department of Computing. His research interests include face attribute recognition, generative adversarial networks and deep learning optimization</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Since July 2017, he has been a Chair Professor in the same department. His research interests include Computer Vision, Pattern Recognition, Image and Video Analysis, and Biometrics, etc. Prof. Zhang has published more than 200 papers in those areas. As of 2018, his publications have been cited more than 30,0 0 0 times in the literature</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Shenyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>China</surname></persName>
		</author>
		<author>
			<persName><surname>Sc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Ph</surname></persName>
		</author>
		<author>
			<persName><surname>China</surname></persName>
		</author>
		<ptr target="http://www4.comp.polyu.edu.hk/cslzhang" />
	</analytic>
	<monogr>
		<title level="m">2006, he joined the Department of Computing, The Hong Kong Polytechnic University, as an Assistant Professor</title>
		<imprint>
			<date type="published" when="1998">1998. 2001. 2001 to 2002. January 2003 to January 2006</date>
		</imprint>
		<respStmt>
			<orgName>The Hong Kong Polytechnic University ; Department of Electrical and Computer Engineering, McMaster University</orgName>
		</respStmt>
	</monogr>
	<note>SIAM Journal of Imaging Sciences and Image and Vision Computing. Clarivate Analytics Highly Cited Researcher&quot; from 2015 to 2017. More information can be found in his homepage</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">He is currently the Chair Professor with the Hong Kong Polytechnic University, since 2005, where he is the Founding Director of the Biometrics Research Centre (UGC/CRC) supported by the Hong Kong SAR Government in 1998. He is a Croucher Senior Research Fellow, Distinguished Speaker of the IEEE Computer Society, and a Fellow of IAPR. So far, he has published over 20 monographs, over 400 international journal papers and over 40 patents from USA/Japan/HK/China. He was selected as a Highly Cited Researcher in Engineering by Thomson Reuters in 2014</title>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">F&apos;08) received the B.Sc. degree in computer science from Peking University, the M.Sc. degree in 1982, and the Ph</title>
		<meeting><address><addrLine>Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Series on Biometrics</publisher>
			<date type="published" when="2015">2015. 2016</date>
		</imprint>
		<respStmt>
			<orgName>Visiting Chair Professor with Tsinghua University and an Adjunct Professor with Peking University, Shanghai Jiao Tong University, HIT, and the University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note>He is Founder and Editor-in-Chief. International Journal of Image and Graphics, the Founder and the Series Editor</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m">International Conference on Biometrics Authentication, an Associate Editor for over ten international journals including the IEEE Transactions</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
