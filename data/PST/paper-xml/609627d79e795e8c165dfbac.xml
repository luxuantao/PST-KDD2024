<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Multiple Attention Network for Semantic Segmentation in Aerial Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-15">15 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Ruigang</forename><surname>Niu</surname></persName>
							<email>niuruigang18@mails.ucas.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenhui</forename><surname>Diao</surname></persName>
							<email>diaowh@aircas.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Kaiqiang</forename><surname>Chen</surname></persName>
							<email>chenkaiqiang14@mails.ucas.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xian</forename><forename type="middle">Sun</forename><surname>Niu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Aerospace Information Research Institute</orgName>
								<orgName type="laboratory">Laboratory of Network Information System Technology (NIST)</orgName>
								<orgName type="institution" key="instit1">are with the Aerospace In-formation Research Institute</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country>China, the Key</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Electronic, Electrical and Communication Engineering</orgName>
								<orgName type="institution" key="instit1">University of Chi-nese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sci-ences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Aerospace Information Research Insti-tute</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Key Lab-oratory of Network Information System Technology (NIST)</orgName>
								<orgName type="institution" key="instit1">Aerospace In-formation Research Institute</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid Multiple Attention Network for Semantic Segmentation in Aerial Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-15">15 Sep 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2001.02870v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic segmentation</term>
					<term>Aerial imagery</term>
					<term>Deep convolution neural networks</term>
					<term>Self-attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation in very high resolution (VHR) aerial images is one of the most challenging tasks in remote sensing image understanding. Most of the current approaches are based on deep convolutional neural networks (DCNNs). However, standard convolution with local receptive fields fails in modeling global dependencies. Prior researches have indicated that attention-based methods can capture long-range dependencies and further reconstruct the feature maps for better representation. Nevertheless, limited by the mere perspective of spacial and channel attention and huge computation complexity of self-attention mechanism, it is unlikely to model the effective semantic interdependencies between each pixel-pair of remote sensing data of complex spectra. In this work, we propose a novel attention-based framework named Hybrid Multiple Attention Network (HMANet) to adaptively capture global correlations from the perspective of space, channel and category in a more effective and efficient manner. Concretely, a class augmented attention (CAA) module embedded with a class channel attention (CCA) module can be used to compute category-based correlation and recalibrate the class-level information. Additionally, we introduce a simple yet effective region shuffle attention (RSA) module to reduce feature redundant and improve the efficiency of selfattention mechanism via region-wise representations. Extensive experimental results on the ISPRS Vaihingen and Potsdam benchmark demonstrate the effectiveness and efficiency of our HMANet over other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S EMANTIC segmentation, also known as semantic label- ing, is one of the fundamental and challenging tasks in remote sensing image understanding, whose goal is to assign pixel-wise semantic class labels for a given image. In particular, semantic segmentation in very high resolution (VHR) aerial images plays an increasingly significance for its widespread applications, such as road extraction <ref type="bibr" target="#b0">[1]</ref>, urban planning <ref type="bibr" target="#b1">[2]</ref> and land cover classification <ref type="bibr" target="#b2">[3]</ref>.</p><p>In recent years, Deep Convolutional Neural Networks (DC-NNs) have demonstrated the powerful capacity of feature extraction and object representations compared with traditional methods in machine learning, such as Random forests (RF) <ref type="bibr" target="#b3">[4]</ref>, Support Vector Machine (SVM) <ref type="bibr" target="#b4">[5]</ref> and Conditional Random Fields (CRFs) <ref type="bibr" target="#b5">[6]</ref>. Particularly, state-of-the-art methods based on the Fully Convolutional Network (FCN) <ref type="bibr" target="#b6">[7]</ref> have made great progress. However, due to the fixed geometry structured, they are inherently limited by local receptive fields and shortrange context information. This task is still very challenging.</p><p>To capture long-range dependencies, such as correlation coefficients between long-distance pixels, Chen et al. <ref type="bibr" target="#b7">[8]</ref> proposed atrous spatial pyramid pooling (ASPP) with multiscale dilation rates to aggregate contextual information. Zhao et al. <ref type="bibr" target="#b8">[9]</ref> further introduced the pyramid pooling module (PPM) to represent the feature map via multiple regions with different sizes. ScasNet <ref type="bibr" target="#b9">[10]</ref> aggregates multi-scale contexts in a self-cascade manner. Nevertheless, the context aggregation methods above are still unable to extract global contextual information, that is, it is unsatisfactory to cover global receptive fields by stacking and aggregating convolutional layers.</p><p>Furthermore, in order to generate dense and pixel-wise contextual information, Non-local Neural Networks <ref type="bibr" target="#b12">[11]</ref> utilizes a self-attention mechanism, which enables a single feature from any position to perceive features from all other positions. It can be seen as a matter of feature reconstruction, that is, the feature representation of each position is a weighted sum of all other counterparts. DANet <ref type="bibr" target="#b13">[12]</ref> introduces spatial-wise and channelwise attention modules to enrich the feature representations. Besides, several works <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b16">15]</ref> improve the efficiency of the self-attention mechanism to some extent.</p><p>Semantic segmentation is essentially a pixel-by-pixel classification task, which requires the network to have a large fields-of-view. Attention-based methods have been proved to be effective ways to obtain global fields-of-view and contexts in semantic segmentation. However, the standard self-attention mechanism has many limitations in modeling effective semantic dependencies between each pixel-pair of remote sensing data of complex spectra. Hence, inspired by the success of attention-based methods above, and considering its limitations, we introduce multiple attention modules into a segmentation network to enrich the perspective of attention extraction and optimize the huge computational complexity of the selfattention mechanism.</p><p>Concretely, pixel-wise attention approaches need to generate a dense attention map to measure the relationships between each pixel-pair, which has a high computation complexity and occupies a huge number of GPU memory. Recent works [ <ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b17">16]</ref> have shown the fact that information redundancy is not conducive to feature representations. What's more, attentionbased methods are restricted to the perspective of space and channel, ignoring category-based information, which is a key factor for semantic segmentation task. The category-based information is directly related to the last convolution of the network. In general, lack of category-based information and huge computation complexity of self-attention mechanism are two tough problems and will be elaborated below.</p><formula xml:id="formula_0">(i) (ii) (iii) (iv) (v)</formula><p>On one hand, for remote sensing images of complex spectral, the class-level information is usually directly reflected between different spectra, and the input data itself has sufficient class-level information. The category-based information is embedded in different spectra, namely different channels of the input feature. But in previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">17,</ref><ref type="bibr">18]</ref>, the category-based information in the general segmentation network is only reflected in the last convolutional layer, that is, the score map representing the probability that each pixel belongs to each category. In other words, through the complex feature extraction and representation of the middle stage of the network, the category-based information of the input data is already ambiguous or missing. Empirically, the lack of classlevel information leads to poor object classification capabilities. Hence, different from other attention-based methods, we argue that retaining the category-based information in the middle stage of the network and extracting the corresponding attention representations. We propose a so-called categorybased correlation that models class-level representation of each pixel and further calculates the relationships between categories and corresponding channels of the feature cube. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, category-based correlation mainly focuses on exploiting contextual information from a categorical perspective, which pays more attention to the pixels of the same category during the feature reconstruction.</p><p>On the other hand, a tricky problem in remote sensing images is that the feature representations of objects with the same category are quite different in complex scenes. Therefore, the pixel-wise attention tends to extract the wrong similarity relationship between pixels, leading to serious classification errors. Besides, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> (b), it has a high computation complexity and occupies a huge number of GPU memory. Several works <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b17">16]</ref> have proved that the invalid redundant information is not conducive to the feature representations. For example, as for a single feature belonging to 'car' in Fig. <ref type="figure" target="#fig_1">2</ref> (a), the pixel-wise attention method usually extracts features of all other positions, among which we actually do not need to focus on the 'building' and 'impervious surface', and it is more likely to extract the wrong similarity because of the complex scenes (such as in the shadow or overlapping). Aiming at the problems above, we employ a more robust region-wise attention mechanism to exploit a wider range of correlations. Empirically, region-wise representation can capture long-range contextual information between pixels in a more efficient manner.</p><p>Towards the above two issues and our corresponding solutions, we propose a novel framework, named Hybrid Multiple Attention Network (HMANet). The HMANet mainly consists of two parallel branches, one of which is the Class Augmented Attention (CAA) module embedded with the Class Channel Attention (CCA) module. Given the input feature, the CAA module first calculates category-based correlation and further generates the weighted class representation via a dense class affinity map. While the CCA module is added to adaptively recalibrate the class-level information through two linear scaling transformation functions, which efficiently helps to enhance the discriminative abilities for each class with a few parameters. The other branch of our network is the Region Shuffle Attention (RSA) module, which aims to capture region-wise global information with a shuffling operator and obtain more robust correlation between objects. Besides, compared with pixel-wise self-attention methods, the grouped region-wise representation requires 20× less GPU memory usage and significantly reduces FLOPs by about 77% with a few parameters. Finally, we concatenate the output features from each branch and the local representation, and then feed them into a convolutional layer to further generate the fine segmentation map.</p><p>Our contributions can be summarized as follows:</p><p>1) We present a Class Augmented Attention (CAA) module to exploit the category-based correlation between pixels and enhance the discriminant ability for each class, within which a Class Channel Attention (CCA) module is embedded to recalibrate the class-level information for better representations adaptively.</p><p>2) The Region Shuffle Attention (RSA) module is proposed to capture region-wise global information and obtain more robust relationships between objects in a more efficient and effective manner. 3) We propose a novel Hybrid Multiple Attention Network (HMANet) by taking advantage of the three attention modules above, which comprehensively captures feature correlations from the perspective of space, channel and category.</p><p>The reminder of this paper is arranged as follows. Related work is briefly introduced in Section. II. Section. III presents the details of our proposed method, including three attention modules, respectively. Experimental evaluations between our HMANet and the state-of-the-art methods, as well as ablation studies on Vaihingen dataset are provided in Section. IV. Finally, the conclusion is outlined in Section. V</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>A full review is beyond the scope of this paper. Here, we review some recent works on semantic segmentation of nature scenes and remote sensing images. Then we turn to attentionbased approach that is more relevant to our work. Semantic Segmentation. Semantic segmentation is one of the fundamental tasks of image understanding. Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b6">[7]</ref> based methods have made great progress in semantic segmentation by leveraging the powerful representation abilities of classification networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> pretrained on large-scale data <ref type="bibr" target="#b20">[21]</ref>. Several model variants are proposed to aggregate multi-scale contextual information that is vital for object perception. Concretely, DeepLabv2 [17] and DeepLabv3 <ref type="bibr" target="#b7">[8]</ref> employ atrous spatial pyramid pooling (ASPP) to embed contextual representation, which consists of parallel convolutions with different dilated rates. PSPNet <ref type="bibr" target="#b8">[9]</ref> proposes a pyramid pooling module (PPM) to extract the contextual information with different scales, each of which can be considered the global representation. UNet <ref type="bibr" target="#b21">[22]</ref>, RefineNet <ref type="bibr" target="#b22">[23]</ref>, DFN <ref type="bibr" target="#b23">[24]</ref>, SegNet <ref type="bibr" target="#b24">[25]</ref>, DeepLabv3+ <ref type="bibr" target="#b25">[26]</ref> and SPGNet <ref type="bibr" target="#b26">[27]</ref> adopt encoder-decoder structure to carefully recover the location information while retaining highlevel semantic features. GCN <ref type="bibr" target="#b27">[28]</ref> utilizes global convolutional module and global pooling to harvest context information for global representations. In addition, BiSeNet <ref type="bibr" target="#b28">[29]</ref> adopts efficient spatial and context path to achieve real-time semantic segmentation.</p><p>Semantic Segmentation of Aerial Imagery. Semantic segmentation in VHR aerial images benefits a lot from deep learning methods. For example, Mou et al. <ref type="bibr" target="#b29">[30]</ref> propose two network units, spatial relation module and channel relation module, to learn relationships between any two positions. TreeUNet <ref type="bibr" target="#b30">[31]</ref> adopts a Tree-CNN block to transmit feature maps via concatenating connections and further fuse multiscale representations. ScasNet <ref type="bibr" target="#b9">[10]</ref> proposes an end-to-end self-cascade network to improve the labeling coherence with sequential global-to-local contexts aggregation. SDNF <ref type="bibr">[18]</ref> combines DCNNs and traditional decision forests algorithm in an end-to-end manner to achieve better classification accuracy. Marmanis et al. <ref type="bibr" target="#b31">[32]</ref> focus on semantically edge detection to restore high-frequency details and further obtain fine object boundaries. DSMFNet <ref type="bibr" target="#b32">[33]</ref> proposes a lightweight DSM fusion module to effectively aggregate depth information, within which Cao et al. <ref type="bibr" target="#b32">[33]</ref> investigate four fusion strategies corresponding to different scenarios.</p><p>Attention-based Methods. Attention is widely used for various tasks, such as machine translation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, scene classification and semantic segmentation. Squeeze-and-Excitation Networks <ref type="bibr" target="#b35">[36]</ref> recalibrated the feature representations by modeling the dependencies between channels. Non-local <ref type="bibr" target="#b12">[11]</ref> first adopts self-attention mechanism as a submodule for computer vision tasks, i.e., video classification, object detection and instance segmentation. CCNet <ref type="bibr" target="#b14">[13]</ref> harvests the contextual information of all the positions by stacking two serial crisscross attention module. DANet <ref type="bibr" target="#b13">[12]</ref> adopts similar spatial and channel attention module to generate information from all pixels, which costs even more computation and GPU memory than the Non-local operator <ref type="bibr" target="#b12">[11]</ref>. A 2 -Nets <ref type="bibr" target="#b17">[16]</ref> and Expectation-Maximization Attention Networks <ref type="bibr" target="#b15">[14]</ref> sample sparse global descriptors to reconstruct the feature maps in an self-attention mechanism. ACFNet <ref type="bibr" target="#b36">[37]</ref> proposes a coarseto-fine segmentation network based on attention class feature module, which can be embedded in any base network. Huang et al. <ref type="bibr" target="#b16">[15]</ref>, Yuan et al. <ref type="bibr" target="#b37">[38]</ref> and Zhu et al. <ref type="bibr" target="#b38">[39]</ref> further improve the efficiency of self-attention mechanism for semantic segmentation.</p><p>Motivated by the success of the attention-based methods above, and considering its limitations, we rethink the attention mechanism from the view of different perspectives and computation cost. Different from the previous works, we propose a Hybrid Multiple Attention to capture global contexts from the perspective of space, channel and category respectively for better feature representations. Moreover, benefiting from the multi-perspective attention mechanism and region-wise representations, HMANet is more efficient and effective than other attention-based methods. Comprehensive empirical results verify the superiority of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the network architecture mainly consists of three attention modules, Class Augmented Attention (CAA) module, Class Channel Attention (CCA) module and Region Shuffle Attention (RSA) module, among which CAA module and CCA module are embedded together as the upper branch of the network. The proposed CAA module aims to extract the class-level information while the CCA module improves the process of feature reconstruction via class channel weighting for better contextual representation. The lower branch of the network is the RSA module, accordingly, which greatly decreases the computational consumption and memory footprint in contrast to the original non-local block in computing longrange dependencies.</p><p>Concretely, given an input image, we first feed it into a convolutional neural network (CNN) to adaptively extract features for better representation, which is designed in a fully convolutional manner <ref type="bibr" target="#b6">[7]</ref>. We take ResNet-101 pretrained on ImageNet dataset as our backbone. In particular, we remove the last two down-sampling operations and use dilated convolutions in stage-3 and stage-4, which is also called a multi-grid strategy for the latter, thereby retaining more spatial information and enlarging the output feature map X to 1/8 of the input image without adding extra parameters. Then the features X from the stage-4 of the backbone would be fed into two parallel attention branches.</p><p>The upper branch is the CAA module embedded with the CCA module. The CAA module is designed to model the dependencies between specific categories and the corresponding features after the dimension reduction, that is, extract the similarity relationships between each category and each channel of the input feature through matrix operations. It helps to obtain a fine-grained feature representation that is more sensitive to object category information and enhance the discriminative ability of the network. The CCA module can be defined as the adaptive feature reconstruction (see Eq. ( <ref type="formula" target="#formula_6">4</ref>)) of class channel information, which can effectively improve the feature representation of category information. It is worth mentioning that the CCA module takes the class affinity matrix (see Eq. ( <ref type="formula" target="#formula_1">1</ref>)) and class attention map as the input features, both of which are generated by the CAA module, then, obtains the adaptive weighted class affinity matrix. Ideally, given the input feature map X ∈ R C×H×W , in which C, H and W denote the number of channels, height and width of feature map respectively, the CAA embedded with CCA module can effectively extract the class-channel correlation and adaptively aggregate long-range contextual information from a category view, eventually, outputs the same size feature map Y ∈ R C×H×W following the self-attention scheme <ref type="bibr" target="#b12">[11]</ref>.</p><p>The lower branch of the network, RSA module, is proposed with the intuition of decomposing the dense point-wise affinity matrix into two sparse region-based counterparts, either of which could efficiently capture the global context in a sparser way via adaptive average pooling method. With the combination of the two affinity matrices, the RSA module could capture abundant spatial contextual information of the local input feature X then output feature Z ∈ R C×H×W . Finally, we concatenate the output features of the two branches {Y, Z} and the local feature representation X to obtain better feature representations, then, the fused features are fed into a convolutional layer to generate the fine segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Class Augmented Attention</head><p>The self-attention mechanism is essentially a kind of matrix multiplication operation in mathematics, in which the two dimensions are the number of channels {C} and the product of height and width {H × W } of the input feature map respectively. The standard channel affinity matrix of size C×C can be obtained by the matrix multiplication of two inputs with dimension C × HW and HW × C, such as channel attention module in DANet <ref type="bibr" target="#b13">[12]</ref>. Intuitively, the definition of non-local operation constrains the scaling of the channel in such kind of channel attention module, that is, the query, key and value functions are eliminated during the operation. Nevertheless, it leads into category information when one of the channels C is replaced by the channel corresponding to the segmentation map supervised by the ground truth, retaining the query, key and value transformation functions in the meantime.</p><p>The intuition of the proposed class augmented attention is to capture long-range contextual information from the perspective of category information, that is, to explicitly model the relationships between each category in the dataset and each channel of the input feature cube. Next, we will elaborate the process to capture class-level contextual information.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, given a local feature X ∈ R C×H×W , output from the 3 × 3 conv after stage-4 of ResNet in our implementation, the class augmented attention module first applies two convolutional layers to generate two feature maps X ∈ R C ×H×W , and P ∈ R N ×H×W , respectively, where C is the reduced channel number of the local feature for less computational cost and P is the class attention map supervised by the ground-truth segmentation. For each channel k in P ∈ R N ×H×W , P k ∈ R H×W is available to represent the confidence that pixels of all position i belongs to class k, where N is the number of categories. the position i in spatial dimension of X and P after a softmax layer. The class affinity operation is defined as follows:</p><formula xml:id="formula_1">s u,k = i x u,i e p k,i N j=1 e pj,i<label>(1)</label></formula><p>where s u,k ∈ S denotes the explicit class correlation between feature X u and</p><formula xml:id="formula_2">P k , u = [1, 2, ..., C ], k = [1, 2, ..., N ], S ∈ R C ×N .</formula><p>Then, we apply a softmax operation along the class dimension to generate the class affinity map A.</p><p>The final class augmented object representation can be formulated as below:</p><formula xml:id="formula_3">Y u = ρ(δ N k=1 (a u,k • e P k N j=1 e Pj ) + X u )<label>(2)</label></formula><p>in which Y u denotes the uth feature plane of the output feature map Y ∈ R C×H×W . a u,k is a scalar value of s u,k after softmax layer. Here, δ(•) and ρ(•) are both transformation functions implemented by 1 × 1 conv → BN → ReLU.</p><p>The original local feature X is added to enhance the feature representation. The Eq. ( <ref type="formula" target="#formula_3">2</ref>) indicates that the final representation of each channel is a category-based weighted sum of all channels in class attention map, which models the categorybased semantic dependencies between feature maps. That is to say, the proposed CAA module improves the perception and discriminability of class-level information in a straightforward manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Class Channel Attention</head><p>The high-level semantics of CNNs are empirically considered to be embedded in the channel dimension, among which each channel map of deep features can be regarded as a classrelated response. Additionally, recent works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40]</ref> have demonstrated the effectiveness of modeling channel correlation in classification and segmentation tasks. Therefore, we propose a class channel attention (CCA) module to exploit class channel dependencies and generate a new class affinity map with rich and adaptive contextual information, which is effectively embedded in the CAA module with a few parameters.</p><p>The main structure of class channel attention module is illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>. Given the class attention map P ∈ R N ×H×W and class affinity map A ∈ R C ×N output from the CAA module above, the adaptive class channel statistical representations can be formulated as follows:</p><formula xml:id="formula_4">W k = σ(f {W1,W2} • (GAP ( e P k N j=1 e Pj )))<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">GAP (P k ) = 1 HW H i=1 W j=1 P k (i, j</formula><p>) is the channelwise global average pooling (GAP) to generate class-related statistics and σ is the Sigmoid activation. Let x = GAP (P k ), the key adaptive feature recalibration function is defined as:</p><formula xml:id="formula_6">f {W1,W2} (x) = W 2 η(W 1 x)<label>(4)</label></formula><p>in which W 2 ∈ R N ×αN and W 1 ∈ R αN ×N and η denotes the ReLU function. Concretely, W 1 (•) and W 2 (•) are two linear fully connected transformations, i.e., dimensionality adjustment layers with ratio α (this parameter value will be discussed in Section IV-D3) to augment and squeeze the representations of category information in the channel dimension, respectively. Noted that we opt to employ a simple ReLU function to ensure the non-linearity of the model and limit the complexity following the Squeeze-and-Excitation Networks <ref type="bibr" target="#b35">[36]</ref>.</p><p>The final output of the CCA module is obtained by recalibrating A with the weighted factor W k and the original class affinity map:</p><formula xml:id="formula_7">A = sof tmax( N i=1 (γW i + 1)A i ) (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where γ is a learnable parameter initialized to 0. The residual connection is added to retain the original representation (see"+1" in Eq. 5), thus, it can be integrated into the standard CAA module above without breaking its initial behavior, which efficiently helps to enhance the feature adaptive recalibration of class information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Region Shuffle Attention</head><p>Attention-based neural networks, in terms of spatial pointwise correlation representations, mainly aim to capture longrange contextual dependencies through a self-attention mechanism or its variants, eventually, generating a dense affinity matrix. Even for smaller feature maps, the point-wise affinity matrix obtained will take up a lot of (GPU) memory. Hence, the key point of the proposed region shuffle attention is to harvest the region-wise dependencies as well as its counterparts after recombination in a sparse and efficient manner. We illustrate our approach via a simple schematic in Fig. <ref type="figure" target="#fig_5">6</ref>. Region Representations. We partition the input feature maps into regions via a permutation operation, each of which is fed into an adaptive global average pooling layer to obtain the region representations afterwards. Then, we merge the point-wise representations of the regions to generate a sparse representation of the whole input feature. Therefore, the selfattention on the original input features can be effectively replaced with the same attention towards the merged counterparts for convenience. Shuffle Attention Representations. Despite the self-attention on the merged feature that can empirically capture long-range contextual information from all positions, the pixel-to-pixel connections are still ambiguous. In order to exploit more explicit contextual dependencies from a regional perspective, we apply a shuffle attention to alternately pool the corresponding sub-regions and compute its self-attention representations, respectively, achieving a complementary representation of spatial information. Further experiments show that the cascade of attention weighted representations of the two sub-regions can effectively enhance the contextual dependencies, superior to the pixel-wise non-local operator. As illustrated in Fig. <ref type="figure" target="#fig_5">6</ref>, we first divide the input feature X into G partitions and each partition contains P positions, where each X G,p ∈ R C×P is a subset of X G . Then, we merge the point statistics after global average pooling to obtain the sparse representation X m ∈ R C×G . We apply self-attention on X m following the non-local operation <ref type="bibr" target="#b12">[11]</ref> as below:</p><formula xml:id="formula_9">A m = sof tmax( θ(X m ) T φ(X m ) √ d )<label>(6)</label></formula><formula xml:id="formula_10">Z m = wA m g(X m ) + X m<label>(7)</label></formula><p>where A m ∈ R G×G is a sparse affinity matrix based on global information and Z m ∈ R C×G is the weighted output features. Here, θ(•) and φ(•) are both transformation functions implemented by 1 × 1 conv → BN → ReLU while g(•) represents 1 × 1 conv. w is a learnable parameter initialized to 0. The regional weighted representation X G can be obtained by region-wise multiplication of Z m and X G . We apply another permutation to regroup the representations, then, the feature X S would be fed into the same region-wise attention block to generate the final representations Y.</p><p>Compared with standard self-attention mechanism, our approach greatly reduces the complexity in time and space from</p><formula xml:id="formula_11">O((H × W ) 2 C) to O(2( 1 G 2 h G 2 w + 1 P 2 h P 2 w )(H × W ) 2 C</formula><p>), where G h and G w are the number of partitions along height and width dimensions while each partition contains P h and P w pixels, respectively.</p><p>In general, the proposed region shuffle attention module makes up for the deficiency of non-local block that it is a huge consumption of memory footprint. Additionally, it can be plugged into any existing architectures at any stage without breaking its initial performance, and optimized in an end-toend manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Hybrid Multiple Attention Network</head><p>Integration of Attention Module. In order to take full advantage of three proposed attention modules, we further aggregate the CAA module embedded with the CCA module (the upper branch illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>) and the RSA module (the lower branch) in an cascading and parallel manner, both of which is concatenated with the local feature. Eventually, the feature after concatenation would be fed into the last convolution to generate the final segmentation map. Loss Function. Besides the conventional multi-class cross entropy loss L ce , we use the auxiliary supervision L aux after stage-3 to improve the performance and make it easier to optimize following PSPNet <ref type="bibr" target="#b8">[9]</ref>. The auxiliary loss can be formulated as:</p><formula xml:id="formula_12">L aux = − 1 BN B i=1 N j=1 K k=1 I(g i j = k) log( exp(p i j,k ) K m=1 exp(p i j,m ) )<label>(8)</label></formula><formula xml:id="formula_13">I(g i j = k) = 1, g i j = k, 0, otherwise<label>(9)</label></formula><p>where B is the mini batch size; N is the number of pixels in each batch; K is the number of categories; p i j,k is the prediction after ResNet-stage-3 of the j-th pixel in the i-th patch for the k-th class; I(g i j = k) is an indicator function as illustrated in Eq. 9, it takes 1 when the ground truth of the j-th position in the i-th patch (i.e. g i j ) belongs to the k-th class, and 0 in other cases.</p><p>The class attention loss L cls from CAA module is also employed as an extra auxiliary supervision. Likewise, the class attention loss can be formulated as:</p><formula xml:id="formula_14">L cls = − 1 BN B i=1 N j=1 K k=1 I(g i j = k) log( exp(a i j,k ) K m=1 exp(a i j,m ) )<label>(10</label></formula><p>) where a i j,k is the response value of the class attention map of the j-th pixel in the i-th patch for the k-th class; other definitions are the same as above.</p><p>Finally, we use three parameters to balance these loss as follows:</p><formula xml:id="formula_15">L = λ 1 L ce + λ 2 L cls + λ 3 L aux (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where λ 1 , λ 2 and λ 3 are set as 1, 0.5 and 0.4 to balance the loss. Noted that the ablation studies for the three loss functions and the sensitivity of the model to the choice of the weight values will be elaborated in Section IV-D1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To validate the effectiveness of our proposed method, we conduct extensive experiments on two state-of-the-art aerial image semantic segmentation benchmarks, i.e., ISPRS 2D Semantic Labeling Challenging for Vaihingen <ref type="bibr" target="#b40">[41]</ref> and Potsdam <ref type="bibr" target="#b41">[42]</ref>, consisting of very high resolution true ortho photo (TOP) tiles and corresponding digital surface models (DSMs) derived from dense image matching techniques. In this section, we first introduce the datasets and implementation details, then we perform extensive ablation experiments on the ISPRS Vaihingen dataset. Finally, we report our results on the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datatsets</head><p>Vaihingen. The Vaihingen dataset contains 33 orthorectified image tiles (TOP) mosaic with three spectral bands (red, green, near-infrared), plus a normalized digital surface model (DSM) of the same resolution. The dataset has a ground sampling distance (GSD) of 9 cm, with an average size of 2494 × 2064 pixels, which involves five foreground object classes and one background class. We use the benchmark organizer defined 16 images for training and 17 to test our model following the previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. Noted that we do not use DSM in our experiments. Potsdam. The Potsdam 2D semantic labeling dataset is composed of 38 high resolution images of size 6000 × 6000 pixels, with a ground sampling distance (GSD) of 5 cm. The dataset offers NIR-R-G-B channels together with DSM and normalized DSM. There are 24 images in training set and 16 images in test set, which have 6 foreground classes corresponding to the Vaihingen benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>To evaluate the performance of the proposed network, we calculate the F 1 score for the foreground object classes with the following formula:</p><formula xml:id="formula_17">F 1 = (1 × β 2 ) • precision • recall β 2 • precision + recall (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where β is the equivalent factor between precision and recall and is set as 1. Intersection over union (IoU) and overall accuracy (OA) are defined as:</p><formula xml:id="formula_19">IoU = T P T P + F P + F N (<label>13</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">OA = T P + T N N (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>in which T P , T N , F P and F N are the number of true positives, true negatives, false positives and false negatives, respectively. N is the total number of pixels. Notably, overall accuracy is computed for all categories including background for a comprehensive comparison with different models. Additionally, the evaluation is carried out using ground truth with eroded boundaries provided in the datasets following previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We use ResNet-101 <ref type="bibr" target="#b18">[19]</ref> pretrained on ImageNet <ref type="bibr" target="#b20">[21]</ref> as our backbone and employ a poly learning rate policy where the initial learning rate is multiplied by 1 − ( iter max iter ) power with power = 0.9 after each iteration following the prior works <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr">17</ref>]. And we utilize stochastic gradient descent (SGD) optimizer with the initial learning rate 0.01 for training. Momentum and weight decay coefficients are set to 0.9 and 0.0005 respectively. We replace the standard BatchNorm with InPlace-ABNSync <ref type="bibr" target="#b45">[46]</ref> to synchronize the mean and standarddeviation of BatchNorm across multiple GPUs. For the data augmentation, we apply random horizontal flipping, random scaling (from 0.5 to 2.0) and random crop over all the training images. The input size for all datasets is set to 512 × 512. We employ 4× NVIDIA Tesla K80 GPU for 80k iterations and batch size is 4. For semantic segmentation, we choose FCN (VGG-16) <ref type="bibr" target="#b6">[7]</ref> pretrained on ImageNet as our baseline, and we also utilize ResNet-101 <ref type="bibr" target="#b18">[19]</ref> baseline for further comparison experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments on Vaihingen Dataset 1) Ablation Study for weight parameters and multiple loss functions:</head><p>The proposed model utilizes multiple loss functions to optimize the learning process. We first conduct experiments to analyze the sensitivity of the model to the choice of the weight parameters λ 2 and λ 3 . Concretely, we set the weight parameter of the main cross entropy loss (i.e. λ 1 ) as 1 and only preserve one of the auxiliary loss functions to further study the optimal value of λ 2 and λ 3 . The experimental results for λ 2 and λ 3 are presented in Tab. I and Tab. II. It can be seen that the choice of λ 2 = 0.5 and λ 3 = 0.4 yield the best result, respectively. Besides, it is worth mentioning that the model is not particularly sensitive to parameter selection. Thus, in order to avoid the influence of the training error of each experiment, we conduct 5 experiments for each value of the parameters, and take the average value as the final result. We further investigate the performance of the three loss functions following the optimal settings in Tab. I and Tab. II. As shown in Tab. III, both auxiliary loss functions have certain improvement effects on model training optimization. It yields a result of 90.98% in overall accuracy and 82.87% in mean IoU when we utilize all the loss functions.</p><p>2) Ablation Study for Attention Modules: In the proposed HMANet, three attention modules are employed on the top of the dilation network to exploit global contextual representations from the perspective of space, channel and category. To further verify the performance of attention modules, we conduct extensive experiments with different settings in Tab. IV. Noted that for a fair comparison with the baseline model FCN (VGG-16), we also use VGG-16 as the backbone on HMANet in this experiment. Besides, we further investigate two integration patterns, that is, the parallel and cascading fashion, to adaptively accomplish information propagation.</p><p>As illustrated in Tab. IV, the proposed attention modules bring remarkable improvement compared with the baseline FCN (VGG-16). We can observe that the use of only class augmented attention module yields a result of 89.15% in overall accuracy and 79.56% in mean IoU, which brings 2.64% and 6.87% improvement in OA and mIoU, respectively. Meanwhile, employing region shuffle attention individually outperforms the baseline by 2.72% in OA and 6.96% in mIoU. Furthermore, when we employ the integration of two corresponding attention modules together, the performance of our network is further boosted up. Finally, it behaves superiorly compared to other methods when we integrate the three attention modules, which improves the segmentation performance over baseline by 3.44% in OA and 7.99% in mIoU. In summary, it can be seen that our approach brings great benefit to object segmentation via exploiting global context from different perspectives.</p><p>We further investigate the effect of different aggregation methods of the three attention modules. As shown in Tab. V, the ResNet101 +Parallel-C-R, corresponding to the schematic diagram in Fig. <ref type="figure" target="#fig_2">3</ref>, achieve the best performance, i.e., 90.98% in overall accuracy, as well as 82.27% in mean IoU. While the two cascading integration patterns, "+Cascade-C-R" and "+Cascade-R-C" achieve 90.88% and 90.76% in overall accuracy, respectively. It shows that the cascading integration patterns lead to a decline in experimental results. The reason may be that the region-wise attention representation is not conducive to the extraction of category information only in the case of direct serial connection.</p><p>3) Ablation Study for Sub-parameters: Ascending ratio. The ascending ratio α introduced in Eq. ( <ref type="formula" target="#formula_6">4</ref>) is a hyperparameter which allows us to control the scale of feature transformations. As the choice of ascending ratio does not have much effect on the computational cost, we only investigate the performance between a range of different α values. As shown in Tab. (VI), we can conclude that our approach consistently outperforms the baseline under different choices of hyperparameters, among which the choice ratio α = 150 achieves slightly better results than others. Qualitatively, the ratio α is the scaling factor of category information, which can takes a moderate value while controlling the computational cost. Effect of the Partition numbers. We further investigate the effect of different partition numbers of the proposed region shuffle attention module, i.e., G and P . We conduct extensive experiments with various choices of G and P , and present the corresponding results in Tab. (VII). Noted that G and P are mutually constrained, namely, we just need to determine the values of G h and G w . We can see that the performance is robust for a range of partition numbers, among which the choice G h = G w = 8 achieve the best 90.79% in overall accuracy and 82.49% in mean IoU. Empirically, the output  stride of the backbone is set to 8, that is, the height and width of the input feature is 64 pixels in our experiments, thus eclectic choice of grouping is more conducive to selfattention weighted representations of each region. In practice, using an identical partition number may not be optimal (due to the distinct roles performed by different base network and different training settings, e.g., output stride and input size), so further improvements may be achievable by tuning the partition numbers to meet the needs of the given base architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Comparison with Context Aggregation Approaches:</head><p>We compare the performance of several well verified context aggregation approaches, i.e., Atrous Spatial Pyramid Pooling (ASPP) in DeepLabv3 <ref type="bibr" target="#b7">[8]</ref>, Pyramid Pooling Module (PPM) in PSPNet <ref type="bibr" target="#b8">[9]</ref>, RCCA in CCNet <ref type="bibr" target="#b14">[13]</ref> and Self-Attention in non-local networks <ref type="bibr" target="#b12">[11]</ref>. All the experiments above are conducted under the same training/testing settings for fairness. We report the related results in Tab. (VIII). Concretely, "+PPM" achieves better performance compared with "+ASPP" in terms of expanding local receptive fields. Both "+Self-Attention" and "+RCCA" generate contextual information from all spatial positions in the feature maps, leading to limited object contexts. In contrast, our HMANet calculates global correlations </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>OA(%) mIoU(%)</p><p>ResNet-101 Baseline 90.12 80.81 + ASPP (Our impl.) <ref type="bibr" target="#b7">[8]</ref> 90.51 81.39 + PPM (Our impl.) <ref type="bibr" target="#b8">[9]</ref> 90.82 82.52 + Self-Attention (Our impl.) <ref type="bibr" target="#b12">[11]</ref> 90.62 82.17 + RCCA (Our impl.) <ref type="bibr" target="#b14">[13]</ref> 90  <ref type="figure" target="#fig_6">7</ref>. We first compare our RSA module with the standard self-attention mechanism in terms of the computation cost measured with GFLOPs. As the size of input feature map increases, the GFLOPs of self-attention mechanism gradually increases exponentially while the counterparts of our RSA module is almost linearly increasing. It can be seen that the RSA module is much more efficient than the self-attention mechanism when processing high-resolution feature maps. Comparison with Context Aggregation modules and Attention modules. We further compare our proposed class augmented attention module and region shuffle attention module with ASPP <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">17]</ref>, PPM <ref type="bibr" target="#b8">[9]</ref>, SA <ref type="bibr" target="#b12">[11]</ref>, RCCA <ref type="bibr" target="#b14">[13]</ref>, OCR <ref type="bibr" target="#b37">[38]</ref> and ISA <ref type="bibr" target="#b16">[15]</ref> in terms of efficiency, including parameters, GPU memory and computation cost (GFLOPs). We report the results in Tab. (IX). Notably, we evaluate the cost of all above methods without considering the cost of backbone and include the cost of 3×3 convolution for dimension reduction to ensure the fairness of the comparison. As shown in Tab. (IX), compared with standard Self-Attention (SA) mechanism, our RSA module requires 20× less GPU memory usage and significantly reduce FLOPs by about 77% with a few parameters, which proves the efficiency of region-wise representations in capturing long-range contextual information.</p><p>6) Comparison with State-of-the-art: We first adopt some common strategies to improve performance following [12, 14, Experimental results are shown in Tab. (X). We successively adopt the above strategies to obtain better object representations, which achieves 0.19% ,0.11% and 0.16% improvements respectively in overall accuracy.</p><p>We further compare our method with existing methods on Vaihingen test set. Notably, most of the methods adopt ResNet-101 as their backbone. Results are shown in Tab. (XI). It can be seen that our HMANet (ResNet-101) outperforms other context aggregation methods and attention-based methods by a large margin. Moreover, our HMANet is much more efficient in parameters, memory and GFLOPs. Especially, our F 1 score of Car is much higher than other approaches, it improves the second best CCNet by 0.93%, which demonstrates the effectiveness of capturing category-based information and global region-wise correlation.</p><p>7) Visualization Results: We provide qualitative comparisons between our HMANet and baseline network in Fig. <ref type="figure" target="#fig_4">512×512</ref> and 1024×1024 patches. In particular, we leverage the red dashed box to mark those challenging regions that are easily to be misclassified. It can be seen that our method outperforms the baseline by a large margin. HMANet predicts more accurate segmentation maps, that is, it can obtain finer boundary information and maintain the object coherence, which demonstrates the effectiveness of modeling categorybased correlation and region-wise representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments on Potsdam Dataset</head><p>We carry out experiments on ISPRS Potsdam benchmark to further evaluate the effectiveness of HMANet. Empirically, we adopt the same training and testing settings on Potsdam dataset. Numerical comparisons with state-of-the-art methods are shown in Tab. (XII). Remarkably, HMANet (ResNet-101) achieve 92.21% in overall accuracy and 87.28% in mean IoU. Notably, we compare the two types of available input images, i.e., RGB and IRRG color modes. Results show that the former can obtain better segmentation maps.</p><p>In addition, qualitative results are presented in Fig. <ref type="figure" target="#fig_7">9</ref>. It can be seen that HMANet produces better segmentation maps than baseline. We mark the improved regions with red dashed boxes (Best viewed in color).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a novel attention-based framework for dense prediction tasks in the field of remote sensing, namely Hybrid Multiple Attention Network (HMANet), which adaptively captures global contextual information from the perspective of space, channel and category. In particular, we introduce a class augmented attention module embedded with a class channel attention module to compute category-based correlation and further adaptively recalibrate the class-level information. Additionally, to address the feature redundancy and improve the efficiency of self-attention mechanism, a region shuffle attention module is presented to obtain robust region-wise representations. Extensive experiments on ISPRS Vaihingen and Potsdam benchmark demonstrate the effectiveness and efficiency of the proposed HMANet.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Visualization of Class Attention Maps corresponding to each class: (i) Imprevious surfaces. (ii) Building. (iii) Low vegetation. (iv) Tree. (v) Car. The upper row tends to represent the class attention response while the lower counterpart represents the non-negative response after non-linear activation. (Best viewed in color).</figDesc><graphic url="image-2.png" coords="2,146.73,147.69,71.17,71.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Intuitive understanding of pixel-wise attention and region-wise attention. The right side shows the the advantage of region-wise attention over the standard pixel-wise attention in terms of the Parameters (measured by M), GPU memory cost (measured by MB) and computation cost (measured by GFLOPs). It can be seen that the region-wise attention requires 20× less GPU memory usage and reduces FLOPs by about 77%.</figDesc><graphic url="image-16.png" coords="3,54.01,59.74,239.38,167.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The pipeline of the proposed Hybrid Multiple Attention Network (HMANet). The key components are the two parallel branches, Class Augmented Attention (CAA) module embeded with Class Channel Attention (CCA) module and Region Shuffle Attention (RSA) module, which obtain the category-based correlation and region-wise contextual dependencies, respectively. Empirically, we concatenate the two output feature maps {Y, Z} and the local representation X to further generate the final segmentation map (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The details of class augmented attention module (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Diagram of class channel attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: An example of region shuffle attention when the numbers of partitions G and positions in each patition P are both 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Comparison of numerical complexity. The x-axis represents the height and width of the input feature map and the y-axis represents the computation cost measured with GFLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Visualization results of HMANet on Potsdam test set.</figDesc><graphic url="image-84.png" coords="14,427.89,297.85,113.39,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparisons of different weight parameters λ 2 .</figDesc><table><row><cell>λ 2</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>OA(%)</cell><cell cols="6">90.75 90.80 90.83 90.85 90.82 90.79</cell></row><row><cell cols="7">mIoU(%) 82.37 82.51 82.53 82.56 82.52 82.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Comparisons of different weight parameters λ 3 .</figDesc><table><row><cell>λ 3</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>OA(%)</cell><cell cols="6">90.72 90.76 90.79 90.75 90.70 90.68</cell></row><row><cell cols="7">mIoU(%) 82.33 82.38 82.48 82.36 82.31 82.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Ablation study for multiple loss functions.Method L ce L cls L aux OA(%) mIoU(%)</figDesc><table><row><cell>HMANet</cell><cell>90.76</cell><cell>82.44</cell></row><row><cell>HMANet</cell><cell>90.85</cell><cell>82.56</cell></row><row><cell>HMANet</cell><cell>90.79</cell><cell>82.48</cell></row><row><cell>HMANet</cell><cell>90.98</cell><cell>82.87</cell></row><row><cell cols="3">TABLE IV: Ablation study for attention modules on Vaihingen</cell></row><row><cell cols="3">test set. CAA represents class augmented attention module,</cell></row><row><cell cols="3">CCA represents channel class attention module, RSA repre-</cell></row><row><cell cols="2">sents region shuffle attention module.</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">CAA CCA RSA OA(%) mIoU(%)</cell></row><row><cell>Baseline [7]</cell><cell>86.51</cell><cell>72.69</cell></row><row><cell>HMANet (VGG-16)</cell><cell>89.15</cell><cell>79.56</cell></row><row><cell>HMANet (VGG-16)</cell><cell>89.23</cell><cell>79.65</cell></row><row><cell>HMANet (VGG-16)</cell><cell>89.58</cell><cell>80.24</cell></row><row><cell>HMANet (VGG-16)</cell><cell>89.66</cell><cell>80.31</cell></row><row><cell>HMANet (VGG-16)</cell><cell>89.95</cell><cell>80.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V :</head><label>V</label><figDesc>Comparison between different integration patterns. Cascade-C-R indicates that CAA embedded with CCA module is followed by RSA module, and vice versa. Parallel-C-R represents CAA embedded with CCA and RSA are appended on the top of the ResNet-101 in parallel.</figDesc><table><row><cell>Method</cell><cell cols="2">OA(%) mIoU(%)</cell></row><row><cell>ResNet-101 Baseline</cell><cell>90.12</cell><cell>80.81</cell></row><row><cell>ResNet-101 + Cascade-C-R</cell><cell>90.88</cell><cell>82.62</cell></row><row><cell>ResNet-101 + Cascade-R-C</cell><cell>90.76</cell><cell>82.45</cell></row><row><cell>ResNet-101 + Parallel-C-R</cell><cell>90.98</cell><cell>82.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI :</head><label>VI</label><figDesc>Performance on Vaihingen test set for different ascending ratio α in CCA module.</figDesc><table><row><cell cols="3">Ratio α OA(%) mIoU(%)</cell></row><row><cell>50</cell><cell>90.78</cell><cell>82.48</cell></row><row><cell>75</cell><cell>90.80</cell><cell>82.49</cell></row><row><cell>100</cell><cell>90.82</cell><cell>82.52</cell></row><row><cell>125</cell><cell>90.84</cell><cell>82.53</cell></row><row><cell>150</cell><cell>90.85</cell><cell>82.54</cell></row><row><cell>175</cell><cell>90.83</cell><cell>82.52</cell></row><row><cell>200</cell><cell>90.81</cell><cell>82.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII :</head><label>VII</label><figDesc>Effect of partition numbers G h and G w within region shuffle attention module.</figDesc><table><row><cell>Method</cell><cell cols="4">G h G w OA(%) mIoU(%)</cell></row><row><cell>ResNet-101 Baseline</cell><cell>-</cell><cell>-</cell><cell>90.12</cell><cell>80.81</cell></row><row><cell></cell><cell>16</cell><cell>16</cell><cell>90.70</cell><cell>82.35</cell></row><row><cell></cell><cell>16</cell><cell>8</cell><cell>90.75</cell><cell>82.44</cell></row><row><cell></cell><cell>8</cell><cell>16</cell><cell>90.77</cell><cell>82.47</cell></row><row><cell>RSA</cell><cell>8</cell><cell>8</cell><cell>90.79</cell><cell>82.49</cell></row><row><cell></cell><cell>8</cell><cell>4</cell><cell>90.78</cell><cell>82.47</cell></row><row><cell></cell><cell>4</cell><cell>8</cell><cell>86.76</cell><cell>82.46</cell></row><row><cell></cell><cell>4</cell><cell>4</cell><cell>90.75</cell><cell>82.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII :</head><label>VIII</label><figDesc>Comparison with context aggregation approaches.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE X :</head><label>X</label><figDesc>Performace comparison between data augmentation (DA), multi-grid (MG) and multi-scale with horizontal flipping (MS + Flip). We report the results on the test set of Vaihingen.</figDesc><table><row><cell cols="3">Method DA MG MS + Flip OA(%) mIoU(%)</cell></row><row><cell>HMANet</cell><cell>90.98</cell><cell>82.87</cell></row><row><cell>HMANet</cell><cell>91.17</cell><cell>83.11</cell></row><row><cell>HMANet</cell><cell>91.28</cell><cell>83.27</cell></row><row><cell>HMANet</cell><cell>91.44</cell><cell>83.49</cell></row><row><cell cols="3">47]. (1) DA: Data augmentation with random scaling (from 0.5</cell></row><row><cell cols="3">to 2.0) and random left-right flipping. (2) Multi-Grid: We em-</cell></row><row><cell cols="3">ploy hierarchical grids of different sizes (1,2,4) within stage-4</cell></row><row><cell cols="3">of ResNet-101. (3) MS + Flip: We average the segmentation</cell></row><row><cell cols="3">score maps from 5 image scales {0.5, 0.75, 1.0, 1.25, 1.5} and</cell></row><row><cell cols="2">left-right flipping counterparts during inference.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE XI :</head><label>XI</label><figDesc>Comparisons with state-of-the-arts on Vaihingen test set.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="4">Imp. surf. Building Low veg. Tree</cell><cell>Car</cell><cell cols="3">mean F 1 OA(%) mIoU(%)</cell></row><row><cell>FCN [7]</cell><cell>VGG-16</cell><cell>88.67</cell><cell>92.83</cell><cell>76.32</cell><cell cols="2">86.67 74.21</cell><cell>83.74</cell><cell>86.51</cell><cell>72.69</cell></row><row><cell>UZ 1 [44]</cell><cell>-</cell><cell>89.20</cell><cell>92.50</cell><cell>81.60</cell><cell cols="2">86.90 57.30</cell><cell>81.50</cell><cell>87.30</cell><cell>-</cell></row><row><cell>RoteEqNet [3]</cell><cell>-</cell><cell>89.50</cell><cell>94.80</cell><cell>77.50</cell><cell cols="2">86.50 72.60</cell><cell>84.18</cell><cell>87.50</cell><cell>-</cell></row><row><cell>S-RA-FCN [30]</cell><cell>VGG-16</cell><cell>91.47</cell><cell>94.97</cell><cell>80.63</cell><cell cols="2">88.57 87.05</cell><cell>88.54</cell><cell>89.23</cell><cell>79.76</cell></row><row><cell>UFMG 4 [48]</cell><cell>-</cell><cell>91.10</cell><cell>94.50</cell><cell>82.90</cell><cell cols="2">88.80 81.30</cell><cell>87.72</cell><cell>89.40</cell><cell>-</cell></row><row><cell>V-FuseNet [49]</cell><cell>-</cell><cell>92.00</cell><cell>94.40</cell><cell>84.50</cell><cell cols="2">89.90 86.30</cell><cell>89.42</cell><cell>90.00</cell><cell>-</cell></row><row><cell>DLR 9 [32]</cell><cell>-</cell><cell>92.40</cell><cell>95.20</cell><cell>83.90</cell><cell cols="2">89.90 81.20</cell><cell>88.52</cell><cell>90.30</cell><cell>-</cell></row><row><cell>TreeUNet [31]</cell><cell>-</cell><cell>92.50</cell><cell>94.90</cell><cell>83.60</cell><cell cols="2">89.60 85.90</cell><cell>89.30</cell><cell>90.40</cell><cell>-</cell></row><row><cell>DANet [12]</cell><cell>ResNet-101</cell><cell>91.63</cell><cell>95.02</cell><cell>83.25</cell><cell cols="2">88.87 87.16</cell><cell>89.19</cell><cell>90.44</cell><cell>81.32</cell></row><row><cell cols="2">DeepLabV3+ [8] ResNet-101</cell><cell>92.38</cell><cell>95.17</cell><cell>84.29</cell><cell cols="2">89.52 86.47</cell><cell>89.57</cell><cell>90.56</cell><cell>81.47</cell></row><row><cell>PSPNet [9]</cell><cell>ResNet-101</cell><cell>92.79</cell><cell>95.46</cell><cell>84.51</cell><cell cols="2">89.94 88.61</cell><cell>90.26</cell><cell>90.85</cell><cell>82.58</cell></row><row><cell>ACFNet [37]</cell><cell>ResNet-101</cell><cell>92.93</cell><cell>95.27</cell><cell>84.46</cell><cell cols="2">90.05 88.64</cell><cell>90.27</cell><cell>90.90</cell><cell>82.68</cell></row><row><cell>BKHN11</cell><cell>ResNet-101</cell><cell>92.90</cell><cell>96.00</cell><cell>84.60</cell><cell cols="2">89.90 88.60</cell><cell>90.40</cell><cell>91.00</cell><cell>-</cell></row><row><cell>CASIA2 [10]</cell><cell>ResNet-101</cell><cell>93.20</cell><cell>96.00</cell><cell>84.70</cell><cell cols="2">89.90 86.70</cell><cell>90.10</cell><cell>91.10</cell><cell>-</cell></row><row><cell>CCNet [13]</cell><cell>ResNet-101</cell><cell>93.29</cell><cell>95.53</cell><cell>85.06</cell><cell cols="2">90.34 88.70</cell><cell>90.58</cell><cell>91.11</cell><cell>82.76</cell></row><row><cell>HMANet (Ours)</cell><cell>VGG-16</cell><cell>91.86</cell><cell>94.52</cell><cell>83.17</cell><cell cols="2">89.81 87.15</cell><cell>89.30</cell><cell>89.95</cell><cell>80.68</cell></row><row><cell cols="2">HMANet (Ours) ResNet-101</cell><cell>93.50</cell><cell>95.86</cell><cell>85.41</cell><cell cols="2">90.40 89.63</cell><cell>90.96</cell><cell>91.44</cell><cell>83.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XII :</head><label>XII</label><figDesc>Numerical comparisons with state-of-the-arts on Potsdam test set.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Backbone</cell><cell cols="4">Imp. surf. Building Low veg. Tree</cell><cell>Car</cell><cell cols="4">mean F 1 OA(%) mIoU(%)</cell></row><row><cell>FCN [7]</cell><cell></cell><cell>VGG-16</cell><cell>88.61</cell><cell>93.29</cell><cell>83.29</cell><cell cols="2">79.83 93.02</cell><cell>87.61</cell><cell></cell><cell>85.59</cell><cell>78.34</cell></row><row><cell>UZ 1 [44]</cell><cell></cell><cell>-</cell><cell>89.30</cell><cell>95.40</cell><cell>81.80</cell><cell cols="2">80.50 86.50</cell><cell>86.70</cell><cell></cell><cell>85.80</cell><cell>-</cell></row><row><cell>UFMG 4 [48]</cell><cell></cell><cell>-</cell><cell>90.80</cell><cell>95.60</cell><cell>84.40</cell><cell cols="2">84.30 92.40</cell><cell>89.50</cell><cell></cell><cell>87.90</cell><cell>-</cell></row><row><cell cols="2">S-RA-FCN [30]</cell><cell>VGG-16</cell><cell>91.33</cell><cell>94.70</cell><cell>86.81</cell><cell cols="2">83.47 94.52</cell><cell>90.17</cell><cell></cell><cell>88.59</cell><cell>82.38</cell></row><row><cell cols="2">V-FuseNet [49]</cell><cell>-</cell><cell>92.70</cell><cell>96.30</cell><cell>87.30</cell><cell cols="2">88.50 95.40</cell><cell>92.04</cell><cell></cell><cell>90.60</cell><cell>-</cell></row><row><cell>TSMTA [50]</cell><cell>Image</cell><cell>ResNet-101</cell><cell>92.91 Ground Truth</cell><cell>97.13</cell><cell>87.03</cell><cell cols="2">87.26 95.16 FCN</cell><cell>91.90</cell><cell>Ours</cell><cell>90.64</cell><cell>-</cell></row><row><cell cols="2">Multi-filter CNN [51]</cell><cell>VGG-16</cell><cell>90.94</cell><cell>96.98</cell><cell>76.32</cell><cell cols="2">73.37 88.55</cell><cell>85.23</cell><cell></cell><cell>90.65</cell><cell>-</cell></row><row><cell>TreeUNet [31]</cell><cell></cell><cell>-</cell><cell>93.10</cell><cell>97.30</cell><cell>86.60</cell><cell cols="2">87.10 95.80</cell><cell>91.98</cell><cell></cell><cell>90.70</cell><cell>-</cell></row><row><cell cols="2">DeepLabV3+ [8]</cell><cell>ResNet-101</cell><cell>92.95</cell><cell>95.88</cell><cell>87.62</cell><cell cols="2">88.15 96.02</cell><cell>92.12</cell><cell></cell><cell>90.88</cell><cell>84.32</cell></row><row><cell>CASIA3 [10]</cell><cell></cell><cell>ResNet-101</cell><cell>93.40</cell><cell>96.80</cell><cell>87.60</cell><cell cols="2">88.30 96.10</cell><cell>92.44</cell><cell></cell><cell>91.00</cell><cell>-</cell></row><row><cell>PSPNet [9]</cell><cell></cell><cell>ResNet-101</cell><cell>93.36</cell><cell>96.97</cell><cell>87.75</cell><cell cols="2">88.50 95.42</cell><cell>92.40</cell><cell></cell><cell>91.08</cell><cell>84.88</cell></row><row><cell>BKHN3</cell><cell></cell><cell>ResNet-101</cell><cell>93.30</cell><cell>97.20</cell><cell>88.00</cell><cell cols="2">88.50 96.00</cell><cell>92.60</cell><cell></cell><cell>91.10</cell><cell>-</cell></row><row><cell>AMA 1</cell><cell></cell><cell>-</cell><cell>93.40</cell><cell>96.80</cell><cell>87.70</cell><cell cols="2">88.80 96.00</cell><cell>92.54</cell><cell></cell><cell>91.20</cell><cell>-</cell></row><row><cell>CCNet [13]</cell><cell></cell><cell>ResNet-101</cell><cell>93.58</cell><cell>96.77</cell><cell>86.87</cell><cell cols="2">88.59 96.24</cell><cell>92.41</cell><cell></cell><cell>91.47</cell><cell>85.65</cell></row><row><cell cols="2">HUSTW4 [52]</cell><cell>-</cell><cell>93.60</cell><cell>97.60</cell><cell>88.50</cell><cell cols="2">88.80 94.60</cell><cell>92.62</cell><cell></cell><cell>91.60</cell><cell>-</cell></row><row><cell>SWJ 2</cell><cell></cell><cell>ResNet-101</cell><cell>94.40</cell><cell>97.40</cell><cell>87.80</cell><cell cols="2">87.60 94.70</cell><cell>92.38</cell><cell></cell><cell>91.70</cell><cell>-</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Information Processing Systems, 2018, pp. 352-361. [17] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs," IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 4, pp. 834-848, 2017. [18] L. Mi and Z. Chen, "Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation," ISPRS Journal of Photogrammetry and Remote Sensing, vol. 159, pp. 140-152, 2020.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by National Natural Science Foundation of China under Grants 61725105 and 41701508.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Integrating fuzzy object based image analysis and ant colony optimization for road extraction from remotely sensed images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maboudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="151" to="163" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mapping urbanization dynamics at regional and global scales using multi-temporal dmsp/ols nighttime light data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Seto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2320" to="2329" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Land cover mapping at very high resolution with rotation equivariant cnns: Towards small yet accurate models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="96" to="107" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random forest classifier for remote sensing classification</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="222" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support vector machines for hyperspectral remote sensing classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gualtieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Cromp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th AIPR Workshop: Advances in Computer-Assisted Recognition</title>
				<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">3584</biblScope>
			<biblScope unit="page" from="221" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multiple conditional random fields ensemble model for urban area detection in remote sensing optical images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3978" to="3988" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semantic labeling in very high resolution images via HMANet (Ours)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno>VGG-16 92.38 96.08 86.93 88.21 95.44 91.81 90.46 83.53</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hmanet</surname></persName>
		</author>
		<idno>Ours) ResNet-101 93.85 97.56 88.65 89.12 96.84 93.20 92.21 87.28</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="78" to="95" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9167" to="9176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Interlaced sparse self-attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12273</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Refinenet: Multipath refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spgnet: Semantic prediction guidance for scene parsing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5218" to="5228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relation matters: Relational context-aware fully convolutional network for semantic segmentation of high-resolution aerial images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Treeunet: Adaptive tree convolutional neural networks for subdecimeter aerial image segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end dsm fusion networks for semantic segmentation in high-resolution aerial images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6798" to="6807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11065</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Ecanet: Efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03151</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">2d semantic labeling contest-vaihingen</title>
		<author>
			<persName><surname>Isprs</surname></persName>
		</author>
		<ptr target="http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-vaihingen.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">2d semantic labeling contest-potsdam</title>
		<author>
			<persName><surname>Isprs</surname></persName>
		</author>
		<ptr target="http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">High-resolution aerial image labeling with convolutional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7092" to="7103" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dense semantic labeling of subdecimeter resolution images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="881" to="893" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic multicontext segmentation of remote sensing images based on convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dalla Mura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7503" to="7520" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semantic segmentation of large-size vhr remote sensing images using a two-stage multiscale training architecture</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Developing a multi-filter convolutional neural network for semantic segmentation using high-resolution aerial imagery and lidar data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Problems of encoderdecoder frameworks for high-resolution remote sensing image segmentation: Structural stereotype and insufficient learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="297" to="304" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
