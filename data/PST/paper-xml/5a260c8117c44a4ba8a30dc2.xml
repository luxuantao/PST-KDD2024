<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The goal of speech separation is to separate target speech from background interference. Speech separation is a fundamental task in signal processing with a wide range of applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
							<email>dwang@cse.ohio-state.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center of Intelligent Acoustics and Immersive Communications</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Silicon Valley AI Lab at Baidu Research</orgName>
								<address>
									<addrLine>1195 Bordeaux Drive</addrLine>
									<postCode>94089</postCode>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The goal of speech separation is to separate target speech from background interference. Speech separation is a fundamental task in signal processing with a wide range of applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech separation</term>
					<term>speaker separation</term>
					<term>speech enhancement</term>
					<term>supervised speech separation</term>
					<term>deep learning</term>
					<term>deep neural networks</term>
					<term>speech dereverberation</term>
					<term>time-frequency masking</term>
					<term>array separation</term>
					<term>beamforming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multitalker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>including hearing prosthesis, mobile telecommunication, and robust automatic speech and speaker recognition. The human auditory system has the remarkable ability to extract one sound source from a mixture of multiple sources. In an acoustic environment like a cocktail party, we seem capable of effortlessly following one speaker in the presence of other speakers and background noises. Speech separation is commonly called the "cocktail party problem," a term coined by Cherry in his famous 1953 paper <ref type="bibr" target="#b25">[26]</ref>.</p><p>Speech separation is a special case of sound source separation. Perceptually, source separation corresponds to auditory stream segregation, a topic of extensive research in auditory perception. The first systematic study on stream segregation was conducted by Miller and Heise <ref type="bibr" target="#b124">[124]</ref> who noted that listeners split a signal with two alternating sinewave tones into two streams. Bregman and his colleagues have carried out a series of studies on the subject, and in a seminal book <ref type="bibr" target="#b14">[15]</ref> he introduced the term auditory scene analysis (ASA) to refer to the perceptual process that segregates an acoustic mixture and groups the signal originating from the same sound source. Auditory scene analysis is divided into simultaneous organization and sequential organization. Simultaneous organization (or grouping) integrates concurrent sounds, while sequential organization integrates sounds across time. With auditory patterns displayed on a time-frequency representation such as a spectrogram, main organizational principles responsible for ASA include: Proximity in frequency and time, harmonicity, common amplitude and frequency modulation, onset and offset synchrony, common location, and prior knowledge (see among others <ref type="bibr" target="#b163">[163]</ref>  <ref type="bibr" target="#b14">[15]</ref> [29] <ref type="bibr" target="#b10">[11]</ref> [30] <ref type="bibr" target="#b31">[32]</ref>). These grouping principles also govern speech segregation <ref type="bibr" target="#b201">[201]</ref>  <ref type="bibr" target="#b154">[154]</ref> [31] <ref type="bibr" target="#b3">[4]</ref> [49] <ref type="bibr" target="#b93">[93]</ref>. From ASA studies, there seems to be a consensus that the human auditory system segregates and attends to a target sound, which can be a tone sequence, a melody, or a voice. More debatable is the role of auditory attention in stream segregation <ref type="bibr" target="#b16">[17]</ref> [151] <ref type="bibr" target="#b148">[148]</ref>  <ref type="bibr" target="#b120">[120]</ref>. In this overview, we use speech separation (or segregation) primarily to refer to the computational task of separating the target speech signal from a noisy mixture.</p><p>How well do we perform speech segregation? One way of quantifying speech perception performance in noise is to measure speech reception threshold, the required SNR level for a 50% intelligibility score. Miller <ref type="bibr" target="#b123">[123]</ref> reviewed human intelligibility scores when interfered by a variety of tones, broadband noises, and other voices. Listeners were tested for their word intelligibility scores, and the results are shown in Figure <ref type="figure">1</ref>. In general, tones are not as interfering as broadband DeLiang Wang, Fellow, IEEE, and Jitong Chen Supervised Speech Separation Based on Deep Learning: An Overview Figure <ref type="figure">1</ref>. Word intelligibility score with respect to SNR for different kinds of interference (from <ref type="bibr" target="#b172">[172]</ref>, redrawn from <ref type="bibr" target="#b123">[123]</ref>). The dashed line indicates 50% intelligibility. For speech interference, scores are shown for 1, 2, and 8 interfering speakers.</p><p>noises. For example, speech is intelligible even when mixed with a complex tone glide that is 20 dB more intense (pure tones are even weaker interferers). Broadband noise is the most interfering for speech perception, and the corresponding SRT is about 2 dB. When interference consists of other voices, the SRT depends on how many interfering talkers are present.</p><p>As shown in Fig. <ref type="figure">1</ref>, the SRT is about -10 dB for a single interferer but rapidly increases to -2 dB for two interferers.</p><p>The SRT stays about the same (around -1 dB) when the interference contains four or more voices. There is a whopping SRT gap of 23 dB for different kinds of interference! Furthermore, it should be noted that listeners with hearing loss show substantially higher SRTs than normal-hearing listeners, ranging from a few decibels for broadband stationary noise to as high as 10-15 dB for interfering speech <ref type="bibr" target="#b43">[44]</ref> [127], indicating a poorer ability of speech segregation. With speech as the most important means of human communication, the ability to separate speech from background interference is crucial, as the speech of interest, or target speech, is usually corrupted by additive noises from other sound sources and reverberation from surface reflections. Although humans perform speech separation with apparent ease, it has proven to be very challenging to construct an automatic system to match the human auditory system in this basic task. In his 1957 book <ref type="bibr" target="#b26">[27]</ref>, Cherry made an observation: "No machine has yet been constructed to do just that [solving the cocktail part problem]." His conclusion, unfortunately for our field, has remained largely true for 6 more decades, although recent advances reviewed in this article have started to crack the problem.</p><p>Given the importance, speech separation has been extensively studied in signal processing for decades. Depending on the number of sensors or microphones, one can categorize separation methods into monaural (singlemicrophone) and array-based (multi-microphone). Two traditional approaches for monaural separation are speech enhancement <ref type="bibr" target="#b113">[113]</ref> and computational auditory scene analysis (CASA) <ref type="bibr" target="#b172">[172]</ref>. Speech enhancement analyzes general statistics of speech and noise, followed by estimation of clean speech from noisy speech with a noise estimate <ref type="bibr" target="#b39">[40]</ref>  <ref type="bibr" target="#b113">[113]</ref>. The simplest and most widely used enhancement method is spectral subtraction <ref type="bibr" target="#b12">[13]</ref>, in which the power spectrum of the estimated noise is subtracted from that of noisy speech. In order to estimate background noise, speech enhancement techniques typically assume that background noise is stationary, i.e. its spectral properties do not change over time, or at least are more stationary than speech. CASA is based on perceptual principles of auditory scene analysis <ref type="bibr" target="#b14">[15]</ref> and exploits grouping cues such as pitch and onset. For example, the tandem algorithm separates voiced speech by alternating pitch estimation and pitch-based grouping <ref type="bibr" target="#b77">[78]</ref>.</p><p>An array with two or more microphones uses a different principle to achieve speech separation. Beamforming, or spatial filtering, boosts the signal that arrives from a specific direction through proper array configuration, hence attenuating interference from other directions <ref type="bibr" target="#b164">[164]</ref>  <ref type="bibr" target="#b13">[14]</ref> [9] <ref type="bibr" target="#b88">[88]</ref>. The simplest beamformer is a delay-and-sum technique that adds multiple microphone signals from the target direction in phase and uses phase differences to attenuate signals from other directions. The amount of noise attenuation depends on the spacing, size, and configuration of the arraygenerally the attenuation increases as the number of microphones and the array length increase. Obviously, spatial filtering cannot be applied when target and interfering sources are co-located or near to one another. Moreover, the utility of beamforming is much reduced in reverberant conditions, which smear the directionality of sound sources.</p><p>A more recent approach treats speech separation as a supervised learning problem. The original formulation of supervised speech separation was inspired by the concept of time-frequency (T-F) masking in CASA. As a means of separation, T-F masking applies a two-dimensional mask (weighting) to the time-frequency representation of a source mixture in order to separate the target source <ref type="bibr" target="#b117">[117]</ref> [172] <ref type="bibr" target="#b170">[170]</ref>. A major goal of CASA is the ideal binary mask (IBM) <ref type="bibr" target="#b75">[76]</ref>, which denotes whether the target signal dominates a T-F unit in the time-frequency representation of a mixed signal. Listening studies show that ideal binary masking dramatically improves speech intelligibility for normal-hearing (NH) and hearing-impaired (HI) listeners in noisy conditions <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b0">[1]</ref> [109] <ref type="bibr" target="#b173">[173]</ref>. With the IBM as the computational goal, speech separation becomes binary classification, an elementary form of supervised learning. In this case, the IBM is used as the desired signal, or target function, during training. During testing, the learning machine aims to estimate the IBM. Although it served as the first training target in supervised speech separation, the IBM is by no means the only training target and Sect. III presents a list of training targets, many shown to be more effective.</p><p>Since the formulation of speech separation as classification, the data-driven approach has been extensively studied in the speech processing community. Over the last decade, supervised speech separation has substantially advanced the state-of-the-art performance by leveraging large training data and increasing computing resources <ref type="bibr" target="#b20">[21]</ref>. Supervised separation has especially benefited from the rapid rise in deep learningthe topic of this overview. Supervised speech separation algorithms can be broadly divided into the following components: learning machines, training targets, and acoustic features. In this paper, we will first review the three components. We will then move to describe representative algorithms, where monaural and array-based algorithms will be covered in separate sections. As generalization is an issue unique to supervised speech separation, this issue will be treated in this overview.</p><p>Let us clarify a few related terms used in this overview to avoid potential confusion. We refer to speech separation or segregation as the general task of separating target speech from its background interference, which may include nonspeech noise, interfering speech, or both, as well as room reverberation. Furthermore, we equate speech separation and the cocktail party problem, which goes beyond the separation of two speech utterances originally experimented with by Cherry <ref type="bibr" target="#b25">[26]</ref>. By speech enhancement (or denoising), we mean the separation of speech and nonspeech noise. If one is limited to the separation of multiple voices, we use the term speaker separation.</p><p>This overview is organized as follows. We first review the three main aspects of supervised speech separation, i.e., learning machines, training targets, and features, in Sections II, III, and IV, respectively. Section V is devoted to monaural separation algorithms, and Section VI to array-based algorithms. Section VII concludes the overview with a discussion of a few additional issues, such as what signal should be considered as the target and what a solution to the cocktail party problem may look like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.CLASSIFIERS AND LEARNING MACHINES</head><p>Over the past decade, DNNs have significantly elevated the performance of many supervised learning tasks, such as image classification <ref type="bibr" target="#b27">[28]</ref>, handwriting recognition <ref type="bibr" target="#b52">[53]</ref>, automatic speech recognition <ref type="bibr" target="#b72">[73]</ref>, language modeling <ref type="bibr" target="#b156">[156]</ref>, and machine translation <ref type="bibr" target="#b157">[157]</ref>. DNNs have also advanced the performance of supervised speech separation by a large margin. This section briefly introduces the types of DNNs for supervised speech separation: feedforward multilayer perceptrons (MLPs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs).</p><p>The most popular model in neural networks is an MLP that has feedforward connections from the input layer to the output layer, layer-by-layer, and the consecutive layers are fully connected. An MLP is an extension of Rosenblatt's perceptron <ref type="bibr" target="#b142">[142]</ref> by introducing hidden layers between the input layer and the output layer. An MLP is trained with the classical backpropagation algorithm <ref type="bibr" target="#b143">[143]</ref> where the network weights are adjusted to minimize the prediction error through gradient descent. The prediction error is measured by a cost (loss) function between the predicted output and the desired output, the latter provided by the user as part of supervision. For example, when an MLP is used for classification, a popular cost function is cross entropy: where 𝑦 𝑖 ̂ and 𝑦 𝑖 are the predicted output and desired output for neuron i, respectively.</p><formula xml:id="formula_0">− 1 𝑁 ∑ ∑ 𝐼</formula><p>The representational power of an MLP increases as the number of layers increases <ref type="bibr" target="#b142">[142]</ref> even though, in theory, an MLP with two hidden layers can approximate any function <ref type="bibr" target="#b69">[70]</ref>. The backpropagation algorithm is applicable to an MLP of any depth. However, a deep neural network (DNN) with many hidden layers is difficult to train from a random initialization of connection weights and biases because of the so-called vanishing gradient problem, which refers to the observation that, at lower layers (near the input end), gradients calculated from backpropagated error signals from upper layers, become progressively smaller or vanishing. As a result of vanishing gradients, connection weights at lower layers are not modified much and therefore lower layers learn little during training. This explains why MLPs with a single hidden layer were the most widely used neural network prior to the advent of DNN.</p><p>A breakthrough in DNN training was made by Hinton et al. <ref type="bibr" target="#b73">[74]</ref>. The key idea is to perform layerwise unsupervised pretraining with unlabeled data to properly initialize a DNN before supervised learning (or fine tuning) is performed with labeled data. More specifically, Hinton et al. <ref type="bibr" target="#b73">[74]</ref> proposed restrictive Boltzmann machines (RBMs) to pretrain a DNN layer by layer, and RBM pretraining is found to improve subsequent supervised learning. A later remedy was to use a rectified linear unit (ReLU) <ref type="bibr" target="#b128">[128]</ref> to replace the traditional sigmoid activation function, which converts a weighted sum of the inputs to a model neuron to the neuron's output. Recent practice shows that a moderately deep MLP with ReLUs can be effectively trained with large training data without unsupervised pretraining. Recently, skip connections have been introduced to facilitate the training of very deep MLPs <ref type="bibr" target="#b153">[153]</ref>  <ref type="bibr" target="#b61">[62]</ref>.</p><p>A class of feedforward networks, known as convolutional neural networks (CNNs) <ref type="bibr" target="#b106">[106]</ref>  <ref type="bibr" target="#b9">[10]</ref>, has been demonstrated to be well suited for pattern recognition, particularly in the visual domain. CNNs incorporate well-documented invariances in pattern recognition such as translation (shift) invariance. A typical CNN architecture is a cascade of pairs of a convolutional layer and a subsampling layer. A convolutional layer consists of multiple feature maps, each of which learns to extract a local feature regardless of its position in the previous layer through weight sharing: the neurons within the same module are constrained to have the same connection weights despite their different receptive fields. A receptive field of a neuron in this context denotes the local area of the previous layer that is connected to the neuron, whose operation of a weighted sum is akin to a convolution 1 . Each convolutional layer is followed by a subsampling layer that performs local averaging or maximization over the receptive fields of the neurons in the convolutional layer. Subsampling serves to reduce resolution and sensitivity to local variations. The use of weight sharing in CNN also has the benefit of cutting down the number of trainable parameters. Because a CNN incorporates domain knowledge in pattern recognition via its network structure, it can be better trained by the backpropagation algorithm despite the fact that a CNN is a deep network.</p><p>RNNs allow recurrent (feedback) connections, typically between hidden units. Unlike feedforward networks, which process each input sample independently, RNNs treat input samples as a sequence and model the changes over time. A speech signal exhibits strong temporal structure, and the signal within the current frame is influenced by the signals in the previous frames. Therefore, RNNs are a natural choice for learning the temporal dynamics of speech. We note that a RNN through its recurrent connections introduces the time dimension, which is flexible and infinitely extensible, a characteristic not shared by feedforward networks no matter how deep they are <ref type="bibr" target="#b169">[169]</ref>; in a way, a RNN can be viewed a DNN with an infinite depth <ref type="bibr" target="#b146">[146]</ref>. The recurrent connections are typically trained with backpropagation through time <ref type="bibr" target="#b187">[187]</ref>. However, such RNN training is susceptible to the vanishing or exploding gradient problem <ref type="bibr" target="#b137">[137]</ref>. To alleviate this problem, a RNN with long short-term memory (LSTM) introduces memory cells with gates to facilitate the information flow over time <ref type="bibr" target="#b74">[75]</ref>. Specifically, a memory cell has three gates: input gate, forget gate and output gate. The forget gate controls how much previous information should be retained, and the input gate controls how much current information should be added to the memory cell. With these gating functions, LSTM allows relevant contextual information to be maintained in memory cells to improve RNN training.</p><p>Generative adversarial networks (GANs) were recently introduced with simultaneously trained models: a generative model G and a discriminative model D <ref type="bibr" target="#b51">[52]</ref>. The generator G learns to model labeled data, e.g. the mapping from noisy speech samples to their clean counterparts, while the discriminatorusually a binary classifierlearns to discriminate between generated samples and target samples from training data. This framework is analogous to a twoplayer adversarial game, where minimax is a proven strategy <ref type="bibr" target="#b144">[144]</ref>. During training, G aims to learn an accurate mapping so that the generated data can well imitate the real data so as to fool D; on the other hand, D learns to better tell the difference between the real data and synthetic data generated by G. Competition in this game, or adversarial learning, drives both models to improve their accuracy until generated samples are indistinguishable from real ones. The key idea of GANs is to use the discriminator to shape the loss function of the generator. GANs have recently been used in speech enhancement (see Sect. V.A).</p><p>In this overview, a DNN refers to any neural network with at least two hidden layers <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b72">[73]</ref>, in contrast to popular learning machines with just one hidden layer such as 1 More straightforwardly a correlation. commonly used MLPs, support vector machines (SVMs) with kernels, and Gaussian mixture models (GMMs). As DNNs get deeper in practice, with more than 100 hidden layers actually used, the depth required for a neural network to be considered a DNN can be a matter of a qualitative, rather than quantitative, distinction. Also, we use the term DNN to denote any neural network with a deep structure, whether it is feedforward or recurrent.</p><p>We should mention that DNN is not the only kind of learning machine that has been employed for speech separation. Alternative learning machines used for supervised speech separation include GMM <ref type="bibr" target="#b147">[147]</ref> [97], SVM <ref type="bibr" target="#b54">[55]</ref>, and neural networks with just one hidden layer <ref type="bibr" target="#b91">[91]</ref>. Such studies will not be further discussed in this overview as its theme is DNN based speech separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.TRAINING TARGETS</head><p>In supervised speech separation, defining a proper training target is important for learning and generalization. There are mainly two groups of training targets, i.e., masking-based targets and mapping-based targets. Masking-based targets describe the time-frequency relationships of clean speech to background interference, while mapping-based targets correspond to the spectral representations of clean speech. In this section, we survey a number of training targets proposed in the field.</p><p>Before reviewing training targets, let us first describe evaluation metrics commonly used in speech separation. A variety of metrics has been proposed in the literature, depending on the objectives of individual studies. These metrics can be divided into two classes: signal-level and perception-level. At the signal level, metrics aim to quantify the degrees of signal enhancement or interference reduction. In addition to the traditional SNR, speech distortion (loss) and noise residue in a separated signal can be individually measured <ref type="bibr" target="#b76">[77]</ref>  <ref type="bibr" target="#b113">[113]</ref>. A prominent set of evaluation metrics comprises SDR (source-to-distortion ratio), SIR (source-tointerference ratio), and SAR (source-to-artifact ratio) <ref type="bibr" target="#b165">[165]</ref>.</p><p>As the output of a speech separation system is often consumed by the human listener, a lot of effort has been made to quantitatively predict how the listener perceives a separated signal. Because intelligibility and quality are two primary but different aspects of speech perception, objective metrics have been developed to separately evaluate speech intelligibility and speech quality. With the IBM's ability to elevate human speech intelligibility and its connection to the articulation index (AI) <ref type="bibr" target="#b114">[114]</ref> the classic model of speech perceptionthe HIT−FA rate has been suggested as an evaluation metric with the IBM as the reference <ref type="bibr" target="#b97">[97]</ref>. HIT denotes the percent of speech-dominant T-F units in the IBM that is correctly classified and FA (false-alarm) refers to the percent of noisedominant units that is incorrectly classified. The HIT−FA rate is found to be well correlated with speech intelligibility <ref type="bibr" target="#b97">[97]</ref>. In recent years, the most commonly used intelligibility metric is STOI (short-time objective intelligibility), which measures the correlation between the short-time temporal envelopes of a reference (clean) utterance and a separated utterance <ref type="bibr">[158] [89]</ref>. The value range of STOI is typically between 0 and 1, which can be interpreted as percent correct. Although STOI tends to overpredict intelligibility scores <ref type="bibr" target="#b63">[64]</ref> [102], no alternative metric has been shown to consistently correlate with human intelligibility better. For speech quality, PESQ (perceptual evaluation of speech quality) is the standard metric <ref type="bibr" target="#b140">[140]</ref> and recommended by the International Telecommunication Union (ITU) <ref type="bibr" target="#b87">[87]</ref>. PESQ applies an auditory transform to produce a loudness spectrum, and compares the loudness spectra of a clean reference signal and a separated signal to produce a score in a range of -0.5 to 4.5, corresponding to the prediction of the perceptual MOS (mean opinion score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ideal Binary Mask</head><p>The first training target used in supervised speech separation is the ideal binary mask <ref type="bibr">[76] [141]</ref> [77] <ref type="bibr" target="#b168">[168]</ref>, which is inspired by the auditory masking phenomenon in audition <ref type="bibr" target="#b126">[126]</ref> and the exclusive allocation principle in auditory scene analysis <ref type="bibr" target="#b14">[15]</ref>. The IBM is defined on a twodimensional T-F representation of a noisy signal, such as a cochleagram or a spectrogram:</p><formula xml:id="formula_1">𝐼𝐵𝑀 = { 1, if 𝑆𝑁𝑅(𝑡, 𝑓) &gt; 𝐿𝐶 0, otherwise<label>(1)</label></formula><p>where 𝑡 and 𝑓 denote time and frequency, respectively. The IBM assigns the value 1 to a unit if the SNR within the T-F unit exceeds the local criterion (LC) or threshold, and 0 otherwise. Fig. <ref type="figure" target="#fig_0">2</ref>(a) shows an example of the IBM, which is defined on a 64-channel cochleagram. As mentioned in Sect. I, IBM masking dramatically increases speech intelligibility in noise for normal-hearing and hearing-impaired listeners. The IBM labels every T-F unit as either target-dominant or interference-dominant. As a result, IBM estimation can naturally be treated as a supervised classification problem. A commonly used cost function for IBM estimation is cross entropy, as described in Section II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Target Binary Mask</head><p>Like the IBM, the target binary mask (TBM) categorizes all T-F units with a binary label. Different from the IBM, the TBM derives the label by comparing the target speech energy in each T-F unit with a fixed interference: speech-shaped noise, which is a stationary signal corresponding to the average of all speech signals. An example of the TBM is shown in Fig. <ref type="figure" target="#fig_0">2(b)</ref>. Target binary masking also leads to dramatic improvement of speech intelligibility in noise <ref type="bibr" target="#b99">[99]</ref>, and the TBM has been used as a training target <ref type="bibr" target="#b50">[51]</ref>  <ref type="bibr" target="#b112">[112]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ideal Ratio Mask</head><p>Instead of a hard label on each T-F unit, the ideal ratio mask (IRM) can be viewed as a soft version of the IBM <ref type="bibr" target="#b152">[152]</ref>  <ref type="bibr" target="#b130">[130]</ref> [178] <ref type="bibr" target="#b84">[84]</ref>:</p><formula xml:id="formula_2">𝐼𝑅𝑀 = ( 𝑆(𝑡, 𝑓) 2 𝑆(𝑡, 𝑓) 2 + 𝑁(𝑡, 𝑓) 2 ) 𝛽 (2)</formula><p>where 𝑆(𝑡, 𝑓) 2 and 𝑁(𝑡, 𝑓) 2 denote speech energy and noise energy within a T-F unit, respectively. The tunable parameter  scales the mask, and is commonly chosen to 0.5.</p><p>With the square root the IRM preserves the speech energy with each T-F unit, under the assumption that 𝑆(𝑡, 𝑓) and 𝑁(𝑡, 𝑓) are uncorrelated. This assumption holds well for additive noise, but not for convolutive interference as in the case of room reverberation (late reverberation, however, can be reasonably considered as uncorrelated interference.) Without the root the IRM in ( <ref type="formula">2</ref>) is similar to the classical Wiener filter, which is the optimal estimator of target speech in the power spectrum. MSE is typically used as the cost function for IRM estimation. An example of the IRM is shown in Fig. <ref type="figure" target="#fig_0">2(c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Spectral Magnitude Mask</head><p>The spectral magnitude mask (SMM) (called FFT-MASK in <ref type="bibr" target="#b178">[178]</ref>) is defined on the STFT (short-time Fourier transform) magnitudes of clean speech and noisy speech:</p><formula xml:id="formula_3">SMM(𝑡, 𝑓) = |𝑆(𝑡, 𝑓)| |𝑌(𝑡, 𝑓)|<label>(3)</label></formula><p>where |𝑆(𝑡, 𝑓)| and |𝑌(𝑡, 𝑓)| represent spectral magnitudes of clean speech and noisy speech, respectively. Unlike the IRM, the SMM is not upper-bounded by 1. To obtain separated speech, we apply the SMM or its estimate to the spectral magnitudes of noisy speech, and resynthesize separated speech with the phases of noisy speech (or an estimate of clean speech phases). Fig. <ref type="figure" target="#fig_0">2</ref>(e) illustrates the SMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Phase-Sensitive Mask</head><p>The phase-sensitive mask (PSM) extends the SMM by including a measure of phase <ref type="bibr" target="#b40">[41]</ref>:</p><formula xml:id="formula_4">PSM(𝑡, 𝑓) = |𝑆(𝑡, 𝑓)| |𝑌(𝑡, 𝑓)| 𝑐𝑜𝑠 𝜃<label>(4)</label></formula><p>where 𝜃 denotes the difference of the clean speech phase and the noisy speech phase within the T-F unit. The inclusion of the phase difference in the PSM leads to a higher SNR, and tends to yield a better estimate of clean speech than the SMM <ref type="bibr" target="#b40">[41]</ref>. An example of the PSM is shown in Fig. <ref type="figure" target="#fig_0">2</ref>(f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Complex Ideal Ratio Mask</head><p>The complex ideal ratio mask (cIRM) is an ideal mask in the complex domain. Unlike the aforementioned masks, it can perfectly reconstruct clean speech from noisy speech <ref type="bibr" target="#b188">[188]</ref>: 𝑆 = 𝑐𝐼𝑅𝑀 * 𝑌 (5) where 𝑆, 𝑌 denote the STFT of clean speech and noisy speech, respectively, and ' * ' represents complex multiplication. Solving for mask components results in the following definition:</p><formula xml:id="formula_5">𝑐𝐼𝑅𝑀 = 𝑌 𝑟 𝑆 𝑟 + 𝑌 𝑖 𝑆 𝑖 𝑌 𝑟 2 + 𝑌 𝑖 2 + 𝑖 𝑌 𝑟 𝑆 𝑖 − 𝑌 𝑖 𝑆 𝑟 𝑌 𝑟 2 + 𝑌 𝑖 2<label>(6)</label></formula><p>where 𝑌 𝑟 and 𝑌 𝑖 denote real and imaginary components of noisy speech, respectively, and 𝑆 𝑟 and 𝑆 𝑖 real and imaginary components of clean speech, respectively. The imaginary unit is denoted by 'i'. Thus the cIRM has a real component and an imaginary component, which can be separately estimated in the real domain. Because of complex-domain calculations, mask values become unbounded. So some form of compression should be used to bound mask values, such as a tangent hyperbolic or sigmoidal function <ref type="bibr">[188] [184]</ref> .</p><p>Williamson et al. <ref type="bibr" target="#b188">[188]</ref> observe that, in Cartesian coordinates, structure exists in both real and imaginary components of the cIRM, whereas in polar coordinates, structure exists in the magnitude spectrogram but not phase spectrogram. Without clear structure, direct phase estimation would be intractable through supervised learning, although we should mention a recent paper that uses complex-domain DNN to estimate complex STFT coefficients <ref type="bibr" target="#b107">[107]</ref>. On the other hand, an estimate of the cIRM provides a phase estimate, a property not possessed by PSM estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.</head><p>Target Magnitude Spectrum The target magnitude spectrum (TMS) of clean speech, or |𝑆(𝑡, 𝑓)|, is a mapping-based training target <ref type="bibr">[116] [196]</ref> [57] <ref type="bibr" target="#b197">[197]</ref>. In this case supervised learning aims to estimate the magnitude spectrogram of clean speech from that of noisy speech. Power spectrum, or other forms of spectra such as mel spectrum, may be used instead of magnitude spectrum, and a log operation is usually applied to compress the dynamic range and facilitate training. A prominent form of the TMS is the log-power spectrum normalized to zero mean and unit variance <ref type="bibr" target="#b197">[197]</ref>. An estimated speech magnitude is then combined with noisy phase to produce the separated speech waveform. In terms of cost function, MSE is usually used for TMS estimation. Alternatively, maximum likelihood can be employed to train a TMS estimator that explicitly models output correlation <ref type="bibr" target="#b175">[175]</ref>. Fig. <ref type="figure" target="#fig_0">2</ref>(g) shows an example of the TMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.</head><p>Gammatone Frequency Target Power Spectrum Another closely related mapping-based target is the gammatone frequency target power spectrum (GF-TPS) <ref type="bibr" target="#b178">[178]</ref>.</p><p>Unlike the TMS defined on a spectrogram, this target is defined on a cochleagram based on a gammatone filterbank. Specifically, this target is defined as the power of the cochleagram response to clean speech. An estimate of the GF-TPS is easily converted to the separated speech waveform through cochleagram inversion <ref type="bibr" target="#b172">[172]</ref>. Fig. <ref type="figure" target="#fig_0">2</ref>(d) illustrates this target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Signal Approximation</head><p>The idea of signal approximation (SA) is to train a ratio mask estimator that minimizes the difference between the spectral magnitude of clean speech and that of estimated speech <ref type="bibr" target="#b186">[186]</ref> [81]:</p><formula xml:id="formula_6">𝑆𝐴(𝑡, 𝑓) = [𝑅𝑀(𝑡, 𝑓)|𝑌(𝑡, 𝑓)| − |𝑆(𝑡, 𝑓)|] 2<label>(7)</label></formula><p>𝑅𝑀(𝑡, 𝑓) refers to an estimate of the SMM. So, SA can be interpreted as a target that combines ratio masking and spectral mapping, seeking to maximize SNR <ref type="bibr" target="#b186">[186]</ref>. A related, earlier target aims for the maximal SNR in the context of IBM estimation <ref type="bibr" target="#b91">[91]</ref>. For the SA target, better separation performance is achieved with two-stage training <ref type="bibr" target="#b186">[186]</ref>. In the first stage, a learning machine is trained with the SMM as the target. In the second stage, the learning machine is fine-tuned by minimizing the loss function of <ref type="bibr" target="#b6">(7)</ref>.</p><p>A number of training targets have been compared using a fixed feedforward DNN with three hidden layers and the same set of input features <ref type="bibr" target="#b178">[178]</ref>. The separated speech using various training targets is evaluated in terms of STOI and PESQ, for predicted speech intelligibility and speech quality, respectively. In addition, a representative speech enhancement algorithm <ref type="bibr" target="#b65">[66]</ref> and a supervised nonnegative matrix factorization (NMF) algorithm <ref type="bibr" target="#b166">[166]</ref> are evaluated as benchmarks. The evaluation results are given in Figure <ref type="figure" target="#fig_1">3</ref>. A number of conclusions can be drawn from this study. First, in terms of objective intelligibility, the masking-based targets as a group outperform the mapping-based targets, although a recent study <ref type="bibr" target="#b155">[155]</ref> indicates that masking is advantageous only at higher input SNRs and at lower SNRs mapping is more advantageous 2 . In terms of speech quality, ratio masking performs better than binary masking. Particularly illuminating is the contrast between the SMM and the TMS, which are the same except for the use of |𝑌(𝑡, 𝑓)| in the denominator of the SMM (see ( <ref type="formula" target="#formula_3">3</ref>)). The better estimation of the SMM may be attributed to the fact that the target magnitude spectrum is insensitive to the interference signal and SNR, whereas the SMM is. The many-to-one mapping in the TMS makes its estimation potentially more difficult than SMM estimation. In addition, the estimation of unbounded spectral magnitudes tends to magnify estimation errors <ref type="bibr" target="#b178">[178]</ref>. Overall, the IRM and the SMM emerge as the preferred targets. In addition, DNN based ratio masking performs substantially better than supervised NMF and unsupervised speech enhancement.</p><p>The above list of training targets is not meant to be exhaustive, and other targets have been used in the literature. Perhaps the most straightforward target is the waveform (time- 2 The conclusion is also nuanced for speaker separation <ref type="bibr" target="#b206">[206]</ref>.</p><p>domain) signal of clean speech. This indeed was used in an early study that trains an MLP to map from a frame of noisy speech waveform to a frame of clean speech waveform, which may be called temporal mapping <ref type="bibr" target="#b160">[160]</ref>. Although simple, such direct mapping does not perform well even when a DNN is used in place of a shallow network <ref type="bibr">[182] [34]</ref>. In <ref type="bibr" target="#b182">[182]</ref>, a target is defined in the time domain but the DNN for target estimation includes modules for ratio masking and inverse Fourier transform with noisy phase. This target is closely related to the PSM <ref type="foot" target="#foot_0">3</ref> . A recent study evaluates oracle results of a number of ideal masks and additionally introduces the socalled ideal gain mask (IGM) <ref type="bibr" target="#b184">[184]</ref>, defined in terms of a priori SNR and a posteriori SNR commonly used in traditional speech enhancement <ref type="bibr" target="#b113">[113]</ref>. In <ref type="bibr" target="#b192">[192]</ref>, the so-called optimal ratio mask that takes into account of the correlation between target speech and background noise <ref type="bibr" target="#b110">[110]</ref> was evaluated and found to be an effective target for DNN-based speech separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.FEATURES</head><p>Features as input and learning machines play complementary roles in supervised learning. When features are discriminative, they place less demand on the learning machine in order to perform a task successfully. On the other hand, a powerful learning machine places less demand on features. At one extreme, a linear classifier, like Rosenblatt's perceptron, is all that is needed when features make a classification task linearly separable. At the other extreme, the input in the original form without any feature extraction (e.g. waveform in audio) suffices if the classifier is capable of learning appropriate features. In between are a majority of tasks where both feature extraction and learning are important.</p><p>Early studies in supervised speech separation use only a few features such as interaural time differences (ITD) and interaural level (intensity) differences (IID) <ref type="bibr" target="#b141">[141]</ref> in binaural separation, and pitch-based features <ref type="bibr" target="#b91">[91]</ref> [78] <ref type="bibr" target="#b54">[55]</ref> and amplitude modulation spectrogram (AMS) <ref type="bibr" target="#b97">[97]</ref> in monaural separation. A subsequent study <ref type="bibr" target="#b177">[177]</ref> explores more monaural features including mel-frequency cepstral coefficient (MFCC), gammatone frequency cepstral coefficient (GFCC) <ref type="bibr" target="#b150">[150]</ref>, perceptual linear prediction (PLP) <ref type="bibr" target="#b66">[67]</ref>, and relative spectral transform PLP (RASTA-PLP) <ref type="bibr" target="#b67">[68]</ref>. Through feature selection using group Lasso, the study recommends a complementary feature set comprising AMS, RASTA-PLP, and MFCC (and pitch if it can be reliably estimated), which has since been used in many studies.</p><p>We conducted a study to examine an extensive list of acoustic features for supervised speech separation at low SNRs <ref type="bibr" target="#b21">[22]</ref>. The features have been previously used for robust automatic speech recognition and classification-based speech separation. The feature list includes mel-domain, linearprediction, gammatone-domain, zero-crossing, autocorrelation, medium-time-filtering, modulation, and pitchbased features. The mel-domain features are MFCC and deltaspectral cepstral coefficient (DSCC) <ref type="bibr" target="#b104">[104]</ref>, which is similar to MFCC except that a delta operation is applied to mel-spectrum. The linear prediction features are PLP and RASTA-PLP. The three gammatone-domain features are gammatone feature (GF), GFCC, and gammatone frequency modulation coefficient (GFMC) <ref type="bibr" target="#b119">[119]</ref>. GF is computed by passing an input signal to a gammatone filterbank and applying a decimation operation to subband signals. A zero-crossing feature, called zero-crossings with peak-amplitudes (ZCPA) <ref type="bibr" target="#b96">[96]</ref>, computes zero-crossing intervals and corresponding peak amplitudes from subband signals derived using a gammatone filterbank. The autocorrelation features are relative autocorrelation sequence MFCC (RAS-MFCC) <ref type="bibr" target="#b204">[204]</ref>, autocorrelation sequence MFCC (AC-MFCC) <ref type="bibr" target="#b149">[149]</ref> and phase autocorrelation MFCC (PAC-MFCC) <ref type="bibr" target="#b86">[86]</ref>, all of which apply the MFCC procedure in the autocorrelation domain. The medium-time filtering features are power normalized cepstral coefficients (PNCC) <ref type="bibr" target="#b95">[95]</ref> and suppression of slowly-varying components and the falling edge of the power envelope (SSF) <ref type="bibr" target="#b94">[94]</ref>. The modulation domain features are Gabor filterbank (GFB) <ref type="bibr" target="#b145">[145]</ref> and AMS features. Pitch-based (PITCH) features calculate T-F level features based on pitch tracking and use periodicity and instantaneous frequency to discriminate speech-dominant T-F units from noise-dominant ones. In addition to existing features, we proposed a new feature called Multi-Resolution Cochleagram (MRCG) <ref type="bibr" target="#b21">[22]</ref>, which computes four cochleagrams at different spectrotemporal resolutions to provide both local information and a broader context.</p><p>The features are post-processed with the auto-regressive moving average (ARMA) filter <ref type="bibr" target="#b18">[19]</ref> and evaluated with a fixed MLP based IBM mask estimator. The estimated masks are evaluated in terms of classification accuracy and the HIT−FA rate. The HIT−FA results are shown in Table <ref type="table">1</ref>. As shown in the table, gammatone-domain features (MRCG, GF, and GFCC) consistently outperform the other features in both accuracy and HIT−FA rate, with MRCG performing the best. Cepstral compaction via discrete cosine transform (DCT) is not effective, as revealed by comparing GF and GFCC features. Neither is modulation extraction, as shown by comparing GFCC and GMFC, the latter calculated from the former. It is worth noting that the poor performance of pitch features is largely due to inaccurate estimation at low SNRs, as ground-truth pitch is shown to be quite discriminative.</p><p>Recently, Delfarah and Wang <ref type="bibr" target="#b33">[34]</ref> performed another feature study that considers room reverberation, and both speech denoising and speaker separation. Their study uses a fixed DNN trained to estimate the IRM, and the evaluation results are given in terms of STOI improvements over unprocessed noisy and reverberant speech. The features added in this study include log spectral magnitude (LOG-MAG) and log mel-spectrum feature (LOG-MEL), both of which are commonly used in supervised separation <ref type="bibr" target="#b196">[196]</ref>  <ref type="bibr" target="#b81">[82]</ref>. Also included is waveform signal (WAV) without any feature extraction. For reverberation, simulated room impulse responses (RIRs) and recorded RIRs are both used with reverberation time up to 0.9 seconds. For denoising, evaluation is done separately for matched noises where the first half of each nonstationary noise is used in training and second half for testing, and unmatched noises where completely new noises are used for testing. For cochannel (two-speaker) separation, the target talker is male while the interfering talker is either female or male. Table <ref type="table">2</ref> shows the STOI gains for the individual features evaluated. In the anechoic, matched noise case, STOI results are largely consistent with Table <ref type="table">1</ref>. Feature results are also broadly consistent using simulated and recorded RIRs. However, the best performing features are different for the matched noise, unmatched noise, and speaker separation cases. Besides MRCG, PNCC and GFCC produce the best results for the unmatched noise and cochannel condition, respectively. For feature combination, this study concludes that the most effective feature set consists of PNCC, GF, and LOG-MEL for speech enhancement, and PNCC, GFCC, and LOG-MEL for speaker separation.</p><p>The large performance differences caused by features in both Table <ref type="table">1</ref> and Table <ref type="table">2</ref> demonstrate the importance of features for supervised speech separation. The inclusion of raw waveform signal in Table <ref type="table">2</ref> further suggests that, without feature extraction, separation results are poor. But it should be noted that, the feedforward DNN used in <ref type="bibr" target="#b33">[34]</ref> may not couple well with waveform signals, and CNNs and RNNs may be better suited for so-called end-to-end separation. We will come to this issue later.</p><p>Table <ref type="table">1</ref>. Classification performance of a list of acoustic features in terms of HIT−FA (in %) for six noises at -5 dB SNR, where FA is shown in parentheses (from <ref type="bibr" target="#b21">[22]</ref>). Boldtype indicates best scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.MONAURAL SEPARATION ALGORITHMS</head><p>In this section, we discuss monaural algorithms for speech enhancement, speech dereverberation as well as dereverberation plus denoising, and speaker separation. We explain representative algorithms and discuss generalization of supervised speech separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.</head><p>Speech Enhancement To our knowledge, deep learning was first introduced to speech separation by Wang and Wang in 2012 in two conference papers <ref type="bibr" target="#b179">[179]</ref>  <ref type="bibr" target="#b180">[180]</ref>, which were later extended to a journal version in 2013 <ref type="bibr" target="#b181">[181]</ref>. They used DNN for subband classification to estimate the IBM. In the conference versions, feedforward DNNs with RBM pretraining were used as binary classifiers, as well as feature encoders for structured perceptrons <ref type="bibr" target="#b179">[179]</ref> and conditional random fields <ref type="bibr" target="#b180">[180]</ref>. They reported strong separation results in all cases of DNN usage, with better results for DNN used for feature learning due to the incorporation of temporal dynamics in structured prediction.</p><p>In the journal version <ref type="bibr" target="#b181">[181]</ref>, the input signal is passed through a 64-channel gammatone filterbank to derive subband signals, from which acoustic features are extracted within each T-F unit. These features form the input to subband DNNs (64 in total) to learn more discriminative features. This use of DNN for speech separation is illustrated in Figure <ref type="figure" target="#fig_2">4</ref>. After DNN training, input features and learned features of the last hidden layer are concatenated and fed to linear SVMs to estimate the subband IBM efficiently. This algorithm was further extended to a two-stage DNN <ref type="bibr" target="#b64">[65]</ref>, where the first stage is trained to estimate the subband IBM as usual and the second stage explicitly incorporates the T-F context in the following way. After the first-stage DNN is trained, a unitlevel output before binarization can be interpreted as the posterior probability that speech dominates the T-F unit. Hence the first-stage DNN output is considered a posterior mask. In the second stage, a T-F unit takes as input a local window of the posterior mask centered at the unit. The twostage DNN is illustrated in Fig. <ref type="figure" target="#fig_3">5</ref>. This second-stage structure Table <ref type="table">2</ref>. STOI improvements (in %) for a list of features averaged on a set of test noises (from <ref type="bibr" target="#b33">[34]</ref>). "Sim." and "Rec." indicate simulated and recorded room impulse responses. Boldface indicates the best scores in each condition. In cochannel (two-talker) cases, the performance is shown separately for a female interferer and male interferer (in parentheses) with a male target talker.  is reminiscent of a convolutional layer in CNN but without weight sharing. This way of leveraging contextual information is shown to significantly improve classification accuracy.</p><p>Subject tests demonstrate that this DNN produced large intelligibility improvements for both HI and NH listeners, with HI listeners benefiting more <ref type="bibr" target="#b64">[65]</ref>. This is the first monaural algorithm to provide substantial speech intelligibility improvements for HI listeners in background noise, so much so that HI subjects with separation outperformed NH subjects without separation.</p><p>In 2013, Lu et al. <ref type="bibr" target="#b116">[116]</ref> published an Interspeech paper that uses a deep autoencoder (DAE) for speech enhancement. A basic autoencoder (AE) is an unsupervised learning machine, typically having a symmetric architecture with one hidden layer with tied weights, that learns to map an input signal to itself. Multiple trained AEs can be stacked into a DAE that is then subject to traditional supervised fine-tuning, e.g. with a backpropagation algorithm. In other words, autoencoding is an alternative to RBM pretraining. The algorithm in <ref type="bibr" target="#b116">[116]</ref> learns to map from the mel-frequency power spectrum of noisy speech to that of clean speech, so it can be regarded as the first mapping based method <ref type="foot" target="#foot_1">4</ref> .</p><p>Subsequently, but independent of <ref type="bibr" target="#b116">[116]</ref>, Xu et al. <ref type="bibr" target="#b196">[196]</ref> published a study using a DNN with RBM pretraining to map from the log power spectrum of noisy speech to that of clean speech, as shown in Fig. <ref type="figure">6</ref>. Unlike <ref type="bibr" target="#b116">[116]</ref>, the DNN used in <ref type="bibr" target="#b196">[196]</ref> is a standard feedforward MLP with RBM pretraining. After training, DNN estimates clean speech's spectrum from a noisy input. Their experimental results show that the trained DNN yields about 0.4 to 0.5 PESQ gains over noisy speech on untrained noises, which are higher than those obtained by a representative traditional enhancement method.</p><p>Many subsequent studies have since been published along the lines of T-F masking and spectral mapping. In <ref type="bibr" target="#b186">[186]</ref> [185], RNNs with LSTM were used for speech enhancement and its application to robust ASR, where training aims for signal approximation (see Sect. III.I). RNNs were also used in <ref type="bibr" target="#b40">[41]</ref> to estimate the PSM. In <ref type="bibr" target="#b132">[132]</ref> [210], a deep stacking network was proposed for IBM estimation and a mask estimate was then used for pitch estimation. The accuracy of both mask estimation and pitch estimation improves after the two modules iterate for several cycles. A DNN was used to simultaneously estimate the real and imaginary components of the cIRM, yielding better speech quality over IRM estimation <ref type="bibr" target="#b188">[188]</ref>. Speech enhancement at the phoneme level has been recently studied <ref type="bibr" target="#b183">[183]</ref>  <ref type="bibr" target="#b17">[18]</ref>. In <ref type="bibr" target="#b58">[59]</ref>, the DNN takes into account of perceptual masking with a piecewise gain function. In <ref type="bibr" target="#b198">[198]</ref>, multi-objective learning is shown to improve enhancement performance. It has been demonstrated that a hierarchical DNN performing subband spectral mapping yields better enhancement than a single DNN performing fullband mapping <ref type="bibr" target="#b38">[39]</ref>. In <ref type="bibr" target="#b161">[161]</ref>, skip connections between non-consecutive layers are added to DNN to improve enhancement performance. Multi-target training with both Figure <ref type="figure">6</ref>. Diagram of a DNN-based spectral mapping method for speech enhancement (from <ref type="bibr" target="#b196">[196]</ref>). The feature extraction and waveform reconstruction modules are further detailed. masking and mapping based targets is found to outperform single-target training <ref type="bibr" target="#b205">[205]</ref>. CNNs have also been used for IRM estimation <ref type="bibr" target="#b83">[83]</ref> and spectral mapping <ref type="bibr" target="#b45">[46]</ref>  <ref type="bibr" target="#b136">[136,</ref><ref type="bibr" target="#b138">138]</ref>.</p><p>Aside from masking and mapping based approaches, there is recent interest in using deep learning to perform end-to-end separation, i.e. temporal mapping without resorting to a T-F representation. A potential advantage of this approach is to circumvent the need to use the phase of noisy speech in reconstructing enhanced speech, which can be a drag for speech quality, particularly when input SNR is low. Recently, Fu et al. <ref type="bibr" target="#b46">[47]</ref> developed a fully convolutional network (a CNN with fully connected layers removed) for speech enhancement. They observe that full connections make it difficult to map both high and low frequency components of a waveform signal, and with their removal, enhancement results improve. As a convolution operator is the same as a filter or a feature extractor, CNNs appear to be a natural choice for temporal mapping.</p><p>A recent study employs a GAN to perform temporal mapping <ref type="bibr" target="#b138">[138]</ref>. In the so-called speech enhancement GAN (SEGAN), the generator is a fully convolutional network, performing enhancement or denoising. The discriminator follows the same convolutional structure as G, and it transmits information of generated waveform signals versus clean signals back to G. D can be viewed as providing a trainable loss function for G. SEGAN was evaluated on untrained noisy conditions, but the results are inconclusive and worse than masking or mapping methods. In another GAN study <ref type="bibr" target="#b122">[122]</ref>, G tries to enhance the spectrogram of noisy speech while D tries to distinguish between the enhanced spectrograms and those of clean speech. The comparisons in <ref type="bibr" target="#b122">[122]</ref> show that the enhancement results by this GAN are comparable to those achieved by a DNN. Not all deep learning based speech enhancement methods build on DNNs. For example, Le Roux et al. <ref type="bibr" target="#b105">[105]</ref> proposed deep NMF that unfolds NMF operations and includes multiplicative updates in backpropagation. Vu et al. <ref type="bibr" target="#b167">[167]</ref> presented an NMF framework in which a DNN is trained to map NMF activation coefficients of noisy speech to their clean version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.</head><p>Generalization of Speech Enhancement Algorithms For any supervised learning task, generalization to untrained conditions is a crucial issue. In the case of speech enhancement, data-driven algorithms bear the burden of proof when it comes to generalization, because the issue does not arise in traditional speech enhancement and CASA algorithms which make minimal use of supervised training. Supervised enhancement has three aspects of generalization: noise, speaker, and SNR. Regarding SNR generalization, one can simply include more SNR levels in a training set and practical experience shows that supervised enhancement is not sensitive to precise SNRs used in training. Part of the reason is that, even though a few mixture SNRs are included in training, local SNRs at the frame level and T-F unit level usually vary over a wide range, providing a necessary variety for a learning machine to generalize well. An alternative strategy is to adopt progressive training with increasing numbers of hidden layers Figure <ref type="figure">7</ref>. DNN architecture for speech enhancement with an autoencoder for unsupervised adaptation (from <ref type="bibr" target="#b98">[98]</ref>). The AE stacked on top of a DNN serves as a purity checker for estimated clean speech from the bottom DNN. 𝑆 (1) denotes the spectrum of a speech signal, 𝑆 (2) the spectrum of a noise signal, and 𝑆 (1)  ̃ an estimate of 𝑆 (1) .</p><p>to handle lower SNR conditions <ref type="bibr" target="#b47">[48]</ref>.</p><p>In an effort to address the mismatch between training and test conditions, Kim and Smaragdis <ref type="bibr" target="#b98">[98]</ref> proposed a two-stage DNN where the first stage is a standard DNN to perform spectral mapping and the second stage is an autoencoder that performs unsupervised adaptation during the test stage. The AE is trained to map the magnitude spectrum of a clean utterance to itself, much like <ref type="bibr" target="#b115">[115]</ref>, and hence its training does not need labeled data. The AE is then stacked on top of the DNN, and serves as a purity checker as shown in Fig. <ref type="figure">7</ref>. The rationale is that well enhanced speech tends to produce a small difference (error) between the input and the output of the AE, whereas poorly enhanced speech should produce a large error. Given a test mixture, the already-trained DNN is fine-tuned with the error signal coming from the AE. The introduction of an AE module provides a way of unsupervised adaptation to test conditions that are quite different from the training conditions, and is shown to improve the performance of speech enhancement.</p><p>Noise generalization is fundamentally challenging as all kinds of stationary and nonstationary noises may interfere with a speech signal. When available training noises are limited, one technique is to expand training noises through noise perturbation, particularly frequency perturbation <ref type="bibr" target="#b22">[23]</ref>; specifically, the spectrogram of an original noise sample is perturbed to generate new noise samples. To make the DNNbased mapping algorithm of Xu et al. <ref type="bibr" target="#b196">[196]</ref> more robust to new noises, Xu et al. <ref type="bibr" target="#b195">[195]</ref> incorporate noise aware training, i.e. the input feature vector includes an explicit noise estimate. With noise estimated via binary masking, the DNN with noise aware training generalizes better to untrained noises.</p><p>Noise generalization is systematically addressed in <ref type="bibr" target="#b23">[24]</ref>. The DNN in this study was trained to estimate the IRM at the frame level. In addition, the IRM is simultaneously estimated over several consecutive frames and different estimates for the same frame are averaged to produce a smoother, more accurate mask (see also <ref type="bibr" target="#b178">[178]</ref>). The DNN has five hidden layers with 2048 ReLUs in each. The input features for each frame are cochleagram response energies (the GF feature in Tables <ref type="table">1 and 2</ref>). The training set includes 640,000 mixtures created from 560 IEEE sentences and 10,000 (10K) noises from a sound effect library (www.sound-ideas.com) at the fixed SNR of -2 dB. The total duration of the noises is about 125 hours, and the total duration of training mixtures is about 380 hours. To evaluate the impact of the number of training noises on noise generalization, the same DNN is also trained with 100 noises as done in <ref type="bibr" target="#b181">[181]</ref>. The test sets are created using 160 IEEE sentences and nonstationary noises at various SNRs. Neither test sentences nor test noises are used during training. The separation results measured in STOI are shown in Table <ref type="table" target="#tab_2">3</ref>, and large STOI improvements are obtained by the 10K-noise model. In addition, the 10K-noise model substantially outperforms the 100-noise model, and its average performance matches the noise-dependent models trained with the first half of the training noises and tested with the second half. Subject tests show that the noise-independent model resulting from large-scale training significantly improves speech intelligibility for NH and HI listeners in unseen noises. This study strongly suggests that large-scale training with a wide variety of noises is a promising way to address noise generalization.</p><p>As for speaker generalization, a separation system trained on a specific speaker would not work well for a different speaker. A straightforward attempt for speaker generalization would be to train with a large number of speakers. However, experimental results <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b100">[100]</ref> show that a feedforward DNN appears incapable of modeling a large number of talkers. Such a DNN typically takes a window of acoustic features for mask estimation, without using the long-term context. Unable to track a target speaker, a feedforward network has a tendency to mistake noise fragments for target speech. RNNs naturally model temporal dependencies, and are thus expected to be more suitable for speaker generalization than feedforward DNN.</p><p>We have recently employed RNN with LSTM to address speaker generalization of noise-independent models <ref type="bibr" target="#b19">[20]</ref>. The model, shown in Figure <ref type="figure">8</ref>, is trained on 3,200,000 mixtures created from 10,000 noises mixed with 6, 10, 20, 40, and 77 speakers. When tested with trained speakers, as shown in Fig. <ref type="figure" target="#fig_5">9</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Speech Dereverberation and Denoising</head><p>In a real environment, speech is usually corrupted by reverberation from surface reflections. Room reverberation corresponds to a convolution of the direct signal and an RIR, and it distorts speech signals along both time and frequency. Reverberation is a well-recognized challenge in speech processing, particularly when it is combined with background noise. As a result, dereverberation has been actively investigated for a long time <ref type="bibr">[5] [191]</ref> [131] <ref type="bibr" target="#b60">[61]</ref>.</p><p>Han et al. <ref type="bibr" target="#b56">[57]</ref> proposed the first DNN based approach to speech dereverberation. This approach uses spectral mapping on a cochleagram. In other words, a DNN is trained to map from a window of reverberant speech frames to a frame of anechoic speech, as illustrated in Fig. <ref type="figure" target="#fig_6">10</ref>. The trained DNN can reconstruct the cochleagram of anechoic speech with surprisingly high quality. In their later work <ref type="bibr" target="#b57">[58]</ref>, they apply spectral mapping on a spectrogram and extend the approach to perform both dereverberation and denoising. A more sophisticated system was proposed recently by Wu et al. <ref type="bibr" target="#b190">[190]</ref>, who observe that dereverberation performance improves when frame length and shift are chosen differently depending on the reverberation time (T60). Based on this observation, their system includes T60 as a control parameter in feature extraction and DNN training. During the dereverberation stage, T60 is estimated and used to choose appropriate frame length and shift for feature extraction. This so-called reverberation-time-aware model is illustrated in Fig. <ref type="figure" target="#fig_7">11</ref>. Their comparisons show an improvement in dereverberation performance over the DNN in <ref type="bibr" target="#b57">[58]</ref>.</p><p>To improve the estimation of anechoic speech from reverberant and noisy speech, Xiao et al. <ref type="bibr" target="#b194">[194]</ref> proposed a DNN trained to predict static, delta and acceleration features at the same time. The static features are log magnitudes of clean speech, and the delta and acceleration features are derived from the static features. It is argued that DNN that predicts static features well should also predict delta and acceleration features well. The incorporation of dynamic features in the DNN structure helps to improve the estimation of static features for dereverberation.</p><p>Zhao et al. <ref type="bibr" target="#b211">[211]</ref> observe that spectral mapping is more effective for dereverberation than T-F masking, whereas masking works better than mapping for denoising. Consequently, they construct a two-stage DNN where the first stage performs ratio masking for denoising and the second stage spectral mapping for dereverberation. Furthermore, to alleviate the adverse effects of using the phase of reverberantnoisy speech in resynthesizing the waveform signal of enhanced speech, this study extends the time-domain signal reconstruction technique in <ref type="bibr" target="#b182">[182]</ref>. Here the training target is defined in the time-domain, but clean phase is used during training unlike in <ref type="bibr" target="#b182">[182]</ref> where noisy phase is used. The two stages are individually trained first, and then jointly trained. The results in <ref type="bibr" target="#b211">[211]</ref> show that the two-stage DNN model significantly outperforms the single-stage models for either mapping or masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Speaker Separation</head><p>The goal of speaker separation is to extract multiple speech signals, one for each speaker, from a mixture containing two or more voices. After deep learning was demonstrated to be capable of speech enhancement, DNN has been successfully applied to speaker separation under a similar framework, which is illustrated in Figure <ref type="figure" target="#fig_8">12</ref> in the case of two-speaker or cochannel separation.</p><p>According to our literature search, Huang et al. <ref type="bibr" target="#b80">[81]</ref> were the first to introduce DNN for this task. This study addresses two-speaker separation using both a feedforward DNN and an RNN. The authors argue that the summation of the spectra of two estimated sources at frame t, 𝑺 ̂1(𝑡) and 𝑺 ̂2(𝑡) , is not guaranteed to equal the spectrum of the mixture. Therefore, a masking layer is added to the network, which produces two final outputs shown in the following equations:</p><formula xml:id="formula_7">𝑺 ̃1(𝑡) = |𝑺 ̂1(𝑡)| |𝑺 ̂1(𝑡)| + |𝑺 ̂2(𝑡)| ⊙ 𝒀(𝑡) (8) 𝑺 ̃2(𝑡) = |𝑺 ̂2(𝑡)| |𝑺 ̂1(𝑡)| + |𝑺 ̂2(𝑡)| ⊙ 𝒀(𝑡)<label>(9)</label></formula><p>where 𝒀(𝑡) denotes the mixture spectrum at t. This amounts to a signal approximation training target introduced in Section III.I. Both binary and ratio masking are found to be effective. In addition, discriminative training is applied to maximize the  <ref type="bibr" target="#b190">[190]</ref>).</p><p>difference between one speaker and the estimated version of the other. During training, the following cost is minimized:</p><formula xml:id="formula_8">1 2 ∑(‖𝑺 1 (𝑡) − 𝑺 ̃1(𝑡)‖ 2 + ‖𝑺 2 (𝑡) − 𝑺 ̃2(𝑡)‖ 2 𝑡 − 𝛾‖𝑺 1 (𝑡) − 𝑺 ̃2(𝑡)‖ 2 − 𝛾‖𝑺 2 (𝑡) − 𝑺 ̃1(𝑡)‖ 2 ) (<label>10</label></formula><formula xml:id="formula_9">)</formula><p>where 𝑺 1 (𝑡) and 𝑺 2 (𝑡) denote the ground truth spectra for Speaker 1 and Speaker 2, respectively, and 𝛾 is a tunable parameter. Experimental results have shown that both the masking layer and discriminative training improve speaker separation <ref type="bibr" target="#b81">[82]</ref>.</p><p>A few months later, Du et al. <ref type="bibr" target="#b37">[38]</ref> appeared to have independently proposed a DNN for speaker separation similar to <ref type="bibr" target="#b80">[81]</ref>. In this study <ref type="bibr" target="#b37">[38]</ref>, the DNN is trained to estimate the log power spectrum of the target speaker from that of a cochannel mixture. In a different paper <ref type="bibr" target="#b162">[162]</ref>, they trained a DNN to map a cochannel signal to the spectrum of the target speaker as well as the spectrum of an interfering speaker, as illustrated in Fig. <ref type="figure" target="#fig_8">12</ref> (see <ref type="bibr" target="#b36">[37]</ref> for an extended version). A notable extension compared to <ref type="bibr" target="#b80">[81]</ref> is that these papers also address the situation where only the target speaker is the same between training and testing, while interfering speakers are different between training and testing.</p><p>In speaker separation, if the underlying speakers are not allowed to change from training to testing, this is the speaker-dependent situation. If interfering speakers are allowed to change, but the target speaker is fixed, this is called targetdependent speaker separation. In the least constrained case where none of the speakers are required to be the same between training and testing, this is called speakerindependent. From this perspective, Huang et al.'s approach is speaker dependent <ref type="bibr">[81] [82]</ref> and the studies in <ref type="bibr" target="#b37">[38]</ref>  <ref type="bibr" target="#b162">[162]</ref> deal with both speaker and target dependent separation. Their way of relaxing the constraint on interfering speakers is simply to train with cochannel mixtures of the target speaker and many interferers.</p><p>Zhang and Wang proposed a deep ensemble network to address speaker-dependent as well as target-dependent separation <ref type="bibr" target="#b206">[206]</ref>. They employ multi-context networks to integrate temporal information at different resolutions. An ensemble is constructed by stacking multiple modules, each performing multi-context masking or mapping. Several training targets were examined in this study. For speakerdependent separation, signal approximation is shown to be most effective; for target-dependent separation, a combination of ratio masking and signal approximation is most effective. Furthermore, the performance of target-dependent separation is close to that of speaker-dependent separation. Recently, Wang et al. <ref type="bibr" target="#b174">[174]</ref> took a step further towards relaxing speaker dependency in talker separation. Their approach clusters each speaker into one of the four clusters (two for male and two for female), and then trains a DNN-based gender mixture detector to determine the clusters of the two underlying speakers in a mixture. Although trained on a subset of speakers in each cluster, their evaluation results show that the speaker separation approach works well for the other untrained speakers in each cluster; in other words, this speaker separation approach exhibits a degree of speaker independency.</p><p>Healy et al. <ref type="bibr" target="#b62">[63]</ref> have recently used a DNN for speakerdependent cochannel separation and performed speech intelligibility evaluation of the DNN with both HI and NH listeners. The DNN was trained to estimate the IRM and its complement, corresponding to the target talker and interfering talker. Compared to earlier DNN-based cochannel separation studies, the algorithm in <ref type="bibr" target="#b62">[63]</ref> uses a diverse set of features and predicts multiple IRM frames, resulting in better separation. The intelligibility results are shown in Figure <ref type="figure" target="#fig_1">13</ref>. For the HI Figure <ref type="figure" target="#fig_1">13</ref>. Mean intelligibility scores and standard errors for HI and NH subjects listening to target sentences mixed with interfering sentences and separated target sentences (from <ref type="bibr" target="#b62">[63]</ref>). Percent correct results are given at four different target-tointerferer ratios. group, intelligibility improvement from DNN-based separation is 42.5, 49.2, and 58.7 percentage points at -3 dB, -6 dB, and -9 dB target-to-interferer ratio (TIR), respectively. For the NH group, there are statistically significant improvements, but to a smaller extent. It is remarkable that the large intelligibility improvements obtained by HI listeners allow them to perform equivalently to NH listeners (without algorithm help) at the common TIRs of -6 and -9 dB. Speaker-independent separation can be treated as unsupervised clustering where T-F units are clustered into distinct classes dominated by individual speakers <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b78">[79]</ref>. Clustering is a flexible framework in terms of the number of speakers to separate, but it does not benefit as much from discriminative information fully utilized in supervised training. Hershey et al. were the first to address speaker-independent multi-talker separation in the DNN framework <ref type="bibr" target="#b68">[69]</ref>. Their approach, called deep clustering, combines DNN based feature learning and spectral clustering. With a ground truth partition of T-F units, the affinity matrix 𝐴 can be computed as: 𝑨 = 𝒀𝒀 𝑇 (11) where 𝒀 is the indicator matrix built from the IBM. 𝑌 𝑖,𝑐 is set to 1 if unit 𝑖 belongs to (or dominated by) speaker 𝑐, and 0 otherwise. The DNN is trained to embed each T-F unit. The estimated affinity matrix 𝑨 ̂ can be derived from the embeddings. The DNN learns to output similar embeddings for T-F units originating from the same speaker by minimizing the following cost function:</p><formula xml:id="formula_10">𝐶 𝒀 (𝑽) = ‖𝑨 ̂− 𝑨‖ 𝐹 2 = ‖𝑽𝑽 𝑇 − 𝒀𝒀 𝑇 ‖ 𝐹 2<label>(12)</label></formula><p>where 𝑽 is an embedding matrix for T-F units. Each row of 𝑽 represents one T-F unit. ‖ • ‖ 𝐹 2 denotes the squared Frobenius norm. Low rank formulation can be applied to efficiently calculate the cost function and its derivatives. During inference, a mixture is segmented and the embedding matrix 𝑽 is computed for each segment. Then, the embedding matrices of all segments are concatenated. Finally, the K-means algorithm is applied to cluster the T-F units of all the segments into speaker clusters. Segment-level clustering is more accurate than utterance-level clustering, but with clustering results only for individual segments, the problem of sequential organization has to be addressed. Deep clustering is shown to produce high quality speaker separation, significantly better than a CASA method <ref type="bibr" target="#b78">[79]</ref> and an NMF method for speakerindependent separation.</p><p>A recent extension of deep clustering is the deep attractor network <ref type="bibr" target="#b24">[25]</ref>, which also learns high-dimensional embeddings for T-F units. Unlike deep clustering, this deep network creates attractor points akin to cluster centers in order to pull T-F units dominated by different speakers to their corresponding attractors. Speaker separation is then performed as mask estimation by comparing embedded points and each attractor. The results in <ref type="bibr" target="#b24">[25]</ref> show that the deep attractor network yields better results than deep clustering.</p><p>While clustering-based methods naturally lead to speakerindependent models, DNN based masking/mapping methods tie each output of the DNN to a specific speaker, and lead to speaker-dependent models. For example, mapping based methods minimize the following cost function:</p><formula xml:id="formula_11">𝐽 = ∑‖|𝑺 ̃𝑘(𝑡)| − |𝑺 𝑘 (𝑡)|‖ 2 𝑘,𝑡<label>(13)</label></formula><p>where |𝑺 ̃𝑘(𝑡)| and |𝑺 𝑘 (𝑡)| denote estimated and actual spectral magnitudes for speaker k, respectively, and t denotes time frame. To untie DNN outputs from speakers and train a speaker-independent model using a masking or mapping technique, Yu et al. <ref type="bibr" target="#b202">[202]</ref> recently proposed permutationinvariant training, which is shown in Fig. <ref type="figure" target="#fig_2">14</ref>. For two-speaker separation, a DNN is trained to output two masks, each of which is applied to noisy speech to produce a source estimate. During DNN training, the cost function is dynamically calculated. If we assign each output to a reference speaker |𝑺 𝑘 (𝑡)| in the training data, there are two possible assignments, each of which is associated with an MSE. The assignment with the lower MSE is chosen and the DNN is trained to minimize the corresponding MSE. During both training and inference, the DNN takes a segment or multiple frames of features, and estimates two sources for the segment. Since the two outputs of the DNN are not tied to any speaker, the same speaker may switch from one output to another Figure <ref type="figure" target="#fig_2">14</ref>. Two-talker separation with permutation-invariant training (from <ref type="bibr" target="#b202">[202]</ref>). across consecutive segments. Therefore, the estimated segment-level sources need to be sequentially organized unless segments are as long as utterances. Although much simpler, speaker separation results are shown to match those obtained with deep clustering <ref type="bibr" target="#b202">[202]</ref>  <ref type="bibr" target="#b101">[101]</ref>.</p><p>It should be noted that, although speaker separation evaluations typically focus on two-speaker mixtures, the separation framework can be generalized to separating more than two talkers. For example, the diagrams in both Figs. 12 and 14 can be straightforwardly extended to handle, say, threetalker mixtures. One can also train target-independent models using multi-speaker mixtures. For speaker-independent separation, deep clustering <ref type="bibr" target="#b68">[69]</ref> and permutation-invariant training <ref type="bibr" target="#b101">[101]</ref> are both formulated for multi-talker mixtures and evaluated on such data. Scaling deep clustering from mixtures of two speakers to more than two is more straightforward than for scaling permutation-invariant training.</p><p>An insight from the body of work overviewed in this speaker separation subsection is that a DNN model trained with many pairs of different speakers is able to separate a pair of speakers never included in training, a case of speaker independent separation, but only at the frame level. For speaker-independent separation, the key issue is how to group well-separated speech signals at individual frames (or segments) across time. This is precisely the issue of sequential organization, which is much investigated in CASA <ref type="bibr" target="#b172">[172]</ref>. Permutation-invariant training may be considered as imposing sequential grouping constraints during DNN training. On the other hand, typical CASA methods utilize pitch contours, vocal tract characteristics, rhythm or prosody, and even common spatial direction when multiple sensors are available, which do not usually involve supervised learning. It seems to us that integrating traditional CASA techniques and deep learning is a fertile ground for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ARRAY SEPARATION ALGORITHMS</head><p>An array of microphones provides multiple monaural recordings, which contain information indicative of the spatial origin of a sound source. When sound sources are spatially separated, with sensor array inputs one may localize sound sources and then extract the source from the target location or direction. Traditional approaches to source separation based on spatial information include beamforming, as mentioned in Sect. I, and independent component analysis <ref type="bibr" target="#b7">[8]</ref> [85] <ref type="bibr" target="#b2">[3]</ref>.</p><p>Sound localization and location-based grouping are among the classic topics in auditory perception and CASA <ref type="bibr" target="#b11">[12]</ref> [15] <ref type="bibr" target="#b172">[172]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Separation Based on Spatial Feature Extraction</head><p>The first study in supervised speech segregation was conducted by Roman et al. <ref type="bibr" target="#b141">[141]</ref> in the binaural domain. This study performs supervised classification to estimate the IBM based on two binaural features: ITD and ILD, both extracted from individual T-F unit pairs from the left-ear and right-ear cochleagram. Note that, in this case, the IBM is defined on the noisy speech at a single ear (reference channel). Classification is based on maximum a posteriori (MAP) probability where the likelihood is given by a density estimation technique. Another classic two-sensor separation technique, DUET (Degenerate Unmixing Estimation Technique), was published by Yilmaz and Rickard <ref type="bibr" target="#b199">[199]</ref> at about the same time. DUET is based on unsupervised clustering, and the spatial features used are phase and amplitude differences between the two microphones. The contrast between classification and clustering in these studies is a persistent theme and anticipates similar contrasts in later studies, e.g. binary masking <ref type="bibr" target="#b70">[71]</ref> vs. clustering <ref type="bibr" target="#b71">[72]</ref> for beamforming (see Sect. VI.B), and deep clustering <ref type="bibr" target="#b68">[69]</ref> versus mask estimation <ref type="bibr" target="#b101">[101]</ref> for talkerindependent speaker separation (see Sect. V.D).</p><p>The use of spatial information afforded by an array as features in deep learning is a straightforward extension of the earlier use of DNN in monaural separation; one simply substitutes spatial features for monaural features. Indeed, this way of leveraging spatial information provides a natural framework for integrating monaural and spatial features for source separation, which is a point worth emphasizing as traditional research tends to pursue array separation without considering monaural grouping. It is worth noting that human auditory scene analysis integrates monaural and binaural analysis in a seamless fashion, taking advantage of whatever discriminant information existing in a particular environment <ref type="bibr" target="#b14">[15]</ref> [172] <ref type="bibr" target="#b29">[30]</ref>.</p><p>The first study to employ DNN for binaural separation was published by Jiang et al. <ref type="bibr" target="#b90">[90]</ref>. In this study, the signals from two ears (or microphones) are passed to two corresponding auditory filterbanks. ITD and ILD features are extracted from T-F unit pairs and sent to a subband DNN for IBM estimation, one DNN for each frequency channel. In addition, a monaural feature (GFCC, see Table <ref type="table">1</ref>) is extracted from the left-ear input. A number of conclusions can be drawn from this study. Perhaps most important is the observation that the trained DNN generalizes well to untrained spatial configurations of sound sources. A spatial configuration refers to a specific placement of sound sources and sensors in an acoustic environment. This is key to the use of supervised learning as there are infinite configurations and a training set cannot enumerate various configurations. DNN based binaural separation is found to generalize well to RIRs and reverberation times. It is also observed that the incorporation of the monaural feature improves separation performance, especially when the target and interfering sources are colocated or close to each other.</p><p>Araki et al. <ref type="bibr" target="#b1">[2]</ref> subsequently employed a DNN for spectral mapping that includes the spatial features of ILD, interaural phase difference (IPD), and enhanced features with an initial mask derived from location information, in addition to monaural input. Their evaluation with ASR related metrics shows that the best enhancement performance is obtained with a combination of monaural and enhanced features. Fan et al. <ref type="bibr" target="#b42">[43]</ref> proposed a spectral mapping approach utilizing both binaural and monaural inputs. For the binaural features, this study uses subband ILDs, which are found to be more effective than fullband ILDs. These features are then concatenated with the left-ear's frame-level log power spectra to form the input to the DNN, which is trained to map to the spectrum of clean speech. A quantitative comparison with <ref type="bibr" target="#b90">[90]</ref> shows that their system produces better PESQ scores for separated speech but similar STOI numbers.</p><p>A more sophisticated binaural separation algorithm was proposed by Yu et al. <ref type="bibr" target="#b203">[203]</ref>. The spatial features used include IPD, ILD, and a so-called mixing vector that is a form of combined STFT values of a unit pair. The DNN used is a DAE, first trained unsupervisedly as autoencoders that are subsequently stacked into a DNN subject to supervised finetuning. Extracted spatial features are first mapped to highlevel indicating spatial directions via unsupervised DAE training. For separation, a classifier is trained to map high-level spatial features to a discretized range of source directions. This algorithm operates over subbands, each covering a block of consecutive frequency channels.</p><p>Recently, Zhang and Wang <ref type="bibr" target="#b208">[208]</ref> developed a DNN for IRM estimation with a more sophisticated set of spatial and spectral features. Their algorithm is illustrated in Fig. <ref type="figure" target="#fig_9">15</ref>, where the left-ear and right-ear inputs are fed to two modules for spectral (monaural) and spatial (binaural) analysis. Instead of monaural analysis on a single ear <ref type="bibr" target="#b90">[90]</ref> [43], spectral analysis in <ref type="bibr" target="#b208">[208]</ref> is conducted on the output of a fixed beamformer, which itself removes some background inference, by extracting a complementary set of monaural features (see Sect. IV). For spatial analysis, ITD in the form of a cross-correlation function, and ILD are extracted. The spectral and spatial features are concatenated to form the input to a DNN for IRM estimation at the frame level. This algorithm is shown to produce substantially better separation results in reverberant multisource environments than conventional beamformers, including MVDR (Minimum Variance Distortionless Response) and MWF (Multichannel Wiener Filter). An interesting observation from their analysis is that much of the benefit of using a beamformer prior to spectral feature extraction can be obtained simply by concatenating monaural features from the two ears.</p><p>Although the above methods are all binaural, involving two sensors, the extension from two sensors to an array with N sensors, with N &gt; 2, is usually straightforward. Take the system in Fig. <ref type="figure" target="#fig_9">15</ref>, for instance. With N microphones, spectral feature extraction requires no change as traditional beamformers are already formulated for an arbitrary number of microphones. For spatial feature extraction, the feature space needs to be expanded when more than two sensors are available, either by designating one microphone as a reference for deriving a set of "binaural" features or by considering a matrix of all sensor pairs in a correlation or covariance analysis. The output is a T-F mask or spectral envelope corresponding to target speech, which may be viewed as monaural. Since traditional beamforming with an array also produces a "monaural" output, corresponding to the target source, T-F masking based on spatial features may be considered beamforming or, more accurately, nonlinear beamforming <ref type="bibr" target="#b125">[125]</ref> as opposed to traditional beamforming that is linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.</head><p>Time-frequency Masking for Beamforming Beamforming, as the name would suggest, tunes in the signals from a zone of arrival angles centered at a given angle, while tuning out the signals outside the zone. To be applicable, a beamformer needs to know the target direction to steer the beamformer. Such a steering vector is typically supplied by estimating the direction-of-arrival (DOA) of the target source, or more broadly sound localization. In Figure <ref type="figure">16</ref>. Diagram of a DNN based array source separation method (from <ref type="bibr" target="#b133">[133]</ref>). reverberant, multi-source environments, localizing the target sound is far from trivial. It is well recognized in CASA that localization and separation are two closely related functions ( <ref type="bibr" target="#b172">[172]</ref>, Chapter 5). For human audition, evidence suggests that sound localization largely depends on source separation <ref type="bibr">[60] [30]</ref>.</p><p>Fueled by the CHiME-3 challenge for robust ASR, two independent studies made the first use of DNN based monaural speech enhancement in conjunction with conventional beamforming, both published in ICASSP 2016 <ref type="bibr" target="#b70">[71]</ref>  <ref type="bibr" target="#b71">[72]</ref>. The CHiME-3 challenge provides noisy speech data from a single speaker recorded by 6 microphones mounted on a tablet <ref type="bibr" target="#b6">[7]</ref>. In these two studies, monaural speech separation provides the basis for computing the steering vector, cleverly bypassing two tasks that would have been required via the DOA estimation: localizing multiple sound sources and selecting the target (speech) source. To explain their idea, let us first describe MVDR as a representative beamformer.</p><p>MVDR aims to minimize the noise energy from nontarget directions while imposing linear constraints to maintain the energy from the target direction <ref type="bibr" target="#b44">[45]</ref>. The captured signals of an array in the STFT domain can be written as:</p><p>𝐲(𝑡, 𝑓) = 𝐜(𝑓)𝑠(𝑡, 𝑓) + 𝐧(𝑡, 𝑓) (14) where 𝐲(𝑡, 𝑓) and 𝐧(𝑡, 𝑓) denote the STFT spatial vectors of the noisy speech signal and noise at frame 𝑡 and frequency 𝑓, respectively, and 𝑠(𝑡, 𝑓) denotes the STFT of the speech source. The term 𝐜(𝑓)𝑠(𝑡, 𝑓) denotes the received speech signal by the array and 𝐜(𝑓) is the steering vector of the array.</p><p>At frequency f, the MVDR beamformer identifies a weight vector 𝐰(𝑓) that minimizes the average output power of the beamformer while maintaining the energy along the look (target) direction. Omitting 𝑓 for brevity, this optimization problem can be formulated as 𝐰 𝑜𝑝𝑡 = argmin 𝐰 {𝐰 𝐻 𝚽 𝑛 𝐰} , subject to 𝐰 𝐻 𝐜 = 1 <ref type="bibr" target="#b14">(15)</ref> where 𝐻 denotes the conjugate transpose and 𝚽 𝑛 is the spatial covariance matrix of the noise. Note that the minimization of the output power is equivalent to the minimization of the noise power. The solution to this quadratic optimization problem is:</p><formula xml:id="formula_12">𝐰 opt = 𝚽 𝑛 −1 𝐜 𝐜 𝐻 𝚽 𝑛 −1 𝐜<label>(16)</label></formula><p>The enhanced speech signal is given by 𝑠(𝑡) = 𝐰 opt 𝐻 𝐲(𝑡) (17) Hence, the accurate estimation of 𝐜 and 𝚽 𝑛 is key to MVDR beamforming. Furthermore, 𝐜 corresponds to the principal component of 𝚽 𝑥 (the eigenvector with the largest eigenvalue), the spatial covariance matrix of speech. With speech and noise uncorrelated, we have 𝚽 𝑥 = 𝚽 𝑦 − 𝚽 𝑛 (18) Therefore, a noise estimate is crucial for beamforming performance, just like it is for traditional speech enhancement.</p><p>In <ref type="bibr" target="#b70">[71]</ref>, an RNN with bidirectional LSTM is used for IBM estimation. A common neural network is trained monaurally on the data from each of the sensors. Then the trained network is used to produce a binary mask for each microphone recording, and the multiple masks are combined into one mask with a median operation. The single mask is used to estimate the speech and noise covariance matrix, from which beamformer coefficients are obtained. Their results show that MVDR does not work as well as the GEV (generalized eigenvector) beamformer. In <ref type="bibr" target="#b71">[72]</ref>, a spatial clustering based approach was proposed to compute a ratio mask. This approach uses a complex-domain GMM (cGMM) to describe the distribution of the T-F units dominated by noise and another cGMM to describe that of the units with both speech and noise. After parameter estimation, the two cGMMs are used for calculating the covariance matrices of noisy speech and noise, which are fed to an MVDR beamformer for speech separation. Both of these algorithms perform very well, and Higuchi et al.'s method was used in the best performing system in the CHiME-3 challenge <ref type="bibr" target="#b200">[200]</ref>. A similar approach, i.e. DNN-based IRM estimation combined with a beamformer, is also behind the winning system in the most recent CHiME-4 challenge <ref type="bibr" target="#b35">[36]</ref>.</p><p>A method different from the above two studies was given by Nugraha et al. <ref type="bibr" target="#b133">[133]</ref>, who perform array source separation using DNN for monaural separation and a complex multivariate Gaussian distribution to model spatial information. The DNN in this study is used to model source spectra, or spectral mapping. The power spectral densities (PSDs) and spatial covariance matrices of speech and noise are estimated and updated iteratively. Figure <ref type="figure">16</ref>  Wiener filter. Finally, the estimated speech signals from multiple microphones are averaged to produce a single speech estimate for ASR evaluation. A number of design choices were examined in this study, and their algorithm yields better separation and ASR results than DNN based monaural separation and an array version of NMF-based separation.</p><p>The success of Higuchi et al. <ref type="bibr" target="#b71">[72]</ref> and Heymann et al. <ref type="bibr" target="#b70">[71]</ref> in the CHiME-3 challenge by using DNN estimated masks for beamforming has motivated many recent studies, exploring different ways of integrating T-F masking and beamforming. Erdogan et al. <ref type="bibr" target="#b41">[42]</ref> trained an RNN for monaural speech enhancement, from which a ratio mask is computed in order to provide coefficients for an MVDR beamformer. As illustrated in Fig. <ref type="figure">17</ref>, a ratio mask is first estimated for each microphone. Then multiple masks from an array are combined into one mask by a maximum operator, which is found to produce better results than using multiple masks without combination. It should be noted that their ASR results on the CHiME-3 data are not compelling. Instead of fixed beamformers like MVDR, beamforming coefficients can be dynamically predicted by a DNN. Li et al. <ref type="bibr" target="#b108">[108]</ref> employed a deep network to predict spatial filters from array inputs of noisy speech for adaptive beamforming. Waveform signals are sent to a shared RNN, whose output is sent to two separate RNNs to predict beamforming filters for two microphones.</p><p>Zhang et al. <ref type="bibr" target="#b209">[209]</ref> trained a DNN for IRM estimation from a complementary set of monaural features, and then combined multiple ratio masks from an array into a single one with a maximum operator. The ratio mask is used for calculating the noise spatial covariance matrix at time t for an MVDR beamformer as follows, </p><p>where 𝑅𝑀(𝑙, 𝑓) denotes the estimated IRM from the DNN at frame l and frequency f. An element of the noise covariance matrix is calculated per frame by integrating a window of neighboring 2L+1 frames. They find this adaptive way of estimating the noise covariance matrix to perform much better than estimation over the entire utterance or a signal segment. An enhanced speech signal from the beamformer is then fed to the DNN to refine the IRM estimate, and mask estimation and beamforming iterate several times to produce the final output. Their 5.05 WER (word error rate) on the CHiME-3 real evaluation data represents a 13.34% relative improvement over the previous best <ref type="bibr" target="#b200">[200]</ref>. Independently, Xiao et al. <ref type="bibr" target="#b193">[193]</ref> also proposed to iterate ratio masking and beamforming. They use an RNN for estimating a speech mask and a noise mask. Mask refinement is based on an ASR loss, in order to directly benefit ASR performance. They showed that this approach leads to a considerable WER reduction over the use of a conventional MVDR, although recognition accuracy is not as high as in <ref type="bibr" target="#b200">[200]</ref>.</p><p>Other related studies include Pfeifenberger et al. <ref type="bibr" target="#b139">[139]</ref>, who use the cosine distance between the principal components of consecutive frames of noisy speech as the feature for DNN mask estimation. Meng et al. <ref type="bibr" target="#b121">[121]</ref> use RNNs for adaptive estimation of beamformer coefficients. Their ASR results on the CHiME-3 data are better than the baseline scores, but are far from the best scores. Nakatani et al. <ref type="bibr" target="#b129">[129]</ref> integrate DNN mask estimation and cGMM clustering based estimation to further improve the quality of mask estimates. Their results on the CHiME-3 data improve over those obtained from RNN or cGMM generated masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.DISCUSSION AND CONCLUSION</head><p>This paper has provided a comprehensive overview of DNN based supervised speech separation. We have summarized key components of supervised separation, i.e. learning machines, training targets, and acoustic features, explained representative algorithms, and reviewed a large number of related studies. With the formulation of the separation problem as supervised learning, DNN based separation over a short few years has greatly elevated the state-of-the-art for a wide range of speech separation tasks, including monaural speech enhancement, speech dereverberation, and speaker separation, as well as array speech separation. This rapid advance will likely continue with a tighter integration of domain knowledge and the data-driven framework and the progress in deep learning itself.</p><p>Below we discuss several conceptual issues pertinent to this overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.</head><p>Features vs. Learning Machines As discussed in Sect. IV, features are important for speech separation. However, a main appeal of deep learning is to learn appropriate features for a task, rather than to design such features. So is there a role for feature extraction in the era of deep learning? We believe the answer is yes. The so-called nofree-lunch theorem <ref type="bibr" target="#b189">[189]</ref> dictates that no learning algorithm, DNN included, achieves superior performance in all tasks. Aside from theoretical arguments, feature extraction is a way of imparting knowledge from a problem domain and it stands to reason that it is useful to incorporate domain knowledge this way (see <ref type="bibr" target="#b176">[176]</ref> for a recent example). For instance, the success of CNN in visual pattern recognition is partly due to the use of shared weights and pooling (sampling) layers in its architecture that helps to build a representation invariant to small variations of feature positions <ref type="bibr" target="#b9">[10]</ref>.</p><p>It is possible to learn useful features for a problem domain, but doing so may not be computationally efficient, particularly when certain features are known to be discriminative through domain research. Take pitch, for example. Much research in auditory scene analysis shows that pitch is a primary cue for auditory organization <ref type="bibr" target="#b14">[15]</ref>  <ref type="bibr" target="#b29">[30]</ref>, and research in CASA demonstrates that pitch alone can go a long way in separating voiced speech <ref type="bibr" target="#b77">[78]</ref>. Perhaps a DNN can be trained to "discover" harmonicity as a prominent feature, and there is some hint at this from a recent study <ref type="bibr" target="#b23">[24]</ref>, but extracting pitch as input features seems like the most straightforward way of incorporating pitch in speech separation.</p><p>The above discussion is not meant to discount the importance of learning machines, as this overview has made it abundantly clear, but to argue for the relevance of feature extraction despite the power of deep learning. As mentioned in Sect. V.A, convolutional layers in a CNN amount to feature extraction. Although CNN weights are trained, the use of a particular CNN architecture reflects design choices of its user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Time-frequency Domain vs. Time Domain</head><p>The vast majority of supervised speech separation studies are conducted in the T-F domain as reflected in the various training targets reviewed in Sect. III. Alternatively, speech separation can be conducted in the time domain without recourse to a frequency representation. As pointed out in Sect. V.A, through temporal mapping both magnitude and phase can potentially be cleaned at once. End-to-end separation represents an emergent trend along with the use of CNNs and GANs.</p><p>A few comments are in order. First, temporal mapping is a welcome addition to the list of supervised separation approaches and provides a unique perspective to phase enhancement <ref type="bibr" target="#b49">[50]</ref>  <ref type="bibr" target="#b103">[103]</ref>. Second, the same signal can be transformed back and forth between its time domain representation and its T-F domain representation. Third, the human auditory system has a frequency dimension at the beginning of the auditory pathway, i.e. at the cochlea. It is interesting to note Licklider's classic duplex theory of pitch perception, postulating two processes of pitch analysis: a spatial process corresponding to the frequency dimension in the cochlea and a temporal process corresponding to the temporal response of each frequency channel <ref type="bibr" target="#b111">[111]</ref>. Computational models for pitch estimation fall into three categories: spectral, temporal, and spectrotemporal approaches <ref type="bibr" target="#b32">[33]</ref>. In this sense, a cochleagram, with the individual responses of a cochlear filterbank <ref type="bibr" target="#b118">[118]</ref>  <ref type="bibr" target="#b172">[172]</ref>, is a duplex representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.</head><p>What's the Target? When multiple sounds are present in the acoustic environment, which should be treated as the target sound at a particular time? The definition of ideal masks presumes that the target source is known, which is often the case in speech separation applications. For speech enhancement, the speech signal is considered the target while nonspeech signals are considered the interference. The situation becomes tricky for multi-speaker separation. In general, this is the issue of auditory attention and intention. It is a complicated issue as what is attended to shifts from one moment to the next even with the same input scene, and does not have to be a speech signal. There are, however, practical solutions. For example, directional hearing aids get around this issue by assuming that the target lies in the look direction, i.e. benefiting from vision <ref type="bibr" target="#b170">[170]</ref>  <ref type="bibr" target="#b34">[35]</ref>. With sources separated, there are other reasonable alternatives for target definition, e.g. the loudest source, the previously attended one (i.e. tracking), or the most familiar (as in the multi-speaker case). A full account, however, would require a sophistical model of auditory attention (see <ref type="bibr">[172] [118]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. What Does a Solution to the Cocktail Party Problem Look Like?</head><p>CASA defines the solution to the cocktail party problem as a system that achieves human separation performance in all listening conditions <ref type="bibr">([172]</ref>, p.28). But how to actually compare the separation performance by a machine and that by a human listener? Perhaps a straightforward way would be compare ASR scores and human speech intelligibility scores in various listening conditions. This is a tall order as ASR performance still falls short in realistic conditions despite tremendous recent advances thanks to deep learning. A drawback with ASR evaluation is the dependency on ASR with all its peculiarities.</p><p>Here we suggest a different, concrete measure: a solution to the cocktail party is a separation system that elevates speech intelligibility of hearing-impaired listeners to the level of normal-hearing listeners in all listening situations. Not as broad as defined in CASA, but this definition has the benefit that it is tightly linked to a primary driver for speech separation research, namely, to eliminate the speech understanding handicap of millions of listeners with impaired hearing <ref type="bibr" target="#b171">[171]</ref>. By this definition, the DNN based speech enhancement described above has met the criterion in limited conditions (see Fig. <ref type="figure" target="#fig_1">13</ref> for one example), but clearly not in all conditions. Versatility is the hallmark of human intelligence, and the primary challenge facing supervised speech separation research today.</p><p>Before closing, we point out that the use of supervised learning and DNN in signal processing goes beyond speech separation, and automatic speech and speaker recognition. The related topics include multipitch tracking <ref type="bibr" target="#b79">[80]</ref> [56], voice activity detection <ref type="bibr" target="#b207">[207]</ref>, and even a task as basic in signal processing as SNR estimation <ref type="bibr" target="#b134">[134]</ref>. No matter the task, once it is formulated as a data-driven problem, advances will likely ensue with the use of various deep learning models and suitably constructed training sets; it should also be mentioned that these advances come at the expense of high computational complexity involved in the training process and often in operating a trained DNN model. A considerable benefit of treating signal processing as learning is that signal processing can ride on the progress of machine learning, a rapidly advancing field.</p><p>Finally, we remark that human ability to solve the cocktail party problem appears to have much to do with our extensive exposure to various noisy environments (see also <ref type="bibr" target="#b23">[24]</ref>). Research indicates that children have poorer ability to recognize speech in noise than adults <ref type="bibr" target="#b53">[54]</ref> [92], and musicians are better at perceiving noisy speech than non-musicians <ref type="bibr" target="#b135">[135]</ref> presumably due to musicians' long exposure to polyphonic signals. Relative to monolingual speakers, bilinguals have a deficit when it comes to speech perception in noise, although the two groups are similarly proficient in quiet <ref type="bibr" target="#b159">[159]</ref>. All these effects support the notion that extensive training (experience) is part of the reason for the remarkable robustness of the normal auditory system to acoustic interference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of various training targets for a TIMIT utterance mixed with a factory noise at -5 dB SNR.</figDesc><graphic url="image-5.png" coords="5,111.60,192.60,129.60,97.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison of training targets. (a) In terms of STOI. (b) In terms of PESQ. Clean speech is mixed with a factory noise at -5 dB, 0 dB and 5 dB SNR. Results for different training targets as well as a speech enhancement (SPEH) algorithm and an NMF method are highlighted for 0 dB mixtures. Note that the results and the data in this figure can be obtained from a Matlab toolbox at http://web.cse.ohio-state.edu/pnl/DNN_toolbox/.</figDesc><graphic url="image-9.png" coords="6,290.95,524.90,260.15,155.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Illustration of DNN for feature learning, and learned features are then used by linear SVM for IBM estimation (from [181]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Schematic diagram of a two-stage DNN for speech separation (from [65]).</figDesc><graphic url="image-13.png" coords="10,222.92,177.20,173.63,175.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a), the performance of the DNN degrades as more training speakers are added to the training set, whereas the LSTM benefits from additional training speakers. For untrained test speakers, as shown in Fig. 9(b), the LSTM substantially outperforms the DNN in terms of STOI. LSTM appears able to track a target speaker over time after being exposed to many speakers during training. With large-scale training with many speakers and numerous noises, RNNs with LSTM represent an effective approach for speaker-and noise-independent speech enhancement. (a) Results for trained speakers at -5 dB SNR. (b) Results for untrained speakers at -5 dB SNR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. STOI improvements of a feedforward DNN and a RNN with LSTM (from [20]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Diagram of a DNN for speech dereverberation based on spectral mapping (from [57]).</figDesc><graphic url="image-18.png" coords="13,335.00,445.70,230.39,250.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Diagram of a reverberation time aware DNN for speech dereverberation (redrawn from<ref type="bibr" target="#b190">[190]</ref>).</figDesc><graphic url="image-19.png" coords="14,140.15,65.40,340.46,173.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Diagram of DNN based two-speaker separation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Schematic diagram of a binaural separation algorithm (from<ref type="bibr" target="#b208">[208]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure17. MVDR beamformer with monaural mask estimation (from<ref type="bibr" target="#b41">[42]</ref>).</figDesc><graphic url="image-24.png" coords="19,124.00,578.25,362.26,123.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>𝑅𝑀(𝑙, 𝑓))𝐲(𝑙, 𝑓)𝐲(𝑙, 𝑓) 𝐻 𝑡+𝐿 𝑙=𝑡−𝐿</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-17.png" coords="11,207.88,54.95,194.04,300.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-21.png" coords="16,143.68,522.65,340.77,198.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-23.png" coords="18,119.18,564.85,371.87,168.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>an output model neuron and 𝑝 𝑖,𝑐 denotes the predicted probability of 𝑖 belonging to class c. N and C indicate the number of output neurons and the number of classes, respectively. 𝐼 𝑖,𝑐 is a binary indicator, which takes 1 if the desired class of neuron 𝑖 is 𝑐 and 0 otherwise.</figDesc><table><row><cell></cell><cell>For function</cell></row><row><cell cols="2">approximation or regression, a common cost function is mean</cell></row><row><cell>square error (MSE):</cell><cell></cell></row><row><cell>1 𝑁</cell><cell>𝑁 ∑(𝑦 𝑖 − 𝑦 𝑖 ̂)2 𝑖=1</cell></row></table><note>𝑖,𝑐 𝑙𝑜𝑔(𝑝 𝑖,𝑐 ) 𝐶 𝑐=1 𝑁 𝑖=1where 𝑖 indexes</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Speech enhancement results at -2 dB SNR measured in STOI (from<ref type="bibr" target="#b23">[24]</ref>).</figDesc><table><row><cell></cell><cell>Babble1</cell><cell>Cafeteria</cell><cell>Factory</cell><cell>Babble2</cell><cell>Average</cell></row><row><cell>Unprocessed</cell><cell>0.612</cell><cell>0.596</cell><cell>0.611</cell><cell>0.611</cell><cell>0.608</cell></row><row><cell>100-noise model</cell><cell>0.683</cell><cell>0.704</cell><cell>0.750</cell><cell>0.688</cell><cell>0.706</cell></row><row><cell>10K-noise model</cell><cell>0.792</cell><cell>0.783</cell><cell>0.807</cell><cell>0.786</cell><cell>0.792</cell></row><row><cell>Noise-dependent model</cell><cell>0.833</cell><cell>0.770</cell><cell>0.802</cell><cell>0.762</cell><cell>0.792</cell></row></table><note>Figure 8. Diagram of an LSTM based speech separation system (from [20]).</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">This was first pointed out by Hakan Erdogan in personal communication.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">The authors also published a paper in Interspeech 2012<ref type="bibr" target="#b115">[115]</ref> where a DAE is trained in an unsupervised fashion to map from the melspectrum of clean speech to itself. The trained DAE is then used to "recall" a clean signal from a noisy input for robust ASR.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>The preparation of this overview was supported in part by an AFOSR grant (FA9550-12-1-0130), an NIDCD grant (R01 DC012048), and an NSF grant (IIS-1409431). We thank Masood Delfarah for help in manuscript preparation and Jun Du, Yu Tsao, Yuxuan Wang, Yong Xu, and Xueliang Zhang for helpful comments on an earlier version.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Determination of the potential benefit of timefrequency gain manipulation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Anzalone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Calandruccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Carney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="480" to="492" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring multi-channel features for denoising-autoencoder-based speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="116" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Underdetermined blind sparse source separation for arbitrarily arranged multiple sensors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mukai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1833" to="1847" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The perception of speech under adverse conditions,&quot; in Speech processing in the auditory system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Assmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Summerfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Popper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Fay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="231" to="308" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Study on the dereverberation of speech based on temporal envelope filtering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Avendano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
				<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="889" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning spectral clustering, with application to speech separation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1963" to="2001" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The third CHiME speech separation and recognition challenge: dataset, task and baselines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ASRU</title>
				<meeting>IEEE ASRU</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5210" to="5214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An information-maximization approach to blind separation and blind deconvolution</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1129" to="1159" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Microphone array signal processing</title>
				<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling learning algorithms towards AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Cambridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large-scale kernel machines</title>
				<imprint>
			<publisher>MIT</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="321" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Schema-based processing in auditory scene analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcadams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="844" to="854" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spatial Hearing: The psychophysics of human sound localization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blauert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Suppression of acoustic noise in speech using spectral subtraction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Boll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Sig. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brandstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Ward</surname></persName>
		</author>
		<title level="m">Microphone arrays: Signal processing techniques and applications</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Auditory scene analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Isolating the energetic component of speech-on-speech masking with ideal time-frequency segregation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Brungart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="4007" to="4018" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effects of attention and unilateral neglect on auditory stream segregation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Carlyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cusack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Foxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="115" to="127" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A phonemebased pre-training approach for deep neural network with application to speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Chazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWAENC</title>
				<meeting>IWAENC</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MVA processing of speech features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="257" to="270" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory for speaker generalization in supervised speech separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3314" to="3318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DNN-based mask estimation for supervised speech separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio source separation</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="207" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A feature study for classification-based speech separation at low signal-to-noise ratios</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1993" to="2002" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Noise perturbation for supervised speech separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Comm</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale training to increase speech intelligibility for hearing-imparied listeners in novel noises</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Yoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2604" to="2612" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="975" to="979" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On human communication</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Cherry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auditory grouping</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Darwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="327" to="333" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Listening to speech in the presence of other sounds</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Darwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. Roy. Soc. B</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="1011" to="1021" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effectiveness of spatial cues, prosody, and talker characteristics in selective attention</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Darwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Hukin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="970" to="977" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequential stream segregation of voiced and unvoiced speech sounds based on fundamental frequency</title>
		<author>
			<persName><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lavandier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grimault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oxenham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hearing Research</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="page" from="235" to="243" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiple F0 estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Cheveigne ; Applications</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational auditory scene analysis: Principles, algorithms, and</title>
				<meeting><address><addrLine>Hoboken NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley &amp; IEEE Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="45" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Features for masking-based monaural speech separation in reverberant conditions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Delfarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1085" to="1094" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hearing aids</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Turramurra, Australia: Boomerang</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The USTC-iFlyteck system for the CHiME4 challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHiME-4 Workshop</title>
				<meeting>the CHiME-4 Workshop</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A regression approach to single-channel speech separation via highresolution deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Speech separation of a target speaker based on deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSP</title>
				<meeting>ICSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical deep neural network for multivariate regresss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="149" to="157" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimum mean-square error short-time spectral amplitude estimator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Sig. Process</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved MVDR beamforming using single-channel mask prediction networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1981" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A regression approach to binaural speech segregation via deep neural network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCSLP</title>
				<meeting>ISCSLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="116" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Effects of fluctuating noise and interfering speech on the speech-reception threshold for impaired and normal hearing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Festen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plomp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="1725" to="1736" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An algorithm for linearly constrained adaptive array processing</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Frost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="926" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SNR-aware convolutional neural network modeling for speech enhancement</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3678" to="3772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Raw waveformbased speech enhancement by fully convolutional networks</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02205v3</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SNR-based progressive learning of deep neural network for speech enhancement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3713" to="3717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The relationship between concurrent stream segregation, pitchbased streaming of vowel sequences, and frequency selectivity</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gaudrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grimault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Béra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Acoustica United with Acustica</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="317" to="327" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Phase processing for single-channel speech enhancement: History and recent advances</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krawczyk-Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mask-based enhancement for very low quality speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7029" to="7033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spondee recognition in a two-talker and a speech-shaped noise masker in adults and children</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Grose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Dev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A classification based approach to speech separation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="3475" to="3483" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural network based pitch tracking in very noisy speech</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2158" to="2168" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning spectral mapping for speech dereverebaration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4661" to="4665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning spectral mapping for speech dereverberation and denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="982" to="992" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Perceptual improvement of deep neural networks for monaural speech enhancement</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWAENC</title>
				<meeting>IWAENC</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">How we localize sounds</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<imprint>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="1999-11">November 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Blind binary masking for reverberation suppression in cochlear implants</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hazrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="1607" to="1614" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An algorithm to increase intelligibility for hearingimpaired listeners in the presence of a competing talker</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delfarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Vasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="4230" to="4239" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An algorithm to increase speech intelligibility for hearingimpaired listeners in novel segments of the same noise type</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Yoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1660" to="1669" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An algorithm to improve speech recognition in noise for hearingimpaired listeners</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Yoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="3029" to="3038" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">MMSE based noise PSD tracking with low complexity</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4266" to="4269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">RASTA processing of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Proc</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Introduction to the theory of neural computation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Redwood City, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Neural network based spectral mask estimation for acoustic beamforming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Robust MVDR beamforming using time-frequency masks for online/offline ASR in noise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5210" to="5214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Mag</title>
		<imprint>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012-11">November 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Speech segregation based on pitch tracking and amplitude modulation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE WASPAA</title>
				<meeting>IEEE WASPAA</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="79" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Monaural speech segregation based on pitch tracking and amplitude modulation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Net</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1135" to="1150" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A tandem algorithm for pitch estimation and voiced speech segregation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2067" to="2079" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An unsupervised approach to cochannel speech separation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="120" to="129" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Pitch estimation in noisy speech using accumulated peak spectrum and sparse estimation technique</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Proc</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1581" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Joint optimization of masks and deep recurrent neural networks for monaural source separation</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Convolutional maxout neural networks for speech separation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISSPIT</title>
				<meeting>ISSPIT</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="24" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">On the ideal ratio mask as the goal of computational auditory scene analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hummersone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Blind Source Separation</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Naik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="349" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Independent component analysis: Algorithms and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Phase autocorrelation (PAC) derived robust speech features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ikbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (PESQ) , and objective method for end-to-end speech quality assessment of narrowband telephone networks and speech codecs</title>
		<idno>ITU-T Recommendation P. 862</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>ITU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Theory and applications of spherical microphone array processing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Naylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<pubPlace>Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">An algorithm for predicting the intelligibility of speech masked by modulated noise maskers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2009" to="2022" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Binaural classification for reverberant speech segregation using deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2112" to="2121" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A supervised learning approach to monaural segregation of reverberant speech</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="625" to="638" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Effect of masker type and age on speech intelligibility and spatial release from masking in children and adults</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Litovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="2177" to="2189" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Determining the energetic and informational components of speech-on-speech masking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kidd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Nonlinear enhancement of onset for robust speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2058" to="2061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Power-normalized cepstral coefficients (PNCC) for robust speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4101" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Auditory processing of speech signals for robust speech recognition in real-world noisy environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Proc</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">An algorithm that improves speech intelligibility in noise for normal-hearing listeners</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Adaptive denoising autoencoders: a fine-tuning scheme to learn from test mixtures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LVA/ICA</title>
				<meeting>LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Role of mask pattern in intelligibility of ideal binarymasked noisy speech</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kjems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Boldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="1415" to="1426" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Speech intelligibility potential of general and specialized deep neural network based speech enhancement systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="153" to="167" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Multi-talker speech separation with utternance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Outcome measures based on classification performance fail to predict the intelligibility of binary-masked speech</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kressner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Rozell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="3033" to="3036" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Phase estimation in single channel speech enhancement using phase decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kulmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mowlaee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="598" to="602" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Delta-spectral cepstral coefficients for robust speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="4784" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Deep NMF for speech separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Fully complex deep neural network for phaseincorporating monaural source separation</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="281" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Neural network adaptive beamforming for robust multichannel speech recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1976" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Factors influencing intelligibility of ideal binary-masked speech: Implications for noise reduction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="1673" to="1682" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">The optimal ratio time-frequency mask for speech separation in terms of the signal-to-noise ratio</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="452" to="458" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A duplex theory of pitch perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C R</forename><surname>Licklider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experientia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="128" to="134" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">SOBM -a binary mask for noisy speech that optimises an objective intelligibility metric</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lightburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="661" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech enhancement: Theory and practice</title>
				<meeting><address><addrLine>Boca Raton FL</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Reasons why current speechenhancement algorithms do not improve speech intelligibility and suggested solutions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Speech restoration based on deep learning autoencoder with layerwised pretraining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1504" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Speech enhancement based on deep denoising autoencoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="555" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">A computational model of binaural localization and separation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Lyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="1148" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Human and machine hearing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Lyon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">An auditory based modulation spectral feature for reverberant speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Maganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matassoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="570" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Sound segregation via embedded repetition is robust to inattention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Masutomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barascud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kashino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimenal Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="386" to="400" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Deep long short-term memory adaptive beamforming networks for multichannel robust speech recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="271" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial networks for speech enhancement and noiserobust speaker verification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michelsanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2008">2008-2012, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">The masking of speech</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="105" to="129" />
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">The trill threshold</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Heise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="637" to="638" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">An analysis of binaural spectro-temporal masking as nonlinear beamforming</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Moghimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="835" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">An introduction to the psychology of hearing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Academic Press</publisher>
			<pubPlace>San Diego CA</pubPlace>
		</imprint>
	</monogr>
	<note>5th ed.</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Cochlear hearing loss</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley</publisher>
			<pubPlace>Chichester UK</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Integrating DNN-based and spatial clustering-based mask estimation for robust MVDR beamforming</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="286" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for robust speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7092" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Naylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Gaubitch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
	<note>Speech dereverberation</note>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Deep stacking networks with time series for speech separation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="6717" to="6721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Multichannel audio source separation with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1652" to="1664" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Long-term SNR estimation of speech signals in known and unknown channel conditions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsiartas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2495" to="2506" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Musician enhancement for speech-in-noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parbery-Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Skoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="653" to="661" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">A fully convolutional neural network for speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07132v1</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">SEGAN: Speech enhancement generative adversarial network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">DNN-based speech mask estimation for eigenvector beamforming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pfeifenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zohrer</surname></persName>
		</author>
		<author>
			<persName><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ) -a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Speech segregation based on sound localization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="2236" to="2252" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<title level="m">Principles of neural dynamics</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Spartan</publisher>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel distributed processing</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Artificial intelligence: A modern approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Spectrotemporal modulation subspace-spanning filter bank features for robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Schadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kollmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="4134" to="4151" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">A Bayesian classifier for spectrographic mask estimation for missing feature speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Comm</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="379" to="393" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Temporal coherence and attention in auditory scene analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micheyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="114" to="123" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Feature extraction from higher-order autocorrelation coefficients for robust speech recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Comm</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1458" to="1485" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Robust speaker identification using auditory features and computational auditory scene analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1589" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Object-based auditory and visual attention</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shinn-Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="182" to="186" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Binary and ratio time-frequency masks for robust speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Comm</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1486" to="1501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>arXiv1505.00387</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">F0 processing and the separation of competing speech signals by listeners with normal hearing and with hearing loss</title>
		<author>
			<persName><forename type="first">V</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Leek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1294" to="1306" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Multiple-target deep learning for LSTM-RNN based speech enhancement</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Hands-free Speech Communication and Microphone Arrays</title>
				<meeting>the Workshop on Hands-free Speech Communication and Microphone Arrays</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="136" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">From feedforward to recurrent LSTM neural networks for language modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="517" to="529" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Speech perception in noise by monolingual, bilingual and trilingual listeners</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Chacra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Language and Communication Disorders</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="411" to="422" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Noise reduction using connectionist models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="553" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Speech enhancement based on deep neural networks with skip connections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5565" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Speech separation based on improved deep neural networks with dual outputs of speech features for both target and interfering speakers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCSLP</title>
				<meeting>ISCSLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="250" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Temporal coherence in the perception of tone sequences</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P A S</forename><surname>Van Noorden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
		<respStmt>
			<orgName>Eindhoven University of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Beamforming: A versatile approach to spatial filtering</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="1988-04">April 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fevotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Active-set Newton algorithm for overcomplete non-negative representations of audio</title>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2277" to="2289" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Combining non-negative matrix factorization and deep neural networks for speech enhancement and automatic speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bigot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">On ideal binary mask as the computational goal of auditory scene analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech separation by humans and machines</title>
				<editor>
			<persName><forename type="first">P</forename><surname>Divenyi</surname></persName>
		</editor>
		<meeting><address><addrLine>Norwell MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="181" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">The time dimension for scene analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Net</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1401" to="1426" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Time-frequency masking for speech separation and its potential for hearing aid design</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trend. Amplif</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="332" to="353" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Deep learning reinvents the hearing aid</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="page" from="32" to="37" />
			<date type="published" when="2017-03">March 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Computational auditory scene analysis: Principles, algorithms, and applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>&amp; IEEE Press</publisher>
			<pubPlace>Hoboken NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Speech intelligibility in background noise with ideal binary time-frequency masking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kjems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Boldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="2336" to="2347" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">A gender mixture detection approach to unsupervised single-channel speech separation based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1535" to="1546" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">A maximum likelihood approach to deep neural network based nonlinear spectral mapping for single-channel speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1178" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Trainable frontend for robust and far-field keyword spotting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Getreuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5670" to="5674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Exploring monaural features for classification-based speech segregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="270" to="279" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Boosting classification based speech separation using temporal dynamics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1528" to="1531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Cocktail party processing via structured prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Towards scaling up classificationbased speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1381" to="1390" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">A deep neural network for timedomain signal reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4390" to="4394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Phoneme-specific speech separation</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="146" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Oracle performance investigation of the ideal masks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWAENC</title>
				<meeting>IWAENC</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LVA/ICA</title>
				<meeting>LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Discriminatively trained recurrent neural networks for single-channel speech separation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GlobalSIP</title>
				<meeting>GlobalSIP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Backpropagation through time: What it does and how to do it</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">The lack of a priori distinction between learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1341" to="1390" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">A reverberationtime-aware approach to speech dereverberation based on deep neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="102" to="111" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">A two-stage algorithm for onemicrophone reverberant speech enhancement</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="774" to="784" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Using optimal ratio mask as training target for supervised speech separation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of APSIPA</title>
				<meeting>APSIPA</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">On timefrequency mask estimation for MVDR beamforming with application in robust speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3246" to="3250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Speech dereverberation for enhancement and recognition using dynamic features constrained deep neural networks and feature adaptation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Adv. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Dynamic noise aware training for speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2670" to="2674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">An experimental study on speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Multiobjective learning and mask-based post-processing for deep neural network based speech enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1508" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Blind separation of speech mixtures via time-frequency masking</title>
		<author>
			<persName><forename type="first">O</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rickard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1830" to="1847" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">The NTT CHiME-3 system: advances in speech enhancement and recognition for mobile multi-microphone devices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ASRU</title>
				<meeting>IEEE ASRU</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">The cocktail party problem: Forty years later</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Yost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Binaural and spatial hearing in real and virtual environments</title>
				<editor>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Gilkey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="329" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Localization based stereo speech source separation using probabilistic time-frequency masking and deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Audio Speech Music Proc</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Robust features for noisy speech recognition based on temporal trajectory filtering of short-time autocorrelation sequences</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Yuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Comm</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Multi-target ensemble learning for monaural speech separation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1958" to="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">A deep ensemble learning method for monaural speech separation</title>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="967" to="977" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Deep belief networks based voice activity detection</title>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="697" to="710" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Deep learning based binaural speech separation in reverberant environments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1075" to="1084" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">A speech enhancement algorithm by iterating single-and multimicrophone processing and its application to robust ASR</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="276" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">A pairwise algorithm using the deep stacking network for speech separation and pitch estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1066" to="1078" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">A two-stage algorithm for noisy and reverberant speech enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
				<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5580" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
