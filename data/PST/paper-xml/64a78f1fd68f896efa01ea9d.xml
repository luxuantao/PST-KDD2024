<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain</title>
				<funder ref="#_sYR7bvr">
					<orgName type="full">Integrated Artificial Intelligence) EP-SRC</orgName>
				</funder>
				<funder>
					<orgName type="full">Cisco</orgName>
				</funder>
				<funder ref="#_6Qcfar6">
					<orgName type="full">Artificial Intelligence and Multimorbidity: Clustering in Individuals, Space and Clinical Context</orgName>
					<orgName type="abbreviated">CISC</orgName>
				</funder>
				<funder ref="#_PPrs5RP">
					<orgName type="full">United Kingdom Research and Innovation</orgName>
				</funder>
				<funder ref="#_pnW2mTW">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">Legal and General PLC</orgName>
				</funder>
				<funder>
					<orgName type="full">Accenture LLP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aryo</forename><surname>Pradipta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luke</forename><surname>Daines</surname></persName>
							<email>luke.daines@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Usher Institute</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
							<email>p.minervini@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Beatrice</forename><surname>Alex</surname></persName>
							<email>b.alex@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Edinburgh Futures Institute</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. We evaluate this framework on multiple clinical outcome prediction datasets, comparing it to clinically trained language models. Our proposed framework achieves a state-of-the-art AUROC score averaged across all clinical downstream tasks. We observe substantial improvements of 6-9% AUROC score in the largescale multilabel classification tasks, such as diagnoses and procedures classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large Language Models (LLMs) have consistently achieved state-of-the-art performance across various NLP tasks. However, while these models exhibit impressive generalisation abilities, they often struggle to perform in specialised domains such as clinical applications, primarily due to the absence of domain-specific knowledge. The complexity of medical terminology and the presence of incomplete sentences in clinical notes contribute to this challenge <ref type="bibr" target="#b10">(Lehman and Johnson, 2023)</ref>. Unfortunately, studies have indicated that even LLMs pretrained with datasets comprising biomedical publications still exhibit suboptimal performance when applied to downstream clinical applications, particularly when compared to LLMs pretrained with clinical notes <ref type="bibr" target="#b0">(Alsentzer et al., 2019;</ref><ref type="bibr" target="#b13">Li et al., 2022;</ref><ref type="bibr" target="#b31">Yang et al., 2022)</ref>. This observation suggests that there are intrinsic nuances specific to the clinical context that can only be effectively captured if LLMs undergo pretraining using clinical datasets.</p><p>The current approach of adapting pretrained LLMs to the clinical domain typically involves fine-tuning the entire model parameters <ref type="bibr" target="#b0">(Alsentzer et al., 2019;</ref><ref type="bibr" target="#b20">Peng et al., 2019;</ref><ref type="bibr" target="#b27">van Aken et al., 2021;</ref><ref type="bibr" target="#b19">Michalopoulos et al., 2021;</ref><ref type="bibr" target="#b10">Lehman and Johnson, 2023)</ref>. However, due to the rapid increase in the size of LLMs, such a practice demands extensive computational resources, which may not be readily accessible to all researchers. Consequently, this challenge will further exacerbate the disparity between the resource-rich and resource-constrained research institutions <ref type="bibr" target="#b22">(Ruder et al., 2022)</ref>.</p><p>To address the substantial computational demands, studies have proposed various Parameter-Efficient Fine-Tuning (PEFT) techniques. These techniques present a practical solution by finetuning a small subset of additional parameters while keeping the remaining pretrained parameters fixed. As a result, this strategy significantly alleviates the computational burden while achieving comparable performance to that of full fine-tuning.</p><p>In this study, we propose a two-step PEFT framework (see Figure <ref type="figure" target="#fig_0">1</ref>). Firstly, we introduce Clinical LLaMA-LoRA, a Low-Rank Adaptation <ref type="bibr">(LoRA, Hu et al., 2022)</ref> PEFT adapter built upon the opensource Large Language Model Meta AI (LLaMA) <ref type="bibr">(Touvron et al., 2023)</ref>. Then, we introduce Downstream LLaMA-LoRA, which is trained on top of the pretrained Clinical LLaMA-LoRA. Downstream LLaMA-LoRA is specifically designed for clinical downstream tasks. The fusion of the two adapters achieves state-of-the-art performance in clinical NLP downstream tasks while considerably reducing the computational requirements. This study presents the following contributions:</p><p>? We introduce Clinical LLaMA-LoRA, a PEFTadapted version of the LLaMA model tailored specifically for the clinical domain.</p><p>? We provide comparisons of multiple PEFT techniques in terms of language modelling performance based on perplexity score, shedding light on the optimal PEFT techniques for the clinical domain-adaptive pretraining.</p><p>? We introduce Downstream LLaMA-LoRA, built on top of Clinical LLaMA-LoRA and tailored specifically for the clinical downstream tasks.</p><p>? We evaluate the proposed mixture of Clinical LLaMA-LoRA and Downstream LLaMA-LoRA on downstream clinical datasets and tasks. Our proposed framework showcases improvements in AUROC scores over the existing clinical LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Biomedical Large Language Models</head><p>General-domain LLMs continue to face challenges when confronted with domain-specific tasks. The complexity associated with the requisite domain knowledge is recognised as a significant factor <ref type="bibr" target="#b14">(Ling et al., 2023)</ref>, particularly within the biomedical domain. Consequently, numerous studies have attempted to adapt LLMs specifically for the biomedical domain.</p><p>An early example of such adaptation is BioBERT <ref type="bibr" target="#b9">(Lee et al., 2019)</ref>, which was pretrained using biomedical research articles from PubMed and PubMed Central. This adaptation has shown improved performance across various biomedical NLP tasks. Recognising the significance of biomedical-specific vocabularies, <ref type="bibr" target="#b3">Gu et al. (2022)</ref> proposed PubMedBERT, which is pretrained on biomedical data from scratch and initialised the model vocabulary with the biomedical corpus. The growing interest in biomedical NLP research has led to the adaptation of even larger models to the biomedical domain <ref type="bibr" target="#b17">(Luo et al., 2022;</ref><ref type="bibr" target="#b24">Singhal et al., 2022;</ref><ref type="bibr" target="#b30">Wu et al., 2023;</ref><ref type="bibr" target="#b25">Singhal et al., 2023)</ref> While these biomedical LLMs have demonstrated advancements in various biomedical NLP benchmarking tasks, studies have revealed that clinical LLMs still outperform their biomedical counterparts in numerous clinical downstream tasks <ref type="bibr" target="#b0">(Alsentzer et al., 2019;</ref><ref type="bibr" target="#b31">Yang et al., 2022;</ref><ref type="bibr" target="#b13">Li et al., 2022;</ref><ref type="bibr" target="#b10">Lehman and Johnson, 2023)</ref>. This suggests that domain-adaptive pretraining using clinical data is still the de facto protocol in adapting LLMs to the clinical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clinical Large Language Models</head><p>Clinical LLMs are often fine-tuned with clinical data from an LLM that is already pretrained with datasets that encompass broader topics. For instance, Bio+ClinicalBERT <ref type="bibr" target="#b0">(Alsentzer et al., 2019)</ref> is domain-adaptively pretrained using clinical notes from the Medical Information Mart for Intensive Care (MIMIC)-III database <ref type="bibr" target="#b8">(Johnson et al., 2016)</ref>, starting from a pretrained BioBERT <ref type="bibr" target="#b9">(Lee et al., 2019)</ref>, which itself is pretrained on biomedical articles. BlueBERT <ref type="bibr" target="#b20">(Peng et al., 2019)</ref> is domainadaptively pretrained using PubMed abstracts and MIMIC-III clinical notes from a BERT model <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>, that is pretrained with generaldomain texts. Similarly, Clinical-T5 <ref type="bibr" target="#b10">(Lehman and Johnson, 2023</ref>) is domain-adaptively pretrained using the union of MIMIC-III and MIMIC-IV <ref type="bibr">(Johnson et al., 2023)</ref> clinical notes from T5-base <ref type="bibr" target="#b21">(Raffel et al., 2020)</ref>, another general-domain LLM.</p><p>All these studies share a common approach, which is to fine-tune the entire model parameters. With massive LLMs, this method has become costprohibitive and inaccessible for many researchers. Figure <ref type="figure">2</ref>: Frameworks of domain-adaptive and downstream fine-tuning to adapt a pretrained LLM from the general domain to the clinical domain. As opposed to a full fine-tuning process which can be prohibitively expensive (left), our approach leverages PEFT techniques to introduce a clinically-specialised adapter that is attached to a pretrained general LLM (right). Our proposed framework also introduces another clinical PEFT adapter trained on the downstream clinical tasks, such as clinical note classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parameter-Efficient Fine-Tuning for Large Language Models</head><p>Suppose that we have a pretrained LLM P ? (y|x); fine-tuning it can be effectively defined as finding the most appropriate parameter changes ?? by optimising the fine-tuning objective. A conventional, full fine-tuning process means that the model needs to learn a ?? whose dimension is equal to the entire parameters of the pretrained LLM |??| = |? 0 |, which is computationally expensive. PEFT techniques address this by tuning the delta ??, which corresponds to a very small fraction of additional trainable parameters during the fine-tuning process.</p><p>Adapter tuning <ref type="bibr" target="#b4">(Houlsby et al., 2019</ref>) is an early PEFT method that involves adding small additional parameters called adapters to each layer of the pretrained model and strictly fine-tuning this small set of new parameters. LoRA <ref type="bibr" target="#b5">(Hu et al., 2022)</ref> is another PEFT approach that trains low-rank matrices to represent the attention weights update of transformer-based models.</p><p>Another group of PEFT approaches leverages the concept of prompting. Prefix Tuning <ref type="bibr" target="#b12">(Li and Liang, 2021)</ref> optimises a sequence of continuous task-specific vectors, called a prefix, which are trainable parameters that do not correspond to real tokens. P-Tuning <ref type="bibr">(Liu et al., 2021b</ref>) uses a similar strategy as Prefix tuning with a focus on text understanding tasks, as opposed to generative tasks. Prompt tuning <ref type="bibr" target="#b11">(Lester et al., 2021)</ref> simplifies Prefix tuning by introducing trainable tokens, called soft prompts, for each downstream task. Liu et al.</p><p>(2021a) introduced P-tuning v2 which uses deep prompt tuning to address the lack of performance gain in the previous prompt tuning techniques.</p><p>By fine-tuning a small fraction of additional parameters, all PEFT approaches alleviate the issue of extensive computational resource requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Figure <ref type="figure">2</ref> shows the comparison between the current and proposed problem definitions. The general problem can be decomposed into two stages: Domain-adaptive Pretraining. Given a pretrained general LLM P ? (y|x) with its parameters ? and a training dataset Z = {(x i , y i )} i=1,...,N . To adapt to the new domain, the model needs to update its weight iteratively from its pretrained state ? 0 to ? = ? 0 + ??. This process of maximising the objective function can be defined as:</p><formula xml:id="formula_0">argmax ? (x,y)?Z |y| t=1 log (P ? (y t | x, y &lt;t ))</formula><p>In the current paradigm, a full fine-tuning process means that the model needs to learn a ?? whose dimension is equal to the entire pretrained parameters |??| = |? 0 |, which is computationally expensive.</p><p>In the proposed paradigm, we tune only small additional parameters ? such that ? = ? 0 + ??(?) whose dimension is very small compared to the original parameters |?| ? |? 0 |. Thus, the training objective can be redefined as:</p><formula xml:id="formula_1">argmax ? (x,y)?Z |y| t=1 log P ?+??(?) (y t | x, y &lt;t )</formula><p>In the current paradigm, the outcome of domainadaptive pretraining would be a clinically-adapted LLM. While in the proposed paradigm, the outcome would be the clinical PEFT component, which can be combined with the untouched pretrained general LLM for downstream applications.</p><p>Downstream Fine-tuning. In the current paradigm, the pretrained clinical LLM is finetuned to the downstream tasks, such as document classification tasks. Suppose that we have a pretrained clinical LLM P ?,? with its domainadapted parameters ? and a newly initialised classifier layer ?, as well as a training dataset Z = {(x i , y i )} i=1,...,N . We want to maximise a specific loss function, such as a cross-entropy loss:</p><formula xml:id="formula_2">argmax ?,? 1 N N i=1 y i log (P ?,? (x i ))</formula><p>In contrast, in the proposed paradigm, the finetuning process only updates the small additional parameters ??(?) and the classifier head ?:</p><formula xml:id="formula_3">argmax ?,? 1 N N i=1 y i log P ?+??(?),? (x i )</formula><p>In fact, we can also decompose the fine-tuning into an additional "delta-updating" process:</p><formula xml:id="formula_4">argmax ?,?,? 1 N N i=1 y i log P ?+??(?)+??(?),? (x i )</formula><p>Similar to the Domain-adaptive Pretraining stage, the dimensions of the additional parameters ? and ? are very small compared to the original parameters. By updating only the additional parameters and the classifier head, the proposed paradigm reduces the computational requirements, making it more efficient and feasible, especially for clinical settings that are often resource-constrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clinical LLaMA-LoRA</head><p>Clinical LLaMA-LoRA is a LoRA adapter built upon LLaMA <ref type="bibr">(Touvron et al., 2023)</ref>. Clinical LLaMA-LoRA is domain-adapted to the clinical domain and fine-tuned to the downstream tasks following the proposed procedure shown on the right-hand side of Figure <ref type="figure">2</ref>. Our approach follows the autoregressive language modelling pretraining objective employed in the original LLaMA training. To ensure compatibility with available computational resources, we use fixed model hyperparameters that allow us to fit the LLM into a single NVIDIA A100-80GB GPU (see Appendix A.1). We optimise the hyperparameters specific to each PEFT method using Gaussian Process regression for Bayesian Optimisation (Frazier, 2018)<ref type="foot" target="#foot_0">1</ref> with a maximum of 20 trials. The detailed hyperparameters search space can be found in Appendix A.2. During this stage, we evaluate the perplexity scores of the LLM variants. Downstream Fine-tuning We fine-tune the Clinical LLaMA-LoRA and Downstream LLaMA-LoRA to clinical document classification tasks:</p><p>? Prolonged mechanical ventilation (PMV): a binary classification task to predict whether a patient will require mechanical ventilation for more than seven days <ref type="bibr" target="#b6">(Huang et al., 2020)</ref>.</p><p>? In-hospital mortality (MOR): a binary classification task to predict whether a patient will survive during their hospital stay <ref type="bibr" target="#b27">(van Aken et al., 2021)</ref>.</p><p>? Length of stay (LOS): a multiclass classification task to predict the length of a patient's hospital stay, categorised into four time-bins: less than three days, three to seven days, one to two weeks, and more than two weeks (van Aken et al., 2021).</p><p>? Diagnoses (DIAG): a large-scale multilabel classification task to predict the differential diagnoses associated with a patient, represented by simplified ICD-9 diagnosis codes (van Aken et al., 2021).</p><p>? Procedures (PROC): a large-scale multilabel classification task to predict the diagnostics or treatments administered to a patient, represented by simplified ICD-9 procedure codes <ref type="bibr" target="#b27">(van Aken et al., 2021)</ref>.</p><p>The label and split statistics of each dataset can be found in Table <ref type="table">1</ref>.</p><p>During this downstream fine-tuning process, we use fixed model hyperparameters to ensure compatibility with the available computational resources, a single NVIDIA A100-80GB GPU (see Appendix B.1). We optimise the hyperparameters specific to each PEFT method using Gaussian Process regression for Bayesian Optimisation with a maximum of 20 trials. The detailed hyperparameters search space of the PEFT method can be found in Appendix B.2.</p><p>For evaluating the performance of the model on these downstream tasks, we report the Area Under the Receiver Operating Characteristic Curve (AU-ROC) scores. Additionally, we report the macroaveraged AUROC score across all clinical tasks as commonly done in NLP benchmarking tasks <ref type="bibr" target="#b28">(Wang et al., 2019;</ref><ref type="bibr" target="#b20">Peng et al., 2019;</ref><ref type="bibr" target="#b3">Gu et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline Models</head><p>The baseline models used in the evaluation are as follows:</p><p>? Bio+ClinicalBERT <ref type="bibr" target="#b0">(Alsentzer et al., 2019)</ref>:</p><p>Bio+ClinicalBERT is pretrained on clinical notes from the MIMIC-III database. It is initialised from a biomedical language model called BioBERT <ref type="bibr" target="#b9">(Lee et al., 2019)</ref>, which is pretrained on biomedical research articles.</p><p>? BlueBERT <ref type="bibr" target="#b20">(Peng et al., 2019)</ref>: BlueBERT is pretrained on clinical notes from the MIMIC-III database and PubMed abstracts starting from the pretrained checkpoint of BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>, a general-domain language model.</p><p>? CORe (van Aken et al., 2021): CORe is pretrained on clinical notes from the MIMIC-III database and biomedical articles starting from the pretrained checkpoint of BioBERT <ref type="bibr" target="#b9">(Lee et al., 2019)</ref>.</p><p>? UmlsBERT <ref type="bibr" target="#b19">(Michalopoulos et al., 2021)</ref>: Umls-BERT is pretrained on clinical notes from the MIMIC-III database starting from the pretrained checkpoint of Bio+ClinicalBERT while modifying the architecture and pretraining objective by incorporating knowledge from the Unified Medical Language System (UMLS) Metathesaurus <ref type="bibr" target="#b23">(Schuyler et al., 1993)</ref>.</p><p>These baseline models have been trained to perform specifically on clinical data, thus providing comparison points for evaluating the performance of the proposed Clinical LLaMA-LoRA in downstream clinical NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pretraining</head><p>The pretraining results can be found in Table <ref type="table" target="#tab_2">2</ref>. We employ PEFT techniques to perform domainadaptive pretraining. All PEFT techniques train a significantly smaller number of parameters, ranging from only 0.001% to 0.24% of the original model parameters, which substantially decreases the computational resources required and shortens the training time. Note that performing fullparameter training of LLaMA and PMC-LLaMA with just a single GPU is unfeasible. Instead, PEFT techniques require less than 24 hours per epoch on average with only a single NVIDIA A100-80GB GPU.</p><p>Among all the PEFT techniques, LoRA emerges as the best-performing one for both LLaMA and PMC-LLaMA in the clinical domain-adaptive pretraining, achieving the lowest perplexity scores of 2.244 and 2.404, respectively. This pretrained LoRA is referred to as Clinical LLaMA-LoRA  in the subsequent sections. The following experiments in downstream fine-tuning will utilise this pretrained Clinical LLaMA-LoRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Downstream results</head><p>From the downstream fine-tuning results shown in Table <ref type="table" target="#tab_3">3</ref>, we can decompose the analysis into multiple research questions:</p><p>Can LoRA help fine-tune LLaMA from other domains (general and biomedical) to achieve higher AUROC scores in clinical tasks? We compare the results obtained by LLaMA and LLaMA + LoRA, as well as PMC-LLaMA and PMC-LLaMA + LoRA, as presented in Table <ref type="table" target="#tab_3">3</ref>.</p><p>The obtained results consistently demonstrate improved AUROC scores when utilising LoRA across all tasks. The macro-averaged AUROC score of LoRA-equipped LLaMA shows a notable 13.01% increase when compared to the LLaMA-only baseline. Similarly, LoRA-equipped PMC-LLaMA exhibits a 12.2% improvement in macro-averaged AUROC compared to the original PMC-LLaMA Both LLaMA and PMC-LLaMA, when equipped with LoRA, exhibit significant AUROC score improvements in all tasks except the prolonged mechanical ventilation prediction task, which is proven challenging for all model variants.</p><p>Furthermore, the marginal difference in AUROC scores between PMC-LLaMA and the generaldomain LLaMA can be attributed to two factors. Firstly, the original LLaMA has been exposed to biomedical concepts during its pretraining, reducing the need for domain-adaptive pretraining to the biomedical domain. Secondly, clinical NLP tasks are challenging, even for biomedical LLMs.</p><p>Can LoRA-equipped LLaMA and PMC-LLaMA perform comparably in comparison to clinically trained LMs? We compare the AU-ROC scores obtained by the baseline models, and LoRA-equipped LLaMA and PMC-LLaMA (see Table <ref type="table" target="#tab_3">3</ref>). Among the baseline models, BlueBERT performs the best with a macro-averaged AUROC score of 69.59%. Compared to BlueBERT, both LLaMA and PMC-LLaMA underperform with macro-averaged AUROC scores of 58.61% and 60.51%, respectively. This finding highlights the importance of clinical-specific fine-tuning. Significant improvements can be observed in LoRA-equipped LLaMA and PMC-LLaMA, with macro-averaged AUROC scores of 71.62% and 72.71%, respectively. We notice considerable improvements in the diagnoses and procedures prediction tasks. For example, LoRA-equipped LLaMA achieves AUROC scores of 78.37% and 87.49% in the diagnoses and procedures prediction tasks, respectively, compared to 73.81% and 77.70% for BlueBERT. This represents improvements of 4.56% in diagnoses prediction and 9.79% in procedures prediction. Improvements are also observed in the results obtained by LoRA-equipped PMC-LLaMA, outperforming BlueBERT by 5% in diagnoses prediction and 9.02% in procedures prediction.</p><p>Overall, LoRA-equipped LLaMA and PMC-LLaMA achieve higher AUROC scores than the baseline clinical LMs in various clinical prediction tasks, particularly in diagnoses, procedures, and mortality predictions, while maintaining competitive AUROC scores in length-of-stay prediction. However, LoRA-equipped LLaMA and PMC-LLaMA still underperform in prolonged mechanical ventilation prediction.  Can LLaMA and PMC-LLaMA with Clinical LLaMA-LoRA achieve higher AUROC scores than the clinically trained LMs? The domainadaptive pretraining step yields the clinicallytrained LoRA adapters for LLaMA and PMC-LLaMA, called Clinical LLaMA-LoRA. We compare the results of Clinical LLaMA-LoRAequipped LLaMA and PMC-LLaMA with the baseline models. We evaluate Clinical LLaMA-LoRA with and without downstream fine-tuning, referred to as "Trainable" and "Frozen" respectively.</p><p>The results indicate that Clinical LLaMA-LoRAequipped LLaMA and PMC-LLaMA outperform the baseline models. LLaMA with a trainable Clinical LLaMA-LoRA achieves an AUROC score of 70.85%, surpassing BlueBERT's score of 69.59%. PMC-LLaMA with a trainable Clinical LLaMA-LoRA achieves an even higher AUROC score of 72.23%. These findings demonstrate that the Clinical LLaMA-LoRA contributes to higher AUROC scores for LLaMA and PMC-LLaMA over clinically trained LLMs.</p><p>Can LLaMA and PMC-LLaMA with Clinical LLaMA-LoRA achieve higher AUROC scores than the other fine-tuning variants? We examine the importance of the domain-adapted LoRA by comparing the results obtained by LLaMA and PMC-LLaMA equipped with Clinical LLaMA-LoRA against the results of LLaMA and PMC-LLaMA fine-tuning, both original and with LoRA.</p><p>Firstly, we evaluate the frozen pretrained Clinical LLaMA-LoRA. Both LLaMA and PMC-LLaMA with frozen Clinical LLaMA-LoRA do not exhibit a significant increase in performance compared to the original fine-tuning. This indi-cates that, despite the domain-adaptive pretraining, the limited number of trainable parameters during the downstream fine-tuning restricts the potential improvement that the model can achieve.</p><p>This reasoning is further supported by the significant improvement observed in the AUROC scores of LLaMA and PMC-LLaMA with trainable Clinical LLaMA-LoRA. LLaMA and PMC-LLaMA with trainable Clinical LLaMA-LoRA achieve 70.85% and 72.23% macro-averaged AU-ROC scores, respectively, massive improvements from the vanilla fine-tuning performance (58.61% and 60.51% AUROC scores respectively).</p><p>However, Clinical LLaMA-LoRA does not yield significant improvements when compared to LLaMA and PMC-LLaMA, which are directly equipped with LoRA without pretraining. For instance, we can observe that LLaMA with LoRA achieves a slightly higher macro-averaged AUROC score of 71.62% compared to LLaMA with Clinical LLaMA-LoRA, which achieves 70.85%.</p><p>Can a downstream LoRA adapter improve the AUROC scores of LLaMA and PMC-LLaMA equipped with Clinical LLaMA-LoRA? By considering Clinical LLaMA-LoRA as the "deltaupdating" outcome of the domain-adaptive pretraining, we can view the downstream fine-tuning process as an additional "delta-updating" step. To investigate the impact of this approach, we conduct experiments by adding a Downstream LLaMA-LoRA to LLaMA and PMC-LLaMA models that were already equipped with Clinical LLaMA-LoRA. From Table <ref type="table" target="#tab_3">3</ref>, we can observe that Downstream LLaMA-LoRA fails to improve the performance of LLaMA and PMC-LLaMA with frozen Clinical LLaMA-LoRA. On the other hand, improvement can be observed when adding Downstream LLaMA-LoRA to LLaMA with trainable Clinical LLaMA-LoRA. This combination of LLaMA with trainable Clinical LLaMA-LoRA and Downstream LLaMA-LoRA achieves the highest macro-averaged AUROC score of 72.81%. The macro-averaged AUROC score of Clinical LLaMA-LoRA was almost similar to that of PMC-LLaMA with LoRA, suggesting similar efficacy between Clinical LLaMA-LoRA and the full fine-tuning process that PMC-LLaMA has undergone. Moreover, Clinical LLaMA-LoRA offers the advantage of reduced computational resources and training time, which is aligned with the requirements of practical implementation in clinical settings.</p><p>Can LoRA help better fine-tune clinicallytrained LMs? The baseline models are relatively smaller in size compared to the LLaMA-based models, which may be a better fit to care providers with limited access to computing resources. To that end, we experimented with fine-tuning the baseline models with LoRA.</p><p>Table <ref type="table" target="#tab_4">4</ref> shows the obtained results. All baseline models see improvements in AUROC scores in all tasks. For instance, the LoRA-equipped Blue-BERT achieves an improved macro-averaged AU-ROC score of 71.56% compared to the conventional fine-tuning with 69.59%.</p><p>This finding highlights the possibility of using LoRA to efficiently fine-tune clinically trained LMs, such as BlueBERT, to downstream use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this study, we propose a two-step PEFT framework. We introduce Clinical LLaMA-LoRA, a LoRA <ref type="bibr" target="#b5">(Hu et al., 2022)</ref> adapter built upon LLaMA <ref type="bibr">(Touvron et al., 2023)</ref>. Then, we introduce Downstream LLaMA-LoRA, a task-specific adapter that is trained on top of the pretrained Clinical LLaMA-LoRA. The fusion of the two adapters achieves state-of-the-art performance with an AU-ROC score of 72.81% macro-averaged across all clinical NLP downstream tasks, which represents a 3.22% improvement over the previous best-performing model. Our proposed framework achieves improvement in performance while reducing the computational requirements, which is suited for clinical settings that are often constrained by their computational power.</p><p>We also find that the LoRA-equipped BlueBERT model achieves a considerable improvement of macro-averaged AUROC score over the full finetuning (71.56% compared to 69.59%), with notable improvements in mortality and length-of-stay prediction. These findings further highlight the potential to achieve strong performance without extensive computational resources.</p><p>Future works may explore developing a schema to address various real-world use cases, building upon the findings of this study. Such a schema would use multiple Downstream LLaMA-LoRA adapters tailored for different use cases while leveraging the pretrained LLM and Clinical LLaMA-LoRA as the foundation. This solution would also be suited for use cases which rely on private data commonly encountered in care provider settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>This study presents a two-step PEFT framework aimed at effectively adapting LLMs to diverse clinical downstream applications. However, the evaluation of our model was restricted to MIMIC-based datasets, which are constrained to English and obtained exclusively within the Commonwealth of Massachusetts, United States of America. Consequently, despite the promising efficacy demonstrated by our proposed method, it would have been advantageous to directly assess its performance across diverse hospital systems spanning various geographical locations and languages. This would enable a more comprehensive understanding of its applicability and generalizability. However, it is essential to acknowledge that conducting such an analysis would require working within a trusted research environment and obtaining the necessary permissions to access the relevant datasets.</p><p>It is crucial to recognise the restrictions imposed on accessing internal clinical datasets, as they limit our ability to evaluate the effectiveness of our approach across different care provider systems. Therefore, we encourage care providers to conduct internal experiments within their trusted research environment to ensure the efficacy of our proposed method within their specific use cases should they adopt this approach.</p><p>Despite the demonstrated performance improvements, the proposed model may still be susceptible to spurious correlations. Predicting patient outcomes solely based on clinical notes presents significant challenges due to the other factors that may not be captured within those notes. For instance, the length of patient's in-hospital stay is not solely correlated with their diagnoses and disease progression. Factors such as the patient's insurance status, which is not typically mentioned in clinical notes, can severely impact the duration of a patient's stay. Therefore, we encourage end users of such clinical LLMs to consider additional measures to ensure predictions that reflect a holistic view of the patient's situation, instead of relying solely on the predictions of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>In this study, we use MIMIC-based datasets obtained after completing the necessary training. These datasets comply with de-identification standards set by the Health Insurance Portability and Accountability Act (HIPAA) through data cleans-ing. Due to privacy concerns, we refrain from including direct excerpts of the data in the paper. We also refrain from publicly sharing the pretrained checkpoints.</p><p>While our model demonstrates effectiveness, it is important to acknowledge the risks associated with relying solely on clinical outcome prediction models. There are crucial pieces of information that can be found beyond the scope of clinical notes. Considering the potential impact on patient health outcomes, it is crucial to exercise caution when utilising these clinical LLMs. Therefore, we propose that the PEFT adapter generated by our framework, in conjunction with the pretrained LLM, should be used as an aid rather than a replacement for trained clinical professionals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the proposed two-step PEFT framework. Clinical LLaMA-LoRA fine-tunes the pretrained LLaMA to the clinical domain. Downstream LLaMA-LoRA further fine-tunes the domain-adapted model to downstream clinical tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Domain-adaptive Pretraining results of LLaMA and PMC-LLaMA trained on MIMIC-IV clinical notes with a language modelling objective. Lower perplexity scores indicate better language modelling performance. The boldface row indicates the model with the lowest perplexity score from each base model variant.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>AUROC scores in clinical downstream document classification tasks. The macro-averaged AUROC score is calculated by taking the average of AUROC scores across all tasks. The boldface cell indicates the highest AUROC score in a column, the row in italic indicates the model variant with the highest macro-averaged AUROC in its category. * Due to restricted computing resources, the fine-tuning of LLaMA and PMC-LLaMA was constrained to only training the final classification layer.</figDesc><table><row><cell>Model</cell><cell cols="5">PMV MOR LOS DIAG PROC Macro Average</cell></row><row><cell>BlueBERT</cell><cell>53.12 76.95 66.36</cell><cell>73.81</cell><cell>77.70</cell><cell>69.59</cell></row><row><cell>UmlsBERT</cell><cell cols="2">55.49 75.87 66.06 64.34</cell><cell>74.19</cell><cell>67.19</cell></row><row><cell>Bio+ClinicalBERT</cell><cell cols="2">54.49 72.92 65.13 65.97</cell><cell>71.73</cell><cell>66.05</cell></row><row><cell>CORe</cell><cell cols="2">52.11 71.52 64.17 72.40</cell><cell>72.73</cell><cell>66.59</cell></row><row><cell>LLaMA *</cell><cell cols="2">51.38 66.80 57.65 60.06</cell><cell>63.83</cell><cell>58.61</cell></row><row><cell>+ LoRA</cell><cell cols="2">51.65 74.89 65.70 78.37</cell><cell>87.49</cell><cell>71.62</cell></row><row><cell>+ Clinical LLaMA-LoRA (Frozen)</cell><cell cols="2">51.62 65.66 58.16 63.47</cell><cell>69.01</cell><cell>61.58</cell></row><row><cell>+ Downstream LLaMA-LoRA</cell><cell cols="2">51.11 66.00 58.04 60.46</cell><cell>65.30</cell><cell>60.18</cell></row><row><cell cols="3">+ Clinical LLaMA-LoRA (Trainable) 55.76 74.81 64.83 76.07</cell><cell>82.76</cell><cell>70.85</cell></row><row><cell>+ Downstream LLaMA-LoRA</cell><cell>56.72 76.99 65.86</cell><cell>78.29</cell><cell>86.17</cell><cell>72.81</cell></row><row><cell>PMC-LLaMA *</cell><cell cols="2">53.06 66.77 57.94 60.17</cell><cell>64.63</cell><cell>60.51</cell></row><row><cell>+ LoRA</cell><cell cols="2">53.84 78.03 66.14 78.81</cell><cell>86.68</cell><cell>72.70</cell></row><row><cell>+ Clinical LLaMA-LoRA (Frozen)</cell><cell cols="2">51.33 67.19 58.13 63.59</cell><cell>68.26</cell><cell>60.06</cell></row><row><cell>+ Downstream LLaMA-LoRA</cell><cell cols="2">50.90 67.00 58.31 60.50</cell><cell>64.42</cell><cell>60.23</cell></row><row><cell cols="3">+ Clinical LLaMA-LoRA (Trainable) 52.88 75.86 65.89 79.66</cell><cell>86.85</cell><cell>72.23</cell></row><row><cell>+ Downstream LLaMA-LoRA</cell><cell cols="2">52.21 76.54 68.42 78.67</cell><cell>87.08</cell><cell>72.58</cell></row><row><cell>Model</cell><cell cols="5">PMV MOR LOS DIAG PROC Macro Average</cell></row><row><cell>BlueBERT</cell><cell cols="3">53.12 76.95 66.36 73.81</cell><cell>77.70</cell><cell>69.59</cell></row><row><cell>+ LoRA</cell><cell cols="3">55.77 81.90 70.48 70.66</cell><cell>78.10</cell><cell>71.56</cell></row><row><cell>UmlsBERT</cell><cell cols="3">55.49 75.87 66.06 64.34</cell><cell>74.19</cell><cell>67.19</cell></row><row><cell>+ LoRA</cell><cell cols="3">56.59 80.33 69.03 69.68</cell><cell>77.53</cell><cell>70.63</cell></row><row><cell>BioClinicalBERT</cell><cell cols="3">54.49 72.92 65.13 65.97</cell><cell>71.73</cell><cell>66.05</cell></row><row><cell>+ LoRA</cell><cell cols="3">56.13 78.81 68.28 68.53</cell><cell>75.19</cell><cell>69.39</cell></row><row><cell>CORe</cell><cell cols="3">52.11 71.52 64.17 72.40</cell><cell>72.73</cell><cell>66.59</cell></row><row><cell>+ LoRA</cell><cell cols="3">55.31 79.27 68.18 67.34</cell><cell>72.36</cell><cell>68.49</cell></row><row><cell cols="4">LLaMA + Clinical LLaMA-LoRA + Downstream LoRA 56.72 76.62 65.86 78.29</cell><cell>86.17</cell><cell>72.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>AUROC scores of the LoRA-equipped baseline models in clinical downstream tasks. The boldface cell indicates the highest AUROC score in a column. The row in italic indicates the model variant with the highest macro-averaged AUROC in its category.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Specifically, we use the W&amp;B Sweep APIs: https:// docs.wandb.ai/guides/sweeps</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements AG was supported by the <rs type="funder">United Kingdom Research and Innovation</rs> (grant <rs type="grantNumber">EP/S02431X/1</rs>), <rs type="institution">UKRI Centre for Doctoral Training in Biomedical AI at the University of Edinburgh, School of Informatics</rs>. PM was partially funded by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under grant agreement no. <rs type="grantNumber">875160</rs>, <rs type="affiliation">ELIAI (The Edinburgh Laboratory</rs> for <rs type="funder">Integrated Artificial Intelligence) EP-SRC</rs> (grant no. <rs type="grantNumber">EP/W002876/1</rs>), an industry grant from <rs type="funder">Cisco</rs>, and a donation from <rs type="funder">Accenture LLP</rs>; and is grateful to NVIDIA for the GPU donations. BA was partially funded by by <rs type="funder">Legal and General PLC</rs> as part of the <rs type="institution">Advanced Care Research Centre</rs> and by the <rs type="funder">Artificial Intelligence and Multimorbidity: Clustering in Individuals, Space and Clinical Context (AIM-CISC)</rs> grant <rs type="grantNumber">NIHR202639</rs>. For the purpose of open access, AG has applied a creative commons attribution (CC BY) licence to any author-accepted manuscript version arising.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PPrs5RP">
					<idno type="grant-number">EP/S02431X/1</idno>
				</org>
				<org type="funding" xml:id="_pnW2mTW">
					<idno type="grant-number">875160</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_sYR7bvr">
					<idno type="grant-number">EP/W002876/1</idno>
				</org>
				<org type="funding" xml:id="_6Qcfar6">
					<idno type="grant-number">NIHR202639</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Configurations</head><p>We use HuggingFace's Transformers <ref type="bibr" target="#b29">(Wolf et al., 2020)</ref> and PEFT <ref type="bibr" target="#b18">(Mangrulkar et al., 2022)</ref> libraries for the experiments. All LLaMA-based models are trained on one NVIDIA A100-80GB GPU, while the baseline models are trained on a single NVIDIA GeForce GTX 1080 Ti-16GB GPU.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Publicly available clinical BERT embeddings</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1909</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A tutorial on bayesian optimization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Frazier</surname></persName>
		</author>
		<idno>CoRR, abs/1807.02811</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458754</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LoRA: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Ying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charolotta</forename><surname>Lindvall</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.clinicalnlp-1.11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Clinical Natural Language Processing Workshop</title>
		<meeting>the 3rd Clinical Natural Language Processing Workshop</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="94" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MIMIC-IV, a freely accessible electronic health record dataset</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Bulgarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayad</forename><surname>Gayles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Shammout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><surname>Gow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><forename type="middle">A</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-022-01899-x</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.13026/RJ8X-V335</idno>
		<title level="m">Clinical-T5: Large Language Models Built Using MIMIC Clinical Text</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prefix-Tuning: Optimizing Continuous Prompts for Generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Clinical-longformer and clinical-bigbird: Transformers for long clinical sequences</title>
		<author>
			<persName><forename type="first">Yikuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramsey</forename><forename type="middle">M</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faraz</forename><forename type="middle">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno>CoRR, abs/2201.11838</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Beyond one-model-fits-all: A survey of domain specialization for large language models</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xujiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmoy</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hejie</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Panalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.18703</idno>
		<idno>CoRR, abs/2305.18703</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">2021a. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>CoRR, abs/2110.07602</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Zhilin Yang, and Jie Tang. 2021b. GPT understands, too. CoRR, abs/2103.10385</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BioGPT: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1093/bib/bbac409</idno>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Bbac409</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Peft: Stateof-the-art parameter-efficient fine-tuning methods</title>
		<author>
			<persName><forename type="first">Sourab</forename><surname>Mangrulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<ptr target="https://github.com/huggingface/peft" />
	</analytic>
	<monogr>
		<title level="m">Lysandre Debut, Younes Belkada, and Sayak Paul</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Umls-BERT: Clinical domain knowledge augmentation of contextual embeddings using the Unified Medical Language System Metathesaurus</title>
		<author>
			<persName><forename type="first">George</forename><surname>Michalopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussam</forename><surname>Kaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5006</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Citation Key: JMLR:v21:20-074</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modular and Parameter-Efficient Fine-Tuning for NLP Models</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts<address><addrLine>Abu Dubai</addrLine></address></meeting>
		<imprint>
			<publisher>UAE. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="23" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The UMLS Metathesaurus: Representing different views of biomedical concepts</title>
		<author>
			<persName><forename type="first">P L</forename><surname>Schuyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W T</forename><surname>Hole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M S</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D D</forename><surname>Sherertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="222" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><forename type="middle">Kumar</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perry</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Gamble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaneal</forename><surname>Sch?rli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">R</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juraj</forename><surname>Gottweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenad</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Rajkomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><forename type="middle">K</forename><surname>Barral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Semturs</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2212.13138</idno>
		<idno>CoRR, abs/2212.13138</idno>
	</analytic>
	<monogr>
		<title level="j">Aakanksha Chowdhery</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Alan Karthikesalingam, and Vivek Natarajan</publisher>
			<pubPlace>Philip Andrew Mansfield, Blaise Ag?era y Arcas, Dale</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards expert-level medical question answering with large language models</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juraj</forename><surname>Gottweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellery</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darlene</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Lachgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">Andrew</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushant</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewa</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Ag?era Y Arcas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenad</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renee</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Semturs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><forename type="middle">K</forename><surname>Barral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><forename type="middle">R</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.09617</idno>
		<idno>CoRR, abs/2305.09617</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.13971</idno>
		<idno>CoRR, abs/2302.13971</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration</title>
		<author>
			<persName><forename type="first">Betty</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens-Michalis</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Mayrdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klemens</forename><surname>Budde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Loeser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.75</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
			<biblScope unit="volume">7</biblScope>
			<pubPlace>New Orleans, LA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>th International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pmc-llama: Further finetuning llama on medical papers</title>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.14454</idno>
		<idno>CoRR, abs/2304.14454</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A large language model for electronic health records. npj Digit</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aokun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><forename type="middle">M</forename><surname>Pournejatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Hoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaleb</forename><forename type="middle">E</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Parisien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheryl</forename><surname>Compas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">G</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Magoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gloria</forename><forename type="middle">P</forename><surname>Harle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duane</forename><forename type="middle">A</forename><surname>Lipori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">R</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">A</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Shenkman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-022-00742-2</idno>
	</analytic>
	<monogr>
		<title level="j">Medicine</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
