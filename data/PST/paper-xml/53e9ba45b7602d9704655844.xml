<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A saliency map based on sampling an image into random rectangular regions of interest</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-02-22">22 February 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Narayan</forename><surname>Tadmeri</surname></persName>
						</author>
						<author>
							<persName><surname>Vikram</surname></persName>
							<email>nvikram@techfak.uni-bielefeld.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Informatics Group</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<postCode>33615</postCode>
									<settlement>Bielefeld</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Research Institute for Cognition and Robotics (CoR Lab)</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<postCode>33615</postCode>
									<settlement>Bielefeld</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marko</forename><surname>Tscherepanow</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Informatics Group</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<postCode>33615</postCode>
									<settlement>Bielefeld</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Britta</forename><surname>Wrede</surname></persName>
							<email>bwrede@techfak.uni-bielefeld.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Informatics Group</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<postCode>33615</postCode>
									<settlement>Bielefeld</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Research Institute for Cognition and Robotics (CoR Lab)</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<postCode>33615</postCode>
									<settlement>Bielefeld</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Applied Informatics Group</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<postCode>33615</postCode>
									<settlement>Bielefeld</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A saliency map based on sampling an image into random rectangular regions of interest</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-02-22">22 February 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">A3482A1CF353909C4353CD2600472590</idno>
					<idno type="DOI">10.1016/j.patcog.2012.02.009</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Visual attention Saliency map Salient region detection Eye-gaze prediction Bottom-up attention</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article we propose a novel approach to compute an image saliency map based on computing local saliencies over random rectangular regions of interest. Unlike many of the existing methods, the proposed approach does not require any training bases, operates on the image at the original scale and has only a single parameter which requires tuning. It has been tested on the two distinct tasks of salient region detection (using MSRA dataset) and eye gaze prediction (using York University and MIT datasets). The proposed method achieves state-of-the-art performance on the eye gaze prediction task as compared with nine other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual attention and its associated cognitive neurobiology has been a subject of intense research over the last five decades. The attention mechanism in humans helps them in focusing their limited cognitive resources to the context relevant stimuli while suppressing the ones which are not important. Having an artificial system which could simulate this attention mechanism could positively impact upon the way in which various cognitive and interactive systems are designed. In view of this, the computer science community has invested considerable time and efforts to realize a computational model of visual attention which at least partially exhibits the characteristics of a human visual attention system. Several computational approaches to visual attention have been proposed in the literature since the seminal work of Koch and Ullman in 1985 <ref type="bibr" target="#b0">[1]</ref>. The same authors proposed the concept of a saliency map, which constitutes the surrogate representation of the visual attention span over an input image for a free viewing task. Saliency maps are now employed for several applications in robotics, computer vision and data transmission. Some of the successful applications are object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, object recognition <ref type="bibr" target="#b3">[4]</ref>, image summarization <ref type="bibr" target="#b4">[5]</ref>, image segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, image compression <ref type="bibr" target="#b7">[8]</ref> and attention guidance for human-robot interaction <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Most of the existing visual attention approaches process pixels or image windows sequentially, which is in contrast to the way the human visual system operates. It has been shown in the literature that the receptive fields inside the retina operate randomly at both positional and scale spaces while processing the visual stimuli <ref type="bibr" target="#b10">[11]</ref>.</p><p>Motivated by this, we propose a simple and efficient method which is based on random sampling of the image into rectangular regions of interest and computing local saliency values. The proposed method is shown to have comparable performance with existing methods for the task of salient region detection. Furthermore, the proposed method also achieves state-of-the-performance for the task of predicting eye-gaze. The method has been tested on MSRA <ref type="bibr" target="#b11">[12]</ref>, York University <ref type="bibr" target="#b12">[13]</ref>, MIT <ref type="bibr" target="#b13">[14]</ref> datasets and benchmarked along with nine of the other existing state-of-theart methods to corroborate its performance.</p><p>The paper is organized as follows. We provide a review of the existing computational approaches to visual attention with a brief description of their strengths and shortcomings in Section 2. The motivation leading to the current work is described in Section 3. The proposed approach is described by means of a pseudo-code and a graphical illustration in Section 4. In Section 5, we describe the experiments carried out to validate the performance of the proposed method for the tasks of eye-gaze prediction and salient region detection. Finally, we conclude this work by highlighting its current shortcomings with a brief discussion about the future directions of the current work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature review</head><p>The theoretical framework for the computation of saliency maps was first proposed in <ref type="bibr" target="#b0">[1]</ref> and later realized in <ref type="bibr" target="#b14">[15]</ref>. Subsequently it has led to the development of several other saliency approaches based on different mathematical and computational paradigms. The existing approaches can be classified into seven distinct categories based on the computational scheme they employ.</p><p>Hierarchical approaches: They perform a multi-scale image processing and aggregate the inputs across different scales to compute the final saliency map.</p><p>Spectral approaches: They operate by decomposing the input image into Fourier or Gabor spectrum channels and obtain the saliency maps by selecting the prominent spectral coefficients.</p><p>Power law based approaches: These approaches compute saliency maps by removing redundant patterns based on their frequency of occurrence. Rarely occurring patterns are considered salient while frequently occurring patterns are labeled redundant.</p><p>Image contrast approaches: The mean pixel intensity value of the entire image or of a specified sub-window is utilized to compute the contrast of each pixel in the image. The contrast is analogously treated as the image saliency.</p><p>Entropy-based approaches: The mutual information between patterns is employed to optimize the entropy value, where a larger entropy value indicates that a given pattern is salient.</p><p>Center-surround approaches: These approaches compute the saliency of a pixel by contrasting the image features within a window centered on it.</p><p>Hybrid approaches: Models of this paradigm employ a classifier in combination with one or more approaches to compute saliency.</p><p>We briefly explore the existing saliency approaches based on the aforementioned categories in the sub-sections to follow. Interested readers are pointed to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> for a more detailed and exhaustive review on saliency approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hierarchical approaches</head><p>The most popular approach in this category is the one proposed by <ref type="bibr">Itti et al. [15]</ref>. This approach computes 41 different feature maps for a given input image based on color, texture, gradient and orientation information. The resulting feature maps are fused into a single map using a winner takes all (WTA) network and an inhibition of return mechanism (IOR). This approach has served as a classical benchmark system. Despite being biologically inspired and efficient, its performance is constrained by the fusion of multiple maps and the requirement to process multiple features. The fusion method employed to compute the master saliency map from the various feature maps plays a vital role in its accuracy. Arriving at a generalized rule of fusion for various maps is complicated and requires intensive crossvalidation to fine tune the fusion parameters. In general, hierarchical methods tend to ignore visually significant patterns which are locally occurring as they are primarily driven by global statistics of an image <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spectral approaches</head><p>Several approaches to compute a saliency map which operate at the spectral domain have thus been proposed in the literature. A Gabor wavelet analysis and multi-scale image processing based approach was proposed in <ref type="bibr" target="#b17">[18]</ref>. Fourier transform has also been extensively utilized to generate saliency maps as seen from <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. In the approach of <ref type="bibr" target="#b7">[8]</ref>, the input image is decomposed into four different channels based on opponent colors and intensity. A quaternion Fourier analysis of the decomposed channels is carried out to compute the master saliency map. Two dimensional Gabor wavelets of Fourier spectrum have also been utilized for computing saliency maps in <ref type="bibr" target="#b19">[20]</ref> and the final saliency map is reweighted using a center-bias matrix. Spectral residual approaches proposed in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> are another class of approaches which highlight the saliency by removing redundancy from the Fourier spectrum. The saliency map is thus constructed from a residual frequency spectrum obtained by the difference between an image frequency spectrum and a mean frequency spectrum. Though straight forward, the method is compounded due to the fact that the pattern of mean frequency spectrum is subjective. These methods are found to be successful in detecting proto-objects in images. As it can be seen from the illustrations in <ref type="bibr" target="#b7">[8]</ref>, Fourierbased methods are affected by the number of coefficients selected for image reconstruction and the scale at which the input image is processed. Like subspace analysis, the method results in the loss of information during image reconstruction and is compromised by illumination, noise and other image artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Power law based approaches</head><p>Power laws are basically used to model human visual perception. Several approaches to saliency which employ power laws have thus been proposed. A Zipf's law based saliency map has been proposed in <ref type="bibr" target="#b22">[23]</ref>. This work is similar to those of <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, except that it operates directly on the pixels rather than on the spectral space. An integrated Weibull's distribution based saliency map has been proposed in <ref type="bibr" target="#b23">[24]</ref>, and its performance on the eye-gaze data is shown to be better than other power law based approaches. The method employs maximum likelihood estimates (MLE) to set the parameters of various distributions. Despite its theoretic appeal, the Weibull's distribution based saliency map has a large parameter set and the heuristic to fix and optimize them constitutes a major drawback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Image contrast approaches</head><p>A simple but efficient approach based on computing the absolute difference of a pixel and the image mean was proposed in <ref type="bibr" target="#b24">[25]</ref>. Similar approaches proposed by the same authors in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7]</ref>, employ maximal symmetric regions and novel dissimilarity metrics respectively to compute the final saliency map. Another symmetry based saliency approach was proposed in <ref type="bibr" target="#b26">[27]</ref>, which employs color, orientation and contrast symmetry features to generate the final saliency map. It also reasonably simulates eye-gaze for free viewing task in case of natural sceneries with no specific salient objects in it. A distance transform based approach was proposed in <ref type="bibr" target="#b27">[28]</ref>, which computes an edge map for each gray scale threshold and fuses them to generate a final saliency map. These methods are successfully applied for proto-object detection and out-beats many state-of-theart methods without having many of their drawbacks. Poor global contrast of an image affects the performance of these methods and local-statistics based approaches for saliency computation have been proposed in the literature to address this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Entropy-based approaches</head><p>The popular approach of <ref type="bibr" target="#b28">[29]</ref> is based on local contrasts and maximizes the mutual information between features by employing independent component analysis (ICA) bases. A set of ICA bases is pre-computed using a patch size of 7 Â 7 pixels. Subsequently it is used to compute the conditional and joint distribution of features for information maximization. Experiments conducted on the York University eye-gaze dataset <ref type="bibr" target="#b12">[13]</ref> have proven its efficiency. But this method is constrained by its emphasis on edges and neglects salient regions <ref type="bibr" target="#b29">[30]</ref>. It also adds a spurious border effect to the resultant image and requires re-scaling of the original image to a lower scale in order to make the computational process more tractable. Another ICA based approach was proposed in <ref type="bibr" target="#b30">[31]</ref> where image selfinformation is utilized to estimate the probability of a target at each pixel position. It is further fused with top-down features derived from ICA bases to build the final saliency map. The method proposed in <ref type="bibr" target="#b31">[32]</ref> employs sparse bases to extract sub-band features from an image. The mutual information between the sub-band features is calculated by realizing a random-walk on them and initializing the site entropy rate as the weight of the edges in the graph. An extension of this paradigm can been seen in the recent approach proposed in <ref type="bibr" target="#b32">[33]</ref>, where entropy of a center versus a surround region is computed as the saliency value of a pixel. Other entropy-based approaches like <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> employ incremental coding length to compute the final saliency map. These methods which rely on information theoretic approaches are in general constrained by the requirements of training bases, the patch size parameters and the size of the training bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Center-surround approaches</head><p>A saliency map based on discriminant center-surround entropy contrast which does not require training bases was proposed in <ref type="bibr" target="#b35">[36]</ref>. It correlates well with human eye-gaze data but is constrained by the subjectivity involved in the computation of weights for fusing the different maps to compute the master saliency map. A recent center-surround contrast method <ref type="bibr" target="#b36">[37]</ref>, identifies pre-attentive segments and computes the mutual saliency between them. An innovative application of this saliency map has been found useful for pedestrian detection. A multi-scale center-surround saliency map was proposed in the work of <ref type="bibr" target="#b37">[38]</ref>, where the saliency of a pixel is determined by the dissimilarity of the center to its surround over multiple scales. Local analysis of gradients along with center-surround paradigm is advocated as an alternative to multi-scale image analysis as it can be seen in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. In <ref type="bibr" target="#b38">[39]</ref>, local steering kernels are employed to compute the saliency of a pixel by contrasting the gradient covariance of a surrounding region. The method is shown to be robust to noise and drastic illumination changes, but it is computationally expensive as a set of compound features needs to be computed for each pixel in the image. In order to achieve a tractable runtime, the image is down-scaled. The approach proposed in <ref type="bibr" target="#b39">[40]</ref> uses color and edge orientation information to compute the saliency map, using a framework similar to the one presented in <ref type="bibr" target="#b38">[39]</ref>. Selecting an appropriate window size for center and surround patches plays an important role in obtaining a higher quality saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Hybrid approaches</head><p>To overcome the issue of patch size parameters, many machine learning based saliency approaches have been proposed. A graph-based visual saliency approach is proposed in <ref type="bibr" target="#b40">[41]</ref> which implements a Markovian representation of feature maps and utilizes a pyscho-visual contrast measure to compute the dissimilarities between features. A saliency map based on modeling eyemovements using support vector machines (SVMs) is proposed in <ref type="bibr" target="#b41">[42]</ref>. However, this approach requires enormous amounts of data to learn saliency weights reasonably. Another successful approach based on kernel density estimation (KDE) is proposed in <ref type="bibr" target="#b42">[43]</ref>. The authors recommend to construct a set of KDE approaches based on region segmentation using the mean shift algorithm. Based on its color distinctiveness and spatial distribution, the color saliency and spatial saliency of each KDE approach are evaluated. The method is found to be successful in salient object detection, but has unwieldy run-time as it relies on image segmentation.</p><p>A salient object selection mechanism based on an oscillatory correlation approach was proposed in <ref type="bibr" target="#b43">[44]</ref>. The method outputs an object saliency map, which resembles image segmentation. The method requires fine tuning of neural network weights for an inhibitory mechanism which is heuristic and hence cannot be used for real-time visual attention. A Hebbain neural network based saliency mechanism which simulates lateral surround inhibition is proposed in <ref type="bibr" target="#b44">[45]</ref>. Unlike <ref type="bibr" target="#b43">[44]</ref>, the method in <ref type="bibr" target="#b44">[45]</ref> is computationally efficient as it employs the pulsed cosine transform to produce the final saliency map.</p><p>A color saliency boosting function is introduced in <ref type="bibr" target="#b45">[46]</ref> which is obtained from an isosalient image surface. The final saliency map is based on the statistics of color image derivatives and employs an information theoretic approach to boost the color information content of an image. An extension of this work can be found in <ref type="bibr" target="#b46">[47]</ref>. It performs reasonably well on both eye-gaze fixation and salient object detection datasets. In <ref type="bibr" target="#b47">[48]</ref>, the probability distribution of colors and orientations of an image is computed and either of these features are selected to compute the final saliency map. This method avoids multi-scale image analysis but fails when the image has a low color or gradient contrast.</p><p>A few hybrid approaches attempt to combine the positive aspects of multi-scale analysis, sub-band decomposition, color boosting and as well as center-surround paradigms. Fusion of the center-surround hypothesis and oriented sub-band decomposition to develop a saliency map has been proposed in <ref type="bibr" target="#b48">[49]</ref>. A radically different approach which tries to detect salient regions by estimating the probability of detecting an object in a given sliding window is proposed in <ref type="bibr" target="#b49">[50]</ref>. They employ the concept of super pixel straddling, coupled with edge density histograms, color contrasts and the saliency map of <ref type="bibr" target="#b14">[15]</ref>. A linear classifier is trained on an image dataset to build a bag-of-features to arrive at a prior for an object in an image. The method is theoretically very attractive, but is subjected to high variations in the performance as too many features, maps and parameters are involved which require fine tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation</head><p>As it can observed from our previous review, many of the existing computational approaches to visual attention are constrained by one or more of the following issues like : requirement of training bases, large set of tunable parameters, integration of complex classifiers, downscaling of the original image in order to obtain a tractable computational run-time, high, complexity associated with programming, and not being biologically motivated. Most of the existing approaches to compute saliency maps process input image pixels sequentially with fixed sliding windows. But salient regions or objects can occur at arbitrary positions, shapes and scales in an image. Research in vision sciences has shown that the human visual system has its receptive fields scattered randomly and does not process input stimuli in a sequential manner <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51]</ref>. Furthermore, it has been shown in <ref type="bibr" target="#b51">[52]</ref> that each visual stimulus is biased by every other stimulus present in the attention space.</p><p>In view of the aforementioned discussion, we propose a random center-surround method which operates by computing local saliencies over random regions of an image. This captures local contrasts unlike the global methods for computing saliency maps. Furthermore, it does not require any training priors and has only a single parameter which needs tuning. The proposed method also avoids competition between multiple saliency maps which is in contrast to the approaches proposed by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The proposed saliency map</head><p>We consider a scenario where the input I is a color image of dimension r Â c Â 3, where r and c are the number of rows and columns respectively. The input image I is initially subjected to a Gaussian filter in order to remove noise and abrupt onsets. It is further converted into the CIE1976 L n a n b n1 space and decomposed into the respective L n , a n and b n channels each of dimension r Â c. The L n a n b n space is preferred over the other color spaces because of its similarity to the human psycho-visual space <ref type="bibr" target="#b25">[26]</ref> and is also recommended as a standard to compute saliency maps in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7]</ref>. The saliency maps due to L n , a n and b n channels are referred to as S L , S a and S b respectively and their dimensions are also equal to r Â c.</p><p>Furthermore, n random sub-windows are generated over each of the L n , a n and b n channels. The upper left and the lower right coordinates of the ith random sub-window is denoted by ðx 1i ,y 1i Þ and ðx 2i ,y 2i Þ respectively. The saliency value at a particular coordinate position in a given channel is defined as the sum of the absolute differences of the pixel intensity value to the mean intensity values of the random sub-windows in which it is contained. The final saliency map S is also of dimension r Â c. Computing the final saliency value for a given position as the Euclidean norm of saliencies over different channels in the L n a n b n space is recommended as an ideal fusion rule in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>A discrete uniform probability distribution function is used to generate the random sub-window co-ordinates, as this helps in placing windows without having any bias towards a specific window size or spatial region of the image. This property is important because salient regions or objects in an image can occur at arbitrary positions and scales.</p><p>The resulting saliency map S is normalized in the interval [0,255] and subsequently subjected to median filtering. The median filter is chosen because of its ability to preserve edges despite eliminating noise. To further enhance the contrast of S, we subject it to histogram equalization as recommended in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b52">53]</ref>. This image enhancement is in consonance with the fact that the human visual system enhances the perceptual contrast of the salient stimulus in a visual scene <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Note that our approach does not downscale the input image to a lower resolution like other approaches <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>. In addition our method does not require prior training bases in contrast to <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>. Also n is the only parameter which requires tuning, the details of which are presented in the next section. The algorithm for the proposed approach is as follows.</p><p>Algorithm: Random center-surround saliency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>(1) Generate saliencies for each component</p><formula xml:id="formula_0">IMG (RGB Image) of size r Â c Â 3 (2) n-</formula><formula xml:id="formula_1">S L ¼ CompSal ðL Ã ,n,x 1 ,y 1 ,x 2 ,y 2 Þ S a ¼ CompSal ða Ã ,n,x 1 ,y 1 ,x 2 ,y 2 Þ S b ¼ CompSal ðb Ã ,n,x 1 ,y 1 ,x 2 ,y 2 Þ</formula><p>Step Update Generate random window co-ordinates Set all elements of x 1 ,y 1 ,x 2 ,y 2 to 0 for i¼1 to n</p><formula xml:id="formula_2">ISM for i ¼ 1 to n Area i ¼ ðx 2i Àx 1i þ 1Þ Á ðy 2i Ày 1i þ 1Þ Sum i ¼ 0 for j ¼ x 1i to x 2i for k ¼ y 1i to y 2i Sum i ¼ Sum i þ ISM j,k end-k end-j Mean i ¼ Sum i Area i for j ¼ x 1i to x 2i for k ¼ y 1i to y 2i ISM j,k ¼ ISM j,k þ 9I j,k ÀMean i 9 end-k end-j end-i Algorithm : Generate_Windows Input :<label>(1</label></formula><formula xml:id="formula_3">x 1i ¼ Random number in ½1,rÀ1 y 1i ¼ Random number in ½1,cÀ1 x 2i ¼ Random number in ½x 1i þ 1,r y 2i ¼ Random number in ½y 1i þ1,c end-i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm: Fusion</head><p>Input :</p><p>(1) Matrices A, B and C of size r Â c Output :</p><p>(1) Fused matrix FM of size r Â c Method Step 1 :</p><p>Apply pixel-wise Euclidean norm Set all elements of FM to 0 for i¼ 1 to r for j ¼1 to c</p><formula xml:id="formula_4">FM i,j ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi A i,j Á A i,j þB i,j Á B i,j þ C i,j Á C i,j p end-j end-i</formula><p>An illustration of the above paradigm is given in Fig. <ref type="figure" target="#fig_2">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>Several experiments were conducted to validate the performance of the proposed saliency map for two distinct tasks of eye-gaze prediction and salient region detection in free viewing conditions. Salient region detection and eye-gaze prediction are the two most significant applications of saliency maps. Salient region detection is relevant in the context of computer vision tasks like object detection, object localization and object tracking in videos <ref type="bibr" target="#b24">[25]</ref>. Automatic prediction of eye-gaze is important in the context of image aesthetics, image quality assessment, human-robot interaction and other tasks which involve detecting image regions that are semantically interesting <ref type="bibr" target="#b13">[14]</ref>. The contemporary saliency maps are either employed to detect salient regions as in the case of <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, or are used to predict eye-gaze patterns as in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39]</ref>. Only few of the existing saliency approaches like <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b14">15]</ref> have consistent performance on both of these tasks. Although these two tasks appear similar, there are subtle differences between them. Salient regions of an image are those which are visually interesting. But human eye-gaze which focuses mainly on salient regions are also distracted by semantically relevant regions <ref type="bibr" target="#b55">[56]</ref>. The performance on the eye-gaze prediction task was validated on two different datasets of York University <ref type="bibr" target="#b12">[13]</ref> and MIT <ref type="bibr" target="#b13">[14]</ref>. Subsequent experiments to corroborate the performance on salient object detection task were conducted on the popular MSRA dataset <ref type="bibr" target="#b11">[12]</ref>.</p><p>The following parameter settings were used as a standard for all the experiments carried out. A rotational symmetric Gaussian low pass filter (size 3 Â 3 with s ¼ 0:5; the default Matlab configuration) was used as a pre-processor on the images for noise removal as recommended in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. The number of distinct random sub-windows n was set to 0:02 Â r Â c, as it led to a more stable performance. Details about fine tuning n is given in Section 5.1. A median filter of size 11 Â 11 was employed to smooth the resultant saliency map before being enhanced by histogram equalization. The employed Gaussian filter, median filter and histogram equalization are based on the usual straight forward methods. All experiments were conducted using Matlab v7.10.0 (R2010a) on an Intel Core 2 Duo processor with Ubuntu 10.04.1 LTS (Lucid Lynx) as operating system. The inbulit srgb2lab Matlab routine was used to convert the input image from RGB colorspace to L n a n b n colorspace. This results in L n , a n and b n images whose pixel intensity values are normalized in the range of [0, 255]. We selected nine state-of-the-art methods of computing saliency maps to compare and contrast the proposed method. The methods are global contrast 2 <ref type="bibr" target="#b24">[25]</ref>, entropy 3 <ref type="bibr" target="#b28">[29]</ref>, graphical approach 4  [41], multi-scale 5 <ref type="bibr" target="#b14">[15]</ref>, local steering kernel 6 <ref type="bibr" target="#b38">[39]</ref>, distance transform 7 <ref type="bibr" target="#b27">[28]</ref>, self-Information 8 <ref type="bibr" target="#b30">[31]</ref>, weighted contrast 9 <ref type="bibr" target="#b29">[30]</ref> and symmetric contrast 10 [26] based image saliency approaches.</p><p>The experiments conducted to corroborate the performance of the proposed method for eye-gaze correlation in a free viewing task is described in the following sub-section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on eye-gaze prediction task</head><p>In order to empirically evaluate the performance on eye-gaze correlation task, we have employed the receiver operating characteristic (ROC)-area under the curve (AUC) as a benchmarking metric. Several popular and recent works like <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref> employ the ROC-AUC metric to evaluate eye-gaze fixation correlation. An ROC graph is a general technique for visualizing, ranking and selecting classifiers based on their performance <ref type="bibr" target="#b57">[58]</ref>. The ROC graphs are two-dimensional graphs in which the true positive rate (TPR) is plotted on the Y axis and the false positive rate (FPR) rate is plotted on the X axis. The TPR (also  called hit rate and recall) and FPR (also called false alarm rate) metrics are computed as in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>:</p><formula xml:id="formula_5">TPR ¼ tp tp þ fn<label>ð1Þ</label></formula><formula xml:id="formula_6">FPR ¼ fp fp þtn<label>ð2Þ</label></formula><p>where tp is the number of true positives, tn is the number of true negatives, fp is the number of false positives and fn is the number of false negatives. An ROC graph depicts relative trade-offs between benefits (TPR) and costs (FPR). Since the AUC is a portion of the area of the unit square, its value will always be between 0 and 1.0. An ideal classifier would give an AUC of 1.0 while random guessing produces an AUC of less than 0.5 <ref type="bibr" target="#b58">[59]</ref>. The saliency map and the corresponding ground truth fixation density map are binarized at each discrete threshold in <ref type="bibr">[0,</ref><ref type="bibr">255]</ref>. This results in a predicted binary mask (from the saliency map) and a ground truth binary mask (from the fixation density map) for each binarizing threshold. The TPR and FPR for each threshold are subsequently computed. The ROC curve is generated by plotting the obtained FPRs versus TPRs and the AUC is calculated. In our case, the AUC indicates how well the saliency map predicts actual human eye fixations. This measurement also has the desired characteristic of transformation invariance, in that the ROC-AUC does not change when applying any monotonically increasing function (such as logarithm) to the saliency measure <ref type="bibr" target="#b30">[31]</ref>.</p><p>We have considered two popular datasets of York University <ref type="bibr" target="#b12">[13]</ref> and MIT <ref type="bibr" target="#b13">[14]</ref> the eye-gaze prediction task. The York University <ref type="bibr" target="#b12">[13]</ref> dataset contains eye fixation records from 20 subjects for a total of 120 images of size 681 Â 511 pixels. This dataset does not consist of images with human faces, animals or images of activities like sports, music concerts etc. which have semantic meanings attributed to them. The MIT dataset <ref type="bibr" target="#b13">[14]</ref> consists of eye fixation records from 15 subjects for 1003 images of size 1024 Â 768 pixels. Unlike the York University dataset <ref type="bibr" target="#b12">[13]</ref>, the <ref type="bibr" target="#b13">[14]</ref> consists of images which may contain faces, expressions, sporting activities which attract attention and are semantically relevant. The variation in image dimensions, the objects consisted, semantics, topology of arrangement in objects between these two considered eye-gaze datasets offers a test-bed to evaluate the robustness of a saliency map. Also note that the age group of the viewers, the eye-gaze recording equipments, and viewing conditions were not identical while creating these two eye-gaze datasets.</p><p>The performance in terms of ROC-AUC <ref type="foot" target="#foot_2">11</ref> is measured and the resulting plots for the York University dataset <ref type="bibr" target="#b12">[13]</ref> and the MIT dataset <ref type="bibr" target="#b13">[14]</ref> are shown in Figs. <ref type="figure">2</ref> and<ref type="figure">3</ref> respectively. It can be observed from Fig. <ref type="figure">2</ref>, that the proposed method has a similar ROC plot as compared to graph-based <ref type="bibr" target="#b40">[41]</ref> saliency, and clearly outperforms all the other methods considered. The same holds true in case of its performance vis-a-vis the MIT dataset <ref type="bibr" target="#b13">[14]</ref> as it can be seen in Fig. <ref type="figure">3</ref>. To quantitatively evaluate the performance we computed the ROC-AUC which is shown in Table <ref type="table">1</ref>. Note that the proposed method has the highest ROC-AUC performance on the York dataset <ref type="bibr" target="#b12">[13]</ref> and attains a performance equivalent to the graph-based <ref type="bibr" target="#b40">[41]</ref> method on the MIT dataset <ref type="bibr" target="#b13">[14]</ref> as it can be observed in Table <ref type="table">1</ref>. This is despite the fact that the proposed method is simple and is devoid of the sophistication associated with the graph-based <ref type="bibr" target="#b40">[41]</ref>, multi-scale <ref type="bibr" target="#b14">[15]</ref> or self-information <ref type="bibr" target="#b30">[31]</ref> based saliency methods. For the MIT dataset <ref type="bibr" target="#b13">[14]</ref> we have not been able to obtain the ROC plots and area under the curve for the self-information <ref type="bibr" target="#b30">[31]</ref> and random center-surround approaches <ref type="bibr" target="#b29">[30]</ref>, since the original source codes of these methods do not work on images as large as 1024 Â 768 pixels. We preferred not to rescale the input images as this might lead to a bias during the evaluation.</p><p>Fig. <ref type="figure">2</ref>. ROC performance plots for the York University <ref type="bibr" target="#b12">[13]</ref> dataset. Note that the proposed method has similar performance like the graph-based <ref type="bibr" target="#b40">[41]</ref> saliency approach and outperforms all the other methods in consideration. Observe that symmetric contrast <ref type="bibr" target="#b25">[26]</ref> and global contrast <ref type="bibr" target="#b24">[25]</ref> based methods have near linear slopes for their ROC-plots, which indicates their ineffectiveness for the task of eye-gaze prediction.</p><p>Fig. <ref type="figure">3</ref>. ROC performance plots for the MIT <ref type="bibr" target="#b13">[14]</ref> dataset. The proposed method attains a similar performance like the graph-based <ref type="bibr" target="#b40">[41]</ref> saliency approach while outperforming the others. Note that symmetric contrast <ref type="bibr" target="#b25">[26]</ref> and global contrast <ref type="bibr" target="#b24">[25]</ref> based methods have a near linear slope, which was similar to their performance on the York dataset <ref type="bibr" target="#b12">[13]</ref> as shown in Fig. <ref type="figure">2</ref>. Observe that the performance of the proposed method and the graph-based <ref type="bibr" target="#b40">[41]</ref> method does not suffer despite the change in eye fixation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>The performance of the methods under consideration in terms of ROC-AUC on the York University <ref type="bibr" target="#b12">[13]</ref> and MIT <ref type="bibr" target="#b13">[14]</ref> datasets. It can be observed that the proposed method has state-of-the-art performance on both of the eye fixation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saliency map</head><p>York University <ref type="bibr" target="#b12">[13]</ref> MIT <ref type="bibr" target="#b13">[14]</ref> Global contrast <ref type="bibr" target="#b24">[25]</ref> 0.54 0.53 Symmetric contrast <ref type="bibr" target="#b25">[26]</ref> 0.64 0.63 Graph-based <ref type="bibr" target="#b40">[41]</ref> 0.84 0.81 Multi-scale <ref type="bibr" target="#b14">[15]</ref> 0.81 0.76 Entropy-based <ref type="bibr" target="#b28">[29]</ref> 0.83 0.77 Self information <ref type="bibr" target="#b30">[31]</ref> 0.67 NA Local steering kernel <ref type="bibr" target="#b38">[39]</ref> 0.75 0.72 Weighted contrast <ref type="bibr" target="#b29">[30]</ref> 0.75 NA Distance transform <ref type="bibr" target="#b27">[28]</ref> 0.77 0.75 Proposed 0.85 0.81</p><p>The experimental parameter n is correlated with the scale of the image where the standard setting is 0:02 Â r Â c. We therefore evaluated the performance of the proposed method on the York dataset <ref type="bibr" target="#b12">[13]</ref> by varying n and recorded the ROC-AUC values. It can be observed from Fig. <ref type="figure">4</ref> that for 10 random samplings, the proposed method already achieves an ROC-AUC of 0.76 and saturates to an ROC-AUC of 0.85 for 60 random samplings and above. We varied n (where n ¼10, 60, 110, 260 and standard configuration) and obtained ROC plots on the York dataset <ref type="bibr" target="#b12">[13]</ref> which are given in Fig. <ref type="figure">5</ref>. We deliberately included minimal number of ROC plots, as all the ROC plots looked very similar when n 460. Despite this, we recommend to use the standard configuration for n. As the size of the image increases so does the possibility that it consists of more objects. In this context, sampling the image in tandem with its size helps in computing a better saliency map. Our method employs random sampling which means that the selected windows at each trail may be different. Hence we decided to study the stability of the proposed method in terms of ROC-AUC for the standard configuration of n on the York University <ref type="bibr" target="#b12">[13]</ref> dataset. The results shown in Fig. <ref type="figure" target="#fig_3">6</ref> reveal that the variations in ROC-AUC values over 10 different trails are not significant, hence adding to the stability of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments on salient region detection task</head><p>The MSRA dataset <ref type="bibr" target="#b11">[12]</ref> consists of 5000 images annotated by nine users. The annotators were asked to enclose what they thought was the most salient part of the image with a rectangle. Fig. <ref type="figure">7</ref> shows an example of this labeling. Naturally occurring objects do not necessarily have a regular contour which can be enclosed accurately inside a rectangle. As it can be seen from Fig. <ref type="figure">7</ref>, unnecessary background is enclosed by this kind of annotation. It has been recently shown in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b59">60]</ref> that a more precise-to-contour ground truth leads to a more accurate evaluation. Motivated by this a precise-to-contour ground truth was released in <ref type="bibr" target="#b24">[25]</ref>, for a subset of 1000 images from the MSRA dataset <ref type="bibr" target="#b11">[12]</ref>. Thus we consider this subset of 1000 images which have accurate ground truth annotations for our experiments.</p><p>In general, the MSRA dataset <ref type="bibr" target="#b11">[12]</ref> is significantly different from the eye-gaze datasets in terms of test protocol, content and image size. Fundamentally, the eye movements were not recorded and annotators were required to enclose the most visually interesting region of the image. Such an annotation task involves high-level cognitive mechanisms and is not stimulus driven. Thus there is a weak association between the exact segmentation masks and the saliency of the image. Despite involving a high-level visual task, the MSRA dataset <ref type="bibr" target="#b11">[12]</ref> is still relevant to test whether there is an association between the predicted saliency map and the ground truth masks of this dataset. Previous studies <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b56">57]</ref> have shown that the positions of the principal maxima in a saliency map are significantly correlated to the positions of areas that people would choose to put a label indicating a region of interest.</p><p>In order to quantitatively evaluate the performance for the task of detecting salient regions, we followed the method Fig. <ref type="figure">4</ref>. Variations in ROC-AUC of proposed method due to change in n on York University <ref type="bibr" target="#b12">[13]</ref> dataset. Observe that the proposed method has an ROC-AUC value of 0.76 for n¼ 10 which is better than the optimal performance of the local steering kernel <ref type="bibr" target="#b38">[39]</ref> (ROC-AUC¼ 0.75) and self information <ref type="bibr" target="#b30">[31]</ref> (ROC-AUC¼0.67) based approaches as seen from Table <ref type="table">1</ref>. The method saturates in terms of ROC-AUC for a small value of n, and hence rigorous testing to fix this parameter could be avoided. The lack of oscillation in ROC-AUC values versus n shows that the proposed method is stable despite changes in the value of n. Fig. <ref type="figure">5</ref>. Variations in ROC plots of the proposed method due to change in n on the York University <ref type="bibr" target="#b12">[13]</ref> dataset. Observe that for n¼10, the proposed method obtains comparable ROC plot to local steering kernel <ref type="bibr" target="#b38">[39]</ref> and distance transform <ref type="bibr" target="#b27">[28]</ref> as seen from Fig. <ref type="figure">2</ref>. With n¼ 60 the performance of the proposed method starts saturating and this helps in processing large images without compromising on run-time. recommended in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>, where the saliency map is binarized and compared with the ground truth mask. The saliency map is thresholded within [0, 255] to obtain a binary mask and is further compared with the ground truth. The thresholds are varied from 0 to 255 and recall-precision metrics are computed at each binarizing threshold. The recall-precision metric is found to be more suitable to evaluate the salient region detection performance than the ROC-AUC metric <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>. The recall (also called TPR or hit-rate) and precision metrics are computed as in <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_7">recall ¼ tp tp þ fn<label>ð3Þ</label></formula><formula xml:id="formula_8">precision ¼ tp tp þfp<label>ð4Þ</label></formula><p>The resulting recall versus precision curve is shown in Fig. <ref type="figure">8</ref>. This curve provides a reliable comparison of how well various saliency maps highlight salient regions in images. It can be observed from Fig. <ref type="figure">8</ref>, that the proposed method has comparable performance with the graph-based <ref type="bibr" target="#b40">[41]</ref> method and outperforms all the other approaches in consideration with an exception of the weighted contrast <ref type="bibr" target="#b29">[30]</ref> based method. However, it could be noted from our previous experiments on eye fixation datasets that the weighted contrast <ref type="bibr" target="#b29">[30]</ref> based method fails to work when the size of the input is large and has far below performance in terms of ROC-AUC as compared to the graph-based <ref type="bibr" target="#b40">[41]</ref> approach and the proposed method. Despite being simple, the proposed method fares well in both the tasks of eye-gaze correlation as well as salient region detection and has as an equivalent performance to the graph-based <ref type="bibr" target="#b40">[41]</ref> approach.</p><p>A common threshold to binarize different saliency maps might not be suitable, as the binarization threshold depends on the image specific statistics. Thus we employ Tsai's moment preserving algorithm <ref type="foot" target="#foot_3">12</ref>  <ref type="bibr" target="#b61">[62]</ref> which has also been recommended in <ref type="bibr" target="#b27">[28]</ref> to select the map specific binarization threshold. We further evaluate the accuracy of the obtained binary masks to the ground truth masks by employing F-measure as suggested in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. The formula for F-measure is given in Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_9">F-measure ¼ 2 Á precision Á recall precision þ recall<label>ð5Þ</label></formula><p>It can be observed from Table <ref type="table">2</ref> that the proposed method and the graph-based method have an identical F-measure performance of 0.64. The weighted contrast-based <ref type="bibr" target="#b29">[30]</ref> saliency attains the highest F-measure of 0.71, but does not perform well in the eye-gaze correlation task. The entropy-based <ref type="bibr" target="#b28">[29]</ref> method has the lowest F-measure performance of 0.43 despite achieving a good performance on the eye-gaze task. Apart from the proposed method and graph-based <ref type="bibr" target="#b40">[41]</ref> saliency, only the multi-scale approach <ref type="bibr" target="#b14">[15]</ref> with a F-measure of 0.58 is seen to perform equally well on eye-gaze detection task.</p><p>We demonstrate the performance of the various approaches considered on the task of salient region detection by considering three sample images from the MSRA <ref type="bibr" target="#b11">[12]</ref> dataset as shown in Fig. <ref type="figure" target="#fig_4">9</ref>. For the purpose of illustration we overlaid the original image with ground truth masks and the other masks obtained after binarizing the respective saliency maps by Tsai's moment preserving algorithm <ref type="bibr" target="#b61">[62]</ref>. It can be observed from Fig. <ref type="figure" target="#fig_4">9</ref>, that the entropy-based <ref type="bibr" target="#b28">[29]</ref> saliency approach fails to effectively localize the salient object in the image. In case of the image consisting of the yellow flower, the entire image is shown as salient; while the salient region of the glowing bulb is treated as irrelevant. Despite being efficient the global contrast <ref type="bibr" target="#b24">[25]</ref> and symmetric contrast <ref type="bibr" target="#b25">[26]</ref> based methods ignore the finer details of the salient region, as both the methods operate on mean pixel intensity value of the image. Conversely, the multi-scale <ref type="bibr" target="#b14">[15]</ref> and local steering kernel <ref type="bibr" target="#b38">[39]</ref> based saliencies highlight the minute details but suppress Fig. <ref type="figure">7</ref>. Rectangular and exact segmentation masks. To the left is the sample image from the MSRA dataset <ref type="bibr" target="#b11">[12]</ref>, the image in the center shows the original rectangular annotation and at right is the accurate-to-countour annotation. Observe the reduction in background information between rectangular and exact annotations. Fig. <ref type="figure">8</ref>. Recall-precision plot on the MSRA <ref type="bibr" target="#b11">[12]</ref> dataset. Observe that the proposed method has a superior recall-precision plot as compared to all the methods except for the graph-based <ref type="bibr" target="#b40">[41]</ref> and weighted contrast <ref type="bibr" target="#b29">[30]</ref> based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The recall,precision and F-measure performance of saliency maps from the various methods under consideration. Observe that the proposed method has an equivalent performance to the graph-based <ref type="bibr" target="#b40">[41]</ref>  salient regions as they have a bias towards strong gradients. The self information <ref type="bibr" target="#b30">[31]</ref> based approach highlights rare segments of the image as salient, and consequently it discards regions of uniform appearance which can be observed from column nine of Fig. <ref type="figure" target="#fig_4">9</ref>. Only the weighted contrast <ref type="bibr" target="#b29">[30]</ref> based method has a near perfect performance on detecting the salient regions. But the previous experiments on eye-gaze prediction have shown that the weighted contrast <ref type="bibr" target="#b29">[30]</ref> based approach is not highly effective. The proposed method highlights the salient regions along with the finer details effectively which shows that it is not biased towards edges like the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Computational run time</head><p>We evaluated the run-time of the proposed saliency approach with reference to the other methods in consideration. The runtime of the various methods were benchmarked on three different scales of a color image as shown in Table <ref type="table">3</ref>. The original plugins of global contrast <ref type="bibr" target="#b24">[25]</ref>, symmetric contrast <ref type="bibr" target="#b25">[26]</ref>, weighted contrast <ref type="bibr" target="#b29">[30]</ref>, entropy-based <ref type="bibr" target="#b28">[29]</ref> and self information-based <ref type="bibr" target="#b30">[31]</ref> saliency approaches are pure Matlab codes. While the codes pertaining to local steering kernel <ref type="bibr" target="#b38">[39]</ref>, multi-scale <ref type="bibr" target="#b14">[15]</ref> and graph-based methods <ref type="bibr" target="#b40">[41]</ref> are quasi Matlab codes which call Cþþ functions for run-time optimization. The original plugin for distance transform <ref type="bibr" target="#b27">[28]</ref> is a binary executable while its original coding language is unknown. The proposed method is programmed in Matlab. An absolute comparison on the basis of run-time might penalize the methods which are coded in Matlab script as they are relatively slower than their Cþþ counterparts. Nevertheless, it gives a relative overview of run-time performance of the all methods under consideration. . Qualitative analysis of results for MSRA <ref type="bibr" target="#b11">[12]</ref> dataset. The first column contains the original image. The remaining columns contain the images overlaid with thresholded (using Tsai's algorithm <ref type="bibr" target="#b61">[62]</ref>) saliency maps obtained from the respective methods mentioned in the figure header. Observe that the proposed method highlights the salient regions along with its finer details effectively, which most of the other methods in consideration fails to achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The computational run-time(in s) of various saliency methods under consideration. Run-times were computed using Matlab v7.10.0 (R2010a), on an Intel Core 2 Duo processor with Ubuntu 10.04.1 LTS (Lucid Lynx) as operating system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saliency map</head><p>Code type Run-time(in s) w.r. It can be observed from Table <ref type="table">3</ref> that the run-time of the local steering kernel <ref type="bibr" target="#b38">[39]</ref>, the graph-based <ref type="bibr" target="#b40">[41]</ref> and the multi-scalebased <ref type="bibr" target="#b14">[15]</ref> saliency approaches do not change significantly irrespective of the size of the input image. This is on account that the input image is rescaled to pre-specified dimension mentioned in their source codes. The rest of the methods process the input image in its original scale and hence the run-time changes with the input image dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and conclusion</head><p>We have compared and contrasted nine existing state-of-the-art approaches to compute a saliency map along with the proposed method for the two different tasks of eye-gaze correlation and salient region detection. Experiments were carried out on large publicly available datasets of York University <ref type="bibr" target="#b12">[13]</ref>, MIT <ref type="bibr" target="#b13">[14]</ref> for eyegaze correlation and the MSRA dataset <ref type="bibr" target="#b11">[12]</ref> for salient region detection. Our results have shown that the proposed method attains state-of-the-art performance on the task of eye-gaze correlation and also has a comparable performance with the other methods for the task of salient region detection. Most of the existing approaches to compute a saliency map fail to perform equally well on both of the aforementioned tasks. But our method along with the graph-based <ref type="bibr" target="#b40">[41]</ref> approach has a reliable performance on both these diverse tasks. It should be observed that the proposed method achieves this performance despite being simple as compared to graph-based <ref type="bibr" target="#b40">[41]</ref>, local steering kernel-based <ref type="bibr" target="#b38">[39]</ref> and other existing sophisticated methods. It has also been shown in the experiments that a very small number of random rectangles (n¼60) is adequate to attain good results. Despite this we recommend to tune the sampling rate in direct proportion to the size of the image, as generating large number of arbitrary sized image windows often leads to a higher probability of completely enclosing a salient region or an object. In spite of its simplicity and elegance the proposed method fails to capture saliency when color contrast is either low or not being the principal influence. In future, we plan to address this bottleneck and further improve our method to capture higher level semantics like detecting pop-outs in terms of orientation, shape and proximities. Exploiting the ability of the proposed method to detect a salient region as a pre-processor for object detection is also envisaged.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/pr Pattern Recognition 0031-3203/$ -see front matter &amp; 2012 Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2012.02.009</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) n-Number of random windows (2) r-Number of rows (3) c-Number of columns Output : x 1 ,y 1 ,x 2 ,y 2 each of size n Method Step 1 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of the proposed method. The input image is subjected to Gaussian filter in the first stage. Subsequently it is converted into the L n a n b n space and the individual L n , a n and b n channels are obtained. For the sake of simplicity we have considered three random regions of interest (ROI) on the respective L n , a n and b n channels. Local saliencies are computed over each of these ROIs and the channel specific saliency maps (S L , S a and S b ) are updated. The final saliency map is then computed by fusing the channel specific saliency maps by a pixel-wise Euclidean norm.</figDesc><graphic coords="5,47.54,60.95,484.63,196.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. ROC-AUC performance of the proposed method for different trails on the York University [13] dataset. Observe that the ROC-AUC values do not change significantly during 10 different trails. The mean ROC-AUC is 0.8554 and the standard deviation (s) is 9:3743eÀ04. The low standard deviation indicates the stability of the proposed method.</figDesc><graphic coords="7,308.45,58.64,237.60,187.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9</head><label>9</label><figDesc>Fig.9. Qualitative analysis of results for MSRA<ref type="bibr" target="#b11">[12]</ref> dataset. The first column contains the original image. The remaining columns contain the images overlaid with thresholded (using Tsai's algorithm<ref type="bibr" target="#b61">[62]</ref>) saliency maps obtained from the respective methods mentioned in the figure header. Observe that the proposed method highlights the salient regions along with its finer details effectively, which most of the other methods in consideration fails to achieve.</figDesc><graphic coords="9,77.97,58.64,429.84,389.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>I (Gray scale Image) of size r Â c (2) n-Number of random windows (3) Co-ordinate vectors x 1 ,y 1 ,x 2 ,y 2 each of size n</figDesc><table><row><cell>Step 6 :</cell><cell>Apply median filter on S</cell></row><row><cell>Step 7 :</cell><cell>Normalize S in [0,255]</cell></row><row><cell>Step 8 :</cell><cell>Apply histogram equalization to S</cell></row><row><cell cols="2">Algorithm: CompSal</cell></row><row><cell cols="2">Input : (1) Output : ISM-Interim saliency map of size r Â c</cell></row><row><cell>Method</cell><cell></cell></row><row><cell>Step 1 :</cell><cell>Set all elements of ISM to 0</cell></row><row><cell>Step 2 :</cell><cell></cell></row></table><note><p>5 : Compute S by pixel-wise Euclidean norm S ¼ Fusion ðS L ,S a ,S b Þ</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>method.</figDesc><table><row><cell>Saliency map</cell><cell>Recall</cell><cell>Precision</cell><cell>FÀmeasure</cell></row><row><cell>Global contrast [25]</cell><cell>0.49</cell><cell>0.63</cell><cell>0.51</cell></row><row><cell>Symmetric contrast [26]</cell><cell>0.40</cell><cell>0.66</cell><cell>0.47</cell></row><row><cell>Graph-based [41]</cell><cell>0.73</cell><cell>0.62</cell><cell>0.64</cell></row><row><cell>Multi-scale [15]</cell><cell>0.67</cell><cell>0.56</cell><cell>0.58</cell></row><row><cell>Entropy-based [29]</cell><cell>0.98</cell><cell>0.29</cell><cell>0.43</cell></row><row><cell>Self information [31]</cell><cell>0.55</cell><cell>0.44</cell><cell>0.46</cell></row><row><cell>Local steering kernel [39]</cell><cell>0.48</cell><cell>0.50</cell><cell>0.46</cell></row><row><cell>Weighted contrast [30]</cell><cell>0.84</cell><cell>0.66</cell><cell>0.71</cell></row><row><cell>Distance transform [28]</cell><cell>0.85</cell><cell>0.33</cell><cell>0.45</cell></row><row><cell>Proposed</cell><cell>0.71</cell><cell>0.63</cell><cell>0.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>t. image size 205 Â 103 308 Â 195 410 Â 259</figDesc><table><row><cell>Global contrast [25]</cell><cell>Matlab</cell><cell>0.0652</cell><cell>0.0898</cell><cell>0.1539</cell></row><row><cell cols="2">Symmetric contrast [26] Matlab</cell><cell>0.0976</cell><cell>0.1172</cell><cell>0.2050</cell></row><row><cell>Graph-based [41]</cell><cell cols="2">Matlab with Cþþ 0.6256</cell><cell>0.4788</cell><cell>0.5577</cell></row><row><cell>Multi-scale [15]</cell><cell cols="2">Matlab with Cþþ 0.4388</cell><cell>0.3820</cell><cell>0.3661</cell></row><row><cell>Entropy-based [29]</cell><cell>Matlab</cell><cell>2.1448</cell><cell>5.0761</cell><cell>10.3915</cell></row><row><cell>Self information [31]</cell><cell>Matlab</cell><cell>1.6466</cell><cell>4.0714</cell><cell>7.6242</cell></row><row><cell cols="2">Local steering kernel [39] Matlab</cell><cell>3.0590</cell><cell>3.1187</cell><cell>3.1133</cell></row><row><cell>Weighted contrast [30]</cell><cell>Matlab</cell><cell>0.2701</cell><cell>0.5939</cell><cell>1.2038</cell></row><row><cell cols="2">Distance transform [28] Binary</cell><cell>0.1266</cell><cell>0.2806</cell><cell>0.5308</cell></row><row><cell>Proposed</cell><cell>Matlab</cell><cell>0.3430</cell><cell>0.5422</cell><cell>1.0766</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>T.N. Vikram et al. / Pattern Recognition 45 (2012) 3114-3124</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The original CIE document-http://www.electropedia.org/iev/iev.nsf/display? openform&amp;ievref=845-03-56. T.N. Vikram et al. / Pattern Recognition 45 (2012) 3114-3124</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_2"><p>Source code used for ROC-AUC computation in this article-http://mark. goadrich.com/programs/AUC/auc.jar.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_3"><p>http://www.cs.tut.fi/ $ ant/histthresh/HistThresh.zip</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Tadmeri Narayan Vikram would like to thank his colleagues Miranda Grahl, Torben T öniges, Christian Schnier and Subhashree Vikram who proof read this article at various stages.</p><p>This work has been supported by the RobotDoC Marie Curie Initial Training Network funded by the European Commission under the 7th Framework Programme (Contract no. 235065).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: towards the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning saliency maps for object categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV International Workshop on the Representation and Use of Prior Knowledge in Vision</title>
		<meeting>the ECCV International Workshop on the Representation and Use of Prior Knowledge in Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object of interest detection by saliency learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khuwuthyakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robles-Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="636" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining attention and recognition for rapid scene analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bonaiuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition-Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition-Workshops</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context saliency based image summarization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="270" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Saliency driven total variation segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Salient region detection and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wils</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Üsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Systems</title>
		<meeting>the International Conference on Computer Vision Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online learning of taskdriven object-based visual attention control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Ahmadabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Araabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1130" to="1145" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From bottom-up visual attention to robot action learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nagai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Development and Learning</title>
		<meeting>the IEEE International Conference on Development and Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Space and attention in parietal cortex</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Colby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Review of Neuroscience</title>
		<meeting>the Annual Review of Neuroscience</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="319" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Features that draw visual attention: an information theoretic perspective</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="page" from="125" to="133" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE International Conference on Computer Vision</title>
		<meeting>eeding of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual attention for robotic cognition: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Begum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computational visual attention systems and their cognitive foundations: a survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Erich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention in hierarchical models of object recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Brain Research</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="57" to="78" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jia</surname></persName>
		</author>
		<title level="m">Saliency detection based on 2d log-gabor wavelets and center bias</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="979" to="982" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference on Multimedia</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency detection: a spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal spectral residual: fast motion saliency detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="617" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Use of power law models in detecting region of interest</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2521" to="2529" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Significance of the Weibull distribution and its sub-models in natural image statistics</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Theory and Applications</title>
		<meeting>the International Conference on Computer Vision Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Üsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Saliency detection using maximum symmetric surround</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Üsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2653" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting eye fixations on complex visual stimuli using local symmetry</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kootstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Boer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schomaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="223" to="240" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple method for detecting salient regions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2363" to="2371" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A random center surround bottom up visual attention model useful for salient region detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tscherepanow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wrede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Applications of Computer Vision</title>
		<meeting>the IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="166" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sun: a Bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2368" to="2375" />
		</imprint>
	</monogr>
	<note>Measuring visual saliency by site entropy rate</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A computational model for saliency maps by using local entropy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="967" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: searching for coding length increments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Incremental sparse saliency detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE International Conference on Image Processing</title>
		<meeting>eeding of the IEEE International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3093" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The discriminant center-surround hypothesis for bottom-up saliency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Esaliency (extended saliency): meaningful attention using stochastic image modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="693" to="708" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Saliency based on multi-scale ratio of dissimilarity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by selfresemblance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Saliency detection: a self-ordinal resemblance approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1260" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A nonparametric approach to bottom-up visual saliency</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Franz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="689" to="696" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nonparametric saliency detection using kernel density estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hebbian-based neural networks for bottom-up visual attention systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing</title>
		<meeting>the International Conference on Neural Information Processing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selecting salient objects in real scenes: an oscillatory correlation model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Quiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A F</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="54" to="64" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image saliency by isocentric curvedness and color</title>
		<author>
			<persName><forename type="first">R</forename><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="2185" to="2192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Salient region detection with opponent color boosting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Workshop on Visual Information Processing</title>
		<meeting>the European Workshop on Visual Information Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Salient region detection by modeling distributions of color and orientation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="892" to="905" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A coherent computational approach to model bottom-up visual attention</title>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thoreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="802" to="817" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<title level="m">What is an object? in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning receptor positions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ahumada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Models of Visual Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stimulus bias, asymmetric similarity, and classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="94" to="140" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An integrative, experience-based theory of attentional control</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Voluntary attention enhances contrast appearance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Taosheng Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carrasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="354" to="362" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Perceptual enhancement of contrast by attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Treue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="435" to="437" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention links sensing to recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Rothenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Relevance of a feed-forward model of visual attention for goal-oriented and free-viewing tasks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Chevet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2801" to="2813" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Face saliency in various human visual saliency models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hardeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Image and Signal Processing and Analysis</title>
		<meeting>International Symposium on Image and Signal Processing and Analysis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="327" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and ROC curves</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A two-stage approach to saliency detection in images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="965" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Interesting objects are visually salient</title>
		<author>
			<persName><forename type="first">L</forename><surname>Elazary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Moment-preserving thresholding: a new approach</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="377" to="393" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Afterwards, he joined the Applied Informatics Group at Bielefeld University, Germany. In December 2007, he finished his Ph.D. thesis entitled &apos;&apos;Image Analysis Methods for Location Proteomics&apos;&apos;. His current research interests include incremental on-line learning, neural networks, evolutionary optimization, feature selection, and the application of such methods in cognitive robotics. Britta Wrede received her masters degree in computational linguistics and the Ph.D. degree (Dr.-Ing.) in computer science from Bielefeld University in 1999 and 2002 respectively. From 1999 to 2002 she pursued a Ph.D. program at Bielefeld University in joint affiliation with the Applied Informatics Group and the graduate program taskoriented communication</title>
	</analytic>
	<monogr>
		<title level="m">GREYC, University of Caen, France between</title>
		<meeting><address><addrLine>Germany; Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2006. 2008. 2010. 2010. 2003</date>
		</imprint>
		<respStmt>
			<orgName>Tadmeri Narayan Vikram has received Masters of Computer Applications from University of Mysore, India ; Applied Informatics Group in Bielefeld University ; Ilmenau Technical University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include computational modeling of visual attention and its applications in developmental robotics. Marko Tscherepanow received his diploma degree in computer science from. After her Ph.D. she received a DAAD PostDoc fellowship at the speech group of the International Computer Science Institute (ICSI) at Berkeley, USA. Currently she heads the Applied Informatics Group at Bielefeld University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
