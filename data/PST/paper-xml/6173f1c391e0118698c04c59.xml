<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simplifying Multimodal Emotion Recognition with Single Eye Movement Modality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xu</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Li-Ming</forename><surname>Zhao</surname></persName>
							<email>lm_zhao@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
							<email>bllu@sjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simplifying Multimodal Emotion Recognition with Single Eye Movement Modality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475701</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multimodal emotion recognition</term>
					<term>generative adversarial networks</term>
					<term>eye movements</term>
					<term>EEG</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal emotion recognition has long been a popular topic in affective computing since it significantly enhances the performance compared with that of a single modality. Among all, the combination of electroencephalography (EEG) and eye movement signals is one of the most attractive practices due to their complementarity and objectivity. However, the high cost and inconvenience of EEG signal acquisition severely hamper the popularization of multimodal emotion recognition in practical scenarios, while eye movement signals are much easier to acquire. To increase the feasibility and the generalization ability of emotion decoding without compromising the performance, we propose a generative adversarial network-based framework. In our model, a single modality of eye movements is used as input and it is capable of mapping the information onto multimodal features. Experimental results on SEED series datasets with different emotion categories demonstrate that our model with multimodal features generated by the single eye movement modality maintains competitive accuracies compared to those with multimodality input and drastically outperforms those single-modal emotion classifiers. This illustrates that the model has the potential to reduce the dependence on multimodalities without sacrificing performance which makes emotion recognition more applicable and practicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Human-centered computing → HCI design and evaluation methods; • Computing methodologies → Artificial intelligence; Philosophical/theoretical foundations of artificial intelligence; Cognitive science.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Emotions penetrate our life everywhere and every day and play an important role in the way we think and behave. Accordingly, the emotion intelligence is gradually attracting more attention, especially with the prospects of deep learning. It can be divided into three stages: emotion recognition, emotion understanding, and emotion regulation, offering enormous potential to be used in broad scenarios such as medical diagnosis and treatment, interpersonal relationship improvement, and user experience optimization of general artificial intelligence applications. As the primary step and a salient milestone <ref type="bibr" target="#b3">[4]</ref>, emotion recognition maintains wide popularity among researchers. Many studies have attempted to find effective modalities to measure emotions, taking facial expression <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref>, eye movements <ref type="bibr" target="#b21">[22]</ref>, EEG signals <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>, and speech <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> as examples. However, the performances of those individual modalities remain at inadequate levels that cannot be generalized because emotions are complex psychophysiological processes associated with both internal and external activities.</p><p>These unsatisfactory findings urge scholars to explore new ways to model the characteristics of emotions. Inspired by the different aspects of information provided by different modalities, some pioneers started to integrate multiple modalities to determine whether the complementary information can help recognize emotions better, and many works indeed have shown that multimodal fusion emotion recognition methods would achieve better performances than those based on a single modality. Among all groups, the combination of EEG signals, reflecting internal physiological responses, and eye movements, representing external subconscious behaviors, has been proven to be a promising approach with high interpretability <ref type="bibr" target="#b21">[22]</ref>. Zheng et al. first adopted a multimodal emotion recognition framework by combining these two modalities in three-class emotion recognition (happy, sad and neutral). The impressive experimental results show that the feature-level fusion strategy works well <ref type="bibr" target="#b25">[26]</ref>. Liu et al. dramatically advanced the state-of-the-art performance of this task by extracting high-level fusion features with a deep neural network model called bimodal deep autoencoder <ref type="bibr" target="#b14">[15]</ref>.</p><p>Although multimodal fusion can achieve better results in emotion recognition, involving more modalities means that there are more possible restrictions in real applications. For example, the process of collecting EEG signals is very complicated. In addition to several inevitable preparations, such as wearing electrode caps and injecting conductive gel, we have to guarantee that the acquisition environment is quiet and without disturbance, since the signals are very subtle and sensitive to interference, thus impeding their use in practical scenarios. Comparatively, other physiological signals are much easier to collect. For instance, eye movement signals can be gathered with small pieces of glasses <ref type="bibr" target="#b15">[16]</ref>. Nevertheless, considering the irreplaceability of EEG among all modalities, it is urgent to figure out how to make use of the EEG modality without being constrained by its limitations.</p><p>Two solutions stand out from other methods. One is cross-modal transfer using signals from one modality as an input and predicting when being given the other modality <ref type="bibr" target="#b19">[20]</ref>, and the other is multimodal feature generation based on a single modality. Researchers in the computer vision field innovatively proposed the idea of matching the image features with trained EEG features and therefore use the EEG-based classifier to automatically categorize objects <ref type="bibr" target="#b23">[24]</ref>. Jiang et al. <ref type="bibr" target="#b10">[11]</ref> was the first to apply their ideas in the emotion recognition task. Deep regression neural networks are used to find the regressive connection between the bimodal high-level representations and eye movement features. However, the relationship might not be linear, and this model does not fit the characteristics of the modalities used. Besides, they only examined on one dataset, which is insufficient to judge the performance.</p><p>Based on these previous attempts, we decide to map the features from a single modality into high-dimensional multimodal features. In this way, signals from multimodalities are not required as input, while the multimodal knowledge has been encoded into the model in the training stage and assists in emotion recognition. To extract the relationship between eye movement features and multimodal features, we adopt the bimodal deep autoencoder <ref type="bibr" target="#b17">[18]</ref> to get this information. Distinguished from Jiang et al. <ref type="bibr" target="#b10">[11]</ref>, using eye movement features as control conditions, a compact yet effective model originating from the conditional generative adversarial networks (CGANs) can generate the corresponding multimodal features by adversarial learning for each emotion class. We conduct experiments on three SEED series datasets with different numbers of emotion classes <ref type="bibr">(3, 4, and 5)</ref> to examine the generalization ability of our model. Extensive experimental results demonstrate that the generative representations achieve competitive performance compared with those using multimodal input, leaving room for reducing modality dependence which makes the technique more practicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multiple physiological modalities have been utilized by researchers to classify emotions. In the literature, there are a surprising number of studies using eye movement signals to perform this task because it not only can observe the users' states naturally and efficiently <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref> but also is easy to wear. Lu's comprehensive experimental results prove that pupil diameter, dispersion, fixation duration, saccade duration, saccade amplitude, and nine event statistics are distinguishable for emotions, which could be used as efficient features for emotion recognition <ref type="bibr" target="#b15">[16]</ref>. However, even the recognition accuracy provided by the state-of-the-art model is not ideal enough to be used in applications. Meanwhile, another group of researchers focusing on EEG signals were surprised by their potentials in emotion recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>. Alarcao et al. <ref type="bibr" target="#b0">[1]</ref> conducted a detailed survey about EEG-based emotion recognition, including stimuli, feature extraction, and classifiers. Considering that emotions are complex psycho-physiological phenomena in nature, scientists turn to build more robust emotion recognition models based on multimodalities that may contain complementary information. The combination of eye movements and EEG has been gaining much attention as a good representative for external behavior activities and internal physiological changes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. Zheng et al. <ref type="bibr" target="#b25">[26]</ref> innovatively combined them and examined on both feature-level fusion and decision-level fusion. The improvement on performances of this groundbreaking work inspired Lu et al. <ref type="bibr" target="#b15">[16]</ref> to testify their relationships and utilize the advantages of it with various modality fusion strategies in emotion recognition. Besides, Liu et al. attempted to use multimodal deep learning techniques to model this task <ref type="bibr" target="#b14">[15]</ref>. These studies have suggested that modality fusion seems to be a reliable approach since it significantly enhances the performance compared with a single modality.</p><p>Although this complementary collocation achieves satisfying performance, it is unfeasible to put it into large-scale applications due to the inconvenience of data acquisition and inter-subject variability of EEG signals. To overcome this issue, cross-modal transfer learning has a growing body of literature that can be categorized into two threads. One is figuring out the relationship between EEG and another modality, conducting a one-to-one mapping to avoid using EEG in the test stage. The other is to perform joint learning on multimodalities and data generation in training and then use one single modality for testing. Both ways have been supported with successful cases from natural language processing, computer vision, etc. For example, Scott et al. <ref type="bibr" target="#b19">[20]</ref> developed a novel deep GANs architecture to effectively bridge the semantic relationship between text and graphs through transforming visual concepts from characters to pixels. Palazzo et al. <ref type="bibr" target="#b18">[19]</ref> attempted to generate corresponding images from EEG signals by combining an LSTM recurrent neural network with conditional GANs. At the same time, Spampinato et al. <ref type="bibr" target="#b23">[24]</ref> conducted an RNN-based method to learn visual stimuli-evoked EEG data and used a CNN-based approach to regress images into the learned EEG representation, thus enabling automated visual classification in a brain-based visual object manifold. Jiang et al. <ref type="bibr" target="#b10">[11]</ref> pioneeringly tested those methods in the emotion recognition field but in a regressive way, which is not suitable for EEG and leaves room for others to explore more methods. Besides, to the best of our knowledge, there is limited work to address the problems of one-to-more cross-modal transfer in the field of emotion recognition. In the following sections, we will present our proposed method to tackle the fore-mentioned problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Our goal is to increase the practicability of the system by utilizing features from a single modality to obtain predictions as accurate as those produced using multimodalities with the help of synthetic multimodal high-dimensional features. Three categories of features are involved in the whole process, including single-modal features from eye movements, real multimodal features from both EEG and eye movements, and generated multimodal features. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the training process can be divided into two phases. Training stage I focuses on multimodal feature fusion to extract real multimodal features with a bimodal deep autoencoder. In training stage II, we adopt conditional generative adversarial networks to generate synthetic multimodal features with single eye movement features. With respect to the test stage, only the eye movement modality is needed as input. The whole process is illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Stage I: Multimodal Feature Extraction</head><p>The quality of real multimodal features is of great importance in the whole process. Multimodal feature generation is fundamentally a question of high-dimensional data generation. In general, the features can be engineered by extracting high-dimensional emotion information from eye movement signals and corresponding EEG signals. Specifically, for each sampling point 𝑖, we align the eye movement signal 𝑿 𝐸𝑌 𝐸𝑖 ∈ R 𝑛 , where 𝑛 represents the dimension of the eye movement feature, with the corresponding EEG signal 𝑿 𝐸𝐸𝐺 𝑖 ∈ R 𝑚 , where 𝑚 represents the dimension of the EEG feature. We choose one of the classic modality fusion methods to extract the high-dimensional multimodal emotion representations from both EEG and eye movement features, namely the bimodal deep autoencoder <ref type="bibr" target="#b17">[18]</ref> shown in training stage I in Figure <ref type="figure" target="#fig_0">1</ref> marked with the red lines. There are two steps in the procedure of the bimodal deep autoencoder. The first is encoding, containing two encoders, 𝑬 𝐸𝐸𝐺 for encoding EEG features and 𝑬 𝐸𝑌 𝐸 for eye movements. The nature of the encoder is the restricted Boltzmann machine (RBM) <ref type="bibr" target="#b20">[21]</ref>. We then train an RBM over the pretrained layers for each modality to model the relationships between them as high-level multimodal representations (𝑟 ). The second step is decoding. As a process symmetrical to encoding, this step reconstructs the original input representations with 𝑫 𝐸𝐸𝐺 and 𝑫 𝐸𝑌 𝐸 .</p><p>For the emotion classifier, we apply a multilayer perceptron (MLP) as the classifier 𝑪, which is a feed-forward network. The classifier is trained in the training stage and takes different kinds of features as input. We train the model by minimizing the following loss:</p><formula xml:id="formula_0">L = L recon + L c (1)</formula><p>where the reconstruction loss L recon is calculated by the mean squared error:</p><formula xml:id="formula_1">L recon = 1 𝑘 𝑿 − 𝑿 ′ 2 2 , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where 𝑘 is the number of features and ∥•∥ 2 2 is the squared 𝐿 2 -norm. For the cross-entropy loss L 𝑐 of the emotion classifier 𝑪, the related formula is as follows:</p><formula xml:id="formula_3">L c = − 𝑖 𝑦 𝑖 log ŷ𝑖<label>(3)</label></formula><p>where 𝑦 𝑖 is the ground truth emotion label for input 𝑥 𝑖 .</p><p>Although there are many multimodal encoders to use, our intuition to choose the bimodal deep autoencoder is that the success of the high-dimensional feature decoding process demonstrates that the extracted representations are of high quality and mutually separable in feature space and are qualified to be used to construct a preferable emotion recognition system. Note that in this stage, features from multimodalities are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Stage II: Multimodal Feature Generation</head><p>Inspired by the broad usage and good performance of generative adversarial networks (GANs) in data augmentation, we select GANs as our basic framework for multimodal feature generation. In the standard GAN structure, a generator 𝑮 is constructed to produce realistic-like data distribution 𝑝 𝐺 with mapping function 𝑮 (𝑧), where 𝑧 is noise sampled from a noise distribution 𝑝 𝑧 (𝑧). Besides, a discriminator 𝑫 is built to play a minimax game against 𝑮 by distinguishing whether the given sample is in real distribution 𝑝 𝑟 or generated distribution 𝑝 𝐺 . The generator-discriminator minimax game for continuous data can be formulated as follows: min</p><formula xml:id="formula_4">𝐺 max 𝐷 L (𝑫, 𝑮) = E 𝑟 ∼𝑝 𝑑 (𝑟 ) [𝑙𝑜𝑔(𝑫 (𝑟 ))]+ E 𝑧∼𝑝 𝑧 (𝑧) [𝑙𝑜𝑔(1 − 𝑫 (𝑮 (𝑧)))]<label>(4)</label></formula><p>We can see that the standard GAN framework requires the generated data to be differentiable so that the gradient can backpropagate from 𝑫 to 𝑮 to update the parameters. This constraint impedes the application of standard GANs to continuous data, i.e., multimodal data. In multimodal data generation tasks, different multimodal data are often associated with the same category, which represents a discrete many-to-one mapping. This problem can be solved by the conditional probabilistic generative model, where the input is combined with a conditioning variable and generates a conditional predictive distribution. The structure of conditional generative adversarial networks (CGANs) <ref type="bibr" target="#b16">[17]</ref> is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. The eye movement features are taken as conditions to guide the generator to produce the corresponding multimodal features from noise. The discrimination results of 𝑫 are used to construct the predefined loss function to guide the training of 𝑮 to guarantee that 𝑮 can produce realistic and eligible multimodal features. More details of our model are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Generator.</head><p>We employ a fully connected deep neural network (FC-DNN) as our generative model. The FC-DNN generator is composed of multiple fully connected regression layers with 𝑡𝑎𝑛ℎ or 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 activation functions. Other variants of DNN may also be used as the generative model, but a typical FC-DNN can efficiently map the input embedding representation to high-dimensional hidden features, i.e., multimodal emotion features. It is notable that the existing vanishing gradient problem <ref type="bibr" target="#b8">[9]</ref> deteriorates the training performance of the generator. This means that during the initial training stage or when 𝑫 is well learned, 𝑫 can always reject the generated multimodal features with high confidence so that the gradient guiding 𝑮 training will approach zero, and the updates of the generator nearly stop. To solve this problem, we improve the FC-DNN model by replacing the 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 activation function with the 𝐿𝑒𝑎𝑘𝑦 − 𝑅𝑒𝐿𝑈 activation function for the head layers.</p><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, to impose the conditional constraint, we feed the eye movement features 𝑿 𝐸𝑌 𝐸 into our FC-DNN generator after concatenation with input noise. The motivation behind this operation is to help 𝑮 generate multimodal features conditioned based on emotional states. Note that the explicit emotion category is unknown during the real-time emotion classification phase. However, eye movement features capture the emotion information and can be treated as an encoded emotion class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Discriminator.</head><p>The discriminator needs to be a determination function 𝑫 (𝑥) to generate a single scalar to represent the probability, where 𝑥 comes from 𝑝 𝑟 rather than 𝑝 𝐺 . We also choose FC-DNN as our discriminator. The concatenation of eye movement features constraint 𝑿 𝐸𝑌 𝐸 with multimodal features 𝑟 or 𝑟 𝐺 as the input is then fed into the discriminator to output the probability that the multimodal feature comes from the real distribution.</p><p>Given the generator 𝑮 and discriminator 𝑫 with the additional conditional constraint of eye movement features, the objectives of our optimization problem based on the original GAN framework can be rewritten from Equation <ref type="formula" target="#formula_4">4</ref>to Equation 5 as:</p><formula xml:id="formula_5">min 𝐺 𝜃 max 𝐷 𝜙 L (𝑫 𝜙 , 𝑮 𝜃 ) = E (𝑟,𝑿 𝐸𝑌 𝐸 )∼𝑝 𝑑 (𝑟 ) [𝑙𝑜𝑔(𝑫 𝜙 (𝑟, 𝑿 𝐸𝑌 𝐸 ))]+ E 𝑟 𝐺 ∼𝐺 𝜃 (. |𝑿 𝐸𝑌 𝐸 ) [𝑙𝑜𝑔(1 − 𝑫 𝜙 (𝑟 𝐺 , 𝑿 𝐸𝑌 𝐸 ))],<label>(5)</label></formula><p>where 𝜃 and 𝜙 denote the parameters of the generator and discriminator, respectively, and 𝑿 𝐸𝑌 𝐸 is the eye movement constraint. Here, we sample the input noise from the Gaussian noise distribution. The maximum term of L (𝑫 𝜙 , 𝑮 𝜃 ) (losses of discriminator) and the minimum term of L (𝑫 𝜙 , 𝑮 𝜃 ) (losses of generator) are optimized in an alternating procedure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Test Stage: Emotion Classification</head><p>As presented in Figure <ref type="figure" target="#fig_0">1</ref>, the test data flows according to the blue lines. Using single-modal eye movement features, the trained 𝑮 can generate corresponding generated multimodal features 𝑟 𝐺 and pass them to the trained emotion classifier 𝑪 to predict the emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets</head><p>To comprehensively verify the performance of our model, we test it on a series of public affective EEG datasets for emotion recognition, including SEED, SEED-IV, and SEED-V. The differences among the three datasets are listed in Table <ref type="table" target="#tab_1">1</ref>. In each dataset, several rigorously screened Chinese movie clips are used to elicit the desired target emotion among corresponding emotion categories. Approximately 15 subjects participated in the experiments three times on different days, with video clips evenly covering each emotion in each experiment. During the experiment, subjects are encouraged to immerse themselves in the video to arouse corresponding emotions. The 62-channel EEG signals and the eye movement signals are recorded with the international 10-20 system using the ESI Neuroscan system and SMI ′ s wearable eye-tracking glasses, respectively, during movie watching.</p><p>The raw EEG signals are downsampled to 200 Hz and filtered with a bandpass of 0-75 Hz with a baseline correction as well as a PCA-based artifact elimination method with Curry 7 software. Different entropy (DE) features are extracted within a nonoverlapping one-second time window from 5 frequency bands (namely, 𝛿: 1-3 Hz, 𝜃 : 4-7 Hz, 𝛼: 8-13 Hz, 𝛽: 14-30 Hz, and 𝛾: 31-50 Hz) of every sample <ref type="bibr" target="#b5">[6]</ref>. Therefore, the dimension of EEG features is 310 per sample, calculated by 62 channels multiplied by 5 bands. We choose DE because various studies have demonstrated that the DE features perform better for EEG-based emotion recognition than other artificial features.</p><p>For the eye movement signals, 33 features, including pupil diameter, dispersion, fixation duration, blink duration, saccade duration, saccade amplitude, blink frequency, maximum fixation duration and so on, are extracted by SMI BeGaze Analysis software as described in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head><p>4.2.1 Verification of Feature Generation. Before examining the performance, we start from feature distribution to certify that the structure we construct works properly as we expect. We pick out the generated multimodal features from each subject and visualize them with t-SNE <ref type="bibr" target="#b24">[25]</ref>, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Each point in the figure represents a feature from a 4 s window size. Figure <ref type="figure" target="#fig_2">2</ref>(a) and Figure <ref type="figure" target="#fig_2">2</ref>(b) depict features from a single modality as eye movements and EEG, respectively. We can see that there are no obvious clusters in either Figure <ref type="figure" target="#fig_2">2</ref>  In Table <ref type="table" target="#tab_2">2</ref>, the mean accuracies and standard deviations on three datasets of our model are compared with other methods. As demonstrated, the table is separated into three zones. The first two rows are of single modality, while the middle two and the last represent multimodal methods and cross-modal structures, respectively. Besides, we also provide the confusion matrices in Figure <ref type="figure" target="#fig_3">3</ref> to directly present the classification results. In general, the more emotion categories there are, the more difficult the task, which is accompanied by relatively lower performance. Note that our purpose is to build  an effective and convenient model with a high generalization ability, so in the detailed discussion below, we devote much attention to the performance on the SEED-V dataset, especially in the figure illustrations.</p><formula xml:id="formula_6">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.2</head><p>Comparison with multimodal methods. The core issue of multimodal methods is the fusion strategy. Regarding to the ways they fuse, the multimodal methods can be categorized into three types: aggregation-based fusion, alignment-based fusion, and their combination. We select the most commonly used methods among the first two types as representatives to compare with our model. As shown in the third <ref type="bibr" target="#b15">[16]</ref> and fourth <ref type="bibr" target="#b14">[15]</ref> rows of Table <ref type="table" target="#tab_2">2</ref>, alignment fusion performs better since it can effectively extract the relationships among multimodalities. Although our model does not surpass the multimodal alignment fusion method in accuracy, it is as competitive as the one fused by concatenation. Considering that this performance is achieved with only a single modality, we believe our structure can help enhance the user experience and reduce the dependence on modalities. From the confusion matrices in Figure <ref type="figure" target="#fig_3">3</ref>, we can straightforwardly deduce the superiority of the multimodal method with the darker color. Nevertheless, in the classification of the happy emotion, our model even outperforms the bimodal deep autoencoder by 4% in Figure <ref type="figure" target="#fig_3">3</ref>(c) and Figure <ref type="figure" target="#fig_3">3(d)</ref>. We also observe that the recognition of disgust is always mixed up with others, especially happy emotion, when the features used are closely related to eye movements as suggested by Figure <ref type="figure" target="#fig_3">3</ref>(a) and Figure <ref type="figure" target="#fig_3">3(d)</ref>. This might be because eye tracking features such as pupil diameters and saccade details are similar when subjects feel happy and disgusted, which is consistent with the findings of Kuo and Heather <ref type="bibr" target="#b7">[8]</ref> that the eye-tracking characteristics of the happy emotion and the disgust emotion have more in common compared with other emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.3</head><p>Comparison with single-modal methods. In essence, our system is a single-modal method that only takes the signals from one modality as the input. We compare it with other single-modal models using either eye movements or EEG signals <ref type="bibr" target="#b15">[16]</ref> [27], and the results are displayed in the first two rows in Table <ref type="table" target="#tab_2">2</ref>. It is apparent from the accuracy values that the EEG signals fit the emotion classification task better and are more reliable than eye movement signals. Regardless of which dataset among the three is employed, our method outperforms all others, with a huge advantage of approximately 5% for EEG signals and 14% for eye movements. This is also reflected in Figure <ref type="figure" target="#fig_3">3</ref>(a) and Figure <ref type="figure" target="#fig_3">3</ref>(b), which show that our model outperforms single eye movement in all emotion types. Classification accuracies of the fear and the happy emotion classification have been enhanced by 6% (86% versus 80%) and 11% (79% versus 68%), respectively. Notably, the result of sad emotion recognition using a single eye modality is unsatisfactory. However, with the same input, our model remarkably increases the accuracy by 32%. In addition, the standard deviation is dramatically smaller than those of the existing approaches, indicating that our model is pretty stable and robust. The model we propose has successfully digged out the values of eye movements in emotion recognition tasks, which makes it possible for emotion intelligence to be used in daily life. More importantly, for similar questions in other fields, it provides a new initiative to improve the performance of single-modal methods by integrating knowledge of other modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Comparison with cross-modal methods.</head><p>There are a few crossmodal models used in the emotion recognition task. Compared with the regressor of Jiang <ref type="bibr" target="#b10">[11]</ref>, it is evident that our model fits the characteristics of data well based on the three relatively higher accuracies. However, although both have a tendency to be more robust when facing more emotion categories, the standard deviations of the regressor decline more quickly than those of our model and even reach 5.07% for the five-class emotion classification task. The relationship between multimodal high-level representations and eye movement features needs to be further explored in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we have proposed a new direction to simplify the multimodal structure in emotion recognition tasks by combining a multimodal fusion strategy and a generative model to explore the underlying connections between single modality input and highdimensional multimodal features. Then, multimodal features can be generated based on a single modality which have the similar characteristics to the real multimodal features extracted from multimodal signals in the training stage. In this way, only information from a single modality is needed in the test stage. The comprehensive results on SEED series datasets demonstrate that our proposed model is as competitive as many multimodal models even though it only requires a single modality as input, which reduces the dependence on multimodalities and makes real-world applications more possible. Moreover, this idea has the potential to be used in other fields to simplify the multimodal process. In future work, we first plan to perfect the model by improving both multimodal fusion and generation parts. Breaking the specification of modalities and expanding the modal coverage is also attractive to us, and the final goal is to build an overall network with which we can use any signal that is the most convenient at the time to perform emotion recognition. Only in this way can we utilize emotion intelligence to benefit people in more practical scenarios such as mental health.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of our system. The training stage can be divided into two parts as multimodal feature extraction and multimodal feature generation. In the test phase, only eye movement signals are needed. The shaded 𝑮 and 𝑪 indicate that they have been trained. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) or Figure 2(b). These results account for the ineffectiveness of emotion classification based on single-modal features. Comparatively, Figure 2(c) and Figure 2(d), presenting the multimodal features extracted by the bimodal deep autoencoder and those generated by our model, respectively, clearly illustrate the distinguished groups where dots of the same emotion gather. We must admit that the distinction among groups in Figure 2(c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Emotion feature visualization, where different colors represent different emotions. (a) Single-modal features of eye movements. (b) Single-modal features of EEG signals. (c) Multimodal features of bimodal deep autoencoder. (d) Generated multimodal features of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Confusion matrices based on the SEED-V dataset. (a) Single-modality of eye movements. (b) Single-modality of EEG. (c) Bimodal deep autoencoder. (d) Our model. The deeper the color, the higher the recognition rate between emotions. The vertical axis represents the true label, and the horizontal axis represents the predicted label.</figDesc><graphic url="image-4.png" coords="6,436.33,97.14,97.58,97.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Initialize encoders 𝑬 𝐸𝐸𝐺 and 𝑬 𝐸𝑌 𝐸 , decoders 𝑫 𝐸𝐸𝐺 and 𝑫 𝐸𝑌 𝐸 , and emotion classifier 𝑪. Optimize 𝑬 𝐸𝐸𝐺 , 𝑬 𝐸𝑌 𝐸 , 𝑫 𝐸𝐸𝐺 , 𝑫 𝐸𝑌 𝐸 , and 𝑪 by minimizing Equation (1).</figDesc><table><row><cell cols="2">Algorithm 1: CGAN-based multimodal feature generation</cell></row><row><cell>algorithm</cell><cell></cell></row><row><cell>Input:</cell><cell></cell></row><row><cell cols="2">EEG data 𝑿 𝐸𝐸𝐺 .</cell></row><row><cell cols="2">Eye movement data 𝑿 𝐸𝑌 𝐸 .</cell></row><row><cell cols="2">Divide the training and test sets according to</cell></row><row><cell>cross-validation.</cell><cell></cell></row><row><cell cols="2">Output: Recognition accuracy of test data.</cell></row><row><cell>Training Stage I :</cell><cell></cell></row><row><cell>1 2 for j=1:n do</cell><cell></cell></row><row><cell>3</cell><cell></cell></row><row><cell>4 end</cell><cell></cell></row><row><cell cols="2">5 return multimodal features 𝑟 and trained emotion</cell></row><row><cell>classifier 𝑪.</cell><cell></cell></row><row><cell cols="2">Training Stage II :</cell></row><row><cell>10 end</cell><cell></cell></row><row><cell cols="2">11 return trained 𝑮.</cell></row><row><cell>Test Phase</cell><cell>:</cell></row><row><cell cols="2">12 Concatenate random noise and eye movement features in</cell></row><row><cell>the test set.</cell><cell></cell></row></table><note><ref type="bibr" target="#b5">6</ref> Initialize the generator 𝑮 and discriminator 𝑫. 7 for j=1:n do 8 Concatenate random noise and eye movement features in the training set.9Optimize 𝑮 and 𝑫 by minimizing Equation<ref type="bibr" target="#b4">(5)</ref>.<ref type="bibr" target="#b12">13</ref> Generate the multimodal feature using trained 𝑮. 14 Use the trained classifier 𝑪 for emotion recognition. 15 return predicted emotion label.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of datasets employed. Except for the variables mentioned in this table, other conditions among all three datasets are the same.</figDesc><table><row><cell>Dataset</cell><cell>Subjects</cell><cell cols="2"># Emotions Emotions</cell><cell># videos / experiment</cell></row><row><cell>SEED[28]</cell><cell>15 (7M8F, mean: 23.27, std: 2.37)</cell><cell>3</cell><cell>happy, sad, neutral</cell><cell>15</cell></row><row><cell cols="2">SEED-IV[27] 15 (7M8F, mean: 23.08, std: 2.05)</cell><cell>4</cell><cell>happy, sad, neutral, fear</cell><cell>24</cell></row><row><cell cols="3">SEED-V[14] 20 (9M11F, mean: 22.15, std: 1.85) 5</cell><cell>happy, sad, neutral, fear, disgust</cell><cell>15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average accuracies and standard deviations(%) of different methods on 3 datasets.</figDesc><table><row><cell>Feature</cell><cell>SEED</cell><cell>SEED-IV</cell><cell>SEED-V</cell></row><row><cell>Representation</cell><cell cols="3">Avg. Std. Avg. Std. Avg. Std.</cell></row><row><cell>Eye</cell><cell cols="3">77.80 14.61 67.82 18.04 59.66 8.77</cell></row><row><cell>EEG</cell><cell cols="3">78.51 14.32 70.33 14.45 68.58 10.27</cell></row><row><cell cols="4">Eye+EEG (concatenate) 81.55 11.79 75.88 16.44 73.65 8.90</cell></row><row><cell>Eye+EEG (align)</cell><cell cols="3">93.05 3.85 86.55 5.72 80.37 6.03</cell></row><row><cell>Eye (regressor)</cell><cell cols="3">75.72 8.87 73.49 7.02 72.80 5.07</cell></row><row><cell>Eye (our work)</cell><cell cols="3">81.02 8.04 75.74 6.66 73.66 6.05</cell></row><row><cell cols="4">is more apparent than in Figure 2(d). The visualized distribution</cell></row><row><cell cols="4">further indicates that our method can generate reliable realistic mul-</cell></row><row><cell cols="4">timodal features, which is a cornerstone to guarantee performance</cell></row><row><cell cols="2">in emotion recognition tasks.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by grants from the National Natural Science Foundation of China (Grant No. 61976135), SJTU TransMed Awards Research (WF540162605), the Fundamental Research Funds for the Central Universities, the 111 Project, and the China Southern Power Grid (Grant No. GDKJXM20185761).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotions recognition using EEG signals: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soraia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Alarcao</surname></persName>
		</author>
		<author>
			<persName><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="374" to="393" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">EEG-based emotion recognition. The Influence of Visual and Auditory Stimuli</title>
		<author>
			<persName><forename type="first">Danny</forename><surname>Oude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bos</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pupil as a measure of emotional arousal and autonomic activation</title>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Miccoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Escrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="602" to="607" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><surname>Birbaumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Guger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donatella</forename><surname>Mattia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Del R Millán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felip</forename><surname>Miralles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eloy</forename><surname>Opisso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BNCI Horizon 2020: towards a roadmap for the BCI community</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of emotion recognition using facial expressions, speech and multimodal information</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serdar</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abe</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungbok</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrikanth</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Multimodal Interfaces</title>
				<meeting>the 6th International Conference on Multimodal Interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="205" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Differential entropy feature for EEG-based emotion classification</title>
		<author>
			<persName><forename type="first">Jia-Yi</forename><surname>Ruo-Nan Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 6th International IEEE/EMBS Conference on Neural Engineering</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Survey on speech emotion recognition: Features, classification schemes, and databases</title>
		<author>
			<persName><forename type="first">Moataz</forename><surname>El Ayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fakhri</forename><surname>Karray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2010.09.020</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2010.09.020" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face in profile view reduces perceived facial expression intensity: An eye-tracking study</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using CNN</title>
		<author>
			<persName><forename type="first">Zhengwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qirong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongzhao</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
				<meeting>the 22nd ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="801" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating multimodal features for emotion classification from eye movement signals</title>
		<author>
			<persName><forename type="first">Huangfei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiya</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ye</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Ming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australian Journal of Intelligent Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context-aware emotion recognition networks</title>
		<author>
			<persName><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10143" to="10152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multisource transfer learning for cross-subject EEG emotion recognition</title>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="3281" to="3293" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification of five emotions from EEG and eye movement signals: Discrimination ability and stability over time</title>
		<author>
			<persName><forename type="first">Tian-Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 9th International IEEE/EMBS Conference on Neural Engineering</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="607" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emotion recognition using multimodal deep learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="521" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining eye movements and EEG to enhance emotion recognition</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1170" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial networks conditioned by brain signals</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaak</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3410" to="3418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Information processing in dynamical systems: Foundations of harmony theory</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
		<respStmt>
			<orgName>Colorado Univ at Boulder Dept of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition in response to videos</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="223" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">EEG emotion recognition using dynamical graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="532" to="541" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning human mind for automated visual classification</title>
		<author>
			<persName><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaak</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6809" to="6817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition using EEG and eye tracking data</title>
		<author>
			<persName><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Nan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="5040" to="5043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emotionmeter: A multimodal framework for recognizing human emotions</title>
		<author>
			<persName><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identifying stable patterns over time for emotion recognition from EEG</title>
		<author>
			<persName><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="417" to="429" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning active facial patches for expression analysis</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2562" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
