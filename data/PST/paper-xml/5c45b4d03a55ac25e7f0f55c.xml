<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphSAR: A Sparsity-Aware Processing-in-Memory Architecture for Large-scale Graph Processing on ReRAMs</title>
				<funder ref="#_awDmrEz">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_WaxbVZj #_qw2FKa4">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Pmtu3Kp">
					<orgName type="full">Beijing Innovation Center for Future Chip</orgName>
				</funder>
				<funder>
					<orgName type="full">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guohao</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of EE</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianhao</forename><surname>Huang</surname></persName>
							<email>tianhaoh@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<email>yu-wang@</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of EE</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhong</forename><surname>Yang</surname></persName>
							<email>yanghz@tsinghua.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of EE</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Wawrzynek</surname></persName>
							<email>johnw@eecs.berkeley.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphSAR: A Sparsity-Aware Processing-in-Memory Architecture for Large-scale Graph Processing on ReRAMs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3287624.3287637</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale graph processing has drawn great attention in recent years. The emerging metal-oxide resistive random access memory (ReRAM) and ReRAM crossbars have shown huge potential in accelerating graph processing. However, the sparse feature of natural graphs hinders the performance of graph processing on ReRAMs. Previous work of graph processing on ReRAMs stored and computed edges separately, leading to high energy consumption and long latency of transferring data. In this paper, we present Graph-SAR, a sparsity-aware processing-in-memory large-scale graph processing accelerator on ReRAMs. Computations over edges are performed in the memory, eliminating overheads of transferring edges. Moreover, graphs are divided considering the sparsity. Subgraphs with low densities are further divided into smaller ones to minimize the waste of memory space. According to our extensive experimental results, GraphSAR achieves 4.43x energy reduction and 1.85x speedup (8.19x lower energy-delay product, EDP) against previous graph processing architecture on ReRAMs (GraphR [1]).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are widely used to model both data and relationships in realworld problems, including neural network modeling, social network analysis, etc. The demand for large-scale graph processing has skyrocketed in recent years. Many graph processing systems have been put forward, including single PC-based <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, cluster-based <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, hardware specialized accelerators <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, and processing-inmemory designs <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>.</p><p>The emerging metal-oxide resistive random access memory (ReRAM) and ReRAM crossbars show huge potential in energy efficient processing-in-memory operations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, especially for matrixvector multiplications <ref type="bibr" target="#b16">[17]</ref>. On the other hand, graph algorithms can be naturally represented by updating destination vertices using source vertices, through the adjacency matrix. Many graph processing architectures/accelerators based on ReRAMs have been proposed (e.g., RPBFS <ref type="bibr" target="#b17">[18]</ref>, GraphR <ref type="bibr" target="#b0">[1]</ref>, and HyVE <ref type="bibr" target="#b18">[19]</ref>), achieving great speedup and energy efficiency improvement.</p><p>The sparse natural graphs are hard to be stored into ReRAM crossbars in the form of adjacency matrix. Instead, in the design of RPBFS, GraphR, and HyVE, edges in the graph are stored using the edge list format. RPBFS adopts a shared memory to store values of vertices. GraphR converts edge list into adjacency matrix, and then stores the matrix into the ReRAM crossbar. HyVE further designs memory hierarchy for vertex storage, which is not mentioned in GraphR. Then, HyVE uses conventional CMOS circuits to process the subgraph edge by edge. Although using ReRAM crossbars can process multiple edges in just one operation, it still suffers from two main problems. (1) Heavy writing overheads. Previous work like GraphR <ref type="bibr" target="#b0">[1]</ref> converted the edge list into the adjacency matrix by sequentially writing each edge into the ReRAM crossbar. Writing data to ReRAMs leads to higher energy consumption and longer latency compared with reading/computing over ReRAMs, especially when multi-level cells are required. <ref type="bibr" target="#b1">(2)</ref> No parallelism inside a block. In GraphR, edges in a subgraph are not processed in parallel because edges are sequentially written to ReRAM crossbars.</p><p>Processing-in-memory is a promising solution to overcoming heavy overheads of writing data to ReRAM crossbars. However, directly storing all subgraphs using the adjacency matrix leads to heavy memory overheads. Thus, we present GraphSAR, a sparsityaware processing-in-memory graph processing accelerator on ReRAMs. Computations are performed directly in the memory. Subgraphs with low densities are divided into smaller ones to reduce memory space waste. The detailed contributions are summarized as follows:</p><p>? Processing-in-memory graph processing. GraphSAR directly processes edges in the memory where edges are stored. In this way. GraphSAR avoids high energy consumption and latency of writing edge values to computing units. ? Sparsity-aware graph processing. Subgraphs with low densities are divided into smaller ones in order to achieve memory space efficiency. Compared with storing all subgraphs using a large adjacency matrix (8?8), we reduce the memory space requirements from 45.87x to 1.54x (normalized to the non processing-in-memory implementation). ? Fewer subgraphs and lower cell bits. We propose a lightweight graph clustering method to reduce processed subgraphs by up to 70.38%, reducing the energy-efficient product (EDP) by 1.78x. We also show that only single-bit cells are required for unweighted graph algorithms, leading to 3.77x lower EDP.</p><p>We take the design of GraphR <ref type="bibr" target="#b0">[1]</ref> as the baseline and compare it with GraphSAR in Figure <ref type="figure" target="#fig_0">1</ref>. Experimental results show that, GraphSAR can achieve 4.43x energy reduction and 1.85x speedup <ref type="bibr">(8.19x</ref> lower EDP) against GraphR, with only 1.54x memory space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Processing and Partitioning Model</head><p>The Gather-Apply-Scatter (GAS) model is used to describe different graph algorithms. In GAS, processing one vertex includes: <ref type="bibr" target="#b0">(1)</ref> Gathering values from incoming neighbors; (2) Applying gathered values to get a new value; (3) Scattering the new value to all outgoing neighbors. Different algorithms only differ in how to compute the new value. There are two main implementations of GAS model: ? Vertex-centric: Iterating over vertices. A vertex's value is propagated to all neighbors, and neighbors are updated. ? Edge-centric: Iterating over edges. The value of an edge's source vertex is propagated to updating the destination vertex.</p><p>The edge-centric model is first proposed by X-stream <ref type="bibr" target="#b2">[3]</ref> to ensure the locality of graph processing. The interval-block graph partitioning method is widely used in previous systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Vertices are divided into disjointed intervals, and edges are divided into corresponding blocks. Edges in a block are from vertices in an interval to vertices in another interval, shown in Figure <ref type="figure" target="#fig_1">2</ref>. By adopting both edge-centric model and interval-block partitioning method, graph algorithms can be executed block by block. Within a block, each edge is processed to propagate the value from the source vertex to the destination vertex. Algorithm 1 shows the flow of graph processing under the edge-centric model after partitioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Processing on ReRAMs</head><p>The low read latency and high energy efficiency make ReRAMs the promising solution as edge storage in graph processing. Previous ReRAM-based graph processing systems all store edges in ReRAMs to perform the edge-centric model. RPBFS <ref type="bibr" target="#b17">[18]</ref> uses a shared memory to store values of all vertices. Such implementation leads to the scalability problem. The size of shared memory increases drastically when graphs become larger or algorithm changes (e.g, when running PageRank, the size of each vertex increases by at least 32x (from 1-bit boolean to 32-bit float)). From the experimental results of RPBFS we can see, when the size of crossbar increases, larger graphs achieve more improvements against smaller graphs. This means that, the performance of RPBFS will decrease drastically when graphs become larger compared with the fixed-size crossbar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The edge-centric model after the interval-block partitioning</head><formula xml:id="formula_0">Input: G = (V , E), initialization Output: Updated V 1: Initialization() 2: while not finished do 3:</formula><p>for each e i .j ? each B x .y do 4:</p><formula xml:id="formula_1">value(v i ) = Update(v i , v j , e i, j ) 5:</formula><p>end for 6: end while 7: return V To ensure the scalability, GraphR <ref type="bibr" target="#b0">[1]</ref> and HyVE <ref type="bibr" target="#b18">[19]</ref> divide a graph into subgraphs using interval-block partitioning. Only a fixed-size subgraph is processed without accessing other parts of the graph. GraphR writes a subgraph into the ReRAM crossbar in the form of adjacency matrix, and values of source/destination vertices are stored in registers. Then, GraphR performs graph operations using the crossbar. Edges are sequentially written to ReRAM crossbars to be converted into the adjacency matrix, leading to high energy consumption and long latency, especially when data are stored in multi-level cells of ReRAMs. We can define the effective parallelism of processing edges in a subgraph in Equation ( <ref type="formula" target="#formula_2">1</ref>) by normalizing the total processing time to the time of processing one edge (T wr ite + T r ead ).</p><formula xml:id="formula_2">p ef f ective = T wr ite + T r ead N ed?es ? T wr ite + T r ead ? N ed?es<label>(1)</label></formula><p>In Equation (1), T wr ite and T r ead represent latency of writing data to and performing computation on ReRAM crossbars, and N ed?es represents the number of edges in a subgraph. We can see that P ef f ective is always less than N ed?es . Moreover, because of heavy overheads of 'Write-and-verify' operations <ref type="bibr" target="#b19">[20]</ref>, T wr ite is usually larger than T r ead in ReRAM crossbars, thus p ef f ective is around 1. p ef f ective is linear to N ed?es only when T wr it e T r ead &lt;&lt; 1, which is unpractical in real devices. Thus, GraphR does not process edges in a subgraph in parallel. HyVE focuses on the memory hierarchy for vertices in graph processing. Edges are processed using the CMOS circuit one by one in HyVE. Thus, HyVE cannot process multiple edges in a subgraph in one operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GraphSAR Model</head><p>Writing adjacency matrices to crossbars leads to heavy overheads on energy consumption and latency. Storing the graph in the form of the adjacency matrix, rather than converting the graph from other formats into the adjacency matrix, is the promising solution to achieve the energy efficient graph processing on ReRAMs. However, due to the sparsity of graphs, directly storing a whole graph in the form of the adjacency matrix suffers from memory space inefficiency. Thus, in this section, we detail our hybrid-centric processing model and sparsity-aware graph partitioning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hybrid-centric Model</head><p>Although GraphR can benefit from processing multiple edges in a block, the parallelism does not apply to blocks with only one edge, and suffers from blocks with low densities. Thus, we can directly process the only edge in blocks with one edge, and divide a larger   <ref type="table">1</ref>.</p><p>sparse blocks into several smaller ones. Note that when dividing the whole graph into small blocks (e.g., 8 ? 8), these blocks also sparsely spread over the graph as edges. Thus, we can process these blocks using a block-centric model which is similar to the edge-centric model mentioned in Section 2.1. We define this block-centric model and further propose the hybrid-centric model as:</p><p>? Block-centric: Iterating over blocks. When processing a block, values of source vertices in the block is propagated through edges, and destination vertices are updated. ? Hybrid-centric: Blocks with only one edge are stored in the edge list (blocks with multiple edges are stored in the block list). Then, iterations are performed over edges and blocks, using edge/block-centric models correspondingly.</p><p>GraphSAR adopts this hybrid-centric model to process blocks with multiple edges and one edge separately. Based on this hybridcentric model, we will detail the sparsity-aware partitioning scheme in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparsity-aware Graph Partitioning</head><p>In order to process edges in the form of the adjacency matrix, we can store all non-empty blocks using adjacency matrix format. However, as we can see in Figure <ref type="figure" target="#fig_2">3</ref>, most non-empty blocks are sparse (only containing 1 edge). Thus, if we store all non-empty 8 ? 8 blocks using adjacency matrix format, a huge amount of memory space is wasted. We normalize the memory space usage of directly storing all blocks into the adjacency matrix to storing all edges using the edge list in Figure <ref type="figure" target="#fig_4">4</ref>. As we can see, 46.87x memory space is required on average, compared with using the edge list.</p><p>According to the hybrid-centric model, graphs can be stored in an edge list (edges in blocks with only one edge) and a block list (blocks with multiple edges) separately. Thus, we can save the memory space by storing all single edges using the edge list instead of an 8 ? 8 adjacency matrix. From Figure <ref type="figure" target="#fig_4">4</ref> we can see, such method can reduce the required memory space to 5.77x on average (normalized to storing all edges using the edge list). However, in some graphs (e.g., WV and CA), the memory space requirement is still larger than 10x.</p><p>Such an idea inspires us that we do not need to store sparse blocks using a large adjacent matrix. We divide graphs into blocks of size 8 ? 8 first<ref type="foot" target="#foot_0">1</ref> . Then, for blocks with only one edge, we store these edges into the edge list. We only store blocks with densities larger than a given threshold (we choose 0.5 here in GraphSAR) into the block list. For blocks with more than half zero elements, we  divide the block into 4 disjoint smaller blocks (e.g., a block of size 8 ? 8 is divided into 4 disjoint blocks of size 4 ? 4), and repeat steps above until these blocks are assigned to the block list or the edge list.</p><p>In this way, we can ensure graphs are stored in the form of small adjacency matrices with more than half non-zero elements, or the edge list. We show the memory space requirement after adopting our sparsity-aware graph partitioning scheme in Figure <ref type="figure" target="#fig_4">4</ref>. As we can see, only 1.63x memory space is required on average, compared with storing all edges into the edge list. In some cases (e.g., GG and FS), there is almost no extra memory space requirements, because nearly all blocks only contain 1 edge (Figure <ref type="figure" target="#fig_2">3</ref>). We will further improve the result by using the clustering method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lightweight Graph Clustering</head><p>Empty subgraphs are skipped in GraphSAR (and in GraphR). Thus, clustering edges in a graph to reduce non-empty subgraphs is a promising way to improve the performance of GraphSAR and GraphR. Inspired by previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, that vertices appear together in the original edge list usually have locality, we propose our lightweight graph clustering method by remapping the index of each vertex. Figure <ref type="figure">5</ref> shows an example of this lightweight graph clustering method. When we load the original edge list, we assign the index to each new vertex from 0. Vertex 0 and 2 are the first two vertices in the edge list, then we map 0 ? 0 and 2 ? 1. After that, we process whole edge list in this way. In the example of Figure <ref type="figure">5</ref>, we can reduce the number of non-empty subgraphs from 6 to 4.</p><p>The detailed results on real-world graphs are shown in Figure <ref type="figure">6</ref>. It is shown that, by adopting the lightweight graph clustering method, the total number of subgraphs to be processed can be reduced by up to 70.38% (26.17% on average). Such index mapping can be maintained by a hash table, and processed at the same time of loading edges. Thus, the time complexity is only O(#ed?es). Although previous work <ref type="bibr" target="#b22">[23]</ref> proposed graph clustering method, it was based on the Breadth-First Search algorithm. Such method has already performed graph algorithms when clustering, leading to heavy overheads. Moreover, as we can see in Figure <ref type="figure" target="#fig_4">4</ref>, our clustering method can further reduce the memory requirements to 1.54x, compared with storing all edges using the edge list.</p><p>4 GraphSAR Architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Architecture</head><p>By storing graphs using the block list and the edge list, GraphSAR can process subgraphs in the memory where these subgraphs are stored. Thus, GraphSAR is different from GraphR <ref type="bibr" target="#b0">[1]</ref> and HyVE <ref type="bibr" target="#b18">[19]</ref>, by both computing and storing edge data in ReRAM crossbar. Figure <ref type="figure" target="#fig_6">7</ref> shows the top view of GraphSAR architecture. As we can see, we organize GraphSAR into banks with peripheral circuits, working as both memory and computation parts. Each bank is composed of multiple mats (ReRAM crossbars), and GraphSAR can perform matrix-vector multiplication based computation on these mats with peripheral circuits. The size of each mat can be 512 ? 512 or 1024 ? 1024 <ref type="bibr" target="#b23">[24]</ref>, while we can perform multiplication on matrices of smaller sizes (e.g., 4 ? 4 or 8 ? 8) by selecting wordlines and bitlines in a mat. The detailed functions of each component and peripheral circuits are described as follows:</p><p>? ReRAM crossbar. Edges are stored in ReRAM crossbars in both edge list and block list formats, shown on the right side of Figure <ref type="figure" target="#fig_6">7</ref>. Edges and blocks of different sizes are stored on different banks for the alignment purpose. The detailed processing flow of two types of graph algorithms (matrixvector multiplication and non-matrix-vector multiplication) is shown later. ? Register file (Reg). Both ReRAM crossbars and sALUs use data in the register file for computation. Vertex data are loaded to the register file during runtime. Note that blocks in the block list sparsely spread over the graph, we can treat each block as an 'edge' connecting multiple vertices. We have four types of 'edges', 8?8, 4?4, 2?2, and also original individual edges. We can treat these four types of 'edges' as four subgraphs separately (We do store blocks with different sizes separately for the alignment purpose, mentioned above). In this way, GraphSAR can adopt the similar scheduling scheme in GraphR with no extra scheduling/routing overheads. ? Simple ALU (sALU). The sALU is a simple customized algorithmic and logic unit, and it is at the output ports of ReRAM crossbar. We add 8 sALUs to each mat, and results from different bitlines can be selected and sent to these sALUs through multiplexers.  into blocks of different sizes, the scheduler is also responsible for selecting wordlines and bitlines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Sample and Hold (S/H). S/H samples and holds analog currents before converting into digital values. ? Analog to Digital Converter (ADC). ADC is used to con-</head><p>vert analog data to digital data. We share ADC among bitlines, based on Previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Shift and Add unit (S/A). GraphR has mentioned that the</head><p>Shift and Add units are used to alleviate the pressure of driver, because the precision of ReRAM cells is low.</p><p>Compared with GraphR, GraphSAR can process graphs directly in the memory where graphs are stored, eliminating overheads of converting graphs from edge list into adjacency matrix. The processing-in-memory design can also provide memory-capacityproportional <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> computation units, rather than fixed number of processing units (graph engines) in GraphR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GraphSAR Processing Flow</head><p>By using the sparsity-aware partitioning method, a graph is divided into blocks and edges. GraphSAR scan blocks in the block list and edges in the edge list to perform graph algorithms based on the hybrid-centric model. The order of blocks and edges in lists is the same as that in GraphR <ref type="bibr" target="#b0">[1]</ref>, where data are stored in the columnoriented order, leading to less usage of registers and write cost.</p><p>There are two main types of graph algorithms, matrix-vector multiplication based and non-matrix-vector multiplication based algorithms. GraphSAR processes them in different ways. We use PageRank and Breadth-First Search as examples of these two types of algorithms, to show the detailed processing flow in GraphSAR.</p><p>? Matrix-vector multiplication based. We take PageRank as the example. For blocks in the block list (blocks with multiple edges), GraphSAR first selects the 4 ? 4 cells from the large crossbar based on peripheral circuits proposed in Fig. <ref type="figure" target="#fig_6">7</ref>. Then, the value of each source vertex is sent to input ports of corresponding rows. The ReRAM crossbar performs the matrix-vector multiplication operation to calculate the rank value of each destination vertex. For edges in the edge list, GraphSAR just uses sALUs to calculate the rank value of the destination vertex. ? Non-matrix-vector multiplication based. We take Breadth-First Search as the example. For blocks in the block list, GraphSAR activates each row sequentially. When a row is activated, the value of corresponding source vertex is transferred to neighbors by selecting corresponding columns, and the sALU on each output port makes the comparison operation between source and destination vertices. For edges in the edge list, GraphSAR also just uses sALU to calculate the rank value of the destination vertex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Single-bit ReRAM Cell Implementation</head><p>We also show how to avoid requirements for high-precision ReRAM cells when running some algorithms. We take PageRank as an example on the right side of Figure <ref type="figure" target="#fig_7">8</ref> to show the difference between GraphSAR and GraphR. In PageRank algorithm, the rank value of each source vertex multiplied by a factor (divided by the out-degree and multiplied by the PageRank factor) before sending to the destination vertex. GraphR stores factors of vertices to corresponding rows in the ReRAM crossbar, and the rank value of each vertex is multiplied by the factor during processing on crossbars. Note that factors of a same source vertices to different destination vertices are same, thus we can directly store that value in the memory instead of calculating it every time. 2 In this way, GraphSAR only needs to store the connectivity of subgraphs using 1-bit cells. Such implementation is available for all unweighted graph algorithms, because all outgoing edges of the same source vertices propagate a same value, edges only represent the connectivity among vertices. Algorithms like Single Source Shortest Path are not applicable in this case because out-going edges of one vertex have different values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>5.1.1 Datasets and Algorithms.We choose 10 real-world graphs from different domains in our experiments, shown in TABLE <ref type="table">1</ref>. 2 The calculation shown at the input port happens when GraphSAR loads values from the memory, in Figure <ref type="figure" target="#fig_7">8</ref>.</p><p>The number of vertices in these graphs ranges from thousands (k) to billions (b), and edges from millions (m) to billions (b). We implement 3 different graph algorithms. Breadth-First Search (BFS), PageRank (PR), and Connected Components (CC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: Graph datasets used in evaluation</head><p>Datasets #Vertices #Edges Type wiki-Vote (WV) <ref type="bibr" target="#b25">[26]</ref> 7.12 k 0.10 m social network cit-HepPh (HP) <ref type="bibr" target="#b25">[26]</ref> 34.5 k 0.42 m citation graph web-Google (GG) <ref type="bibr" target="#b25">[26]</ref> 0.88 m 5.11 m web graph com-Youtube (YT) <ref type="bibr" target="#b25">[26]</ref> 1.13 m 2.99 m community soc-Pokec (PK) <ref type="bibr" target="#b25">[26]</ref> 1.63 m 30.6 m social network roadNet-CA (CA) <ref type="bibr" target="#b25">[26]</ref> 1.97 m 2.77 m road network wiki-Talk (WT) <ref type="bibr" target="#b25">[26]</ref> 2.39 m 5.02 m communication twitter-2010 (TW) <ref type="bibr" target="#b26">[27]</ref> 41.7 m 1.47 b social network com-Friendster (FS) <ref type="bibr" target="#b25">[26]</ref> 65.6 m 1.81 b community yahoo-web (YH) <ref type="bibr" target="#b27">[28]</ref> 1.41 b 6.64 b web graph 5.1.2 Configurations.To evaluate the latency and energy consumption of GraphSAR, we choose NVSim <ref type="bibr" target="#b28">[29]</ref> from many ReRAM simulators <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. We modify the model of ReRAM cell in NVSim.</p><p>The parameter of ReRAM cell model is from the same source <ref type="bibr" target="#b19">[20]</ref> in GraphR <ref type="bibr" target="#b0">[1]</ref> (read/write energy consumption: 1.08pJ/7.4pJ, <ref type="foot" target="#foot_1">3</ref> read-/write latency: 29.31ns/50.88ns, HRS/LRS resistance: 25M?/50K?, read/write voltage: 0.7V/2V, current of LRS/HRS: 40?A/2?A). We model registers using Cacti <ref type="bibr" target="#b31">[32]</ref>, and the data of Analog/Digital converters are from <ref type="bibr" target="#b32">[33]</ref>. For other logics, we evaluate the performance using code instrumentation to get the memory access trace, and the map the trace to corresponding device considering the model. For multi-level ReRAM cells, we modify NVSim according to the parallel sensing scheme proposed in <ref type="bibr" target="#b19">[20]</ref>, which enables the 'Write-and-verify' process of accessing multi-level ReRAM cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benefits of GraphSAR Designs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Benefit of Clustering.</head><p>We have proposed the lightweight graph clustering method to reduce the number of blocks to be processed. In this way, more blocks are skipped and less (but denser) blocks are processed. Shown in Figure <ref type="figure" target="#fig_8">9</ref>, using lightweight graph clustering method leads to averagely 1.37x energy reduction and 1.30x speedup (1.78x lower EDP). Graph GG benefits most from our clustering method, because over 99% non-empty blocks are with only one edge (shown in Figure <ref type="figure" target="#fig_2">3</ref>). These blocks are merged and the total number nonempty blocks are reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Benefit of Single-bit Implementation</head><p>.By adopting such method, GraphSAR does not need to read/write multi-level ReRAM cells. Different from GraphR where multiple ReRAM crossbars with a Shift and Add units are used to calculate an edge stored in multi-bit cells, only one crossbar is required in GraphSAR.</p><p>As you can see in Figure <ref type="figure" target="#fig_9">10</ref>, using our single-bit implementation leads to averagely 2.37x energy reduction and 1.15x speedup (2.73x lower EDP) compared with multi-bit implementation. Such implementation even results in higher energy reduction on some high-precision required algorithms (e.g., PR, 3.77x on average), because more ReRAM crossbars are required in these algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with GraphR</head><p>Based on the clustering and single-bit implementation mentioned in Section 5.2, the performance of GraphSAR is improved. Such methods can also be applied to GraphR to improve the performance of GraphR. Thus, in this section, we simulate GraphR with clustering and single-bit implementation, and compare it with GraphSAR.</p><p>As we can see in Figure <ref type="figure" target="#fig_11">11</ref>. Compared with GraphR, Graph-SAR achieves 4.43x energy reduction and 1.85x speedup (8.19x lower EDP). Such comparison can show performance improvements by adopting the sparsity-aware partitioning and processingin-memory schemes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with HyVE</head><p>Blocks can be treated as 'edges' in GraphSAR. HyVE is another graph processing accelerator using ReRAM, which processes graphs edge by edge. Compared with HyVE, GraphSAR can process multiple edges in a block. We compare the speedup and energy reduction between GraphSAR and HyVE, and list the ratio of edges in the block list of GraphSAR.</p><p>As we can see in Figure <ref type="figure" target="#fig_13">12</ref>, GraphSAR achieves 1.29x speedup and 2.18x energy reduction compared with HyVE on average. Moreover, when there are more edges in the block list, the performance improvement becomes greater. GraphSAR outperforms HyVE by processing multiple edges simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Design Space Exploration</head><p>As the emerging technology, ReRAM designs have been evolving year by year, and the parameters of ReRAMs can be quite different based on various experimental measurements. We have compared performance between GraphSAR and GraphR in previous sections using parameters mentioned in our evaluation setup. To further compare the performance between GraphSAR and GraphR on different ReRAM cell models, we modify NVsim by choosing different optimization targets, to get parameters of ReRAM in different situations. We calculate the average ratio of energy consumption and execution time when running PageRank on our datasets.</p><p>As we can see in TABLE <ref type="table" target="#tab_2">2</ref>, GraphSAR outperforms GraphR under different configurations. The reason is the GraphSAR eliminates overheads of writing ReRAM cells during processing in GraphR.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose GraphSAR, a sparsity-aware processingin-memory architecture for large-scale graph processing on ReRAMs.</p><p>Frequent writing to ReRAMs leads to heavy overheads, especially when writing data to ReRAMs and only performing computation once over these data (e.g, GraphR <ref type="bibr" target="#b0">[1]</ref>). Compared with GraphR, GraphSAR actually exploits the processing-in-memory feature of ReRAMs by directly processing edges in the memory, with only 1.54x memory space overheads. By removing overheads of writing data to ReRAMs, GraphSAR achieves 4.43x energy reduction and 1.85x speedup (8.19x lower energy-delay product, EDP) against GraphR.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between GraphR and GraphSAR, including two optimization designs in GraphSAR.</figDesc><graphic url="image-1.png" coords="1,317.57,175.82,252.85,104.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interval-block graph partitioning, 4 vertices and 6 edges are divided into 2 intervals and 2 ? 2 = 4 blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Percentage of subgraphs (8 source and 8 destination vertices) with only one edge in all subgraphs to be processed, graphs are from TABLE 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>YT PK CA WT TW FS YH Mean using 8?8 blocks using 8?8 blocks and edge list sparsity-aware partitioning sparsity-aware partitioning + clustering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Memory space requirements when storing graphs in different ways (normalized to storing all edges into the edge list), graphs are from TABLE 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Clustering in GraphSAR, there are 9 vertices (dots) in the graph, each small orange square represents a block.</figDesc><graphic url="image-17.png" coords="3,317.75,592.76,158.14,85.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The detailed bank-level architecture of GraphSAR (left), and the data allocation of the edge list and the block list (right).</figDesc><graphic url="image-19.png" coords="4,78.35,83.54,227.95,62.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Detail processing flow of GraphSAR (left) and single-bit ReRAM cell implementation in GraphSAR (right). The left top shows an example of selecting a 4?4 from an 8?8 ReRAM crossbar. The rest part shows how GraphSAR processes subgraphs stored in the block list and the edge list, performing different algorithms. The size of processed matrix is set to 4 ? 4 as an example, and the PageRank factor is set to 0.8.</figDesc><graphic url="image-63.png" coords="5,53.45,83.30,168.61,172.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Benefit of lightweight graph clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Benefit of using single-bit cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Comparison between GraphR and GraphSAR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>of edges in the block list</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The relationship between the energy-efficiency and edges in blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>sALU Scheduler ADC Global row decoder Subarray Subarray addr bank block edge block edge block list edge list edge edge</head><label></label><figDesc></figDesc><table><row><cell>Row</cell><cell>multiplexer multiple p p xer</cell><cell>ReRAM crossbar ReRAM crossbar ReRAM ReRAM crossbar crossbar</cell><cell>ReRAM crossbar ReRAM crossbar ReRAM crossbar ReRAM crossbar</cell><cell>ReRAM crossbar ReRAM crossbar ReRAM crossbar ReRAM crossbar</cell></row><row><cell></cell><cell></cell><cell>Column Column Column Column</cell><cell>Column Column Column Column</cell><cell>Column Column Column Column</cell></row><row><cell></cell><cell></cell><cell>multiplexer multiplexer multiplexer p multiplexer</cell><cell>multiplexer multiplexer multiplexer p multiplexer</cell><cell>multiplexer multiplexer multiplexer p multiplexer</cell></row><row><cell></cell><cell></cell><cell>S/H S/H S/H S/H</cell><cell>S/H S/H S/H S/H</cell><cell>S/H S/H S/H S/H</cell></row><row><cell></cell><cell></cell><cell></cell><cell>I/O interface inte f f rface I</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Reg</cell><cell>S/A</cell><cell></cell></row></table><note><p>? Scheduler. The scheduler is responsible for vertex data loading and scheduling in GraphSAR. Because graphs are divided</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Comparison under different configurations (im- provement of GraphSAR compared with GraphR).</head><label>2</label><figDesc></figDesc><table><row><cell>Opt.target</cell><cell cols="2">Time Energy</cell></row><row><cell>Area optimized</cell><cell>1.31</cell><cell>2.18</cell></row><row><cell cols="2">Read latency optimized 19.40</cell><cell>22.52</cell></row><row><cell cols="2">Write latency optimized 9.53</cell><cell>10.07</cell></row><row><cell>Read energy optimized</cell><cell>6.77</cell><cell>174.68</cell></row><row><cell cols="2">Write energy optimized 6.34</cell><cell>149.75</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>If the number of vertices cannot be divided by 8, we will add some (at most 7) independent vertices without any edge connecting to them. In this way, the number of vertices can be divided by 8.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The write energy consumption used here (7.4 pJ) is much smaller than that used in GraphR(3.91nJ). The data used in GraphR are based on the whole ReRAM chip rather than ReRAM cell. We refer to<ref type="bibr" target="#b30">[31]</ref> to get a much lower write energy consumption. Because GraphR needs to write ReRAM cells more frequently than GraphSAR, GraphR benefits more from lower write energy consumption.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgement</head><p>This work was supported by <rs type="funder">Beijing National Research Center for Information Science and Technology (BNRist)</rs>, <rs type="funder">Beijing Innovation Center for Future Chip</rs>, the project of <rs type="institution">Science and Technology on Reliability Physics and Application Technology of Electronic Component Laboratory</rs> (<rs type="grantNumber">61428060401162806001</rs>), <rs type="funder">National Key R&amp;D Program of China</rs> <rs type="grantNumber">2017YFA0207600</rs>, and <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61622403</rs>, <rs type="grantNumber">61621091</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Pmtu3Kp">
					<idno type="grant-number">61428060401162806001</idno>
				</org>
				<org type="funding" xml:id="_awDmrEz">
					<idno type="grant-number">2017YFA0207600</idno>
				</org>
				<org type="funding" xml:id="_WaxbVZj">
					<idno type="grant-number">61622403</idno>
				</org>
				<org type="funding" xml:id="_qw2FKa4">
					<idno type="grant-number">61621091</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graphr: Accelerating graph processing using reram</title>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graphchi: Large-scale graph computation on just a pc</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">X-stream: Edge-centric graph processing using streaming partitions</title>
		<author>
			<persName><forename type="first">Amitabha</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gridgraph: Large-scale graph processing on a single machine using 2-level hierarchical partitioning</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ATC</title>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nxgraph: An efficient graph processing system on a single machine</title>
		<author>
			<persName><forename type="first">Yuze</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pregel: a system for large-scale graph processing</title>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Malewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed GraphLab: a framework for machine learning and data mining in the cloud</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="716" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graphicionado: A high-performance and energy-efficient accelerator for graph analytics</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fpgp: Graph processing framework on fpga a case study of breadth-first search</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphops: A dataflow library for graph analytics acceleration</title>
		<author>
			<persName><forename type="first">Tayo</forename><surname>Oguntebi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Foregraph: Exploring large-scale graph processing on multifpga architecture</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A scalable processing-in-memory accelerator for parallel graph processing</title>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graphh: A processing-in-memory architecture for large-scale graph processing</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCAD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graphp: Reducing communication of pim-based graph processing with efficient data partition</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="544" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High precision tuning of state for memristive devices by adaptable variation-tolerant algorithm</title>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Alibart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">75201</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overcoming the challenges of crossbar resistive memory architectures</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="476" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Technological exploration of rram crossbar array for matrix-vector multiplication</title>
		<author>
			<persName><forename type="first">Lixue</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCST</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel reram-based processing-in-memory architecture for graph computing</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NVMSA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hyve: Hybrid vertex-edge memory hierarchy for energyefficient graph processing</title>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DATE. EDA Consortium</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding the trade-offs in multi-level cell reram memory design</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gemini: A computation-centric distributed graph processing system</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="301" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Locality exists in graph processing: Workload characterization on an ivy bridge server</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Beamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IISWC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards memristor based accelerator for sparse matrix vector multiplication</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinru</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCAS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="121" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Design of cross-point metal-oxide reram emphasizing reliability and cost</title>
		<author>
			<persName><forename type="first">Dimin</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCAD</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prime: A novel processing-in-memory architecture for neural network computation in reram-based main memory</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">SNAP Datasets: Stanford large network dataset collection</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What is twitter, a social network or a news media?</title>
		<author>
			<persName><forename type="first">Haewoon</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Yahoo! altavista web page hyper-link connectivity graph</title>
		<author>
			<persName><forename type="first">Yahoo</forename><surname>Webscope</surname></persName>
		</author>
		<ptr target="http://webscope.sandbox.yahoo.com/" />
		<imprint>
			<date type="published" when="2002">2002. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nvsim: A circuit-level performance, energy, and area model for emerging non-volatile memory</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging Memory Technologies</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="15" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mnsim: Simulation platform for memristor-based neuromorphic computing system</title>
		<author>
			<persName><forename type="first">Lixue</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCAD</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1009" to="1022" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Investigating the switching dynamics and multilevel capability of bipolar metal oxide resistive switching memory</title>
		<author>
			<persName><forename type="first">Shimeng</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Physics Letters</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">103514</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cacti: An enhanced cache access and cycle time model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Wilton</surname></persName>
		</author>
		<author>
			<persName><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSSC</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="677" to="688" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adc performance survey 1997-2017</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Murmann</surname></persName>
		</author>
		<ptr target="http://web.stanford.edu/~murmann/adcsurvey.html" />
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
