<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoSA: Scheduling by Constrained Optimization for Spatial Accelerators Qijing Huang</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-05">5 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minwoo</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Grace</forename><surname>Dinh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Norell</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aravind</forename><forename type="middle">Kalaiah</forename><surname>Facebook</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Wawrzynek</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yakun</forename><forename type="middle">Sophia</forename><surname>Shao</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CoSA: Scheduling by Constrained Optimization for Spatial Accelerators Qijing Huang</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-05">5 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2105.01898v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>scheduling</term>
					<term>accelerator</term>
					<term>neural networks</term>
					<term>compiler optimizations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in Deep Neural Networks (DNNs) have led to active development of specialized DNN accelerators, many of which feature a large number of processing elements laid out spatially, together with a multi-level memory hierarchy and flexible interconnect. While DNN accelerators can take advantage of data reuse and achieve high peak throughput, they also expose a large number of runtime parameters to the programmers who need to explicitly manage how computation is scheduled both spatially and temporally. In fact, different scheduling choices can lead to wide variations in performance and efficiency, motivating the need for a fast and efficient search strategy to navigate the vast scheduling space.</p><p>To address this challenge, we present CoSA, a constrainedoptimization-based approach for scheduling DNN accelerators. As opposed to existing approaches that either rely on designers' heuristics or iterative methods to navigate the search space, CoSA expresses scheduling decisions as a constrained-optimization problem that can be deterministically solved using mathematical optimization techniques. Specifically, CoSA leverages the regularities in DNN operators and hardware to formulate the DNN scheduling space into a mixed-integer programming (MIP) problem with algorithmic and architectural constraints, which can be solved to automatically generate a highly efficient schedule in one shot. We demonstrate that CoSA-generated schedules significantly outperform state-of-the-art approaches by a geometric mean of up to 2.5× across a wide range of DNN networks while improving the time-to-solution by 90×.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep neural networks (DNNs) have gained major interest in recent years due to their robust ability to learn based on large amounts of data. DNN-based approaches have been applied to computer vision <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b56">[57]</ref>, machine translation <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b67">[68]</ref>, audio synthesis <ref type="bibr" target="#b65">[66]</ref>, recommendation models <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b45">[46]</ref>, autonomous driving <ref type="bibr" target="#b10">[11]</ref> and many other fields. Motivated by the high computational requirements of DNNs, there have been exciting developments in both research and commercial spaces in building specialized DNN accelerators for both edge <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b71">[72]</ref> and cloud applications <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b68">[69]</ref>.</p><p>State-of-the-art DNN accelerators typically incorporate large arrays of processing elements to boost parallelism, together with a deep multi-level memory hierarchy and a flexible networkon-chip (NoC) to improve data reuse. While these architectural structures can improve the performance and energy efficiency of DNN execution, they also expose a large number of scheduling parameters to programmers who must decide when and where each piece of computation and data movement is mapped onto the accelerators both spatially and temporally. Here, we use schedule to describe how a DNN layer is partitioned spatially and temporally to execute on specialized accelerators. Given a target DNN layer and a specific hardware architecture, there could be millions, or even billions, of valid schedules with a wide range of performance and energy efficiency <ref type="bibr" target="#b48">[49]</ref>. Considering the vast range of DNN layer dimensions and hardware architectures, there is a significant demand for a generalized framework to quickly produce efficient scheduling options for accelerators of varying hardware configurations.</p><p>Achieving high performance on a spatially distributed architecture requires several factors to be carefully considered, including tiling for good hardware utilization, pipelining data movement with compute, and maximizing data re-use. Previous scheduling frameworks have attempted to reflect these considerations by formulating an analytical cost model, pruning the scheduling space with known hardware constraints, and then exhaustively searching for the best candidate based on their cost models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b70">[71]</ref>. However, navigating the scheduling space in such a brute-force fashion can easily become intractable for larger DNN layers and more complex hardware architectures. Other notable efforts have employed feedback-driven approaches, such as black-box tuning, beam search, and other machine learning algorithms with iterative sampling <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b37">[38]</ref>. However, these schedulers typically require massive training datasets and large-scale simulations to learn performance models, making it infeasible to extend them to other types of hardware accelerators, especially those still under development. Hence, there is a clear need for efficient scheduling mechanisms to quickly navigate the search space and produce performant scheduling options.</p><p>In this work, we demonstrate CoSA, a constrainedoptimization-based approach to schedule DNN accelerators. In contrast to prior work that either requires exhaustive bruteforce-based or expensive feedback-driven approaches, CoSA expresses the DNN accelerator scheduling as a constrainedoptimization problem that can be deterministically solved using today's mathematical optimization libraries in one pass. In particular, CoSA leverages the regularities in both DNN layers and spatial hardware accelerators where the algorithmic and hardware parameters can be clearly defined as scheduling constraints. Specifically, CoSA formulates the DNN scheduling problem as a prime-factor allocation problem that determines 1) tiling sizes for different memory levels, 2) relative loop ordering to exploit reuse, and 3) how computation should be executed spatially and temporally. CoSA constructs the scheduling constraints by exposing both the algorithmic behaviors, e.g., layer dimensions, and hardware parameters, e.g., memory and network hierarchies. Together with clearly defined and composable objective functions, CoSA can solve the DNN scheduling problem in one shot without expensive iterative search. Our evaluation demonstrates that CoSA-generated schedules outperform state-of-the-art approaches by 2.5× across different DNN network layers, while requiring 90× less scheduling time as it does not require iterative search. In summary, this work makes the following contributions:</p><p>• We formulate DNN accelerator scheduling as a constrained-optimization problem that can be solved in a single pass. To the best of our knowledge, CoSA is the first constrained-optimization-based approach to tackle major DNN scheduling decisions in one shot. • We take a communication-oriented approach in the CoSA formulation that highlights the importance of data transfer across different on-chip memories and exposes the cost through clearly defined objective functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head><p>In this section, we discuss the complexity of DNN scheduling space and the state-of-the-art schedulers to navigate the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DNN Scheduling Space</head><p>Scheduling is a crucial decision-making process for the compilers to effectively assign workload to compute resources. With the emergence of numerous DNN accelerators with diverse architectures, there is a need for a fast, performant, and explainable approach to scheduling. Our work focuses</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scheduler</head><p>Search Algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Brute-force Approaches:</head><p>Timeloop <ref type="bibr" target="#b48">[49]</ref> Brute-force &amp; Random dMazeRunner <ref type="bibr" target="#b22">[23]</ref> Brute-force Triton <ref type="bibr" target="#b64">[65]</ref> Brute-force over powers of two Interstellar <ref type="bibr" target="#b70">[71]</ref> Brute-force Marvel <ref type="bibr" target="#b13">[14]</ref> Decoupled Brute-force</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feedback-based Approaches:</head><p>AutoTVM <ref type="bibr" target="#b14">[15]</ref> ML-based Iteration Halide <ref type="bibr" target="#b55">[56]</ref> Beamsearch <ref type="bibr" target="#b2">[3]</ref>, OpenTuner <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b44">[45]</ref> FlexFlow <ref type="bibr" target="#b37">[38]</ref> MCMC Gamma <ref type="bibr" target="#b39">[40]</ref> Genetic Algorithm Mind Mapping <ref type="bibr" target="#b34">[35]</ref> Gradient-based Search Constrained Optimization Approaches:</p><p>Polly+Pluto <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b29">[30]</ref> Tensor Comprehension <ref type="bibr" target="#b66">[67]</ref> Polyhedral Transformations Tiramisu <ref type="bibr" target="#b7">[8]</ref> CoSA Mixed-Integer Programming (MIP) on operator-level scheduling, which aims to optimize the performance of each operator, i.e. DNN layer, on specific hardware. Operator-level scheduling typically comprises three key loop optimizations: loop tiling, loop permutation, and spatial mapping. Loop tiling describes which loops are mapped to which memory hierarchy and the corresponding tile sizes.</p><p>Loop permutation determines the relative order of the loops, while spatial mapping binds one or more loop dimensions to spatial hardware resources, such as parallel processing elements, instead of mapping them to temporal (i.e. sequential) execution. Each optimization can have a significant impact on the performance, and all three optimizations need to be considered together to achieve the best performance. Consider scheduling a 3×3 convolution layer in ResNet50 <ref type="bibr" target="#b33">[34]</ref> with 256 input and output channels, and an output dimension of 14×14, on an accelerator with five levels of memory. If we split each individual loop bound into its prime factors and assign each one to a memory level, we would have billions of schedules to consider. Among the randomly sampled schedules from all possible loop tilings, half of them fail to satisfy the buffer capacity constraints (e.g. a schedule is invalid if it requires a 4KB buffer, though the available buffer size is only 2KB.). Fig. <ref type="figure" target="#fig_0">1</ref> shows the performance distribution of the valid schedules. We observe a wide performance difference among the valid schedules, with the best one outperforming the worst one by 7.2×. In addition, we observe clusters of schedules that have similar latencies in the Fig. <ref type="figure" target="#fig_0">1</ref>, revealing structure in the solution space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. State-of-the-art Schedulers</head><p>Given that the scheduling space for a DNN layer can have billions of valid schedules, finding a good schedule through exhaustive search can become an intractable problem. Table <ref type="table" target="#tab_1">I</ref> shows some recent efforts to tackle this complexity.</p><p>1) Brute-force Approaches: Recent efforts combine exhaustive search with heuristics to manually prune the scheduling </p><formula xml:id="formula_0">K Q P K R R R R R R R R R R R R R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Processing Element</head><p>Router Global Buffer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DRAM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN Accelerator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accumulation Buffer</head><p>Weight Buffer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs Weights Outputs</head><p>Input Buffer space <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b70">[71]</ref>. To lower the cost of exhaustive search, schedulers in this category typically use a lightweight analytical model to estimate latency, throughput, and power consumption to compare all valid mappings of a given layer to find the best schedule. The disadvantages of this approach are two-fold. First, such a brute-force search tends to be exceedingly expensive for complex hardware architectures, making it infeasible to find a good schedule quickly. Second, the generated schedules often do not perform optimally since analytical models may fail to consider the communication latency across the spatial hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MULT Adder</head><p>2) Feedback-based Approaches: Other recent efforts use feedback-driven approaches along with machine learning or other statistical methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b55">[56]</ref> to improve the accuracy of the cost model and search for the solution using black-box or gradient-based search. Although such approaches can potentially learn the distribution of the scheduling space, they typically require a large amount of training data due to their feedback-driven nature. As a result, these approaches are mainly applicable to post-silicon hardware where performing a large-scale measurement is possible but are not feasible for hardware under development.</p><p>3) Constrained-optimization Approaches: Constrainedoptimization problems, in which objective functions are maximized or minimized subject to given sets of constraints, have demonstrated the ability to solve many complex largescale problems in a reasonable time. Such methods have been widely used in architecture and systems research for instruction scheduling <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, high-level synthesis <ref type="bibr" target="#b21">[22]</ref>, memory partitioning <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b36">[37]</ref> [21], algorithm selection <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b72">[73]</ref>, and program synthesis <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b61">[62]</ref>.</p><p>In particular, polyhedral transformation has leveraged constrained-optimization-based approach for auto-vectorization and loop tiling <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Prior work targets general-purpose CPUs and GPUs that run with finegrained instructions and hardware-managed cache, as opposed to the software-managed spatial accelerators that we target. In addition, existing polyhedral-based approaches <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref> lack direct support for tile-size optimization. Instead, they take the tile size as input and apply a transformation based on the given tile size. Due to this limitation, the tile size decision cannot be co-optimized with other loop transformations, e.g. loop permutation, in one pass, leading to sub-optimal schedules.</p><p>To address the drawbacks of existing approaches and leverage the regularities from the DNN workloads and the accelerator design for optimization, CoSA employs constrained optimization to tackle the DNN scheduling problem in one pass. CoSA presents a unique domain-specific representation for DNN scheduling that better captures the utilization and communication cost and encodes different loop transformations, i.e., tiling size, loop permutation, and spatial mapping decisions, in one formulation. This unified representation enables us to solve for all three optimizations in one pass and produce efficient schedules for a complex accelerator system with a multi-level memory hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE COSA FRAMEWORK</head><p>To navigate the large scheduling space of DNN accelerators, we develop CoSA, a constrained-optimization-based DNN scheduler to automatically generate high-performance schedules for spatially distributed accelerators. CoSA not only deterministically solves for a good schedule in one pass without the need for exhaustive search or iterative sampling, but can also be easily applied to different network layers and hardware architectures. This section discusses the CoSA framework and how CoSA formulates the DNN scheduling problem with mixed-integer programming (MIP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CoSA Overview</head><p>CoSA optimizes operator-level schedules for mapping DNN layers onto spatial DNN accelerators. Specifically, CoSA formulates the scheduling problem as a constrained-optimization problem with variables representing the schedule, constraints representing DNN dimensions and hardware parameters, and objective functions representing goals, such as maximizing buffer utilization or achieving better parallelism. Fig. <ref type="figure">2</ref> shows the target problem space of CoSA. CoSA takes the specifications of the DNN layers and the underlying spatial accelerator as input constraints and generates a valid and high-performance schedule based on the objective functions in one pass.</p><p>1) Target Workload: The work targets the DNN operators that can be expressed by a nested loop with 7 variables as loop bounds: R, S, P, Q, C, K, N . R and S refer to the convolution kernel width and height, P and Q refer to the output width and height, C refers to the input channel size, K refers to the output channel size, and N refers to the batch size, as illustrated in Fig. <ref type="figure">2</ref>. The convolution operation computes the dot product of the filter size R × S × C of inputs and weights to generate one point in the output. Matrix multiplications can be expressed in this scheme as well.</p><p>2) Target Architecture: CoSA targets spatial architectures with an array of processing elements (PEs) connected via an on-chip network and with multiple levels of memory hierarchy, a commonly adopted architecture template in today's DNN accelerator designs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b70">[71]</ref>.</p><p>3) Target Scheduling Decisions: CoSA-generated schedules describe how a specified DNN layer is executed on a given spatial architecture. Listing 1 shows an example of a schedule. Here, we use a loop-nest representation <ref type="bibr" target="#b48">[49]</ref> to explicitly describe how the computation of a convolution layer is mapped to levels of memory hierarchies. We highlight three aspects of the schedule: 1) loop tiling, which describes which loops are mapped to which memory level and the values of the loop bounds; 2) loop permutation, which handles the relative ordering between loops in the same memory hierarchy; and 3) spatial mapping, which defines which loops are mapped to parallel spatial resources (shown as spatial_for loops in Listing 1). All three factors play a key role in the efficiency of the scheduling choice. Next, we highlight the implications of loop permutation and spatial mapping, both of which are less explored than the well-studied loop tiling. Fig.  convolution layer on a given hardware design. All the schedules use the same loop tiling and spatial mapping except the loop ordering at the global-buffer level, as indicated in the labels of the X-axis, where CKP means the input channel dimension (C) is the outermost loop, and the output height dimension (P) is the innermost loop. In this case, selecting P as the outermost loop, i.e. PCK and PKC, can lead to a 1.7× speedup for this layer, motivating the need to consider the implications of loop permutation in the scheduling problem. Fig. <ref type="figure" target="#fig_4">4</ref> shows the impact of spatial mapping on DNN execution. We notice that there is a 4.3× gap between best (rightmost) and worst (leftmost) schedules for the layer in consideration. The fundamental reason for the differences is the different communication traffic generated by different spatial mapping options. The best schedule, i.e., the rightmost schedule in the figure (s:P2C4K2, t:P2K2), is obtained when factors P = 2, C = 4, K = 2 are mapped to the spatial loops, which cannot be achieved by simply choosing either model or data parallelism in the spatial partition. As a result, a systematic evaluation of different spatial mapping choices is required to find a good schedule.</p><p>The rest of the section discusses how CoSA formulates the scheduling variables, constraints, and objectives to solve the DNN scheduling problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CoSA Variables and Constants</head><p>This section discusses the variables and constants, summarized in Table <ref type="table" target="#tab_4">II</ref>  DNN Layer: R = 3, S = 1, 1) Variable Representation: We devise a mathematical representation for the DNN schedules and formulate the scheduling problem as a prime-factor allocation problem. Given a layer specification, we first factorize each loop bound into its prime_f actors. If the loop bound themselves are large prime number, we can pad them and then factorize. We assign each prime factor to a scheduling configuration that is composed of a combination of three decisions: 1) the mapped memory level, 2) the permutation order, and 3) the spatial mapping. Each prime factor has exactly one scheduling configuration.</p><formula xml:id="formula_1">P = 1, Q = 1, C = 1, K = 4, N = 3 − → Prime Factors: = [[3],[1],[1],[1],[1],[</formula><p>Here, we use a binary matrix X to represent the prime factor allocation, i.e., the scheduling space, shown in Table <ref type="table" target="#tab_5">III</ref>. The four dimensions of X are: 1) the layer dimension variables (indexed by j), 2) the prime factors of the loop bounds (indexed by n), 3) whether it is a spatial or temporal mapping (indexed by k), and 4) the memory and the permutation levels (indexed by i). With the prime factor decomposition, CoSA's encoding can represent all possible schedules and guarantees that the optimization solves for the full search space. Table <ref type="table" target="#tab_5">III</ref> shows an example binary matrix X that represents the schedule shown in Listing 1. First, CoSA performs the tiling optimizations by assigning the prime factors to different memory levels. For example, dimension K is split into two tiles, where the inner tile of size 2 is allocated to the input buffer, and the outer tile of size 2 is allocated in the global buffer. Second, mapping a prime factor to spatial execution is indicated by whether the factor is mapped to a spatial column s or a temporal column t in the table. In this example, both prime factors for K are spatially mapped. Finally, for loop permutation, we add rank indices O 0 , O 1 , ..., O Z to the memory level of interest, where only one prime factor can be mapped to each rank. The lowest-ranked factor is allocated to the innermost loop, while the highest-ranked factor is allocated to the outermost loop.</p><p>In the example shown in Table <ref type="table" target="#tab_5">III</ref>, the problem dimension N is mapped at the O 1 level in the global buffer for temporal mapping, which means the factor N = 3 will be assigned rank 1 in the global-buffer level. Without other factors in the global-buffer level, factor N = 3 with the smallest rank will become the innermost loop in permutation. For the ranking of permutation, we reserve enough slots for all prime factors at all memory levels. Not all the slots need to be filled since a prime factor can only be allocated to one memory level.</p><formula xml:id="formula_2">Related Idx W IA OA v R - j S - P Q C K N Related Idx W IA OA v Register i AccBuf WBuf InputBuf GlobalBuf DRAM</formula><p>2) Constant Parameters: In addition to the loop-related variables, we have intrinsic relations across different components in the architecture and layer specifications which must be encoded by constant parameters. CoSA uses two constant binary matrices to encode the unique relations in the DNN scheduling space, shown in Tabel IV. The first binary constant matrix, A, encodes the association between layer dimensions (i.e., rows of the matrix) and data tensors (i.e., columns of the matrix). For each input (IA), weight (W), and output (OA) tensor, matrix A indicates which layer dimensions, i.e., R, S, P, Q, C, K, N , should be used to calculate the data transaction size as well as multicast and reduction traffic on the accelerators.</p><p>In addition, we introduce another binary matrix B to represent which memory hierarchy can be used to store which data tensor. DNN accelerators typically deploy a multi-level memory hierarchy, where each memory level can be used to store different types of data tensors. For example, matrix B shown in Table <ref type="table" target="#tab_6">IV</ref> represents an architecture that has dedicated input and weight buffers for input activation and weight, respectively, while providing a shared global buffer to store input and output activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CoSA Constraints</head><p>This section discusses the constraints derived from the target accelerator architecture that must be satisfied in CoSA and shows how to express them with CoSA variables and constants.</p><p>1) Buffer Capacity Constraint: To generate a valid schedule in a software-managed memory system, a key constraint is to ensure that the size of data to be sent to the buffer does not exceed the buffer capacity. The hardware memory hierarchy can be represented by the binary constant matrix B discussed earlier. For each memory buffer, based on the tensor-dimension correlation matrix A, we calculate the tiling size of each tensor by multiplying the relevant prime factors together indicated by X. Both spatial and temporal factors should be included in the buffer utilization. Let N j be the number of prime factors for the layer dimension j. Then the utilization of the buffer level I can be expressed as:</p><formula xml:id="formula_3">I−1 i=0 6, Nj j=0,n=0 1 k=0 prime_f actor j,n , X (j,n),i,k A j,v B I,v = 1 1, otherwise<label>(1)</label></formula><p>We then set the upper bound of the buffer utilization to the capacity of different buffer sizes, represented using M I,v . However, a problem with this utilization constraint is that it involves products of the decision variables X, making it nonlinear and infeasible to solve with standard constraint solvers. To address this limitation, we take the logarithm of both sides of the constraints to obtain a linear expression for the utilization and encode the if-else statement as:</p><formula xml:id="formula_4">U I,v = I−1 i=0 6, Nj j=0,n=0 1 k=0 log(prime_f actor j,n )A j,v B I,v X (j,n),i,k ≤ log(M I,v ), ∀I<label>(2)</label></formula><p>To encode different precisions for different data tensors, we add the logarithm of the datatype sizes precision v to U I,v .</p><p>2) Spatial Resource Constraint: Another set of CoSA constraints is from the limited number of spatial resources. At the chip level, there is a limited number of PEs. At the PE level, there is a limited number of multiply-and-accumulate (MAC) units. In CoSA, once a factor is assigned to spatial mapping in the configuration, it needs to satisfy: 1) each problem factor can only be mapped to either spatial or temporal execution, 2) factors that map to spatial execution do not exceed the resource limit in the architecture. These two constraints can be expressed in the equations below:</p><formula xml:id="formula_5">1 k=0 X (j,n),i,k == 1, ∀(j, n), i<label>(3)</label></formula><p>6, Nj j=0,n=0 log(prime_f actor j,n )X (j,n),I,0 ≤ log(S I ), ∀I</p><p>where S I is the number of available spatial resources at the level I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Objective Functions</head><p>In this section, we describe the objective functions for CoSA. Each objective can be either used individually to optimize a single aspect of performance, e.g., utilization, compute, and communication, or combined with others.</p><p>1) Utilization-Driven Objective: High on-chip buffer utilization improves data-reuse opportunity. As demonstrated in the prior work <ref type="bibr" target="#b24">[25]</ref>, communication lower bounds can be achieved when the tiling block size is optimized for buffer utilization in a system with one-level cache. In this work, we formulate a utilization objective that aims to maximize the buffer utilization of all tensors, so the overall communication is minimized. We use the same formulation for the buffer utilization as in III-C1 and maximize the following linear utilization function:</p><formula xml:id="formula_7">Û til = I−1 i=0 2 v=0 U i,v<label>(5)</label></formula><p>Here, maximizing the sum of utilization for all buffer levels and all tensors in the logarithm form is equivalent to maximizing the geometric mean of the buffer utilization. Users can also attach weights to the different buffer levels or different data tensors if they want to optimize for the utilization of a specific level of the memory.</p><p>2) Compute-Driven Objective: The total number of compute cycles is another factor that affects the quality of schedules. In this formulation, we multiply all the temporal factors for the estimated compute cycles in each PE. Intuitively, this objective allows the constraint solver to exploit the parallelism in the system by mapping more iterations to the spatial resources than to temporal iterations. The objective can be expressed as a linear function again with logarithm taken:</p><formula xml:id="formula_8">Ĉomp = I i=0 6, Nj j=0,n=0 log(prime_f actor j,n )X (j,n),i,1 (6)</formula><p>3) Traffic-Driven Objective: Communication latency is a key contributing factor to the performance of spatial architecture. CoSA also includes a traffic-driven objective to capture the communication cost. Specifically, communication traffic can be decomposed into three terms: 1) data size per transfer, 2) spatial factors of multicast and unicast traffic, and 3) temporal iterations. Multiplying these three factors will get the total amount of traffic in the network. Next, we discuss how we capture each of these factors using CoSA's representation.</p><p>First, similar to the buffer utilization expression, data size per transfer can computed using the allocated prime factors in matrix X, together with the dimension-tensor correlation matrix A, as shown in the equation below:</p><formula xml:id="formula_9">D v = I−1 i=0 6, Nj j=0,n=0 1 k=0 log(prime_f actor j,n )A j,v X (j,n),i,k<label>(7)</label></formula><p>Second, spatial factors would incur different multicast, unicast, and reduction patterns. The dimension-tensor correlation matrix A discussed in Sec III-B2 can be used to indicate different traffic patters. Specifically, depending on whether the spatial dimension, indicated by the binary matrix X, is related to the specific tensor in consideration, represented by the constant matrix A, different traffic patterns, e.g., multicast vs. unicast or reduction vs. unicast, would occur. Fig. <ref type="figure">5</ref> shows how the intrinsic tensor-dimension correlation matrix A can be used to calculate different traffic patterns for different variables. For example, as shown in Fig. <ref type="figure">5a</ref>, if the dimension P is mapped spatially, A P,W = 0 implies multicast traffic for weight tensor W. Since weight is not related to P , when we send weights from global buffer to PEs, the weight traffic will be multicasted to the destination PEs. If the dimension C is mapped spatially, A C,W = 1 (Fig. <ref type="figure">5b</ref>) implies unicast traffic for weight tensor W as weight is related to C. Similarly, if the dimension C is mapped spatially, A C,OA = 0 (Fig. <ref type="figure">5c</ref>) implies reduction traffic for output tensor OA, where partially sum needs to be reduced across C before sending back to GB. If the dimension P is mapped spatially, (Fig. <ref type="figure">5d</ref>) would indicate unicast traffic for output tensor OA, as each traffic contributes to different regions of the output.</p><formula xml:id="formula_10">A P,OA = 1 R R R R R R R R R R R R R PE Router GB R R R R R R R R R R R R R R R R R R R R a. Multicast: A P,W = 0 R R R R R R R R GB R R R R R R R R R R R R R R R R GB R R R R R R R</formula><p>CoSA formulates this relationship in the following equation:</p><formula xml:id="formula_11">L v = 6, Nj j=0,n=0 log(prime_f actor j,n )X (j,n),I,0 A j,v<label>(8)</label></formula><p>The third term, temporal iteration is used to calculate the number of data transfers at the NoC level. We introduce a traffic iteration factor Y that is a function of X at the permutation level, A, and B. Y indicates if the outer NoC loop bound should be used for different variables. With Y, we ensure that, for each variable, if a relevant factor term is seen inside the current loop level, the current loop level's factor should be used to compute the traffic iteration regardless of whether it is related to the data tensor of the variable of interest. This is a term that drives the reuse optimization. Mathematically, Y is constrained as:</p><formula xml:id="formula_12">Y v,z ≥ 6, Nj j=0,n=0 X (j,n),z,1 A j,v B I,v , ∀z, ∀v Y v,z ≥ Y v,z−1 , ∀z &gt; 0, ∀v<label>(9)</label></formula><p>Where z represents the position index for permutation and Z equals the total valid levels for permutation. The traffic iteration term can thus be expressed as: <ref type="bibr" target="#b9">(10)</ref> This turns the linear objective into quadratic as we multiply Y with X to indicate whether there is a factor at the current permutation level.</p><formula xml:id="formula_13">T v = Z−1 z=0 6,Nj j=0,n=0 log(prime_f actor j,n )Y v,z X (j,n),z,1</formula><p>After we calculate each individual term, we can combine them together for each tensor that contributes to the total traffic in the network. Similar to the logarithmic transformation we did earlier, instead of multiplying these three terms together, we take the logarithm on both sides to get a linear expression of the traffic, as shown in the equation below:</p><formula xml:id="formula_14">T raf = 2 v=0 (D v + L v + T v ) (11)</formula><p>4) Overall Objective: One can construct a composite objective comprised of a linear combination of Û til, Ĉomp, and T</p><p>raf , where we want to minimize the compute and communication latency while maximizing the on-chip buffer utilization:</p><formula xml:id="formula_15">Ô = −w U Û til + w C Ĉomp + w T T raf (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>where w U , w T , w C are user-selected parameters controlling the importance of each objective. For a system with doublebuffering optimization, w T can be set to map the traffic sizes to the cycles for memory accesses. This brings w T T raf to be of the same importance as w C Ĉomp in the optimization. Another formulation of the overall objective function to balance the memory access and compute cycles is to minimize the difference of the two terms: D = w T T raf − w C Ĉomp. The weights of different objectives can be determined by using a set of micro-benchmarks that characterize the compute, memory, and communication latencies of the target architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Limitation of CoSA</head><p>CoSA leverages the regularity from both the problem and the architecture space, where it assumes a dense CNN workload and does not exploit the sparsity of the data. It also best targets hardware systems with deterministic behavior and explicitly managed scratchpads. This is because, in systems with nondeterministic behaviors, it can be challenging to construct optimization objectives that capture the impact of such behaviors. However, CoSA can be augmented with an iterative search on the objective functions and their corresponding hyperparameters to approximate the unknown hardware performance model and directly prune off the invalid points from the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>This section discusses the evaluation platforms we use followed by the experimental setup for CoSA evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Platforms</head><p>We evaluate the schedules generated by CoSA on two platforms: 1) Timeloop for cycle performance and energy consumption, and 2) our cycle-exact NoC simulator for overall latency performance. The latter more accurately captures the communication overhead and concurrent hardware behaviors on a spatial architecture.</p><p>Timeloop provides microarchitecture and technologyspecific energy models for estimating the performance and energy on DNN accelerators. Timeloop reports the performance in terms of the maximum cycles required for each processing element to complete the workload and to perform memory accesses, assuming perfect latency hiding with double buffering. The energy consumption in Timeloop is calculated by multiplying the access count on each hardware component with the energy per access and summing the products up. The access count is inferred from the schedule and the energy per access is provided by an energy reference table in Timeloop.</p><p>NoC Simulator augments the Timeloop analytical compute model for PEs with a synthesizable NoC implementation to reflect the communication cost. Communication is one of the key contributing factors for latency in a NoC-based system, especially for the communication bound schedules.</p><p>The NoC simulator is transaction-based and cycle-exact for modeling the on-chip traffic. Leveraging the synthesizable SystemC router design from Matchlib <ref type="bibr" target="#b40">[41]</ref> that supports unicast and multicast requests, we construct a resizable 2-D mesh network and implement an X-Y routing scheme. The simulator captures both computation and communication latencies by concurrently modeling data transfers in the NoC, the PE executions, and off-chip DRAM accesses based on the DRAMSim2 model <ref type="bibr" target="#b57">[58]</ref>, where the impact of traffic congestion on the NoC can also be manifested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Schedulers</head><p>We evaluate CoSA with respect to two other scheduling schemes: 1) a Random scheduler that searches for five different valid schedules, from which we choose the one with the best result for the target metric, and 2) the Timeloop Hybrid mapper in Timeloop <ref type="bibr" target="#b48">[49]</ref> that randomly selects a tiling factorization, prunes superfluous permutations, and then linearly explores the pruned subspace of mappings before it proceeds to the next random factorization. For this mapper, we keep the default termination condition where each thread self-terminates after visiting 500 consecutive mappings that are valid yet sub-optimal. The mapper is run with 32 threads, each of which independently searches the scheduling space until its termination condition is met. Once all threads have terminated, Timeloop returns the best schedule obtained from all 16,000+ valid schedules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Setup</head><p>Mixed-Integer Program (MIP) Solver: CoSA uses Gurobi <ref type="bibr" target="#b31">[32]</ref>, a general-purpose optimization solver for MIP and other constrained programming, as the solver. We specify the CoSA variables, constraints, and objective functions before we invoke the solver. The solver takes at most seconds to return a schedule for DNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN workloads:</head><p>We measure the performance of CoSAgenerated schedules over a wide range of DNN workloads targeting different DNN tasks with diverse layer dimensions, including: ResNet-50 <ref type="bibr" target="#b33">[34]</ref>, ResNeXt-50 (32x4d) <ref type="bibr" target="#b69">[70]</ref>, and Deepbench <ref type="bibr" target="#b23">[24]</ref> (OCR and Face Recognition). The precision used for the benchmarks is 8-bit for the input and weights, and 24-bit for the partial sums. We do not pad the dimensions to be multiples of 2, as it incurs more overhead and outweighs the benefits it provides to allow more scheduling options.</p><p>Baseline architecture: We consider a spatial-array architecture like Simba <ref type="bibr" target="#b58">[59]</ref> as our baseline. Detailed specifications of the hardware constructs are summarized in Table <ref type="table" target="#tab_7">V</ref>. We demonstrate that the CoSA framework is general to be applied for different architecture parameters while delivering highperformance scheduling options in one shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>In this section, we demonstrate the improved time-to-solution, performance, and energy of CoSA compared to baseline schedulers, across different evaluation platforms and different DNN architectures on a diverse set of DNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Time to Solution</head><p>We compare the average time for CoSA and the baseline schedulers to generate the schedule of each layer from the four target DNN workloads. Table <ref type="table" target="#tab_9">VI</ref> shows that CoSA's optimization-driven approach offers more than 90× (4.2s vs. 379.9s) time-to-solution advantage over the Timeloop Hybrid search strategy. Timeloop Hybrid search sampled 67 million schedules per layer and evaluated more than 16 thousand valid ones among them, leading to a long runtime. With Random search, a random sampling of 20K samples in 4.6 seconds resulted in only five valid schedules, further demonstrating the need to have a constraint-based strategy to prune the invalid search space directly. In the following section, we show that CoSA not only shortens the time-to-solution but also generates high-quality schedules.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation on Timeloop Performance and Energy Models</head><p>We compare the performance of the Random search, the Timeloop Hybrid mapper, and the CoSA scheduler for four different DNN workloads. The evaluations are based on our baseline architecture described in Table V and the Timeloop evaluation platform mentioned in Section IV-A.</p><p>1) Performance: Fig. <ref type="figure">6</ref> shows the speedup reported by Timeloop for different scheduling schemes relative to Random search. Fig. <ref type="figure">6</ref> demonstrates that the CoSA-generated schedules are not only valid but also outperform the ones generated by both Random search and Timeloop Hybrid search. The geometric mean of the speedups of CoSA schedules relative to the Random and Timeloop Hybrid search ones are 5.2× and 1.5× respectively across four DNNs.</p><p>In the few layers where Timeloop Hybrid search slightly outperforms CoSA, we find a higher iteration count at the 3_7_512_512_1. The goal is to minimize the total objective in Eq. 12.</p><p>CoSA achieves the lowest values for all objective functions on this layer among all approaches.</p><p>DRAM level in Timeloop Hybrid schedules, which helps to reduce the size of each DRAM transaction and balance the pipeline. Fine tuning the weights of the objective functions could be used to further improve the CoSA-generated schedules.</p><p>A more exhaustive Timeloop Hybrid search (32K valid schedules) results in an improvement of only 7.5% in latency while increasing runtime by 2×. We find that even with 2× more valid samples evaluated, Timeloop Hybrid search still cannot generate schedules that are of similar efficiency to CoSA.</p><p>2) Energy: We use the Timeloop energy model to evaluate the energy of different schedules. Because energy cost is highly correlated with the access count on each hardware component, our traffic objective in CoSA is used for the schedule optimization targeting energy efficiency. Fig. <ref type="figure">7</ref>  selected from 16,000+ valid schedules optimizing the energy.</p><p>3) Objective Breakdown: A detailed breakdown of the CoSA objective function on ResNet50 layer 3_7_512_512_1 is included in Fig. <ref type="figure" target="#fig_7">8</ref>. Our overall objective function aims to capture an optimization heuristic to maximize the utilization and minimize the compute and traffic costs at the same time with a weighted sum of the three. Fig. <ref type="figure" target="#fig_7">8</ref> shows that CoSA achieves the lowest total objective among all approaches, and optimizes all three sub-objectives simultaneously. This observation on the objective values aligns with our empirical results in Fig. <ref type="figure">6</ref>, where CoSA schedule runs 7× faster than the ones generated by Random and Timeloop Hybrid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Different HW Architectures:</head><p>We further explore the performance of CoSA with different DNN architecture parameters such as different PE array sizes and different SRAM buffer sizes. We apply the same weights for the evaluation on the same architecture and customize the objective weights in Eqn.12 using a micro-benchmark for different architectures. Fig. <ref type="figure">9</ref> shows the geomean speedup of CoSA across all networks on two different hardware architectures.</p><p>PE Array Dimension. We scale the number of PEs up by 4× and increase both the on-chip communication and DRAM bandwidth by 2× correspondingly. Both of these modifications significantly impact the compute and communication patterns of DNN layer executions. With a larger spatial array of arithmetic units, this case study presents a scheduling problem where decisions about spatial and temporal mapping can be especially crucial to attaining high performance. Fig. <ref type="figure">9a</ref> shows that CoSA achieves 4.4× and 1.1× speedup compared to Random and Timeloop Hybrid search respectively across four networks. This shows that the performance of our scheduler can scale and generalize to NoCs with more PEs, which tend to be more </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation on NoC Simulator</head><p>To further compare the quality of schedules generated by different scheduling schemes, we evaluate them on our NoC simulation platform. The NoC simulation platform more accurately captures the communication overhead from the onchip network as compared to the Timeloop models. Fig. <ref type="figure" target="#fig_0">10</ref> shows the speedup relative to the Random baseline. We observe that CoSA-generated schedules outperform the baseline schedules for all four DNN workloads, with the greatest performance gains occurring for convolutional layers, e.g. DeepBench layers. Intriguingly, for these same layers, Timeloop Hybrid scheduler actually under-performs Random search as its internal analytical model does not accurately capture the communication traffic in the network. On the other hand, there is no significant difference between the performance of FC layers among different schedules, as the FC layers are heavily memory-bound with low PE utilization. The DRAM access time dominates in these layers even with the best schedules with respect to reuse of buffered data.</p><p>Overall, CoSA achieves a geometric average of up to 3.3× speedup relative to the best Random search solutions and 2.5× relative to Timeloop Hybrid search schedules across the four networks. Furthermore, unlike the iterative nature of Random and Timeloop Hybrid search schedules, CoSA schedules are consistently performant with the one-shot solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation on GPU</head><p>To show the potential use of CoSA for general-purpose hardware, we also formulate GPU scheduling as a constrainedoptimization problem using CoSA. We evaluate the performance of CoSA on GPU and compare it against TVM <ref type="bibr" target="#b14">[15]</ref>.</p><p>Target GPU. We target NVIDIA K80 GPU with 2496 CUDA cores and a 1.5MB L2 cache. This GPU has a 48KB shared memory and 64KB local registers, shared by a maximum of 1024 threads in each CUDA thread block. The thread block is a programming abstraction that represents a group of threads that can be run serially or in parallel in CUDA. The maximum dimension of a thread block is (1024, 1024, 64). Violating these constraints in the CUDA kernel results in invalid schedules.</p><p>Constraints. CoSA expresses the hardware constraints for GPU thread groups and shared/local memory similarly to how we specify the spatial resource and buffer capacity constraints in Section III-C. Each thread group can be seen as a spatial level with a specific size. The product of all three thread group sizes is enforced to be smaller than 1024. The share memory utilization is calculated as buffer capacity constraints, and the register utilization is calculated by multiplying the total number of threads with the inner loop register utilization.</p><p>Objective Functions. In CoSA, we compute the compute objective by discounting the total compute cycles with the total number of threads for GPU, to reflect the performance gain from thread-level parallelism. We then adjust the weights of the other objectives using a micro-benchmark.</p><p>We run TVM with the XGBoost tuner for 50 trials per layer as the baseline. CoSA generates valid schedules in one shot with a time-to-solution 2, 500× shorter than TVM (0.02s vs. 50s per layer). The CoSA-generated schedules achieve 1.10× geomean speedup compared to the TVM schedules on ResNet50 as shown in Fig. <ref type="figure" target="#fig_0">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we present CoSA, an optimization-driven approach to DNN scheduling. Harnessing the regularities from DNN workloads and target accelerator designs, we formulate scheduling into a constrained optimization problem that can be solved directly without incurring the high cost of iterative scheduling. We devise a single mathematical formulation to simultaneously solve for all three key optimizations in scheduling: loop tiling, loop permutation, and spatial mapping. Comparing our results to schedules generated from the stateof-the-art work, our approach achieves up to 2.5× speedup and 22% better energy-efficiency, with 90× shorter time-tosolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Execution latency histogram of 40K valid scheduling choices for a ResNet-50 layer on a spatial accelerator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>weight width and height P, Q: output width and height W,H: input width and height C: input channel size K: output channel size N: batch size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :Listing 1 :</head><label>21</label><figDesc>Fig. 2: DNN scheduling problem formulation with CoSA. CoSA takes 1) DNN layer dimensions and 2) DNN accelerator parameters and expresses the scheduling problem into a constrained optimization problem to produce a performant schedule in one shot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Performance comparison of schedules with different loop permutations for a convolution operator with the layer dimensions of R = S = 3, P = Q = 8, C = 32, K = 1024. The leftmost schedule (CKP) refers to a relative ordering where the input channel dimension (C) is the outermost loop and the output height dimension (P) is the innermost loop. Since this layer is weight-heavy, loop permutations that emphasize weight reuse, e.g., PCK and PKC, are more efficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Performance comparison of schedules with different spatial mappings for a convolution operator with the layer dimensions of R = S = 1, P = Q = 16, C = 256, K = 1024. Factors in s list are for spatial mapping, and factors in t list are for temporal mapping. For example, s:P4C4,t:K4 represents a mapping where a factor 4 of the P dimension and a factor 4 of the C dimension are mapped to spatial execution in a system with 16 PEs, leaving K's factor 4 to temporal mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 GlobalFig. 5 :</head><label>15</label><figDesc>Fig. 5: Different traffic patterns based on the constant matrix A. The two figures (top) show how the constant A encodes the traffic types (multicast, unicast, reducation) for different data tensors from the global buffer to PEs. The figures on the bottom show its implication on output tensor reduction traffics.</figDesc><graphic url="image-4.png" coords="7,91.81,185.04,56.04,59.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 OVERALLFig. 6 :Fig. 7 :</head><label>267</label><figDesc>Fig. 6: Speedup of different schedules relative to Random search on the baseline 4×4 NoC architecture. X-axis labels follow the naming convention R_P_C_K_Stride where S = R and Q = P in all workloads. CoSA achieves 5.2× and 1.5× higher geomean speedup across four DNN workloads compared to the Random and Timeloop Hybrid search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Objective function breakdown for ResNet-50 layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>State-of-the-art DNN accelerator schedulers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, used in CoSA formulation.</figDesc><table><row><cell cols="2">CoSA Variables</cell><cell>CoSA Constants</cell><cell></cell><cell>Indices</cell></row><row><cell>X</cell><cell>binary matrix to represent a schedule</cell><cell>A layer dimension to data tensor mapping B memory level to data tensor mapping</cell><cell cols="2">i j n prime factor index memory level layer dimension k mapping choice</cell></row><row><cell></cell><cell></cell><cell></cell><cell>z</cell><cell>permutation level</cell></row><row><cell></cell><cell></cell><cell></cell><cell>v</cell><cell>data tensor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>CoSA Notations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Example binary matrix X representing a schedule. A checkmark in s, t indicates spatial or temporal mapping. A checkmark in O0, ..., OZ indicates the rank for loop permutation. In this schedule, the loop tile of size 3 from problem dimension N is allocated within the GlobalBuf at the innermost loop level, assigned for temporal execution. Both loop tiles from K are mapped to spatial resources.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2,2][3]]</cell></row><row><cell>Idx</cell><cell></cell><cell></cell><cell>Perm</cell><cell></cell><cell>Schedule</cell><cell></cell></row><row><cell>j</cell><cell></cell><cell>Layer Dim.</cell><cell cols="2">R = 3 ...</cell><cell>K = 4</cell><cell></cell><cell>N = 3</cell></row><row><cell>n</cell><cell cols="2">Prime Factors</cell><cell>3</cell><cell>...</cell><cell>2</cell><cell>2</cell><cell>3</cell></row><row><cell>k</cell><cell cols="2">s / t Mapping</cell><cell>s t</cell><cell></cell><cell cols="3">s t s t s t</cell></row><row><cell></cell><cell></cell><cell>Register</cell><cell>...</cell><cell></cell><cell></cell><cell></cell></row><row><cell>i</cell><cell>... Memory Levels</cell><cell>... InputBuf GlobalBuf</cell><cell>... O 0 O 1 O 2 ...</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>O Z</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Constant binary matrices A (left) and B (right). A encodes how different layer dimensions associate with data tensors. B encodes which data tensor can be stored in which memory hierarchy.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>The baseline DNN accelerator architecture.</figDesc><table><row><cell>Arithmetic :</cell><cell></cell><cell>Storage :</cell><cell></cell><cell>Network :</cell><cell></cell></row><row><cell>MACs</cell><cell>64 / PE</cell><cell>Registers</cell><cell>64B / PE</cell><cell>Dimension</cell><cell>4×4</cell></row><row><cell>Weight/Input Precision</cell><cell>8bit</cell><cell cols="2">Accum. Buffer 3KB / PE Weight Buffer 32KB / PE</cell><cell>Router Flit Size</cell><cell>Wormhole 64b</cell></row><row><cell>Partial-Sum Precision</cell><cell>24bit</cell><cell>Input Buffer Global Buffer</cell><cell>8KB / PE 128KB</cell><cell>Routing Multicast</cell><cell>X-Y Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Time-to-solution Comparison. CoSA outputs only one valid schedule per layer. CoSA's runtime is 1.1× and 90× shorter than the Random and Timeloop Hybrid search, respectively.</figDesc><table><row><cell>0 1 2 3 4 5 6 7 8 9 10 Speedup [Platform: Timeloop]</cell><cell>11_55_3_64_4</cell><cell cols="6">5_27_64_192_1 11 69 13 3_13_192_384_1 3_13_384_256_1 11 AlexNet 3_13_256_256_1 1_1_9216_4096_1</cell><cell>1_1_4096_4096_1</cell><cell>1_1_4096_1000_1</cell><cell cols="2">GEOMEAN</cell><cell>0 1 2 3 4 5 6 7 8 9 10</cell><cell>7_112_3_64_2</cell><cell>1_56_64_64_1</cell><cell>3_56_64_64_1</cell><cell cols="2">1_56_64_256_1 23 23</cell><cell>1_56_256_64_1</cell><cell>1_56_256_128_1</cell><cell cols="2">3_28_128_128_2 13</cell><cell>1_28_128_512_1</cell><cell></cell><cell>1_28_256_512_2</cell><cell cols="2">1_28_512_128_1 14</cell><cell>1_28_512_256_1</cell><cell cols="3">3_14_256_256_2 ResNet-50 1_14_256_1024_1 1_14_512_1024_2</cell><cell>1_14_1024_256_1</cell><cell>3_14_256_256_1 Random 1_14_1024_512_1</cell><cell>3_7_512_512_2 19</cell><cell>1_7_512_2048_1 Timeloop Hybrid 1_7_1024_2048_2 1_7_2048_512_1 15</cell><cell>3_7_512_512_1</cell><cell>1_1_2048_1000_1</cell><cell>GEOMEAN CoSA</cell></row><row><cell>0 1 2 3 4 5 6 7 8 9 10 Speedup [Platform: Timeloop]</cell><cell>7_112_3_64_2</cell><cell>1_56_64_128_1</cell><cell>3_56_4_128_1</cell><cell>1_56_128_256_1</cell><cell>1_56_64_256_1</cell><cell>1_56_256_128_1</cell><cell>1_56_256_256_1</cell><cell>3_28_8_256_2</cell><cell>1_28_256_512_1</cell><cell>1_28_256_512_2</cell><cell cols="6">1_28_512_256_1 ResNeXt-50 (32x4d) 3_28_8_256_1 1_28_512_512_1 3_14_16_512_2 1_14_512_1024_1 1_14_512_1024_2 15 15 13</cell><cell>1_14_1024_512_1</cell><cell cols="2">3_14_16_512_1 11 12 16 1_14_1024_1024_1</cell><cell>3_7_32_1024_2</cell><cell cols="2">1_7_1024_2048_1</cell><cell>1_7_1024_2048_2</cell><cell cols="2">1_7_2048_1024_1 29</cell><cell>3_7_32_1024_1</cell><cell>1_1_2048_1000_1</cell><cell>GEOMEAN</cell><cell>3_480_1_16_1</cell><cell>3_240_16_32_1</cell><cell>3_120_32_64_1</cell><cell>3_60_64_128_1</cell><cell>3_108_3_64_2</cell><cell>3_54_64_64_1</cell><cell>3_27_128_128_1</cell><cell>3_14_128_256_1</cell><cell>3_7_256_512_1</cell><cell>GEOMEAN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>demonstrates that CoSA, using no simulation feedback, can generate schedules 22% more energy-efficient than the best Timeloop Hybrid solutions Speedup relative to Random search reported by Timeloop model on different hardware architectures. CoSA's performance generalizes across different hardware architectures with different computing and on-chip storage resources. Speedup reported by NoC simulator relative to Random search on the baseline 4×4 NoC architecture. CoSA achieves 3.3× and 2.5× higher geomean speedup across four DNN workloads compared to the Random and Timeloop Hybrid search on the more communication sensitive NoC simulator.</figDesc><table><row><cell>Speedup [Platform: Timeloop]</cell><cell>0 1 2 3 4 5 6</cell><cell></cell><cell cols="3">AlexNet</cell><cell></cell><cell cols="3">ResNet-50</cell><cell cols="9">ResNeXt-50 DeepBench Random Timeloop Hybrid</cell><cell cols="5">GEOMEAN 1.0 4.0 4.4 CoSA</cell><cell cols="2">Speedup [Platform: Timeloop]</cell><cell>0 1 2 3 4 5 6 7 8 9 10</cell><cell></cell><cell cols="2">AlexNet</cell><cell>ResNet-50 ResNeXt-50 DeepBench GEOMEAN 1.0 4.1 5.7 Random Timeloop Hybrid CoSA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(a) 8 × 8 PEs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Larger Buffers</cell></row><row><cell cols="2">Fig. 9: 11_55_3_64_4 0 1 2 3 4 5 7 12 Speedup [Platform: NoC Sim]</cell><cell cols="2">5_27_64_192_1 8</cell><cell cols="2">3_13_192_384_1</cell><cell cols="3">3_13_384_256_1 86 115 10 3_13_256_256_1 6 AlexNet 1_1_9216_4096_1</cell><cell cols="2">1_1_4096_4096_1</cell><cell>1_1_4096_1000_1</cell><cell cols="2">GEOMEAN</cell><cell>0 1 2 3 4 5</cell><cell>7_112_3_64_2</cell><cell>1_56_64_64_1</cell><cell>3_56_64_64_1 5 7</cell><cell cols="2">1_56_64_256_1</cell><cell>1_56_256_64_1</cell><cell cols="2">1_56_256_128_1</cell><cell>3_28_128_128_2</cell><cell></cell><cell cols="2">1_28_128_512_1</cell><cell>1_28_256_512_2 14 9</cell><cell cols="2">1_28_512_128_1 6 6</cell><cell>1_28_512_256_1</cell><cell>3_14_256_256_2 6 ResNet-50 1_14_256_1024_1 1_14_512_1024_2 5</cell><cell>1_14_1024_256_1</cell><cell>3_14_256_256_1 7 8 Random 1_14_1024_512_1</cell><cell>3_7_512_512_2</cell><cell>1_7_512_2048_1 Timeloop Hybrid 1_7_1024_2048_2 1_7_2048_512_1 6 6</cell><cell>3_7_512_512_1 7</cell><cell>1_1_2048_1000_1</cell><cell>GEOMEAN CoSA</cell></row><row><cell>0 1 2 3 4 5 Speedup [Platform: NoC Sim]</cell><cell cols="2">7_112_3_64_2 8 7 1_56_64_128_1</cell><cell cols="2">3_56_4_128_1</cell><cell>1_56_128_256_1</cell><cell>1_56_64_256_1</cell><cell>1_56_256_128_1</cell><cell>1_56_256_256_1</cell><cell cols="2">3_28_8_256_2 8 6</cell><cell>1_28_256_512_1</cell><cell>1_28_256_512_2</cell><cell cols="7">1_28_512_256_1 10 7 3_28_8_256_1 10 6 ResNeXt-50 (32x4d) 1_28_512_512_1 3_14_16_512_2 1_14_512_1024_1 1_14_512_1024_2 6 28</cell><cell>1_14_1024_512_1</cell><cell>3_14_16_512_1 18</cell><cell>1_14_1024_1024_1</cell><cell cols="2">3_7_32_1024_2 6 6</cell><cell cols="2">1_7_1024_2048_1</cell><cell>1_7_1024_2048_2 6 6</cell><cell>1_7_2048_1024_1</cell><cell>3_7_32_1024_1</cell><cell>1_1_2048_1000_1</cell><cell>GEOMEAN</cell><cell>0 1 2 3 4 5</cell><cell>3_480_1_16_1</cell><cell>3_240_16_32_1 7 7 6 6 3_120_32_64_1 3_60_64_128_1 DeepBench 3_108_3_64_2 3_54_64_64_1 3_27_128_128_1 6 16</cell><cell>3_14_128_256_1</cell><cell>3_7_256_512_1</cell><cell>GEOMEAN</cell><cell>0 1 2 3 4 5</cell><cell>Random Timeloop Hybrid CoSA 1.0 1.3 3.3 OVERALL</cell></row><row><cell cols="2">Fig. 10:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>We also increase the sizes of the local and global buffers to demonstrate that CoSA can achieve consistently good schedules across different architectures. The sizes of local buffers, i.e. accumulation, weight, and input buffers, are doubled and the global buffer size increased 8×. Modified memory capacities, at the PE and global buffer level, are likely to impact the optimal strategy for data re-use and NoC communication traffic reduction. With CoSA, we show 5.7× speedup over Random and 1.4× speedup over Timeloop Hybrid search in Fig.9b, demonstrating CoSA's capability across different architectures.</figDesc><table><row><cell>0.0 0.6 1.2 1.8 2.4 3.0 Speedup [Platform: GPU]</cell><cell>7_112_3_64_2 4</cell><cell>1_56_64_64_1</cell><cell>3_56_64_64_1</cell><cell>1_56_64_256_1</cell><cell>1_56_256_64_1</cell><cell>1_56_256_128_1</cell><cell>3_28_128_128_2</cell><cell>1_28_128_512_1</cell><cell>1_28_256_512_2</cell><cell>1_28_512_128_1</cell><cell>1_28_512_256_1 ResNet-50 3_14_256_256_2 1_14_256_1024_1 1_14_512_1024_2</cell><cell>1_14_1024_256_1</cell><cell>3_14_256_256_1</cell><cell>1_14_1024_512_1</cell><cell>3_7_512_512_2</cell><cell>1_7_512_2048_1 TVM 1_7_1024_2048_2</cell><cell>1_7_2048_512_1 4</cell><cell>3_7_512_512_1</cell><cell>1_1_2048_1000_1 CoSA GEOMEAN 1.0 1.2</cell></row><row><cell cols="20">Fig. 11: Speedup relative to TVM reported on K80 GPU.</cell></row><row><cell cols="12">affected by communication costs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">SRAM Size.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank Lianmin Zheng for providing the TVM tuning scripts and scheduling templates, and Kostadin Ilov for the computing system support. This work was supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA, Berkeley Wireless Research Center, ADEPT Lab industrial sponsors (Intel, Apple, Futurewei, Google, Qualcomm, Seagate, Western Digital), and a Facebook Faculty Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Edge TPU</title>
		<ptr target="https://cloud.google.com/edge-tpu/" />
		<imprint>
			<date type="published" when="2018-12-05">2018-12-05</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polyhedral autotransformation with no integer linear programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
				<meeting>the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to optimize halide with tree search and random programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Search-based program synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">AWS Inferentia: High Performance Machine Learning Inference Chip</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/machine-learning/inferentia/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Opentuner: An extensible framework for program autotuning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bosboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>-M. O'reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
				<meeting>the International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Heterogeneous memory management for embedded systems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Avissar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Compilers, Architecture, and Synthesis for Embedded Systems</title>
				<meeting>the International Conference on Compilers, Architecture, and Synthesis for Embedded Systems</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tiramisu: A polyhedral compiler for expressing fast and portable code</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization (CGO)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pencil: A platform-neutral compute intermediate language for accelerator programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Beaugnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Betts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Donaldson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ketema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Absar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Haastregt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kravets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lokhmotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hajiyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
				<meeting>the International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic generation of peephole superoptimizers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Explaining how a deep neural network trained with end-to-end learning steers a car</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pluto+ algorithm: A practical approach for parallelization and locality optimization of affine loop nests</title>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A practical automatic polyhedral parallelizer and locality optimizer</title>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
				<meeting>the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Marvel: A data-centric compiler for dnn operators on spatial accelerators</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Haridas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TVM: An Automated End-to-end Optimizing Compiler for Deep Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DianNao: A Small-footprint High-throughput Accelerator for Ubiquitous Machine-learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2014-03">March 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eyeriss: A Spatial Architecture for Energy-efficient Dataflow for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DaDianNao: A Machine-learning Supercomputer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An architecture-agnostic integer linear programming approach to cgra mapping</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation Conference (DAC)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic memory partitioning and scheduling for throughput and power optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Design Automation of Electronic Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An efficient and versatile scheduling algorithm based on sdc formulation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation Conference (DAC)</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DMazeRunner: Executing perfectly nested loops on dataflow accelerators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><surname>Deepbench</surname></persName>
		</author>
		<ptr target="http://www.github.com/baidu-research/deepbench" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Communication-optimal tilings for projective nested loops with arbitrary bounds</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ShiDianNao: Shifting Vision Processing Closer to the Sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alkalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sapek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>A Configurable Cloud-Scale DNN Processor for Real-Time AI</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tetris: Scalable and Efficient Neural Network Acceleration with 3D Memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Polly-polyhedral optimization in llvm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aloor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simbürger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Größlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-N</forename><surname>Pouchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Polyhedral Compilation Techniques (IMPACT)</title>
				<meeting>the First International Workshop on Polyhedral Compilation Techniques (IMPACT)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The architectural implications of facebook&apos;s dnn-based personalized recommendation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cottel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hempstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<meeting>the International Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gurobi optimizer reference manual</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gurobi</surname></persName>
		</author>
		<ptr target="http://www.gurobi.com" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Optimization</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximizing multiprocessor performance with the suif compiler</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Wei</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mind mappings: enabling efficient algorithm-accelerator mapping space search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-performance deep-learning coprocessor integrated into x86 soc with server-class cpus industrial product</title>
		<author>
			<persName><forename type="first">G</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Palangpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Arden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Houck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Autotm: Automatic tensor movement in heterogeneous memory systems using integer linear programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lowe-Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond data and model parallelism for deep neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
				<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luc Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In-Datacenter Performance Analysis of a Tensor Processing Unit,&quot; in</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">GAMMA: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design (ICCAD)</title>
				<meeting>the International Conference on Computer-Aided Design (ICCAD)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Invited: A modular digital vlsi flow for high-productivity soc design</title>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Krimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation Conference (DAC)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">When polyhedral transformations meet simd code generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Veras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-N</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
				<meeting>the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)</title>
				<meeting>the Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Programmable Interconnects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatically scheduling halide image processing pipelines</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Mullapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><forename type="middle">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hybrid optimization/heuristic instruction scheduling for programmable accelerator codesign</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nowatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
				<meeting>the International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A general constraint-centric scheduling framework for spatial architectures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nowatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sartin-Tarm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Estan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Robatmili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Timeloop: A Systematic Approach to DNN Accelerator Evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<meeting>the International Symposium on Performance Analysis of Systems and Software (ISPASS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Predictive modeling in a polyhedral optimization space</title>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cavazos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-N</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bastoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of parallel programming</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Swizzle inventor: Data movement synthesis for gpu kernels</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jangda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hagedorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Barthels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Torlak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Chlorophyll: Synthesis-aided compiler for low-power spatial architectures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jelvis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Totla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chasins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
				<meeting>the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Plasticine: A reconfigurable architecture for parallel paterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koeplinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadjis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sigma: A sparse and irregular gemm accelerator with flexible interconnects for dnn training</title>
		<author>
			<persName><forename type="first">E</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nadella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<meeting>the International Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-time Object Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dramsim2: A cycle accurate memory system simulator</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cooper-Balis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE computer architecture letters</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Simba: Scaling deep-learning inference with multi-chip-module-based architecture</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Simba: Scaling deep-learning inference with multi-chip-module-based architecture</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The NVIDIA Deep Learning Accelerator</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sijstermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot Chips</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Programming by sketching for bit-streaming programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rabbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ebcioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
				<meeting>the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An 11.5 tops/w 1024-mac butterfly structure dual-core sparsity-aware neural processing unit in 8nm flagship mobile soc</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Solid State Circuits Conference (ISSCC)</title>
				<meeting>the International Solid State Circuits Conference (ISSCC)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)</title>
				<meeting>the Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Triton: an intermediate language and compiler for tiled neural network computations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
				<meeting>the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)</title>
				<meeting>the Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">ScaleDeep: A Scalable Compute Architecture for Learning and Evaluating Deep Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nagaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Interstellar: Using halide&apos;s scheduling language to analyze dnn accelerators</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operation Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Cambricon-X: An Accelerator for Sparse Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Microarchitecture</title>
				<meeting>the International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Janus: Statically-driven and profile-guided automatic dynamic binary parallelisation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization (CGO)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
