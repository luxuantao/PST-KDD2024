<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim</title>
				<funder ref="#_uEUzfYT">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">German Academic Exchange Service (DAAD)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
							<email>anandsamajdar@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Georgia Tech Atlanta</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Moritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Georgia Tech Atlanta</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Otto-von-Guericke Univ</orgName>
								<address>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Univ. of Rochester Rochester</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Whatmough</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">ARM ML Research Lab Boston</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">ARM ML Research Lab Boston</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Georgia Tech Atlanta</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The compute demand for deep learning workloads is well known and is a prime motivator for powerful parallel computing platforms such as GPUs or dedicated hardware accelerators. The massive inherent parallelism of these workloads enables us to extract more performance by simply provisioning more compute hardware for a given task. This strategy can be directly exploited to build higher-performing hardware for DNN workloads, by incorporating as many parallel compute units as possible in a single system. This strategy is referred to as scaling up. Alternatively, it's feasible to arrange multiple hardware systems to work on a single problem to exploit the given parallelism, or in other words, scaling out. As DNN based solutions become increasingly prevalent, so does the demand for computation, making the scaling choice (scale-up vs scale-out) critical.</p><p>To study this design-space, this work makes two major contributions. (i) We describe a cycle-accurate simulator called SCALE-SIM for DNN inference on systolic arrays, which we use to model both scale-up and scale-out systems, modeling on-chip memory access, runtime, and DRAM bandwidth requirements for a given workload. (ii) We also present an analytical model to estimate the optimal scale-up vs scale-out ratio given hardware constraints (e.g, TOPS and DRAM bandwidth) for a given workload. We observe that judicious choice of scaling can lead to performance improvements as high as 50? per layer, within the available DRAM bandwidth. This work demonstrates and analyzes the trade-off space for performance, DRAM bandwidth and energy, and identifies sweet spots for various workloads and hardware configurations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, the urgent compute demands stemming from Deep Neural Network (DNN) workloads have reinvigorated research into computer architecture, systems and high performance software design. The raw parallelism and reuse opportunities have spring-boarded devices like GPUs, which formerly were considered special purpose into ubiquity. This trend has also engendered a whole breed of hardware accelerators, which are aimed at squeezing out extremely high performance within commodity power and area budgets, owing to their custom design <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Nonetheless, within the field of deep learning, the hunger to consume more compute power seems insatiable. As the machine learning community develops deep learning based solutions for newer more complex problems, the DNN models SCALE-SIM was developed jointly at ARM Research, Boston and Georgia Tech. Download link: https://github.com/ARM-software/SCALE-Sim become larger and more compute intensive. Furthermore, DNN-based methods are now being deployed to an ever increasing suite of applications, which means that the frequency of encountering a DNN-based workload is increasing as well <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b22">[23]</ref>. For computer architecture, this trend simply translates into the need to create more powerful and efficient hardware, or in other words to scale the performance of the hardware system.</p><p>The fundamental approaches to building efficient DNN accelerators have become fairly standard: since the majority of computation is some form of matrix-matrix multiplication <ref type="foot" target="#foot_0">1</ref> , DNN accelerators typically employ a regular array of multiplyaccumulate (MAC) units to compute these efficiently by leveraging data reuse within the array. The current differences seen across published accelerator microarchitectures today mainly lie in the memory hierarchy <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref> and dataflow strategies <ref type="bibr" target="#b3">[4]</ref> employed.</p><p>The optimal approach to scalability, however, remains an open question. Extracting higher performance essentially translates into allocating more parallel compute for a given workload. One way of achieving this is by creating a monolithic array with a large number of MAC units. The Google TPU <ref type="bibr" target="#b11">[12]</ref> is an example of such a design. This approach is known as Scale-UP. Alternatively, the effect of scaling can also be achieved by allocating multiple such units to collaboratively work on a given problem, or in other words by Scale-OUT. Microsoft's Brainwave <ref type="bibr" target="#b6">[7]</ref> is an instance of such a design. In fact, scale-out need not be across separate chips; NVIDIA's approach of a multitude of loosely coupled tensor cores <ref type="bibr" target="#b0">[1]</ref> across SMs can also be viewed as an instance of scale-out. Both approaches involve a number of trade-offs. While a monolithic scale-up design provides the opportunity to exploit reuse, hence avoiding costly off-chip accesses, the operand reuse opportunities are of course finite, which ultimately limits utilization. On the other hand, a scale-out design may provides more mapping flexibility to maximize hardware utilization may additionally also be cheaper to design and (re-)configure.</p><p>In this paper, we analyze, identify, and quantify the components of this trade-off space to provide a systematic approach for making scaling decisions. To enable this study, we develop a cycle accurate simulator for DNN accelerators called SystoliC AcceLErator SIMulator (SCALE-SIM) <ref type="bibr" target="#b23">[24]</ref>, which models compute performance, on-chip and off-chip memory accesses, and interface bandwidth information for a given neural network. SCALE-SIM implements two elements: (i) a compute unit based around a systolic array parameterized by size and aspect ratio, and (ii) a simple accelerator memory system with three double buffered SRAM memories of user specified sizes, which buffers the matrices for two operands and one result. The inputs to the tool are the layer dimensions of a given neural network workload, and the hardware architecture parameters. SCALE-SIM can model both scaleup (one partition) and scale-out (multiple partition) instances.</p><p>To allow for fast design space exploration and rapid identification of design insights, we augment the simulator with an analytical model that captures the first-order execution time of a single systolic array. Unlike SCALE-SIM, the analytical model does not consider cycle by cycle accesses and bandwidth demands due to limited memory sizes. Instead, it captures the first-order performance, and thus helps prune the search space. As we will describe in Section III, we use this model to determine the most performant configuration for both monolithic (scale-up) and partitioned (scale-out) systems for a given workload.</p><p>Using SCALE-SIM augmented with analytical models, we systematically explore the design space of DNN accelerators, focusing on understanding the trade-off of scale-up versus scaling-out configurations in Section IV. We find that the fundamental trade-off is between performance and DRAM bandwidth demands. Finally, we propose a heuristic-driven approach that efficiently identifies the optimal scaling strategy, along with the design configuration within a particular scaling strategy, for a given set of workloads. In summary the following are the main contributions of this paper:</p><p>1) We develop SCALE-SIM, a cycle accurate, configurable systolic array based DNN accelerator simulator; 2) We develop an analytical model for compute the runtime of DNNs on a systolic array, and using this to determine the optimal size, aspect ratio and number of partitions for achieving the best performance for a given workload; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCALE-Sim</head><formula xml:id="formula_0">SRAM R/W DRAM R/W Conv1, Conv2, FC1,?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cycle accurate traces</head><p>Cycles, Bandwidth, Utilization etc. 3) We present an in-depth study of the trade-off space pertaining to scaling decisions in DNN acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>II. SCALE-SIM: SYSTOLIC ACCELERATOR SIMULATOR SCALE-SIM is a cycle-accurate behavioural simulator that provides a publicly available open-source modeling infrastructure for array-based DNN accelerators. SCALE-SIM enables designers to quickly iterate over and validate their upcoming designs with respect to the various optimization goals for their respective implementation points. In this section, we first provide some background on systolic arrays and second, we describe our modeling methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background: Systolic Arrays and Dataflows</head><p>Systolic arrays are a class of simple, elegant and energyefficient architectures for accelerating general matrix multiplication (GEMM) operations in hardware. They appear in many academic and commercial DNN accelerator designs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>. An overview of system integration is shown in Figure <ref type="figure" target="#fig_0">1</ref> Compute. The compute microarchitecture comprises several Multiply-and-Accumulate (MAC) units (also known as Processing Elements, or PEs), connected in a tightly coupled two dimensional mesh. Data is fed from the edges from SRAMs, which then propagates to the elements within the same row (column) via unidirectional neighbour-to-neighbour links. Each MAC unit stores the incoming data in the current cycle in an internal register and then forwards the same data to the outgoing link in the next cycle. This store and forward behavior results in significant savings in SRAM read bandwidth and can very effectively exploit reuse opportunities provided by convolution/GEMM operations, making it a popular choice for accelerator design. Note that this data movement and operand reuse is achieved: (1) without generating or communicating any address data, and (2) only using hardwired local register-to-register inter-PE links, without any interconnect logic or global wires. For these two reasons, the systolic array is extremely energy and area efficient.</p><p>Memory. Systolic Arrays are typically fed by local linearlyaddressed SRAMs on the two edges of the array, with outputs collected along a third edge. These local SRAMs are often double buffered and are backed by the next level of the memory hierarchy. Data Reuse. A typical convolution can be viewed as a small filter kernel being slid over a given input matrix, with each overlap generating one output pixel. When the convolution operation is formulated as successive dot-product operations, three reuse patterns are immediately evident:</p><p>? Each convolution window uses the same filter matrix, to generate pixels corresponding to a given output channel. ? The adjacent convolution windows share portions of the input matrix if the stride is smaller than window dimension. ? To generate a output pixel in different output channels, different filter matrices use the same convolution window. These reuses can be exploited via the dataflow or mapping of the DNN over the array.</p><p>Dataflow. There are three distinct strategies of mapping compute or dataflows onto the systolic array named Output Stationary (OS), Weight Stationary (WS), and Input Stationary (IS) <ref type="bibr" target="#b3">[4]</ref> as shown in Figure <ref type="figure" target="#fig_2">3</ref>. The "stationarity" of a given dataflow is determined by the tensor whose element is not moved (i.e. stationary) for the maximum duration of time throughout the computation. Although many different dataflows exist for spatial arrays, we only consider true systolic dataflows that only use local communication.</p><p>The OS dataflow depicted in Figure <ref type="figure" target="#fig_2">3</ref>(a), therefore refers to the mapping where each MAC units is responsible for all the computations required for a OFMAP pixel. All the required operands are fed from the edges of the array, which are distributed to the MAC processing elements (PE) using internal links to the arrays. The partial sums are generated and reduced within each MAC unit. Once all the MAC units in the array complete the generation of output pixels assigned to itself, the peer to peer links are used to transfer the data out of the array. No computation takes place in the array during this movement. An alternative high performance implementation using a separate data plane to move generated output is also possible, however, it is costly to implement.</p><p>The WS dataflow on the other hand uses a different strategy as shown in Figure <ref type="figure" target="#fig_2">3(b)</ref>. The elements of the filter matrix are pre-filled and stored into each PE prior to the start of computation, such that all the elements of a given filter are allocated along a column. The elements of the IFMAP matrix are then streamed in through the left edge of the array, and each PE generates one partial sum every cycle. The generated partial sums are then reduced across the rows, along each column in parallel to generated one OFMAP pixel (or reduced sum) per column.</p><p>The IS dataflow is similar to WS, with the difference being in the order of mapping. Instead of pre-filling the array with elements of the filter matrix, elements of the IFMAP matrix are stored in each PE, such that each column has the IFMAP elements needed to generate a given OFMAP pixel. Figure <ref type="figure" target="#fig_2">3(c</ref>) depicts the mapping. We describe these dataflows in more detail in Section III-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. System Integration</head><p>We consider the typical offload model of accelerator integration in SCALE-SIM. We attach the DNN accelerator to the system interconnect, using a slave interface on the accelerator, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. The CPU is the bus master which interacts with the accelerator by writing task descriptors to memory-mapped registers inside the accelerator. When a task is offloaded to the accelerator, the CPU master can context switch to progress other jobs, while the accelerator wakes up and starts computing, independently generating its memory requests and side channel signals. When the computation has finished, the accelerator notifies the CPU, which accesses the results from the accelerator internal memory.</p><p>Thus, the cost on the system performance for integrating an accelerator is the extra accesses on the system bus, which could be modelled as interface bandwidth requirement. SCALE-SIM allows for modeling the main memory behavior by generating accurate read and write bandwidths at the interface, which can then be fed into a DRAM simulator e.g., DRAM-Sim2 <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation</head><p>Internally, SCALE-SIM takes an inside-out implementation approach. Specifically, the simulator assumes that the accelerator is always compute bound and the PEs are always used to the maximum possible utilization -as dictated by the dataflow ? SCALE-SIM generates cycle accurate read addresses for elements required to be fed on the top and left edges of the array such that the PE array never stalls. These addresses are effectively the SRAM read traffic for filter and input matrices, as dictated by the dataflow. Given the reduction takes a deterministic number of cycles after the data has been fed in, SCALE-SIM generates an output trace for the output matrix, which essentially constitutes the SRAM write traffic. ? SCALE-SIM parses the generated traffic traces, to determine total runtime for compute and data transfer to and from SRAM. The data transfer time is essentially the cycle count of the last output trace entry. The SRAM trace also depicts the number of rows and columns that have valid mapping in each cycle. This information couples with the dataflow is used to determine the utilization of the array, every cycle. ? In SCALE-SIM the elements of both the input operand matrices, and the generated elements of the output matrix is serviced by dedicated SRAM buffers backed via a double buffered mechanism, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. As the sizes of these buffers are known from user inputs, SCALE-SIM parses the SRAM traces and determines the time available to fill these buffers such that no SRAM request is a miss. Using this interfaces SCALE-SIM generates a series of prefetch requests to SRAM which we call the DRAM trace. ? The DRAM traces are the used to estimate the interface bandwidth requirements for the given workload and the provided architecture configuration. ? The trace data generated at the SRAM and the interface level is further parsed to determine the total on-chip and off-chip requests, compute efficiency, and other high level metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Validation of the tool</head><p>We validated SCALE-SIM against an RTL implementation of a systolic array. Figure <ref type="figure" target="#fig_3">4</ref> depicts the cycles obtained when matrix multiplications are performed on varying arrays sizes (X-axis) under full utilization with OS dataflow, from RTL implementation and SCALE-SIM simulations. As depicted by  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. User Interface</head><p>Figure <ref type="figure" target="#fig_1">2</ref> depicts the inputs files used by the simulator, the outputs that are generated. SCALE-SIM takes two files as input from the user: one is a hardware configuration, and the other is a neural network topology for the workload. The configuration file contains the user specification for architectural parameters, like the array size, the memory size, and the path to the topology file. Table I depicts the complete list of parameters, which are mostly self-explainatory. For layers such as fully-connected (i.e. matrix-vector), the input parameters correspond to convolutions where the size of the filters are same as that of the IFMAP.</p><p>The topology file contains the layer topology dimensions for each of the layers in the given neural network workload. This is a comma-separated value (CSV) file, with each row listing all the required hyper-parameters for a given layer -Table <ref type="table" target="#tab_2">II</ref> gives the complete list of all the entries in a given row. SCALE-SIM parses the topology file one line at a time and simulates the execution of the layer. This is a natural approach for traditional neural networks which are primarily composed of a single path. However, modern DNNs often contain "cells" that are composed of multiple convolution layers in parallel <ref type="bibr" target="#b9">[10]</ref>. SCALE-SIM serializes the execution of such layers in the same order in which they are listed in the topology file. SCALE-SIM generates two types of outputs. First is the cycle accurate traces for SRAM and DRAM reads and writes. The traces are also CSV files, which list the cycle and the addresses of data transferred in a given cycle. The other type of output files are reports with aggregated metrics obtained by parsing information from the traces. These include cycle counts, utilization, bandwidth requirements, total data transfers etc. The trace-based methodology is very easy to debug and highly-extensible to new analyses and architectures.    In SCALE-SIM, all the simulated metrics including runtime are determined at the end of a round of simulation. However running simulation for all possible data points in a large search space is expensive and sometimes unnecessary. In this section we describe an effective analytical model for runtime, which accounts for the data movement patterns simulated by SCALE-SIM. Please note however, the analytical model does not model the memory accesses and bandwidth demand arising due to limited memory which is captured by SCALE-SIM. We use this model to estimate costs and prune the search space for the subsequent scalability study described in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mapping across Space and Time</head><p>In dense DNN computations, running different types of layers generalize to matrix-matrix multiplications of different sizes. For systolic arrays, we consider the operand matrices of dimensions S R ? T and T ? S C respectively, where S R and S C are the spatial dimensions along which computation is mapped, and T is the corresponding temporal dimension. These matrices are obtained by projecting the original operand matrices into the available spatio-temporal dimensions. For example, for multiplying matrices of size M ? K and K ? N , the dimension M is mapped to S R , dimension N is mapped to S C and the dimension K to T .</p><p>Figure <ref type="figure" target="#fig_6">5</ref> illustrates the mapping of a 2D convolution onto the three dataflows. Figure <ref type="figure" target="#fig_6">5a</ref> shows the mapping corresponding to output stationary (OS) dataflow. The first operand matrix, with size S R ? T , is a rearranged input feature map (IFMAP) matrix. Each row consists of elements corresponding to one convolution window, while the number of rows is the number of OFMAP pixels generated per filter. The second operand matrix contains unrolled filter elements, with each filter unrolled along each column, resulting in a T ?S C matrix.</p><p>Figure <ref type="figure" target="#fig_6">5b</ref> and Figure <ref type="figure" target="#fig_6">5c</ref> depict the mapping for other two dataflows; Weight Stationary (WS) and Input Stationary (IS). For WS, the number of convolution windows maps to S R , while S C is equal to the number of filters. As seen in Section II the partial sums for each OFMAP pixel are generated every subsequent cycle making the mapping along the temporal dimension T equal to the number of OFMAP pixels generated. In the IS dataflow however, the order and direction of feeding the IFMAP matrix and the filter matrices are interchanged. This implies that the mapping along the S R and S C dimensions for this dataflow is the same size as the convolution window and number of OFMAP pixels generated per filters respectively. While the temporal dimension T maps the number of filters. Table III summarizes these dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Runtime for Scale-Up</head><p>With the above abstraction of mapping in place, it is feasible to model the runtime for various dataflows, under the assumption of either a restricted or unrestricted number of compute elements. In our discussions we will only use multiply-andaccumulate (MAC) units as the compute elements within the systolic array.</p><p>1) Runtime with unlimited MAC units: Given an unlimited amount of MAC units, the fastest execution for any dataflow is achieved using the maximal array size of S R ? S C . However, note that even though all the multiplication operations are done in one cycle, the runtime needs to account for both the store and forward nature of the array, and the existence of the temporal dimension T ( &gt; 0).</p><p>Figure <ref type="figure" target="#fig_8">6</ref> shows the steps followed for moving data in the three dataflows introduced in Section II. Figure <ref type="figure" target="#fig_8">6a</ref> depicts the steps when implementing the OS dataflow. As mentioned before the IFMAP matrix is fed from the left while the filter elements are pushed in from the top edge. To account for the store and forward nature of the arrays and match the data arrival time at all the PEs, the data distribution is skewed; the PE at the top left corner of the array receives both the operands at the first cycle, the PEs in the next column and next row get their operands in the next cycles, their neighbours  in the cycle after that and so on. The PE at the bottom right corner of the array (marked in blue), is the last to receive the operand data. It is easy to see that the cycle at which the first operands arrive at this PE is S R + S C -2 (adding steps 1 , 2 and 3 ). In this dataflow, each PE receives two operands per cycle and generates one OFMAP pixel value by in-place accumulation. It takes T cycles to generate on output, which is equal to the number of elements in a convolution window. The generated outputs are taken out from the bottom edge of the array. While it is possible to take out the output along other edges as well, using the bottom edge is the fastest alternative. The time required to completely drain the array of the generated output is S R cycles after the PE at the right most corner has finished computation (step 4 ). Therefore, the total time taken for entire computation is,</p><formula xml:id="formula_1">? scaleup min = 2S R + S C + T -2<label>(1)</label></formula><p>In Figure <ref type="figure" target="#fig_8">6b</ref> we perform the same analysis for WS dataflow. Here, the filter matrix is fed into the array from the top and is kept alive untill the computations involving these operands are complete. Skewing is not needed as no computation is taking place while the filters are being fed. This takes S R cycles (step 1 ). Once the filter elements are in place, the elements of the IFMAP matrix are fed from the left edge of the array. Each PE reads the IFMAP operand, multiplies it with the stored weight and forwards the partial sum to the PE in the neighbouring row for reduction. The first data arrives at the last row after S C -1 cycles (step 2 ). The IFMAP matrix is fed in one column at a time, therefore every column in the systolic array receives T operands, one each cycle, corresponding to the number of columns in the IFMAP matrix (step 3 ). Furthermore, for all the partial sums generated reduction occurs across the rows, for each column. After the top row receives and operand from the IFMAP, it takes S R -1 cycles to reduce (step 4 ). Therefore the array is drained out of all partial sums, after reduction happens in the rightmost column. The total runtime therefore is,</p><formula xml:id="formula_2">? scaleup min = 2S R + S C + T -2</formula><p>Using similar analysis and Figure <ref type="figure" target="#fig_6">5c</ref>, we can show that the above expression holds true for the IS dataflow as well. Thus Equation 1 captures the runtime for all the dataflows in a systolic array when the number of MAC units is infinitely large 2) Runtime with limited MAC units: Having a large enough systolic array which can map all the compute at once is often not practically feasible. Due to the large amount of computation compared to hardware compute units, it is necessary to tile the workload into chunks. We term this practice as folding where each of these chunks are called a fold 2 . Folds can be generated by slicing the compute along the S R and S C dimensions. When using a R ? C array, the number of fold along rows (F R ) and columns (F C ) are determined as follows.</p><formula xml:id="formula_3">F R = S R /R , F C = S C /C<label>(2)</label></formula><p>Figure <ref type="figure">7</ref> illustrates this. Analysis similar to Section III-B1 can be used to express the time taken in each of these folds as is given by the following equation, for all dataflows.</p><formula xml:id="formula_4">? F = 2R + C + T -2<label>(3)</label></formula><p>Where R and C are the rows and columns of the systolic array and T is the temporal dimensions. The total runtime can therefore be expressed from Equation 2 and Equation 3 as following.</p><formula xml:id="formula_5">? scaleup = (2R + C + T -2) S R /R S C /C<label>(4)</label></formula><p>The above equation provides us with the insights on the factors affecting runtime. For a given workload and array configuration, choice of dataflow assigns the values for S R , S C and T respectively, which could be selected to minimize ? . On the other hand if the workload and dataflow is fixed, for a given number of MAC units, the optimal values of R and C could be determined to reduce the runtime as well.</p><p>Equation 4 can be used to determine the optimal configuration for a given matrix by implementing search over the possible R and C values. For workloads with multiple matrix operations, this model can be used as a cost model as depicted later in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimal Partitioning for Scale-Out</head><p>In our previous analysis we have only considered a single array to study the affect of micro-architectural and design parameters on runtime. Instead of creating a single monolithic architecture with multiple PEs (i.e., scale-up), an alternative Fig. <ref type="figure">8</ref>: Scale Out design choice is to employ multiple units of systolic arrays, each responsible for one partition of the output feature map, to increase the available parallelism (i.e., scale-out) In this section we will model the runtime of such systems.</p><p>The scaled out configuration introduces another set of parameters, as shown in Figure <ref type="figure">8</ref>. Unlike in scale-up where all the MAC units are arranged in a R ? C array, in scaledout configuration, the MAC PEs are grouped into P R ? P C systolic arrays, each with a PE array of R ? C.</p><p>Using this approach for a given number of partitions P = P R ?P C , the effective workload mapped for computation over each partition can be determined by,</p><formula xml:id="formula_6">S R = S R /P R , S C = S C /P C<label>(5)</label></formula><p>Within each array, we can use Equation <ref type="formula" target="#formula_5">4</ref>to decide the optimal aspect ratio (R ? C) for running the partitioned workload. Since the individual partitions execute in parallel, the total runtime of the scaled-out system is simply the runtime of the slowest cluster which can be determined by Equation <ref type="formula" target="#formula_5">4</ref>and Equation 5</p><formula xml:id="formula_7">? scaleout = (2R + C + T -2) S R /R S C /C<label>(6)</label></formula><p>IV. ANALYSIS OF SCALING</p><p>The primary aim of scaling a hardware accelerator, is to improve the runtime of a given workload. Since there are many ways of scaling a system, the first natural question to ask is whether any one of the methods proves beneficial over the others. To answer this question, we computed runtime using the analytical model described in Section III, when using different configurations of monolithic vs scaled out arrays, given the same budget for MAC units. For workloads in our experiments, we used the convolution layers in Resnet50 CNN <ref type="bibr" target="#b9">[10]</ref> and a few representative layers from widely used contemporary natural language processing models: GNMT <ref type="bibr" target="#b29">[30]</ref>, DeepSpeech2 <ref type="bibr" target="#b2">[3]</ref>, Transformer <ref type="bibr" target="#b25">[26]</ref>, and neural collaborative filtering <ref type="bibr" target="#b10">[11]</ref>. The matrix dimensions corresponding to these workloads are detailed in Table <ref type="table" target="#tab_4">IV</ref> Search Space for Scale-up and Scale-out.  point for corresponding to five different compute capabilities denoted by number of MAC units. On the x axis we have all possible dimensions for a systolic array with these mac units. The y axis represents the partitioned configurations when scaling out. We limit the smallest systolic dimensions to 8x8 to ensure we have a reasonable size arrays per partition when scaling out. The color of each point denotes the normalized stall free run time when TF0 is run using OS dataflow. Run times are normalized to the highest runtime among all the configurations for a fixed number of MAC units. Effect of Aspect Ratio on Scale-up Array. From this chart we can get a first order estimate of runtime variation between partitioned and monolithic configurations. We observe that the highers runtimes are usually located near the points corresponding to y value of 1 ? 1, which represent the mono- lithic configurations. Figure <ref type="figure" target="#fig_10">9(b-c</ref>) depicts the various aspect ratio (Row:Column) configurations for monolithic arrays with 4096 and 16384 MAC units respectively. The first observation is that, the difference in runtime for optimum configuration and others can vary by several orders of magnitude even when the workload is the same, depending on the size of the array. In fact, with larger arrays this difference is exacerbated. Second, the aspect ratio of the optimal configuration is not the same at different performance points, necessitating the need to have a framework to examine various configurations. When considering the array utilization, another interesting trend arises. For configurations with low array utilization, the runtime of the layer is high, which is expected. Also, runtime generally drops with array utilization. Interestingly, when the array dimensions become significantly rectangular, the effect of utilization is less pronounced. In these configurations even though a high utilization is achieved, the improvement in runtime is minimal. This is due to the fact that the time to fill in and take out the data starts dominating, as captured in Equation <ref type="formula" target="#formula_4">3</ref>.</p><p>Comparison of Best Runtime. Moving to the points up along the y-axis in Figure <ref type="figure" target="#fig_10">9</ref>(a) show almost monotonic improvement in performance, depicting that partitioning is always beneficial. To further investigate this trend in Figure <ref type="figure" target="#fig_11">10</ref> we plot the stall free runtimes corresponding to the fastest scaled out (monolithic) configuration normalized to the lowest runtime achieved among all the scaled-out (partitioned) configurations using equal MAC units. Figure <ref type="figure" target="#fig_11">10</ref>(a) plots the rations for first and last five convolution and fully connected layers of Resnet50 CNN for different number of MAC units. It can be observed that monolithic configurations are sometimes significantly slower (25x for CB2a 1 layer) that partitioned configurations, and never faster that the corresponding partitioned configuration. Moreover, for a given layer, the relative slowdown tends to amplify when the hardware is scaled. This trend is also replicated in language models, which predominantly use fully connected layers as seen in Figure <ref type="figure" target="#fig_11">10(b)</ref>. Here for 65536 MAC units the best monolithic configuration is 50x slower than the best partitioned configurations.</p><p>Note that since the runtimes involved in the above charts are stall free, the memory is not involved in slowdown. Therefore, the root cause of this slowdown can be understood by a closer look into the analytical model. First we should remember that in both monolithic and partitioned configurations the amount of serial computation is equal assuming all the MACs are utilized, or in other words the number of folds are equal. However from Equation <ref type="formula" target="#formula_5">4</ref>we can see that the runtime per fold is directly proportional to the array dimensions. Which explains the trend that the partitioned configurations are always faster. Furthermore, the difference in runtime per layer is amplified if the number of folds are high, even when both the arrays are fully utilized and the difference comes from data loading and unloading times. Also, utilizing the entire array in a monolithic configuration, howsoever flexible, is often not possible, as we can notice in Figure <ref type="figure" target="#fig_10">9(b-c</ref>), which limits the amount of available compute resources and thus, contributes further to the relative slowdown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cost of scaling out</head><p>Observations from the experiments in the previous section seem to suggest that scaling out is the best strategy to achieve the optimal runtime. However this choice involves paying additional costs as we discuss below.</p><p>The immediate cost of a partitioned design is the loss of spatial reuse. In a big systolic array any element read from the memory is used by processing elements along a row or column by forwarding it on the internal links of the array. Dividing up the array into smaller parts reduces the number of rows, or columns, or both, resulting in drastic reduction of this reuse opportunity. This is then reflected in terms of number of SRAM reads, data replication, and the input bandwidth (BW) demand from the DRAM. The loss of reuse within the array over short wires also leads to longer traversals over an on-chip/off-chip network (depending on the location of the partitions) to distribute data to the different partitions and collecting outputs -which in turn can affect overall energy.</p><p>Runtime vs. DRAM BW Requirement. In Figure <ref type="figure" target="#fig_0">11</ref> we plot the DRAM BW requirement and runtime for layer CBa 3 in Resnet-50 and layer TF0 in Transformer, as a function of number of partitions, for given number of MAC units. For all the three cases a total of 512KB of SRAM is allocated for IFMAP buffer, 512KB for Filter buffer, and 256 KB for OFMAP buffer. This memory is evenly distributed among the partitions in case of scaling out. The BW numbers are obtained from our cycle accurate simulator when running the output stationary dataflow. As the number of partitions increase, the runtime goes down, however, BW requirements also rise due to loss of reuse originally provisioned by the internal wires, and increased replication of the data among the partitions, bringing down the effective memory capacity. The sweet spot lies at the intersection of runtime and bandwidth curves. When scaling to higher number of MAC units, it is interesting to note that the BW requirement is often higher than traditional DRAM BW. For instance, for both Resnet and Transformer layers with 2 18  MAC units, about 10 KB/cycle of DRAM bandwidth is needed for stall free operation at the sweet spot.</p><p>Energy Consumption. In Figure <ref type="figure" target="#fig_1">12</ref> we study the effect of scaling out on energy. Figure <ref type="figure" target="#fig_1">12</ref> For a given workload and hardware configuration, the energy consumption directly depends on the cycles MAC units have been active and the number of accesses to SRAM and DRAM. The counteracting effects of these factors can be observed in Figure <ref type="figure" target="#fig_0">11</ref>, therefore lays down an interesting tradeoff space. As the figure depicts, for lower number of MAC units (256, 1024 and 4096), the configuration with minimum energy is the monolithic configuration. However with increase in number of MAC units, the point of minimum energy moves towards the right of the chart, favouring more number of partitions. On other words the energy saved in by stealing runtime from powering the massive compute array is more significant than the extra energy spent by the loss of reuse. Furthermore, the bulkier the array, more the savings in compute to counteract the losses in reuse, which explains the observed trend.</p><p>To summarize the data indicates scaling out is beneficial for performance and with larger MAC units is more energy efficient that scaling up. However the cost paid is the extra bandwidth requirement to keep compute units fed, which even at sweet spots are significantly higher than the best scaled-up configuration for large MAC units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimizing for multiple workloads</head><p>Any hardware accelerator should be performant for different workloads. To find such a globally optimized hardware accelerator, a global cost function must be minimized. However, as Figure <ref type="figure" target="#fig_10">9</ref>(a) depicts even for a single workload as the global cost function is large and discontinuous. Optimally searching such a space for finding the global minima is out of the scope of this paper. Instead we propose a method to find reasonable pareto-optimal points for a given set of workloads Considering the runtime as cost, our analytical model from Sec. III-B and III-C or SCALE-SIM yields a runtime-optimal configuration, a k = (S C , S R , R, C), for each individual layer (i.e. workload w l = (S C , S R , T )). We then search among these candidates for the globally optimized one, A. In case of runtime, the total runtime is additive and thus it is calculated by summing the runtimes T r of all workloads w l for each candidate a k :</p><formula xml:id="formula_8">A = argmin a k w l T r (w l , a k )</formula><p>As the number of candidates is limited, exhaustive search is feasible to find the optima.</p><p>In Figure <ref type="figure" target="#fig_2">13</ref> we plot the costs (runtime) of the various candidate configurations normalized to the cost of the paretooptimal configuration obtained by the method mentioned above, for layers in Resnet50 and the language models mentioned in Table <ref type="table" target="#tab_4">IV</ref>. In Figure <ref type="figure" target="#fig_3">14</ref>   locally optimal candidates for scale-out is depicted. In both these cases we observe that the pareto optimal configuration is up to 8x faster than the locally optimal configurations. However, the second and third best configurations are within 20% for smaller number of MAC in both scaled-up and scaled-out configurations. However as the MACs increase the spread of runtimes and we see about 50% increase un rutime for second and third best configurations, while slower configuration taking several factors more cycles to complete than the best configuration.</p><p>V. RELATED WORK Kwon et al. <ref type="bibr" target="#b14">[15]</ref>, propose a data centric model to determine the cost of a given dataflow over user specified accelerator configurations defined by a set of directives. This cost model is then used to search for an optimal dataflow. Timeloop <ref type="bibr" target="#b18">[19]</ref> also uses a similar approach to determine the best mapping strategy, by analytical estimation of runtime and energy. Caffeine <ref type="bibr" target="#b30">[31]</ref> describes analytical modelling of roofline performance to determine the hardware configuration for efficient FPGA-based CNN acceleration. Finally, ASV <ref type="bibr" target="#b5">[6]</ref> constructs analytical energy/latency models of a given DNN and architectural configuration, while using constrained optimization to identify the best scheduling policy. However, all these works only consider a design space of monolithic accelerator configurations (scale-up).</p><p>DyHard-DNN <ref type="bibr" target="#b20">[21]</ref> proposes the idea of morphable systolic arrays, with circuit techniques to save power. However unlike this work, the tradeoff space of partitioned designs is not investigated. ScaleDEEP <ref type="bibr" target="#b26">[27]</ref> proposes a partitioned architecture, but includes heterogeneous compute units tailored for various types of workloads.</p><p>Tetris <ref type="bibr" target="#b7">[8]</ref> describes a custom accelerator infrastructure, comprised of multiple partitioned units, implemented in the logic layer of a 3D memory. Tangram <ref type="bibr" target="#b8">[9]</ref> extends the tiled accelerator architecture and adds extra functionality to improve compute and memory utilization by proposing custom dataflow for inter-layer pipelining and intra-layer reuse over a custom NoC. Moreover, the trade-off space for monolithic vs partitioned design is not exposed and explored in these papers.</p><p>Kung et al. <ref type="bibr" target="#b12">[13]</ref> proposed a partitioned systolic array based solution. In this work the authors employ specialized 3D chip fabrication, and used through-silicon vias (TSVs) to mitigate the high memory bandwidth requirement. Simba <ref type="bibr" target="#b24">[25]</ref> implements a scale-out accelerator using multi-chip modules (MCMs). The authors analyze the cost of scale-out infrastructure and propose a custom architecture comprised of MCMs accounting for the costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we analyze the various alternative approaches to Scale-up and Scale-out DNN accelerator designs. To conduct this study, we construct and describe a cycle accurate DNN accelerator simulator called SCALE-SIM. The simulation results provide memory accesses and bandwidth requirements for various layers of CNN and natural language processing model workloads for varying monolithic and partitioned systolic array based configurations. We also present an analytical model for performance estimation to chart and prune the search space of optimum configurations more rapidly. Our studies depict the inherent trade-off space for performance, DRAM bandwidth, and energy and identifies the sweet spots within the spaces for different workloads and performance points. We have open sourced our tool and hope that it benefits the community to conduct insightful studies as the one presented in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Schematic showing the integration model of accelerator in a systems context</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Schematic depicting the inputs needed and the outputs generated by SCALE-SIM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Schematic showing the mapping in various dataflows (a) Output stationary; (b) Weight stationary; (c) Input stationary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Figure depicting the cycles obtained by RTL implementation and SCALE-Sim simulation for varying array sizes under full utilization in use. With this implementation model, the simulation in SCALE-SIM takes place in following steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Output stationary dataflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Input stationary dataflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Data Flow Mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Schematic depicting steps to model runtime for dataflows in systolic array.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 7: Scale Up</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: (a) The search space of all possible scale-up (monolithic)and scale-out (partitioned) configurations, with different array sizes; the color represents runtime for TF0 layer of the Transformer model, normalized to max runtime across configurations for a given array size. The variation in runtime and array utilization for all scaled-up configurations when running TF0 layer for (b) 214 MACs, (c) 216  MACs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Ratio of no stall runtimes obtained in best scaled-up array configuration vs best scaled-out (partitioned) configuration for a few layers in (a) Resnet50 and (b) Language models, for different MAC units</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Fig.11: Trends for best possible stall free runtime and DRAM bandwidth requirements when the number of partitions are increased from monolithic array in CBa 3 layer in Resnet50 for (a) 218 MAC units, (b) 216 MAC units, and (c) 214 MAC units; and TF0 layer in Transformer for (d) 218 MAC units, (e) 216 MAC units, and (f) 214 MAC units</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Fig. 13: Total runtime loss vs. best configuration for scaleup ie. aspect ratio (R:C). Colors differentiate configurations ordered by runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>SCALE-SIM config description</figDesc><table><row><cell>Parameter</cell><cell>Description</cell></row><row><cell>ArrayHeight</cell><cell>Number of rows of the MAC systolic array</cell></row><row><cell>ArrayWidth</cell><cell>Number of columns of the MAC systolic array</cell></row><row><cell>IfmapSRAMSz</cell><cell>Size of the working set SRAM for IFMAP in KBytes</cell></row><row><cell>FilterSRAMSz</cell><cell>Size of the working set SRAM for filters in KBytes</cell></row><row><cell>OfmapSRAMSz</cell><cell>Size of the working set SRAM for OFMAP in KBytes</cell></row><row><cell>IfmapOffset</cell><cell>Offset to the generated addresses for IFMAP px</cell></row><row><cell>FilterOffset</cell><cell>Offset to the generated addresses for filter px</cell></row><row><cell>OfmapOffset</cell><cell>Offset to the generated addresses for OFMAP px</cell></row><row><cell>DataFlow</cell><cell>Dataflow for this run. Legal values are 'os','ws', and 'is'</cell></row><row><cell>Topology</cell><cell>Path to the topology file</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>SCALE-SIM Topology file description</figDesc><table><row><cell>Parameter</cell><cell>Description</cell></row><row><cell>Layer Name</cell><cell>User defined tag</cell></row><row><cell>IFMAP Height</cell><cell>Dimension of IFMAP matrix</cell></row><row><cell>IFMAP Width</cell><cell>Dimension of IFMAP matrix</cell></row><row><cell>Filter Height</cell><cell>Dimension of one Filter matrix</cell></row><row><cell>Filter Width</cell><cell>Dimension of one Filter matrix</cell></row><row><cell>Channels</cell><cell>Number of Input channels</cell></row><row><cell>Num Filter</cell><cell>Number of Filter matrices. This is also the number of</cell></row><row><cell></cell><cell>OFMAP channels</cell></row><row><cell>Strides</cell><cell>Strides in convolution</cell></row><row><cell cols="2">the figure the cycle counts obtained by both the methods are</cell></row><row><cell cols="2">in good agreement.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Spatio-Temporal Allocation of DNN Dimensions</figDesc><table><row><cell></cell><cell>Spatial Rows</cell><cell>Spatial Columns</cell><cell>Temporal (T )</cell></row><row><cell></cell><cell>(S R )</cell><cell>(S C )</cell><cell></cell></row><row><cell>Output Stationary</cell><cell>N of map</cell><cell>N f ilter</cell><cell>Wconv</cell></row><row><cell>Weight Stationary</cell><cell>Wconv</cell><cell>N f ilter</cell><cell>N of map</cell></row><row><cell>Input Stationary</cell><cell>Wconv</cell><cell>N of map</cell><cell>N f ilter</cell></row></table><note><p>N f ilter : Number of convolution filters N of map : Number of OFMAP pixels generated by filter Wconv : Number of partial sums generated per output pixels III. ANALYTICAL MODEL FOR RUNTIME</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Matrix dimensions of our language model workloads. mapped to S R , S C , and T</figDesc><table><row><cell>Name</cell><cell>S R</cell><cell>T</cell><cell>S C</cell></row><row><cell>GNMT0</cell><cell>128</cell><cell>4096</cell><cell>2048</cell></row><row><cell>GNMT1</cell><cell>320</cell><cell>4096</cell><cell>3072</cell></row><row><cell>GNMT2</cell><cell>1632</cell><cell>1024</cell><cell>36548</cell></row><row><cell>GNMT3</cell><cell>2048</cell><cell>32</cell><cell>4096</cell></row><row><cell>DB0</cell><cell>1024</cell><cell>50000</cell><cell>16</cell></row><row><cell>DB1</cell><cell>35</cell><cell>2560</cell><cell>4096</cell></row><row><cell>TF0</cell><cell>31999</cell><cell>84</cell><cell>1024</cell></row><row><cell>TF1</cell><cell>84</cell><cell>4096</cell><cell>1024</cell></row><row><cell>NCF0</cell><cell>2048</cell><cell>128</cell><cell>1</cell></row><row><cell>NCF1</cell><cell>256</cell><cell>2048</cell><cell>256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>the normalized costs for all</figDesc><table><row><cell></cell><cell></cell><cell>Resnet 50</cell><cell></cell><cell></cell><cell>Language model layers</cell></row><row><cell>perf. loss</cell><cell>2 4 6</cell><cell>fastest fastest 2 nd 3 nd 4 th slowest</cell><cell>perf. loss</cell><cell>2 4</cell><cell>fastest</cell></row><row><cell></cell><cell></cell><cell>2e8 2e10 2e12 2e14 2e16</cell><cell></cell><cell></cell><cell>2e8 2e10 2e12 2e14 2e16</cell></row><row><cell></cell><cell></cell><cell>number of MAC units</cell><cell></cell><cell></cell><cell>number of MAC units</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this case matrix-matrix multiplication also encompasses the degenerate cases of matrix-vector and vector-vector products, where one or both operand matrices have dimension equal to one.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>VII. ACKNOWLEDGEMENTS</head><p>We would like to express our gratitude to <rs type="person">Abhinav Himanshu</rs>, <rs type="person">Felix Kao</rs>, <rs type="person">Sachit Kuhar</rs>, <rs type="person">Vineet Nadella</rs>, and <rs type="person">Natesh Raina</rs> for their help. Special thanks to <rs type="person">Amrita Mathuriya</rs> for her insightful advice on scaling discussions. We would also like to express our gratitude the anonymous reviewers for their insightful comments and suggestions on improving the paper. Finally, we thank our open source contributors and users, for their pull requests, bug reports and constructive suggestions to improve SCALE-SIM. We look forward to keep improving SCALE-SIM in a healthy community driven fashion.</p><p>This work was supported by <rs type="funder">NSF</rs> <rs type="grantNumber">CRII 1755876</rs>, a <rs type="grantName">Google Faculty Award</rs>, and by a fellowship within the <rs type="programName">IFI programme</rs> of the <rs type="funder">German Academic Exchange Service (DAAD)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uEUzfYT">
					<idno type="grant-number">CRII 1755876</idno>
					<orgName type="grant-name">Google Faculty Award</orgName>
					<orgName type="program" subtype="full">IFI programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nvidia tesla v100 gpu architecture</title>
		<ptr target="http://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Xilinx ml suite</title>
		<ptr target="https://github.com/Xilinx/ml-suite" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Solid-State Circuits Conference, ser. ISSCC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4978" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ASV: Accelerated Stereo Vision System</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A configurable cloud-scale dnn processor for realtime ai</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual International Symposium on Computer Architecture</title>
		<meeting>the 45th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tetris: Scalable and efficient neural network acceleration with 3d memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="751" to="764" />
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tangram: Optimized coarse-grained dataflow for scalable nn accelerators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="807" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>abs/1704.04760</idno>
		<ptr target="http://arxiv.org/abs/1704.04760" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maestro: A memory-on-logic architecture for coordinated parallel use of many systolic arrays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 30th International Conference on Application-specific Systems, Architectures and Processors (ASAP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2160</biblScope>
			<biblScope unit="page" from="42" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Packing sparse convolutional neural networks for efficient systolic array implementations: Column combining under joint optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="821" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding reuse, performance, and hardware cost of dnn dataflows: A data-centric approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A 16-nm Always-On DNN Processor With Adaptive Clocking and Multi-Cycle Banked SRAMs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1982" to="1992" />
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On-Chip Memory Technology Design Space Explorations for Mobile Deep Neural Network Accelerators</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 56th ACM/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">SMAUG: End-to-End Full-Stack Simulation Infrastructure for Deep Learning Workloads</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Likun</forename><surname>Xi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04481</idno>
		<imprint>
			<date type="published" when="2019-12">Dec 2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Timeloop: A systematic approach to dnn accelerator evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Buffets: An efficient and composable storage idiom for explicit decoupled data orchestration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="137" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dyhard-dnn: Even more dnn acceleration with dynamic hardware reconfiguration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Putic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dramsim2: A cycle accurate memory system simulator</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="19" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Applications of Deep Neural Networks for Ultra Low Power IoT</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2017-11">Nov 2017</date>
			<biblScope unit="page" from="589" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scale-sim: Systolic cnn accelerator simulator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simba: Scaling deep-learning inference with multichip-module-based architecture</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scaledeep: A scalable compute architecture for learning and evaluating deep networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="13" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A 16nm 25mm2 SoC with a 54.5x Flexibility-Efficiency Range from Dual-Core Arm Cortex-A53 to eFPGA and Cache-Coherent Accelerators</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Symposium on VLSI Circuits</title>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="34" to="C35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer Learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference</title>
		<meeting>the 2nd SysML Conference<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Caffeine: Towards uniformed representation and acceleration for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Euphrates: Algorithm-SoC Co-Design for Low-Power Mobile Continuous Vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA</title>
		<meeting>of ISCA</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
