<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Deep Learning to Annotate the Protein Universe</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-05-02">May 2, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maxwell</forename><forename type="middle">L</forename><surname>Bileschi</surname></persName>
							<email>mlbileschi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Drew</forename><surname>Bryant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Theo</forename><surname>Sanderson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brandon</forename><surname>Carter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<country>MIT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
							<email>lcolwell@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Chemistry</orgName>
								<orgName type="institution">Cambridge University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Using Deep Learning to Annotate the Protein Universe</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-02">May 2, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1101/626507</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding the relationship between amino acid sequence and protein function is a long-standing problem in molecular biology with far-reaching scientific implications. Despite six decades of progress, state-of-the-art techniques cannot annotate 1/3 of microbial protein sequences, hampering our ability to exploit sequences collected from diverse organisms. To address this, we report a deep learning model that learns the relationship between unaligned amino acid sequences and their functional classification across all 17929 families of the Pfam database. Using the Pfam seed sequences we establish a rigorous benchmark assessment and find a dilated convolutional model that reduces the error of both BLASTp and pHMMs by a factor of nine. Using 80% of the full Pfam database we train a protein family predictor that is more accurate and over 200 times faster than BLASTp, while learning sequence features it was not trained on such as structural disorder and transmembrane helices. Our model co-locates sequences from unseen families in embedding space, allowing sequences from novel families to be accurately annotated. These results suggest deep learning models will be a core component of future protein function prediction tools.</p><p>Predicting the function of a protein from its raw amino acid sequence is the critical step for understanding the relationship between genotype and phenotype. As the cost of DNA sequencing drops and metagenomic sequencing projects flourish, fast and efficient tools that annotate open reading frames with function will play a central role in exploiting this data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Identifying proteins that catalyze novel reactions, bind specific microbial targets or work together to build new molecules will accelerate advances in biotechnology. Current practice for functional prediction of a novel protein sequence involves alignment across a large database of annotated sequences using algorithms such as BLASTp [3], or 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>profile hidden Markov models (pHMMs) built from aligned sequence families such as those provided by Pfam <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>While these approaches are generally successful, at least one-third of microbial proteins cannot be annotated through alignment to characterized sequences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Moreover, the run times of methods such as BLASTp scale nearly linearly with the size of the labelled database, which is growing exponentially <ref type="bibr" target="#b7">[8]</ref>. Running all 17,929 Pfam HMMs against a single sequence takes a few minutes, and about 90 hours for the 54.5 million sequences in Pfam full <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Broad protein families require muliple HMM profiles to model their diversity <ref type="bibr" target="#b11">[12]</ref>, while more than 22% of the highly-curated families in Pfam 32.0 have no functional annotation. More generally, models that predict function from sequence are limited by pipelines that require substitution matrices, sequence alignment, and hand-tuned scoring functions.</p><p>Deep learning provides an opportunity to bypass these bottlenecks and directly predict protein functional annotations from sequence data. In these frameworks, a single model learns the distribution of multiple classes simultaneously, and can be rapidly evaluated. Besides providing highly accurate models, the intermediate layers of a deep neural network trained with supervision can capture high-level structure of the data through learned representations <ref type="bibr" target="#b12">[13]</ref>. These can be leveraged for exploratory data analysis or supervised learning on new tasks, in particular those with limited data. For example, novel classes can be identified from just a few examples through few-shot learning.</p><p>This raises the question of whether deep learning can provide protein function prediction tools with broad coverage of the protein universe, as found in the 17929 families of the recent Pfam 32.0 release <ref type="bibr" target="#b13">[14]</ref>. Recent work that applies deep learning is either restricted to regimes that are not practical or representative in terms of the number of families or required family size, or does not provide comparison to existing approaches that enjoy widespread use. For example, DeepSF classifies sequences into 1195 SCOP fold classes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, while DeepFam <ref type="bibr" target="#b16">[17]</ref> considers 2892 COG families of more than 100 sequences each. SECLAF uses hundreds of SwissProt classes with more than 150 sequences to train a deep model <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. DEEPre uses sequence and Pfam annotations to predict enzyme commission (EC) classes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. D-SPACE reports powerful embeddings learned by a deep model, but does not compare classification performance with existing methods <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b23">[24]</ref> a graph convolutional network reduces the required family size to 20 sequences, reporting 58% accuracy. It is encouraging that novel deep learning predictions for four functional classes were experimentally validated in <ref type="bibr" target="#b24">[25]</ref>.</p><p>Building confidence in deep learning approaches requires benchmarks that enable fair and rigorous comparison with existing state of the art methods and among deep model architectures. In this paper we pose protein function prediction as a sequence annotation task. We use 1.34 million Pfam seed sequences to construct a benchmark, and assess the performance of deep models at annotating unaligned protein domain sequences. This benchmark has an order of magnitude both more families and fewer examples per family than previous deep learning efforts (Supplementary Fig. <ref type="figure" target="#fig_0">1</ref>). We show that the deep models are substantially faster at annotating held-out test sequences than state of the art profile HMM and BLASTp approaches, and reduce the error rate almost ten-fold. We use the joint representation learned across protein families in one-shot learning to annotate sequences from small families that the model was not trained on. These findings support claims that deep learning models have the potential to provide a general solution to the challenge of protein functional annotation, and accelerate our ability to understand and exploit metagenomic sequence data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We use the Pfam seed dataset to construct a benchmark 1 and compare the performance of deep learning models with existing methods including BLASTp, phmmer and HMMER. ProtENN uses a simple majority vote across an ensemble of 13 Deep CNN (ProtCNN) models to achieve a predictive accuracy of 99.84%, reducing both the HMM and BLASTp error rates by a factor of 9, to 201 misclassified sequences (Table <ref type="table" target="#tab_0">1</ref>, Supplementary Fig. <ref type="figure">2</ref>). Fig. <ref type="figure" target="#fig_0">1</ref> shows that both ProtCNN and ProtENN generalize well to sequences that are distinct from those in the training set. A single ProtCNN model has an error rate of 0.5%, reducing the HMM and BLASTp error rates threefold. Furthermore the HMMs as implemented by HMMER 3.1b yield no prediction for 445 sequences (0.35%) of the test set, increasing the number of errors to 2229. Enforcing the Pfam curated gathering thresholds for family membership would return multiple above-threshold hits for 8.5% of test sequences, further decreasing the reported HMM accuracy.</p><p>Overall, the HMM makes multiple errors for both large and small Pfam families, while ProtENN makes single errors for 151 of the 164 families where it falters. Specifically, the 201 sequences misclassified by ProtENN are drawn from 164 families of average size 141, while a single ProtCNN misclassifies 625 sequences from 550 families. In contrast, the 1784 errors made by the HMM are drawn from 392 families of average size 1091. Fig. <ref type="figure">2</ref> shows error rates for families where both the HMM and ProtENN make mistakes. We find 11 sequences that are consistently predicted incorrectly in exactly the same way by all ensemble elements of ProtENN. Supplementary Table <ref type="table">6</ref> suggests that there is some ambiguity about their correct family label in each case. For example, our models predict that the sequence R7EGP4_9BACE/13-173 from Pfam family DUF1282, actually belongs to the YIP1 family. The hhsearch <ref type="bibr" target="#b3">[4]</ref> tool predicts that DUF1282 is similar to the Yip1 family, while, BLASTp finds that this sequence is identical to sequences annotated as the YIP1 family, agreeing with the ProtENN prediction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Annotation for Pfam full</head><p>The predictive accuracy of deep learning models typically improves as the amount of welllabelled training data increases. Likewise, accuracy of nearest-neighbor methods models such as BLASTp also improves, but at the expense of computational performance. To compare these approaches on a larger dataset, we use the 54.5 million sequences of the Pfam full database <ref type="bibr" target="#b8">[9]</ref> and follow the protocol established for the seed benchmark: we randomly split each family, assigning 80% of sequences to the train set and 10% each to dev and test sets, and carry out a hyperparameter search to optimize ProtCNN accuracy for this new task. To provide a highly accurate baseline we impute labels via the top BLASTp hit, using the training set as the query database. Our resulting ProtCNN model has an error rate of just 1.26% (⇠69k errors), lower than the BLASTp error rate of 1.78% (⇠97k errors). ProtENN, ensembled across 13 ProtCNN models, reduces the error even further to just 0.5% (⇠25k errors). Fig. <ref type="figure" target="#fig_1">3</ref> shows that ProtENN is more accurate than BLASTp across all deciles of sequence identity, and also for sequences from all but the smallest 10% of families. This reduction in error rate is particularly attractive when coupled with the gains in computational performance achieved by ProtENN.</p><p>Figure <ref type="figure">2</ref>: Comparison of the 67 families for which there was at least one error for both ProtENN and HMM top pick for the Pfam seed dataset. ProtENN achieves better performance for families that lie above the line y=x, while the opposite is true for those families that fall below this line. HMM top pick tends to have higher error rates for larger families. Additional details are available in Supplementary Fig. <ref type="figure" target="#fig_2">4</ref> and Supplementary Table <ref type="table">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Performance</head><p>Once trained, the deep learning models provide substantial speed improvements for protein domain sequence annotation. Table <ref type="table" target="#tab_2">2</ref> compares their computational performance with existing methods on our Pfam seed benchmark. ProtCNN is almost twice as fast as HMMER 3.1b, the fastest (though not the most accurate) existing method.</p><p>The BLASTp calculation for the Pfam full test set (⇠10% of Pfam full) takes 34 days on a 96 core CPU, while computing all test predictions for a single ProtCNN model takes less than 3.6 hours on a single GPU. Our most accurate ProtCNN model for Pfam full has just a single ResNet block, and as a result it is more than 200 times faster and more accurate than BLASTp. The estimated prediction times for the 54.5 million sequences of the Pfam full dataset are given in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What does ProtCNN learn?</head><p>To interrogate what ProtCNN learns about the natural amino acids, we add a 5-dimensional trainable representation between the one-hot amino acid input and the embedding network (see Methods for details), and retrain our ProtCNN model on the same unaligned sequence data from Pfam full, achieving the same performance. Fig. <ref type="figure" target="#fig_2">4A</ref> (left) shows the cosine similarity matrix of the resulting learned embedding, while Fig. <ref type="figure" target="#fig_2">4A</ref> (right) shows the BLOSUM62 matrix, created using aligned sequence blocks at roughly 62% identity <ref type="bibr" target="#b25">[26]</ref>.  The structural similarities between these matrices suggest that ProtCNN has learned known amino acid substitution patterns from the unaligned sequence data.</p><p>We next ask whether ProtCNN can distinguish between variants of the same protein domain sequence with single amino acid substitutions, despite the lack of residue-level supervision during training. To measure the predicted impact of sequence changes, we use a single ProtCNN trained on Pfam full to calculate the model's predicted distribution over classes for the original and modified sequences. We then compute the KL-divergence between these two probability distributions to quantify the effect of the subsitution on the model prediction. Fig. <ref type="figure" target="#fig_2">4B</ref> reports this measure for every possible single amino acid substitution within an ATPase domain sequence. Most substitutions in the disordered region are predicted to have negligible effect, with the exception of mutations to phenylalanine, tyrosine and tryptophan, amino acids that are known to promote order <ref type="bibr" target="#b26">[27]</ref>. This ATPase domain also contains two transmembrane helices, within which the order of amino acid (using IUPAC amino acid codes) preference according to ProtCNN is FMLVI YACTS WGQHN KRPED. The suggestion that charged amino acids and proline are avoided within these regions again agrees with existing knowledge. An additional example saturation mutagenesis prediction is shown in Supplementary Fig. <ref type="figure" target="#fig_3">5</ref>. full) appropriately predicts that most substitutions in the disordered region are unlikely to change the protein's function. Substitutions to phenylalanine, tyrosine and tryptophan are predicted to have the largest effect on function within the disordered region, in agreement with their known order-promoting properties <ref type="bibr" target="#b26">[27]</ref>. The wild type sequence is available in Supplemental Table <ref type="table">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-Shot Sequence Classification</head><p>Finally, we show that ProtCNN can accurately classify sequences from families that the model has not been trained on. This is motivated by the biologically important question of novel family identification, where each novel family is anchored by a single founder sequence. Taking all but the final layer of ProtCNN generates a map from the space of protein sequence to a 1100-dimensional embedding space. Since the model is trained for classification, sequences from different families should be well-separated by this map. This provides a variety of opportunities, such as annotation of domains of unknown function and supervised learning on small datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>  We compute an average embedding for each of the 12361 large training families, and embed a single training sequence from each of the 5568 small families held out from the model. This yields a representative embedding for each of the 17929 families in Pfam 32.0. We then use proximity in this space to carry out one-shot learning for the smallest classes, which were most difficult for ProtCNN to annotate. Specifically, for each of the 710 test sequences, we perform nearest-neighbor classification (Per-Family 1-NN ) using cosine similarity in embedding space with the set of representatives. Table <ref type="table" target="#tab_5">4</ref> shows that this approach achieves a one-shot classification error rate of 15%. Increasing to two-shot learning results in an error rate of 9%.</p><p>In Per-Family 1-NN we use all available training examples for the small classes to construct per-family embeddings. In contrast, Per-Instance 1-NN finds the nearest neighbor for each test sequence among the embeddings of every training sequence. Table <ref type="table" target="#tab_5">4</ref> shows that Per-Family 1-NN is particularly powerful at accurately classifying sequences from small families. In the final three rows, we provide an upper bound for performance by training ProtCNN on data from all families, and repeating the experiments described above. Again, the Per-Family 1-NN approach outperforms both ProtCNN and Per-Instance 1-NN at accurately classifying sequences from small families. We speculate that Per-Family 1-NN performs better than Per-Instance 1-NN due to noise reduction that results from averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Both  <ref type="bibr" target="#b16">[17]</ref>. Our ProtENN trained across Pfam full yields a three-fold reduction in the error rate and at least an order of magnitude improvement in computational efficiency compared to BLASTp. We also show that the representation of protein sequence space learned by ProtCNN can be used in one-shot learning to classify sequences from unseen protein families. This suggests an iterative approach to novel family construction, in which a single founder sequence is used to find additional family members, which are used to update the average embedding for this putative new family and so forth, inspired by current methods such as PSI-BLAST <ref type="bibr" target="#b2">[3]</ref>, jackhmmer <ref type="bibr" target="#b10">[11]</ref> and hhblits <ref type="bibr" target="#b3">[4]</ref>. This result suggests that a deep model trained on an existing corpus of data (here the training sequences from large Pfam seeds) can build a new family from a single sequence. Future work will test this approach beyond the confines of the benchmark Pfam seed dataset.</p><p>Our ProtENN models achieve extremely high accuracy without prior knowledge of protein sequence properties encoded through substitution matrices, sequence alignment or hand-curated scoring functions. Additionally, the trained models enable new sequences to be labelled much more quickly than using existing state of the art alignment-based approaches. The embedding network in each ProtCNN model maps an input sequence to a single vector representation that alone can be used for accurate family classification, pairwise sequence comparison or other downstream analysis. This differs substantially from approaches such as BLASTp, phmmer and HMMER that perform classification using explicit alignment. We note that simpler models provide useful attribution of model decision making, and we anticipate that similar insights will emerge from work that improves the interpretation and understanding of deep learning models <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>.</p><p>In this work, we focus on protein domain sequence annotation to provide a benchmark with broad coverage that enables comparison with the state of the art profile HMMs provided by Pfam 32.0. Using this benchmark, we report that ResNet-based ProtCNN models are faster and more accurate than current approaches, and that simple ProtCNN model ensembles provide increased accuracy. The performance of ProtCNN is most clearly limited by memory footprint, a barrier that can be overcome with additional computational resources. The model training protocol that we describe can be applied to any set of labelled protein sequence data; while the Pfam database is carefully curated, at least 25% of sequences have no experimentally validation function <ref type="bibr" target="#b13">[14]</ref>, and additional experimental functional characterization of protein sequences would be highly valuable. Our results suggest that deep learning models can rapidly and efficiently annotate novel protein sequences, such approaches have the potential to unlock novel molecular diversity for both therapeutic and biotechnology applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning Models</head><p>We use unaligned sequence data to train deep learning models that learn the distribution across protein families through joint optimization of a softmax regression loss function. The input network maps a sequence of L amino acids to an L ⇥ 20 binary array, where each column is a one-hot amino acid representation. Sequences are padded to the length of the longest sequence in the batch with all-zero vectors on the right (Fig. <ref type="figure" target="#fig_3">5A</ref>). The embedding neural network maps the L ⇥ 20 array containing the one-hot amino acid representation of the sequence to an L ⇥ F array that contains an embedding for each sequence residue (see Fig. <ref type="figure" target="#fig_3">5B</ref>). For residues outside the set of the 20 natural amino acids, we use a column of zeros. All processing in the the subsequent embedding network is designed such that it is invariant to the padding that was introduced for a given sequence. Details of neural network hyperparameters that were tuned using the development set are provided in Supplementary Tables 2, 3, 4 and 5, and used in Supplementary Figure <ref type="figure">2</ref>. Fig. <ref type="figure" target="#fig_3">5</ref> depicts the input, embedding and prediction networks that make up each deep learning model. The input and prediction networks have the same functional form for all models, while the embedding network varies.</p><p>Our ProtCNN networks use residual networks (ResNets <ref type="bibr" target="#b31">[32]</ref>, a variant of convolutional neural networks that train faster and are more stable, even with many layers <ref type="bibr" target="#b31">[32]</ref>). Fig. <ref type="figure" target="#fig_3">5C</ref> depicts the ResNet architecture, which includes dilated convolutions <ref type="bibr" target="#b32">[33]</ref>. The ProtCNN networks are translationally invariant, an advantage for working with unaligned protein sequence data. Convolutional architectures build up layered representations spanning many residues. An n-dilated 1d-convolution takes standard convolution operations over every nth element in a sequence, allowing local and global information to be combined without greatly increasing the number of model parameters. An important composite hyperparameter is the receptive field size of each per-residue feature, which describes the length of the subsequence that affects its value. Using dilated convolutions enables larger receptive field sizes to be obtained without an explosion in the number of model parameters. For this rigorous benchmark setup, Supplementary Fig. <ref type="figure">2</ref> suggests that larger receptive fields correspond to higher accuracy values (though see the note below about memory footprint). To our knowledge, this is the first application of dilated convolutions to protein sequence classification.</p><p>The L ⇥ F array is then pooled along the length of the sequence ensuring invariance to padding. The prediction network maps the output of the embedding network F to a distribution over labels using a multi-class logistic regression model, where the vector of probabilities is obtained as SoftMax(W f + b) and W and b are learned weights and biases. The model prediction is the most likely label under this distribution. At train time, the log-likelihood and its gradient with respect to the parameters of the prediction and embedding networks are computed using standard forward and back propagation.</p><p>ProtCNN is orders of magnitude faster at making predictions than BLASTp; the basic 13 certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.</p><p>The copyright holder for this preprint (which was not this version posted May 4, 2019. ; https://doi.org/10.1101/626507 doi: bioRxiv preprint</p><formula xml:id="formula_0">a A ADE [[1, 0, 0, ...]] [[1, 0, 0, ...], [0, 1, 0, ...], [0, 0, 1, ...]] [[1, 0, 0, ...],</formula><p>[0, 0, 0, ...], [0, 0, 0, ...]] numerical operations required can be parallelized both along the length of the sequence and across multiple sequences, and can be accelerated on modern hardware. In addition to the CNN models we also trained a recurrent neural network (RNN) with single-layer bidirectional LSTM <ref type="bibr" target="#b33">[34]</ref>, which achieved accuracy of 0.982 on the Pfam seed dataset. Further details are provided in the Supplementary Information. Replicate Deep CNN models trained on different orderings of the same data with different random parameter initializations were found to make distinct errors, which led us to use an ensemble of ProtCNN models that we call ProtENN.</p><formula xml:id="formula_1">[[1, 0, 0, ...], [<label>0</label></formula><p>Overall those ProtCNN models that perform best tend to have the largest memory footprint, to some extent irrespective of how that memory footprint is achieved. Increasing the number of model parameters via the number of filters, the kernel size and/or the number of ResNet blocks, and increasing the batch size can all produce performance improvements. Fundamentally, the memory footprint of the models we trained was limited by the amount of memory available on a single GPU, necessitating trade offs among these different factors. We didn't explore TPUs, multiple GPUs or CPUs, all of which could result in better models. This suggests that there is headroom for future machine learning developments on this task. Among the experiments that we ran, perhaps surprisingly, the best performing ProtCNN for Pfam full consisted of a single residual block with 2000 filters, a kernel size of 21, and a batch size of 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>We use the Adam optimizer <ref type="bibr" target="#b34">[35]</ref>. The learning rate is subject to exponential decay following a warm-up period, although the warm-up period was not tuned. At train time, we present the model with randomly-drawn batches. Consistent with popular experience <ref type="bibr" target="#b35">[36]</ref>, we find that it is important to use gradient clipping for the RNN, and adaptive gradient clipping worked significantly better than static gradient clipping <ref type="bibr" target="#b36">[37]</ref>. In response, we use adaptive gradient clipping for all the deep models. For more information on training and inference performance across the different models, see the Supplementary Methods, and Table <ref type="table">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rigorous Benchmark Dataset</head><p>To benchmark the performance of different models at unaligned protein domain sequence classification and compare deep models to current state of the art models including profile HMMs we use the highly curated Protein families (Pfam) database <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>. The 17929 families of the Pfam 32.0 release are labelled using HMMs that provide broad coverage of the known protein universe; 77.2% of the ⇠137 million sequences in UniprotKB have at least one Pfam family annotation, including 74.5% of proteins from reference proteomes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>. Many domains have functional annotations, although at least 22% of Pfam domains have "unknown function". The HMM for each Pfam family is built from a manually curated family seed alignment, containing between 1 and 4545 protein domain sequences of length 4-2037 amino acids. Some basic statistics about the Pfam family seed sequences can be found in Supplementary Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>We split each Pfam family with at least 10 seed sequences randomly into disjoint dev 2 (10%, rounding down to the nearest integer) and test (10%) sets, allocating remaining sequences to the training set (Table <ref type="table" target="#tab_7">5</ref>). Of the 17929 Pfam seed families, 4858 families have &lt; 10 seed sequences and are only present in the train set. This results in heldout test sequences for 13071 families, where 2819 families have exactly one sequence in each of the test and dev sets. Including families that only exist as training examples makes the task harder because there are more ways each test sequence can be misclassified. Note that we do not expect the HMM-based approach to achieve 100% accuracy because the training data is a subset of the seed data set used in Pfam. For reproducibility, we provide the split Pfam seed dataset for download, 3 and at Kaggle, together with an interactive Jupyter notebook. 4  For the HMMs, we retain the alignment information from the whole Pfam seed for all splits to avoid any artifacts introduced by realignment, and enable optimal performance. During training this provides the HMM with information about the held-out test sequences used to measure performance, meaning that the reported accuracy should be taken as an upper bound. In contrast all alignment information is removed from the data for our deep learning models and for the other baselines. Our strategy of working with the Pfam seed sequence set circumvents the computationally intensive process of evaluating phmmer and profile HMM performance on the full set of ⇠54.5 million Pfam sequences. We did a small amount of tuning to increase the speed of running phmmer so as to give a fair comparison; more information can be found in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k-mer</head><p>An alignment-free approach is provided by a k-mer (or n-gram) based model, where each sequence is represented by the set of k-mers that it contains. We train a multi-class logistic regression model on vectors of k-mer counts using the same stochastic gradient descent procedure as used by our deep models (Supplementary Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLASTp</head><p>BLASTp <ref type="bibr" target="#b37">[38]</ref> is one of the most well known algorithms for searching for similar sequences, and among the current state of the art. It uses an alignment to rank sequences according to their similarity to a target sequence, and from this, a user can impute functional annotation by ascribing known functions of similar sequences. We use BLASTp as a 1-nearest neighbor algorithm by first using makeblastdb (version 2.7.1+) with the training data, and then query sequences from that database using blastp -query, taking only the top hit. This implementation returns no hit for 259 (0.21%) of the 126171 sequences in the Pfam seed test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HMM top pick implementation</head><p>Profile HMMs are widely regarded as a state of the art modeling technique for protein sequence classification. We used hmmbuild from HMMER 3.1b to construct a profile HMM from the aligned train sequences for each family in Pfam 32.0. We implement a simple top pick HMM strategy (see Methods) to avoid any handicap from the filters built into HMMER 3.1b. To further obtain the best possible profile HMM performance we retain the alignment from the entire Pfam seed, avoiding dependence on any particular realignment method. We then use the hmmsearch function from HMMER 3.1b to search all 17929 profiles against the set of unaligned test sequences using the default parameter settings.</p><p>The scores for each hit are recorded, and the profile with the highest score called as the HMM prediction for that test sequence. To ensure the HMMER 3.1b statistical filters do not hamper performance, we manually turn them off to the extent that at least one hit is reported for each test sequence, and take the top scoring hit. To implement this, for those test sequences with no profile hit after this first pass, we employ a second hmmsearch pass using the '--max' option, which turns off all filters and runs full Forward/Backward inference on every target to increase the sensitivity of the search at a significant cost in speed <ref type="bibr" target="#b38">[39]</ref>. In experiments that retain the HMMER 3.1b filters for hmmsearch, we found that 8.5% of test sequences returned multiple hits above the family specific gathering thresholds that are used by Pfam to regulate family membership. Reporting these results would have resulted in a significantly lower precision score for HMMER than for the deep models, which is why we have chosen instead to remove the statistical filter and report the top hit (Supplementary Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>The positive results obtained in the absence of rigorous statistical filters likely reflect the fact that we are working with sequences that were originally classified by Pfam, and so passed the rigorous statistical thresholds set for inclusion in a Pfam family. Those sequences that did not pass these filters, and hence were not included in any Pfam family may well have posed a more significant challenge to our implementation. For this reason we do not recommend that this HMM implementation is used in settings other than working with these benchmark datasets. For Pfam full we do not use the HMMs as a baseline because these models were used to label the data, so will achieve 100% accuracy by default. The Pfam full dataset has 17772 families overall, and our test and dev sets contain sequences from 16755 families.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of model error rate (log scale) on the held-out test set for Pfam seed sequence data as a function of (A) sequence identity (for the highest scoring pair found in the training set as reported by BLASTp), and (B) as a function of family size. ProtENN performs substantially better than all other models across all distances and family sizes. Additional breakdowns of this data are available in Supplementary Fig.3.</figDesc><graphic url="image-2.png" coords="4,184.65,335.33,257.05,189.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of model error rate on the held out test set for Pfam full sequence data (A) as a function of sequence identity (for highest scoring pair found in the training set as reported by BLASTp), and (B) as a function of family size.</figDesc><graphic url="image-5.png" coords="7,184.65,346.48,257.04,194.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (A) The amino acid embedding extracted from the trained ProtCNN model yields cosine similarities in embedding space that reflect the overall structure of the BLOSUM62 matrix [26]. (B) Predicted change in function for each missense mutation in ATPase domain AT1A1_PIG/161-352 from family PF00122.20. The ProtCNN model (trained using Pfam</figDesc><graphic url="image-7.png" coords="9,120.40,353.43,385.56,107.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Architecture descriptions of neural networks. (A) Adding padding to a sequence for batching. (B) The model architecture shared by all neural networks includes the Input (red), Embedding (yellow), and Prediction (green) Networks. (C) ResNet architecture used by the ProtCNN models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc><ref type="bibr" target="#b2">3</ref>. Performance on randomly-split data. For additional breakdown of this data see Supplementary Figs.3 and 4.</figDesc><table><row><cell>Model</cell><cell cols="2">Error rate Number of errors</cell></row><row><cell>HMM Top Pick</cell><cell>1.414%</cell><cell>1784</cell></row><row><cell>phmmer</cell><cell>1.531%</cell><cell>1932</cell></row><row><cell>BLASTp</cell><cell>1.654%</cell><cell>2087</cell></row><row><cell>k-mer</cell><cell>9.994%</cell><cell>12610</cell></row><row><cell>RNN</cell><cell>1.800%</cell><cell>2271</cell></row><row><cell>1-ResNet block CNN</cell><cell>1.120%</cell><cell>1413</cell></row><row><cell>2-ResNet block CNN</cell><cell>0.852%</cell><cell>1075</cell></row><row><cell>ProtCNN</cell><cell>0.495%</cell><cell>625</cell></row><row><cell>ProtENN</cell><cell>0.159%</cell><cell>201</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>7</head><label></label><figDesc>certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.</figDesc><table><row><cell>bioRxiv preprint</cell><cell>doi:</cell><cell>; https://doi.org/10.1101/626507</cell><cell cols="3">this version posted May 4, 2019.</cell><cell>The copyright holder for this preprint (which was not</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Inferences per second</cell><cell>Time to predict all 1.34 million Pfam seed sequences</cell></row><row><cell></cell><cell></cell><cell>phmmer</cell><cell></cell><cell>2.52</cell><cell>6.15 days</cell></row><row><cell></cell><cell></cell><cell>HMMER</cell><cell></cell><cell>150.20</cell><cell>2.48 hours</cell></row><row><cell></cell><cell></cell><cell cols="2">HMMER (no filters)</cell><cell>0.66</cell><cell>23.58 days</cell></row><row><cell></cell><cell></cell><cell>BLASTp</cell><cell></cell><cell>11.27</cell><cell>15.2 hours</cell></row><row><cell></cell><cell></cell><cell>ProtCNN</cell><cell></cell><cell>247.39</cell><cell>1.5 hours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Inference speed comparison of different models trained on the Pfam seed training set. BLASTp was run on a 96 core CPU, while ProtCNN was run on a P100 GPU (See Supplementary Table7for hardware details).</figDesc><table><row><cell></cell><cell>Inferences per second</cell><cell>Time to predict all 54.5 million Pfam full sequences</cell></row><row><cell>BLASTp</cell><cell>1.85</cell><cell>340 days</cell></row><row><cell>ProtCNN</cell><cell>415.74</cell><cell>36 hours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Inference speed comparison of different models trained on the Pfam full training set. BLASTp was run on a 96 core CPU, while ProtCNN was run on a NVIDIA P100 GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. Here, we proceed by training a ProtCNN model on the 12361 Pfam seed families with more than 9 training sequences. The remaining 5568 families consist of 710 families that have a single held out test sequence, and the 4858 smallest families that have no test sequences (because they were so small, see methods). By construction, this ProtCNN model achieves 0% accuracy on the 710 test sequences from families not seen during training.</figDesc><table><row><cell></cell><cell>Families Included</cell><cell>Overall</cell><cell>Small Family</cell><cell>Large Family</cell></row><row><cell>Prediction Method</cell><cell>in Training</cell><cell>Error Rate</cell><cell>Error Rate</cell><cell>Error Rate</cell></row><row><cell>ProtCNN</cell><cell></cell><cell>1.0%</cell><cell>100.0%</cell><cell>0.4%</cell></row><row><cell>One-Shot 1-NN</cell><cell></cell><cell>0.8%</cell><cell>14.9%</cell><cell>0.7%</cell></row><row><cell>Two-Shot 1-NN</cell><cell>Large only</cell><cell>0.8%</cell><cell>9.0%</cell><cell>0.7%</cell></row><row><cell>Per-Family 1-NN</cell><cell></cell><cell>0.7%</cell><cell>1.3%</cell><cell>0.7%</cell></row><row><cell>Per-Instance 1-NN</cell><cell></cell><cell>0.6%</cell><cell>2.4%</cell><cell>0.6%</cell></row><row><cell>ProtCNN</cell><cell></cell><cell>0.4%</cell><cell>3.0%</cell><cell>0.4%</cell></row><row><cell>Per-Family 1-NN</cell><cell>All</cell><cell>0.7%</cell><cell>0.8%</cell><cell>0.7%</cell></row><row><cell>Per-Instance 1-NN</cell><cell></cell><cell>0.5%</cell><cell>2.3%</cell><cell>0.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance when classifying using nearest neighbors in embedding space. resolution on error rates is limited by the number of sequences in the test set (710).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>ProtCNN and ProtENN produce state of the art accuracy for protein domain annotation on a benchmark built from Pfam seed that is representative of known protein sequence space. With just under 1.1 million training examples across 17929 output families of vastly different sizes (Supplementary Fig. 1) our ProtENN models are highly accurate despite having no access to the alignments used to train the HMMs. These results present a significant advance over prior work; for example, 8727 families from the benchmark have at least one test sequence and fewer than the minimum 67 training examples used for classification by DeepFam</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Number of training and testing examples for the randomly split Pfam seed data. Note that 16755 families have sequences in the dev and test sets for the Pfam full data.phmmerWe take the set of unaligned training sequences as a sequence database, and using phmmer from HMMER 3.1b<ref type="bibr" target="#b10">[11]</ref> we query each test sequence against this database to find the closest match. Those test sequences that return hits above the default phmmer reporting threshold are then annotated with the label of the training sequence hit with the highest bit score. Out of the 126171 sequences in the test set, 42 did not return a hit using this approach. All training sequences that are not reported as hits by the phmmer function are assumed to have a zero bit score match to that query sequence.</figDesc><table><row><cell>Number of families</cell></row></table></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>https://console.cloud.google.com/storage/browser/ brain-genomics-public/research/proteins/pfam/random_split, interactive notebook at https: //www.kaggle.com/googleai/pfam-seed-random-split</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1026</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering huge protein sequence sets in linear time</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2542</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gapped blast and psi-blast: a new generation of protein database search programs</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Stephen F Altschul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><forename type="middle">A</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Schäffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Webb</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3389" to="3402" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein homology detection by hmm-hmm comparison</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="951" to="960" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hmmer web server: interactive sequence similarity searching</title>
		<author>
			<persName><forename type="first">Jody</forename><surname>Robert D Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><surname>Eddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">suppl_2</biblScope>
			<biblScope unit="page" from="W29" to="W37" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mutant phenotypes for thousands of bacterial genes of unknown function</title>
		<author>
			<persName><surname>Morgan N Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Kelly M Wetmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayashree</forename><surname>Callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hualan</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Kuehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">S</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumi</forename><surname>Lamson</surname></persName>
		</author>
		<author>
			<persName><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combrex-db: an experiment centered database of protein function: knowledge, predictions and knowledge gaps</title>
		<author>
			<persName><forename type="first">Yi-Chien</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Rachlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">P</forename><surname>Anton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kasif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steffen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D330" to="D335" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compressive genomics for protein databases</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Noah M Daniels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Gallant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenore</forename><forename type="middle">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Baym</surname></persName>
		</author>
		<author>
			<persName><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="283" to="290" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pfam: a comprehensive database of protein domain families based on seed alignments</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Erik Ll Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><surname>Durbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="420" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hidden markov model speed heuristic and iterative hmm search procedure</title>
		<author>
			<persName><forename type="first">Johnson</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elon</forename><surname>Portugaly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">431</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerated profile hmm searches</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e1002195</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pfam: the protein families database</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Robert D Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jody</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penelope</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><surname>Coggill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Ruth Y Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirstie</forename><surname>Heger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liisa</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Holm</surname></persName>
		</author>
		<author>
			<persName><surname>Mistry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D222" to="D230" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pfam protein families database in 2019</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>El-Gebali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matloob</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorna</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Smart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D427" to="D432" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepsf: deep convolutional neural network for mapping protein sequences to folds</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badri</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1295" to="1303" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<author>
			<persName><forename type="first">Antonina</forename><surname>Andreeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Howorth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><forename type="middle">Jp</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Chothia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><forename type="middle">G</forename><surname>Murzin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scop database in 2004: refinements integrate structure and sequence family data</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="D226" to="D229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepfam: deep learning based alignment-free method for protein family modeling and prediction</title>
		<author>
			<persName><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsik</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjune</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="254" to="262" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Near perfect protein multi-label classification with deep neural networks</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Szalkai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vince</forename><surname>Grolmusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="50" to="56" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Seclaf: a webserver and deep neural network design tool for hierarchical biological sequence classification</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Szalkai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vince</forename><surname>Grolmusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="2487" to="2489" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Uniprot: the universal protein knowledgebase</title>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D158" to="D169" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepre: sequence-based enzyme ec number prediction by deep learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramzan</forename><surname>Umarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="760" to="769" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">mldeepre: Multi-functional enzyme function prediction with hierarchical multi-label deep learning</title>
		<author>
			<persName><forename type="first">Zhenzhen</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep semantic protein representation for annotation, discovery, and engineering</title>
		<author>
			<persName><forename type="first">Ariel</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">J</forename><surname>Hannum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><forename type="middle">R</forename><surname>Dwiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Smoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">R</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Eads</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">C</forename><surname>Lafave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harini</forename><surname>Eavani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page">365965</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Protein family classification with multi-layer graph convolutional networks</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Mansur R Kabuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2390" to="2393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep recurrent neural network for protein function prediction from sequence</title>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08318</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Amino acid substitution matrices from protein blocks</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Henikoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jorja</surname></persName>
		</author>
		<author>
			<persName><surname>Henikoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="10915" to="10919" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Top-idp-scale: a new amino acid scale measuring propensity for intrinsic disorder</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Campen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celeste</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Uversky</surname></persName>
		</author>
		<author>
			<persName><surname>Keith Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein and peptide letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="956" to="963" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-only deep representation learning</title>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">589333</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">What made you do this? understanding black-box decisions with sufficient input subsets</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gifford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Critiquing protein family classification models using sufficient input subsets</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In preparation</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent nn</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">first results. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Basic local alignment search tool</title>
		<author>
			<persName><forename type="first">Warren</forename><surname>Stephen F Altschul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Webb</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Hmmer user&apos;s guide. biological sequence analysis using profile hidden markov models</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Eddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The pfam protein families database: towards a more sustainable future</title>
		<author>
			<persName><forename type="first">Penelope</forename><surname>Robert D Finn</surname></persName>
		</author>
		<author>
			<persName><surname>Coggill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Ruth Y Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">L</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matloob</forename><surname>Punta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amaia</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><surname>Sangrador-Vegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D279" to="D285" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Google vizier: A service for black-box optimization</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Daniel Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhodeep</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName><surname>Karro</surname></persName>
		</author>
		<author>
			<persName><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1487" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A helix propensity scale based on experimental studies of peptides and proteins</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Pace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Scholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biophysical journal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="422" to="427" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
