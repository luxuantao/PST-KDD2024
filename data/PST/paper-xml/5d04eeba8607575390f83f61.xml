<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Translation Ranger: Operating System Support for Contiguity-Aware TLBs</title>
				<funder ref="#_nncBU9x #_8FrYzTr">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zi</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University &amp; NVIDIA</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ISCA &apos;19</orgName>
								<address>
									<addrLine>June 22-26</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
							<email>dlustig@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ISCA &apos;19</orgName>
								<address>
									<addrLine>June 22-26</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
							<email>abhishek@cs.yale.edu</email>
							<affiliation key="aff2">
								<orgName type="department">ISCA &apos;19</orgName>
								<address>
									<addrLine>June 22-26</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ISCA &apos;19</orgName>
								<address>
									<addrLine>June 22-26</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Nellans</surname></persName>
							<email>dnellans@nvidia.com</email>
							<affiliation key="aff3">
								<orgName type="department">ISCA &apos;19</orgName>
								<address>
									<addrLine>June 22-26</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Translation Ranger: Operating System Support for Contiguity-Aware TLBs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3307650.3322223</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Translation Lookaside Buffers</term>
					<term>Memory defragmentation</term>
					<term>Operating system</term>
					<term>Heterogeneous memory management</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Virtual memory (VM) eases programming effort but can suffer from high address translation overheads. Architects have traditionally coped by increasing Translation Lookaside Buffer (TLB) capacity; this approach, however, requires considerable hardware resources. One promising alternative is to rely on software-generated translation contiguity to compress page translation encodings within the TLB. To enable this, operating systems (OSes) have to assign spatially-adjacent groups of physical frames to contiguous groups of virtual pages, as doing so allows compression or coalescing of these contiguous translations in hardware. Unfortunately, modern OSes do not currently guarantee translation contiguity in many real-world scenarios; as systems remain online for long periods of time, their memory can and does become fragmented.</p><p>We propose Translation Ranger, an OS service that recovers lost translation contiguity even where previous contiguitygeneration proposals struggle with memory fragmentation. Translation Ranger increases contiguity by actively coalescing scattered physical frames into contiguous regions and can be leveraged by any contiguity-aware TLB without requiring changes to applications. We implement and evaluate Translation Ranger in Linux on real hardware and find that it generates contiguous memory regions 40? larger than the Linux default configuration, permitting TLB coverage of 120GB memory with typically no more than 128 contiguous translation regions. This is achieved with less than 2% run time overhead, a number that is outweighed by the TLB coverage improvements that Translation Ranger provides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computer systems organization ? Heterogeneous (hybrid) systems; ? Software and its engineering ? Virtual memory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Virtual memory (VM) eases programming in many ways. It abstracts the complexity of physical memory, provides memory protection and process isolation, and facilitates communication between cores and/or compute units through a shared virtual address space. Virtual memory is used today not just for CPUs but also increasingly for GPUs and other accelerators <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">47]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows an example system with a group of CPUs, GPUs, and other accelerators, where each type of device can directly access both its own memory as well the memory of other devices via the virtual memory system.</p><p>High performance virtual memory can be achieved by embedding a Translation Lookaside Buffer (TLB) in each computation unit to cover all physical memory. However, since covering all of physical memory would require considerable translation storage overheads <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">45]</ref>, vendors today choose to implement TLBs that cover only a portion of the total memory space. For example, Intel has been (approximately) doubling its CPU TLB resources every generation from Sandybridge through Skylake <ref type="bibr" target="#b19">[20]</ref>, resulting in TLBs with thousands of entries today. Vendors like AMD implement even larger TLBs for their GPUs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">49]</ref>, but these large TLBs consume non-trivial area and power <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">36]</ref> and are ill-suited for other types of accelerators with limited hardware resources <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b45">45]</ref>.</p><p>Recent studies focusing on the address translation wall <ref type="bibr" target="#b6">[7]</ref> propose using translation contiguity to mitigate VM overheads <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39]</ref>. A contiguous region maps a set of contiguous virtual pages to a corresponding group of spatially-adjacent physical frames. Contiguous regions are desirable for emerging TLB designs that can compress these translations into a single TLB entry. Ideally, a contiguous region of N pages can be stored with just a single TLB entry rather than N entries. Figure <ref type="figure" target="#fig_1">2</ref> shows the general concept of contiguity <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">39]</ref>, and is representative of several aggressive proposals to exploit contiguity with range TLBs <ref type="bibr" target="#b22">[23]</ref>, devirtualizing memory <ref type="bibr" target="#b17">[18]</ref>, and direct segments <ref type="bibr" target="#b3">[4]</ref>. These approaches, combined with traditional huge page techniques <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b46">46]</ref>, may be able to drive address translation overheads to near zero in future systems.</p><p>Although influential, existing translation contiguity proposals typically require either specific amounts of contiguity (e.g., discrete page-sized contiguity for huge pages <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b44">44]</ref>), restricted types of contiguity (e.g. where virtual and physical pages must be identity mapped <ref type="bibr" target="#b17">[18]</ref>), or serendipitously-generated contiguity (e.g., TLB coalescing <ref type="bibr" target="#b39">[39]</ref>). Others still require large swaths of contiguity that can be created only at memory (pre)allocation time (e.g., direct segments and ranges <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>). The question of how OSes can actively generate unrestricted and general-purpose contiguity from any arbitrary starting condition (e.g., after active use) remains open.</p><p>Our goal is to develop OS support for creating general-purpose translation contiguity in a robust manner across all execution environments. This means that unlike prior work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref>, we cannot generate contiguity only during initial memory allocations; we must also generate it throughout the workload's lifetime. It means that we must be able to generate contiguity on real-world systems with long uptimes, where memory may be (heavily) fragmented by diverse workloads that are spawned and terminated over time. Further, we cannot not rely on OS/application customization via mechanisms like identity mappings <ref type="bibr" target="#b17">[18]</ref> or programmer-specified segments <ref type="bibr" target="#b3">[4]</ref> which can preclude important OS features like copyon-write or paging to disk, and may affect security features like address space layout randomization (ASLR) <ref type="bibr" target="#b17">[18]</ref>.</p><p>To achieve this, we propose Translation Ranger, a new OS service that actively coalesces fragmented pages from the same virtually contiguous range to generate unrestricted amounts of physical memory contiguity in realistic execution scenarios. This enables any previously proposed TLB optimization-e.g., from COLT <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b39">39]</ref>, to direct segments <ref type="bibr" target="#b3">[4]</ref>, to Range TLB <ref type="bibr" target="#b22">[23]</ref>, to hybrid coalescing TLBs <ref type="bibr" target="#b37">[37]</ref> to devirtualizing memory <ref type="bibr" target="#b17">[18]</ref>-to compress information about address translations into fewer hardware TLB entries. The contributions of this work are:</p><p>(1) We propose active OS page coalescing to generate unbounded amounts of translation contiguity in all execution environments regardless of the amount of memory fragmentation in the system. Because it does not depend on custom hardware support, Translation Ranger is widely applicable in any system with TLB support for contiguity, whether that system is already commercially available <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">39]</ref> or relies on emerging research proposals such as range TLBs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>.</p><formula xml:id="formula_0">V0 ? P4</formula><p>Traditional TLB Contiguity-aware TLB (2) We implement Translation Ranger in the Linux v4.16 kernel to assess the feasibility of our approach. Our realsystem implementation sheds light on the subtle challenges of building Translation Ranger within real OSes. Chief among them are the challenges of reducing page migration overheads, dealing with the presence of pages deemed nonmovable by the kernel, and understanding the impact of page coalescing on user application performance. We demonstrate that it is useful to coalesce pages not only at allocation time, but also post-allocation in highly-fragmented systems. This observation goes beyond prior work which generally avoids post-allocation defragmentation because of its presumed overheads <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">39]</ref>. To encourage further research on translation contiguity and coalescing TLBs, we have opensourced our kernel implementation.<ref type="foot" target="#foot_0">1</ref> (3) We show that Translation Ranger generates significant contiguity (&gt; 90% of 120GB application footprints are covered using only 128 contiguous regions, compared to &lt; 1% without coalescing). It does so with low overhead (&lt; 2% of overall application run time while coalescing 120GB memory), thereby ensuring that the performance gain delivered via coalescing is a net win for applications.</p><formula xml:id="formula_1">V3 ? P7 V12 ? P8 V15 ? P11 [V0-V3] ? [P4-P7] [V12-V15] ? [P8-P11] [V0-V3] ? [P4-P7] [V12-V15] ? [P8-P11] ? ? Page Table V0 P4 V1 P5 V2 P6 V3 P7 V12 P8 V13 P9 V14 P10 V15 P11 V0 P4 V1 P5 V2 P6 V3 P7 V12 P8 V13 P9 V14 P10 V15 P11 ? V0 P4 V1 P5 V2 P6 V3 P7 V12 P8 V13 P9 V14 P10 V15 P11 ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>The increasing overheads of address translation and virtual memory have prompted research on techniques to mitigate their cost <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref><ref type="bibr" target="#b49">49]</ref>. We summarize these efforts in Table <ref type="table" target="#tab_0">1</ref> and discuss them before describing Translation Ranger in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Using Translation Contiguity</head><p>The earliest approaches to exploiting translation contiguity focused on huge pages <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b46">46]</ref>. OSes form huge pages by allocating groups of spatially-adjacent physical frames to a spatially-adjacent group of virtual pages in discrete-sized chunks at aligned memory boundaries.  Although huge pages can be effective, they only offer discrete chunks of contiguity. As memory capacities continue to grow, translation contiguity amounts in excess of 1GB or sizes between the discrete page sizes of 2MB and 1GB will be useful. For this reason, recent work has considered translation contiguity approaches complementary to traditional huge pages. All of these techniques require hardware support from the TLB, as outlined in Table <ref type="table" target="#tab_0">1</ref>. For example, direct segments <ref type="bibr" target="#b3">[4]</ref>, uses programmer-OS coordination to mark gigabyte-to terabyte-sized primary segments of memory that are guaranteed to be mapped using contiguous translations. While this approach can substantially reduce TLB misses, direct segments can be challenging to use for real-world workloads which need more than one primary direct segment (to allocate contiguous segments in different parts of their address space) and because of the need for explicit programmer intervention. Redundant memory mappings <ref type="bibr" target="#b22">[23]</ref> support arbitrary translation ranges in TLBs, but require invasive OS changes to produce these contiguous translation ranges. Devirtualizing memory <ref type="bibr" target="#b17">[18]</ref> extends the concepts of direct segments for area-constrained accelerators, but works under the optimistic assumption that OSes can always offer large contiguous memory regions for devirtualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Allocation-Time Contiguity</head><p>One way to create allocation-time contiguity is to reserve the memory in advance of application run. Libhugetlbfs achieves this by reserving memory at boot time <ref type="bibr" target="#b26">[27]</ref>. Allocations for 2MB or 1GB pages are satisfied from these reserved memory pools. Similarly, device drivers often use customized memory allocators that perform similar reservation in advance <ref type="bibr" target="#b10">[11]</ref>.</p><p>Other approaches in Table <ref type="table" target="#tab_0">1</ref> target contiguous allocations at run time. Prior work achieves this by making changes to the widelyused buddy memory allocator <ref type="bibr" target="#b24">[25]</ref>. The buddy allocator can be a source of translation contiguity because it groups contiguous free memory into free page pools of different sizes, from 1 to 2 N pages, where N is called the max order of the buddy allocator. Standard allocations will thus contain a maximum of 2 N contiguous pages. For example, Linux's buddy allocator supports maximum contiguous allocations of 4MB. However, because the buddy allocator must support fast insertions and deletions, free pages are stored in unordered lists. This means that one or more memory allocations cannot guarantee contiguity greater than 2 N pages even if two or more contiguous 2 N pages are available in the same free list.</p><p>Prior work creates allocation-time contiguity by increasing the max order of the buddy allocator <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>. However, this creates multiple problems. First, Linux's sparsemem (used to support discontiguous physical address spaces, which is common in modern systems) requires each contiguous physical address range to be aligned to 2 N [50]. This means that if the max order is increased to support contiguous free ranges of 1GB, many gigabytes of memory may be wasted. Second, increasing the max order does not solve the problem of fragmentation; it only allows programs to obtain large allocations in low fragmentation scenarios. Fragmentation does not directly affect the contiguity of in-use memory ranges, but it does affect the amount of contiguity available at allocation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Memory Fragmentation</head><p>To quantify the problem of fragmentation, we ran a set of benchmarks multiple times starting with a fresh-booted system with 128GB of memory (see Table <ref type="table" target="#tab_2">2</ref>) and we increased the max order of the buddy allocator to allocate large contiguous regions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>. Figure <ref type="figure" target="#fig_2">3</ref> shows that all benchmarks have &gt;80% of their footprint covered by 1GB contiguous regions (not to be confused with 1GB pages, which must also be aligned) on their first execution immediately after boot. However, as the benchmarks keep running, the number of 1GB contiguous regions drops to only 20%. With Translation Ranger, we enable better contiguity in both fresh-booted systems and heavily-fragmented systems.  Large contiguous regions are often fragmented in long-running systems for several reasons. For example, in-use pages can prevent otherwise-free buddy pages from being promoted to a larger free page pool for allocation. We show an example of this in Figure <ref type="figure" target="#fig_3">4a</ref>. Inuse pages allocated are non-movable (also called wired in FreeBSD or non-paged in Windows <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b42">42]</ref>), which means that they cannot be defragmented <ref type="bibr" target="#b34">[34]</ref>. The use of non-moveable free page pools, which are dedicated for kernel page allocations, can minimize the interleaving of kernel pages with user pages, as shown in Figure <ref type="figure" target="#fig_3">4b</ref>; this avoids non-movable page fragmentation, but prevents large contiguous regions beyond each pool size from being formed <ref type="bibr" target="#b34">[34]</ref>. This also explains why benchmarks in Figure <ref type="figure" target="#fig_2">3</ref> lose 1GB contiguous regions over multiple rounds of executions.</p><p>Surprisingly, existing OS memory compaction techniques can sometimes harm contiguity. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, Linux uses memory compaction to move in-use pages to one end of physical address space, leaving the other end with contiguous free frames. However compaction moves only base pages and not transparent huge pages (THP) because existing TLBs, which cannot coalesce contiguous THPs into a single hardware entry, are unable to benefit from higher contiguity. Compaction is also unaware of the contiguity of in-use pages, so if a set of contiguous in-use pages are moved to a set of scattered free pages, the original contiguity may be destroyed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TRANSLATION RANGER</head><p>We design Translation Ranger with several goals in mind. Translation Ranger should go beyond the restricted amount of contiguity offered by techniques like huge pages; it should support arbitrarily sized contiguity whenever possible. This contiguity should be generated even on systems with high load and memory fragmentation after page allocation. Finally, contiguity generation should be robust to system behavior no matter how many workloads have spawned, have died, or are executing on that system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Overview</head><p>Translation Ranger creates contiguity from both virtual pages and physical frames. Contiguity in virtual pages depends upon the layout of a process's virtual address space. Most OSes organize each process's virtual address space as multiple non-overlapping virtual address ranges. In Linux, each virtual address range is described by a struct vm_area_struct, or virtual memory area (VMA). Applications obtain virtually-contiguous address ranges via mmap or malloc. Physical frames are allocated and assigned to virtual pages lazily when the virtual page is accessed for the first time. This means that contiguous regions are created when contiguous virtual pages in a VMA are assigned contiguous physical frames. However, since the OS may assign any physical frame to a faulting virtual page, contiguous virtual pages in a VMA are typically mapped to non-contiguous physical frames as shown in Figure <ref type="figure" target="#fig_5">6</ref>.</p><formula xml:id="formula_2">(Begin) Physical Frames (End) VMA1 V1 V2 V3 V0 P93 P94 P95 P96 P97 P98 P99 P93 P94 P95 P96 P97 P98 P99 ? (Begin) Physical Frames (End) VMA1 V1 V2 V3 V0 P93 P94 P95 P96 P97 P98 P99 P93 P94 P95 P96 P97 P98 P99 ? Memory compaction P3 P4 P5 P6 P2 P7 P1 P0 P3 P4 P5 P6 P2 P7 P1 P0 P3 P4 P5 P6 P2 P7 P1 P0 P3 P4 P5 P6 P2 P7 P1 P0</formula><p>Translation Ranger's approach to generating translation contiguity is to rearrange the system's physical memory mappings such that each VMA can be covered by as few contiguous regions as possible, with regions that are as large as possible. Ideally, a single VMA would constitute one contiguous region, and could be tracked using just one TLB entry. To minimize region counts and maximize region sizes, Translation Ranger does the following:</p><p>(1) It assigns an anchor point to each VMA. An anchor point is a virtual page (VPN) and physical frame (PFN) pair, (V anchor , P anchor ), that acts as a reference around which Translation Ranger builds contiguity using all pages in this VMA. (2) It actively coalesces memory within each VMA based on the assigned anchor point. Figure <ref type="figure" target="#fig_5">6</ref> shows an example of active coalescing for the case where (V0, P4) is the anchor point. The VMA is coalesced by 1 migrating P2 to P4, 2 exchanging P5 with P9, 3 exchanging P7 with P6, 4 exchanging P7 with P3, ultimately leading to V0-V3 mapping to P4-P7. (3) As a background daemon, it periodically iterates over all active VMAs in the system to maintain contiguity during the course of VMA allocations, expansions, and contractions, which occur naturally through the lifetime of an application. We detail each of these steps in the next three subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Per-VMA Anchor Point Assignment</head><p>To achieve large regions of contiguity, Translation Ranger has to select anchor points carefully. It is natural to consider using the translation corresponding to the first in-use virtual page of a VMA as a good candidate for the anchor point when no anchor point has been selected for this VMA. However, to make this anchor point useful, the OS has to allocate the physical frames for these virtual pages to satisfy several requirements. The question of how multiple VMAs interact is central to this issue. Consider, for example, Figure 7a, where VMA1 contains V0-V3, while VMA2 contains V12-V15. If VMA1 and VMA2 use (V0, P4) and (V12, P8) as their respective anchor points, each VMAs can maximize the contiguity it achieves.</p><formula xml:id="formula_3">Physical Frames VMA [V0, V3] V1 V2 V3 V0 Physical Frames VMA [V0, V3] V1 V2 V3 V0 ? ? ? ? P3 P4 P5 P6 P2 P8 P9 P7 P3 P4 P5 P6 P2 P8 P9 P7 P3 P4 P5 P6 P2 P8 P9 P7 P3 P4 P5 P6 P2 P8 P9 P7</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page frame coalescing</head><p>On the other hand, Figure <ref type="figure" target="#fig_6">7b</ref> shows that a less fortunate anchor point selection of (V12, P6) for VMA2 damages contiguity formation because this anchor point selections causes inter-VMA overlap of physical frames. In this situation, coalescing VMA2 will wipe out (some of) VMA1's contiguity and vice-versa. Even worse, when these VMA sizes change, we have to revisit these mappings, which can dramatically increase the number of page migrations needed to compensate for the inter-VMA interference and increase Translation Ranger's overheads.</p><p>To avoid interference between regions during coalescing, Translation Ranger tracks the physical address space as coalesced and uncoalesced regions. Translation Ranger assigns newly allocated VMAs to the uncoalesced physical region whenever possible. When the translations within a VMA are coalesced, this region is marked as coalesced. To find uncoalesced regions for new allocations, Translation Ranger simply uses an algorithm similar to first-fit <ref type="bibr" target="#b53">[53]</ref>, i.e., scan linearly through all coalesced regions and stop at the first sufficiently large hole between any two such regions. When no such hole is found, Translation Ranger tries to accommodate it by removing the smallest VMA's anchor point; otherwise, it skips this VMA. This approach avoids inter-VMA interference, and thus substantially mitigates Translation Ranger overheads by minimizing page migrations caused by the interference. This also provides useful information for tackling the problem of VMA size changes, which we discuss further in Section 3.4.</p><p>Another consideration for anchor point placement is physical page alignment. In general, virtual page and physical frame alignment are chosen based on the needs of TLB implementations. Traditional TLBs assume that virtual pages and physical frames are aligned based on page size. For example, TLBs that can cache 2MB huge pages require that the virtual page and physical frame of the huge page begin at 2MB address boundaries. For such cases, we ensure that anchor points are aligned to the largest architectural page size smaller than or equal to the region in question. For example, a 6MB region will be 2MB-aligned so that it can be in-place promoted to three 2MB large pages. On the other hand, newer contiguity-aware TLBs like range TLBs lift this restriction and do not require any form of alignment in the contiguous regions <ref type="bibr" target="#b22">[23]</ref>. </p><formula xml:id="formula_4">VMA1 [V0, V3] V1 V2 V3 V0 VMA1 [V0, V3] V1 V2 V3 V0 VMA2 [V12, V15] V13 V14 V15 V12 VMA2 [V12, V15] V13 V14 V15 V12 Physical Frames ? P3 P4 P5 P6 P2 P8 P9 P7 P10 P11 P12 P13 P14 P3 P4 P5 P6 P2 P8 P9 P7 P10 P11 P12 P13 P14 ? (a) Avoiding interference Physical Frames VMA1 [V0, V3] V1 V2 V3 V0 VMA1 [V0, V3] V1 V2 V3 V0 VMA2 [V12, V15] V13 V14 V15 V12 VMA2 [V12, V15] V13 V14 V15 V12 ? P3 P4 P5 P6 P2 P8 P9 P7 P10 P11 P12 P13 P14 P3 P4 P5 P6 P2 P8 P9 P7 P10 P11 P12 P13 P14 ? (b) Interference during coalescing</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Intra-VMA Page Coalescing</head><p>After the anchor point of a VMA is selected based on the steps described above, Translation Ranger coalesces pages within the VMA to create contiguity. Figure <ref type="figure" target="#fig_5">6</ref> shows how coalescing proceeds after an anchor point selection within each VMA. When performing coalescing within a VMA, Translation Ranger has to handle several practical issues:</p><p>In-Use Page Frames. Target physical frames may be in one of two states: free or in-use. If a target page frame P n is free, Translation Ranger use Linux's page migration mechanism to move the source frame's data to the target frame. If a target page frame P n is in use, we cannot simply clobber it. One solution would be to move the contents of the in-use frame to an intermediate physical frame before migrating the source frame to the target frame, but this suffers from extra storage and copy time overhead. In addition, under memory pressure, allocating a new intermediate physical frame can trigger the page reclamation process, leading to significant performance degradation. Therefore, we directly exchange two pages by unmapping the two pages at the same time, exchanging the content of the two pages, and finally remapping these two pages <ref type="bibr" target="#b54">[54]</ref>. Instead of copying data into new pages, this patch transfers data between source and target pages using CPU registers as the temporary storage for in-flight iterative data exchange operations. This approach requires no extra storage or page allocation and even supports the exchange of THPs. It supports exchanging between two anonymous pages as well as between one anonymous page and one file-backed page, but it does not support exchange of two file-backed pages (due to complicated file system locking and their rare occurrence).</p><p>Non-Movable Pages. Another issue that Translation Ranger must handle is the presence of non-movable pages. Some examples of non-movable pages are those in use by the kernel (e.g., for</p><formula xml:id="formula_5">VMA1 [V0, V6] V1 V2 V3 V0 VMA1 [V0, V6] V1 V2 V3 V0 VMA2 [V12, V15] V13 V14 V15 V12 VMA2 [V12, V15] V13 V14 V15 V12 Physical Frames ? P3 P4 P5 P6 P2 P8 P9 P7 P10 P11 P12 P13 P14 P3 P4 P5 P6 P2 P8 P9 P7 P10 P11 P12 P13 P14 ? V4 V5 V6</formula><p>(a) VMA1 grows its size at the end. Its new virtual pages V4, V5, and V6 overlap with VMA2 during memory coalescing.</p><formula xml:id="formula_6">VMA1 [V0, V1] V1 V2 V3 V0 VMA1 [V0, V1] V1 V2 V3 V0 VMA2 [V12, V15] V13 V14 V15 V12 VMA2 [V12, V15] V13 V14 V15 V12 Physical Frames ? P3 P4 P5 P6 P2 P8 P9 P7 P10 P11 P12 P13 P14 P3 P4 P5 P6 P2 P8 P9 P7 P10 P11 P12 P13 P14 ? (b) VMA1</formula><p>shrinks its size at the end. The freed virtual pages V2 and V3 leave a physical page gap and fragment memory. slab allocations, page tables, or other kernel data structures). Alternately, a page may be marked as busy because it is involved in I/O operations, migration, dirty page writeback, or DMA. We handle each of these non-movable page types in different ways. Since kernel pages usually have a long lifetime and limited OS support for page migration, we simply skip them during the coalescing process. However, busy pages are usually ephemeral; therefore, even if these pages cannot be moved currently, Translation Ranger will try to coalesce them again in the future using iterative coalescing. If Translation Ranger must skip a non-movable frame, it will continue coalescing the remaining frames using the originally selected anchor point. We investigated the value of creating new anchor points following non-movable pages, but we found that doing so yields little benefit. Generating additional anchor points needlessly splits a large contiguous region into two smaller contiguous regions should a busy page (the cause of a new anchor) become non-busy in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Iterative Page Frame Coalescing</head><p>Applications can grow and shrink their VMAs over time. This is problematic if a VMA grows so that its physically contiguous region overlaps and interferes with another VMA, or if a VMA shrinks enough that the freed physical frames become a free memory fragment. We show both cases in Figure <ref type="figure" target="#fig_7">8</ref>. To avoid this problem, Translation Ranger tracks per-VMA size along with each VMA's coalesced region size during each coalescing iteration. If, on future iterations, Translation Ranger discovers a VMA that has now grown and overlaps with another VMA (by examining coalesced region information), Translation Ranger relocates the coalesced region of one of the two VMAs by assigning a new anchor point.</p><p>To minimize page frame relocation overheads, the smaller of the two is moved. On the other hand, if no overlapping occurs, the new addition to the VMA will also be coalesced and the VMA's coalesced region size will be adjusted accordingly.</p><p>Translation Ranger is also designed to coalesce large important VMAs and ignore smaller shorter-lived VMAs (&lt; 2MB) such as those used to map data structures like thread stacks. The rationale is that these VMAs tend to be sufficiently small such that that coalescing their page frames them yields little additional contiguity relative to the overhead of the necessary page migrations. We believe more sophisticated strategies, e.g., consolidating multiple thread stacks, or lazy VMA deallocation to lengthen VMA lifetimes, could further decrease coalescing overhead and generate even larger contiguous regions, but we leave these for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Additional Implementation Challenges</head><p>Synonyms and Copy-on-Write. To handle synonyms, Translation Ranger coalesces a physical range based on the anchor point from the first created VMA, and ignores the anchor points of the synonym VMAs. This can be done efficiently in Linux by checking anon_vma for anonymous VMAs and address_space for file-backed VMAs. Care needs to be taken when generating contiguity on pages created by copy-on-write (COW) to avoid unnecessary coalescing work. Translation Ranger skips forked VMAs that share the same physical pages, the same way it does for skipping synonym VMAs. After COW, Translation Ranger creates a new anchor point with the COW physical page for that VMA.</p><p>Reducing Runtime Overheads. Excessive page migration during coalescing can incur high runtime overheads. To avoid this, Translation Ranger selects anchor points to avoid inter-VMA interference (see Figure <ref type="figure" target="#fig_6">7</ref>). Furthermore, it focuses on large and long-lived VMAs to avoid wasting coalescing effort (see Section 3.4). Moreover, we enable users or system administrators to control Translation Ranger's runtime impact through a tunable parameter. We envision that this tunable will be used the same way as the vast majority of other existing operating system services like khugepaged. That is, if there is hardware to take advantage of contiguity (via TLB optimizations like range TLBs), we can expect administrators to run Translation Ranger more frequently to aggressively generate contiguity, though we will reflect on a sane default in Section 5.3. Finally, we design Translation Ranger as a background daemon that is not on the critical path of application execution and can "steal" idle CPU cycles. This is similar in spirit to the design of already-existing daemons like khugepaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL METHODOLOGY</head><p>Translation Ranger is widely deployable on systems with and without fragmentation and can leverage any previously-proposed TLB hardware that supports translation contiguity <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39]</ref>. Naturally, Translation Ranger's performance benefits will vary depending on the target system's fragmentation levels and the contiguity-aware TLBs that leverage it. To achieve good performance, Translation Ranger must generate enough translation contiguity for contiguity-aware TLBs to offset any runtime overheads from Translation Ranger's page movement operations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Platform</head><p>We implement Translation Ranger in Linux kernel v4. <ref type="bibr" target="#b15">16</ref> and evaluate it on a two-socket Intel server (see Table <ref type="table" target="#tab_2">2</ref>). We run a variety of benchmarks from SPEC ACCEL <ref type="bibr" target="#b21">[22]</ref>, the GAP benchmark suite<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b5">[6]</ref>, graph500 <ref type="bibr" target="#b31">[31]</ref>, and GUPS <ref type="bibr" target="#b43">[43]</ref> (see Table <ref type="table" target="#tab_4">3</ref>). Translation Ranger running as a service daemon is collocated with applications in the same memory node and tuned to periodically coalesce application memory; to produce contiguity statistics, application memory is scanned every 5 seconds to retrieve the virtual-to-physical mappings of each page belonging to the application. This statistics collection is engineered to have negligible impact on runtime and would not be present in production deployments. A central insight from our work is that system load and fragmentation levels is critical to the question of how much contiguity can be generated. The success of contiguity-aware TLBs rests on the OSes ability to generate contiguity robustly across a wide variety of scenarios. To stress-test whether Translation Ranger indeed generates contiguity in heavily fragmented environments, we go beyond prior work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref><ref type="bibr" target="#b40">[40]</ref> to use a methodology that preconditions memory before our evaluations.</p><p>We first use an existing methodology used by kernel developers to artificially fragment the free memory lists as if our system was long-running system with all its free lists randomized <ref type="bibr" target="#b52">[52]</ref>. We then further load the system by run a synthetic benchmark, memhog, an application that allocates memory throughout the physical address space, and has been used in prior studies to create fragmentation. Together, these steps ensure that memory is in a fragmented state similar to a realistic steady state shown in Figure <ref type="figure" target="#fig_2">3</ref>, which prevents applications obtaining unrealistic contiguity from sequential memory accesses. We also configure our benchmarks to use 95% of total free memory similar to many datacenter and HPC environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Configurations</head><p>To understand Translation Ranger's effectiveness on improving memory contiguity, we use Linux's default buddy allocator configuration (Linux Default) as our baseline. This baseline is what has been used by prior contiguity-aware TLB designs that  rely on serendipitously generated contiguity <ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref>. We also compare against an enhanced buddy allocator with its max order increased to 20, permitting 2GB contiguous region allocations. This Large Max Order is representative of approaches from prior work like redundant memory mappings and devirtualizing memory because it relies on generating contiguity at allocation time and hence serves as a valuable point of comparison to our approach <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>. Furthermore, we compare Translation Ranger to Enhanced khugepaged, which is another technique Linux uses to generate contiguity by collapsing scattered 4KB pages into a new THP. To conservatively assess Translation Ranger's relative benefits, we tune khugepaged to scan the entire application footprint every 5s as opposed to its default of defragmenting only 16MB of memory every 60s. Finally, when profiling Translation Ranger, we quantify the results when coalescing every 5 seconds, and every 50 seconds to showcase the relationship between runtime overheads and Translation Ranger's ability to generate contiguity. For all five configurations, THPs are enabled by default and the buddy allocator uses Linux's default max order 11 to allow a maximum 4MB contiguous page allocation, except for Large Max Order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Contiguity Metrics</head><p>We focus on two metrics to evaluate the effectiveness of Translation Ranger on real systems. First, we count the total number of contiguous regions needed to cover the entire application memory footprint (TotalNum ContigRegions ). Our goal is to reduce the total number these regions such that their total amount is comparable to the most aggressive eager paging and identity mapping techniques from prior work (e.g., direct segments, range TLBs, devirtualizing memory <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>).</p><p>We calculate the percentage of total application footprint covered by the largest 32 contiguous regions (MemCoverage 32Regions ) and the percentage of total application footprint covered by the largest 128 contiguous regions (MemCoverage 128Regions ). Our goal is to show that even small 32-128 entry contiguity-aware TLBs can capture the majority of the application footprint. This metric has been previously used by prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> to understand contiguity improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">System Overheads</head><p>Translation Ranger can add overhead to systems because pages undergoing migration are not accessible to applications and require TLB invalidations and shootdowns. Naturally, these overheads will be offset by improved TLB hit rates. Nevertheless, to mitigate even the cost of page migration, we present all benchmark runtimes for the five measured configurations in the conservative scenarios where contiguity-aware TLBs are absent. We normalize these runtimes to our baseline, Linux Default. Our platform supports discrete page sizes (4KB and 2MB) but does not take advantage of other kinds of contiguity, so excess application runtime due to coalescing can be viewed as the software tax of our system. Ultimately, we will show that Translation Ranger generates contiguity comparable to the most aggressive prior proposals, while simultaneously incurring such low overheads that the benefits will come virtually "for free" on most systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>We begin by showing translation contiguity results for all benchmarks, followed by highlighting two interesting cases to provide more detail about how applications behave over time. Finally, we show system runtime overheads and discuss Translation Ranger's applicability to other OSes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Translation Contiguity Results</head><p>To concisely show individual results, we show several metrics, including TotalNum ContigRegions , MemCoverage 32Regions , and MemCoverage 128Regions in Figure <ref type="figure" target="#fig_8">9</ref> and Figure <ref type="figure" target="#fig_9">10</ref> using violin plots <ref type="bibr" target="#b51">[51]</ref>.</p><p>Violin plots aggregate all numbers over application runtime into a distribution represented by a"violin plot" parallel to the y-axis. The thickness of a violin plot indicates how often the values from y-axis occur. The arithmetic mean of all values is shown as a diamond symbol within each plot.</p><p>Figure <ref type="figure" target="#fig_8">9</ref> shows the TotalNum ContigRegions distributions for all benchmarks. We observe that most benchmarks show TotalNum ContigRegions aggregating in one primary area, which reflects their bulk memory allocation behavior; the number of regions does not change much over time. On the other hand, 556.psp and 570.pbt, which allocate memory recurrently, have their TotalNum ContigRegions spread across a range of values along the yaxis due to frequent small allocations.</p><p>Among the five configurations, Linux Default and Enhanced khugepaged require many more contiguous regions (violin plots are thick at the top of each plot) to cover each application's memory footprint. This is primarily because they are both limited by the buddy allocator, which yields contiguity regions up to 4MB. The Large Max Order configuration can generate much larger, and thus fewer contiguous regions, since it modifies the buddy allocator to provide up to 2GB contiguous regions. However, Translation Ranger (at either frequency) is able to coalesce memory more effectively and needs far fewer contiguous regions to cover each application footprint; i.e., it is the most successful technique for generating contiguity.</p><p>Figure <ref type="figure" target="#fig_9">10</ref> shows the coverage of each technique when using the largest 32 or 128 contiguous regions. The plots tend to cluster together. This is because TLB coverage is influenced by the size of the largest contiguous regions present in the system and not just the total number of regions. Among all five OS configurations, Linux Default and Enhanced khugepaged can typically only cover &lt; 1% of each application footprint (with either 32 or 128 regions), since they can achieve at most 4MB contiguous regions. The Large Max Order configuration can typically cover up to 40% of the application footprint, but is also limited by 2GB contiguous regions from its buddy allocator modifications. Theoretically, the Large Max Order configuration should be able to cover all application footprints with 128 contiguous regions, if each region is at least 1GB. However, due to memory fragmentation, not all contiguous regions obtained from the buddy allocator are maximally-sized.</p><p>Translation Ranger (at all the frequencies we studied; we show 5s and 50s frequencies in our graphs) consistently creates much larger contiguous regions that can cover the majority of each application's footprint. Utilizing 128 contiguous regions, Translation Ranger can typically cover &gt; 90% of a 120GB application footprint. In comparison, the last level TLB of a CPU today typically contains 1536 entries and hence even when they exclusively use THPs (2MB) they can only cover 2GB of footprint. Thus Translation Ranger combined with existing coalescing TLB proposals can typically improve TLB coverage by over 30? as shown in Table <ref type="table">4</ref>. Meanwhile, TLB storage could be reduced by 85% from a 1536-entry traditional TLB to a 128-entry range cache, where the former uses about 13.5KB<ref type="foot" target="#foot_2">3</ref> and the latter uses 2KB <ref type="bibr" target="#b48">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Notable Individual Benchmarks</head><p>In order to provide more insight into how Translation Ranger achieves contiguity, we highlight two workloads: 503.postencil, which allocates memory in bulk and 556.psp, which allocates and frees memory frequently.</p><p>503.postencil Figure <ref type="figure" target="#fig_11">11a</ref> shows the contiguity results over the runtime of 503.postencil, which first creates a huge address region, fills it with physical frames, then processes all data in memory. The left most plot in Figure <ref type="figure" target="#fig_11">11a</ref> shows the TotalNum ContigRegions over application runtime for the 5 contiguity producing approaches. This plot shows the same data as the left most plot of Figure <ref type="figure" target="#fig_8">9</ref>, but spreads time out over the x-axis, thereby showing how the contiguity changes over time. To translate between them, first consider the left most plot of Figure <ref type="figure" target="#fig_11">11a</ref>. We can see TotalNum ContigRegions for Linux Default increases from 0 to about 32,000 during the first 40% of application runtime and becomes stable for the remaining 60% runtime. In the left most plot of Figure <ref type="figure" target="#fig_8">9</ref>, the Linux Default violin plot is thickest around the value of 32,000, then is fairly uniformly distributed between 0 and 32,000.</p><p>From the leftmost plot in Figure <ref type="figure" target="#fig_11">11a</ref>, we see several interesting trends. For example, with Translation Ranger more frequent invocation (every 5 seconds) is able to generate large contiguous regions more quickly than the less frequent invocation (every 50 seconds). However, after application memory allocations become stable (at approximately 40% of the application runtime), the less frequent invocation eventually results in a similar number of contiguous regions being produced for the remainder of the execution.</p><p>In the middle and right plots plots of Figure <ref type="figure" target="#fig_11">11a</ref> we observe that the Linux Default and Enhanced khugepaged configurations can cover very little (&lt; 1%) of the application footprint throughout the application lifetime. The Large Max Order configuration is a substantial improvement with 12.5% of the application footprint being covered with the largest 32 contiguous regions and 32.3% with the largest 128 contiguous regions respectively. However Translation Ranger can cover at least 80% of application footprint with just 32 contiguous regions, and over 95% of the 121GB footprint using 128 contiguous regions after just several iterations of coalescing. To summarize, contiguity-aware TLB designs will attain about 3? TLB coverage (or could reduce their hardware resources proportionally), if they simply use Translation Ranger instead of their own software enhancements.</p><p>556.psp Figure <ref type="figure" target="#fig_11">11b</ref> shows the contiguity results for 556.psp, which frequently allocates and deallocates memory. The left most plot of Figure <ref type="figure" target="#fig_11">11b</ref> shows high volatility across all configurations for TotalNum ContigRegions because frequent memory allocations add many fragmented small pages and deallocations remove existing contiguous regions. All five experimental configurations suffer from this type of application memory allocation pattern.</p><p>Because Translation Ranger iteratively coalesces memory to generate contiguous regions, it requires 40% fewer contiguous regions to cover the entire application footprint than Linux Default, Large Max Order, or Enhanced khugepaged. The middle and right plots in Figure <ref type="figure" target="#fig_11">11b</ref> show that using the largest 32 or 128 contiguous regions, Linux Default and Enhanced khugepaged can cover &lt; 1% of the application footprint. Large Max Order does a little better, covering about 8% initially, but decreases to &lt; 2% because of the frequent memory allocations and deallocations.</p><p>In contrast, Translation Ranger (with frequency set to every 50 seconds) can cover more than 64% of the application footprint with the largest 32 contiguous regions and the coverage is over 72% during the bulk of application runtime; increasing the number of regions to 128 further improves the coverage of the application's footprint to 75%. Additional improvements can come from running Translation Ranger more frequently but do not seem necessary in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Translation Ranger Overheads</head><p>So far we have shown that Translation Ranger creates systematically larger and dramatically fewer contiguous regions compared to other approaches, but we must also consider its "software tax". Figure <ref type="figure" target="#fig_1">12</ref> shows the application execution time across all five configurations normalized to our baseline, Linux Default, averaged across 5 runs to account for variation. Large Max Order, which on average adds 1.1% runtime overhead, incurs very minor slowdowns due to zeroing every free large page at allocation time. The overhead of Enhanced khugepaged is negligible; although it runs very frequently (every 5 seconds), the majority of each workload's memory is already THPs, leaving few base pages for it to convert to THPs. This also explains its small improvements in contiguity for most workloads. Finally, we show that Translation Ranger adds, on average, 1.7% overhead when run every 50 seconds.</p><p>bc-kron provides an interesting study, because it actually runs faster than expected with Enhanced khugepaged and Translation Ranger. bc-kron cannot allocate as many THPs as possible at page allocation time, which causes Linux Default and Large Max Order suffer, whereas both Enhanced khugepaged and Translation Ranger are able to generate more THPs out of fragmented in-use pages after page allocation time. This improves the application performance by reducing TLB misses and page table walk overheads thanks to TLB support for huge pages on our test system.</p><p>When Translation Ranger is tuned to run more aggressively, every 5 seconds, it also produces more contiguity. We measure an average runtime overhead of of 2.3%, which is 0.6% more than running Translation Ranger every 50 seconds. This small overhead buys more pronounced contiguity with over 90% of application footprint consistently covered with just 32 contiguous chunks through application runtime. We conclude that Translation Ranger has minimally invasive software overheads comparable to prior solutions that are so small that it will will be more than offset by the performance improvements achieved via TLB efficiency, reported at 20-30% in prior proposals <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">39]</ref>. Because Translation Ranger generates substantially more contiguity than those studies, we also expect to see more performance, but an exhaustive study of numerous contiguity-aware TLB designs is beyond the scope of this comprehensive OS implementation of software support for contiguity aware TLBs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>Applicability to other OSes. The Translation Ranger concept is not Linux specific and is compatible with other OSes like Windows and FreeBSD, which maintain VMA-like data structures per process. For example, FreeBSD uses vm_map_entry to identify a contiguous virtual address range instead of a VMA, vm_object to represent a group of physical frames from one memory object instead of anon_vma for an anonymous memory object and address_mapping for a file in Linux. To port Translation Ranger to FreeBSD, we can generate contiguity on each vm_map_entry and assign one anchor point to it instead of each VMA.</p><p>Permission Checks. All recent work on contiguous regions, including ours, assumes that contiguous regions belong to single VMAs with uniform permissions for all their virtual pages. If the permission of a virtual address range in this VMA is changed, the VMA will be broken into two or more new VMAs with corresponding updated permissions and the page table entries will be updated respectively. This also breaks one contiguous region into multiple regions, and contiguity-aware TLBs will need additional entries to address the original memory address range. To mitigate this problem, new address translation designs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> that can decouple permission checks from virtual-to-physical address translation, could be helpful, as they allow the TLB to maintain the original contiguous region entry but with additional permission subsections.</p><p>NUMA Effects. This initial study focuses on only one memory node in our system as a tractable configuration to thoroughly understand Translation Ranger. Translation Ranger can easily be extended to multi-node NUMA systems, though Translation Ranger's cross-socket traffic overheads will have to be integrated with previously-proposed NUMA paging policies, like autoNUMA, Carrefour-LP, and Ingens <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26]</ref> to minimize overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Prior work on memory allocation has studied ways to increase contiguity and reduce memory fragmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">33]</ref> and is discussed in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Memory Allocation. Internal and external memory fragmentations are two major problems for memory allocations. To mitigate internal memory fragmentation, the SLAB allocator and others (e.g., jemalloc and tcmalloc) pack small memory objects with the same size together in one or more pages to avoid wasting space <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>. For external memory fragmentation, OSes use buddy allocators to achieve fast memory allocation and restrict external fragmentation <ref type="bibr" target="#b24">[25]</ref>. Linux developers separate kernel and user memory allocations to further reduce external fragmentation <ref type="bibr" target="#b16">[17]</ref>. Additionally, peripheral devices often require access to physically contiguous memory. Linux accommodates these devices with drivers that use boot-time allocation and reserve contiguous memory before others can request memory via Contiguous Memory Allocators (CMAs) <ref type="bibr" target="#b10">[11]</ref>. CMAs use memory compaction to migrate fragmented pages and offer large contiguous physical memory for devices use, especially for DMA data transfer <ref type="bibr" target="#b33">[33]</ref>. These approaches try to preserve contiguity for future use, but cannot prevent large free memory blocks from being broken into small ones when memory requests are small in sizes.</p><p>Huge pages. Significant prior work has gone into improving huge pages. For example, a huge pages have one access bit, causing memory access imbalance problems in NUMA systems <ref type="bibr" target="#b14">[15]</ref>. Several proposals try to manage huge page wisely by restricting huge page creation and splitting huge pages when they cause utilization imbalance issues <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref>. These utilization issues could also happen on contiguous regions created by Translation Ranger, thus integrating these policies with Translation Ranger could further improve application performance in NUMA systems.</p><p>In heterogeneous memory systems, classifying hot and cold pages to determine how to move them between fast/slow memories is important. Previous work by Thermostat analyzes huge page utilization by sampling sub-pages in each huge page and migrates these cold sub-pages to slow memory to make efficient use of fast memory <ref type="bibr" target="#b1">[2]</ref>. Thermostat could provide useful utilization information to Translation Ranger to assess page hotness in identifying which VMAs are particularly worth coalescing.</p><p>The high cost of huge page allocation has been a problem for Linux and can lead to application performance degradation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">29]</ref>. Recent work on Linux prevents free page fragmentation and eliminates most huge page allocation costs by aggregating kernel page allocations <ref type="bibr" target="#b34">[34]</ref>. With the help of this work, Translation Ranger could also improve in-use page fragmentation and free page fragmentation, generating even larger contiguous regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>Translation Ranger is an effective low-overhead technique for coalescing scattered physical frames and generating translation contiguity. The enormous contiguous regions created by Translation Ranger can be used by emerging contiguity-aware TLBs to minimize address translation overhead for all computation units in heterogeneous systems. Accelerators in particular will benefit, as they often have limited hardware resources for address translation. Translation Ranger can scale easily with increasing memory sizes regardless of the limitations imposed by modern memory allocators. With less than 2% runtime overhead, Translation Ranger generates contiguous regions covering more than 90% of 120GB application footprints with at most 128 regions, which can be fully cached by contiguity-aware TLBs to minimize address translation overhead. To address ever-increasing memory sizes, contiguityaware TLBs provide promising hardware support and Translation Ranger is the software cornerstone needed to enable these designs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hypothetical system comprised of CPUs, GPUs, and other accelerators utilizing a single shared virtual and physical address space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A contiguity-aware TLB (left) uses two entries to cache four translations each, while a traditional TLB (right) requires eight entries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: There is plenty of contiguity available at boot time, but memory becomes fragmented soon thereafter. a single entry to cache what would otherwise take 512 or 262,144 entries..Although huge pages can be effective, they only offer discrete chunks of contiguity. As memory capacities continue to grow, translation contiguity amounts in excess of 1GB or sizes between the discrete page sizes of 2MB and 1GB will be useful. For this reason, recent work has considered translation contiguity approaches complementary to traditional huge pages. All of these techniques require hardware support from the TLB, as outlined in Table1. For example, direct segments<ref type="bibr" target="#b3">[4]</ref>, uses programmer-OS coordination to mark gigabyte-to terabyte-sized primary segments of memory that are guaranteed to be mapped using contiguous translations. While this approach can substantially reduce TLB misses, direct segments can be challenging to use for real-world workloads which need more than one primary direct segment (to allocate contiguous segments in different parts of their address space) and because of the need for explicit programmer intervention. Redundant memory mappings<ref type="bibr" target="#b22">[23]</ref> support arbitrary translation ranges in TLBs, but require invasive OS changes to produce these contiguous translation ranges. Devirtualizing memory<ref type="bibr" target="#b17">[18]</ref> extends the concepts of direct segments for area-constrained accelerators, but works under the optimistic assumption that OSes can always offer large contiguous memory regions for devirtualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Some possible types of fragmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Defragmentation via memory compaction (e.g., in Linux) might destroy in-use contiguity as an unintended side effect of creating more free memory contiguity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Coalescing the pages in a Virtual Memory Area (VMA): after coalescing page frames, virtual pages V0-V3 map to contiguous physical frames P4-P7. Filled page frame boxes denotes those mapped by V0-V3, marked boxes denotes the frames mapped by other VMAs, and blank boxes denotes free frames. The VMA's Anchor Point is (V0, P4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Anchor points (shown with red arrows) must be chosen carefully to prevent inter-VMA interference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: VMA size changes: VMA growth and shrinkage. They cause inter-VMA interference and memory fragmentation, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Total number of contiguous regions covering entire application memory (TotalNum ContigRegions ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The percentage of total application footprint covered by the largest 32 contiguous regions (MemCoverage 32Regions ) is shown in the top and percentage of total application footprint covered by the largest 128 contiguous regions ( MemCoverage 128Regions ) is shown in the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Contiguity results over time for 556.psp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Total number of contiguous regions covering entire application memory (TotalNum ContigRegions ) is shown in the left most plot; percentage of total application footprint covered by the largest 32 contiguous regions (MemCoverage 32Regions ) is shown in the middle plot; percentage of total application footprint covered by the largest 128 contiguous regions ( MemCoverage 128Regions ) is shown in the right most plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Techniques used or proposed by industrial or academic research groups for high performance address translation.</figDesc><table><row><cell>For example, the x86-64 architecture supports 2MB and</cell></row><row><cell>1GB huge pages if the OS can allocate 512 or 262,144 contiguous 4KB</cell></row><row><cell>virtual pages and physical frames aligned to 2MB or 1GB address</cell></row><row><cell>boundaries, respectively. This permits x86-64 compliant TLBs to use</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>System configurations and per-core TLB hierarchy.</figDesc><table><row><cell></cell><cell>Experimental Environment</cell></row><row><cell>Processors</cell><cell>2-socket Intel E5-2650v4 (Broadwell), 24 cores/socket, 2 threads/core, 2.2 GHz</cell></row><row><cell></cell><cell>4KB pages: 64-entry, 4-way set assoc.</cell></row><row><cell>L1 DTLB</cell><cell>2MB pages: 32-entry, 4-way set assoc.</cell></row><row><cell></cell><cell>1GB pages: 4-entry, 4-way set assoc.</cell></row><row><cell>L1 ITLB</cell><cell>4KB pages: 128-entry, 4-way set assoc. 2MB pages: 8-entry, fully assoc.</cell></row><row><cell>L2 TLB</cell><cell>4KB&amp;2MB pages: 1536-entry, 6-way set assoc. 1GB pages: 16-entry, 4-way set assoc.</cell></row><row><cell>Memory</cell><cell>128GB DDR4 (per socket)</cell></row><row><cell>OS</cell><cell>Debian Buster -Linux v4.16.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Benchmark descriptions and memory footprints.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Figure 12: Benchmark runtime for all five configurations: Linux Default, Large Max Order, Enhanced khugepaged, and Translation Ranger with two running frequency. All runtime is normalized to Linux Default.</figDesc><table><row><cell>1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Normalized runtime</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>503.postencil</cell><cell>551.ppalm</cell><cell>553.pclvrleaf</cell><cell>555.pseismic</cell><cell>556.psp</cell><cell>559.pmniGhost</cell><cell>560.pilbdc</cell><cell>563.pswim</cell><cell>570.pbt</cell><cell>graph500-omp</cell><cell>gups</cell><cell>bc-kron</cell><cell>bc-urand</cell><cell>cc-kron</cell><cell>cc-urand</cell><cell>pr-kron</cell><cell>pr-urand</cell><cell>geomean</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Benchmarks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Linux Default</cell><cell cols="4">Large Max Order</cell><cell cols="4">Enhanced khugepaged</cell><cell></cell><cell cols="3">Translation Ranger (every 5s)</cell><cell></cell><cell cols="3">Translation Ranger (every 50s)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/ysarch-lab/translation_ranger_isca_2019</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We scaled up two synthetic input graphs, Kron and Urand, and run three kernels, Between Centrality (bc), Connected Components (cc), and PageRank (pr) with these two input graphs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We assume the TLB use 36bit (or</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>4.5B) tag for VPN and 36bit (or 4.5B) data for PFN and permission bits, so 4.5B ? 2 ? 1536/1024 = 13.5KB.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors thank <rs type="person">J?n Vesel?</rs> and <rs type="person">Guilherme Cox</rs> for their input on aspects of this work. This work was partially funded by <rs type="funder">NSF</rs> awards <rs type="grantNumber">1319755</rs> and <rs type="grantNumber">1916817</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nncBU9x">
					<idno type="grant-number">1319755</idno>
				</org>
				<org type="funding" xml:id="_8FrYzTr">
					<idno type="grant-number">1916817</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Separating Translation from Protection in Address Spaces with Dynamic Remapping</title>
		<author>
			<persName><forename type="first">Reto</forename><surname>Achermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Faraboschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejan</forename><surname>Milojicic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Ndu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">L</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">N M</forename><surname>Watson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3102980.3103000</idno>
		<ptr target="https://doi.org/10.1145/3102980.3103000" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Workshop on Hot Topics in Operating Systems (HotOS &apos;17)</title>
		<meeting>the 16th Workshop on Hot Topics in Operating Systems (HotOS &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="118" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thermostat: Applicationtransparent Page Management for Two-tiered Main Memory</title>
		<author>
			<persName><forename type="first">Neha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<idno type="DOI">10.1145/3037697.3037706</idno>
		<ptr target="https://doi.org/10.1145/3037697.3037706" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;17)</title>
		<meeting>the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="631" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://www.amd.com/Documents/Compute_Cores_Whitepaper.pdf" />
		<title level="m">Compute Cores</title>
		<imprint>
			<date type="published" when="2014-08">2014. Aug-2018</date>
			<biblScope unit="volume">04</biblScope>
		</imprint>
		<respStmt>
			<orgName>AMD Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient Virtual Memory for Big Memory Servers</title>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<idno type="DOI">10.1145/2485922.2485943</idno>
		<ptr target="https://doi.org/10.1145/2485922.2485943" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA &apos;13)</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture (ISCA &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reducing Memory Reference Energy with Opportunistic Virtual Caching</title>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2337159.2337194" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual International Symposium on Computer Architecture (ISCA &apos;12)</title>
		<meeting>the 39th Annual International Symposium on Computer Architecture (ISCA &apos;12)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="297" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The GAP Benchmark Suite</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03619</idno>
		<ptr target="http://arxiv.org/abs/1508.03619" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Preserving Virtual Memory by Mitigating the Address Translation Wall</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2017.3711640</idno>
		<ptr target="https://doi.org/10.1109/MM.2017.3711640" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="6" to="10" />
			<date type="published" when="2017-09">2017. September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Slab Allocator: An Object-caching Kernel Memory Allocator</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bonwick</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1267257.1267263" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Summer 1994 Technical Conference on USENIX Summer 1994 Technical Conference</title>
		<meeting>the USENIX Summer 1994 Technical Conference on USENIX Summer 1994 Technical Conference<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USTC&apos;94). USENIX Association</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new x86 core architecture for the next generation of computing</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.1109/HOTCHIPS.2016.7936224</idno>
		<ptr target="https://doi.org/10.1109/HOTCHIPS.2016.7936224" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Hot Chips 28 Symposium (HCS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">AutoNUMA: the other approach to NUMA scheduling</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="http://lwn.net/Articles/488709/" />
		<imprint>
			<date type="published" when="2012-08">2012. Aug-2018</date>
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Corbet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rubini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Kroah-Hartman</surname></persName>
		</author>
		<title level="m">Linux Device Drivers</title>
		<imprint>
			<publisher>O&apos;Reilly Media, Inc</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient Address Translation for Architectures with Multiple Page Sizes</title>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://www.facebook.com/notes/facebook-engineering/scalable-memory-allocation-using-jemalloc/480222803919" />
		<title level="m">Scalable memory allocation using jemalloc</title>
		<imprint>
			<publisher>Jason Evans</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Online; accessed 04-Aug-2018</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large Pages May Be Harmful on NUMA Systems</title>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Decouchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Funston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Qu?ma</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2643634.2643659" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC&apos;14). USENIX Association</title>
		<meeting>the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC&apos;14). USENIX Association<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Menage</surname></persName>
		</author>
		<ptr target="http://goog-perftools.sourceforge.net/doc/tcmalloc.html" />
		<title level="m">Tcmalloc: Thread-caching malloc</title>
		<imprint>
			<date type="published" when="2009-08">2009. Aug-2018</date>
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The what, the why and the where to of anti-fragmentation</title>
		<author>
			<persName><forename type="first">Mel</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Whitcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linux Symposium</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Devirtualizing Memory in Heterogeneous Systems</title>
		<author>
			<persName><forename type="first">Swapnil</forename><surname>Haria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173162.3173194</idno>
		<ptr target="https://doi.org/10.1145/3173162.3173194" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;18)</title>
		<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="637" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">HSA Platform System Architecture Specification -Provisional 1</title>
		<author>
			<orgName type="collaboration">HSA Foundation</orgName>
		</author>
		<ptr target="http://www.slideshare.net/hsafoundation/hsa-platform-system-architecture-specification-provisional-verl-10-ratifed" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Online; accessed 04-Aug-2018</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 Architectures Optimization Reference Manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Memory Fragmentation Problem: Solved?</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1145/286860.286864</idno>
		<ptr target="https://doi.org/10.1145/286860.286864" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Symposium on Memory Management (ISMM &apos;98)</title>
		<meeting>the 1st International Symposium on Memory Management (ISMM &apos;98)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SPEC ACCEL: A Standard Application Suite for Measuring Hardware Accelerator Performance</title>
		<author>
			<persName><forename type="first">Guido</forename><surname>Juckeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Colgrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Grund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wen-Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huian</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><forename type="middle">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Perminov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Shelepugin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Stratton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Van Waveren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rengan</forename><surname>Wienke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Kumaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing Systems. Performance Modeling, Benchmarking, and Simulation</title>
		<editor>
			<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Jarvis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Wright</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simon</forename><forename type="middle">D</forename><surname>Hammond</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="46" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Redundant Memory Mappings for Fast Access to Large Memories</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furkan</forename><surname>Ayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri?n</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>?nsal</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2749471</idno>
		<ptr target="https://doi.org/10.1145/2749469.2749471" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual International Symposium on Computer Architecture (ISCA &apos;15)</title>
		<meeting>the 42Nd Annual International Symposium on Computer Architecture (ISCA &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="66" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Energy-efficient address translation</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>Unsal</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2016.7446100</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2016.7446100" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="631" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Fast Storage Allocator</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><surname>Knowlton</surname></persName>
		</author>
		<idno type="DOI">10.1145/365628.365655</idno>
		<ptr target="https://doi.org/10.1145/365628.365655" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="623" to="624" />
			<date type="published" when="1965-10">1965. Oct. 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coordinated and Efficient Huge Page Management with Ingens</title>
		<author>
			<persName><forename type="first">Youngjin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangchen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmett</forename><surname>Witchel</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi16/technical-sessions/presentation/kwon" />
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Savannah, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="705" to="721" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Adam</forename><surname>Litke</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/171451/" />
		<imprint>
			<date type="published" when="2018-08">Aug-2018</date>
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lowe-Power</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Inferring Kaveri&apos;s Shared Virtual Memory Implementation</title>
		<ptr target="http://www.lowepower.com/jason/inferring-kaveris-shared-virtual-memory-implementation.html" />
		<imprint>
			<date type="published" when="2018-08">Aug-2018</date>
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mongodb</forename><surname>Manual</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The Design and Implementation of the FreeBSD Operating System</title>
		<author>
			<persName><forename type="first">Kirk</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">V</forename><surname>Mckusick</surname></persName>
		</author>
		<author>
			<persName><surname>Neville-Neil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Introducing the Graph 500</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">B</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cray User&apos;s Group</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Practical, Transparent Operating System Support for Superpages</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitararn</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Cox</surname></persName>
		</author>
		<idno type="DOI">10.1145/844128.844138</idno>
		<ptr target="https://doi.org/10.1145/844128.844138" />
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="89" to="104" />
			<date type="published" when="2002-12">2002. Dec. 2002</date>
		</imprint>
	</monogr>
	<note>SI</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Michal</forename><surname>Nazarewicz</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/486301/" />
		<title level="m">A deep dive into CMA</title>
		<imprint>
			<date type="published" when="2018-07">Jul-2018</date>
			<biblScope unit="volume">08</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Making Huge Pages Actually Useful</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravinda</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173162.3173203</idno>
		<ptr target="https://doi.org/10.1145/3173162.3173203" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;18)</title>
		<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prediction-based superpage-friendly TLB designs</title>
		<author>
			<persName><forename type="first">Myrto</forename><surname>Papadopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2015.7056034</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2015.7056034" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="210" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SEESAW: Using Superpages to Improve VIPT Caches</title>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Parasar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2018.00026</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2018.00026" />
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hybrid TLB Coalescing: Improving TLB Translation Coverage Under Diverse Fragmented Memory Allocations</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taekyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungi</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><surname>Huh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079856.3080217</idno>
		<ptr target="https://doi.org/10.1145/3079856.3080217" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA &apos;17)</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="444" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Increasing TLB reach by exploiting clustering in page translations</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuko</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2014.6835964</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2014.6835964" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aamer Jaleel, and Abhishek Bhattacharjee</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viswanathan</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2012.32</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2012.32" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="258" to="269" />
		</imprint>
	</monogr>
	<note>-45)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large Pages and Lightweight Memory Management in Virtualized Environments: Can You Have it Both Ways</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>MICRO</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Architectural Support for Address Translation on GPUs: Designing Memory Management Units for CPU/GPUs with Unified Address Spaces</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Pichai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="743" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">E</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ionescu</surname></persName>
		</author>
		<title level="m">Windows Internals, Part 2: Covering Windows Server 2008 R2 and Windows 7 (Windows Internals)</title>
		<meeting><address><addrLine>Redmond, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Microsoft Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Performance evaluation and optimization of random memory access on multicores with high productivity</title>
		<author>
			<persName><forename type="first">V</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhatotia</surname></persName>
		</author>
		<idno type="DOI">10.1109/HIPC.2010.5713168</idno>
		<ptr target="https://doi.org/10.1109/HIPC.2010.5713168" />
	</analytic>
	<monogr>
		<title level="m">2010 International Conference on High Performance Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Concurrent support of multiple page sizes on a skewed associative TLB</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Seznec</surname></persName>
		</author>
		<idno type="DOI">10.1109/TC.2004.21</idno>
		<ptr target="https://doi.org/10.1109/TC.2004.21" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="924" to="927" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Toward Cache-Friendly Hardware Accelerators</title>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Yakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viji</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Brooks</surname></persName>
		</author>
		<ptr target="http://www.eecs.harvard.edu/~shao/papers/shao2015-scaw.pdf" />
	</analytic>
	<monogr>
		<title level="m">HPCA Sensors and Cloud Architectures Workshop (SCAW)</title>
		<imprint>
			<date type="published" when="2015-02-07">2015. 2015-02-07</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Surpassing the TLB Performance of Superpages with Less Operating System Support</title>
		<author>
			<persName><forename type="first">M</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASPLOS</title>
		<meeting>ASPLOS</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Tatkar</surname></persName>
		</author>
		<ptr target="https://community.oracle.com/docs/DOC-994842" />
		<title level="m">What Is the SPARC M7 Data Analytics Accelerator?</title>
		<imprint>
			<date type="published" when="2018-08">Aug-2018</date>
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Small Cache of Large Ranges: Hardware Methods for Efficiently Searching, Storing, and Updating Big Dataflow Tags</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Banit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashidhar</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Valamehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2008.4771782</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2008.4771782" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 41)</title>
		<meeting>the 41st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 41)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="94" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Observations and opportunities in architecting shared virtual memory for heterogeneous systems</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Oskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharjee</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2016.7482091</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2016.7482091" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="161" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Andy</forename><surname>Whitcroft</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/134804/" />
		<title level="m">sparsemem memory model</title>
		<imprint>
			<date type="published" when="2018-08">Aug-2018</date>
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Violin_plot" />
		<imprint>
			<date type="published" when="2018-08">Aug-2018</date>
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/767614/" />
		<title level="m">Randomize free memory</title>
		<imprint>
			<date type="published" when="2018-12">2018. Dec-2018</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Dynamic storage allocation: A survey and critical review</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Neely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Boles</surname></persName>
		</author>
		<editor>Memory Management, Henry G. Baler</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1" to="116" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Nimble Page Management for Tiered Memory Systems</title>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297858.3304024</idno>
		<ptr target="https://doi.org/10.1145/3297858.3304024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;19)</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
