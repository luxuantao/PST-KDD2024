<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-19">19 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Min</forename><surname>Sung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>POSTECH</roleName><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Neural Processing Research Center</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Neural Processing Research Center</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-19">19 Aug 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3383313.3412216</idno>
					<idno type="arXiv">arXiv:2008.08273v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sequential Recommendation</term>
					<term>Self-attention</term>
					<term>Temporal Embedding</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, self-attention based models have achieved state-of-the-art performance in sequential recommendation task. Following the custom from language processing, most of these models rely on a simple positional embedding to exploit the sequential nature of the user's history. However, there are some limitations regarding the current approaches. First, sequential recommendation is different from language processing in that timestamp information is available. Previous models have not made good use of it to extract additional contextual information. Second, using a simple embedding scheme can lead to information bottleneck since the same embedding has to represent all possible contextual biases. Third, since previous models use the same positional embedding in each attention head, they can wastefully learn overlapping patterns. To address these limitations, we propose MEANTIME (MixturE of AtteNTIon mechanisms with Multi-temporal Embeddings) which employs multiple types of temporal embeddings designed to capture various patterns from the user's behavior sequence, and an attention structure that fully leverages such diversity. Experiments on real-world data show that our proposed method outperforms current state-of-the-art sequential recommendation methods, and we provide an extensive ablation study to analyze how the model gains from the diverse positional information.</p><p>CCS Concepts: • Information systems → Retrieval models and ranking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Capturing users' preferences from their history is essential for making effective recommendations, because users' preferences and items' characteristics are both dynamic. Furthermore, users' preferences depend heavily on the context (e.g., one is likely to be interested in buying a keyboard after purchasing a desktop). Therefore, sequential recommendation aims to predict the next set of items that users are likely to prefer by exploiting their history.</p><p>Many algorithms have been proposed to better understand the sequential history of users <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b35">36]</ref>. Despite their excellent performance, most of them ignore the interactions' timestamp values. While recent works such as TiSASRec <ref type="bibr" target="#b13">[14]</ref> successfully incorporated time information, their usage of time was also limited to a single embedding scheme. Since the timestamp values are rife with information, it would be beneficial to explore many forms of temporal embeddings that can fully extract diverse patterns, and model architectures that can fully leverage such diversity.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.</p><p>In this paper we propose MEANTIME (MixturE of AtteNTIon mechanisms with Multi-temporal Embeddings) which introduces multiple temporal embeddings to better encode both absolute and relative positions of a user-item interaction in a sequence. Moreover, we employ multiple self-attention heads that handle each positional embedding separately. This is profitable because each head can play a role of expert that focuses on a particular pattern of user behaviors. Extensive experiments show that MEANTIME outperforms state-of-the-art algorithms on real-world datasets. Our contributions are as follows:</p><p>• We propose to diversify the embedding schemes that are used to encode the positions of user-item interactions in a sequence. This is done by applying multiple kernels to positions/values of timestamps in a sequence to create unique embedding matrices.</p><p>• We also propose a novel model architecture that operates multiple self-attention heads simultaneously, where each head specializes in extracting certain patterns from the users' behaviors by utilizing one of the positional embeddings we propose above.</p><p>• We conducted thorough experiments to show the effectiveness of our method on real-world datasets. We also provide a comprehensive ablation study that helps understand the effect of various components in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporal Recommendation</head><p>Since temporal information hold contextual information, they can be very crucial for recommendation performance. TimeSVD++ and BPTF <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref> adopted time factor into the matrix factorization method, and TimeSVD++ was one of the main contributions for the winning of Netflix Grand Prize <ref type="bibr" target="#b10">[11]</ref>. Time-LSTM <ref type="bibr" target="#b36">[37]</ref> equipped LSTMs with several forms of time gates to better model the time intervals in user's interaction sequence. Recently, CTA <ref type="bibr" target="#b24">[25]</ref> used multiple parametrized kernel functions on temporal information to calibrate the self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequential Recommendation</head><p>Sequential recommendation aims to suggest relevant items based on the user's sequential history. Markov-chain based methods <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> assume that users' behaviors are affected only by their last few behaviors. FPMC <ref type="bibr" target="#b17">[18]</ref> merges Markovchains with matrix factorization method for next-basket recommendation. Since the first suggestion by GRU4Rec <ref type="bibr" target="#b8">[9]</ref>, many RNN-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> brought the success of RNN into item sequence understanding. To overcome the strong order constraint of RNN models, CNN-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> were proposed. Some works adopted graph neural network (GNN) to understand user's session as a graph <ref type="bibr" target="#b25">[26]</ref>.</p><p>As for attention mechanisms, NARM <ref type="bibr" target="#b12">[13]</ref> and STAMP <ref type="bibr" target="#b14">[15]</ref> incorporated vanilla attention mechanism with RNN.</p><p>More recently, SASRec <ref type="bibr" target="#b9">[10]</ref> successfully employed self-attention mechanism, which was a huge success in NLP areas <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>. BERT4Rec <ref type="bibr" target="#b18">[19]</ref> improved SASRec by adopting Transformer <ref type="bibr" target="#b21">[22]</ref> and Cloze-task based training method. TiSASRec <ref type="bibr" target="#b13">[14]</ref> enhanced SASRec by merging timestamp information into self-attention operations. Our work seeks to further expand this trend by suggesting a novel model architecture that uses multiple types of temporal embeddings and self-attention operations to better capture the diverse patterns in user's behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>Several self-attention based recommendation models have been proposed recently <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>. However, the positional factors have been rather overlooked in these models for many reasons. First, they don't utilize timestamp values which hold important contextual information. While TiSASRec <ref type="bibr" target="#b13">[14]</ref> successfully addressed this issue, they also used a simple embedding scheme for temporal values. This can possibly lead to an information bottleneck since the same embedding has to represent all possible positional biases. Furthermore, since all attention heads use the same positional information, they might wastefully learn the same patterns. As a way to mitigate these problems, we devise a novel model architecture that injects various temporal embeddings for each attention head as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Let U be a set of users, and V a set of items. For each user u ∈ U , we have a sequence of items</p><formula xml:id="formula_0">V u = [v u 1 , ..., v u k , ..., v u |V u | |v u k ∈ V ]</formula><p>that the user previously interacted with in chronological order and the corresponding time sequence of the interaction</p><formula xml:id="formula_1">T u = [t u 1 , ..., t u k , ..., t u |V u | |t u k ∈ N]</formula><p>that stores the absolute timestamp values. Our goal is to predict the next item v u nex t that the user u is likely to interact with at the target timestamp t u nex t based on the given history (V u ,T u ). Our model is based on a fixed-length model with length N as in previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>. Given the history</p><formula xml:id="formula_2">(V u ,T u ) of user u, we feed v = [v u |V u |−N +2 , ..., v u |V u | , [MASK]] and t = [t u |V u |−N +2 , ..., t u |V u | , t u nex t</formula><p>] to the model. Then the model estimates v nex t as output. If the history is shorter than N − 1, it is padded by a special token [PAD].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input Embedding</head><p>The embedding layers convert v and t to hidden features that are provided to the attention module. In the case of item array v, each element has an item index. This array is embedded as E it em ∈ R N ×h using the conventional look-up operation with a learnable item embedding table In the case of t, each element has a timestamp value. In previous studies, these values were embedded in a single embedding scheme, if they were used at all. However, given that user's behaviors are mixed with various patterns, a single embedding might not suffice. Moreover, using different encoding functions (instead of plain learnable values) for each embedding is beneficial since we can train each of them to learn a unique pattern. With these in mind, we propose six methods to embed t: three absolute methods (Figure <ref type="figure" target="#fig_0">1</ref>(a)), and three relative methods (Figure <ref type="figure" target="#fig_0">1(b)</ref>).</p><formula xml:id="formula_3">M I ∈ R (|V |+1)×h ,</formula><p>Absolute embeddings encode each position in the sequence as E Day , E Pos , or E Con ∈ R N ×h . The first is Dayembedding which converts the day of each timestamp into an embedding vector to get E Day . We keep a learnable embedding matrix M D ∈ R |D |×h where |D| is the number of possible days in the span of the dataset. In our analysis, Day is a reasonable choice of time length considering the trade-off between matrix size and the effectiveness. Pos-embedding is analogous to the learnable positional embedding used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>, where each position has a corresponding embedding in M P ∈ R N ×h . We convert each position to get E Pos . Note that in this case, the embeddings depend only on the indices of time array and not on the timestamp values. Con-embedding is similar to Pos-embedding, except that all position vectors are shared (M C ∈ R 1×h ) to remove positional bias. Hence, E Con is a stack of repeated vectors.</p><p>Relative embeddings encode the relationship between each interaction pair in the sequence as E Sin , E Exp , or E Loд ∈ R N ×N ×h by utilizing the temporal difference information. First, we define a matrix of temporal differences D ∈ R N ×N whose element is defined as d ab = (t a − t b )/τ , where τ is an adjustable unit time difference. We introduce three encoding functions for D. First, Sin encoder converts the difference d ab to a hidden vector ì θ ab ∈ R 1×h , by the following equation:</p><formula xml:id="formula_4">ì θ ab,2c = sin( d ab f req 2c h ) ì θ ab,2c+1 = cos( d ab f req 2c h )<label>(1)</label></formula><p>where ì θ ab,c is the c t h value of the vector ì θ ab and f is an adjustable parameter. Likewise, Exp encoder and Log encoder converts d ab to ì e ab and ì l ab respectively by applying the following equations:</p><formula xml:id="formula_5">ì e ab,c = exp( −|d | f req c h ) ì l ab,c = loд(1 + |d ab | f req c h )<label>(2)</label></formula><p>Stacking these vectors gives us E Sin , E Exp and E Loд respectively.</p><p>Each embedding offers a distinctive view on temporal data. For example, Sin captures periodic occurrences. Larger time gap is either quickly decayed to zero in Exp or manageably increased in Log. Day can confine the attention scope within the same (or similar) day. Pos can pick up the patterns learnt by the previous models. Finally, Con removes positional bias altogether so that the attention can focus only on the relationship between items. These distinctive views enable our self-attention heads (Figure <ref type="figure" target="#fig_0">1(c</ref>)) to attend to different characteristics of the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Attention Structure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Attention</head><p>Architecture. Multi-head self-attention proposed in Transformer <ref type="bibr" target="#b21">[22]</ref> shows excellent performance in recommendation tasks <ref type="bibr" target="#b18">[19]</ref>. MEANTIME is also based on this structure. Traditionally, self-attention based models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19</ref>] employ a simple absolute positional encoding by feeding the sum of E I t em and E Pos as input as shown in As explained in section 3.2, we can diversify the positional information that is fed to the attention module. MEANTIME utilizes 3 absolute embeddings and 3 relative embeddings. When fed a relative embedding, the head will operate as in Figure <ref type="figure" target="#fig_2">2</ref>(b) with either E Sin , E Exp or E Loд as input to K R . When fed an absolute embedding, the head will operate as in Figure <ref type="figure" target="#fig_2">2</ref>(c) with either E Day , E P os or E Con as input to Q A and K A . As in previous works, the dimension of each head is h n . The choice of n and embedding types are adjustable with regards to the characteristics of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Stacking</head><p>Layers. After self-attention, MEANTIME operates similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>. We apply Position-wise Feed Forward Network (FFN) to the result of self-attention to complete a single layer:</p><formula xml:id="formula_6">FFN(x) = GELU(xW 1 + b 1 )W 2 + b 2 (3)</formula><p>where</p><formula xml:id="formula_7">W 1 ∈ R h×4h , b 1 ∈ R 4h ,W 2 ∈ R 4h×h and b 2 ∈ R h are learnable parameters.</formula><p>Then we stack L such layers. As in previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>, we apply a residual connection for each sublayer to facilitate training:</p><formula xml:id="formula_8">y = x + Dropout(Attention(LayerNorm(x))) z = y + Dropout(FFN(LayerNorm(y)))<label>(4)</label></formula><p>Note that while some works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> apply LayerNorm at the very end, we empirically chose to apply LayerNorm in the front. This is also in accordance with the recent report on the order of layer normalization in Transformers <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Prediction Layer.</head><p>Given the outputs [o 1 , ..., o N ] ∈ R N ×h from the last layer, we obtain the item score distribution for each position by:</p><formula xml:id="formula_9">P (V |v, t) = softmax(GELU(o i W P + b p )E T + b O )<label>(5)</label></formula><p>where Note that we use shared item embeddings for both input and output prediction. P (v i = v |v, t) represents the probability that an item at position i is item v.</p><formula xml:id="formula_10">W P ∈ R h×h , b P ∈ R h ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>To train our model, we adopt the existing technique <ref type="bibr" target="#b18">[19]</ref>. We sample a subarray of items</p><formula xml:id="formula_11">v = [v u k , ..., v u k +N −1 ] and timestamps t = [t u k , ..., t u k +N −1 ] from (V u ,T u</formula><p>), and convert v to v ′ by randomly masking a portion of it with [MASK] by some probability ρ. Then we feed v ′ and t to our model to get P(V |v ′ , t). Next, we calculate the loss as follows:</p><formula xml:id="formula_12">L = − v ′ i is masked log P(v i = v i |v ′ , t)<label>(6) 4 EXPERIMENTS</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>4.1.1 Datasets. We evaluate our model on four real-word datasets with various domains and sparsity: MovieLens 1M <ref type="bibr" target="#b2">[3]</ref>, MovieLens 20M <ref type="bibr" target="#b2">[3]</ref>, Amazon Beauty <ref type="bibr" target="#b15">[16]</ref> and Amazon Game <ref type="bibr" target="#b15">[16]</ref>. We follow the common data preprocessing procedure from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>. We convert each dataset into an implicit dataset by treating each numerical rating or review as a presence of a user-item interaction. We group the interactions by user ids and then sort them by timestamps to form a sequence for each user. Following the custom practice <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>, we discard users and items with less than five interactions to ensure the quality of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation.</head><p>To evaluate the performance of each model, we adopt the widely used leave-one-out evaluation task.</p><p>For each user's item sequence, we hold out the last item for test, and the second last item for validation. We use the rest for training. We follow the common practice <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref> of letting the model rank the ground truth item together with 100 randomly sampled negative items which haven't yet been interacted by the user. As in <ref type="bibr" target="#b18">[19]</ref>, we sample the negative items according to their popularity to make the task more realistic. To score the ranked list, we use two common metrics:</p><p>Recall, and Normalized Discounted Cumulative Gain (NDCG). Both Recall@K and NDCG@K are designed to have larger values when the ground truth item is ranked higher up in the top-k list. We report with K = 5 and 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines Models &amp; Implementation Details</head><p>In order to validate the effectiveness of our method, we compare it with the state-of-the-art baselines: MARank <ref type="bibr" target="#b31">[32]</ref>,</p><p>SASRec <ref type="bibr" target="#b9">[10]</ref>, TiSASRec <ref type="bibr" target="#b13">[14]</ref>, and BERT4Rec <ref type="bibr" target="#b18">[19]</ref>. These models are evaluated based on the code provided by the authors. We implemented MEANTIME<ref type="foot" target="#foot_0">1</ref> with PyTorch. All models are fully tuned through an extensive grid-search on hyperparameters: hidden dimension h ∈ {16, 32, 64, 128}, number of heads n ∈ {2, 4}, dropout ratio ∈ {0.2, 0.5}, and weight decay wd ∈ {0, 0.00001}. We used the same maximum sequence length (N = 200 for MovieLens, N = 50 for Amazon) and the number of layers (L = 2) for all transformer-based models to ensure fairness. We train all models until their validation accuracy doesn't improve for 20 epochs, and then report with the test set. We report the optimal result for each model. As for embedding types in MEANTIME, we used the best combination of embedding types among all possible combinations with n = 2, 4: Day+Pos+Sin+Log for MovieLens 1M and 20M, Day+Sin+Exp+Log for Amazon Beauty, and Day+Pos+Sin+Exp for Amazon Game. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the performance of all models on four datasets. It also shows the relative improvements of our model compared to the strongest baselines. Our model gives the best performance in all metrics and datasets. On average, MEANTIME achieves 9.07%, 6.36%, 10.43%, and 9.14% improvements over the strongest baselines in Recall@5, Recall@10, NDCG@5 and NDCG@10, respectively. Given that we tune all models under the same set of hyperparameters (including the number of attention heads), we can deduce that the performance boost comes from the novel multi-temporal embeddings and attention operations that we proposed. Especially, although TiSASRec uses additional temporal information, it does not show impressive improvement over SASRec and BERT4Rec. We argue that using a single embedding scheme throughout all attention heads was a hindrance to their potential improvement. In contrast, each attention head takes different temporal embedding to extract specialized patterns in our design. This joint design of diverse temporal embeddings and attention mechanism was the key to our improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Multi-temporal Embeddings</head><p>Figure <ref type="figure" target="#fig_5">3</ref> shows the comparison of all possible combinations of temporal embeddings on ML-1M and Amazon Game dataset. We fixed all other hyperparameters (e.g., h = 60). Interestingly, the importance of each embedding seems to differ in each dataset. In ML-1M, relative position embeddings (especially Log) seem to play an important role, whereas in Amazon Game, absolute position embeddings (especially Day) seem to be essential. Therefore, the optimal combination is also different for each dataset. Further analysis with different h shows us that while optimal combination differs slightly for each h, the overall tendency stays the same for each dataset. The same applies to other datasets. This suggests that datasets with distinct characteristics require carefully chosen temporal embeddings that provide the right views on the context of user-item interactions. Note that while we only used n = 2 and 4 for Table <ref type="table" target="#tab_0">1</ref> to be fair with other baselines, using all possible n can further improve our performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper we presented MEANTIME, which uses time information and multiple encoding functions to provide distinctive positional embeddings to self-attention modules. Experiments on four real-world datasets show that our model outperforms state-of-the-art baselines in sequential recommendation. Through extensive ablation study, we also showed that different datasets require different positional embeddings to gain optimal performance. This suggests that one must carefully tune the positional factor in their model considering the characteristics of their own dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Model Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where h is the hidden feature size. M I holds embeddings for |V | items and [MASK] token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Brief overview of single attention head in multi-headed attention structure. (a) absolute self-attention in Transformer (b) relative self-attention in Transformer-XL and MEANTIME (c) absolute self-attention in MEANTIME.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 (</head><label>2</label><figDesc>Figure 2(a). Advanced transformers [1, 29] enhanced this as shown in Figure 2(b). E Pos is replaced by the relative positional embedding E R that only depends on the difference between the positional indices of the array, and positional</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>and b O ∈ R |V | are learnable parameters and E T ∈ R h×|V | is an item embedding table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. NDCG@10 Comparison with All Possible Combinations of Positional Embeddings on ML-1M (left) and Game (right) Dataset. C, D, P, S, E and L represent Con, Day, Pos, Sin, Exp and Log, repectively. We used h = 60 for all experiments.</figDesc><graphic url="image-1.png" coords="7,76.60,95.04,209.92,88.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance Comparison.</figDesc><table><row><cell cols="2">Datasets Metrics</cell><cell>MARANK</cell><cell cols="3">SAS TISAS BERT MEANTIME Improvement</cell></row><row><cell>ML-1M</cell><cell>Recall@5</cell><cell cols="2">0.4427 0.5708 0.5869 0.5862</cell><cell>0.6415</cell><cell>9.31%</cell></row><row><cell></cell><cell>Recall@10</cell><cell cols="2">0.5993 0.6966 0.7102 0.6987</cell><cell>0.7412</cell><cell>4.36%</cell></row><row><cell></cell><cell>NDCG@5</cell><cell cols="2">0.3042 0.4142 0.4307 0.4420</cell><cell>0.4932</cell><cell>11.58%</cell></row><row><cell></cell><cell>NDCG@10</cell><cell cols="2">0.3548 0.4550 0.4708 0.4786</cell><cell>0.5255</cell><cell>9.81%</cell></row><row><cell cols="2">ML-20M Recall@5</cell><cell cols="2">0.3938 0.5274 0.5412 0.5910</cell><cell>0.6225</cell><cell>5.34%</cell></row><row><cell></cell><cell>Recall@10</cell><cell cols="2">0.5503 0.6887 0.7021 0.7176</cell><cell>0.7491</cell><cell>4.39%</cell></row><row><cell></cell><cell>NDCG@5</cell><cell cols="2">0.2665 0.3700 0.3814 0.4469</cell><cell>0.4739</cell><cell>6.04%</cell></row><row><cell></cell><cell>NDCG@10</cell><cell cols="2">0.3170 0.4223 0.4336 0.4879</cell><cell>0.5150</cell><cell>5.55%</cell></row><row><cell>Beauty</cell><cell>Recall@5</cell><cell cols="2">0.1577 0.2011 0.2081 0.2271</cell><cell>0.2470</cell><cell>8.78%</cell></row><row><cell></cell><cell>Recall@10</cell><cell cols="2">0.2302 0.2714 0.2805 0.3095</cell><cell>0.3330</cell><cell>7.58%</cell></row><row><cell></cell><cell>NDCG@5</cell><cell cols="2">0.1085 0.1464 0.1512 0.1633</cell><cell>0.1764</cell><cell>8.05%</cell></row><row><cell></cell><cell>NDCG@10</cell><cell cols="2">0.1318 0.1689 0.1745 0.1898</cell><cell>0.2041</cell><cell>7.52%</cell></row><row><cell>Game</cell><cell>Recall@5</cell><cell cols="2">0.2846 0.3925 0.3857 0.4211</cell><cell>0.4752</cell><cell>12.85%</cell></row><row><cell></cell><cell>Recall@10</cell><cell cols="2">0.4217 0.5201 0.5085 0.5590</cell><cell>0.6100</cell><cell>9.12%</cell></row><row><cell></cell><cell>NDCG@5</cell><cell cols="2">0.1893 0.2735 0.2735 0.2970</cell><cell>0.3446</cell><cell>16.03%</cell></row><row><cell></cell><cell>NDCG@10</cell><cell cols="2">0.2334 0.3147 0.3133 0.3415</cell><cell>0.3882</cell><cell>13.67%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Our code is available at https://github.com/SungMinCho/MEANTIME</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by Samsung Electronics and the National Research Foundation of Korea (NRF) grant (PF Class Heterogeneous High Performance Computer Development, NRF-2016M3C4A7952587) funded by the Ministry of Science, ICT &amp; Future Planning (No. 2013R1A3A2003664).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. the North American Chapter</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Human Language Technologies (NAACL-HLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (tiis)</title>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vista: A Visually, Socially, and Temporally-aware Model for Artistic Recommendation</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Recommender Systems</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>RecSys</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Translation-based Recommendation</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>Recommender Systems (RecSys</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusing similarity models with markov chains for sparse sequential recommendation</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining (ICDM)</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fusing similarity models with markov chains for sparse sequential recommendation</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining (ICDM)</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with top-k gains for session-based recommendations</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<title level="m">Session-based Recommendations with Recurrent Neural Networks. International Conference on Learning Representations (ICLR</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<title level="m">Self-Attentive Sequential Recommendation. International Conference on Data Mining (ICDM)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The bellkor solution to the netflix grand prize</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>Netflix prize documentation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collaborative filtering with temporal dynamics</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural attentive session-based recommendation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management (CIKM</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Time Interval Aware Self-Attention for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Search and Data Mining</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>WSDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">STAMP: short-term attention/memory priority model for session-based recommendation</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Refuoe</forename><surname>Mokhosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton Van Den Hengel</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Personalizing session-based recommendations with hierarchical recurrent neural networks</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Quadrana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>Recommender Systems (RecSys</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factorizing personalized markov chains for next-basket recommendation</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web (WWW)</title>
				<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<title level="m">BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. International Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards neural mixture recommender for long range dependent user sequences</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Personalized top-n sequential recommendation via convolutional sequence embedding</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Search and Data Mining (WSDM)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiushi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<title level="m">Sequential Prediction of Social Media Popularity with Deep Temporal Context Networks. International Joint Conference on Artificial Intelligence (IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent recommender networks</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">How</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Search and Data Mining</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>WSDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Déjà vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Jibang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renqin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Kuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<title level="m">Temporal collaborative filtering with bayesian probabilistic tensor factorization. International Conference on Data Mining (ICSM)</title>
				<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04745</idno>
		<title level="m">On layer normalization in the transformer architecture</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical temporal convolutional networks for dynamic recommender systems</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuck</forename><surname>Rosenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A dynamic recurrent model for next basket recommendation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-order attentive ranking model for sequential recommendation</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple convolutional generative network for next item recommendation</title>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Search and Data Mining</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>WSDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequential click prediction for sponsored search with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep interest evolution network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<title level="m">What to Do Next: Modeling User Behaviors by Time-LSTM. International Joint Conference on Artificial Intelligence (IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
