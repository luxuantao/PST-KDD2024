<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diagnosing deep learning models for high accuracy age estimation from a single image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-01-04">January 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
							<email>jlxing@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
							<email>kai.li@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
							<email>wmhu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
							<email>cfyuan@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>hbling@temple.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer and Information Sciences</orgName>
								<orgName type="institution">Temple University</orgName>
								<address>
									<postCode>19122</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diagnosing deep learning models for high accuracy age estimation from a single image</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-01-04">January 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">CFB3D7659ADC716117D0418BE24E66B2</idno>
					<idno type="DOI">10.1016/j.patcog.2017.01.005</idno>
					<note type="submission">Received date: 14 July 2016 Revised date: 23 November 2016 Accepted date: 4 January 2017 Preprint submitted to Pattern Recognition</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Age estimation</term>
					<term>deep learning</term>
					<term>multi-task learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given a face image, the problem of age estimation is to predict the actual age from the visual appearance of the face. In this work, we investigate this problem by means of the deep learning techniques. We comprehensively diagnose the training and evaluating procedures of the deep learning models for age estimation on two of the largest datasets. Our diagnosis includes three different kinds of formulations for the age estimation problem using five most representative loss functions, as well as three different architectures to incorporate multi-task learning with race and gender classification. We start our diagnoses process from a simple baseline architecture from previous work. With appropriate problem formulation and loss function, we obtain state-of-the-art performance with the simple baseline architecture. By further incorporating our newly proposed deep multi-task learning architecture, the age estimation performance is further improved with high-accuracy race and gender classification results obtained simultaneously. With all the insights gained from the diagnosing process, we finally build a deep multi-task age estimation model which obtains a MAE of 2.96 on the Morph II dataset and 5.75 on the WebFace dataset, both of which improve previous best results by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Age estimation, i.e., predicting the age from a face image, has long been a challenging problem in computer vision, with many applications like precision advertising, intelligent surveillance, face retrieval and recognition. The main challenges of this problem come from the fact that faces may be shot from people of different races, genders, and under conditions of large pose variations, bad illuminations, and spurious makeups <ref type="bibr" target="#b0">[1]</ref>. Human beings ourselves can only give a very rough estimation of the age by only looking at the face.</p><p>Classic age estimation methods usually involve two consecutive but relatively independent procedures, feature extraction from the face image and age estimation from the feature. The objective of the feature extraction procedure is to extract invariant features representing the aging information. Many different kinds of features have been used in previous works, such as the local binary pattern (LBP) <ref type="bibr" target="#b1">[2]</ref> and the Gabor features <ref type="bibr" target="#b2">[3]</ref>. With the extracted features, general machine learning algorithms like the Support Vector Machine <ref type="bibr" target="#b3">[4]</ref> can be used to predict the age.</p><p>The age estimation accuracy of the above classic methods heavily depends on the manually designed features and the employed learning algorithms. Both selections of the designed feature and learning algorithm need many experiences and efforts. Recently, with the fast development of Convolutional Neural Network (CNN), feature representation and classification model can be effectively learned from end to end. Although deep learning has been successfully applied to many computer vision problems, there are very few studies on how to build a high accuracy deep age estimation model, especially on digging out the underlying oracles for building such a model.</p><p>To this end, we intend to perform a comprehensive diagnosis of the deep learning models for the age estimation task, and try to find out the most important and effective factors behind the building of the model. To speedup training and testing of deep age models under different configurations, we design a baseline architecture inspired by <ref type="bibr" target="#b4">[5]</ref> as the basic component to start the diagnosis process. Starting from this baseline architecture, we have diagnosed different aspects to build the deep age estimation model, including the types of model formulation, the choices of loss function, as well as the strategies to incorporate information like race and gender via multi-task learning. Our diagnosing studies are helpful to getting better understandings of a deep age estimation model. Moreover, by accumulating the insights from all these investigations, we finally obtain a very deep age estimation model that outperforms all previous methods by a large margin.</p><p>Overall, the main contributions of this work can be summarized in three-fold:</p><p>1. We have performed comprehensive diagnoses of the deep learning models for the age estimation problem by investigating three different kinds of formulations with five different loss functions to find that the regression based formulation with MAE loss is the best choice.</p><p>2. We have proposed a new architecture for simultaneously performing age estimation, gender and race classification which outperforms other deep multi-task learning architectures.</p><p>3. We have obtained a very deep age estimation model which significantly outperforms all previous solutions on two of the largest benchmark datasets.</p><p>We hope that these findings along with the whole diagnosing process facilitate the deployment of deep age estimation models for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Early works on age estimation are mainly focused on designing robust aging features and selecting learning algorithms. Some features are specifically designed for the age estimation problem, such as the facial features and wrinkles <ref type="bibr" target="#b5">[6]</ref>, the learned AGES (AGing pattErn Subspace) <ref type="bibr" target="#b6">[7]</ref> features, as well as the biologically inspired features (BIF) <ref type="bibr" target="#b7">[8]</ref>. General texture description features like the Local Binary Patterns (LBP) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> and the Gabor feature <ref type="bibr" target="#b2">[3]</ref> are also widely employed for age estimation. Given the aging features, classification models like linear SVM <ref type="bibr" target="#b7">[8]</ref>,Fuzzy LDA <ref type="bibr" target="#b2">[3]</ref>, Probabilistic Boosting Tree <ref type="bibr" target="#b9">[10]</ref>, or regression models like Support Vector Regression <ref type="bibr" target="#b7">[8]</ref>, Kernel Partial Least Squares <ref type="bibr" target="#b10">[11]</ref>,</p><p>Neural Network <ref type="bibr" target="#b11">[12]</ref> and Semidefinite Programming <ref type="bibr" target="#b12">[13]</ref> are explored to estimate the ages.</p><p>Early studies also find that, by incorporating other kinds of facial traits like gender and race information, the performance of age estimation can be substantially improved <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16]</ref>. In the experiments conducted by Guo <ref type="bibr" target="#b14">[15]</ref>, the age estimation error can be reduced by more than 20% if trained separately on male and female. Similar results are also reported from other previous works <ref type="bibr" target="#b9">[10]</ref>. Therefore, joint analysis of these facial traits becomes a natural choice for obtaining better age estimation results.</p><p>Recently, the deep learning models have been consistently demonstrated as a very powerful framework for solving many computer vision problems, e.g., image classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, object detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">22]</ref>, face verification <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>, and facial attribute analyses <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>. The core philosophy within the deep learning framework is to let the network directly learn the feature representations and simultaneously train with the prediction tasks from end to end, which helps the deep learning models set new records for many vision tasks.</p><p>Although with many successes, deep learning models are still mostly thought hard to implement and needs many tips and tricks <ref type="bibr" target="#b27">[27]</ref>. To deploy the deep learning models to facial age estimation, although a very few studies have made some attempts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b26">26]</ref>, the performance gain obtained from these studies are not as significant as those obtained on other vision problems using deep learning models. Moreover, some of these studies focus on some other objectives, e.g., providing a benchmark dataset <ref type="bibr" target="#b4">[5]</ref>, or exploit complicated architectures, such as the 23 sub-networks multi-scale architecture in <ref type="bibr" target="#b28">[28]</ref>, the 36 local subnetworks tree-structured architecture in <ref type="bibr" target="#b26">[26]</ref>. Therefore, we believe that the full potentials of deep learning models for the age estimation problem are still not fully explored, which motivates us to perform a comprehensive diagnosis of the deep age estimation model to dig out its most important parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Diagnosing deep age estimation models</head><p>We now introduce our comprehensive diagnosing process of the deep learning based age estimation models. Model formulation and model architecture are two of the most important components for a deep age estimation problem.</p><p>From model formulation perspective, we have investigated different kinds of formulations for the age estimation problem with the incorporations of commonly used loss functions for each kind of formulation. From the other model architecture perspective, we have studied three different model architectures that incorporate multi-task learning from race and gender classification for the age estimation problem. In the following, we first introduce the basic settings for the diagnosing process. Then we elaborate our diagnosing process on the model formulation and model architecture, respectively. In Table <ref type="table" target="#tab_0">1</ref>, we list all the abbreviations used in our diagnosing process for easy reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Diagnosing settings</head><p>To facilitate the diagnosing process, we provide here some basic settings for the diagnosing process, including the architecture of the baseline model, the selections of the benchmark datasets, and the designation of the evaluation metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Architecture</head><p>Figure <ref type="figure">2</ref>: The baseline architecture employed in our diagnosing process. This architecture is mainly motivated from the AlexNet architecture <ref type="bibr" target="#b16">[17]</ref>, which is one of the most famous modern deep learning architecture, and the one used in the Adience benchmark <ref type="bibr" target="#b4">[5]</ref>, which demonstrate good performance for age group classification firstly using a deep architecture. This relatively simple and shallow architecture also greatly speeds up the diagnosing process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">A baseline deep architecture</head><p>The baseline architecture in the following diagnosing is illustrated in Fig. <ref type="figure">2</ref> filters with a stride 4, followed by a 3 × 3 max pooling layer with a stride 2 and a local response normalization layer <ref type="bibr" target="#b16">[17]</ref>. The second convolutional layer (Conv2) has 256 5 × 5 filters, followed by a 3 × 3 max pooling layer with a stride 2 and a local response normalization layer. The last convolutional layer (Conv3) has 384 3 × 3 filters. Again, followed by a 3 × 3 max pooling layer with a stride 2. The last two layers (FC1 and FC2) are two 512-D fully-connected layers. All the five layers are with the Rectified Liner Units (ReLU) <ref type="bibr" target="#b16">[17]</ref>. This baseline architecture is an AlexNet <ref type="bibr" target="#b16">[17]</ref> based architecture, which employs large size of convolution kernel and stride in the early layers and reduces their sizes gradually as the layer goes deeper. Our choice of this architecture as the baseline is initially motivated by the previous work <ref type="bibr" target="#b4">[5]</ref> which employed this architecture to perform age group classification and demonstrated very good performance. Furthermore, as this baseline architecture is relatively small, it can greatly speedup the diagnosing process and save much time for both model training and testing. Based on these considerations, we therefore employ it as the baseline architecture for all the diagnosing process. It is worth noting that other modern CNN architectures such as VGGNet <ref type="bibr" target="#b18">[19]</ref> and GoogLeNet <ref type="bibr" target="#b29">[29]</ref> with smaller kernel size can be easily integrated into our final diagnosing model.</p><p>We have performed this kind of experiments in our final comparisons with the state-of-the-art methods in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Age estimation datasets</head><p>There are many datasets for age estimation in the literature <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref>.</p><p>Most of these datasets, however, are relatively small. Since training a good deep neural network generally requires a large amount of training data, we therefore select two of the largest benchmark datasets to perform the diagnoses, the Morph II <ref type="bibr" target="#b33">[33]</ref> dataset and the WebFace <ref type="bibr" target="#b34">[34]</ref> dataset.</p><p>Morph II Dataset: Morph II contains about 55,000 face images of more than 13,000 subjects. Age ranges from 16 to 77 years old. Morph II is a multiethnic dataset with additional gender and race labels. It has about 77% Black faces and 19% White faces, while the remaining 4% includes Asian, Hispanic, Indian, and Other. Since Morph II is highly unbalanced in terms of race and gender distributions, in order to get a balanced training set, we follow the pre- vious study <ref type="bibr" target="#b10">[11]</ref> to split this dataset into three non-overlapped subsets S1, S2</p><p>and S3 (see WebFace Dataset: The WebFace dataset contains 59,930 face images.</p><p>Age ranges from 1 to 80 years old. The WebFace dataset is also a multi-ethnic dataset with additional gender labels. Unlike Morph II, this dataset is captured in the wild environment, images contain large pose and expression variations, which makes this dataset much more challenging. Following <ref type="bibr" target="#b34">[34]</ref>, we conduct experiments on this dataset using a four-fold cross validation protocol.  With this strategy employed for all the diagnosed models in our paper, we believe the diagnosing results of all the models are referable. We will describe the "age-aware sampling" strategy in detail in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Evaluation metrics</head><p>The most widely used evaluation metric for age estimation in the literature is the Mean Absolute Error (MAE), which is defined as follows,</p><formula xml:id="formula_0">MAE = 1 N N i=1 |ŷ i -y i | ,<label>(1)</label></formula><p>where N is the number of testing samples, y i is the ground truth age and ŷi is the predicted age of the i-th sample. This MAE metric has been the standard evaluation metric for the age estimation problem since the very beginning of age estimation research <ref type="bibr" target="#b30">[30]</ref>. This evaluation metric has been widely employed in the previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b28">28]</ref>, and in order to make a direct comparison with previous methods, we therefore employ this metric as one basic metric in this paper.</p><p>Although widely adopted, the MAE metric has several shortcomings. When evaluating a large testing set, improvements of some testing samples may not make a significant difference on the MAE value, especially when the MAE value is already very low. For most of the testing samples, the compared deep age estimation models can all produce correct predictions. The differences between different models are their ability to deal with a small number of difficult testing samples. These differences cannot be well reflected by MAE metric. Another shortcoming of MAE is that it cannot reflect the distributions of the estimation errors. To overcome these shortcomings, we design two other metrics to evaluate and compare the performance of different age estimation methods.</p><p>The first metric is Cumulative Correct Score (CCS) that is used to compare multiple methods. CCS is defined as the number of test images such that the absolute age estimation error is not higher than a year threshold t, i.e.,</p><formula xml:id="formula_1">CCS(t) = N i=1 h(|ŷ i -y i | -t),<label>(2)</label></formula><formula xml:id="formula_2">h (x) =      1, if x ≤ 0 0, otherwise .<label>(3)</label></formula><p>In practice, people are more concerned about age estimator's performance at different thresholds than a single MAE number. So we can study CCS at different thresholds to clearly locate the difference of performance between multiple methods. For example, if the CCS of one method at different thresholds are consistently larger than other methods, we can conclude that this method performs better than others.</p><p>The second metric is the Relative Cumulative Correct Score (RCCS) which is specifically used to compare two methods. RCCS is defined as,</p><formula xml:id="formula_3">RCCS a b (t) = CCS a (t) -CCS b (t),<label>(4)</label></formula><p>where CCS a is the CCS of method a, CCS b is the CCS of method b. From RCCS we can clearly see which method is better at different thresholds. Both of the above two metrics can reflect the error distribution of the method and thus are more intuitive and expressive than a single MAE number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model formulation</head><p>Age estimation can be formulated as a multi-class classification problem, a regression problem, or a ordinal regression problem. In this part, we intend to diagnose the effects of these three different types of formulations in the deep learning framework, which has not been explicitly studied by previous work to the best of our knowledge. </p><formula xml:id="formula_4">L(X ) = - 1 N N i=1 y iyi log p iyi ,<label>(5)</label></formula><p>where p i is the predicted C-dimensional probability vector for X i by the CNN, p iyi is the y i -th element of p i , and y i is the one-hot encoding of y i , i.e., y iyi = 1 and 0 otherwise.</p><p>Label distribution based method. It is noted that in the one-hot encoding based methods, the ages are treated as independent from each other.</p><p>To consider the correlations between face images with different ages under the classification framework, <ref type="bibr" target="#b35">[35]</ref> proposed a label distribution learning method for age estimation. The general idea is extending the one-hot encoding of one face image to a label distribution. In this work, we use the Gaussian label distribution. For the i-th face image X i with age label y i , then the k-th dimension of the corresponding target label distribution is defined as follows:</p><formula xml:id="formula_5">d ik = 1 σ √ 2πZ exp(- (k -y i ) 2 2σ 2 ), k = 1, . . . , C,<label>(6)</label></formula><p>where σ is the standard deviation of the Gaussian distribution, and Z is a normalization factor that makes sure k d ik = 1, i.e.,</p><formula xml:id="formula_6">Z = 1 σ √ 2π k exp(- (k -y i ) 2 2σ 2 ). (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>The training objective is defined as follows based on the cross-entropy loss: </p><formula xml:id="formula_8">L(X ) = - 1 N N i=1 C k=1 d ik log p ik ,<label>(8) 3.2</label></formula><formula xml:id="formula_9">L(X ) = 1 N N i=1 (ŷ i -y i ) 2 , (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>where ŷi is the prediction and y i is the ground truth age. This MSE loss has very nice mathematical properties like convexity and being continuously differentiable, which makes it widely used in regression based age estimation and many other regression problems.</p><p>MAE based method. Besides the MSE loss, we find that the Mean Absolute Error (MAE) can potentially provide a better loss for a deep age estimation model, which is defined as follows:</p><formula xml:id="formula_11">L(X ) = 1 N N i=1 |ŷ i -y i | . (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>The main inspiration behind is that the performance of an age estimation model is evaluated using the MAE metric (Eq. 1). Using this evaluation metric as the loss function for the deep model provides a more straightforward objective for end-to-end model training. To our knowledge, MAE is not directly used as the loss function in previous work for age estimation. The main reason for this may be that the MAE loss is not a smooth function, which makes it hard to optimize for traditional age estimation methods. With recent developments of optimizing non-smoothing functions like ReLU <ref type="bibr" target="#b16">[17]</ref> and PReLU <ref type="bibr" target="#b17">[18]</ref> in the deep learning framework, the MAE loss function can be optimized effectively using the stochastic gradient descent algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Ordinal regression formulation for age estimation</head><p>The human face matures in different ways depending on the person's age.</p><p>For example, facial aging effects appear as changes in the shape of the face during childhood and changes in skin texture during adulthood. Based on this observation, previous works <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> argue that it is difficult for regression based methods to handle this aging pattern non-stationary problem, and suggest using ordinal regression based methods for age estimation. In this work, we also diagnose this ordinal regression formulation for age estimation using CNN. </p><formula xml:id="formula_13">X k = {X i , y k i } N i=1</formula><p>, where y k i ∈ {0, 1} is the label indicating whether the age of the i-th face image is larger than k. Training on this X k one can obtain the k-th binary classifier f k . After obtaining all these C -1 binary classifiers, for a given test face image X j , the predicted age ŷj is calculated as follows:</p><formula xml:id="formula_14">ŷj = 1 + C-1 k=1 f k (X j ),<label>(11)</label></formula><p>where f k (X j ) is the binary classification result for X j by f k .</p><p>One may notice that this approach has the problem of unbalanced classes.</p><p>For example, for the age C -1, almost all examples will be of the class less than C -1 years. We tackle this problem from two perspectives. First, from the network structure, we adopt one network to collectively implement all these C -1 binary classifiers in our experiments. In particular, our network has a multiple-output structure where each output corresponds to a binary classifier.</p><p>Thus, these C -1 classification sub-problems (tasks) are simultaneously trained in an end-to-end manner. So, these tasks can learn and benefit from each other.</p><p>For example, the tasks with balanced classes may help the learning of tasks faced with unbalanced problems. Second, from the training process, we try to make the mini-batch as uniform as possible (same amount of positive and negative samples) for each task during network training, which can alleviate the unbalanced problem to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model architecture</head><p>Besides the model formulation, the model architecture is also very important for the deep age estimation problem. For the model architecture, we mainly study the different ways to design the architecture to incorporate multi-task learning for more effective age estimation. Since age, gender, and race are three closely related facial traits of a human, early studies on age estimation suggest that incorporating these three different kinds of traits together can improve the results of age estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. Moreover, it will be very beneficial to use a single network in a practical system which can save much computation and memory. These facts naturally motivate us to investigate multi-task learning of gender, race, and age to obtain additional performance gain. To perform multi-task learning of age, gender, and race upon the baseline architecture, we will introduce three different multi-task learning architectures in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Parallel multi-task learning architecture</head><p>The parallel multi-task learning architecture fuses the different tasks concurrently and has been widely adopted in previous deep learning models <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b22">22]</ref>. This type of architecture has show very good performances in problems like face recognition <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40]</ref> and object detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>To deploy this multi-task learning strategy to the baseline architecture, we attach three fully-connected layers to the last layer (i.e., FC2) of the baseline architecture (Fig. <ref type="figure" target="#fig_5">4</ref>(a)). Each fully-connected layer is then followed by a loss function designed for the task. The weights W 1 , ..., W 5 associated with the first five layers are shared by all the three tasks. W a , W g , and W r are the task specific parameters. Denote the training dataset as X = {X i , y a i , y r i , y g i } N i=1 , where y a i , y r i , and y g i are the labels for age, race and gender, respectively. The loss for the parallel multi-task learning model is defined as follows:</p><formula xml:id="formula_15">L(X ) =L a + αL g + βL r = 1 N N i=1 |F a i -y a i | -α 1 N N i=1 2 k=1 y g ik log F g ik -β 1 N N i=1 Cr k=1 y r ik log F r ik ,<label>(12)</label></formula><p>where α and β are hyper-parameters to tune the importance of each task, F a i is the predicted age for the i-th sample, F g i is the predicted gender probability vector of the i-th sample, F g ik is the k-th element of F g i , and y g ik = 1 if the i-th sample has gender label k and 0 otherwise. The meanings of F r i , F r ik , y r ik are similar for race, and C r is number of race labels. Note that here we use  the MAE loss for the regression based age estimation task (F a i is a scalar), and Softmax loss for the race and gender classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Deeply supervised multi-task learning architecture</head><p>The deeply supervised multi-task learning architecture fuses different tasks progressively in different layers, which is inspired by previous work like deeply supervised nets <ref type="bibr" target="#b41">[41]</ref> and many other variants like GoogLeNet <ref type="bibr" target="#b29">[29]</ref> and <ref type="bibr" target="#b24">[24]</ref>. This kind of architecture has also demonstrated performance improvement on tasks like image classification.</p><p>To deploy this multi-task learning strategy to the baseline architecture, we add supervision branches after certain intermediate layers for gender and race classification tasks. Age estimation is done at last layer, which is similar to the parallel architecture. Please see Fig. <ref type="figure" target="#fig_5">4</ref>(b) for more details. The intuition behind the deeply supervised multi-task learning architecture is that the three tasks (age, gender, race) are not of the same difficulty. Age estimation is more difficult than the other two tasks, and it requires more layers of abstractions with large capacity. Therefore, the loss function for the age estimation is connected to the highest-level features. The race classification task on Morph II only involves black and white people, which is relatively easy to distinguish based on only low-level features (e.g., color and texture), it is thus connected to the first convolutional layer. Gender classification is slightly more difficult than race classification, so it requires slightly higher-level features and is connected to the second convolutional layer. Since feature maps at lower convolutional layers may be noisy and not discriminative enough, following <ref type="bibr" target="#b41">[41]</ref>, we add a dimensionality reduction layer (e.g., Conv g and Conv r layers with kernel size 1 × 1 in Fig. <ref type="figure" target="#fig_5">4(b)</ref>)</p><p>and two discriminative non-linear mapping layers before the final classification.</p><p>The overall loss is the same as the parallel architecture discussed above, and the network is trained using stochastic gradient descent algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Hybrid multi-task learning architecture</head><p>Although the above two architectures use gender and race information, they do not consider the relationship between tasks. Previous studies on age estimation <ref type="bibr" target="#b14">[15]</ref> suggest that age estimation can be influenced by the gender and race differences dramatically, i.e., age estimation errors can be increased when across gender and/or race.</p><p>Inspired </p><formula xml:id="formula_16">F a = k∈{BF,BM,WF,WM} F group k F k ,<label>(13)</label></formula><p>where F group k is the element of F group corresponding to race gender group k.</p><p>To train the hybrid multi-task age estimation model, we design a three-step procedure: 1) Pre-training race gender group classification part using all the training data; 2) Pre-training group specific age estimation part using group specific training data; and 3) Fine-tuning the whole network from end to end using all the training data. The testing procedure is simple, we can use Eq. 13</p><p>to obtain the final age estimations. We can also obtain the race and gender predictions from the group classification part easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and analyses</head><p>In this section, we first describe some details about our experimental settings.</p><p>Then, we give the experimental results on different model formulations and different multi-task architectures. We will also analyze the model depth for age estimation. Finally, we compare our best model with the state-of-the-art age estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental settings</head><p>The face images in the dataset were preprocessed in a standard way, i.e., the faces in the images are detected and aligned, then cropped and normalized to 256 × 256. For all the following experiments, we use the Caffe <ref type="bibr" target="#b42">[42]</ref> toolbox, which provides a flexible framework to develop new deep learning models, and makes our work easy to reproduce. All the model protocol files and training results in our experiments will be released in the Caffe model zoo. We train all the network using mini-batch (set to 128) stochastic gradient descent with momentum (0.9) and weight decay (5 × 10 -4 ). For all fully-connected layers we use a dropout ratio of 0.5. We use data augmentation similar to <ref type="bibr" target="#b16">[17]</ref>, i.e., randomly cropping of 227 × 227 pixels from the 256 × 256 input face images, then randomly mirroring it before feeding it to the network. The initial learning rate is 10 -3 . We divide the learning rate by 10 every 10000 iterations, and training stops at 50000 iterations. These hyper-parameters are chosen based on a hold-out validation set. We found that all the networks converge well under these settings, so we use the same parameters for different models to make fair comparisons between different methods.</p><p>As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the datasets used in this paper are imbalanced at some specific ages.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analyses of classification based deep age estimation methods</head><p>To study the classification formulation for age estimation using CNN, we train two models Net OH and Net LDL from the baseline architecture (Fig. <ref type="figure">2</ref>).</p><p>Net OH is based on the One-Hot encoding and Net LDL is based on the Label Distribution Learning introduced in Section 3.2.1. The age estimation results of these two models on both datasets are shown in Table <ref type="table" target="#tab_8">3</ref> and Table <ref type="table" target="#tab_9">4</ref>.</p><p>We can clearly see that Net LDL outperforms Net OH on both datasets. This is because in Net OH , the age labels are assumed to be independent to one another.</p><p>However, Net LDL considers the correlations between different ages. These results</p><p>show that it is better to use label distribution learning for classification based deep age estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analyses of regression based deep age estimation methods</head><p>To study the regression formulation for age estimation using CNN, we train Since the widely adopted Morph II benchmark is collected in controlled environment, the performance on it is already very high. For most of the testing samples, the compared deep age estimation models can all produce correct predictions, and improvements of some hard testing samples may not make a significant difference on the MAE value. In order to better show the gain in performance of Net MAE over Net MSE , we use our RCCS metric (Eq. 4) to compare Net MAE and Net MSE . The results are show in Fig. <ref type="figure" target="#fig_6">5</ref>. We can see that</p><formula xml:id="formula_17">two</formula><p>Net MAE can make more correct predictions than Net MSE at all the thresholds (from 0 to 6 years) since all the numbers in Fig. <ref type="figure" target="#fig_6">5</ref> are positive numbers.</p><p>Thanks to this MAE loss function which is not only robust to outliers but    The reason for this can be explained as follows. From birth to adulthood, the greatest change of the face is the craniofacial growth, and during adult aging, the most perceptible change becomes texture change which is subtler than the craniofacial growth. Therefore, it is more difficult to estimate the age for the adult people than for the young people, and this fact is also true for our human beings. We can also see that Net MAE performs better than Net LDL and Net OR at most of the different testing ages.</p><p>As mentioned in Section 3.2.3, previous works argue that it is difficult for regression based age estimation methods to handle the aging pattern nonstationary problem and it is better to use ordinal regression based methods.</p><p>In this work, we show that this claim may not be true under the deep learning framework. Fig. <ref type="figure" target="#fig_12">8</ref> shows the output responses of the first convolutional layer of the regression based deep age estimation model Net MAE on two face images.</p><p>We can see that for the young face (Fig. <ref type="figure" target="#fig_12">8</ref>   We are curious about whether using multi-task learning with race and/or gender information can further improve the performance of age estimation. Table <ref type="table" target="#tab_13">5</ref> and Table <ref type="table" target="#tab_14">6</ref> show the results of our three multi-task learning architectures introduced in Section 3.3 on both datasets. From our new CCS metric (Eq. 2), we can also see that the Net Hybrid obtains the largest CCS at different thresholds (from 0 to 4 years) on both datasets.</p><p>These results demonstrate that our hybrid architecture can make more accurate predictions which is very important for practical use. The main reason behind this is that our hybrid architecture considers the relationship between tasks, and encodes this information directly into the network design.   The results are show in Fig. <ref type="figure">9</ref>. We can see that the MAE of white people is smaller than that of black people. The main reason behind this is that it may be easier to detect the facial appearance changes of white people than those of black people. We can also see that the MAE of male is smaller than that of female, this is because males and females may have different face aging patterns.</p><p>Many female faces may potentially show younger appearances than male face due to the different extent in using makeups and accessories <ref type="bibr" target="#b0">[1]</ref>, and this fact makes it more difficult to estimate the age of females.  On the WebFace dataset, our model improves the best results by about 2 years. Since the WebFace dataset is built from faces in the wild, few methods conducted experiments on this challenging dataset. We have compared our </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Some example face images in the two benchmark datasets used in this paper. The images in the top row are from the Morph II dataset, and the images in the bottom row are from the WebFace dataset. As we can see, age estimation is a challenging task in computer vision. Even for human beings ourselves, it is very difficult to tell the accurate age from a face image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Age distribution of the training set of WebFace dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Age distributions of the training set of both datasets used in this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Ordinal regression can be considered an intermediate problem in between regression and classification. The most successful and widely used algorithm to solve the ordinal regression problem is to transform it into a series of simpler binary classification subproblems<ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39]</ref>. For each age k ∈ {1, 2, ..., C -1}, a binary classifier is trained according to whether the age of a face is larger than k. Then the age of a test face is predicted based on the classification results of these C -1 binary classifiers. Specifically, given the original training face images X = {X i , y i } N i=1 , for the k-th binary classification subproblem a specific training data is constructed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Internal structure of each computational block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of our three multi-task learning architectures. F denote the intermediate layer outputs and W are the weights for each computational block. Computational blocks of the same type are shown in the same color. Fig. 4(d) shows the internal structure of each kind of block. For example, the Conv1 and FC1 in Fig. 4(a) are instantiations of "Conv Block 1" and "FC Block 1" in Fig. 4(d), respectively. We use a, g, and r to denote age, gender and race. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The RCCS between Net MAE and Net MSE on Morph II dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>based methods, respectively. The results are show in Fig.6and Fig.7. We can see that the difficulties of age estimation from different ages are not the same. The MAE of the young people are smaller than that of the adult people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Age estimation performances of Net LDL , Net OR and Net MAE at different testing ages on Morph II dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Age estimation performances of Net LDL , Net OR and Net MAE at different testing ages on WebFace dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 : 4 . 5 .</head><label>845</label><figDesc>Figure 8: Visualizing output of the first convolutional layer of Net MAE . For the input faces (a) and (c), (b) and (d) show the output responses of the first convolutional layer of Net MAE for these two faces respectively. The first convolutional layer of Net MAE contains 96 feature maps, whose outputs are shown here on a 10 × 10 grid.</figDesc><graphic coords="26,222.40,375.38,204.13,204.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>5 MAEFigure 9 :</head><label>59</label><figDesc>Figure 9: The age estimation results of Net VGG Hybrid for different races (Black vs. White) and genders (Female vs. Male) on Morph II dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>4. 8 .</head><label>8</label><figDesc>Comparison with the state-of-the-art methods Table 9 and Table 10 compare our best model (i.e., Net VGG Hybrid ) with several recently published methods on Morph II and WebFace datasets.All of the methods evaluated in this section are using the same training and testing set partition protocol which is discussed in Section 3.1.2 for fair comparisons. Our results outperform all the other state-of-the-art methods on both datasets by a large margin. On the Morph II dataset, our best model reduces MAE by 0.65 years which is a significant improvement. To the best of our knowledge, this is for the first time an MAE below 3 years has been obtained on the Morph II age estimation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>List of abbreviations used in the diagnosing process.</figDesc><table><row><cell>Abbreviation</cell><cell>Explanation</cell></row><row><cell>BF</cell><cell>Black Female</cell></row><row><cell>BM</cell><cell>Black Male</cell></row><row><cell>LDL</cell><cell>Label Distribution Learning</cell></row><row><cell>OH</cell><cell>One Hot</cell></row><row><cell>OR</cell><cell>Ordinal Regression</cell></row><row><cell>WF</cell><cell>White Female</cell></row><row><cell>WM</cell><cell>White Male</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The number of images in the three splits of Morph II dataset.</figDesc><table><row><cell>Gender Race</cell><cell>Black</cell><cell>White</cell><cell>Others</cell></row><row><cell>Female</cell><cell cols="3">S1:1285 S2:1285 S3:3187 S1:1285 S2:1285 S3:31 S3:129</cell></row><row><cell>Male</cell><cell cols="3">S1:3980 S2:3980 S3:28843 S1:3980 S2:3980 S3:39 S3:1843</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>). S1 and S2 are balanced in terms of race and gender dis-</cell></row><row><cell>tributions after this split and being used as training set separately. Specifically,</cell></row><row><cell>in all experiments, the training and testing are repeated for twice: 1) training</cell></row><row><cell>on S1, testing on S2+S3 and 2) training on S2, testing on S1+S3.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>by these findings, we further propose a hybrid multi-task learning architecture for age estimation by explicitly incorporating this prior knowledge</figDesc><table /><note><p><p><p><p>into the network architecture. See Fig.</p>4(c</p>) for more details. This architecture mainly comprises of four parts: 1) shared part (i.e., Conv1 and Conv2), 2) race gender group (i.e., BF, BM, WF, WM) classification part, 3) group specific age estimation part, and 4) a fusion layer that fuses the prediction made by each group specific age estimator.</p>F group is the output of group classification part indicating the probability of an input belonging to each race gender group. Each of the group specific age estimator part excels in estimating the age of images belongs to one specific race gender group. After we obtain the group probabilities F group and all high accurate group specific age predictions (i.e., F BF ,. . . , F WM ) for an input, we employ the average fusion strategy to get the final prediction F a as follows:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>To alleviate its impact on the results of deep age estimation models, we apply an "age-aware sampling" strategy during training. In practice, we use two types of lists, one is age list, and the other is per-age face image list. At each iteration during training, we first sample an age M in the age list, then sample a face image in the per-age face image list of age M , and repeat this process multiple times to create a mini-bath. When reaching the end of the per-age face image list of age M , a random shuffle operation is performed to reorder the face images of age M . When reaching the end of age list, we also perform a random shuffle operation to reorder the ages. With this sampling strategy for the mini-batch based training process, we can get a mini-batch as uniform as possible with respect to ages, and thus alleviate the imbalanced problem to some extent.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>The age estimation results of the three different formulations on Morph II dataset using the training and testing set split protocol in Table2.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">s2+s3 MAE s1+s3 MAE Average MAE</cell></row><row><cell>Classification based methods</cell><cell>Net OH</cell><cell>3.85</cell><cell>3.89</cell><cell>3.87</cell></row><row><cell></cell><cell>Net LDL</cell><cell>3.48</cell><cell>3.49</cell><cell>3.49</cell></row><row><cell>Regression based methods</cell><cell>Net MSE</cell><cell>3.44</cell><cell>3.43</cell><cell>3.44</cell></row><row><cell></cell><cell>Net MAE</cell><cell>3.40</cell><cell>3.39</cell><cell>3.40</cell></row><row><cell>Ordinal regression based method</cell><cell>Net OR</cell><cell>3.46</cell><cell>3.48</cell><cell>3.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>The age estimation results of the three different formulations on WebFace dataset using the four-fold cross validation protocol.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="5">Fold1 MAE Fold2 MAE Fold3 MAE Fold4 MAE Average MAE</cell></row><row><cell></cell><cell>NetOH</cell><cell>6.65</cell><cell>6.76</cell><cell>6.63</cell><cell>6.64</cell><cell>6.67</cell></row><row><cell>Classification based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>NetLDL</cell><cell>6.20</cell><cell>6.24</cell><cell>6.10</cell><cell>6.26</cell><cell>6.20</cell></row><row><cell></cell><cell>NetMSE</cell><cell>6.40</cell><cell>6.46</cell><cell>6.30</cell><cell>6.44</cell><cell>6.40</cell></row><row><cell>Regression based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>NetMAE</cell><cell>6.12</cell><cell>6.13</cell><cell>5.99</cell><cell>6.22</cell><cell>6.12</cell></row><row><cell>Ordinal regression based method</cell><cell>NetOR</cell><cell>6.19</cell><cell>6.28</cell><cell>6.20</cell><cell>6.30</cell><cell>6.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>models Net MSE and Net MAE from the baseline architecture. Net MSE is based on the Mean Squared Error and Net MAE is based on the Mean Absolute Error introduced in Section 3.2.2. The age estimation results of these two models on Morph II and WebFace datasets are shown in Table3and Table4.We can see that Net MAE is better than Net MSE on both datasets. There are two reasons to explain these results. First, the MAE loss is more robust to outliers than the MSE loss, which is very important in practice, because label noises are inevitable in real-world datasets. For example, the WebFace dataset contains many more label errors than the Morph II dataset, so the performance gap between Net MAE and Net MSE is larger on WebFace dataset than on the</figDesc><table /><note><p>Morph II dataset. Second, MAE is the evaluation metric for age estimation algorithm (see Eq. 1), so directly optimize this MAE loss can improve the age estimation performance. For example, even though the Morph II dataset was complied in a controlled environment and has few label errors, Net MAE which directly optimizes the final evaluation metric still performs better than Net MSE on this dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>4.4. Analyses of ordinal regression based deep age estimation methodsWe use Net OR to denote the Ordinal Regression based deep age estimation model introduced in Section 3.2.3. From the age estimation results in Table3and Table4, we can see that Net OR performs better than Net OH and is comparable to Net LDL . Surprisingly, the regression base methods Net MAE still outperforms Net OR on both datasets. We also plot the age estimation performances at different testing ages on both datasets for Net LDL , Net MAE and Net OR which are representative methods of classification, regression, and ordinal regression</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>). These intuitive visualizations demonstrate that the regression based deep model Net MAE can learn this non-stationary aging process automatically and effectively. Based on the above analysis of the three formulations for age estimation using CNN, we can see that the regression based deep model Net MAE is the most promising one despite of its simplicity. Compared with Net OH which assumes that each age is independent from other ages, Net MAE provides a natural formulation which takes account of the continuous nature of age. Compared with Net MSE , Net MAE is more robust to outliers which is very important in read-world datasets. Compared with Net LDL and Net OR , Net MAE can not only directly optimize the final evaluation metric but also can automatically capture the non-stationary aging process and thus obtains competitive results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>The results of our three multi-task learning architectures on Morph II dataset.From the MAE evaluation metric, we can see that the parallel architecture Net Parallel is comparable to or slightly worse than the single task model Net MAE , which is also reported in<ref type="bibr" target="#b28">[28]</ref>. Deeply supervised architecture Net Deeply is slightly better than Net Parallel , but again shows no improvement over the single task model Net MAE . These results suggest that simply inserting multiple loss functions to the network and forcing it to learn from each other dose not work well on age estimation. Our proposed hybrid architecture Net Hybrid , on the contrary, works better than the other two architectures and shows improvements over the single task model Net MAE .</figDesc><table><row><cell>Method</cell><cell cols="9">Training Set Testing Set CCS(0) CCS(1) CCS(2) CCS(3) CCS(4) MAE (yrs.) Average MAE (yrs.)</cell></row><row><cell></cell><cell>S1</cell><cell>S2+S3</cell><cell>4642</cell><cell>13676</cell><cell>21316</cell><cell>27400</cell><cell>32194</cell><cell>3.40</cell><cell></cell></row><row><cell>NetMAE</cell><cell>S2</cell><cell>S1+S3</cell><cell>4734</cell><cell>13741</cell><cell>21564</cell><cell>27621</cell><cell>32341</cell><cell>3.39</cell><cell>3.40</cell></row><row><cell></cell><cell>S1</cell><cell>S2+S3</cell><cell>4627</cell><cell>13575</cell><cell>21547</cell><cell>27499</cell><cell>32154</cell><cell>3.42</cell><cell></cell></row><row><cell>Parallel architecture NetParallel</cell><cell>S2</cell><cell>S1+S3</cell><cell>4682</cell><cell>13557</cell><cell>21478</cell><cell>27397</cell><cell>32185</cell><cell>3.43</cell><cell>3.43</cell></row><row><cell></cell><cell>S1</cell><cell>S2+S3</cell><cell>4684</cell><cell>13649</cell><cell>21363</cell><cell>27435</cell><cell>32036</cell><cell>3.40</cell><cell></cell></row><row><cell>Deeply supervised architecture NetDeeply</cell><cell>S2</cell><cell>S1+S3</cell><cell>4751</cell><cell>13707</cell><cell>21196</cell><cell>27264</cell><cell>32088</cell><cell>3.43</cell><cell>3.42</cell></row><row><cell></cell><cell>S1</cell><cell>S2+S3</cell><cell>4720</cell><cell>14013</cell><cell>21758</cell><cell>27924</cell><cell>32738</cell><cell>3.32</cell><cell></cell></row><row><cell>Hybrid architecture NetHybrid</cell><cell>S2</cell><cell>S1+S3</cell><cell>4814</cell><cell>13973</cell><cell>21825</cell><cell>28080</cell><cell>32871</cell><cell>3.30</cell><cell>3.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>The results of our three multi-task learning architectures on WebFace dataset. Table 7 and Table 8 show the age estimation results of Net VGG Hybrid on both dataset. We can see that Net VGG Hybrid based on the very deep VGGNet performs much better than Net Hybrid which is based on the shallow baseline architecture. These results demonstrate that using deeper model with smaller convolutional kernel size can further improve the age estimation performance.4.7. Analyses of age estimation accuracy under different races and gendersWe use Morph II dataset to analysis the age estimation accuracy for people of different races and genders, since this dataset has both race and gender labels.</figDesc><table><row><cell>Method</cell><cell cols="8">Testing Fold CCS(0) CCS(1) CCS(2) CCS(3) CCS(4) MAE (yrs.) Average MAE (yrs.)</cell></row><row><cell></cell><cell>Fold1</cell><cell>1372</cell><cell>4017</cell><cell>5942</cell><cell>7296</cell><cell>8403</cell><cell>6.12</cell><cell></cell></row><row><cell></cell><cell>Fold2</cell><cell>1427</cell><cell>4089</cell><cell>5990</cell><cell>7345</cell><cell>8466</cell><cell>6.13</cell><cell></cell></row><row><cell>NetMAE</cell><cell>Fold3</cell><cell>1442</cell><cell>4137</cell><cell>6117</cell><cell>7534</cell><cell>8576</cell><cell>5.99</cell><cell>6.12</cell></row><row><cell></cell><cell>Fold4</cell><cell>1345</cell><cell>4067</cell><cell>5995</cell><cell>7323</cell><cell>8412</cell><cell>6.22</cell><cell></cell></row><row><cell></cell><cell>Fold1</cell><cell>1322</cell><cell>3932</cell><cell>5904</cell><cell>7314</cell><cell>8419</cell><cell>6.16</cell><cell></cell></row><row><cell></cell><cell>Fold2</cell><cell>1347</cell><cell>4046</cell><cell>5941</cell><cell>7342</cell><cell>8437</cell><cell>6.17</cell><cell></cell></row><row><cell>Parallel architecture NetParallel</cell><cell>Fold3</cell><cell>1405</cell><cell>4072</cell><cell>6015</cell><cell>7482</cell><cell>8602</cell><cell>6.05</cell><cell>6.15</cell></row><row><cell></cell><cell>Fold4</cell><cell>1355</cell><cell>3954</cell><cell>5892</cell><cell>7320</cell><cell>8441</cell><cell>6.23</cell><cell></cell></row><row><cell></cell><cell>Fold1</cell><cell>1350</cell><cell>3953</cell><cell>5965</cell><cell>7343</cell><cell>8485</cell><cell>6.13</cell><cell></cell></row><row><cell></cell><cell>Fold2</cell><cell>1346</cell><cell>3975</cell><cell>5870</cell><cell>7227</cell><cell>8342</cell><cell>6.20</cell><cell></cell></row><row><cell>Deeply supervised architecture NetDeeply</cell><cell>Fold3</cell><cell>1400</cell><cell>4042</cell><cell>6098</cell><cell>7512</cell><cell>8588</cell><cell>6.05</cell><cell>6.15</cell></row><row><cell></cell><cell>Fold4</cell><cell>1351</cell><cell>3955</cell><cell>5859</cell><cell>7291</cell><cell>8446</cell><cell>6.22</cell><cell></cell></row><row><cell></cell><cell>Fold1</cell><cell>1813</cell><cell>4504</cell><cell>6399</cell><cell>7695</cell><cell>8667</cell><cell>6.03</cell><cell></cell></row><row><cell></cell><cell>Fold2</cell><cell>1790</cell><cell>4438</cell><cell>6312</cell><cell>7674</cell><cell>8701</cell><cell>6.08</cell><cell></cell></row><row><cell>Hybrid architecture NetHybrid</cell><cell>Fold3</cell><cell>1777</cell><cell>4453</cell><cell>6359</cell><cell>7785</cell><cell>8871</cell><cell>5.91</cell><cell>6.03</cell></row><row><cell></cell><cell>Fold4</cell><cell>1751</cell><cell>4290</cell><cell>6212</cell><cell>7589</cell><cell>8660</cell><cell>6.08</cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p>4.6. Analyses of the model depth for age estimation</p>Very deep CNNs, such as VGGNet</p><ref type="bibr" target="#b18">[19]</ref> </p>and GoogLeNet</p><ref type="bibr" target="#b29">[29]</ref> </p>have achieved great success for many computer vision tasks. Since our baseline architecture in Fig.</p>2</p>is relatively shallow compared to these very deep architectures, we are curious about whether using these very deep architectures can further improve the performance of age estimation. Based on this consideration, we train another mode Net VGG Hybrid which is a hybrid multi-task architecture based on the very deep VGGNet. Compare to our baseline architecture which is shallow (3 convolutional layers) with large kernel size (7 × 7), the VGGNet is deeper (16 layers) with smaller kernel size (3 × 3).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>The age estimation results of Net Hybrid and Net VGG Hybrid on Morph II dataset using the training and testing set split protocol in Table2.</figDesc><table><row><cell>Method</cell><cell cols="3">S2+S3 MAE S1+S3 MAE Average MAE</cell></row><row><cell>Net Hybrid</cell><cell>3.32</cell><cell>3.30</cell><cell>3.31</cell></row><row><cell>Net VGG Hybrid</cell><cell>2.96</cell><cell>2.95</cell><cell>2.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>The age estimation results of Net Hybrid and Net VGG Hybrid on WebFace dataset using the four-fold cross validation protocol.</figDesc><table><row><cell>Method</cell><cell cols="5">Fold1 MAE Fold2 MAE Fold3 MAE Fold4 MAE Average MAE</cell></row><row><cell>Net Hybrid</cell><cell>6.03</cell><cell>6.08</cell><cell>5.91</cell><cell>6.08</cell><cell>6.03</cell></row><row><cell>Net VGG Hybrid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>Comparison with the state-of-the-art methods on Morph II dataset.</figDesc><table><row><cell>Methods</cell><cell cols="3">Gender Accuracy Race Accuracy Age MAE</cell></row><row><cell>BIF [8]</cell><cell>96.6</cell><cell>-</cell><cell>5.09</cell></row><row><cell>KPLS [11]</cell><cell>98.4</cell><cell>98.9</cell><cell>4.18</cell></row><row><cell>KCCA [16]</cell><cell>98.5</cell><cell>99.0</cell><cell>3.98</cell></row><row><cell>Ridge [34]</cell><cell>97.7</cell><cell>-</cell><cell>4.80</cell></row><row><cell>Tree-a-CNN [26]</cell><cell>98.4</cell><cell>-</cell><cell>3.61</cell></row><row><cell>Multi-scale-CNN [28]</cell><cell>98.0</cell><cell>98.6</cell><cell>3.63</cell></row><row><cell>Net VGG Hybrid</cell><cell>98.7</cell><cell>99.2</cell><cell>2.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Comparison with the state-of-the-art methods on WebFace dataset.model to all other published results we can find on this dataset, including the latest one in<ref type="bibr" target="#b26">[26]</ref>. Our 1.97 years reduction of MAE is a significant improvement over the state-of-the-art methods considering the difficulty of this dataset. This competing performance of our model indicates the effectiveness of our diagnoses in model formulation, loss function, multi-task learning architecture, and model depth for age estimation.5. Conclusion and future workIn this paper, we investigate the deep learning based age estimation problem. We have performed in-depth diagnoses of the deep learning models for the age estimation problem. Started from a simple baseline architecture, we have progressively improved its performance by investigating three different kinds of formulations of the model using five different loss functions, as well as the model architecture design for multi-task learning. By accumulating all these findings, we finally obtain a very deep age estimation model with high prediction accuracy. We hope these findings and results to be useful for the research and application of the deep age estimation techniques.In our future work, we plan to study the age estimation problem regarding to a specific age group or a specific person, using the deep learning models. For the age-specific age estimation problem, we need to find a principled way to learn age-dependent optimization objectives for the deep age estimation model.For the person-specific age estimation problem, we plan to design some transfer learning based mechanism to adapt the knowledge learned from a general deep age estimation model to a dedicated deep age estimation model for a specific person.</figDesc><table><row><cell>Methods</cell><cell cols="2">Gender Accuracy Age MAE</cell></row><row><cell>BIF [8]</cell><cell>79.3</cell><cell>10.65</cell></row><row><cell>RF [43]</cell><cell>-</cell><cell>9.38</cell></row><row><cell>Ridge [34]</cell><cell>87.0</cell><cell>9.75</cell></row><row><cell>Tree-a-CNN [26]</cell><cell>89.7</cell><cell>7.72</cell></row><row><cell>Net VGG Hybrid</cell><cell>92.3</cell><cell>5.75</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This work is partly supported by the 973 basic research program of China (Grant No. 2014CB349303), the Natural Science Foundation of China (Grant No. 61472421, 61672519, and 61303178), and the Strategic Priority Research</p><p>Program of the CAS (Grant No. XDB02070003). We thank NVIDIA Corporation for donating a GeForce GTX Titan X GPU used in this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Age synthesis and estimation via faces: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intel</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1955" to="1976" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Demographic classification with local binary patterns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Bio</title>
		<meeting>IEEE Int. Conf. Comput. Bio</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face age classification on consumer images with gabor feature and fuzzy LDA method</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Bio</title>
		<meeting>IEEE Int. Conf. Comput. Bio</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Age estimation using a min-max modular support vector machine</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Info. Process</title>
		<meeting>Int. Conf. Neural Info. ess</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. Workshops</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Age classification from facial images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitoria</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic age estimation based on facial aging patterns</title>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith-Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intel</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2234" to="2240" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human age estimation using bio-inspired features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic age classification with LBP</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gunay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nabiyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Comput. Info. Sci</title>
		<meeting>Int. Symp. Comput. Info. Sci</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A probabilistic boosting tree for face gender classification on consumer images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Bio</title>
		<meeting>IEEE Int. Conf. Comput. Bio</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simultaneous dimensionality reduction and human age estimation via kernel partial least squares regression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ranking with uncertain labels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Multimedia Expo</title>
		<meeting>IEEE Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="96" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning autostructured regressor from uncertain nonnegative labels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Age and gender estimations by modeling statistical relationship among faces</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koshimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Knowledge-Based Intel</title>
		<meeting>Int. Conf. Knowledge-Based Intel</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="559" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A study on automatic age estimation using a large database</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1986" to="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint estimation of age, gender and ethnicity: CCA vs. PLS</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Face Gesture</title>
		<meeting>IEEE Int. Conf. Face Gesture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Info. Process. Systems</title>
		<meeting>Adv. Neural Info. ess. Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int</title>
		<meeting>IEEE Int</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Info. Process. Systems</title>
		<meeting>Adv. Neural Info. ess. Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2518" to="2525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Info. Process. Systems</title>
		<meeting>Adv. Neural Info. ess. Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shape driven kernel adaptation in convolutional neural network for robust facial traits recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="222" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Neural networks: tricks of the trade, 2nd Edition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Age estimation by multi-scale convolutional network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comput. Vis</title>
		<meeting>Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward automatic simulation of aging effects on face images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intel</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human age estimation with regression on discriminative aging manifold</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="584" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensic Secur</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Face Gesture</title>
		<imprint>
			<biblScope unit="page" from="341" to="345" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Visual image recognition system with object-level image representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>National University of Singapore</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Facial age estimation by learning from label distributions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intel</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2401" to="2412" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ordinal hyperplanes ranker with cost sensitivities for age estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A learning framework for age rank estimation based on face images with scattering transform</title>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Imag. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="785" to="798" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A simple approach to ordinal classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Mach. Learn</title>
		<meeting>Eur. Conf. Mach. Learn</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ordinal regression by extended binary classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Info. Process. Systems</title>
		<meeting>Adv. Neural Info. ess. Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2892" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Arti. Intel. Stat. Conf</title>
		<meeting>Arti. Intel. Stat. Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relative forest for attribute prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comput. Vis</title>
		<meeting>Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="316" to="327" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
