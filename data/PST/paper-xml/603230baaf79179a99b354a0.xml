<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-04">4 August 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hai</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhifei</forename><surname>Li</surname></persName>
							<email>zhifei1993@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhaoli</forename><surname>Zhang</surname></persName>
							<email>zl.zhang@ccnu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Tingting</forename><surname>Liu</surname></persName>
							<email>tingtingliu89619@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Neal</forename><forename type="middle">N</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Engineering Research Center for E-Learning</orgName>
								<orgName type="institution" key="instit2">Central China Normal University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Education</orgName>
								<orgName type="institution">Hubei University</orgName>
								<address>
									<postCode>430062</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for Educational Big Data</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Northeastern State University</orgName>
								<address>
									<postCode>74464</postCode>
									<settlement>Tahlequah</settlement>
									<region>OK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-04">4 August 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TNNLS.2021.3055147</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks</head><p>Zhifei Li , Student Member, IEEE, Hai Liu , Senior Member, IEEE, Zhaoli Zhang , Member, IEEE, Tingting Liu , Member, IEEE, and Neal N. Xiong, Senior Member, IEEE Abstract-Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relationpath. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.</p><p>Index Terms-Graph heterogeneity, graph neural networks (GNNs), knowledge graph (KG) embedding, KGs, link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>K NOWLEDGE graphs (KGs) are beneficial for use in various intelligence applications, such as recommendation systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, information retrieval <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, and question answering <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. To facilitate these knowledge-driven applications, numerous types of KGs <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref> have been developed in the past decades. KGs are multi-relational graphs that store structural information about human knowledge. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), KGs are generally comprised of nodes and edges that represent numerous concrete facts. The nodes are the various entities, and the edges denote the relations between adjoining entities. These elements are organized in the form of a (subject entity, relation, object entity) triplet such as (C. Ronaldo, play_for, Juventus).</p><p>Although KGs already contain massive entities, relations, and triplets, they are still confronted with issues such as completeness, partialness, and newly-added knowledge <ref type="bibr" target="#b9">[10]</ref>. To overcome these issues, researchers have directed their attention to link prediction, which seeks to predict missing facts in KGs. Existing link prediction methods are recognized as KG embedding (KGE) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. KGE learns the embedded expression of relations and entities to preserve the inherent structure of the KGs. Then these embeddings are utilized to facilitate subsequent link prediction tasks.</p><p>However, few previous KGE methods have structure enforcement and incorporate connectivity structure into the embedding space. By contrast, graph neural networks (GNN) <ref type="bibr" target="#b12">[13]</ref> can effectively aggregate local information for each node. On the one hand, as a representation learning tool for graph data, GNN can leverage the neighbor features associated with nodes. On the other hand, the learning efficiency of convolution computing can be improved by imposing the same aggregation function. However, KGs usually come with multiple types of entities and relations, which are widely known as the heterogeneous information networks <ref type="bibr" target="#b13">[14]</ref>. We can observe that the entities show different semantic features under each relation-based triplet. Due to the complexity of heterogeneous graphs, traditional GNN methods cannot be directly applied to KGs. Thus, the following issues have to be considered in the design of effective GNN architecture for KGs:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Heterogeneity of KGs</head><p>KGs have an intrinsic property of heterogeneity in which various types of entities have various attributes and their features may belong to disparate embedding spaces. Still taking Fig. <ref type="figure" target="#fig_0">1</ref>(a) as an example, the node of the player involves different entity attributes including occupation, country, club, and teammate through different relations. Meanwhile, the node of the country may be involved in the player and club. Thus, handling such complex structural graph data and reserving multiple feature information simultaneously is an urgent issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Importance of Relation</head><p>The heterogeneity of KGs is usually reflected by the relation-path <ref type="bibr" target="#b14">[15]</ref>, which demonstrates complex semantic features that involve different triplets. Furthermore, it can </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Influence of Aggregator</head><p>The entities fuse the neighbor features of each relation-path through the aggregator function, which is a key component of GNN architecture. Different from deep learning over Euclidean space (e.g., images, sentences, and videos), the graph-structured data generally have no regular sequence. Thus, the aggregator functions should run over a disordered set of feature vectors. Meanwhile, it needs to be trainable and maintain high computational efficiency during neural networks training. Many effective aggregator functions, and we need to investigate the performance influence with GNN architecture composed of different aggregator functions.</p><p>Based on the preceding analysis, we propose a novel heterogeneous relation attention networks framework for learning KGE, named HRAN. This framework considers the relation importance through the attention mechanism and examines three diverse aggregator functions. In particular, to handle various types of entities, it first fuses each relation-path-based entities' features which can represent one type of semantic information. Then features with various semantic information are aggregated through different relation-paths to the entity. Furthermore, the attention mechanisms are utilized to obtain the weight values of different relation-paths. Based on the learned attention values for each relation-path, our method can aggregate the appropriate combination of neighbor features in a hierarchical structure. Thus, the aggregated entity features can effectively tackle the rich semantic information and complex structure in heterogeneous KGs.</p><p>The major contributions of this article are as follows. 1) A novel end-to-end heterogeneous relation attention networks (HRAN) framework is proposed. Specifically, HRAN fuses each type of semantic-specific information through the relation-paths. It can aggregate neighbor features in a hierarchical manner and preserve the diverse feature information simultaneously. Our work enables direct application of the GNN on the heterogeneous KGs, and further facilitates the subsequent link prediction tasks.</p><p>2) The attention mechanism is utilized to learn the importance of each relation-path. Based on the learned attention values, the proposed method can selectively aggregate informative features and suppress useless ones. Moreover, HRAN adopts three effective aggregator functions to reduce the variance and computation complexity, which can be applied to the large-scale heterogeneous graphs. 3) Extensive experiments are conducted to evaluate the performance of the proposed method. The results show the superiority of HRAN by comparing it with the state-of-the-art methods. More importantly, by analyzing the influence of the attention mechanism, the proposed HRAN demonstrates its potential advantage for experimental results. The rest of this article is organized as follows: related work is reviewed and analyzed in Section II. The research problem is formally defined, and the details of the HRAN are introduced in Section III. The experimental setup and results of the proposed method are represented in Section IV. Finally, the conclusions and recommendations for future work are discussed in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Neural Networks</head><p>GNNs aim to extend the deep neural networks for graph-structured data. Xu et al. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> present a theoretical framework to show its expressive power for different graph structures and reasoning tasks. Bruna et al. <ref type="bibr" target="#b17">[18]</ref> first proposed convolutional networks to graphs based on spectral approaches. In follow-up works, Kipf et al. <ref type="bibr" target="#b18">[19]</ref> further simplify the graph convolutions through a localized first-order approximation, named graph convolutional networks (GCN). Hamilton et al. <ref type="bibr" target="#b19">[20]</ref> propose the GraphSAGE framework based on node sampling and features aggregating. It can efficiently generate node embeddings by leveraging neighbor feature information. Chen et al. <ref type="bibr" target="#b20">[21]</ref> introduce an efficient GCN variant FastGCN for inductive node classification, which is enhanced with importance sampling. Xu et al. <ref type="bibr" target="#b21">[22]</ref> explore the jumping knowledge (JK) networks that enable better structure-aware representation through flexibly leveraging different neighborhood ranges. Wu et al. <ref type="bibr" target="#b22">[23]</ref> observe that the considerable complexity of GCN may be burdensome and unnecessary, and then propose simple graph convolution (SGC). SGC can reduce the redundant complexity by replacing the nonlinearities into a single linear transformation. Based on autoregressive moving average (ARMA) filters, Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 09:59:47 UTC from IEEE Xplore. Restrictions apply.</p><p>Bianchi et al. <ref type="bibr" target="#b23">[24]</ref> propose a novel graph convolutional layer which shows a more flexible response via a rich transfer function. Interested readers can refer <ref type="bibr" target="#b12">[13]</ref> for further comparison.</p><p>As an important learning technique, the attention mechanisms (e.g., self-attention <ref type="bibr" target="#b24">[25]</ref>) have been successfully applied to many graph-based applications, such as recommendation systems <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Inspired by the attention mechanisms, Velickovic et al. <ref type="bibr" target="#b27">[28]</ref> present the graph attention networks (GAT) for node classification, which learns the weight values between a node and its neighbors. However, we can observe that the aforementioned GNN methods are designed for homogeneous graphs. They cannot directly apply in heterogeneous KGs composed of various types of entities and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Knowledge Graph Embedding</head><p>KG embedding (KGE) has been extensively studied in relation to prediction tasks of missing or incorrect triplets in KGs. KGE seeks to translate the semantics of relations and entities into low-dimensional continuous embedding spaces. Such a goal is achieved through a general methodology of defining a score function for the triplet. The aim of the optimization is usually to score a correct triplet higher than incorrect triplets. Based on the type of score function, existing methods can be broadly classified into three types: translational distance-based methods, semantic matching-based methods, and neural networks-based methods.</p><p>The most representative translational distance method is TransE <ref type="bibr" target="#b28">[29]</ref>, which regards the relation as a translation from the subject entity to the object entity. Despite its simplicity and efficiency, TransE involves issues when modeling complex relations. Thus, a range of variants including TransH <ref type="bibr" target="#b29">[30]</ref>, TransR <ref type="bibr" target="#b30">[31]</ref>, and TransD <ref type="bibr" target="#b31">[32]</ref> are proposed to transform entities and relations into different subspaces. TorusE <ref type="bibr" target="#b32">[33]</ref> further keeps the translational property and embeds relations and entities on a lie group. RotatE <ref type="bibr" target="#b33">[34]</ref> introduces a rotation-based translational method in the complex vector space. ModE <ref type="bibr" target="#b34">[35]</ref> embeds entities into the polar coordinate system, which combines the modulus and phase information. The semantic matching methods measure the probability of a triplet by matching latent semantics in the embedding space. The typical method DistMult <ref type="bibr" target="#b35">[36]</ref> learns embeddings through a bilinear operation, which contains a bilinear product between entities, and a diagonal matrix for relations. ComplEx <ref type="bibr" target="#b36">[37]</ref> extends DistMult to the complex space and model asymmetric relations by taking the conjugate of the object entity embedding. The details and comparisons of these methods are provided in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref>. However, the aforementioned methods learn less expressive features in comparison with deep and multilayer architecture, thus potentially limiting their performance.</p><p>With its considerable success in natural language processing and computer vision, deep neural networks have been studied for learning KGE. For instance, Dettmers <ref type="bibr" target="#b37">[38]</ref>, Nguyen et al. <ref type="bibr" target="#b38">[39]</ref>, and Vashishth et al. <ref type="bibr" target="#b39">[40]</ref> adopt a multiple layer of convolutional neural networks (CNN) over embeddings for link prediction tasks, thereby achieving expressive performance in common benchmark data sets. To incorporate neighbor feature information surrounding an entity, Schlichtkrull et al. <ref type="bibr" target="#b40">[41]</ref>, Nathani et al. <ref type="bibr" target="#b41">[42]</ref>, Shang et al. <ref type="bibr" target="#b43">[43]</ref> propose extensions of GNN for multi-relational KGs. Although the aforementioned methods achieve performance improvement on link prediction tasks, they do not consider the graph heterogeneity and attention mechanisms in the network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we present a novel heterogeneous relation attention networks (HRAN) framework for KG embedding. The proposed method follows a hierarchical structure including entity-level aggregation, relation-level aggregation, and triplet prediction. Before introducing the HRAN framework, the basic concepts of link prediction and graph convolution are summarized. The main notations utilized throughout this article are presented in Table <ref type="table" target="#tab_0">I</ref>.</p><p>A. Preliminaries 1) Link Prediction: Formally, the KGs G can be expressed as</p><formula xml:id="formula_0">G = (E, R, T )<label>(1)</label></formula><p>where Given an entity with a specific relation, link prediction aims to predict another suitable entity that is able to compose a correct triplet, that is predicting object entity e o given subject entity and relation (e s , r ). For each triplet, such a goal is achieved through a general methodology of defining a score function ϕ(e s , r, e o ) ∈ R. The aim of the optimization is generally to score a correct triplet higher than incorrect triplets. 2) Graph Convolution: In this article, the graph convolution is utilized to aggregate node features and generate embeddings for link prediction tasks in an end-to-end manner. Let H (l)  represent the nodes feature matrix of the l-th layer in GNN, the forward propagation can be written as</p><formula xml:id="formula_1">E = {e 1 , e 2 ,</formula><formula xml:id="formula_2">H (l) = f D− 1 2 Ã D− 1 2 H (l−1) W (l) (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where f is the activation function. Ã = (A + I) ∈ R |E|×|E| denotes the adjacency matrix of G, which contains the selfconnections. The symbol D represents the degree matrix of Ã, that is Dii = j Ãi j . And W (l) ∈ R d (l−1) ×d (l) is a layer-specific weight matrix. For the directed graph containing asymmetric adjacency matrix, the Ã can be normalized by an inverse diagonal matrix D−1 as</p><formula xml:id="formula_4">H (l) = f ( D−1 ÃH (l−1) W (l) ).</formula><p>(</p><formula xml:id="formula_5">)<label>3</label></formula><p>It can observe that the above convolution operation is proposed to deal with homogeneous graph-structured data. As relations and relation-path-based entities are two fundamental elements in KGs, a novel GNN framework for heterogeneous graph data is necessary to capture their intrinsic differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Entity-Level Aggregation</head><p>Fig. <ref type="figure" target="#fig_2">2</ref> presents the overall framework of HRAN. Due to the heterogeneity of KGs, various types of entities may occur in different feature spaces. It is not suitable to directly aggregate all neighbor features for each entity. Thus, the entity-level aggregation is proposed to first aggregate each relationpath-based entity features.</p><p>Based on h 0 e and r 0 r as the initial entity and relation features respectively, we first fuse each relation-path-based entity features and the aggregation equation is given as</p><formula xml:id="formula_6">h (l−1) N r (e) = agg h (l−1) i ∀i ∈ N r (e) ∀r ∈ R (4)</formula><p>where N r (e) denotes a set of relation-path-based entities. The symbols h (l−1) i and h (l−1)</p><formula xml:id="formula_7">N r (e)</formula><p>are the i-th entity and aggregated feature based on the relation-path r in the (l-1)-layer, respectively. In this article, the aggregator function agg in entity level is followed as GCN. Thus, (4) can be rewritten as</p><formula xml:id="formula_8">h (l−1) N r (e) = 1 N r (e) ⎛ ⎜ ⎝ i∈N r (e) h (l−1) i ⎞ ⎟ ⎠ ∀r ∈ R (5)</formula><p>where </p><formula xml:id="formula_9">N r 1 (e) , h (l−1) N r 2 (e) , . . . , h (l−1) N r |R| (e) }. Since the aggregated feature h (l−1) N r (e)</formula><p>is generated through a single relationpath, each of them will be semantic-specific and can capture one type of semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relation-Level Aggregation</head><p>In the relation-level aggregation part, various types of semantic information are aggregated through the relation-paths related to the entity. Due to the heterogeneity of KGs, there are multiple types of semantic information reflected by entities. Each semantic-specific aggregated feature can only capture information from one aspect. To aggregate more comprehensive semantic information, the multiple features needed to be revealed by different relation-paths. Moreover, treating each relation-path equally weakens the semantic features aggregated by important relation-paths. To address these issues, we propose a novel relation-based attention mechanism to obtain the importance of different relation-paths then utilized to aggregated various types of semantic information.</p><p>To learn the importance of different relation-paths, taking |R| groups of entity-level aggregated features as input, the learned weights of each relation-path {r 1 , r 2 , . . . , r |R| } are denoted as follows:</p><formula xml:id="formula_10">α (l−1) r 1 , . . . , α (l−1)</formula><formula xml:id="formula_11">r |R| = att r (l−1) r 1 , . . . , r (l−1)</formula><p>r |R| <ref type="bibr" target="#b5">(6)</ref> in which</p><formula xml:id="formula_12">α (l−1) r = att r (l−1) r ∀r ∈ R (7)</formula><p>where att denotes the attention function designed by deep neural networks. It can capture the relation-based importance and utilize it to selectively aggregate informative features behind a heterogeneous KG.</p><p>To learn the weight value of each relation-path, a nonlinear transformation (e.g., one-layer MLP) is first utilized to transform the relation-specific feature r (l−1) r . Then the importance of relation-specific feature is measured through the attention vector q. The final α (l−1) r is obtained by the activation function. The process is shown as follows:</p><formula xml:id="formula_13">α (l−1) r = σ q tanh Wr (l−1) r + b ∀r ∈ R (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where W denotes the transformation weight matrix, b represents the bias vector, q is the attention vector, and σ is the sigmoid function. In particular, the aforementioned parameters are independent of entity or relation features.</p><p>Obviously, the higher α (l−1) r learns, the more important relation-path r is.</p><p>After obtaining the importance of each relation-path, each relation-path-based aggregated features can be weighted with the learned α (l−1) r as a coefficient. Then, all relation-path-based aggregated neighbor features can be concatenated and fused to obtain the final aggregated neighbor features for each entity as follows:</p><formula xml:id="formula_15">h (l−1) N (e) = agg CONCAT α r h (l−1) N r (e) , ∀r ∈ R (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>where CONCAT denotes the concatenate operation and agg is the aggregator function. Since the aggregating process should be trainable and maintain high computational efficiency during method training, three effective ways mean/max/sum are presented as follows:</p><formula xml:id="formula_17">h (l−1) N (e) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 1 |r | d 1 ∀r∈R α r h (l−1) N r (e) max CONCAT α r h (l−1) N r (e) ∀r ∈ R d 1 ∀r∈R α r h (l−1)</formula><p>N r (e) <ref type="bibr" target="#b9">(10)</ref> the symbol d represents the d-dimension feature. The sum aggregator is approximately similar to the aggregator function utilized in the GCN framework. The mean and max aggregators are inspired by the pooling approach in CNNs. Then the graph convolution propagation can be updated through the non-linearity transformation as</p><formula xml:id="formula_18">h (l) e = f W (l) e h (l−1) N (e) r (l) r = f W (l) r r (l−1)</formula><p>r <ref type="bibr" target="#b10">(11)</ref> where W (l)  e and W (l) r are entity-specific and relation-specific connection coefficient matrices, respectively, and f is the rectified linear units (ReLU). However, it can observe that all neighboring features are fused in h (l−1)</p><p>N (e) but not the entity feature h (l−1) e itself. Thus, the self-loops need to be incorporated in the convolution propagation. Furthermore, to make the method more flexible, the hyperparameter β is introduced, named self-attention value. Then <ref type="bibr" target="#b10">(11)</ref> can be redefined as follows:</p><formula xml:id="formula_19">h (l) e = f W (l) e (1 − β)h (l−1) N (e) + βh (l−1) e (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>the symbol β determines the reservation ratio of the entity feature itself in the self-loops. Finally, we can concatenate each entity features h (L)   e and relation features r (L)   r in the last layer to obtain the embedding matrices. And it can be defined as</p><formula xml:id="formula_21">E = CONCAT h (L) e ∀e ∈ E R = CONCAT r (L) r ∀r ∈ R . (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>Compared with previous GCN-based methods, the proposed entity-level aggregation and relation-level aggregation can aggregate entity features from various semantic aspects through different relation-paths. Moreover, the attention mechanism is utilized to selectively aggregate informative features based on important relation-paths. The overall process is shown in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Triplet Prediction</head><p>Then the convolution operation is implemented on the input matrix M by N different filters r = [ </p><formula xml:id="formula_24">α (l−1) r ← att r (l−1)</formula><p>r , ∀r ∈ R ; // Calculate the relation-path attention value. 9:</p><formula xml:id="formula_25">h (l−1) N (e) ← agg CONCAT α (l−1) r h (l−1) N r (e)</formula><p>, ∀r ∈ R ; // Fuse all relation-paths based features. 10:</p><formula xml:id="formula_26">h (l) e ← f W (l) e (1 − β)h (l−1)</formula><p>N (e) +βh (l−1) e ; // Update entity features. 11: </p><formula xml:id="formula_27">r (l) r ← f W (l) r r (l−</formula><formula xml:id="formula_28">16: return E ∈ R |E|×d , R ∈ R |R|×d R N ×h×w to generate the feature maps V = [v 1 , v 2 , . . . , v N ] ∈ R N ×H ×W as follows: v n (i, j ) = M n r (i, j ) = h a=1 w b=1 M(i + a, j + b) n r (a, b)<label>(16)</label></formula><p>where refers to the convolution operation. The symbol N r ∈ R h×w represents the relation-specific convolutional filter with the size of height h and width w. Furthermore, v n ∈ R H ×W is the outputted feature map, where H = 2−h +1 and W = d − w + 1.</p><p>In predicting the linked object entity e o , the outputted feature maps are flattened and projected onto a d-dimension vector which contains the potential semantic connection between the subject entity e s and relation r . Then the score function ϕ(e s , r, e o ) of ConvD can be defined as</p><formula xml:id="formula_29">ϕ(e s , r, e o ) = f ((vec( f (V))W))e o (<label>17</label></formula><formula xml:id="formula_30">)</formula><p>where vec(x) denotes the flattening operation, W ∈ R (N ×H ×W )×d is the fully connected linear transformation matrix to project recalibration feature maps onto a d-dimension vector, and then utilized to score with candidate object entities. Based on ( <ref type="formula" target="#formula_23">15</ref>)-( <ref type="formula" target="#formula_29">17</ref>), the score function can be rewritten as follows:</p><formula xml:id="formula_31">ϕ(e s , r, e o ) = f (vec( f (CONCAT{e s , r} r ))W)e o . (<label>18</label></formula><formula xml:id="formula_32">)</formula><p>Finally, the prediction probability of a triplet (e s , r , e o ) is defined as ŷ(e s , r, e o ) = σ (ϕ(e s , r, e o ) + b) ∈ (0, 1)</p><p>where b is a bias term, and σ (x) = 1/(   <ref type="bibr" target="#b19">(20)</ref> in which</p><formula xml:id="formula_34">y = 1 if (e s , r, e o ) ∈ T 0 if (e s , r, e o ) ∈ T (<label>21</label></formula><formula xml:id="formula_35">)</formula><p>where T is a set of incorrect triplets generated by corrupting the correct triplet set T . Then the loss function for HRAN can be defined as follows: </p><formula xml:id="formula_36">min L = − log p(G| ) = − (es ,</formula><formula xml:id="formula_37">. (<label>22</label></formula><formula xml:id="formula_38">)</formula><p>The HRAN is regularized by using the dropout technique <ref type="bibr" target="#b44">[44]</ref> in the training process. Batch normalization <ref type="bibr" target="#b45">[45]</ref> is adopted after each layer to stabilize, regularize, and increase the rate of convergence. The label smoothing <ref type="bibr" target="#b46">[46]</ref> is utilized to lessen overfitting and improve generalization. The Adam optimizer <ref type="bibr" target="#b47">[47]</ref>, a fast and computationally efficient tool for gradient-based optimization, is employed to optimize the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND DISCUSSION</head><p>In this section, the performance of HRAN is evaluated against several state-of-the-art methods on link prediction tasks. Extensive experiments and analysis are conducted to answer the following questions: Q1. Can the entity and relation embeddings generated by heterogeneous GNNs effectively predict missing links in KGs? (Experiment 1) Q2. What is the effect of relation-path-based attention mechanism in the whole method framework? (Experiment 2) Q3. What the influence of using different aggregator functions on link prediction performance? (Experiment 3) Before introducing the experiment results, the general settings of the proposed method are briefly summarized. </p><formula xml:id="formula_39">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ rank s i = ⎛ ⎝ xs i / ∈T test I ϕ(x i ) &lt; ϕ xs i ⎞ ⎠ + 1 rank o i = ⎛ ⎝ xo i / ∈T test I ϕ(x i ) &lt; ϕ xo i ⎞ ⎠ + 1 (<label>23</label></formula><formula xml:id="formula_40">)</formula><p>where I[P] is the indicator function that returns 1 with the condition P is true, and returns 0 otherwise. Three general evaluation metrics are utilized to measure prediction accuracy. It includes mean rank (MR), mean reciprocal rank (MRR), and Hits@k (for k = 1, 3, and 10), which are defined as follows:</p><formula xml:id="formula_41">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ MR : 1 2|T test | x i ∈T test rank s i + rank o i MRR : 1 2|T test | x i ∈T test 1 rank s i + 1 rank o i Hits@k : 1 2|T test | x i ∈T test I rank s i ≤ k + I rank o i ≤ k (<label>24</label></formula><formula xml:id="formula_42">)</formula><p>where MR and MRR are average ranks and average inverse ranks for all test triplets, respectively, and Hits@k is the percentage of ranks lower than or equal to k. The lower MR and higher MRR or Hits@k are, the better is the performance.</p><p>2) Data Sets: Extensive experiments are conducted on the following benchmark data sets: WN18, FB15k-237, and WN18RR. Each of them contains massive entities and relations, and is split into train, valid, and test set. The statistics of these data sets are shown in Table <ref type="table" target="#tab_4">III</ref>, and detailed information are listed as follows.</p><p>1) WN18 <ref type="bibr" target="#b28">[29]</ref> is extracted from WordNet <ref type="bibr" target="#b6">[7]</ref> and contains 18 relations and 40 943 entities. The entities represent word senses, and relations define lexical relationships between entities. 2) FB15k-237 <ref type="bibr" target="#b37">[38]</ref> is a subset of FB15k <ref type="bibr" target="#b28">[29]</ref> with the reversible relations removed. This subset contains 14 541 entities with 237 different relations. 3) WN18RR <ref type="bibr" target="#b37">[38]</ref> is a subset of WN18 with the reversible relations removed. This subset contains approximately 40 943 entities with 11 different relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Comparison Methods:</head><p>To validate the performance of our proposed method, several state-of-the-art baseline methods are adopted from the field of link prediction, including translational distance-based methods, semantic matching-based methods, and neural networks-based methods. Their details are presented as follows.</p><p>1) TransE: TransE <ref type="bibr" target="#b28">[29]</ref> converts entities and relations into translation embeddings to model multi-relational data. It is the most widely used link prediction method. 2) TransD: TransD <ref type="bibr" target="#b31">[32]</ref> extends TransE to model complex relations by projecting the entities into a relation-related space. 3) TorusE: TorusE <ref type="bibr" target="#b32">[33]</ref> further embeds relations and entities on a lie group. It is scalable to large KGs due to its complexity. 4) RotatE: RotatE <ref type="bibr" target="#b33">[34]</ref> represents entities as complex vectors and relations as rotations in complex vector space. It can effectively infer various relation patterns. 5) ModE: ModE <ref type="bibr" target="#b34">[35]</ref> embeds entities into the polar coordinate system and only uses the modulus part. 6) DistMult: DistMult <ref type="bibr" target="#b35">[36]</ref> measures the probability of a triplet by matching latent semantics in the embedding space. 7) ComplEx: ComplEx <ref type="bibr" target="#b36">[37]</ref> extends DistMult to the complex space and model symmetric and antisymmetric relations. 8) ConvE: ConvE <ref type="bibr" target="#b37">[38]</ref> first proposes the representative multi-layer CNN-based architecture for link prediction. 9) ConvKB: Each triplet in ConvKB <ref type="bibr" target="#b38">[39]</ref> is concatenated as a three-column matrix, and CNN is utilized to extract global relationships between entities and relations. 10) InteractE: InteractE <ref type="bibr" target="#b39">[40]</ref> intends to improve link prediction performance by increasing the number of convolution interactions. 11) R-GCN: R-GCN <ref type="bibr" target="#b40">[41]</ref> is a generalization of a graph convolutional networks for dealing with highly multi-relational data in KGs. 12) SACN: SACN <ref type="bibr" target="#b43">[43]</ref> introduces the weighted graph convolutional networks to take the joint benefit of GCN and ConvE together. 13) HRN: It is a variant of HRAN, which removes the attention mechanism and assigns the same importance to each relation-path. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14) HRAN:</head><p>The proposed heterogeneous graph networks that aggregate features from different semantic aspects and allocate appropriate weights to the relation-path. 4) Implementation Details: For the experiments of HRN and HRAN, the hyperparameters were selected through grid search during training as follows: entity and relation embedding dimension [50, 100, 200, 300], layer depth [1, 2, 3], self-attention value sampled from 0 to 0.9, aggregator function [mean, max, sum], filter number <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr">72]</ref>, filter size [2 ×1, 2 × 3, 2 × 5, 2 × 7], label smoothing [0.1, 0.2, 0.3], batch size [64, 128, 256], learning rate [0.0001, 0.001, 0.005], and dropout rate sampled from 0.1 to 0.5. The following parameters worked well on all data sets: entity and relation embedding dimension of 200, layer depth of 1, self-attention value of 0.5, aggregator function of sum, filter number of 48, label smoothing of 0.1, and batch size of 128. In particular, we set the learning rate at 0.001, the dropout rate at 0.3, and the filter size at 2×5 for WN18. The learning rate is at 0.0001, the dropout rate at 0.4, and the filter size is 2×7 for FB15k-237 and WN18RR. Additionally, the proposed methods were implemented using software library PyTorch <ref type="bibr" target="#b48">[48]</ref> on a PC server equipped with NVIDIA GTX 1080Ti and Intel i7 9700K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 1: Results on Link Prediction 1) Overall Results:</head><p>The link prediction results of MR, MRR, and Hits@k on the WN18RR and FB15k-237 data sets are shown in Table <ref type="table" target="#tab_7">IV</ref>. The best score is in bold and the second-best score is underlined. The Hits@10 and MRR during the training process on FB15k-237 data set are shown in Fig. <ref type="figure" target="#fig_5">3</ref>. The observations can be summarized as follows:</p><p>In a general manner, neural networks-based methods show better performance than translational distance or semantic matching methods. Furthermore, the proposed HRAN generally outperforms other neural networks-based methods and achieves the state-of-the-art performance for general evaluation metrics on the FB15k-237 data set. On the WN18RR data set, it also performs well in four out of five metrics except for Hits@10 in RotatE, which adopts rotation operation in complex space. Specifically, compared with the latest GNN-based method SACN on the WN18RR and FB15k-237 data sets, HRAN achieves 29.3% and 17.5% relative improvement in MR, respectively.</p><p>Then it can find that HRN obtains the second-best performance in 2 out of 5 metrics on two data sets, which is a variant of HRAN. The results demonstrate the superiority of proposed heterogeneous GNNs. Furthermore, HRAN outperforms HRN for general evaluation metrics on two data sets, where the former learns the importance of each relation-path and allocates appropriate weights to them in the GNN framework. In addition, as shown in Fig. <ref type="figure" target="#fig_5">3</ref>, HRAN and HRN are not prone to overfitting and finally achieve the best performance. This result further confirms the expressiveness and robustness of the proposed network architecture.</p><p>The above results prove that the proposed HRAN can generate expressive embeddings for entities and relations then utilized for link prediction tasks. It could also enhance the method effectiveness by incorporating the attention mechanism of the relation-path. The results indicate that it is necessary to focus on some important relation-paths in the heterogeneous graphs.</p><p>2) Detailed Results: The performance of HRAN and HRN on detailed relation categories are investigated. There are complex relations in KGs, which can be classified into 1-to-1, 1-to-N, N-to-1, and N-to-N. For each relation r, the average number of subject entities per object entity spo r and the average number of object entities per subject entity ops r have to be computed. Then, the four categories of relations can be classified as follows:</p><formula xml:id="formula_43">⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ spo r &lt; η and ops r &lt; η ⇒ 1 − to − 1 spo r &lt; η and ops r ≥ η ⇒ 1 − to − N spo r ≥ η and ops r &lt; η ⇒ N − to − 1 spo r ≥ η and ops r ≥ η ⇒ N − to − N (<label>25</label></formula><formula xml:id="formula_44">)</formula><p>where η = 1.5 is set as suggested in TransE <ref type="bibr" target="#b28">[29]</ref>. As a result, 2, 7, 7, 2 relations belong to 1-to-1, 1-to-N, N-to-1, and N-to-N on WN18, respectively. The results of MRR on different relation categories are summarized in Table <ref type="table" target="#tab_8">V</ref>. The best score is in bold and the second-best score is underlined. On the WN18 data set, compared with other methods, HRAN performs well with 1-to-N, N-to-1, and N-to-N relation categories. This condition achieves the state-of-the-art performance for most relation names, especially on N-to-N relations. Among these relations, 71.4% of MRR is greater than or equal to 0.95. Although it removes the attention mechanism compared with HRAN, it can also find that HRN achieves impressive results on some relation names.</p><p>In summary, the proposed methods can effectively deal with most of the complex relations in KGs. The main reason is that the novel score function based on convolutional dynamic neural networks (ConvD) is proposed. ConvD generates relation-specific filters for each relation, which aims to extract relation-specific semantic features in the fact of triplets with different relations. Thus, each entity will show different aspects of the semantic feature under different relationships. It makes the proposed methods more flexible and can deal with these complex relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 2: Effect of Attention Mechanism 1) Attention Value Study:</head><p>As mentioned, the importance of relation-paths can be learned in the end-to-end training process. To verify the ability of the attention mechanism, taking FB15k-237 data set as an example, several relation-paths and corresponding attention values are reported in Fig. <ref type="figure" target="#fig_6">4</ref>. Obviously, a single relation-path and its attention value have a positive correlation. For the relation-paths country and capital_of, higher attention values are given, which means that HRAN considers these two relations as more important in the graph structure. The results indicate that the attention mechanism can reveal the difference between these relation-paths and weights them adequately. Based on the attention values of each relation-path, HRAN can selectively aggregate informative features.</p><p>2) Node Degree Analysis: To further examine the performance of HRAN and HRN in the same graph structure, experiments with different degree scopes are conducted. In KGs, the node degree means the neighbor entities under all relation-paths for each entity. The mean degree of three data sets used in our experiments are shown in Table <ref type="table" target="#tab_4">III</ref>. The entity with a higher degree denotes that it contains more neighbor entities. This type of entity can generally aggregate more complex semantic information from their neighbor entities. As shown in Table VI, the mean results of Hits@3 and Hits@10 are calculated. We can observe that the mean of Hits@3 and Hits@10 is increased as the increase of degree scope. It means that HRAN and HRN can benefit from the aggregation of more neighbor information, then the generated embeddings of entities and relations can be more expressive. Then, compared with HRN, HRAN can achieve better performance even in a low-degree scope. The results prove that the attention mechanism in the framework can enhance the proposed method effectively.</p><p>In sum, the proposed attention mechanism can adaptively learn the weights of each relation-path and selectively aggregate more informative features in some important relationpath. Therefore, even in a low node degree, the generated embeddings of entities and relations can be more expressive then used for link prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment 3: Comparison of Different Aggregator and Score Functions</head><p>In this part, the influence of different aggregator and score functions is evaluated. After obtaining the embedding representation of entities and relations, we have to predict the correct triplet via a score function. In this article, different score functions are adopted including translational distance-based method (TransE), semantic matching-based method (DistMult), and neural network-based methods (ConvE, Conv-TransE, and the proposed ConvD). In our results, several variants of HRAN are denoted as X (Y) + M. The symbol M represents different score functions. And X (Y) denotes the proposed method X with three different aggregator functions Y, which aims to obtain the embeddings of entities and relations. The overall results are shown in Table <ref type="table" target="#tab_9">VII</ref>. The best score is in bold, the second-best score is underlined, and the results achieved in Experiment 1 are highlighted as • .</p><p>First, compared with traditional methods, it can observe that neural network-based score functions receive better performance which is expressive through multi-layers of non-linear feature extraction. Second, based on different score functions, the proposed methods utilized the connectivity structure in KGs, can generate expressive embeddings and obtain a substantial improvement in link prediction tasks. Third, the sum aggregator generally provides better MR and Hits@10 among three aggregator functions. In particular, the HRAN (Sum) + ConvD achieves the state-of-the-art performance on the WN18RR data set. In general, ConvD generates relation-specific filters that can flexibly deal with complex relations in KGs. This condition leads to a substantial improvement in the experiments.</p><p>Overall, the preceding results indicate that the GNN architecture composed of different aggregator functions have a significant influence on the performance of the methods. Thus, it is recommended to evaluate the influence of aggregator function on a per data set basis. Moreover, by incorporating the graph structure information, the proposed methods can generate more expressive embeddings, then utilized for predicting missing links.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Parameter Sensitivity 1) Value of Self-Attention β:</head><p>The self-attention value β determines the weight of self-loops. To investigate the effect of the self-attention value, some experiments on the WN18RR data set are conducted with the value β sampled from 0 to 0.9. The overall results are plotted in Fig. <ref type="figure" target="#fig_7">5</ref>(a). It can find that the proposed methods achieve better performance compared with no self-loops (β = 0). And the best results are obtained when β is equal to 0.5. These observations indicate that the self-loops are important and can improve methods performance effectively.</p><p>2) Depth of Network Layers L: To check the effect of network layers, we explored the performance of proposed methods with the various numbers of depths. The results are shown in Fig. <ref type="figure" target="#fig_7">5(b</ref>). Based on the results, it can find that HRAN and HRN achieve the best performance as the depth equals to 1. And the experimental results lead to be worse as the number of network layers increases. The main reason is that the aggregated features will be over-smooth as the layer of GNNs improves.</p><p>3) Dimension of Embedding d: To explore the significance of different embedding dimensions, several experiments are conducted with the dimensionalities d ∈ [50, 100, 200, 300] on the WN18RR and FB15k-237 data sets. The results of MRR and Hits@10 achieved by HRAN and HRN are shown in Fig. <ref type="figure" target="#fig_8">6</ref>. It can find that outstanding results could be achieved with a dimension size 200 for both data sets. This condition leads to worse performance for larger or smaller embedding dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Case Study</head><p>To intuitively demonstrate the capabilities of HRAN, we conducted a case study of prediction results on the   <ref type="table" target="#tab_10">VIII</ref>, given a specified group of subject entities and relations, the predicted object entities are depicted. The top four prediction scores of object entities are selected as the results. The items presented in Italics indicate the correct prediction of object entities in KGs. It can observe that the proposed method can predict the most correct object entities when dealing with these 1-to-N relations. And these results reflect common-sense predictions even if the correct answer is not always top-ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this article, to capture complex structures and rich semantics in heterogeneous KGs, we have proposed heterogeneous relation attention networks (HRAN). HRAN aggregates neighbor features separately through the relation-paths. Meanwhile, the importance of each relation-path is learned through attention mechanism, which is utilized to selectively aggregate informative features. For the triplet prediction, the ConvD is proposed which generates relation-specific filters. Then, from each entity, relation-specific semantic features can be extracted during the convolution operation. Experimental results on link prediction tasks demonstrate the effectiveness of the proposed methods. Moreover, the sensitivity effect and further analysis of different parameters are investigated. For future work, since sampling useful incorrect training examples is a crucial task, the exploration may be conducted to generate incorrect triplets by utilizing the latest generative adversarial networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustrative example of the KGs and nodes aggregation. (a) KGs are heterogenous which contain various types of nodes (i.e., player, country, club) and relation-paths (i.e., play_for, nationality_of ). (b) Nodes aggregation for KGs should treat each relation-path based neighbors separately and learn the importance of them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. . . , e |E| } and R = {r 1 , r 2 , . . . , r |R| } denote the set of entities and relations, respectively. T ⊆ E × R × E denotes the set of triplets, and a triplet can be expressed as (e s , r , e o ). The bold letters e s , r, e o ∈ R d denote the d-dimension embeddings of e s , r , e o . And E ∈ R |E|×d , R ∈ R |R|×d denote the matrices of entities and relations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of proposed HRAN framework. (a) First, the relation-path based neighbors of entities are aggregated according to each relation-path based adjacency matrix. (b) Then, the aggregating information with learned weight of each relation-path is aggregated by different aggregator functions. (c) Score function is finally utilized to provide a probabilistic prediction of whether the triplet is correct or not. (a) Entity-level aggregation. (b) Relation-level aggregation. (c) Triplet prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>h (l−1) i denotes the neighbor node feature for each relation-path r. These features are summed together and processed by a normalization term |N r (e) |. Here | •| denotes the length of a set. Then the aggregated features h (l−1) N r (e) from relation-path r can be obtained. Given the relation-path set {r 1 , r 2 , . . . , r |R| }, |R| groups of aggregated features for each entity can be obtained as {h (l−1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>For</head><label></label><figDesc>triplet prediction, to deal with multiple relation categories in KGs, a novel score function based on CNN is introduced, called convolutional dynamic neural networks (ConvD). It generates relation-specific filters for each relation-based triplet. Then each subject entity can be extracted relation-specific semantic features in the fact of triplets with different relations, and then used for prediction with object entity. It can effectively improve the method generalization. Thus, ConvD is more flexible and able to deal with different relation categories in KGs. Given an input triplet (e s , r , e o ), we first have to identify the embedding of each entity and relation in the matrices of E and R, respectively. They can be obtained as e s = x e s E, r = x r R, and e o = x e o E (14) where x e s , x r , and x e o denote the high-dimensional one-hot index vectors of e s , r , and e o . The input matrix M in CNN can be obtained as by concatenating the embeddings of subject entity e s and relation r M = CONCAT{e s , r}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Convergence results of HRAN and HRN on FB15k-237 Data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of different relation-paths and corresponding attention value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Parameter sensitivity of (a) value of self-attention, and (b) depth of network layers on WN18RR data set.</figDesc><graphic url="image-13.png" coords="11,179.73,208.11,120.58,100.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Influence of different embedding dimensions on (a) WN18RR, and (b) FB15k-237 data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I NOTATIONS</head><label>I</label><figDesc>AND EXPLANATIONS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1 + exp(−x)) expresses a probabilistically interpretable prediction of whether the triplet (e s , r , e o ) is correct or not. Instead of adopting shared filters in previous methods, ConvD aims to generate relation-specific (dynamic) filters, thereby extracting relation-specific features from the subject entity embedding. The training processes of ConvD are shown in Algorithm 2. Moreover, the score functions of ConvD and several state-of-the-art methods are summarized in Table II.</figDesc><table><row><cell cols="7">Algorithm 2 Convolutional Dynamic Neural Networks for</cell></row><row><cell cols="4">Triplet Prediction</cell><cell></cell><cell></cell></row><row><cell cols="2">Require:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">triplets.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3:</cell><cell cols="6">S batch ← ∅; // Initialize train triplets set.</cell></row><row><cell cols="6">4: for (e s , r, e o ) ∈ T batch do</cell></row><row><cell>5:</cell><cell>for e</cell><cell cols="2">o ∈ E do</cell><cell></cell><cell></cell></row><row><cell>6:</cell><cell cols="2">T ←</cell><cell>e s , r, e</cell><cell>o</cell><cell cols="2">, ∧(e s , r, e</cell><cell>o ) / ∈ T ; // Generate the</cell></row><row><cell></cell><cell cols="6">incorrect triplets set.</cell></row><row><cell>7:</cell><cell cols="2">end for</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8:</cell><cell cols="5">S batch ← S batch ∪</cell><cell>(e s , r, e o ),</cell><cell>e s , r, e</cell><cell>o</cell><cell>; //</cell></row><row><cell></cell><cell cols="6">Generate the train triplets set.</cell></row><row><cell>9:</cell><cell cols="3">e embeddings.</cell><cell></cell><cell></cell></row><row><cell>10:</cell><cell cols="6">ŷ(e s , r, e o ) ← sigmoid(ϕ(e s , r, e o )+b); // Return</cell></row><row><cell></cell><cell cols="6">the correct triplet prediction probability.</cell></row><row><cell>11:</cell><cell cols="2">ŷ(e s , r, e</cell><cell cols="3">o ) ← sigmoid</cell><cell>ϕ(e s , r, e</cell><cell>o )+b</cell><cell>. // Return</cell></row><row><cell></cell><cell cols="6">the incorrect triplet prediction probability.</cell></row><row><cell cols="3">12: end for</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">13: end for</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>KGs G = (E, R, T ); Embeddings matrices E and R; Epoch number N; Batch size b; Score function ϕ(x) . Ensure: Triplet prediction probability ŷ(x) 1: for n = 1, 2, ..., N do 2: T batch ← sample(T , b); // Sample a minibatch s ← e s , r ← r , e o ← e o , e o ← e o ; // Look up the 14: return ŷ(e s , r, e o ), ŷ(e s , r, e o ) E. Training Objective In HRAN, it intends to maximize the likelihood function p(G| ) with the observed KGs G given all method parameters . If the triplet (e s , r , e o ) is correct, its probability Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 09:59:47 UTC from IEEE Xplore. Restrictions apply.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF SCORE FUNCTIONS IN VARIOUS METHODSŷ(e s , r, e o ) is expected to be equal to 1; otherwise, the probability is equal to 0. Thus, the likelihood function is defined as the product of Bernoulli distribution</figDesc><table><row><cell>p(G| ) =</cell><cell>( ŷ(e s , r, e o )) y</cell><cell>1 −</cell><cell>ŷ e s , r, e</cell><cell>o</cell><cell>1−y</cell></row><row><cell cols="2">(e s ,r,e o )∈T</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(e s ,r,e</cell><cell>o )∈T</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>) are obtained by replacing the object (resp. subject) entity with any other entity. Then it checks if the method acquires a big score to x i and a small score to incorrect triplets. The left rank s i and right rank o</figDesc><table><row><cell>TABLE III</cell></row><row><cell>STATISTICS OF THE DATA SETS</cell></row><row><cell>A. General Settings</cell></row><row><cell>1) Evaluation Metrics: Following the filtered setting in</cell></row><row><cell>TransE [29]. For the i-th test triplet x i , all its possible incorrect</cell></row><row><cell>triplets xo i / ∈ T (resp. xs i / ∈ T</cell></row></table><note>i ranks of the i-th test triplet are each associated with corrupting either the subject or object entities under the score function as follows:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF LINK PREDICTION BY MR, MRR, AND HITS@K ON WN18RR AND FB15K-237 DATA SETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V COMPARISON</head><label>V</label><figDesc>RESULTS OF EACH RELATION BY MRR ON WN18 DATA SET</figDesc><table><row><cell>TABLE VI</cell></row><row><cell>NODE DEGREE STUDY USING FB15K-237 DATA SET</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII RESULTS</head><label>VII</label><figDesc>OF LINK PREDICTION BY MR AND HITS@10 ON WN18RR DATA SET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII EXAMPLES</head><label>VIII</label><figDesc>OF LINK PREDICTION ON FB15K-237 DATA SET FB15k-237 data set. As shown in Table</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 09:59:47 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for the constructive comments, which helped improve this article.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported in part by the National Natural Science Foundation of China under Grant 6201101288, Grant 62077020, Grant 62005092, Grant 61875068, and Grant 61505064 and in part by the Fundamental Research Funds for the Central Universities under Grant 2020YBZZ006, Grant CCNU20ZT017, and Grant CCNU2020ZN008.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>His current research interests include representation learning, graph neural networks, and knowledge graphs.</p><p>Dr. Li is a student member of CCF (China Computer Federation).</p><p>Hai Liu (Senior Member, IEEE) received the M.S. degree in applied mathematics and the Ph.D. degree in pattern recognition and artificial intelligence from the Huazhong University of Science and Technology (HUST), Wuhan, China, in 2010 and 2014, respectively.</p><p>Since June 2017, he has been an Assistant Professor with the National Engineering Research Center for E-Learning, Central China Normal University, Wuhan. In 2021, he was selected as the "China-European Commission Talent Programme" under the National Natural Science Foundation of China (NSFC). He is also a Senior Researcher with the UCL Interaction Centre, University College London, London, U.K., where he will be hosted by Prof. Sriram Subramanian, and he will hold the position for one year until February 2022. He has authored more than 70 peer-reviewed articles in international journals from multiple domains. More than six articles are selected as the ESI highly cited articles. His current research interests include deep learning, artificial intelligence, head pose estimation, gaze estimation, and pattern recognition. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring high-order user preference on the knowledge graph for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A knowledge-based recommendation system that includes sentiment analysis and deep learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Ruggiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Informat</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2124" to="2135" />
			<date type="published" when="2019-04">Apr. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fuzzy knowledge-based prediction through weighted rule interpolation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4508" to="4517" />
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A knowledge-based semisupervised hierarchical online topic detection framework</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3307" to="3321" />
			<date type="published" when="2019-09">Sep. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Answering natural language questions by subgraph matching over knowledge graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="837" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding based question answering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th ACM Int. Conf. Web Search Data Mining</title>
				<meeting>12th ACM Int. Conf. Web Search Data Mining</meeting>
		<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD Int. Conf. Manage. Data</title>
				<meeting>ACM SIGMOD Int. Conf. Manage. Data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Yago: A core of semantic knowledge</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. World Wide Web</title>
				<meeting>16th Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pathbased reasoning approach for knowledge graph completion using CNN-BiLSTM with attention mechanism</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jagvaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<date type="published" when="2020-03">Mar. 2020. 112960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A review: Knowledge reasoning over knowledge graph</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<date type="published" when="2020-03">Mar. 2020. 112948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2978386</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021-01">Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of heterogeneous information network analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="37" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
				<meeting>Conf. Empirical Methods Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Learn. Represent</title>
				<meeting>7th Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What can neural networks reason about</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Conf. Learn. Represent</title>
				<meeting>8th Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. Learn. Represent</title>
				<meeting>2nd Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Learn. Represent</title>
				<meeting>5th Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. Learn. Represent</title>
				<meeting>2nd Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th Int. Conf. Mach. Learn</title>
				<meeting>35th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J H</forename><surname>Wu Souza Amauri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C T K Q</forename><surname>Zhang Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th Int. Conf. Mach. Learn</title>
				<meeting>36th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graph neural networks with convolutional ARMA filters</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01343</idno>
		<ptr target="https://arxiv.org/abs/1901.01343" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">KGAT: Knowledge graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
				<meeting>25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge-aware graph neural networks with label smoothness regularization for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
				<meeting>25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Learn. Represent</title>
				<meeting>6th Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th AAAI Conf</title>
				<meeting>28th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th AAAI Conf</title>
				<meeting>29th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Lang. Process</title>
				<meeting>53rd Annu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized translation-based embedding of knowledge graph</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="941" to="951" />
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Learn. Represent</title>
				<meeting>7th Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning hierarchy-aware knowledge graph embeddings for link prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th AAAI Conf</title>
				<meeting>34th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Learn. Represent</title>
				<meeting>3rd Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Int. Conf. Mach. Learn</title>
				<meeting>33rd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional 2D knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd AAAI Conf</title>
				<meeting>32nd AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer</title>
				<meeting>Conf. North Amer</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interacte: Improving convolution-based knowledge graph embeddings by increasing feature interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th AAAI Conf</title>
				<meeting>34th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3009" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Eur. Semantic Web Conf</title>
				<meeting>15th Eur. Semantic Web Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning attentionbased embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 57th Annu. Meeting Assoc. Comput. Linguistics</title>
				<meeting>57th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Authorized licensed use limited to: Tsinghua University</title>
		<imprint/>
	</monogr>
	<note>Downloaded on December 31,2022 at 09:59:47 UTC from IEEE Xplore. Restrictions apply</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end structure-aware convolutional networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd AAAI Conf</title>
				<meeting>33rd AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3060" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Int. Conf. Mach. Learn</title>
				<meeting>32nd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Learn. Represent</title>
				<meeting>3rd Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
