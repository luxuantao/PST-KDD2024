<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Clustering Large and Sparse Co-occurrence Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2003-02-14">February 14, 2003</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqiang</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Clustering Large and Sparse Co-occurrence Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2003-02-14">February 14, 2003</date>
						</imprint>
					</monogr>
					<idno type="MD5">DCC68393EE63A285DF174CE34F158CF7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel approach to clustering co-occurrence data poses it as an optimization problem in information theory -in this framework, an optimal clustering is one which minimizes the loss in mutual information. Recently a divisive clustering algorithm was proposed that monotonically reduces this loss function. In this paper we show that sparse high-dimensional data presents special challenges which can result in the algorithm getting stuck at poor local minima. We propose two solutions to this problem: (a) a prior to overcome infinite relative entropy values as in the supervised Naive Bayes algorithm, and (b) local search to escape local minima. Finally, we combine these solutions to get a powerful algorithm that is computationally efficient. We present detailed experimental results to show that the proposed method is highly effective in clustering document collections and outperforms previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Clustering is a central problem in unsupervised learning <ref type="bibr" target="#b6">[7]</ref>. Presented with a set of data points, clustering algorithms group the data into clusters according to some notion of similarity between data points. However, the choice of similarity measure is a challenge and often an ad hoc measure is chosen. Information Theory comes to the rescue in the important situations where non-negative co-occurrence data is available. A novel formulation poses the clustering problem as one in information theory: find the clustering that minimizes the loss in (mutual) information <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref>. This information-theoretic formulation leads to a "natural" divisive clustering algorithm that uses relative entropy as the measure of similarity and monotonically reduces the loss in mutual information <ref type="bibr" target="#b5">[6]</ref>.</p><p>However, sparse and high-dimensional data presents special challenges and can lead to qualitatively poor local minima. In this paper, we demonstrate these failures and then propose two solutions to overcome these problems. First, we use a prior as in the supervised Naive Bayes algorithm to overcome infinite relative entropy values caused by sparsity. Second, we propose a local search strategy that is highly effective for high-dimensional data. We combine these solutions to get an effective, computationally efficient algorithm. A prime example of high-dimensional co-occurrence data is word-document data; we show that our algorithm returns clusterings that are better than those returned by previously proposed information-theoretic approaches.</p><p>The following is a brief outline of the paper. Section 2 discusses related work while Section 3 presents the information-theoretic framework and divisive clustering algorithm of <ref type="bibr" target="#b5">[6]</ref>. The problems due to sparsity and high-dimensionality are illustrated in Section 4. We present our two-pronged solution to the problem in Section 5 after drawing an analogy to the supervised Naive Bayes algorithm in Section 5.1. Detailed experimental results are given in Section 6. Finally we present our conclusions and ideas for future work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Clustering is a widely studied problem in unsupervised learning, and a good survey of existing methods can be found in <ref type="bibr" target="#b6">[7]</ref>. For the case of co-occurrence data, our information-theoretic framework is similar to the one used in the Information Bottleneck method <ref type="bibr" target="#b17">[18]</ref>, which yields a "soft" clustering of the data. An agglomerative hard clustering version of the method is given in <ref type="bibr" target="#b16">[17]</ref> while methods based on sequential optimization are used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. As we demonstrate in Section 6, our proposed algorithm yields better clusterings than the above approaches, while being more computationally efficient.</p><p>Information-theoretic methods have been used for a variety of tasks in machine learning <ref type="bibr" target="#b13">[14]</ref> including text classification <ref type="bibr" target="#b14">[15]</ref>. Distributional clustering of words was first proposed in <ref type="bibr" target="#b12">[13]</ref> and subsequently used by <ref type="bibr" target="#b0">[1]</ref> for reducing the feature size for text classifiers. A general statistical framework for analyzing co-occurrence data based on probabilistic clustering by mixture models was given in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Divisive Information-Theoretic Clustering</head><p>Let X and Y be two discrete random variables that take values in the sets {x 1 , . . . , x m } and {y 1 , . . . , y n } respectively. Suppose that we know their joint probability distribution p(X, Y ); often this can be estimated using co-occurrence data. A useful measure of the information that X contains about Y (and vice versa) is given by their mutual information, I(X; Y ) <ref type="bibr" target="#b3">[4]</ref>.</p><p>Suppose that we want to cluster Y . Let Ŷ denote the random variable that ranges over the disjoint clusters ŷ1 , . . . , ŷk , i.e., ∪ k i=1 ŷi = {y 1 , . . . , y n }, and ŷi ∩ ŷj = φ, i = j. A novel information-theoretic approach to clustering is to seek that clustering which results in the smallest loss in mutual information <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref>. This loss can be quantified by the following theorem.</p><p>Theorem 1 <ref type="bibr" target="#b5">[6]</ref> The loss in mutual information due to clustering Y is given by</p><formula xml:id="formula_0">I(X; Y ) -I(X; Ŷ ) = k j=1 p(ŷ j )JS p ({p(X|y i ) : y i ∈ ŷj })</formula><p>where p(ŷ j ) = yi∈ŷj p(y i ), p (y i ) = p(y i )/p(ŷ j ) for y i ∈ ŷj , and JS denotes the generalized Jensen-Shannon divergence as defined in <ref type="bibr" target="#b10">[11]</ref>.</p><p>The generalized Jensen-Shannon(JS) divergence used above is a measure of the "distance" between probability distributions. The smaller the JS-divergence the more "cohesive" or "compact" is the cluster. The above theorem implies that the loss in mutual information equals a weighted sum of within-cluster JSdivergences. A proof of Theorem 1 is given in <ref type="bibr" target="#b5">[6]</ref>. By properties of the JS-divergence, the loss in mutual information may be re-written as I(X; Y ) -I(X; Ŷ ) = k j=1 yi∈ŷj p(y i )KL(p(X|y i ), p(X|ŷ j )) and so KL, which denotes the relative entropy or Kullback-Leibler divergence <ref type="bibr" target="#b3">[4]</ref>, emerges as the "natural" distortion measure.</p><p>The divisive clustering algorithm of Figure <ref type="figure">1</ref> provides a way to obtain a sequence of clusterings that monotonically reduces the loss in mutual information given by Theorem 1. The algorithm iteratively (i) repartitions the distributions p(X|y i ) by their closeness in KL-divergence to the cluster distributions p(X|ŷ j ), and (ii) subsequently, given the new clusters, re-computes the optimal cluster distributions. This algorithm is general in that it can be applied to any co-occurrence table. In <ref type="bibr" target="#b5">[6]</ref> the algorithm was shown to converge. It was applied to feature clustering to reduce the model size of the resulting classifiers and was found to outperform previously proposed agglomerative strategies <ref type="bibr" target="#b0">[1]</ref>. The word-class co-occurrence data tackled here was not sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Challenges due to Sparsity and High-Dimensionality</head><p>We now demonstrate how the divisive information-theoretic clustering algorithm can falter due to sparsity and high-dimensionality.</p><p>Algorithm: DITC (p(X, Y ), k, {ŷ j } k j=1 ) Input: p(X, Y ) is the empirical joint probability distribution, k is the number of desired clusters. Output: The set of clusters of ŷ, {ŷ</p><formula xml:id="formula_1">(τ ) j } k j=1 . Method: 1. Initialization: τ ← 0. Choose {p(X|ŷ (τ ) j ) k</formula><p>j=1 to be the conditional distributions p(X|y)'s that are "maximally" far apart. 2. Repeat until convergence 2a.assign cluster: for each yi, find its new cluster index as j * (yi) = arg minj KL(p(X|yi), p(X|ŷ (τ ) j )), 2b.compute cluster distributions: p(ŷ</p><formula xml:id="formula_2">(τ ) j ) = y i ∈ŷ (τ ) j p(yi), p(X|ŷ (τ ) j ) = 1 p(ŷ (τ ) j ) y i ∈ŷ (τ ) j p(yi)p(X|yi).</formula><p>2c.τ ← τ + 1. y 1 y 2 y 3 0.1 0 0 0.9 0.9 0.1 0 0.1 0.9</p><p>. These distributions may be thought of as points on the three-dimensional probability simplex as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Suppose we want to cluster y 1 , y 2 and y 3 into two clusters; clearly the optimal clustering puts {y 1 , y 2 } in one cluster and {y 3 } in the other. However suppose the initial clusters are ŷ1 = {y 1 } and ŷ2 = {y 2 , y 3 }. Then the cluster distributions will be ŷ1 = (0.1, 0.9, 0) and ŷ2 = (0, 0.5, 0.5), respectively (note that we are using ŷi to denote a cluster as well as its distribution p(X|ŷ i )-the particular usage should be clear from context). As shown in Table <ref type="table" target="#tab_0">1</ref>, the Kullback-Leibler divergences KL(y 1 , ŷ2 ), KL(y 2 , ŷ1 ) and KL(y 3 , ŷ1 ) are infinite. Therefore Algorithm DITC of Figure <ref type="figure">1</ref> gets stuck in this initial clustering and the resulting percentage loss in mutual information equals 55.3% assuming p(y i ) = 1 3 , 1 ≤ i ≤ 3 (see Theorem 1). On the other hand, the percentage mutual information lost for the optimal partition, ŷ * 1 = {y 1 , y 2 } and ŷ * 2 = {y 3 }, equals 10.4%, Clearly, the DITC algorithm misses the optimal partition due to the presence of zeros in the cluster distributions that result in infinite KL-divergences.</p><p>Example 2 (High dimensionality) For the second example, we took a collection of 30 documents consisting of 10 documents each from the three distinct classes MEDLINE, CISI and CRAN (see Section 6.1 for more details). These 30 documents contain a total of 1073 words and so the data is very high-dimensional. However, when we run DITC using the word-document co-occurrence data, there is hardly any movement of documents between clusters irrespective of the starting partition. The left confusion matrix (see Section 6.2 for definition) in Table <ref type="table" target="#tab_1">2</ref> shows that a typical clustering returned by Algorithm DITC is quite poor. Note that this set of documents is easy to cluster since the documents come from three very different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proposed Algorithm</head><p>In this section, we propose a computationally efficient algorithm that avoids the above problems. To motivate our first solution, we draw a parallel between our unsupervised method and the supervised Naive Bayes algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Naive Bayes Connection</head><p>Consider a supervised setting where ŷ1 , ..., ŷk correspond to k given document classes and x 1 , ..., x m correspond to the m words/features to be used for classification. The Naive Bayes classifier assumes class conditional word independence and computes the most probable class for test document d as</p><formula xml:id="formula_3">argmax ŷi p(ŷ j ) m t=1 p(x t |ŷ j ) N (xt,d)<label>(1)</label></formula><p>where N (x t , d) is the number of occurrences of word x t in document d <ref type="bibr" target="#b11">[12]</ref>. Taking logarithms in (1), dividing throughout by the length of the document |d| and adding the entropy -</p><formula xml:id="formula_4">m t=1 p(x t |d) log p(x t |d) (where p(x t |d) = N (x t , d)/|d|), the Naive Bayes rule (1) is transformed to argmin ŷj KL(p(X|d), p(X|ŷ j )) - p(ŷ j ) |d| .</formula><p>Note the similarity of this rule to Step 2a of Algorithm DITC (Figure <ref type="figure">1</ref>). The MLE estimate of p(x t |ŷ j ) equals the fraction of occurrences of word x t in class ŷj ; however this implies that if x t does not occur in ŷj then p(x t |ŷ j ) = 0. This is a well known problem with MLE estimates and can lead to infinite KL-divergence values resulting in incorrect classifications. To overcome this problem, it is common to modify the MLE estimate by introducing a "prior", for example, Laplace's rule of succession:</p><formula xml:id="formula_5">p (x t |ŷ j ) = 1 + N (x t , ŷj ) m + m t=1 N (x t , ŷj ) . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>Algorithm: DITC prior (p(X, Y ), k, {ŷ j } k j=1 , α) Input: p(X, Y ) is the empirical joint probability distribution, k is the number of desired clusters, α is the prior. Output: The set of clusters of ŷ, {ŷ</p><formula xml:id="formula_7">(τ ) j } k j=1 . Method: 1. Initialization: τ ← 0. Choose {p(X|ŷ (τ ) j )} k</formula><p>j=1 to be the conditional distributions p(X|y)'s that are "maximally" far apart. 2. Repeat until convergence 2a.assign cluster: for each yi, find its new cluster index as j * (yi) = arg minj KL(p(X|yi), p(X|ŷ </p><formula xml:id="formula_8">(τ ) j )), 2b.compute cluster distributions: p(ŷ (τ ) j ) = y i ∈ŷ (τ ) j p(yi), p(X|ŷ (τ ) j ) = 1 1+α 1 p( ŷ(τ) j ) y i ∈ŷ (τ ) j p(yi)p(X|yi) + αu(X) 2c.τ ← τ + 1, α ← α/2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Using a prior</head><p>As in the supervised Naive Bayes method, we wish to perturb p(x t |ŷ j ) to avoid zero probabilities. Recall that in our unsupervised case p(X|ŷ j ) refers to a cluster distribution. The important question is: what should be the perturbation? For reasons that will become clearer later, we perturb the cluster distribution to:</p><formula xml:id="formula_9">p (X|ŷ j ) = 1 1 + α p(X|ŷ j ) + α • u(X) ,<label>(3)</label></formula><p>where α is a constant and u(X) is the uniform distribution ( 1 m , ..., 1 m ). The value of this prior has a pleasing property: the perturbed cluster distribution p (X|ŷ j ) can be interpreted as the mean distribution for ŷj obtained after perturbing each element of the joint distribution p(x, y) to p (x, y) = 1 1+α p(x, y) + α m p(y) . This can be easily verified: a little algebra reveals that p (y) = x p (x, y) = p(y) (thus the marginals p(y) are unchanged) and p (X|ŷ j ) = 1 p (ŷj )</p><formula xml:id="formula_10">yi∈ŷj p (y i )p (X|y i ) = 1 p(ŷj ) yi∈ŷj p(yi) 1+α (p(X|y i ) + α • u(X)</formula><p>) is seen to give <ref type="bibr" target="#b2">(3)</ref>.</p><p>Another notable observation about our prior is that if α is taken to equal m t N (wt,ŷj ) in (3), then we get Laplace's rule of succession (2) that is used in the supervised Naive Bayes method. What should be the value of α in our clustering algorithm? Experimental results reveal that an "annealing" approach helps, i.e., start the algorithm with a large value of α and decrease α progressively as the number of iterations increase. Algorithm DITC prior in Figure <ref type="figure" target="#fig_1">3</ref> summarizes our method. Note that our approach is different from the deterministic annealing approach of <ref type="bibr" target="#b13">[14]</ref>, which performs a "soft" clustering at every step and needs to identify cluster splits as a "temperature" parameter is decreased.</p><p>Algorithm DITC was shown to converge in <ref type="bibr" target="#b5">[6]</ref> and so the convergence of Algorithm DITC prior follows since α → 0 as the iteration count increases. This is another reason to halve α with every iteration. Back to our Example 1, with α set to 1, it can be shown that Algorithm DITC prior is able to generate the optimal partition. Our use of priors also leads to better document clustering than Algorithm DITC (detailed results are in Section 6). However, problems due to high-dimensionality as in Example 2 are not completely cured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Local Search</head><p>To further improve our algorithm, we now turn to a local search strategy that allows us to escape undesirable local minimum, especially in the case of high-dimensionality. Our local search principle, called first variation in <ref type="bibr" target="#b4">[5]</ref>, refines a given clustering by incrementally moving a distribution from one cluster to another in order to achieve a better objective function value. Precisely, a first variation of a partition {ŷ j } k j=1 is a partition {ŷ j } k j=1 obtained by removing a distribution y from a cluster ŷi of and assigning it to an existing cluster ŷl . Among all the kn possible first variations, corresponding to each combination of y and ŷl , we denote the one that gives the smallest loss in mutual information as nextFV {ŷ j } k j=1 . We now present the details  </p><formula xml:id="formula_11">y i ∈ ŷ- j ) -p(ŷ j )JS p ({p(X|y i ) : y i ∈ ŷj ) +p(ŷ + l )JS p ({p(X|y i ) : y i ∈ ŷ+ l ) -p(ŷ l )JS p ({p(X|y i ) : y i ∈ ŷl ),</formula><p>where ŷj = ŷj -{y} and ŷ+ l = ŷl ∪ {y}. From <ref type="bibr" target="#b10">[11]</ref>, JS π ({p i :</p><formula xml:id="formula_12">1 ≤ i ≤ n}) = H ( i π i p i ) -i π i H(p i )</formula><p>, where H(p i ) is the entropy of p i . Hence, we can expand the above JS-divergences to get</p><formula xml:id="formula_13">δ = p(ŷ - j )H(ŷ - j ) - yi∈ŷ - j p(y i )H(y i ) -p(ŷ j )H(ŷ j ) - yi∈ŷj p(y i )H(y i ) +p(ŷ + l )H(ŷ + l ) - yi∈ŷ + l p(y i )H(y i ) -p(ŷ l )H(ŷ l ) - yi∈ŷ + l p(y i )H(y i ) = p(ŷ - j )H(ŷ - j ) -p(ŷ j )H(ŷ j ) + p(ŷ + l )H(ŷ + l ) -p(ŷ l )H(ŷ l ). (<label>4</label></formula><formula xml:id="formula_14">)</formula><p>Note that in the above, for brevity we have used y and ŷ to also denote the distributions p(X|y) and p(X|ŷ) respectively. The above analysis shows that δ can be computed in time that is linear in the dimensionality m. Equation ( <ref type="formula" target="#formula_13">4</ref>) suggests that δ is expected to be large when changing cluster affiliation leads to a substantial change in the cluster distributions ŷj and ŷ+ l ; this happens exactly in the cases where DITC prior is seen to fail, i.e., when data is high-dimensional and clusters are small in size.</p><p>So far we have only considered moves that reduce loss in mutual information. An enhancement is to allow a chain of first variation moves that may lead to temporary increases. The algorithm searches for a sequence of successive moves that result in the largest possible reduction in the loss of mutual information (this enhancement was inspired by the successful graph partitioning heuristic of <ref type="bibr" target="#b8">[9]</ref>). Algorithm DITC LocalSearch in Figure <ref type="figure" target="#fig_3">4</ref> gives an outline of our proposed strategy. Note that the algorithm maintains a set of unmarked y's that correspond to the distributions not moved.</p><p>Finally, Figure <ref type="figure" target="#fig_4">5</ref> presents our algorithm that incorporates both the ideas of priors and local search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We now present experimental results when we apply our information-theoretic algorithm to the task of clustering document collections using word-document co-occurrence data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data sets</head><p>For our test data, we use various subsets of the 20-newsgroup data (NG20) <ref type="bibr" target="#b9">[10]</ref> and the smart collection (ftp://ftp.cs.cornell.edu/pub/smart). NG20 consists of approximately 20,000 newsgroup postings collected from 20 different usenet newsgroups. We report results on NG20 and various subsets of this data set of size 500 each: Binary (talk.politics.mideast, talk.politics.misc; 250 documents from each category), Multi5 (comp.graphics, rec.motorcycles, rec.sport.baseball, sci.space, talk.politics.mideast; 100 from each) and Multi10 (alt.atheism, comp.sys.mac.hardware, misc.forsale, rec.autos, rec.sport.hocky, sci.crypt, sci.electronics, sci.med, sci.space, talk.politics.gun; 50 from each). By merging the five "comp", the three "religion", the three "politics", the two "sport" and the two "transportation" categories, we generated ten meta-categories in this corpus, which we call NG10. In order for our results to be comparable, we applied the same preprocessing as in <ref type="bibr" target="#b15">[16]</ref> to all the news group data sets, i.e. we removed stopwords and selected the 2000 words with the highest contribution to the mutual information, removed documents with less than 10 word occurrences and removed all the headers except the subject line.</p><p>From smart, we used medline, cisi, and cranfield subcollections. medline consists of 1033 medical journals abstracts, cisi consists of 1460 abstracts from information retrieval papers, while cranfield consists of 1400 aerodynamical systems abstracts. We also created 3 subsets of 30, 150, and 300 documents respectively; each data set was created by equal sampling of the three collections. After removing stopwords, the number of words for the 30, 150 and 300 document data sets is 1073, 3658 and 5577 respectively. We will refer to the entire data set as Classic3 and the subsets as C30, C150 and C300 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation measures</head><p>Since we know the underlying class labels for our data sets, we can evaluate clustering results by forming a confusion matrix where entry(i, j) gives the number of documents in cluster i that belong to the true class j. For an objective evaluation measure, we use micro-averaged precision which was used in <ref type="bibr" target="#b15">[16]</ref>. The idea is to first associate each cluster with the most dominant class label in that cluster. Then for each class c ∈ C, we define α(c, ŷ) to be the number of points correctly assigned to c, β(c, ŷ) to be the number of documents incorrectly assigned to c and γ(c, ŷ) to be the number of documents incorrectly not assigned to c. The micro-averaged precision is</p><formula xml:id="formula_15">P (ŷ) = c α(c, ŷ) c α(c, ŷ) + β(c, ŷ) and the micro-averaged recall is R(ŷ) = c α(c, ŷ) c α(c, ŷ) + γ(c, ŷ)</formula><p>Note that for uni-labeled data, P (ŷ) = R(ŷ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results and Analysis</head><p>We first show that Algorithm DITC PLS (with prior and local search) is superior to Algorithms DITC prior and DITC LocalSearch. Note that all our algorithms are deterministic since we choose initial cluster distributions that are "maximally" far apart from each other <ref type="bibr" target="#b2">[3]</ref>.</p><p>Algorithm DITC prior cures the problem of sparsity to some extent and its results are superior to DITC, for example, Table <ref type="table" target="#tab_3">3</ref> shows the confusion matrices resulting from the two algorithms. An interesting option in DITC prior is the starting value of α. Indeed, as Figure <ref type="figure">6</ref> shows, the starting values of α can result in quite different values of mutual information preserved and micro-averaged precision. The trend seems to be that larger starting values of α lead to better results (we observe this trend over other data sets too). This is interesting in itself since larger α values correspond to starting with "smeared" cluster distributions, or in other words, with high joint entropy values H(X, Ŷ ). This phenomena needs to be further studied and we feel it might have some relationship to the temperature parameter in deterministic annealing <ref type="bibr" target="#b13">[14]</ref>.</p><p>However, the starting values of α cease to be an issue when we use DITC PLS, which is seen to be "immune" to different starting values in Figure <ref type="figure">6</ref>. Note that these figures also validate our optimization criterion: there is a definite correlation between the mutual information preserved and micro-averaged precision, which was also observed in <ref type="bibr" target="#b15">[16]</ref>. Thus DITC PLS is seen to be more stable than DITC prior in addition to yielding higher quality results. Tables <ref type="table">4</ref> and<ref type="table">5</ref> further show that DITC LocalSearch also yields better clustering than DITC prior. However, DITC PLS is much more computationally efficient than DITC LocalSearch since it yields better starting partitions before invoking the slower local search procedure; hence DITC PLS is our method of choice.</p><p>We now compare our Algorithm DITC PLS with previously proposed information-theoretic algorithms. <ref type="bibr" target="#b16">[17]</ref> proposed the use of an agglomerative algorithm that first clusters words, and then uses this clustered feature space to cluster documents using the same agglomerative information bottleneck method. More recently <ref type="bibr" target="#b15">[16]</ref> improved their clustering results by using sequential information bottleneck (sIB). We implemented the sIB algorithm for purpose of comparison; since the sIB method starts with a random partition we ran 10 trials and report the performance numbers with error bars (standard deviation) in Figures <ref type="figure">7</ref> and<ref type="figure" target="#fig_6">8</ref>, which also contain performance results for our algorithms (recall that our algorithm are deterministic). Figures <ref type="figure">7</ref> and<ref type="figure" target="#fig_6">8</ref> again reveal the correlation between the preserved mutual information and micro-averaged precision. DITC PLS is seen to be the best performing algorithm, and beats sIB on at least 3 of the data sets; for example, the average micro-averaged precision of sIB on Multi5 is .8 while DITC PLS yields .95. Note that numbers for our sIB implementation are averages of 10 runs while the published numbers in <ref type="bibr" target="#b15">[16]</ref> are the best among 15 restarts (they did not give error bars). Also, the Binary, Multi10 and Multi5 datasets are formed by a sampling of the newsgroups, so our data sets are a bit different from theirs. Note that the NG10 and NG20 data sets used by us and <ref type="bibr" target="#b15">[16]</ref> are identical, and so are the micro-averaged precision values (in this case they do give averaged results over 10 runs, see <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Table 2]</ref>.</p><p>For large data sets, DITC prior gives results that are comparable to those with prior and local search; this leads to big savings in time since DITC prior is much faster than sIB as shown in Table <ref type="table" target="#tab_4">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and future work</head><p>In this paper, we have proposed the use of priors and local search to yield a computationally efficient clustering algorithm that directly optimizes the preservation of mutual information. We draw an analogy between our unsupervised method and the supervised Naive Bayes algorithm. In future work we would like to investigate the relationship between our choice of α and the temperature schedule in deterministic annealing <ref type="bibr" target="#b13">[14]</ref>. At one   level our algorithm differs from deterministic annealing since the latter does "soft clustering", however our large initial values of α at earlier iterations correspond to large entropy values H(X; Ŷ ), so we suspect there might be a theoretical connection. The information theoretic framework is general and we intend to apply it to more interesting problems, such as clustering the states of a Markov chain by analyzing its probability transition matrix. We also believe that this information-theoretic framework is "natural" for applying MDL techniques for selecting the "right" number of clusters. Further, we intend to extend our algorithm to cocluster or simultaneously cluster both X and Y so that the loss in mutual information I(X; Y ) -I( X; Ŷ ) is minimized.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Divisive information-theoretic clustering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Adding priors to Algorithm DITC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm: 2 .</head><label>2</label><figDesc>DITC LocalSearch (p(X, Y ), k, {ŷ j } k j=1 , f ) Input: p(X, Y ) is the empirical joint probability distribution, k is the number of desired clusters, f is first variation chain length. Output: The set of clusters of ŷ, {ŷ Repeat until convergence. l ← 0, U ← ŷ, do the following f times: , U . Mark the moved distribution and delete it from U. 2b.Record the change of loss in mutual information in ObjChange[l]. l ← l + 1. 2c.MaxChange ← maxi i l=0 ObjChange[l] MaxI ← arg maxi i l=0 ObjChange[l], i = 0, 1..., f -1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Algorithm DITC enhanced by local search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Divisive information theoretic clustering with prior and local search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Mutual information preserved and Micro-averaged precision returned by DITC prior with various starting α-values on Multi10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Fraction of mutual information preserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The KL-divergences from y 1 , y 2 , y 3 to ŷ1 , ŷ2</figDesc><table><row><cell>ŷ1</cell><cell>ŷ2</cell></row><row><cell cols="2">y 1 0 y 2 ∞ 0.531 ∞ y 3 ∞ 0.531</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Typical confusion matrix returned by DITC on a small document collection.</figDesc><table><row><cell></cell><cell cols="3">MED CRAN CISI</cell></row><row><cell>ŷ1</cell><cell>3</cell><cell>0</cell><cell>4</cell></row><row><cell>ŷ2</cell><cell>7</cell><cell>1</cell><cell>4</cell></row><row><cell>ŷ3</cell><cell>0</cell><cell>9</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>DITC PLS (p(X, Y ), k, {ŷ j } k j=1 , α, f ) Input: p(X, Y ) is the empirical joint probability distribution, k is the number of desired clusters, α is the prior, f is LS chain length. Output: The set of clusters of ŷ, {ŷ</figDesc><table><row><cell>Method:</cell><cell cols="2">(τ ) j } k j=1 .</cell></row><row><cell cols="2">1. Initialization: τ ← 0. Choose {p(X|ŷ far apart.</cell><cell>(τ ) j )} k j=1 to be the conditional distributions p(X|y)'s that are "maximally"</cell></row><row><cell>2. Repeat until convergence</cell><cell></cell></row><row><cell cols="3">2a.Run DITC prior (p(X, Y ), k, {ŷ τ j } k j=1 , α) 2c.Run DITC LocalSearch (p(X, Y ), k, {ŷ τ j } k j=1 , f )</cell></row></table><note><p>Algorithm:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Confusion matrices for 3893 documents.</figDesc><table><row><cell cols="2">ŷ1 847</cell><cell>41</cell><cell>275</cell><cell>1016</cell><cell>1</cell><cell>2</cell></row><row><cell cols="3">ŷ2 142 954</cell><cell>86</cell><cell>1</cell><cell>1389</cell><cell>1</cell></row><row><cell>ŷ3</cell><cell>44</cell><cell cols="2">405 1099</cell><cell>16</cell><cell>9</cell><cell>1457</cell></row><row><cell></cell><cell cols="3">DITC partition</cell><cell cols="3">DITC prior partition</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Computation time (in seconds) on large data sets (≥ 3000).</figDesc><table><row><cell>Data set</cell><cell cols="3">sIB DITC DITC prior</cell></row><row><cell>Classic3</cell><cell>94.92</cell><cell>1.35</cell><cell>1.67</cell></row><row><cell>NG20</cell><cell>6244</cell><cell>35.87</cell><cell>29.92</cell></row><row><cell>NG10</cell><cell>2459</cell><cell>16.71</cell><cell>14.75</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research was supported by NSF CAREER Award No. ACI-0093404.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributional clustering of words for text classification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning simple relations: Theory and applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Berkhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Becher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd SIAM Int&apos;l Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="420" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scaling clustering algorithms to large databases</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;03</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative clustering of high dimensional text data augmented by local search</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2002 IEEE International Conference on Data Mining</title>
		<meeting>The 2002 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A divisive information-theoretic feature clustering algorithm for text classification</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research(JMLR): Special Issue on Variable and Feature Selection</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1265" to="1287" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical models for co-occurrence and histogram data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="192" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An efficient heuristic procedure for partitioning graphs. The Bell System Technical</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Kernighan</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="307" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">News Weeder: Learning to filter netnews</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int&apos;l Conf. Machine Learning</title>
		<meeting>12th Int&apos;l Conf. Machine Learning<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Divergence measures based on the Shannon entropy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributional clustering of English words</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Annual Meeting of the ACL</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deterministic annealing for clustering, compression, classification, regression, and related optimization problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2210" to="2239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Using Machine Learning to Improve Information Access</title>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>CS Department, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised document classification using sequential information maximization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Document clustering using word clusters via the information bottleneck method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing</title>
		<meeting>of the 37-th Annual Allerton Conference on Communication, Control and Computing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
