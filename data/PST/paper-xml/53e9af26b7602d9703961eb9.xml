<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fuzzy Basis Functions: Comparisons with Other Basis Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyun</forename><forename type="middle">Mun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Signal and Image Processing Institute</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering Systems</orgName>
								<orgName type="institution">University of Southem Califomia</orgName>
								<address>
									<postCode>90089-2564</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, ZEEE</roleName><forename type="first">Jerry</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Signal and Image Processing Institute</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering Systems</orgName>
								<orgName type="institution">University of Southem Califomia</orgName>
								<address>
									<postCode>90089-2564</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fuzzy Basis Functions: Comparisons with Other Basis Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5FB7888560D5EC60605C0FF4A6B6320F</idno>
					<note type="submission">received March 24. 1993: revised SeDtember 15.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fuzzy basis functions (FBF's) which have the capability of combining both numerical data and linguistic information, are compared with other basis functions. Because a FBF network is different from other networks in that it is the only one that can combine numerical and linguistic information, comparisons are made when only numerical data is available. In particular, a FBF network is compared with a radial basis function (RBF) network from the viewpoint of function approximation. Their architectural interrelationships are discussed. Additionally, a RBF network, which is implemented using a regularization technique, is compared with a FBF network from the viewpoint of overcoming ill-posed problems. A FBF network is also compared with Specht's Probabilistic Neural Network and his General Regression Neural Network (GR") from an architectural point of view. This is motivated by the similarities of the FBF and G R " formulas. Then, a FBF network is compared with a Gaussian sum approximation in which Gaussian functions play a central role. For comparison, we simulated a simple function approximation problem to compare a FBF network with a RBF network. Finally, we summarize the architectural relationships between all the networks discussed in this paper, and compare the different approximations from the point of view of the assumptions made about the available data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION ECENTLY, Wang and Mendel</head> <ref type="bibr" target="#b27">[34]</ref> <p>introduced fuzzy ba-R sis functions (FBF's) which have the capability of combining both numerical data and linguistic information. These basis functions are quite general. Their exact mathematical structure depends on four choices that one must make for any fuzzy logic system, namely, type of fuzzification, membership function, inference mechanism, and defuzzification strategy. Wang and Mendel frequently choose singleton fuzzification, Gaussian membership functions, product inference, and height method of defuzzification, in which case their FBF network can be summarized by the following mathematical </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , N</head><p>are the points in the output space R determined by the fuzzy rule base <ref type="bibr" target="#b27">[34]</ref> both ??'s are the centers of consequent membership functions. We can represent (1) as a sum of two functions</p><p>(2)</p><p>where f ~( x ) consists of basis functions which deal with numerical data and f~(x) consists of basis functions which deal with linguistic information. Note that in (I), the Z1 coefficients of f ~( x ) can be obtained from the given fuzzy rule base directly, whereas the Z1 coefficients of f ~( x ) can be obtained using learning rules such as least squares, least mean-squares, or back-propagation <ref type="bibr" target="#b29">[36]</ref>.</p><p>To people familiar with aspects of approximation theory for deterministic data, the formulas for either f ~( x )</p><p>or f~(x) look familiar. The radial symmetry of the Gaussian membership functions causes us to wonder whether (1) is just a Gaussian radial basis function expansion (others have suggested to us privately that they are the same). We show below that it is not; instead, it is a nonlinear function of Gaussian radial basis functions. Interestingly enough, Gaussian radial basis functions are themselves special cases of Generalized radial basis functions [8] and hyper-basis functions <ref type="bibr">[8]</ref>. We also explore the relations between these more general radial basis functions and FBF's below. There are other literatures in which Gaussian functions play a central role in approximations; hence, we are motivated to explore the relationship between (1) and results from these approximation problems. We were intrigued by the similarity between the formula for Specht's generalized regression neural network (GR") <ref type="bibr">[30]</ref> and (l), and wondered again whether (1) was just another G R " . Below we explain why it is not (see, also, Wang <ref type="bibr" target="#b29">[36]</ref>). We also examine the relationship between FBF networks and Specht's probabilistic neural network (PNN) <ref type="bibr" target="#b24">[31]</ref>, and FBF networks and Gaussian sum approximations. The latter are used in nonlinear filtering. An important difference between these approximation problems and the ones described in the previous paragraph is that data, including the quantities being estimated by a G R " , PNN or</p><formula xml:id="formula_0">f ( x ) = fiL'(Z) + f L b )</formula><p>10634706/95$04.00 0 1995 IEEE a Gaussian sum approximation, are assumed from the very beginning to be random. No such modeling assumption is made or needed for FBF networks.</p><p>Our comparisons between FBF's and the different types of radial basis functions, or the other functions just described, is only valid for the special case when no linguistic information is used by the FBF network (i.e., f ~( z ) = 0). We make this important point here, so that it is at once clear to the reader that, in general, a FBF network is indeed different from all of these other networks, because it is the only one that can combine numerical and linguistic information (see, also, Wang <ref type="bibr" target="#b29">[36]</ref>). Consequently, the rest of this paper treats the special case for a FBF network when only numerical data is available (i.e., f ~( z ) = O), in which case it is indeed legitimate to question whether or not the FBF as described by (1) is new.</p><p>Kosko [ 111 and[ 121 has developed the additive fuzzy system which has the following form for correlation product encoding or output scaling</p><formula xml:id="formula_1">m j = 1</formula><p>where Vj is the volume (area) of jth consequent fuzzy set and c j is the centroid of it. Note that (3) reduces to (1) when the volumes (areas) Vj in (3) are identical and when aj factors into n Gaussian sets.</p><p>In Section 11, FBF's are compared with a radial basis function (RBF) network which is widely used for interpolation. Because interpolation is a subset of approximation, comparisons are made from the viewpoint of function approximation. Additionally, since the problem of learning a smooth mapping from examples is ill-posed, in the sense that information in the data is not sufficient to uniquely reconstruct the mapping in regions where the data are not available, regularization techniques are compared which change the ill-posed problems into well-posed problems. Finally, the constraints of a regularization technique are interpreted from the viewpoint of linguistic rules.</p><p>In Section 111, a FBF network is compared with Specht's PNN and his G R " . Because the PNN and G R " are based on Parzen's window, which exploits random data, our comparisons are made from an architectural point of view (since the assumptions about the available data are different, it is reasonable to make comparisons based only on network structures).</p><p>In Section IV, a FBF network is compared with a Gaussian sum approximation that was developed as early as 1971 to cope with Kalman filtering for nonGaussian systems.</p><p>In Section V, we present a simulation of a simple function approximation problem to compare a FBF network with a RBF network.</p><p>Finally, in Section VI, we present figures which summarize the architectural relationships between the networks discussed in this paper, and the interrelationships of all the networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fuzzy Logic System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fuzzy Rule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. COMPARISON BETWEEN FBF'S AND RBF'S A. FBF Network</head><p>The basic configuration of a fuzzy logic system (FLS) is shown in Fig. <ref type="figure" target="#fig_1">1</ref>. There are four principal components in a fuzzy logic system: 1) fuzzy rule base which comprises fuzzy rules describing how the fuzzy system performs; 2) fuzzy inference engine which uses the rules in the fuzzy rule base to determine a mapping based on fuzzy logic operations; 3) fuzzifier which maps crisp points in the input space into fuzzy sets in the input space; and, 4) defuzzifier which maps fuzzy sets in the output space into crisp points in the output space. Within each component, there are many different choices that can be made, and many combinations of these choices result in different fuzzy logic systems. It would, therefore, be very cumbersome to compare every case; hence, in this study, we consider only one general case, namely, the fuzzy logic system with: singleton fuzzifier, height method defuzzifier, product inference, and Gaussian membership functions <ref type="bibr" target="#b29">[36]</ref>, given in (1). We have chosen this case so that the resulting FLS can be compared with seemingly similar systems that also use Gaussian or radially-symmetric functions.</p><p>Our FLS can be represented as the FBF expansion we fix all the parameters in pj(s) at the very beginning of the FBF expansion design procedure, so that the only free design parameters are O j , then f(s) in (4) is linear in the design parameters. The FBF network can then be regarded as a special case of the linear regression model,</p><formula xml:id="formula_2">M j = 1</formula><p>where d ( t ) is the desired output, dj are the unknown parameters, p j (t) are regressors, which are some fixed functions of z ( t ) , and, E ( t ) is an error signal which is assumed to be orthogonal to the regressors, By providing input and desired output pairs, the values of the B j ' s can be determined using, for example, least squares. Note, also, that in general the parameters of the FBF's need not be fixed ahead of time. They to guarantee good results. One of the solutions to this problem is the regularization technique which exploits smoothness constraints [8], <ref type="bibr" target="#b6">[22]</ref>, and <ref type="bibr" target="#b7">[23]</ref>. It consists of replacing the matrix by CP + a I , where I is the identity matrix, and cr is a small parameter whose magnitude is proportional to the amount of noise in the data points. The coefficients of the RBF network are then given by A = (CP + a l ) -' y .</p><p>(13) can be optimized along with the dj's using a back-propagation procedure <ref type="bibr" target="#b28">[35]</ref>.</p><p>Note that the original interpolation is recovered by letting (Y go to zero. Interpolation is the limit of approximation when there is no noise in the data. It is proved in <ref type="bibr" target="#b6">[22]</ref> that, for networks derived from regularization, and in particular for radial basis function networks, a best approximation exists which guarantees that the approximation problem has a unique solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RBF Network</head><p>The RBF network is one of the possible solutions to the real multivariate interpolation problem, that Can be stated as follows: given N different Points {zi E R", = 1, . . . , and N real numbers {yi E RJi = 1, . . . , N}, find a function F from R" to R satisfying the interpolation conditions ' , N .</p><p>(7) C. <ref type="bibr">Comparisons</ref> The RBF approach consists Of choosing from a linear If we choose the parameters in (1) as gf = a for all space of dimension N that depends on the data points {zi}. i = 1 , 2 , . . . , n and 1 = 1 , 2 , . . . , M , with f L ( z ) = 0, then</p><p>The basis of this space is chosen to be the set of functions  where z E R" is the input vector; and the Xi's ( 1 5 i 5 N ) are parameters. When the given sample values are presumed to be accurate, and it is required to perform a smooth interpolation between sample points, (8) can be solved by imposing the interpolation conditions F ( z j ) = y j , j = 1 , . . . , N . The solution is A = F l y (9)</p><formula xml:id="formula_3">. _ 1 gzl exp [-(z -z1lT(z -zl) 1 pxp [ -( z -z 1 ) W 1 $ ( T ) = exp (-r2/2o2), T &gt; 0 p e x p [-2a2 1 . p [-2$ {$(l/z-zill),i = l , . . . ,</formula><formula xml:id="formula_4">f(z) = M $ ( T ) = T , T &gt; 0 (linear approximation), 2a2 $ ( T ) = r3, T &gt; 0 (cubicapproximation), 1 1 5 -21112 1 1 5 -21112 $ ( T ) = r2 log T , T &gt; 0 (thin platesplines),<label>(14)</label></formula><formula xml:id="formula_5">(10) = d1l . i -4 0 (11) Y N x l [ ! l l , ' " &gt; y N ] * . (<label>12</label></formula><formula xml:id="formula_6">)</formula><p>where</p><formula xml:id="formula_7">A N x l = [ X l , ' ' ' , ANIT</formula><p>and Some analysis regarding the singularity of (9) is made by Micchelli [ 191.</p><p>If the data are subject to measurement errors or stochastic variations, a strict interpolation is meaningless. Consequently, the interpolation property of the RBF network is not sufficient C$(ll. -z"l)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1=1</head><p>Comparing (15) with (8) from an architectural point of view, we see that, whereas the RBF network is a linear combination of radial basis functions, the FBF network is a nonlinear combination of radial basis functions; hence, a RBF network can be used as a FBF network with the addition of lateral connections between the REiF's, something suggested in <ref type="bibr" target="#b4">[20]</ref>. Due to the height method of defuzzification operation of a FBF network, which leads to the denominator of (15), it is impossible to classify a FBF as a RBF.</p><p>After determining the network structure, it is natural to ask how to determine the coefficients of (8) or (15). The computation of the coefficients of the RBF network becomes a very time consuming job as N becomes large. To overcome this, several methods have been proposed. Chen [7] uses an orthogonal least squares (OLS) algorithm to select a subset of significant basis functions from a given set of basis functions. Poggio and Girosi [8] proposed a generalized RBF (GRBF) network which has movable centers that do not necessarily coincide with some of the data points z;. They applied the regularization technique to the approximation problem.</p><p>It consists of looking for the function f that minimizes the functional where P is a constraint operator, 11 . 112 is a norm on the function space to which f belongs and Q is a regularization parameter. The structure of the operator P embodies the a priori knowledge about the solution. Poggio and Girosi [8] also extended the GRBF to a hyper-basis function (HyperBF) network by choosing a different smoothing parameter for each basis function (e.g., a different o for each Gaussian RBF). The main idea is to consider the mapping to be approximated by the sum of several functions, each one with its own prior, that deals with different constraints to stabilize a system. Consequently a FBF network with different o's becomes a nonlinear combination of HyperBF's. In other words, a HyperBF network can also be used as a FBF network with the addition of lateral connections between the HyperBF's.</p><p>From the point of view of learning as approximation, the problem of learning a smooth mapping from examples is ill-posed in the sense that the information in the data is not sufficient to uniquely reconstruct the mapping in regions where the data are not available. A priori assumptions about the mapping are needed to make the problem well-posed. In particular, the mapping may be smooth, which is one of the most general constraints. We may regard this constraint as a linguistic rule (although it is not an IF-THEN rule). Poggio and Girosi therefore absorb linguistic rules using a constraint operator, whereas a FBF network absorbs linguistic rules directly into its basis functions.</p><p>The most important advantage of the FBF network over other networks is that linguistic IF-THEN rules can be translated into FBF's to make the FBF network an universal approximator. To cope with an ill-posed problem, the number of training data must be large enough to excite all the modes of the system. FBF's can incorporate some linguistic IF-THEN rules which play the role of unobserved modes. In <ref type="bibr" target="#b30">[37]</ref>, fuzzy adaptive filters were developed with and without linguistic rules. By incorporating some linguistic rules into the fuzzy adaptive filters, the adaptation speed was greatly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">COMPARISONS BETWEEN FBF NETWORKS AND NETWORKS FOR WHICH THE DATA ARE ASSUMED TO BE RANDOM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Parzen's Estimate of a Probability Density Function</head><p>Central to nonlinear estimation and stochastic control problems is the determination of the probability density function of the state conditioned on the available measurement data. If this a posteriori density function is known, then an estimate of the state for any performance criterion can be determined. Parzen <ref type="bibr" target="#b5">[21]</ref> showed how one may construct a family of estimates, f,(z), of a probability density function (PDF) f(z), as <ref type="bibr" target="#b1">(17)</ref> which is consistent at all points z at which the PDF is continuous. Let X1, . . . , X, be independent random variables identically distributed as a random variable X whose distribution function F ( x ) = P[X 5 x] is absolutely continuous. (</p><formula xml:id="formula_8">)<label>24</label></formula><p>Cacoullos [6] has extended Parzen's results to cover the multivariate case. In the particular case of the Gaussian kemel, the multivariate estimates can be expressed as <ref type="bibr" target="#b9">(25)</ref> where i = pattern number, m = total number of training patterns, Xi = ith training pattern, a = smoothing parameter, and p = dimensionality of measurement space, i.e., dim (2).</p><p>Observe that fm(z) looks like (8) in which all the Xi's are the same; hence, in retrospect, Parzen's PDF, fm(z) can now be called a RBF network. Except for a new name, Parzen's important result remains unchanged. Note, though, that in Parzen's work signal x is random, whereas RBF's do not assume the data is random.</p><p>Although Parzen proved the existence of a consistent estimate in mean-square, he did not indicate how to choose the weighting function on the basis of a finite set of data. Several methods have been proposed for the practical use of Parzen's method. Breiman et al. <ref type="bibr">[4]</ref> suggested that even better density estimates could be obtained using Parzen windows and finite data sets if a different o is used for each exemplar (data point). Their suggestion stems from the observations that Parzen's method can not respond appropriately to variations in the PDF (i.e., there should be a distinction between low density regions and high density regions); and, none of the asymptotic results give any helpful leads on how the shape factor a should be selected to give the best estimate of the unknown density. In other words, the rate of convergence depends critically on the density and its derivatives. To make the sharpness of the kernel data-responsive, they proposed the class of variable kernel estimates where d;,k is the distance from the point xi to its kth nearest neighbor, a!k is a constant multiplicative factor, m is the dimension of z and K is a selected kernel. Observe that in low density regions, d i , k will be large and the kernel will be spread out. This method can be regarded as an extended version of the lcth nearest neighbor estimator [15] which is adaptive to local sample density, but is discontinuous. The variable kernel approach offers a combination of the desirable smoothness properties of the Parzen-type estimators with the data-adaptive character of the kth nearest neighbor approach. Breiman et al., observed that the best value of a for the Parzen estimator depends on which measure of error is used, and hence would be much more difficult to use in practice than the variable kernel method when the PDF is unknown. They concluded, through some simulations, that the variable kernel estimate was superior to the Parzen estimate. The main disadvantage of the variable kernel estimate is its lack of systematic learning rules. It is based on a rule which tries to find the best value of lc by varying it from an initial guess.</p><p>Equation (1) can also be regarded as a variable kernel estimate if we do not fix the value of g ' s ahead of time; but, as in the case of RBF's, the FBF's in (1) do not assume the data is random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Probabilistic Neural Networks</head><p>Specht's PNN <ref type="bibr" target="#b24">[31]</ref> is based on a nonparametric estimation of a probability density function, so that a Bayes decision rule can be used for pattern classification. Consider the twocategory situation in which the state of nature S is known to be either S A or Sg. Let the measurements be represented by the p-dimensional vector z = [XI, ~1 .</p><p>, xPlt; then, the Bayes</p><formula xml:id="formula_9">decision rule becomes 4x1 = SA if ~A ~A ~A ( Z ) &gt; ~B ~B ~B ( X ) (27) d ( z ) = S B if ~A ~A ~A ( z ) &lt; ~B ~B ~B ( z ) (28)</formula><p>where f ~( z ) and f g ( z ) are the PDF's for categories A and B , respectively, 1~ is the loss function associated with the decision d ( z ) = Sg when S = S ~; l g is the loss function associated with the decision d ( z ) = SA when S = sg; hA is the a priori probability of occurrence of patterns from category A; and hg = 1 -h~ is the a priori probability that S = Sg.</p><p>The boundary between the region in which the Baye's decision d ( z ) = SA and the region in which d ( z ) = Sg is given by the equation</p><formula xml:id="formula_10">(29) f A (2) = T f B (2)</formula><p>where</p><p>The key to using (29) is the ability to estimate PDF's based on training patterns. Specht's probabilistic neural network uses Parzen's method to estimate the PDF, consequently, if we use Gaussian functions for the weighting function of (17), it becomes a RBF network. When this approach was first proposed and used for pattern recognition, however, there were two limitations inherent in the use of Parzen's method: 1) the entire training set must be stored and used during testing; and, 2) the amount of computation necessary to classify an unknown point is proportional to the size of the training set. Both considerations severely limited the direct use of Parzen's method in real time. To overcome these limitations, Specht proposed polynomial discriminant functions [32], which approximated Parzen's estimates, using Taylor series, to reduce the number of calculations. With the advent of VLSI technology, Specht implemented the PNN using Parzen's method without simplification [3 11.</p><p>A technique similar to Specht's polynomial discriminant functions was explored by other researchers [l], [3], <ref type="bibr" target="#b1">[17]</ref>, who referred to their work using the term "potential functions." This term first appeared in the pattern recognition literature when the Soviets [l] introduced a simple algorithm of potentials. Their method was originally suggested by the idea that, if data samples are thought of as points in a multidimensional space, and if electrical charges are placed at these points, the electrostatic potential would serve as a useful discriminant function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. General Regression Neural Networks</head><p>Specht [30] also extended his PNN to a G R " . A more general approach to forming an associative memory is to avoid distinguishing between inputs and outputs. By concatenating the input vector and the output vector into one longer measurement vector, the joint PDF can be obtained. Let f ( z , z ) be the joint probability density function of a random vector, z E R", and a random variable, a E R. The conditional mean of z given z (also called a mean-squared estimator) is given by Let (Z', F'), 1 = 1,2, . . . , N , be sample values of the random variables z and a; then, a consistent estimator of f (z, a ) , based upon Parzen's method, is given from (25) as</p><formula xml:id="formula_11">1 l n = (2T)("+1)/2a("+l) E c 1=1 1 [ 2a2 (5 -Z y ( z -3 1 )</formula><p>'exp -Substituting ( <ref type="formula">32</ref>) into ( <ref type="formula">31</ref>) and performing the integration yields the following <ref type="bibr" target="#b26">(33)</ref> 1 1</p><formula xml:id="formula_12">(z -Z"*(z -El) (2 -T y ( z -i+) z(z) = B(z1z) = " ;</formula><p>1=1 which is the probabilistic general regression used in Specht's GRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparisons</head><p>Comparing ( <ref type="formula">33</ref>) with (l), we see that they are almost the same. If we choose the parameters in (l), as M = N,af = for all i = 1 , 2 , . . . , n and I = 1 , 2 , . . . , M , T f = the ith element of the sample vector Z', and, F' = the sample Zl, and f ~( z ) = 0, then the equation for the fuzzy system (1) becomes structurally the same as the equation for the probabilistic general regression <ref type="bibr" target="#b26">(33)</ref>. In this case, it seems that the height method defuzzifier makes the FBF network play the role of a mean-squared estimator. Note, also, that the product inference rule makes the multivariate kernel a product of univariate kernels. Poggio and Girosi <ref type="bibr" target="#b6">[22]</ref>, [231 also showed the relation between regularization and Bayes estimation, and demonstrated that ( <ref type="formula">16</ref>) coincides with maximum a posteriori (MAP) estimation provided that the noise is additive and Gaussian, and the prior is Gaussian. Note that the mean-squared estimator gives the same result as the MAP estimator when measurements and unknown parameters are jointly Gaussian 1181.</p><p>Although there is no statistical consideration when constructing a fuzzy system, if we apply Parzen's conditions ( <ref type="formula">18</ref>)-( <ref type="formula">23</ref>) to fuzzy membership function (MF) ~F : U -+ [0,1], they can be easily satisfied by relaxing the constraint that the range of the membership function satisfies <ref type="bibr" target="#b5">(21)</ref>. Equations ( <ref type="formula">22</ref>) and ( <ref type="formula">23</ref>) guide the choice of the smoothing parameter, i.e., 17 in the special case of (1).</p><p>We can also apply <ref type="bibr" target="#b24">(31)</ref> to a triangular weighting function, Ic(y), one that satisfies Parzen's conditions. In this case f(z, z ) = ~-( n + 1) N where A(xi -?F:/I~) denotes a triangular function centered at xf with base c and height 2/17. In (l), if we use the triangular membership function with same d s , we again obtain <ref type="bibr" target="#b28">(35)</ref>; therefore, the resulting fuzzy system again turns out to be a G R " .</p><p>Although a FBF network and G R " are similar in special situations, they are quite different from many fundamental points of view. For example, FBF networks are constructed from a combination of sample data pairs (2" Z'), and fuzzy IF-THEN rules, whereas the G R " is constructed only from the sample data pairs (Z', Z'). Additionally, the data is assumed to be random for a G R " , whereas no such assumption is made or needed for a FBF network. Whereas FBF networks provide a very good framework to combine linguistic information and measured numerical information , the GRNN can only make use of the numerical information.</p><p>In this section, we showed that a PNN is a special case of RBF networks. We, also showed that the structure of a G R " is a special case of FBF networks, i.e., given the same set of information (sample pairs), we can construct a fuzzy system (using fuzzy logic principles) which has exactly the same structure as the GRNN, by modifying the range of the MF. This can be justified by Parzen's conditions. Also, we can speculate that the sharper the shape of the consequent MF, the stronger is our belief in a fuzzy set. Wang <ref type="bibr" target="#b29">[36]</ref> mentioned this point and claimed that the modified height method defuzzifier, which exploits this speculation by modifying the range of the consequent MF, would result in better performance than the height method defuzzifier.</p><p>The principle advantages of GRNN's are fast learning and convergence to the optimal regression surface as the number of samples become very large. The disadvantage of G W ' s is the amount of computation required of the trained system to estimate a new output vector. Burrascano</p><p>[5] has suggested using learning vector quantization to find representative samples, to reduce the size of the training set for a G R " . Schioler and Hartmann <ref type="bibr">[28]</ref> proposed an algorithm to alleviate the computational burden based on the ideas for automatic recruitment of new centers. The OLS learning algorithm [7] also overcomes this problem to a large degree by selecting a subset of significant regressors using projections.</p><p>-IV. COMPARISONS BETWEEN FBF'S <ref type="bibr" target="#b27">(34)</ref> AND GAUSSIAN s U h 4 APPROXIMATIONS and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gaussian Sum Approximations</head><p>Gaussian sum approximations have been proposed as a means to accomplish practical nonlinear Bayesian filtering [2], [29j. They are motivated by the fact that the Kalman filter, which is valid only for linear Gaussian systems, continues to be widely (and heuristically) used for nonlinear or nonGaussian</p><formula xml:id="formula_13">N l = l i = l systems.</formula><p>Consider a probability density function f(x). The problem of approximating f (x) can be conveniently considered within the context of delta families of positive type <ref type="bibr">[lo]</ref>. Using the delta families, the following result can be used for the approximation of a density function f(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(35)</head><p>Theorem [IO]: Let 6, belong to a delta family whose limit function behaves like a delta function; then the sequence f o (x) fd.1 = . I , &amp;(x -.If(.) d. <ref type="bibr" target="#b29">(36)</ref> converges uniformly to f (z) on every interior subinterval of (-CO, 00).</p><p>When f has a finite number of discontinuities, the theorem or n which is formed by the convolution of 6, and f , as</p><formula xml:id="formula_14">p = Caapi. (<label>43</label></formula><formula xml:id="formula_15">) '20 i = l</formula><p>Several methods, [9] and <ref type="bibr" target="#b4">[20]</ref>, have been developed for choosing the parameters ai, pi, and ui. Let P denote the Gaussian representation given by</p><formula xml:id="formula_16">P = [(a,, p z , uz), 2 = 1 , 2 , . ' . , n ] .<label>(44)</label></formula><p>is still valid except at the points of discontinuity. If 6, is required to satisfy the condition that The optimal Gaussian sum approximation [9] is obtained by minimizing the sum of the squared errors between samples of ^-6,(z) dx = 1 (37) the original signal and the approximation signal with respect to the parameters E'; i.e., it follows from (36) that f , is a probability density function for all u ; therefore, in Kalman filtering, the following delta family is a natural choice for density approximations where</p><p>It is shown in <ref type="bibr" target="#b8">[24]</ref> that the Gaussian density tends to the delta function as the variance tends to zero. Using (38), the density approximation f, is written as</p><formula xml:id="formula_17">00 fu(.) = s _ , No(x -.)f(.) d. . (<label>39</label></formula><formula xml:id="formula_18">)</formula><p>It is this form that provides the basis for the Gaussian sum approximations. It is clear that Nu (xu ) f ( u ) is integrable on (-CO, CO) and is at least piecewise continuous; thus, (39) can itself be approximated on any finite interval by a Riemann sum.</p><p>Consequently, an approximation of fo over some bounded interval (a, b) can be written as</p><formula xml:id="formula_19">" # P = arg min E (<label>45</label></formula><formula xml:id="formula_20">) P i=l</formula><p>The solution of (46) may be derived by solving the system of nonlinear equations obtained by setting aE/aPl = 0, for 1 = 1,2, . . . ,3n (the 3n unknown parameters in P). The steepestdescent method is a commonly used approach to the solution of nonlinear minimization problems. While it guarantees local convergence, it poses some restrictions in learning rate, and, converges slowly. Some modified algorithms are proposed in <ref type="bibr">[9]</ref> to obtain an iterative optimization procedure which results in fast convergence. Generally, the performance of the gradient descent method is strongly dependent on the choice of the initial parameters. Additionally, the number of Gaussian basis functions also i"1 <ref type="bibr">(40)</ref> For practical purposes, it is desirable that f be approximated to within an acceptable accuracy by a relatively small number of terms of the series. For the subsequent discussion, it is convenient to write the Gaussian sum approximation as affects the approximation error. In [9], a scale-space image of the signal is used to estimate these parameters. Unlike (40), it has been assumed in (41) that the variance of can vary from one term to another, to obtain greater flexibility for approximations using a finite number of terms. Certainly, as the number of terms increase, it is necessary to require that ( ~i tend to become equal and vanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons</head><p>The problem of choosing the parameters ai, p i , and (T; to obtain the best approximation fn to some density function has been considered <ref type="bibr" target="#b0">[16]</ref>, <ref type="bibr" target="#b4">[20]</ref>. In many problems, it may be desirable to cause the approximation to match some of the moments. For example, if the mean associated with f is fi, then the constraint that f n have mean value p would be Comparing (47) with (8), we see that they are almost the same. If we let u: = u for all i = 1 , 2 , . . . , n they are exactly the same; hence, the Gaussian sum approximation can be regarded as an extended version of a RBF network where the radial basis function is Gaussian. It can also be viewed as a class of variable kernel estimates when K in (26) is Gaussian. Note that the data is assumed to be random for a Gaussian sum approximation, whereas no such assumption is made or needed for a IU3F or FBF network.</p><p>In addition, a FBF network has rich kernels. To show this, we have plotted in Fig. <ref type="figure">2</ref> the FBF's when the centers of the Gaussian MF's are equally spaced. The two FBF's at the center, which are radially symmetric, look like Gaussian functions, whereas the exterior FBF's look like sigmoidal functions. Although this behavior is not generally true unless the centers of the FBF's are equally spaced, it is quite likely that the  MFs' centers will be equally spaced in function approximation problems. In [14], it is shown that Gaussian functions are good at characterizing local properties, whereas sigmoidal nonlinearities are good at capturing global properties; hence, FBF's combine the advantages of both the Gaussian basis functions and the sigmoidal nonlinearities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATION AND COMPARISONS</head><p>To compare a FBF network with a RBF network, we approximated the function shown in Fig. <ref type="figure" target="#fig_7">3</ref> using both networks. This function was sampled every 0.025 sec. so that there are at most 241 training points. Three experiments were performed:</p><p>1) The training data was sampled at four rates, 0.025 sec., 0.1 sec., 0.2 sec. and 0.4 sec. The latter three sampling times correspond to downsampling the data 4: 1, 8: 1 and 16:1, respectively. The purpose of this experiment was to see how well the two networks performed when less and less training data was available.</p><p>2) Using the 4:l downsampled data, some of the training data was removed from either its left-or right-hand sides. Linguistic rules, that represent a priori structural knowledge about the unknown function, were then added Gaussian kernels were used in the RBF networks, whereas Gaussian membership functions were used in the FBF networks. The RBF networks had 13 kernels whose centers ranged from 0-6 and were equally spaced at intervals of 0.5. The FBF's used 13 rules whose Gaussian membership functions were centered at exactly the same 13 points that the RBF network kernels were. The phrase "x (or y) is close to x j (or yj)" was represented by the membership function exp ( -[ ( x -x ~) ~/ ~o -~] ) .</p><p>Let f ~( x ) , ~F I ( x ) , and f ~2 ( x ) denote the RBF network approximation, FBF network approximation without linguistic rules, and FBF network approximation with linguistic rules, respectively. The formulas for these approximations are In (50), shown at the bottom of this page, T denotes the number of linguistic rules that replace numerical data, and yi denotes the center of the consequent membership function in the rule. In most cases T = 1. Tables <ref type="table">11</ref> and<ref type="table">I11</ref> indicate exactly which rule(s) were used. In all cases, we chose D = 0.6 and used least squares to determine the w; weights. Mean-squared errors for the three experiments are summarized in Tables 1-111. In all cases the FBF approximations gave better results than the RBF approximation. We have not included plots for the RBF or FBF approximations because they are all quite close to the original function depicted in Fig. <ref type="table">I1</ref> and<ref type="table">I11</ref> demonstrate that linguistic information does indeed provide rich information to the FBF network, and that such information can make a substantial difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The results in Tables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we showed that a FBF network is a nonlinear combination of RBF's or HyperBF's depending on the value of D. We interpreted the constraints of regularization techniques as linguistic rules to compare how they are utilized in function approximation and showed that Poggio and Girosi absorb linguistic rules using a constraint operator, whereas a FBF network absorbs linguistic rules directly into its basis functions. We also showed that the structure of a G R " is a special case of a FBF network, although the data is assumed to be random for a G R " , whereas it is nonrandom for a FBF network. We also showed that the Gaussian sum approximation can be regarded as an extended version of a Gaussian RBF network.</p><p>We studied a simple function approximation problem, to compare a FBF network with a RBF network. The FBF network with and without linguistic rules showed better performance than the RBF network, due to the richness of FBF kernels, which agrees with <ref type="bibr" target="#b26">[33]</ref>.</p><p>Fig. <ref type="figure" target="#fig_9">4</ref> summarizes the relationships between a RBF network (including the GRBF and the HyperBF networks) and a FBF network, which are based on nonrandom numerical data, as RBF's and are the Only basis functions that handle linguistic information as well as nonrandom numerical data;</p><p>.. [28] H. Schioler and U. Hartmann, "Mapping neural network derived from the Parzen window estimator," Neural Networks, vol. 5, pp. 903-909, 1992. approximations; and, 3) FBF's are applied to data (and rules) without an underlying assumption of randomess about the data. H~~ to apply msys to "random" data is a for a future study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) R B F ' ~ encompass other techniques such as ~~~~~i~ sum</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>+</head><label></label><figDesc>52 [ fi exp (-; (2) ' ) I 5 [ fi exp (-; (9) l=M;l where x = (21, . . . , x,)~; zf, and nf are real-valued parameters; 2 , I = 1,. . . , M are coefficients; 2 , I = M + 1,. .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Basic configuration of fuzzy logic system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>in which pFj(x;) = exp (-(1/2)(q are Gaussian membership functions, and B j = 2 3 E R are constants. If</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>= ( r 2 + c2)lI2, T &gt; 0 (multiquadrics). The solution to the interpolation problem has the following If we use the Gaussian radial basis function notation, then (14) can be expressed as form [26] M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Parzen's conditions on the weighting function k(y) are SUP Ik(Y)l&lt;0O (18) -w &lt; y &lt; w where sup indicates the supremum 00 s _ , Ik(Y&gt;I dy &lt; 03, ) dy = 1. (21) In (17), o = a(n) is chosen as a function of n such that lim o(n) = 0 11-00 and lim no(n) = 00. (23) n-ccParzen proved that the estimate f n ( x ) is consistent in the mean-squared sense in that E{lfn(z)f(x)12} + 0 72 4 00.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>to the representation of m-dimensional signals, then f m , u ( x ) = a i ~o ( z xi). i=l If we extend the 1-D Gaussian sum approximation in (41) n where Er==, a; = 1, and ai 2 0 for all i , and n 5 m in (40).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Function to be approximated using FBF network and RBF network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>close to zero, THEN y is close to 3 IF x is close to 6, THEN y is close to 0.3) Using the 4:l downsampled data, some of the training data was removed wherever the function has a local extremum. Linguistic rules, that represent additional a priori structural knowledge about the unknown function, were then added to the FBF network to see if these rules would compensate for the lost data. The rules are: d3): Id4): IFxiscloseto1.7, THENyiscloseto2.5 IF x is close to 0.5, THEN y is close to 11.25 d5): IF x is close to 3, THEN y is close to 7.5 d6): IF x is close to 4.3, THEN y is close to -2.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 4 .C</head><label>4</label><figDesc>Fig. 4. data plus linguistic information, and (b) random data. Comparison of different approximations from the point of view of the assumptions made about the available data: (a) Nonrandom numerical</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>[</head><label></label><figDesc>29] H. W. Sorenson and D. L. Alspach, ''Recursive Bayesian estimation using Gaussian sums," Automatica, vol. 7, no. 4, pp. 465479, July 1971. 1301 D. F. Specht, "A general remession neural network," IEEE Trans. Neural</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An algorithm for least-squares estimation of nonlinear parameters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Marquardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of SIAM</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="7" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Potential functions in mathematical pattem recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Meisel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="911" to="918" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
		<title level="m">Lessons in Digital Estimation Theory</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interpolation of scattered data: Distance matrices and conditionally positive definite functions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constr. Approx</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast learning in Networks of locally-tuned processing units</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Darken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="294" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A theory of networks for approximation and learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Laboratory, MIT, A.I. Memo</title>
		<imprint>
			<biblScope unit="volume">1140</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Networks for approximation and learning</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1990-09">Sept. 1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1481" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">SiKnals and Systems, PWS-KENT, 2nd . -well as the relationships between the Parzen type networks (PNN, G R &quot; ) and the Gaussian sum approximation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Poularikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1991. 1981</date>
			<publisher>Cambridge Univ. press</publisher>
		</imprint>
	</monogr>
	<note>from the point of view of the assumptions made about the available ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Radial basis functions for multivariable intemolation: A re---view</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Note that, 1) FBF&apos;s are architecturally different from Eds</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mason</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Cox</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>oxford univ, Press</publisher>
			<date type="published" when="1987">1987. 1987</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="143" to="167" />
		</imprint>
	</monogr>
	<note>Proc. 12th Biennial Numerical Anal. Con$ Dundee</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Theoretical foundations of potential function method in pattern recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Aizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rozonoer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Avtomatika i Telemekhanika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="971" to="936" />
			<date type="published" when="1964-05">May, 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear Bayesian estimation using Gaussian sum approximations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Alspach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Sorenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="448" />
			<date type="published" when="1972-08">Aug. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the method of potential functions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Braverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Avtomatika i Telemekhanika</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="213C" to="L2138" />
			<date type="published" when="1965-12">Dec. 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning vector quantization for the probabilistic neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Purcell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burrascano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">458461</biblScope>
			<date type="published" when="1977-05">May 1977. July 1991</date>
		</imprint>
	</monogr>
	<note>Technometrics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimation of a multivariate density</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cacoullos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals Institute Statistical Mathematics (Tokyo)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="189" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Orthogonal least squares learning algorithm for radial basis function networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F N</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="302" to="309" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Networks and the best approximation property</title>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Laboratory, MIT, A.I. memo</title>
		<imprint>
			<biblScope unit="volume">1164</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Signal representation based on a Gaussian decomposition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goshtasby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schonfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1991 Con$ Inform</title>
		<meeting>1991 Con$ Inform<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Koreyaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Methods. New York Academic</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="330" to="333" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural Networks and Fuzzy Systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kosko</surname></persName>
		</author>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page">1</biblScope>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fuzzy systems as universal approximators</title>
		<author>
			<persName><surname>__</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">an early version appears in Proc. 1st IEEE Int. Con$ Fuzzy Syst.-IEEE FUZZ &apos;92</title>
		<imprint>
			<date type="published" when="1992-03">Nov. 1994. Mar. 1992</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1153" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the application of the potential function method to pattern recognition and system identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Lemke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968-04">Apr. 1968</date>
		</imprint>
		<respStmt>
			<orgName>Purdue Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A critical overview of neural network pattern classifiers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1991 IEEE Workshop Neural Networks for Signal Processing</title>
		<meeting>1991 IEEE Workshop Neural Networks for Signal essing<address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A nonparametric estimate of a multivariate probability density function</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">0</forename><surname>Looftgaarden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Quesenbeny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist., . . Nehvoiks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic neural networks</title>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generation of polynomial discriminant functions for pattern recognition</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="308" to="311" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A closer look at the Radial Basis Function (RBF) networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Asilomar Con$ Signals, Syst. Computers</title>
		<meeting><address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-11">Nov. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fuzzy basis functions, universal approximation, and orthogonal least squares learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="814" />
			<date type="published" when="1992-09">Sept. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Back-propagation fuzzy systems as nonlinear dynamic system identifiers</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Con$ Fuzzy Systems</title>
		<meeting>IEEE Int. Con$ Fuzzy Systems<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Analysis and design of fuzzy systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of S. CA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fuzzy adaptive filters, with application to nonlinear channel equalization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="170" />
			<date type="published" when="1992-08">Aug. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyun Mun Kim received the B.S. degree in control and instrumentation engineering from Seoul National University, Korea in 1984. He received the M.S. degree in electrical and computer engineering from</title>
	</analytic>
	<monogr>
		<title level="m">Since 1991, he has been working towards the Ph.D. degree in electrical engineering at the University of Southern California. He was a Researcher in ETRI</title>
		<meeting><address><addrLine>Raleigh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985">1985. 1986 to 1991</date>
		</imprint>
		<respStmt>
			<orgName>North Carolina State University</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include fuzzy signal processing and image coding techniques</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
