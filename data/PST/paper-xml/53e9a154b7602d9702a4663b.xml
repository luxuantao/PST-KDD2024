<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Verifiable Resource Accounting for Cloud Computing Services</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vyas</forename><surname>Sekar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intel Labs Petros Maniatis Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Verifiable Resource Accounting for Cloud Computing Services</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2CBB03FB8C6C212DEFC997DE7E5FA90B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>B.8 [Hardware]: Performance and Reliability; C.4 [Performance of Systems]: [measurement techniques] Measurement</term>
					<term>Reliability</term>
					<term>Security</term>
					<term>Verification Cloud computing</term>
					<term>Accounting</term>
					<term>Metering</term>
					<term>Resource auditing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cloud computing offers users the potential to reduce operating and capital expenses by leveraging the amortization benefits offered by large, managed infrastructures. However, the black-box and dynamic nature of the cloud infrastructure makes it difficult for them to reason about the expenses that their applications incur. At the same time, the profitability of cloud providers depends on their ability to multiplex several customer applications to maintain high utilization levels. However, this multiplexing may cause providers to incorrectly attribute resource consumption to customers or implicitly bear additional costs thereby reducing their cost-effectiveness. Our position in this paper is that for cloud computing as a paradigm to be sustainable in the long term, we need a systematic approach for verifiable resource accounting. Verifiability here means that cloud customers can be assured that (a) their applications indeed physically consumed the resources they were charged for and (b) that this consumption was justified based on an agreed policy. As a first step toward this vision, in this paper we articulate the challenges and opportunities for realizing such a framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Computing as a service is seeing a phenomenal growth in recent years. The primary motivation for this shift is the promise of reduced operating and capital expenses, and the ease of dynamically deploying and scaling new services without maintaining a dedicated compute infrastructure. With increased popularity and adoption, however, new and unforeseen challenges emerge.</p><p>A common problem that cloud customers face today is the inability to make sense of the cost footprint of their outsourced computation (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b28">30]</ref>). Because customers have little or no visibility into the infrastructure, these charges may have no obvious direct connection to their application tasks. For example, when execution can be elastic in reaction to dynamic workloads, customers may face inexplicable charges in a pay-as-you-go billing model. This problem is further exacerbated in shared or "public" cloud infrastructures. Providers reduce capital and management costs by multiplexing several customers on the same infrastructure using hardware virtualization. While virtualization provides some degree of isolation between customers, there are many shared resources that cannot be perfectly isolated. This can result in unforeseen externalities that may inflate an application's resource footprint. For example, a misbehaving application may cause cache misses or network congestion and increase the computation footprint for other applications sharing the same physical platform.</p><p>At the same time, although providers do generate income with this business model, their profitability is not clear. (Many providers are reluctant to release accurate revenue-expense estimates <ref type="bibr" target="#b1">[2]</ref>.) We speculate that this arises at least in part because providers are still struggling to figure out how to precisely monitor and bill their customers' and their own resource consumption. Fine-grained monitoring of virtualized computations is a difficult proposition; it becomes only harder when it must also be defensible enough to put on an invoice. As a result, some resources that are difficult to monitor and attribute to client computations are not accounted for. For instance, I/O time <ref type="bibr" target="#b37">[39]</ref> and internal network bandwidth <ref type="bibr" target="#b35">[37]</ref> are not metered, even though these have a non-trivial impact on the provider's operating costs and the performance of other applications <ref type="bibr" target="#b30">[32]</ref>. Similarly, sharing effects (e.g., memory pressure due to contention among co-scheduled jobs) impose costs that are challenging to measure and causally attribute to their source. Consequently, providers either incorrectly account for these costs, passing the inaccuracy on to their customers, or implicitly bear the costs, thereby increasing their own operating expenses.</p><p>As the heady honeymoon phase of cloud computing wanes, cloud providers and customers need to fine-tune their business strategies to remain cost-effective <ref type="bibr" target="#b27">[29]</ref>; "good enough" accounting and billing will no longer be good enough. Even as early as 2008, 61% of IT executives and CIOs rated the "pay only for what you use" as a very important perceived benefit of the cloud model <ref type="bibr" target="#b5">[7]</ref> and more than 80% of respondents rated competitive pricing and performance assurances/SLAs as very important provider attributes <ref type="bibr" target="#b4">[6]</ref>.</p><p>Despite this early confirmation that resource usage and billing are top concerns for IT managers, these have received little or no attention from the industry or the research community. Our informal discussions with industry personnel and other researchers suggests that popular perception on this topic is often quite extreme. One view believes that this is a "non-problem" in that the technical means already exist and it is only a matter of time before market forces resolve it. The opposite view believes that this is obviously a critical problem, but we do not have the technical means to do so! Given that popular perception is polarized, and yet there is little work in this context, our goal in this position paper is to stimulate more active discussions to shed light on this topic.</p><p>Our position in this paper is that for cloud computing services to become successful and sustainable, we need a systematic framework for verifiable resource accounting. Such a framework will benefit both cloud customers and providers. It eases any concerns that customers may have with providers' pricing and performance guarantees. It also gives customers a basis for accurately comparing different cloud providers. At the same time, having such a framework enables providers to faithfully capture their operating expenses by billing for resource expenditures that they do not currently account for and preventing customers from trying to game their billing procedures <ref type="bibr" target="#b41">[43]</ref>. Furthermore, easing such pricing and performance concerns will encourage more cloud adoption and improve profitability by increasing the utilization of a provider's infrastructure.</p><p>Having taken this position, our approach in this paper is to solve the high-level problem at a conceptual level, and then explore the challenges and opportunities for realizing this conceptual framework in practice by relaxing the requirements along a number of axes. In the rest of this paper, we start in Section 2 by defining the problem in the context of an abstract operating model with threehigh level components: the cloud customer, the cloud provider, and a service called the verifier. With this setup, in Sections 3 and 4, we describe the challenges involved in two different aspects of verifiability. In each case, we sketch candidate solutions to address these challenges. We discuss some outstanding issues in Section 5 and related work in Section 6 before concluding in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM STATEMENT</head><p>To help formally define the problem, we introduce an abstract operating model shown in Figure <ref type="figure">1</ref>. There are three logical participants: the customer C , the provider P , and the verifier V . At a high-level, C asks P to run the computation task T . Subsequently, P gives C a consumption report R describing what resources it thinks T consumed. For example, a provider may report a time series of consumption vectors, whose elements correspond to CPU usage, memory bandwidth, memory size, I/O bandwidth, network bandwidth, and energy, aggregated over pre-determined time quanta, for the duration of the task. C takes this report R together with the task T and additional data (the roles of which will become clearer later in this section) to the verifier V and checks if R is a valid resource report for T . Attribution: The attribution model determines how resource consumption is attributed to the owner of a task. This model represents the charging policy of a provider, and covers issues such as whether a task owner should be charged for resources used for task migration, how the cost of tasks thrashing is borne by task owners and the service provider, etc. To this end, we optionally extend the consumption report R with a separate report of indirect, external consumption E , similar to R, but attributed to task T due to external factors such as contention. Together, R and E make up what the provider is going to invoice customer C for task T . Conceptually, we represent the attribution model A as a function that computes the values of R and E given a computation T under given conditions. Intuitively, the attribution model can be regarded as a "golden simulator" of the leased architecture (including its management software). This allows a customer to simulate ahead of time how a computation will consume resources given some inputs and environment conditions (including assumptions about other co-tenant customers' workloads). The first question concerns itself with the veracity of the consumption vector. A provider should not be able to charge a customer with resources it did not expend on her behalf. The second question concerns itself with the efficiency of the provider's infrastructure, with respect to scheduling and provisioning. If the provider conservatively-or erroneously-used 1 GByte of main memory for a task that only required 1 MByte, it should arguably not be able to pass on the cost of its inefficiency to its customer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Provider</head><p>To aid in the verification process, the provider may optionally give C a witness W . We leave the specification of the W open depending on the type and level of verifiability desired by C . For example, the witness may simply be hardware attestations to guarantee the integrity of the reports. As discussed earlier, the customer has access to a logical verifier, an oracle that returns Yes/No answers given a consumption report R and E , the witness W , the task T , and the attribution model A. The simplest verifier might be one that uses W as a sanity check to identify gross tampering with R and E . A more involved verifier may use A together with the W to emulate the task T and compute local versions of R and E . Then it can check if these emulated values are close to the actual R and E received. Note that the verifier need not be run by the customer (i.e., it could be a third-party software service) and that its compute cost will be significantly smaller than the original T . Thus, having to run a verifier does not diminish the appeal of outsourcing computation to the cloud. some conservation of work), and (b) the provider correctly assigns the consumption of a resource to the principal responsible for using that resource.</p><p>To address these sub-challenges, we start with a hypothetical clean-slate solution and then proceed to discuss how we can efficiently approximate these with existing technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A clean slate solution</head><p>In order to obtain very fine-grained resource footprints of a wide spectrum of resources, the reporting mechanism should ideally be implemented at a trusted hardware layer. This is because of two reasons. First, the hardware is in the best position to observe the physical resource usage of a number of resources (e.g., cache misses, I/O requests) that may be abstracted away from the software stack. Second, a hardware root-of-trust is more reliable than OS-or VMMlevel trust in a virtualized environment <ref type="bibr">[5]</ref>.</p><p>Suppose we had a trusted hardware layer that provides the following primitive. For each time epoch, the hardware generates an attested report specifying the active atomic principals and the amount of each resource consumed by each such principal during this epoch. Specifically, a trusted monitoring component on the hardware generates for each timestep t an attested log entry of the form {U , S1, . . . , SN }, where U refers to an atomic principal associated with the hardware context and Sis are the various resources that need to be accounted for. To provide the hardware layer with visibility into application-layer principals, we can extend ideas from prior work on resource containers <ref type="bibr" target="#b11">[13]</ref> on practically exposing higher-layer principals to lower-layers.</p><p>Having a trusted consumption monitor satisfies two key requirements. First, because the reports are generated using in-hardware monitoring that is trusted to report actual consumption, a provider cannot convincingly charge customers for resources that it never consumed. Second, a provider cannot double-charge the same resources to multiple customers, since only a single customer is associated with every reported consumed resource, and the monitor reports only what was consumed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Practical Approximations</head><p>While the above clean-slate solution is conceptually complete, there are three practical challenges: reporting bandwidth for transmitting fine-grained per-epoch measurements, performance overhead for getting such attested measurements, and dependence on a trusted hardware primitive that does not exist today. Next, we discuss potential solutions to address each challenge. Reporting bandwidth: Reporting the entire trusted resource consumption log described above may be prohibitive. For example, assuming reports of 10 32-bit resource features per second, a large provider like Amazon (say 100K instances) would have at least an overhead 10 * 32 * 100K ≈ 32 Mbps of outbound traffic to send these reports (ignoring metadata and cryptography). To reduce this bandwidth cost, we envision an aggregation step that processes the log entries before generating R. That is, given the attested reports across multiple time epochs, a trusted aggregator generates a statistical summary of the resource consumption vector across time. Different classes of resources may define domain-specific aggregation functions. For example, we may want the sum as an aggregator for CPU cycles, for memory we may want to find the max and the sum of the utilization, and for I/O resources we may want to count the total bandwidth-time footprint. Note that the aggregation can even take place inline, meaning that individual log entries need not be physically generated or stored anywhere. Measurement overhead: Running fine-grained measurements on a per-instruction or per-CPU cycle granularity can introduce a non- trivial performance penalty for the application processes being monitored <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b32">34]</ref>. We discuss three potential opportunities to reduce this performance impact:</p><p>• The first, is offloading monitoring to a dedicated co-resident processor on a multi-core platform, similar to previous work on Log-Based Architectures <ref type="bibr" target="#b32">[34]</ref>. The main idea is to generate events (e.g., explicit memory operations) that are efficiently routed to a secondary core, which then performs on-line analysis or packaging for off-line perusal, leaving the main core to continue performing its main functionality. • The second is sampling. Instead of maintaining fine-grained per-cycle or per-second counters, we can use a subset or sample of the resource utilizations. Sampling meshes well with the idea of aggregation to reduce reporting bandwidth. If we are only interested in a coarse-grained or aggregate statistic, we can choose a suitable sampling strategy that can give tight bounds on the bias and variance of the estimate (e.g., <ref type="bibr" target="#b8">[10]</ref>). However, a sampling approach must be constrained to forestall adversarial activity such as cycle stealing <ref type="bibr" target="#b41">[43]</ref>, where malicious VMs opportunistically swap themselves out before a sample is taken, to charge their own activities to another VM's customer. Randomization seems to be a cheap but effective solution to counter such malice. • Further along the spectrum, we can take snapshots of the resource consumption vector every time a new principal (e.g., a process) acquires or releases a resource; Figure <ref type="figure" target="#fig_1">2</ref> demonstrates this for the CPU, where process context switches are the acquisition/release boundaries. As long as the trusted monitor can guarantee that no principal other than the allocated one can use the resource between consecutive acquire/release events, then such measurements can be used to generate high-accuracy reports with minimal overhead.</p><p>Relaxing hardware dependence: Realizing such trusted hardware capabilities typically involves fairly long development cycles on the order of tens of months. A natural question, then, is if we can approximate the trusted reporting and aggregation primitives using existing hardware and software capabilities.</p><p>The ideas behind Log-based Architectures <ref type="bibr" target="#b32">[34]</ref> can help here. Rather than performing a full per-principal attested measurement in an online fashion, we can use simpler, but dedicated hardware that just records the instruction stream. Then, we can have a postprocessing step that reconstructs the sequence of actions and associates the resource consumptions to the active principals in each epoch. The challenge here is to provide this post-mortem processing with sufficient context to do the attribution accurately.</p><p>The capabilities we desire, strictly speaking, can be implemented at the VMM layer. Unfortunately, modern VMMs are sufficiently complex and involve several millions of lines of code. Thus, adding the VMM to our trusted computing base may not be a feasible alternative. Fortunately, what we need is a very small subset of  the functionality that VMMs provide and this can be implemented as a small statically verifiable module. Here we can build on the promise of several recent efforts for building a small, but trusted, shim layer that performs a more restricted set of monitoring tasks. For example, recent work has shown that it is possible to implement a shim layer to intercept network packets <ref type="bibr" target="#b14">[16]</ref> and to isolate trusted functionality from a legacy OS <ref type="bibr" target="#b24">[26]</ref>. The challenge here is to evaluate whether the software-based solutions have enough visibility into monitoring low-level effects (e.g., cache stress), if they need to be reported as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">"SHOULD I"-VERIFIABILITY</head><p>"Should I"-verifiability concerns itself not with whether specific usage attributable to a customer took place-the subject of the previous section-but with the justification for that usage. Given the workload at the infrastructure provider and the resource allocation policy agreed upon by the customer, should the customer's job have consumed the charged resources?</p><p>We assume here that the customer and the provider have an agreedupon specification of the "right" resource-allocation policy. We consider two types of specifications: (1) a prescriptive specification that describes in some executable pseudocode the resource allocation decisions that should be made, given current workloads; or (2) a quantitative specification that describes an upper bound for the resources the allocator should devote, as a function of the given task and other workload. We study these two kinds of "Should I"-verifiability below, first by considering ideal solutions and then relaxing those solutions to make them practical.</p><p>In theory, a prescriptive specification can be verified via re-execution (Figure <ref type="figure" target="#fig_3">3</ref>). Assuming that all resource allocation decisions are made by a self-contained component-the allocator handling memory, CPU, I/O, and other resource allocation decisions-a verifier with a trustworthy log of all inputs and outputs to the allocator can re-execute it and compare allocation decisions to those logged, to ensure the remote allocator was run correctly. This is similar to the notion of remote verification of distributed computations, as in PeerReview <ref type="bibr" target="#b18">[20]</ref>. In PeerReview, a computation (the allocator in our case) is checked by a remote verifier by being re-executed according to a tamper-evident record of its inputs, comparing the outputs to a tamper-evident record of the outputs at the provider, and looking for deviations between the two streams of outputs.</p><p>In practice, this idealized solution to prescriptive "should I"verifiability is hampered by several challenges. First, the raw log of inputs to the allocator is voluminous (possibly multiple entries per second from each machine in the infrastructure); collecting and sharing that log with the customer could require significant bandwidth and storage. Second, making that log tamper-evident requires computational resources for cryptographic or other "conditioning" of the log, possibly involving a TPM in the process. Third, the logic of a resource allocator (its code) or the policy may be proprietary to the provider, so sharing its specifics to enable reexecution may be unacceptable for privacy reasons.</p><p>Quantitative "should I"-verifiability, ideally, requires no information about the logic of an allocator, but more information (than the prescriptive approach) about the state of the whole system. Given the (and tamper-evident) time series of historical resource utilizations and their allocations to different customer principals, the quantitative approach applies mathematical functions to the inputs to estimate an upper bound on utilization for a new task. If that upper bound is significantly lower than the charged resources, the customer suspects the provider for liberal scheduling. This concept appears related to SLA verification, with a twist. While SLA verification establishes that a service provided a minimum guaranteed level of resources to a customer, quantitative verifiability establishes that the service allocated to a customer (and subsequently charged for) no more resources than required by the submitted task.</p><p>With quantitative verifiability, the practical challenges in terms of the volume and integrity-protection of raw data are somewhat steeper. This is because the verifier now requires not only decisions made by the allocator but also instantaneous measurements of state properties (e.g., utilization). On the other hand, quantitative verifiability does not require information about the allocator's code or policy, since the verifier is independent of the logic of the allocator and requires no re-execution or emulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Practical Approximations</head><p>We first consider prescriptive verifiability. The log-volume challenge is one of location. One way to address it is by moving the verifier (or the verifier's trusted agent) closer to the source of the log, on the provider's platform. For instance, if the provider's platform includes an execution environment trusted by the verifier (e.g., a verifier-trusted minimal OS and application, booted on the provider's platform using a hardware root of trust), then the verification could run there, obviating the need for transporting or storing large logs. This is complementary to doing local aggregation for "Did I"-verifiability.</p><p>The second challenge, that of the tamper-evidence of logs, remains difficult regardless of where the verifier operates. On one hand, collecting the logs must be trustworthy and, on the other hand, the logs must maintain their integrity before analysis by the verifier. Similar to "Did I"-verifiability, an approach to mitigate this challenge might be to use a trusted hypervisor to collect and authenticate the log (e.g., by trapping on logged events so that the VM cannot evade monitoring) before forwarding it to the verifier; e.g., via techniques like Secure In-VM Monitoring <ref type="bibr" target="#b34">[36]</ref>. Combining this trusted log collector with a trusted execution environment for the verifier's code would compound the benefits of addressing both log-related challenges.</p><p>The final question is what the verifier code should be, whether it runs on the provider's platform or at the customer. Certainly running the exact same allocator would simplify re-execution, but at the cost of the provider's privacy. One approach might be to encode an abstraction of the allocator that hides the details of how decisions are made, protecting the provider's policy. To do this, some choices that are deterministic in the provider's code could become non-deterministic in the abstraction; any choice that would be allowed by possible provider policies would then be a plausible, legal but non-deterministic choice. This inherent tension between privacy and accountability is not unique to our framework. However, we can actually leverage the inherent fuzziness of verification in our favor. That is, any practical verification step will have to tolerate some noise in measurement. Consequently, we do not need exact or deterministic replay and an approximate model of the proprietary code might suffice. Some of the proposed approaches above apply just as well to the challenges of quantitative verifiability. The privacy of fine-grained measurements of the performance of a VMM on which the customer's VM executes (as well as co-tenant VMs competing for local resources) is better preserved by running the verifier locally; in this way, performance logs are never directly disclosed outside the provider's platform. However, formal privacy-preserving techniques, such as differential privacy, might be required to ensure that the aggregates collected and used to ensure compliance by the verifier do not, themselves, disclose sensitive information about cotenant VMs <ref type="bibr" target="#b36">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>We have already highlighted many of the key technical challenges in the previous section. Here, we discuss other economic and policy concerns. Incentives for providers: The incentives for customers of cloud services to desire and demand verifiable resource accounting are quite evident. The three main incentives for providers, however, are more subtle and worth highlighting. First, providing more accountability in cloud billing will encourage more customers to move services to the cloud. A major concern for cloud providers is that their profit margins are hurt when their infrastructure is underutilized. Increasing the customer base will alleviate this concern. Second, deploying the mechanisms needed to provide verifiability will also ensure that providers can be less conservative in how they charge customers for specific resources or induced externalities. Third, anecdotal evidence suggests that cloud customers may try to game the billing system <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b41">43]</ref>. The mechanisms we envision can enable providers to better detect such evasion.</p><p>Won't the market solve this problem? One objection could be that market effects will obviate the need for verifiable accounting. In other words, providers who consistently overcharge customers or are unable to correctly attribute resource consumption to customers and pay the cost themselves will eventually be weeded out. There is some truth to this statement; economic realities will remove obvious inefficiencies. But three key factors suggest that market effects alone will not be sufficient. First, the economics for most infrastructure services (e.g., telephony, data networks, public utilities) invariably have a natural economies-of-scale property. Economies of scale implies that these markets converge to a handful of active and profitable providers. Consequently, there may be insufficient competition to deal with the problem. Second, while the choice of migrating across cloud platforms is nice in theory, it is difficult in practice. These include both difficulty in performing an apples-toapples comparison across diverse service cloud offerings (platform services vs. software services) and the non-trivial costs in migrating applications across providers <ref type="bibr" target="#b21">[23]</ref>. Finally, even for the market effects to eventually manifest, the technical means for providers to eliminate the inefficiencies must exist. As we discussed, there are unresolved technical challenges in attribution and accounting in the face of external effects in a cloud environment. Relaxing provider assistance: The previous sections implicitly assumed that the provider generates an attested report of resource consumption. It would also be interesting to explore to what extent we can relax this provider support. There are two possible options for customers.</p><p>The first option is to use resource prediction. While such tools are motivated by provisioning or scheduling applications, they can also provide basic verification checks. For example, the client can extract key features of the workload from its own tamper-resistant execution log and check if the predicted resource consumption roughly matches the reports from the provider. Resource prediction is challenging because workloads are hard to characterize and there are many hidden factors. Two recent approaches to counter these challenges appear promising; the notion of "relative fitness" to extrapolate results across different configurations <ref type="bibr" target="#b25">[27]</ref> and better selection of program features <ref type="bibr" target="#b20">[22]</ref>.</p><p>The second opportunity is for different customers to collaboratively detect violations <ref type="bibr" target="#b38">[40]</ref>. Suppose each client Ci receives a time vector per resource j: Ri,j = [Mi,j,1, Mi,j,2, • • • , Mi,j,T ]. As a simple check, the clients can check for conservation of resources in the Ri,j, or other similar consistency invariants across distinct reports <ref type="bibr" target="#b9">[11]</ref>. That is, if the provider has a cap Mj on how much of resource j it can allocate per time quantum, the clients can check if ∀t, i Mi,j ≤ Mj. If there is missing data (e.g., the provider's capacity is unknown or some customers do not participate), then they could use tomographic techniques <ref type="bibr" target="#b40">[42]</ref> to guess these unknown values. The challenge here is to enable collaborative verification without compromising the participants' private information (e.g., via secure multi-party computation). Other cloud charging models: So far, our focus was on pay-peruse billing models where customers are charged per quantum of resource consumed. There are other "time"-based billing models in use today; the most popular example being machine instances on Amazon's EC2. In this time-based billing model, the problem turns from one of verifiable accounting to one of SLA verification-Did my task get the expected compute throughput over the time it should have been active? While SLA verification seems like a different problem, we can reuse the underlying mechanisms presented earlier. For example, the Did-I step from Section 3 can now reflect a throughput vector and we can check if the throughput is what the customer paid for. However, the throughput could be lower than what was paid for either because of provider inefficiencies (e.g., the provider did not schedule it correctly) or because the task was itself idle (e.g., stalling for remote requests). Then, the Should-I strategies from Section 4 can help distinguish these two cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>The initial success of cloud computing and the development of new software paradigms to support this emerging class of applications has motivated a lot of related work in these areas. We discuss these next and put these in the context of our vision for enabling verifiable resource accounting. Benchmarking: Cloud customers today have a number of providers that vary in both their service offerings and infrastructure capabilities. Recent work from Li et al. compares the costs of running a particular application under different popular providers and suggests that this choice is not easy <ref type="bibr" target="#b21">[23]</ref>. Along a similar vein, because the workloads for cloud applications are themselves not well understood, recent work makes the case for a unified set of benchmarks to evaluate common cloud computing frameworks <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b39">41]</ref>. The emergence of these tools for comparing and benchmarking providers reflects the need for cloud customers to objectively evaluate providers' cost-performance tradeoffs. Such tools are intended to help customers in choosing a suitable service provider. These benchmarking mechanisms can also be used to provide some level of Should-I verifiability; e.g., comparing the resource footprints of running the same workload across different providers and checking for inconsistencies. Optimizing cloud platforms: Configuring and provisioning cloud platforms is a non-trivial optimization problem. There are several aspects that need to be accounted for including scheduling, server placement, network locality, etc. Several such optimizations have been proposed in the literature to overcome inefficiencies in existing cloud computing platforms <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b31">33]</ref>. A case for verifiable accounting will further inspire the development and adoption of such optimizations in today's cloud frameworks. Resource monitoring: Several efforts in industry [1, <ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b16">18]</ref> and academia <ref type="bibr" target="#b15">[17]</ref> have identified challenges in scalably monitoring resource consumption in cloud virtualized environments. Our framework can build upon and extend such monitoring tools. However, there are two key ways in which our vision differs. The first is verifiability; the existing work focuses more on efficient collection and does not provide any evidence of correctness to the cloud customers. The second is that many of these proposals are providercentric; we want to take these a step further and also empower cloud customers. Recent work on verifiable network measurement <ref type="bibr" target="#b9">[11]</ref> is closely related to our problem definition. However, the technical challenges of solving this problem for OS-level resource accounting are significantly more complex than verifiable packet-or flowlevel measurements. Cloud accountability and security: Cloud security is a growing concern both in industry and the research community. For example, cloud customers may want to ensure that the provider faithfully runs their application software <ref type="bibr" target="#b17">[19]</ref>. Knowing that the application code ran unmodified or verifying the input-output consistency is no doubt useful. However, they are not sufficient for verifiable accounting because of environment effects. Other work ensures that the provider that has not tampered with or lost data <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b19">21]</ref>, or that it respects certain consistency guarantees <ref type="bibr" target="#b29">[31]</ref>. These target other aspects of accountability; our work focuses specifically on accountability for resource accounting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>Our position is that cloud-based leased computing paradigms need a mechanism for verifiable resource accounting to be sustainable and dependable in the long term. Such a mechanism will benefit both cloud customers and cloud providers. It gives customers assurances that they really did consume and should be paying for the computing resources they are billed. At the same time, it affords providers with the opportunity to improve their profit margins by more accurately accounting for resources and by increasing their infrastructure utilization via increased cloud adoption. As a first step toward realizing this vision, in this paper, we defined the problem of verifiable resource accounting and highlighted the challenges and potential alternatives to realize this vision in practice. We acknowledge that the solutions outlined here are merely starting points and several practical issues (e.g., performance impact and overhead of the monitoring framework, accidental leakage of private information) remain unresolved. We plan to address these as part of a reference implementation in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Verifiability:</head><label></label><figDesc>Verifiability aims to give the customer assurances about two questions: 1. Did I consume what I was charged? 2. Should I have consumed what I was charged?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Taking snapshots of the resource consumption vector before/after a context switch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Prescriptive "Should I"-verifiability.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>"DID I"-VERIFIABILITYIn this section, we focus on the first question: Did I consume what I was charged? Addressing this question requires infrastructure support to ensure that (a) the provider does not create spurious charges of cycles that were never consumed by any principal (i.e.,</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>Cloud</surname></persName>
		</author>
		<ptr target="http://goo.gl/UVQQi" />
		<title level="m">The Question Vendors Keep Dodging</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cloud storage providers need sharper billing metrics</title>
		<ptr target="http://goo.gl/Drq4J" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">11 post-session q&amp;a</title>
		<ptr target="http://www.usenix.org/events/hotcloud11/stream/warfield/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://blogs.idc.com/ie/?p=213" />
		<title level="m">IT Cloud Services User Survey, pt.3: What Users Want From Cloud Services Providers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="http://blogs.idc.com/ie/?p=210" />
		<title level="m">IT Cloud Services User Survey: Top Benefits and Challenges</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="http://perspectives.mvdirona.com/2009/02/16/ServiceBillingIsHard.aspx" />
		<title level="m">Service billing is hard</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="http://www.vmware.com/products/vcenter-chargeback/overview.html" />
		<title level="m">Vmware vcenter chargeback</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The space complexity of approximating the frequency moments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. STOC</title>
		<meeting>STOC</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Verifiable Network-Performance Measurements</title>
		<author>
			<persName><forename type="first">K</forename><surname>Argyraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNEXT</title>
		<meeting>CoNEXT</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Provable Data Possession at Untrusted Stores</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Curtmola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kissner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCS</title>
		<meeting>CCS</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Resource containers: A new facility for resource management in server systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of OSDI</title>
		<meeting>of OSDI</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://cloudcomputing.sys-con.com/node/858723" />
		<title level="m">Navigating the Fog -Billing, Metering and Measuring the Cloud</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hitune: Dataflow-based performance analysis for big data cloud</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX ATC</title>
		<meeting>USENIX ATC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ETTM: A Scalable Fault Tolerant Network Manager</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uppal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Brajkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NSDI</title>
		<meeting>NSDI</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performance Profiling in a Virtualized Environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sherawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HotCloud</title>
		<meeting>HotCloud</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Google-wide profiling: A continuous profiling infrastructure for data centers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accountable Virtual Machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haeberlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aditya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of OSDI</title>
		<meeting>of OSDI</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PeerReview: Practical accountability for distributed systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haeberlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kouznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SOSP</title>
		<meeting>of SOSP</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PORs: Proofs of retrievability for large files</title>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Kaliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCS</title>
		<meeting>CCS</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting execution time of computer programs using sparse polynomial regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CloudCmp: Comparing Public Cloud Providers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IMC</title>
		<meeting>of IMC</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving mapreduce performance in heterogeneous environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delay Scheduling: A Simple Technique for Achieving Locality and Fairness in Cluster Scheduling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EuroSys</title>
		<meeting>EuroSys</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flicker: An Execution Infrastructure for TCB Minimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mccune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Parno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurosys</title>
		<meeting>Eurosys</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling the Relative Fitness of Storage</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mesnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Sambasivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMETRICS</title>
		<meeting>SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Case for Consumer-centric Resource Accounting Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mihoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Molina-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Cloud Computing</title>
		<meeting>IEEE Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Operating systems should support business change</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="8" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Ngo</surname></persName>
		</author>
		<ptr target="http://www.bizspark.com/Blogs/yi-jian_ngo/Lists/Posts/Post.aspx?ID=85" />
		<title level="m">Metering in the cloud</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enabling Security in Cloud Storage SLAs with CloudProof</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX ATC</title>
		<meeting>USENIX ATC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual Platform Architectures for Resource Metering in Datacenters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMETRICS</title>
		<meeting>SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards automatic optimization of mapreduce programs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOCC</title>
		<meeting>SOCC</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Log-Based Architectures for General-Purpose Monitoring of Deployed Code</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Architectural and System Support for Improving Software Dependability</title>
		<meeting>Workshop on Architectural and System Support for Improving Software Dependability</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The hibench benchmark suite: Characterization of the mapreduce-based data analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE Workshops</title>
		<meeting>ICDE Workshops</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Secure In-VM Monitoring Using Hardware Virtualization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCS</title>
		<meeting>CCS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sharing the data center network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NSDI</title>
		<meeting>NSDI</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Get off of my cloud: Exploring Information Leakage in Third-Party Compute Clouds</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CCS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exertion-based billing for cloud storage access</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HotCloud</title>
		<meeting>HotCloud</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A collaborative monitoring mechanism for making a multitenant platform accountable</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HotCloud</title>
		<meeting>HotCloud</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The case for evaluating mapreduce performance using workload suites</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Griffith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MASCOTS</title>
		<meeting>MASCOTS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Network Tomography: estimating source-destination traffic intensities from link data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statistics Association</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">433</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Desnoyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sundaram</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1103.0759" />
		<title level="m">Scheduler Vulnerabilities and Attacks in Cloud Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
