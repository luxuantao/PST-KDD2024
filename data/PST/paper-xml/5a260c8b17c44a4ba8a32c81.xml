<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE Transactions on Multimedia 2 TABLE I MOTIVATIONS: A FEW EXAMPLES OF CURRENT FER RESEARCH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">L</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Information and System Sciences</orgName>
								<orgName type="department" key="dep2">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics and Informatics</orgName>
								<orgName type="laboratory">LIRIS UMR 5205</orgName>
								<orgName type="institution">Ecole Centrale de Lyon</orgName>
								<address>
									<postCode>69134</postCode>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IEEE Transactions on Multimedia 2 TABLE I MOTIVATIONS: A FEW EXAMPLES OF CURRENT FER RESEARCH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9ED092873B5A1548D7993772BF954703</idno>
					<idno type="DOI">10.1109/TMM.2017.2713408</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial expression recognition</term>
					<term>deep fusion convolutional neural network</term>
					<term>multimodal</term>
					<term>textured 3D face scan</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel and efficient Deep Fusion Convolutional Neural Network (DF-CNN) for multi-modal 2D+3D Facial Expression Recognition (FER). DF-CNN comprises a feature extraction subnet, a feature fusion subnet and a softmax layer. In particular, each textured 3D face scan is represented as six types of 2D facial attribute maps (i.e., geometry map, three normal maps, curvature map, and texture map), all of which are jointly fed into DF-CNN for feature learning and fusion learning, resulting in a highly concentrated facial representation (32dimensional). Expression prediction is performed by two ways: 1) learning linear SVM classifiers using the 32-dimensional fused deep features; 2) directly performing softmax prediction using the 6-dimensional expression probability vectors. Different from existing 3D FER methods, DF-CNN combines feature learning and fusion learning into a single end-to-end training framework. To demonstrate the effectiveness of DF-CNN, we conducted comprehensive experiments to compare the performance of DF-CNN with handcrafted features, pre-trained deep features, finetuned deep features, and state-of-the-art methods on three 3D face datasets (i.e., BU-3DFE Subset I, BU-3DFE Subset II, and Bosphorus Subset). In all cases, DF-CNN consistently achieved the best results. To the best of our knowledge, this is the first work of introducing deep CNN to 3D FER and deep learning based feature-level fusion for multi-modal 2D+3D FER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and 2D+3D multi-modal FER (which uses both 2D and 3D face data). From the second perspective, they are divided into: 1) recognition of prototypical facial expressios (i.e., anger, disgust, fear, happiness, sadness and surprise), 2) detection and recognition of facial Action Units (AU, e.g., brow raiser, lip tightener, and mouth stretch). From the third perspective, they are categorized into static (still images) or dynamic (image sequences) FER <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b67">[68]</ref>. In this paper, we focus on the problem of recognizing the six prototypical facial expressions using multi-modal 2D and 3D static face data (i.e., textured 3D face scans).</p><p>In the literature of FER, the majority of methods are based on 2D face images or videos (e.g., <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>). Despite significant advances have been achieved, 2D methods still fail to solve the challenging problems of illumination and pose variations <ref type="bibr" target="#b36">[37]</ref>. Designing FER systems using infrared facial images is a beneficial attempt to solve the illumination issue <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. But infrared images are usually fail to capture subtle facial deformations, e.g., skin wrinkles <ref type="bibr" target="#b17">[18]</ref>, and also sensitive to the effect of wearing glasses, which is often occur in uncontrolled condition. With the fast development of 3D imaging and scanning technologies, FER using 3D face scans has attracted more and more attentions <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b39">[40]</ref>. This is mainly due to that 3D face scans are naturally robust to lighting and pose variations. Moreover, 3D facial shape deformations caused by facial muscle movements contain important cues to distinguish different expressions. To meet the requirements of real applications, FER based on multimodality data (e.g., visual and audio <ref type="bibr" target="#b48">[49]</ref>, visible and infrared face images <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>), especially using both 2D face images and 3D face models <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref> [42], <ref type="bibr" target="#b49">[50]</ref>, is becoming a promising research direction due to that there exist large complementarity among different modalities.</p><p>This paper is a new attempt along this promising direction, which dedicates to exploring multi-modal 2D+3D FER method by combing the advantages of both 2D and 3D face data. The main challenges of such combination involve the following two issues: 1) how to find a unified framework to generate discriminative facial representations for both 2D and 3D face data? 2) how to optimally combine the facial representations of 2D and 3D face data for expression prediction? As illustrated in Table <ref type="table">I</ref>, handcrafted features such as HOG <ref type="bibr" target="#b6">[7]</ref>, LBP <ref type="bibr" target="#b64">[65]</ref>, and Gabor <ref type="bibr" target="#b63">[64]</ref> have been widely used for facial representations in 2D FER. Similarly, these handcrafted features have also been widely employed in 3D FER, which are used to describe 3D facial shape information by coding different types of geometric maps like depth-SIFT <ref type="bibr" target="#b0">[1]</ref>, normal-LBP <ref type="bibr" target="#b22">[23]</ref>, and curvature-HOG <ref type="bibr" target="#b21">[22]</ref>. Recently, with the significant breakthrough of deep learning, such kind of handcrafted features have been proven to be suboptimal. Thanks to the continuous updating and releasing of large 2D expression datasets (e.g., Acted Facial Expressions in the Wild (AFEW) <ref type="bibr" target="#b9">[10]</ref> and Static Facial Expressions in the Wild (SFEW) <ref type="bibr" target="#b8">[9]</ref>), leaning facial representations using deep learning is becoming the mainstream in 2D FER. For example, following the Emotion Recognition in the Wild (EmotiW) Grand Challenge, a large number of deep learning based approaches, such as deep convolutional neural network (CNN) <ref type="bibr" target="#b59">[60]</ref>, deep belief network (DBN) <ref type="bibr" target="#b18">[19]</ref>, and autoencoder <ref type="bibr" target="#b38">[39]</ref> have been successfully used in 2D FER as shown at the right side of Table <ref type="table">I</ref>.</p><p>However, to the best of our knowledge, deep learning has never been used to learn 3D facial representations in 3D FER. This motivates us to fill this gap although a very limited number of 3D face scans with expression labels are available. Inspired by the fact that the off-the-shelf pre-trained deep CNN models have surprising and consistent good generalization ability for various visual recognition tasks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b37">[38]</ref>, A promising way is using transfer learning method that fine tunes a pre-trained deep CNN model using as many as possible 3D face data.</p><p>Deep CNN can provide a unified framework to learn facial representations for both 2D and 3D face data. Then, how to find a strategy to optimally combine these learned 2D and 3D facial representations is becoming the key issue. As illustrated in Table <ref type="table">I</ref>, the suboptimal handcrafted feature-level fusion and score-level fusion are widely used in current multi-modal 2D+3D FER methods. The importance weights of 2D and 3D facial features have not be well explored. This motivates us to design a learning-base fusion strategy, i.e., a novel deep fusion network, which can automatically learn sophisticated fusion weights of 2D and 3D facial representations for multi-modal 2D+3D FER. Overall, this paper presents a unified end-toend learning framework (i.e., Deep Fusion CNN or DF-CNN), which can deals with both feature learning and fusion learning for multi-modal 2D+3D FER. Therefore, the main novelties and contributions of this paper can be summarized as follows:</p><p>• This is the first work of introducing deep CNN to 3D FER and using learned features to describe 3D facial expressions. To overcome the issue that training 3D faces are far from enough, we propose to use multiple types of facial attribute maps to learn facial representations by fine tuning pre-trained deep CNN models trained from largescale image dataset for generic visual tasks. • This paper proposes to use a deep fusion net (i.e., a learning-based feature-level fusion) to learn the optimal combination weights of 2D and 3D facial representations for multi-modal 2D+3D FER. This is totally different from the suboptimal handcrafted feature-level fusion and score-level fusion used in existing 2D+3D FER. • This paper presents a Deep Fusion CNN, which combines feature learning and fusion learning into a unified endto-end training framework, and consistently outperforms the handcrafted features, pre-trained deep features, finetuned deep features, and state-of-the-art 3D FER methods on three 3D face datasets. The remainder of this paper is organized as follows. Related works for 2D, 3D and 2D+3D FER are introduced in Section II. Section III gives an overview of the proposed approach. Section IV introduces the computational details of generating different facial attribute maps. Section V describes our DF-CNN in detail, involving net architecture, training strategy, and visualization. Experimental results are shown in Section VI, and Section VII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>A. Related works on 3D and 2D+3D FER Current 3D FER approaches are mainly model-based or feature-based <ref type="bibr" target="#b11">[12]</ref>. Model-based methods generally employ dense rigid registration and non-rigid fitting techniques to get the one-to-one point correspondence among face scans. This generates a generic expression deformable model, which can be used to fit unknown face scans, and the fitting parameters are finally used as expression features. For example, Mpiperis et al. <ref type="bibr" target="#b34">[35]</ref> proposed to build a novel bilinear facial deformable model to characterize the behaviors of facial nonrigid deformations. Given a new 3D face model, its expression and identity parameters can be estimated using the welltrained bilinear model. These parameters are then used as expression features and fed into the Maximum Likelihood classifier for expression prediction. Similarly, Gong et al. <ref type="bibr" target="#b14">[15]</ref> suggested to learn a model to decompose the shape of an expressive face into a neutral-style basic facial shape component (BFSC) and an expression shape component (ESC). The ESC is then used to design expression features. Zhao et al. <ref type="bibr" target="#b65">[66]</ref> proposed to build a statistical facial feature model (SFAM) for automatic facial landmarking, both 3D shape and 2D texture features are extracted around these landmarks for expression recognition. Feature-based methods generally extract local expression features around facial landmarks based on surface geometric attributes or differential quantities. For example, 3D landmark distances <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, local surface patch distances <ref type="bibr" target="#b23">[24]</ref> [32] <ref type="bibr" target="#b32">[33]</ref>, geometry and normal maps <ref type="bibr" target="#b35">[36]</ref>, conformal images <ref type="bibr" target="#b62">[63]</ref>, surface normal <ref type="bibr" target="#b25">[26]</ref> and curvatures <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b52">[53]</ref> are some popular features use for 3D FER. As a typical local feature-based method, Maalej et al. <ref type="bibr" target="#b31">[32]</ref>  <ref type="bibr" target="#b32">[33]</ref> proposed to extract local surface patches around 70 facial landmarks in the 3D mesh. These patches were then parameterized by a set of closed iso-level curves at the landmarks. The distance between two patches was computed by the geodesic distance of deforming their corresponding isolevel curves in the Riemannian shape analysis space. Finally, multi-boosting and support vector machines (SVM) classifiers were used to classify the six prototypical facial expressions. By combing the advantages of both feature-based and modelbased methods, Zhen et al. <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref> proposed to study 3D FER problem from the perspective of facial muscular movement model. Their method first automatically segments 3D face shapes into several facial regions according to the muscular movement model. Then, each region is described by a set of geometric features. The weights of different regions are learned by genetic algorithm, and SVM classifier with score-level fusion is used for expression prediction. Savran et al. <ref type="bibr" target="#b41">[42]</ref> utilized multi-modal 2D+3D face data for facial AU detection. They found that 3D data generally perform better than 2D data, especially for lower AUs. Moreover, the fusion of two modalities can improve the detection rates from 93.5% (2D) and 95.4% (3D) to 97.1% (2D+3D). Li et al. <ref type="bibr" target="#b23">[24]</ref> proposed a fully automatic multi-modal 2D+3D feature based FER approach. Both 2D texture descriptors and 3D geometry descriptors are used to describe the appearances and geometric deformations of local facial patches around automatically detected 2D and 3D facial landmarks. The complementarity between 2D descriptors, 3D descriptors, and 2D+3D descriptors are demonstrated in their experiments based on both feature-level and score-level fusion strategies of the SVM classifier.</p><p>The main weakness of model-based methods lie in that they require to establish dense correspondence among face scans, which is still a challenging issue. Moreover, time consuming procedures like dense 3D face registration and model fitting are usually indispensable in practice. Feature-based methods generally perform better than model-based ones. However, their performances are largely dependent on the accuracy of 3D facial landmarking, which is also a challenging task <ref type="bibr" target="#b11">[12]</ref>. FER based on 2D+3D multi-modal data is becoming a promising research direction due to that there exist large complementarity among different modalities. Giving a complete survey for 3D FER is out the scope of this paper, readers are strongly suggested to refer to the comprehensive survey <ref type="bibr" target="#b39">[40]</ref> for the issues of 3D and 4D face acquisition, dense correspondence, alignment, tracking, available databases, as well as the details of feature extraction, selection, classification, and temporal modeling for static and dynamic 3D facial expression recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related works on 2D FER</head><p>Rifai et al. <ref type="bibr" target="#b38">[39]</ref> designed a multi-scale contractive convolutional network to learn hierarchical expression features which are robust to the variations of factors like pose, identity, morphology of the face. Tang <ref type="bibr" target="#b47">[48]</ref> demonstrated the advantages of replacing the softmax loss function of a deep CNN by a linear SVM loss for 2D FER. Liu et al. <ref type="bibr" target="#b29">[30]</ref> proposed a unified Boosted Deep Belief Network framework to iteratively optimizing the expression training process of feature learning, feature selection, and classifier construction. Burkert et al. <ref type="bibr" target="#b1">[2]</ref> proposed a convolutional neural network (CNN) architecture for 2D FER and claimed that it outperforms the earlier proposed CNN based approaches. Liu et al. <ref type="bibr" target="#b28">[29]</ref> designed a 3D CNN incorporating a deformable parts learning component for dynamic expression analysis. The authors also proposed the action unit inspired deep networks for 2D FER <ref type="bibr" target="#b27">[28]</ref>. Khorrami et al. <ref type="bibr" target="#b19">[20]</ref> showed both qualitatively and quantitatively that CNNs can learn facial action units when doing expression recognition, and their method achieved state-of-theart performance on the extended Cohn-Kanade (CK+) and the Toronto Face Dataset (TFD). Kahou et al. <ref type="bibr" target="#b18">[19]</ref> developed a deep learning approach for emotion recognition in video. Their method respectively trained a CNN for video and a deep belief net for audio. "Bag of moutm" features are also extracted to further improve the performance. To fusion different models, the ensemble weights are determined with random search. The idea of ensemble multiple deep models has also been used in <ref type="bibr">Kim et al. [21]</ref>. This work trained 216 deep CNNs by varying network architectures, input normalization, and weight initialization and by adopting several learning strategies. Then, the valid-accuracy-based exponentially-weighted decision fusion method was proposed to ensemble different CNNs.</p><p>The work by Yu and Zhang <ref type="bibr" target="#b59">[60]</ref> is probably the most related work to ours. This method proposed to independently train multiple differently initialized CNNs and output their training responses. To combine multiple CNN models, they proposed to learn the ensemble weighs of the network responses by minimizing the log likelihood loss or hinge loss. Despite with the same spirit of fusing deep models, our proposed learning strategy differs from <ref type="bibr" target="#b59">[60]</ref> significantly. First, they trained multiple CNNs by varying the network initialization, while we only need to train a single CNN for different facial attribute maps. As shown in our experiments (Section VI-D), this kind of single network training can largely reduce both computate time and memory consumption, while still preserve the accuracy. Second, their method learned different weights for different networks, thus corresponding to a learningbased score-level fusion strategy, while ours corresponds to a learning-based feature-level fusion strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW OF THE PROPOSED APPROACH</head><p>Figure <ref type="figure">1</ref> illustrates the pipeline of the proposed DF-CNN approach for 2D+3D FER. Given a set of preprocessed textured 3D face scans with different expressions, each of which is first represented as six types of 2D facial attribute maps (see Section IV), including geometry map (3D coordinates), three normal component maps (normal vectors), normalized curvature map (principle curvatures), and texture map. Then, these six facial attribute maps of each textured 3D face scan are jointly fed into the feature extraction subnet (repetitions of convolution, ReLU, and pooling layers) with sharing parameters, resulting in several hundreds of multi-channel feature maps. All these feature maps are fed into the following feature fusion subnet (including a reshape and two feature fusion layers), leading to a highly concentrated facial representation (32-dimensional fused deep feature). Finally, the softmax-loss Fig. <ref type="figure">1</ref>. Pipeline of the proposed deep fusion CNN (DF-CNN) based multi-modal 2D+3D FER approach. Each textured 3D face scan is represented as six types of 2D facial geometric and photometric attribute maps (i.e., 3D coordinates based geometry map, normal vectors based normal maps, principle curvatures based curvature map, and texture map). These attribute maps are jointly fed into the feature extraction subnet of DF-CNN with sharing parameters, generating hundreds of multi-channel feature maps. All these feature maps are then fed into the feature fusion subnet (including a reshape and two fusion layers) of DF-CNN, resulting in a highly concentrated facial representation (32-dimensional fused deep feature). Finally, the softmax-loss layer is followed for network training (see section V-A for details). The final expression label prediction is performed by two ways: learning linear SVM classifiers using the 32-dimensional fused deep features or directly performing softmax prediction based on the 6-dimensional probability vectors. or softmax layer is followed for network training or expression prediction (see Section V-A for details).</p><p>For DF-CNN training, considering that there are very limited numbers of textured 3D face scans with expression labels, the feature extraction subnet is initialized using the off-theshelf convolutional layers of a pre-trained deep model (e.g., vgg-net-m). This kind of pre-trained deep models have been proven to have a good generalization ability for generic visual recognition tasks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b37">[38]</ref>. The feature fusion subnet is randomly initialized, and the whole net is trained by the backprorogation algorithm using the softmax-loss function and the stochastic gradient descent (SGD) algorithm.</p><p>For DF-CNN testing, six facial attribute maps of each textured 3D face scan are jointly fed into the feature extraction and feature fusion subnets, generating a highly concentrated facial representation (32-dimensional fused deep feature). This deep feature is further transformed into a 6-dimensional expression probability vector by the final softmax layer. Expression label prediction is preformed by training linear SVM classifiers using the 32-dimensional fused deep features (i.e., DF-CNN svm ) or directly performing softmax prediction based on the 6-dimensional probability vectors (i.e., DF-CNN softmax ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ATTRIBUTE MAPS OF A TEXTURED 3D FACE</head><p>To comprehensively describe the geometric and photometric attributes of a textured 3D face scan, six types of 2D facial attribute maps, namely the geometry map, texture map, three normal maps, as well as normalized curvature map are employed. Given a raw textured 3D face scan, we first run the preprocessing pipeline algorithm (see section VI-A) to generate a 2D texture map I t and a geometry map I g . The coordinates information of each geometry map are then used to estimate the surface normals and curvatures, resulting in three normal component maps I x n , I y n , and I z n , and one normalized curvature (i.e. shape index) map I c . Finally, a textured 3D face scan I can be described by six types of 2D facial attribute maps: I = {I g , I x n , I y n , I z n , I c , I t }, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>. The details for generation of normal maps and curvature map are introduced as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Normal maps</head><p>Given a normalized facial geometry map I g represented by a m × n × 3 matrix:</p><formula xml:id="formula_0">I g = [p ij (x, y, z)] m×n = [p ijk ] m×n×{x,y,z} ,<label>(1)</label></formula><p>where p ij (x, y, z) = (p ijx , p ijy , p ijz ) T , (1 ≤ i ≤ m, 1 ≤ j ≤ n, i, j ∈ Z) represents the 3D coordinates of point p ij . Let its unit normal vector matrix (m × n × 3) be</p><formula xml:id="formula_1">I n = [n(p ij (x, y, z))] m×n = [n ijk ] m×n×{x,y,z} ,<label>(2)</label></formula><p>where n(p ij (x, y, z)) = (n ijx , n ijy , n ijz ) T , (1 ≤ i ≤ m, 1 ≤ j ≤ n, i, j ∈ Z) denotes the unit normal vector of p ij . In this paper, we utilize the local plane fitting method <ref type="bibr" target="#b16">[17]</ref> to estimate I n . That is to say, for each point p ij ∈ I g , its normal vector n(p ij ) can be estimated as the normal vector of the following local fitted plane:</p><formula xml:id="formula_2">S ij : n ijx q ijx + n ijy q ijy + n ijz q ijz = d,<label>(3)</label></formula><p>where (q ijx , q ijy , q ijz ) T represents any point within the local neighborhood of point p ij and d = n ijx p ijx + n ijy p ijy + n ijz p ijz . In this work, a neighborhood of 5 × 5 window is used. To simplify, each normal component in equation ( <ref type="formula" target="#formula_1">2</ref>) can be represented by an m × n matrix:</p><formula xml:id="formula_3">I n =      I x n = [n x ij ] m×n , I y n = [n y ij ] m×n , I z n = [n z ij ] m×n .<label>(4)</label></formula><p>where </p><formula xml:id="formula_4">(n x ij , n y ij , n z ij ) T 2 = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Curvature map</head><p>Similar to the local plane fitting method used for normal estimation, we explored the local cubic fitting method <ref type="bibr" target="#b13">[14]</ref> to estimate the principle curvatures. This method assumes that the local geometry of a surface is approximated by a cubic surface patch. For robustly solving the local fitting problem, both the 3D coordinates and the normal vectors of the neighboring points of the point p ij ∈ I g to be estimated are used. That is, we are fitting the following equations:</p><formula xml:id="formula_5">     z(x, y) = a 2 x 2 + bxy + c 2 y 2 + dx 3 + ex 2 y + f xy 2 + gy 3 z x = ax + by + 3dx 2 + 2exy + f y 2 z y = bx + cy + 3gy 2 + 2f xy + ex 2 .</formula><p>(5) These equations can be solved by the least squares regression, and the shape operator S can be computed as:</p><formula xml:id="formula_6">S =   a b b c   .</formula><p>Then, the eignvalues of S give the two principle curvatures κ 1 and κ 2 at point p ij ∈ I g . The normalized curvatures (i.e., shape index value) at this point is defined by:</p><formula xml:id="formula_7">1 2 - 1 π arctan κ 1 + κ 2 κ 1 -κ 2 .<label>(6)</label></formula><p>Figure <ref type="figure" target="#fig_0">2</ref> shows six types of 2D geometric and photometric facial attribute maps of six textured face scans with six prototypical facial expressions of subject F0001 in the BU-3DFE database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DEEP FUSION CONVOLUTIONAL NEURAL NETWORK</head><p>This section first describes the architecture and training details of DF-CNN. To intuitively highlight the discriminative ability of DF-CNN, both the highly concentrated 32dimensional fused deep features and the expression-specific saliency maps are visualized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DF-CNN: Architecture and Training</head><p>The architecture of DF-CNN is formed by a feature extraction subnet, a feature fusion subnet, and a softmax layer. The feature extraction subnet is used to generate hierarchical and over-completed facial representations (i.e., feature maps) for each type of attribute maps. And the feature fusion subnet is used to combine hundreds of feature maps from different types of attribute maps into a highly concentrated deep feature. The main building blocks of feature extraction subnet include the convolutional layers and ReLU nonlinearity, while the reshape layer and fusion layers are main components of feature fusion subnet. The details of these components are introduced as follows:</p><p>Convolutional Layer and ReLU Nonlinearity. A convolutional layer transforms a 3D volume of activation maps (i.e., feature maps) to another through a set of learnable 3D filters. In particular, input a volume of activation maps of the previous layer</p><formula xml:id="formula_8">Y l-1 ∈ R W l-1 ×H l-1 ×D l-1 , and K l 3D filters {W l k } K l k=1 , each with size W l f × H l f × D l-1 , it outputs a 3D volume of activation maps Y l ∈ R W l ×H l ×D l at layer l.</formula><p>Let the convolutional stride be S, and the amount of zero padding be P , then we have</p><formula xml:id="formula_9">W l = (W l-1 -W l f + 2P )/S + 1, H l = (H l-1 -H l f + 2P )/S + 1, and D l = K l . The k-th 2D activation map Y l k is denoted by Y l k = ϕ(W l k * Y l-1 + b l k ),<label>(7)</label></formula><p>where b l k ∈ R denotes the bias term of k-th filter W l k , * is the convolution operator, and ϕ is the rectified linear units (ReLU): ϕ(x) = max(0, x).</p><p>Reshape Layer. This layer is used to concatenate all the 3D volumes of activation maps produced from all types of 2D facial attribute maps. Suppose DF-CNN has L convolutional layers in total, and acts on N different types of facial attribute maps, then the reshape layer operation is defined as:</p><formula xml:id="formula_10">Y L Re = Reshape({Y L (I i )} N i=1 ) =[Y L (I 1 )|, • • • , |Y L (I N )] ∈ R W L ×H L ×(K L ×N ) ,<label>(8)</label></formula><p>where I i is i-th type of facial attribute maps, and the notation</p><formula xml:id="formula_11">[•|, • • • , |•] denotes the concatenation of 3D matrices along feature channel dimension.</formula><p>Feature Channel Fusion Layer. This is a fully connected layer, which is used to fuse all the activation volumes extracted from all types of facial attribute maps in feature channel dimension. Let this feature channel fusion layer be the (L+1)th layer, and its input be the output of the reshape layer Y L Re , which is fully connected with</p><formula xml:id="formula_12">K L+1 3D filters {W L+1 k } K L+1 k=1 , each with size 1 × 1 × (K L × N ), then the output of this layer is Y L+1 ∈ R W L+1 ×H L+1 ×D L+1 . Here W L+1 = W L , H L+1 = H L , and D L+1 = K L+1 . The k-th 2D activation map Y L+1 k is denoted by Y L+1 k = ϕ(W L+1 k * Y L Re + b L+1 k ),<label>(9)</label></formula><p>where b L+1 k ∈ R denotes the bias term of k-th filter W L+1 k . That is to say, to achieve an activation volume Y L+1 with much smaller number of feature channels, the number of filters K L+1 should be much smaller than the number of feature channels in the previous activation volume Y L Re . Spatial Dimension Fusion Layer. This is also a fully connected layer, which is used to fuse the activation volume Y L+1 in the height-width spatial dimension. Let this fusion layer be the (L + 2)-th layer, and its input be the output volume of feature channel fusion layer Y L+1 , which is fully 1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.  connected with</p><formula xml:id="formula_13">K L+2 3D filters {W L+2 k } K L+2 k=1 , each with size of W L+2 k ∈ R W L+1 ×H L+1 ×K L+1 , then the output activation feature of this layer is Y L+2 ∈ R 1×1×K L+2 . The k-th 2D value Y L+2 k is denoted by Y L+2 k = ϕ(W L+2 k * Y L+1 + b L+2 k ),<label>(10)</label></formula><p>where b L+2 k ∈ R denotes the bias term of k-th filter W L+2 k . Softmax Layer. Given K possible expression classes, the softmax layer has K nodes denoted by p i , where i = 1, 2, • • • , K. p i specifies a discrete probability distribution of expressions, therefore, K i=1 p i = 1. Let Y L+2 be the output of spatial dimension fusion layer, and</p><formula xml:id="formula_14">{W L+3 k ∈ R 1×1×K L+2 } K</formula><p>k=1 be K weights fully connecting spatial dimesion fusion layer to softmax layer. Then the total input into a softmax layer, denoted by Y L+3 , is</p><formula xml:id="formula_15">Y L+3 k = W L+3 k Y L+2 + b L+3 k ∈ R,<label>(11)</label></formula><p>then we have</p><formula xml:id="formula_16">p i = exp(Y L+3 k ) 6 j exp(Y L+3 j ) . (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>The predicted expression class î would be î = arg max i p i .</p><p>In practice, considering that there are very limited numbers of 3D face scans with expression labels, we use the convolutional architecture and parameters of a pre-trained deep CNN model to build and initialize the convolutional layers. In particular, we choose vgg-net-m <ref type="bibr" target="#b3">[4]</ref> as the pre-trained deep model since it performs well and involves moderate amount of parameters. In principle, other pre-trained deep CNN models or newly designed deep CNN models are also possible to be used if enough numbers of training samples are available. The parameters of fusion layers and softmax layer are randomly initialized. The detailed architecture of DF-CNN, including the sizes and numbers of filters and activation maps for each layer, is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>As shown in Figure <ref type="figure" target="#fig_2">3</ref>, DF-CNN comprises five convolutional layers, a reshape layer, two fusion layers, and a softmax layer. Moreover, ReLU neuron is used after all convolutional layers and feature fusion layers. The max pooling layer is used following the first, second, and the fifth convolutional layers. And Local Response Normalization (LRN) layer is used before the first and second pooling layers.</p><p>Each 2D facial attribute map is converted to color scale and resized to 224 × 224 × 3, and then all six types of attribute maps of each textured 3D face scan are jointly fed into feature extraction subnet of DF-CNN, generating six activation volumes, each with size of 6 × 6 × 512. These six activation volumes are concatenated and reshaped into size of 6 × 6 × 3, 072 by reshape layer (Reshape6 in Fig. <ref type="figure" target="#fig_2">3</ref>). The reshaped activation volumes are fused by the following feature channel fusion layer (Fusion7 in Fig. <ref type="figure" target="#fig_2">3</ref>), resulting in an activation volume with size of 6 × 6 × 32. This activation volume is further fused by spatial dimension fusion layer (Fusion8 in Fig. <ref type="figure" target="#fig_2">3</ref>), generating a highly concentrated facial representation (i.e., 32-dimensional fused deep feature). This fusion layer is followed by another fully connected layer, which outputs a 6-dimensional expression probability vector. Finally, a softmax loss layer is used to train all the parameters of DF-CNN based on the back-propagation algorithm.</p><p>During training, the weight decay parameter is set to 5e-4. The learning rate and momentum parameters are set to 1e-4 and 0.9, respectively. The open source implementation MatConvNet<ref type="foot" target="#foot_0">1</ref> is used to build DF-CNN. During testing, expression label of a textured 3D face scan is predicted by two ways: 1) training linear SVM classifiers using the highly concentrated 32-dimensional deep features (i.e., DF-CNN svm ).</p><p>2) performing softmax prediction based on the 6-dimensional vectors of expression probabilities (i.e., DF-CNN softmax ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DF-CNN: Deep Feature Visualization</head><p>To have an intuitive impression and gain insight into the discriminative ability of DF-CNN, we visualize both the "low-level" and "high-level" deep features extracted from the first convolution layer and the last fusion layer of DF-CNN, respectively. Figure <ref type="figure" target="#fig_2">3</ref> shows that there are totally 96 3D filters in the first convolution layer, thus we can generate 96 "low-level" feature maps for each type of facial attribute maps. Figure <ref type="figure">4</ref> illustrates 11 typical feature maps for each type of facial attribute maps of a textured 3D face scan with happiness expression. From this figure, we can see that diverse feature maps can be extracted from DF-CNN using different filters and different attribute maps. Moreover, each feature map looks similar to conventional gradient-like facial maps extracted from the shadow handcrafted features (e.g., LBP and Gabor face maps in <ref type="bibr" target="#b24">[25]</ref>). Such a large number of feature maps can comprehensively capture various expression-related facial shape or texture deformations, of course with very high dimensions. Therefore, how to combine such a large number of over-completed and redundant deep representations into a single compact facial representation becomes the key issue to be solved. Fortunately, DF-CNN is designed to handle this problem, and providing us a high-level, low-dimensional, and high discriminative facial representation.</p><p>To highlight the high discriminative property DF-CNN, Figure <ref type="figure">5</ref> visualizes the clustering structures of t-SNE <ref type="bibr" target="#b50">[51]</ref> based 2-dimensional embedding of handcrafted feature, pre-trained deep feature, and 32-dimensional fused deep feature associated with six prototypical facial expressions. In particular, the same features (i.e., Gabor, vgg-net-m-conv5, and 32-dimensional fused deep feature) are used as those in Section VI-B. Notice that for Gabor and vgg-net-m-conv5, the features of different attribute maps are concatenated together (i.e., feature-level fusion) to generate a single high-dimensional representation of each textured 3D face scan. The feature dimensions of Gabor and vgg-net-m-conv5 are 40,320 (6 attribute maps, each one is described as a 6,720-dimensional Gabor feature) and 110,592 (6 attribute maps, each one is described as feature maps with size 6 × 6 × 512 ), respectively. Figure <ref type="figure">5</ref> shows that the 32-dimensional fused deep feature has an obvious clustering structure for different expression categories, while other two types of features demonstrate large category-wised overlapping. This clearly indicates that the 32-dimensional fused deep features learned by DF-CNN has more discriminative power to distinguish different expressions than handcrafted feature Gabor and pre-trained deep feature vgg-net-m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DF-CNN: Saliency Map Visualization</head><p>Since different facial expressions relate to different ways of local facial shape deformations, the importance weights of different facial parts are generally quite different for expression predicting as shown in <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b68">[69]</ref>. In this section, we show that the importance weights can be revealed by pixel-level expression related saliency maps of DF-CNN.</p><p>To this end, we visualize the importance of each pixel for its final discrimination ability of different facial expressions. For example, for "happiness", we visualize the saliency map for a textured 3D face by the importance of each image pixel contributing to the final discrimination of "happiness". To compute the saliency map of a textured 3D face scan I Λ = {I g , I x n , I y n , I z n , I c , I t } w.r.t. an expression indexed by e, we construct a score function for assigning this face to expression e by:</p><formula xml:id="formula_19">S(I Λ |e, Θ) = w e T f (I Λ , Θ), (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where Θ is the set of learned parameters (i.e., filters and biases) of DF-CNN, f (I Λ , Θ) is the 32-dimensional fused </p><p>where x denotes any pixel of an attribute map. ∂f (IΛ,Θ)</p><formula xml:id="formula_22">∂I(x)</formula><p>is the gradient of fused deep feature w.r.t. the attribute map I at pixel x, which can be computed by the back-propagation algorithm of DF-CNN from the spatial dimension fusion layer. Its absolute value |G(x|I Λ , e, Θ)| measures the importance of pixel x in labeling I as expression e. We call this term computed over all pixels of all facial attribute maps of a textured 3D face scan as saliency map.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> visualizes some examples of saliency maps for different expressions. The saliency map is re-scaled to [0, 1]. We visualize it by fusing the face texture map with a dark blue background using the saliency map as weights. The less important pixels are shown in dark blue in these maps. We observe some interesting phenomena from these maps. First, mouth is the most salient facial part for discriminating all these expressions of interest, particularly for sadness and surprise. Second, the distributions of those salient maps for all expressions are approximately consistent with the patterns of facial shape deformations, which may spread over the whole faces with different importance. These observations indicate that the proposed DF-CNN can provide a discriminative facial representation and can distinguish facial expressions using the discriminative facial parts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL EVALUATION</head><p>To evaluate the effectiveness of DF-CNN for multi-modal 2D+3D FER, we will compare its performance with popular handcrafted features, pre-trained deep features, fine-tuned deep features, and state-of-the-art methods over three expression subsets of two 3D face datasets (i.e., BU-3DFE and Bosphorus). Finally, we will discuss the issues of feature extraction with or without parameter sharing, effectiveness of learningbased fusion, and optimality of linear SVM based expression prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Databases and Preprocessing</head><p>BU-3DFE Database. The BU-3DFE (Binghamton University 3D Facial Expression) Database <ref type="bibr" target="#b58">[59]</ref> has been the benchmarking for static 3D FER <ref type="bibr" target="#b11">[12]</ref>. It includes 100 subjects (56 females and 44 males), with age ranging from 18 to 70 years old, and with a variety of racial ancestries (e.g., White, Black, East-Asian). Each subject has 25 samples of seven expressions: one sample for neutral, and other 24 samples for six prototypical expressions (anger, disgust, fear, happiness, sadness, and surprise), each includes four levels of intensity (see Fig. <ref type="figure" target="#fig_5">7</ref>). As a result, this database consists of 2,500 2D texture images and 2,500 geometric shape models. To fairly compare DF-CNN with state-of-the-art methods, and to validate the effectiveness of DF-CNN for samples with lower levels of expression intensity, the following two subsets are used.</p><p>• BU-3DFE Subset I. This subset is the standard dataset used for 3D FER. It contains attribute maps) of 100 subjects with 6 prototypical expressions of four levels of intensity. To our knowledge, the samples with lower levels of expression intensity have not been used for 3D FER. Bosphorus 3D Face Database. The Bosphorus 3D Face Database <ref type="bibr" target="#b40">[41]</ref> has been widely used for 3D face recognition under adverse conditions, 3D facial action unit detection, 3D facial landmarking, etc. It contains 105 subjects and 4,666 pairs of 3D face models and 2D face images with different action units, facial expressions, poses and occlusions. In this dataset, there are totally 65 subjects performing the six prototypical expressions with near frontal view. Each person has only one 2D (or 3D) sample for each expression, resulting in 390 2D and 3D face pairs. To better partition, we use the following subset for experimental evaluations.</p><p>• Bosphorus Subset. It contains 360 2D and 3D face pairs (i.e., 2,160 facial attribute maps) of 60 subjects with 6 prototypical expressions. Preprocessing. We performed similar preprocessing for both BU-3DFE subsets and Bosphorus subset. First, we used the Iterative Closest Point algorithm for 3D face registration. Then, we performed nose detection, face cropping, re-sampling, and projection procedures using the 3D face normalization method proposed in <ref type="bibr" target="#b33">[34]</ref>. Finally, we achieved the normalized 2D range images (i.e., geometry maps) with x, y, and z coordinates. Once we have geometry maps, other geometric facial attribute maps can be estimated according to the method introduced in Section IV. The 2D texture maps of BU-3DFE dataset are generated by projecting 3D texture images with linear interpolation. Samples of preprocessed facial attribute maps of BU-3DFE database are shown in Fig. <ref type="figure" target="#fig_0">2</ref>. And Figure <ref type="figure" target="#fig_6">8</ref> illustrates some samples of 2D texture images of Bosphorus subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation and comparison on BU-3DFE Subset I</head><p>Experimental Protocol. This experimental protocol is firstly used in <ref type="bibr" target="#b14">[15]</ref> and has been proven to be more stable than the one used in <ref type="bibr" target="#b52">[53]</ref>. In this protocol, 60 subjects, each with 12 samples (i.e., 6 prototypical expressions with two higher levels of intensity) are randomly selected from the BU-3DFE subset I. That is to say, 720 textured 3D face scans (i.e., 4,320 2D facial attribute maps) are used. To achieve stable results, 1,000 times random and independent 54-versus-6-subject-partition experiments (1,000 times train and test sessions in total) are performed. For each partition, 648 textured 3D face scans of 54 subjects are used for training and 72 textured 3D face scans of 6 subjects are used for testing. Different partitions are independently trained and tested, and the average expression recognition accuracy of all the 1,000 test sessions across all 6 prototypical expressions are reported for the final evaluation.</p><p>In particular, we use the remaining 40 subjects (i.e., 2,880 2D facial attribute maps) of BU-3DFE Subset I to train our DF-CNN. Once DF-CNN is trained, it is then used to extract the 32-dimensional fused deep features of the other 60 subjects. These fused deep features are then used to train linear SVM classifiers for expression prediction using above 1,000 times 54-versus-6-subject-partition experiments (i.e., DF-CNN svm ). This paragraph compares the performance of DF-CNN with the ones achieved by using handcrafted features. Four classical handcrafted image features: MS-LBP, dense-SIFT, HOG, and Gabor, which have been proven to be quite efficient for both 2D and 3D facial expression analysis, are employed for comparisons. Please refer to <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and <ref type="bibr" target="#b24">[25]</ref>, respectively for the implementations of these features. When used for multimodal 2D+3D FER, these features are first extracted from each type of facial attribute maps, then respectively fed into linear SVM<ref type="foot" target="#foot_2">2</ref> classifier with default parameter of C. To achieve final results, score-level fusion of SVM scores with sum rule is used.</p><p>Table <ref type="table">II</ref> shows the average expression recognition accuracies across all six expressions of four handcrafted features, and the proposed DF-CNN on BU-3DFE subset I. From Table <ref type="table">II</ref>, we can conclude that: 1) Gabor and HOG generally perform better than dense-SIFT and MS-LBP. In particular, Gabor achieves the highest fusion accuracy of 84.72%, which outperforms HOG, dense-SIFT, and MS-LBP by 0.98%, 1.56%, and 2.98%, 2) For different facial attribute maps, normal maps (I x n , I y n , and I z n ) generally perform better than others, and the fusion of all six attribute (i.e., All) achieves the best performance. These results indicate that different facial attribute maps indeed contain large complementary information for multi-modal 2D+3D FER. 3) DF-CNN svm and DF-CNN softmax achieves similar and much better results (86.86% vs. 86.20%) than handcrafted features.</p><p>2) Comparison with pre-trained deep features: This paragraph compares the performance of DF-CNN with the ones achieved by using deep features extracted from three deep models (i.e., caffe-alex, vgg-net-m, and vgg-net-16) pre-trained on the ImageNet database <ref type="bibr" target="#b3">[4]</ref>. Notice that the convolutional layers of vgg-net-m is used to initialize our DF-CNN. Similar to the case of handcrafted features, each type of facial attribute maps are separately fed into these pre-trained models to    2) For the fusion scores, vggnet-m-conv5 and caffe-alex-full7 achieve slightly better results than others among pre-trained deep features. 3) The deep features extracted from convolutional layers (i.e., conv5) of pre-trained deep models generally perform much better than the ones extracted from fully connected layers (i.e., full7). 4) Our method achieves consistently better results than all pretrained deep features. Notice that the dimension of vgg-net-m-conv5 for one type of facial attribute maps is 18,432, which is much higher than the 32-dimensional fused deep feature produced by DF-CNN.</p><p>Table <ref type="table" target="#tab_5">IV</ref> compares the average confusion matrices achieved by Gabor feature, vgg-net-m-conv5 and DF-CNN svm . It can be seen that DF-CNN svm outperforms Gabor for all expressions except anger (with a difference of 3.45%). It is worth noting that DF-CNN svm has more powerful discriminative ability to distinguish fear expression, promoting the accuracy upto 13.26% and 12.83% for Gabor feature and vgg-net-m-conv5.</p><p>3) Comparison with fine-tuned deep models: To further demonstrate the effectiveness of DF-CNN, we also compared it with fine-tuned deep models. The same pre-trained deep models: caffe-alex, vgg-net-m, and vgg-net-16 are used for fine-tuning. For each pre-trained deep model, we keep the net architecture of all layers and parameters unchanged except the final fully connected layer. In particular, since we have six expression classes, the filter weight with size of 1×1×4096× 1000 is changed to 1 × 1 × 4096 × 6 and randomly initialized, and the corresponding 1000-dimensional bias vector is also replaced by a 6-dimensional zero vector. Then, we separately fine-tune the pre-trained deep models using different facial attribute maps, resulting in six fine-tuned deep models for each pre-trained deep model. Finally, testing data associated with each kind of attribute maps are fed into the corresponding fine-tuned deep model for feature extraction. Similar to DF-CNN, expression prediction for each kind of attribute maps is achieved by the following two ways: 1) learning linear SVM classifiers using the 4,096-dimensional fine-tuned deep features (e.g., vgg-net-m-ft-full7 svm ); 2) performing softmax 1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE Transactions on Multimedia prediction using the 6-dimensional fine-tuned deep features of expression probabilities (e.g., vgg-net-m-ft softmax ). To achieve fusion results, score-level fusion with sum rule are used for both cases. Table <ref type="table" target="#tab_5">V</ref> shows the average expression recognition accuracies of fine-tuned deep features and DF-CNN on BU-3DFE subset I. From Table <ref type="table" target="#tab_5">V</ref>, we can find that: 1) Fusion of multiple facial attribute maps can also significantly improve the accuracies for all fine-tuned deep features. 2) Fine-tuned deep feature vgg-net-16 achieves significantly better results for texture maps, and also achieves the highest accuracies (86.01% and 85.14%) for both two prediction ways. This conclusion of deeper net performs better is consistent with the one in <ref type="bibr" target="#b3">[4]</ref>. 3) Our DF-CNN initialized by vgg-net-m still achieves the best results. It's necessary to compare the results of Table <ref type="table" target="#tab_3">III</ref> and Table V. It's easy to find that significant improvements have been achieved from pre-trained to fine-tuned deep features, particularly for the case of 4096-dimensional deep features extracted from the penultimate fully connected layer (i.e., full7). For example, the improvements are upto 16.52% for curvature maps and 17.07% for texture maps when considering the pre-trained and fine-tuned vgg-net-16-full7.</p><p>Table VI compares the average confusion matrices achieved by two fine-tuned deep features: vgg-net-m-ft-full7 svm , vgg-net-16-ft-full7 svm , and our DF-CNN svm . It's not difficult to see that DF-CNN svm achieves consistent better results than vgg-net-mft-full7 svm for all six expressions. It even achieves better results than vgg-net-16-ft-full7 svm , which is fine-tuned from a much deeper pre-trained deep model. In particular, the superiority for fear expression is upto 4.86% .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Comparison with other methods:</head><p>To comprehensively evaluate the effectiveness of DF-CNN, we compared it with 18 state-of-the-art methods on BU-3DFE subset I. To give a thoroughly analysis, four aspects, including the data modality, expression feature, expression classifier, and recognition accuracy are compared in Table <ref type="table" target="#tab_8">VII</ref>.</p><p>1) For data modality, we can see that all previous methods reported their results using only 3D data exception of <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b65">[66]</ref>. It is worth noting that Li et al. <ref type="bibr" target="#b23">[24]</ref> proposed a local feature-based multimodal 2D+3D FER method, and studied the complementarity between 2D and 3D features. However, their fusion results were produced by handcrafted feature-level and score-level fusion schemes. In contrast, our method can automatically combine different 3D geometric and 2D photometric maps into a single 32-dimensional fused deep feature.</p><p>2) For expression feature, one way is directly building histograms of surface geometric quantities, such as coordinates (e.g., <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>), normals (e.g., <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b66">[67]</ref>), and curvatures (e.g., <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>). Another way is extracting popular handcrafted features (e.g., HOG, SIFT, LBP, DWT) from depth maps (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b56">[57]</ref>), normal maps (e.g., <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b21">[22]</ref>), or curvature maps (e.g., <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b62">[63]</ref>). As mentioned in our introduction section, all these state-ofthe-art works for 3D-FER are based on handcrafted expression features. In contrast, our method can learn highly concentrated and discriminative facial representation (only 32-dimensional) from six types of facial attribute maps.</p><p>3) For expression classifier, SVM (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b25">[26]</ref>) is the most popular classifier compared with others such as Neural Networks (NN), Maximal Likelihood (ML), Bayesian Belief Net (BBN), multi-boosting, and Sparse Representation-based Classifier (SRC). It is worth noting that a majority of methods are based on SVM classifier with non-linear RBF kernel (e.g., <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b66">[67]</ref>) or using multiple kernel learning <ref type="bibr" target="#b22">[23]</ref> to combine multiple high-dimensional features (e.g., normal-LBP in <ref type="bibr" target="#b22">[23]</ref>), while our results are based on linear SVM classifier with default parameter.</p><p>4) For recognition accuracy, benefiting from the end-toend training framework of DF-CNN, the fused deep features produced by DF-CNN have strong discriminative ability to distinguish different expressions. In particular, our method (DF-CNN svm ) achieves the highest accuracy of 86.86% compared with all state-of-the-art methods using the same (or very similar <ref type="bibr" target="#b0">[1]</ref>) experimental protocol. Notice that both the experimental protocol used in our paper <ref type="bibr" target="#b14">[15]</ref> and the similar one used in <ref type="bibr" target="#b0">[1]</ref> have been proved stable since the scores are achieved by averaging 100 times independent 10-fold crossvalidation tests. It should be pointed out that directly comparisons of the two accuracy columns in Table VII are far from fair since the results listed in the second column were achieved based on an unstable experimental protocol (i.e., 10-fold or 20-fold cross-validation) firstly used in <ref type="bibr" target="#b52">[53]</ref>. For example, the  accuracy of <ref type="bibr" target="#b32">[33]</ref> is reduced from 98.81% to 92.75% when using 20-fold instead of 10-fold cross-validation. As produced by Gong et al. <ref type="bibr" target="#b14">[15]</ref>, the accuracies of <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b45">[46]</ref> were dropped significantly (more than 20%) when using a more stable experimental protocol. Overall, different from stateof-the-art methods, the proposed DF-CNN combines feature learning and fusion learning into a single end-to-end training framework, and achieves the best accuracy for multimodal 2D+3D FER under the more stable experimental protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation and comparison on other datasets</head><p>This section will show more experimental results evaluated on BU-3DFE subset II and Bosphorus subset.</p><p>Experimental Protocol. To get more training data and to reduce the effect of data bias for DF-CNN training, we used the standard 10-fold cross-validation (10 train and test sessions) experimental setting. That is, different DF-CNNs should be trained for different sessions, and the average recognition accuracies of 10 different DF-CNNs across all six prototypical expressions are reported for evaluations and comparisons. In we can conclude that: 1) As before, Gabor feature still achieves the best results among handcrafted features. It even slightly outperforms the pre-trained deep feature vgg-net-m-conv5 (80% vs. 79.75%). 2) Fine-tuned deep features achieve significantly better results than pre-trained deep features, e.g., 81.08% for vgg-net-m-ft-full7 vs. 77.38% for vgg-net-m-full7. 3) Our DF-CNN based methods achieve comparable (81.04% vs. 81.08%) or slightly better (81.33% vs. 81.08%) results compared with fine-tuned deep features. It is worth noting that for each train and test session, DF-CNN only needs to train a single CNN for both feature learning and feature fusion, while fine-tuned deep feature based method needs to respectively train different deep models for different types of facial attribute maps, and respectively extract fine-tuned deep features from different deep models and combine all scores by hand. This leads to much more consumptions of training time and parameter space compared with DF-CNN. Notice that the results of two BU-3DFE subsets clearly indicate that the samples with lower levels of expression intensity are indeed much more difficult to be recognized than the higher level ones.</p><p>2) Results on Bosphorus subset: Table IX reports the performance comparisons of the proposed DF-CNN with handcrafted features, pre-trained deep features, and fine-tuned deep features on Bosphorus subset. Similar to the conclusions achieved on BU-3DFE subset I and subset II, we have: 1) Gabor feature achieves the highest accuracy of 77.50% among handcrafted features. 2) Fine-tuned deep feature (i.e., vgg-  net-m-ft-full7) also significantly outperforms the pre-trained one (i.e., vgg-net-m-full7). Note that although the pre-trained deep feature vgg-net-m-conv5 achieves the same accuracy of 79.72% as the fine-tuned deep feature vgg-net-m-ft softmax , the feature dimension is much higher (18, 432 × 6 vs. 6). 3) Our DF-CNN based methods achieve slightly better results compared with the fine-tuned deep features. Overall, Bosphorus subset is the most difficult dataset among the three subsets used in this paper.</p><p>3) Comparison with other methods: To compare the performance of the proposed DF-CNN with other methods on BU-3DFE subset II Bosphorus subset, we reproduced three state-of-the-art methods (i.e., <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and <ref type="bibr" target="#b56">[57]</ref>) on these two datasets using the same experimental protocol (i.e., 10-fold cross-validation with the same subjects for training and testing in each train and test session) as DF-CNN. In particular, <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b23">[24]</ref> are two of our previous methods.</p><p>Results of <ref type="bibr" target="#b56">[57]</ref> were reproduced using the code shared by the authors. Notice that multiple kernel learning was used in <ref type="bibr" target="#b22">[23]</ref>, non-linear SVM was used in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b56">[57]</ref> for expression prediction, respectively. For fair comparison, the non-linear SVM was replaced by linear SVM classifier, and the sum rule based score-level fusion was used for <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b56">[57]</ref>. Table X reports the performance comparisons of DF-CNN with state-of-the-art methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b56">[57]</ref> on both BU-3DFE subset II and Bosphorus subset. From this table, we can see that method <ref type="bibr" target="#b22">[23]</ref> achieves the lowest accuracy on both subsets. Methods <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b56">[57]</ref> achieve very similar results (80.42% vs. 80.46%) on BU-3DFE subset II, while method <ref type="bibr" target="#b23">[24]</ref> performs better by 2.22% on Bosphorus subset. Our DF-CNN achieves the best results on both two subsets. Similar to the case on BU-3DFE subset I, DF-CNN has significant superiority to distinguish fear expression. For example, on the Bosphorus subset, DF-CNN svm achieves an average recognition rate of 65% for fear expression, which is much higher than the results of 36.67%, 51.67%, and 43.33% achieved by <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and <ref type="bibr" target="#b56">[57]</ref>, respectively. It is worth noting that distinguishing the samples of Bosphorus subset with fear expression and surprise expression is a very difficult task even for humans as illustrated in Fig. <ref type="figure" target="#fig_6">8</ref>. From this figure, we can see that there only exist very subtle differences between fear and surprise pairs of the same person.</p><p>Overall, the proposed DF-CNN unifies feature learning and fusion learning into a single end-to-end training framework, and performs better than handcrafted features, pre-trained deep features, fine-tuned deep features, and state-of-the-art methods, resulting in a good generalization ability on BU-3DFE subset II and Bosphorus subset for multimodal 2D+3D FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>To further validate the effectiveness of DF-CNN, three issues: feature extraction with or without parameter sharing, effectiveness of learning-based fusion, and optimality of linear SVM based expression prediction are discussed in this paragraph. Noting that all the following discussions are based on BU-3DFE subset I and the corresponding experimental protocol introduced in section VI-B.</p><p>1) Feature extraction with or without parameter sharing: As shown in Fig. <ref type="figure">1</ref>, the CNN parameters are shared for different types of facial attribute maps in the feature extraction subnet of DF-CNN. Alternatively, different attribute maps can also been separately fed into different CNNs for feature fusion, then adding the following feature fusion and expression prediction layers. Clearly, the latter one (namely a ) needs to learn more parameters and thus perhaps performs better. In    <ref type="table" target="#tab_14">XII</ref>, we can see that MKL with kernellevel fusion achieves better results than liner SVM with scorelevel fusion in both cases. Our fine-tuned DF-CNN, which combines feature learning and fusion learning in a single endto-end training framework, achieves significant better results than liner SVM and MKL.</p><p>Moreover, to see the effect of feature fusion subnet, we fixed all the initialized CNN parameters of the feature extraction subnet, and only learned the parameters of the following feature fusion subnet. This is equivalent to learn hierarchical fusion weights to combine the high-dimensional pre-trained deep features. From Table <ref type="table" target="#tab_14">XII</ref>, we can see that this kind of pure fusion learning-based DF-CNN can achieve slightly better results (84.79% vs. 84.22%) than linear SVM with handcrafted score-level fusion, but significantly worse than the proposed DF-CNN. This indicates that the combination of feature learning and fusion learning into a single end-toend training framework is very important for the proposed DF-CNN. 3) Optimality of Linear SVM based prediction: To validate the optimality of using linear SVM classifier for expression prediction, we compared it with five popular clas-sifiers: logistic regression, k-Nearest Neighbor, naive bayes, random forests, and rbf-kernel SVM. All experiments were carried out on BU-3DFE subset I based on the 1,000 times 54vs-6 experimental setting, and using the 32-dimensional fused deep features produced by DF-CNN. The hyper-parameters of these classifiers (e.g., the value of k in k-Nearest Neighbor, number of trees in random forests, and γ in rbf-kernel SVM) were carefully selected by cross-validation on the training set of each train session. In contrast, the parameter in linear SVM was set to be the default value 1 for all 1,000 times train sessions.</p><p>Table XIII reports the comparison results. We can see that: 1) All classifiers achieve comparable results except logistic regression, which indicates again that the 32-dimensional fused deep feature is very discriminative. 2) Among all classifiers, linear SVM has obvious advantages in both accuracy and speed (without parameter tuning). Therefore, linear SVM is generally the best candidate classifier for expression prediction using fused deep features produced by DF-CNN. Finally, it is worth noting that we have also studied the issue of optimal dimension for the fused deep feature produced by DF-CNN. Our experimental results indicate that the 32dimensional fused deep feature can achieve slightly better results than both 16-dimensional and 64-dimensional fused deep features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORK</head><p>This paper presents a novel deep fusion convolution neural network (DF-CNN) for subject-independent multi-modal 2D+3D FER. DF-CNN comprises a feature extraction subnet, a feature fusion subnet, and a softmax-loss layer. Each textured 3D face scan is firstly represented as six types of facial attribute maps, all of which are then jointly fed into DF-CNN for feature extraction and feature fusion, resulting in a highly concentrated facial representation. Expression prediction is performed by two ways: 1) learning linear SVM classifiers using the 32-dimensional fused deep features; 2) directly performing softmax prediction using the 6-dimensional expression probabilities. Different from existing methods for 3D FER, DF-CNN combines feature learning and fusion learning into a single end-to-end training framework. To demonstrate the effectiveness of DF-CNN, we conducted comprehensive experiments to compare the performance of DF-CNN with handcrafted features, pre-trained deep features, fine-tuned deep features, and the state-of-the-art methods on three subsets of two popular 3D face datasets (i.e., BU-3DFE and Bosphorus). In all cases, DF-CNN consistently achieves the best results. In the future, some other issues of DF-CNN such as how to choose the optimal pre-trained deep CNN for initialization, and the optimal loss function for training will be studied. Moreover, we will also study to extend current DF-CNN framework to multi-modal 2D+3D video based facial expression recognition, or other multi-modal facial emotion prediction problems such as action unit detection and expression intensity estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the six types of 2D geometric and photometric facial attribute maps of six textured 3D face scans (subject F0001 in the BU-3DFE dataset) with six prototypical facial expressions (i.e., anger, disgust, fear, happiness, sadness, and surprise). The left hand column shows: the geometry maps, texture maps, and curvature maps, and the three normal maps (components x, y, and z) are shown at the right hand column.</figDesc><graphic coords="5,48.96,56.07,251.05,87.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE Transactions on Multimedia</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of the proposed deep fusion convolutional neural network (DF-CNN). Six types of facial attribute maps of a textured 3D face model are jointly fed into five feature convolutional layers (convolution + ReLU + Pooling), a reshape layer, a feature channel fusion layer, a spatial dimension fusion layer, and a final softmax layer. The sizes and numbers of input data, feature maps and filters are listed for each layer.</figDesc><graphic coords="6,48.96,56.07,514.05,74.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Visualization of 11 typical feature maps of geometric and photometric facial attribute maps extracted from the first convolution layer of DF-CNN.From top to bottom are the feature maps for the geometry map, texture map, curvature map, and normal maps with components x, y, and z.</figDesc><graphic coords="7,100.37,56.07,411.25,209.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visualization of the DF-CNN based facial expression saliency maps. From top to bottom rows: saliency maps for anger, disgust, fear, happiness, sadness and surprise. The less important pixels are shown in dark blue.deep feature of I Λ , and w e is the weight of a trained SVM classifier for expression e using f (I Λ , Θ). Obviously, the higher value of w e T f (I Λ , Θ) implies higher confidence in labeling this textured 3D face as expression e. We next compute the gradient of score function in Eqn. (14) w.r.t. the input pixels:</figDesc><graphic coords="8,48.96,56.06,251.06,258.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Samples of 2D texture maps of BU-3DFE database with different genders, ethnicities, ages, expressions (from left to right, anger, disgust, fear, happiness, sadness and surprise) and levels of expression intensity (from top to bottom: level 1 to level 4).</figDesc><graphic coords="8,324.53,56.07,225.95,190.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Six pairs of 2D texture images with fear and surprise expressions of Bosphorus database. It's not easy even for humans to distinguish these fear and surprise pairs illustrated in this figure.</figDesc><graphic coords="13,48.96,56.07,251.06,126.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 )</head><label>2</label><figDesc>Effectiveness of learning-based fusion: To show the effectiveness of learning-based fusion, we compared DF-CNN with two classifiers: linear SVM with score-level fusion and multiple kernel learning (MKL) with kernel-level fusion. Deep CNN features DF-CNN-in-conv5 and DF-CNNft-conv5 are respectively extracted from the feature extraction subnet of DF-CNN with initialized and fine-tuned CNN parameters. From Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Both visualization and quantification results indicate that the 32-dimensional fused deep feature of DF-CNN has strong discriminative ability to distinguish different facial expressions. 1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE Transactions on Multimedia</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,100.37,56.06,411.24,170.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE Transactions on Multimedia</figDesc><table /><note><p>1,200 2D and 3D face pairs (i.e., 7,200 2D facial attribute maps) of 100 subjects with 6 prototypical expressions and two higher levels of expression intensity. • BU-3DFE Subset II. This subset includes all samples of BU-3DFE except the 100 neutral samples. It contains 2,400 2D and 3D face pairs (i.e., 14,400 2D facial 1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE Transactions on Multimedia</figDesc><table><row><cell>10</cell></row></table><note><p>1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF THE AVERAGE ACCURACIES WITH PRE-TRAINED DEEP FEATURES ON BU-3DFE SUBSET I.</figDesc><table><row><cell>Method</cell><cell>Ig</cell><cell>I x n</cell><cell>I y n</cell><cell>I z n</cell><cell>Ic</cell><cell>It</cell><cell>All</cell></row><row><cell>caffe-alex-conv5</cell><cell cols="7">77.53 78.87 81.50 78.71 80.83 81.40 83.74</cell></row><row><cell>vgg-net-m-conv5</cell><cell cols="7">80.38 80.37 81.68 81.23 79.23 82.14 84.22</cell></row><row><cell cols="8">vgg-net-16-conv5-3 81.72 78.55 83.06 81.25 76.95 78.46 83.78</cell></row><row><cell>caffe-alex-full7</cell><cell cols="7">68.64 73.43 76.64 75.72 74.52 74.45 82.56</cell></row><row><cell>vgg-net-m-full7</cell><cell cols="7">73.34 74.99 77.51 76.77 68.81 70.93 81.56</cell></row><row><cell>vgg-net-16-full7</cell><cell cols="7">76.71 72.22 73.87 74.61 64.35 67.03 82.45</cell></row><row><cell>DF-CNNsvm</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.86</cell></row><row><cell>DF-CNN softmax</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF THE AVERAGE ACCURACIES WITH FINE-TUNED DEEP FEATURES ON BU-3DFE SUBSET I.</figDesc><table><row><cell>Method</cell><cell>Ig</cell><cell>I x n</cell><cell>I y n</cell><cell>I z n</cell><cell>Ic</cell><cell>It</cell><cell>All</cell></row><row><cell>caffe-alex-ft-full7svm</cell><cell cols="7">79.44 79.84 80.51 79.50 79.46 80.83 84.05</cell></row><row><cell cols="8">vgg-net-m-ft-full7svm 79.68 82.85 82.15 80.30 82.01 81.62 84.85</cell></row><row><cell cols="8">vgg-net-16-ft-full7svm 80.21 82.30 82.04 80.43 80.87 84.10 86.01</cell></row><row><cell>caffe-alex-ft softmax</cell><cell cols="7">78.19 80.96 81.94 78.75 78.89 80.83 83.61</cell></row><row><cell>vgg-net-m-ft softmax</cell><cell cols="7">78.33 83.06 82.78 81.11 81.11 80.42 85.00</cell></row><row><cell>vgg-net-16-ft softmax</cell><cell cols="7">78.33 82.08 80.69 79.19 79.31 84.17 85.14</cell></row><row><cell>DF-CNNsvm</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.86</cell></row><row><cell>DF-CNN softmax</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.20</cell></row><row><cell cols="8">subset I. From Table III, we can find that: 1) Different pre-</cell></row><row><cell cols="8">trained deep features have different superiorities associated</cell></row><row><cell cols="8">with different facial attribute maps. For example, vgg-net-</cell></row><row><cell cols="8">16-conv5-3 achieves the best score for I y n , while vgg-net-m-conv5 performs best for I x n .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF THE AVERAGE CONFUSION MATRICES WITH FINE-TUNED DEEP MODEL FOR ALL FACIAL ATTRIBUTE MAPS ON BU-3DFE SUBSET I.</figDesc><table><row><cell></cell><cell cols="5">vgg-net-m-ft-full7svm (average accuracy = 84.85)</cell><cell></cell></row><row><cell>%</cell><cell>AN</cell><cell>DI</cell><cell>FE</cell><cell>HA</cell><cell>SA</cell><cell>SU</cell></row><row><cell>AN</cell><cell>81.48</cell><cell>1.92</cell><cell>0.83</cell><cell>0</cell><cell>15.77</cell><cell>0</cell></row><row><cell>DI</cell><cell>1.95</cell><cell>81.91</cell><cell>8.58</cell><cell>3.14</cell><cell>0</cell><cell>4.42</cell></row><row><cell>FE</cell><cell>3.42</cell><cell>6.96</cell><cell>73.51</cell><cell>11.57</cell><cell>1.99</cell><cell>2.56</cell></row><row><cell>HA</cell><cell>0</cell><cell>0.77</cell><cell>3.48</cell><cell>95.74</cell><cell>0</cell><cell>0</cell></row><row><cell>SA</cell><cell>15.88</cell><cell>0.19</cell><cell>4.18</cell><cell>0</cell><cell>79.75</cell><cell>0</cell></row><row><cell>SU</cell><cell>0</cell><cell>0.83</cell><cell>1.67</cell><cell>0</cell><cell>0.78</cell><cell>96.72</cell></row><row><cell></cell><cell cols="5">vgg-net-16-ft-full7svm (average accuracy = 86.01)</cell><cell></cell></row><row><cell>%</cell><cell>AN</cell><cell>DI</cell><cell>FE</cell><cell>HA</cell><cell>SA</cell><cell>SU</cell></row><row><cell>AN</cell><cell>86.19</cell><cell>2.52</cell><cell>0.83</cell><cell>0</cell><cell>10.45</cell><cell>0</cell></row><row><cell>DI</cell><cell>1.97</cell><cell>82.00</cell><cell>9.59</cell><cell>2.27</cell><cell>0</cell><cell>4.17</cell></row><row><cell>FE</cell><cell>2.73</cell><cell>7.42</cell><cell>74.38</cell><cell>12.99</cell><cell>0.79</cell><cell>1.68</cell></row><row><cell>HA</cell><cell>0</cell><cell>0.83</cell><cell>3.47</cell><cell>95.69</cell><cell>0</cell><cell>0</cell></row><row><cell>SA</cell><cell>16.21</cell><cell>0</cell><cell>4.29</cell><cell>0</cell><cell>79.50</cell><cell>0</cell></row><row><cell>SU</cell><cell>0</cell><cell>0</cell><cell>1.67</cell><cell>0</cell><cell>0.06</cell><cell>98.27</cell></row><row><cell></cell><cell cols="5">DF-CNNsvm (average accuracy = 86.86)</cell><cell></cell></row><row><cell>%</cell><cell>AN</cell><cell>DI</cell><cell>FE</cell><cell>HA</cell><cell>SA</cell><cell>SU</cell></row><row><cell>AN</cell><cell>82.08</cell><cell>3.60</cell><cell>2.42</cell><cell>0</cell><cell>11.90</cell><cell>0</cell></row><row><cell>DI</cell><cell>3.27</cell><cell>84.94</cell><cell>5.70</cell><cell>2.50</cell><cell>0</cell><cell>3.59</cell></row><row><cell>FE</cell><cell>1.84</cell><cell>5.28</cell><cell>79.24</cell><cell>8.33</cell><cell>0.81</cell><cell>4.50</cell></row><row><cell>HA</cell><cell>0</cell><cell>0</cell><cell>3.74</cell><cell>96.26</cell><cell>0</cell><cell>0</cell></row><row><cell>SA</cell><cell>12.63</cell><cell>0.10</cell><cell>5.56</cell><cell>0.53</cell><cell>81.18</cell><cell>0</cell></row><row><cell>SU</cell><cell>0</cell><cell>0.07</cell><cell>1.67</cell><cell>0</cell><cell>0.83</cell><cell>97.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE Transactions on Multimedia</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>OF EXPRESSION FEATURES, CLASSIFIERS, AND ACCURACIES WITH THE STATE-OF-THE-ART ON BU-3DFE SUBSET I (NOTICE THAT THE ACCURACIES IN THE LEFT COLUMN ARE ACHIEVED BY AVERAGING 100 ROUND INDEPENDENT 10-FOLD CROSS-VALIDATION TESTS, WHILE THE ONES IN THE RIGHT COLUMN ARE ACHIEVED BY AVERAGING ONLY ONE OR TWO ROUND 10-FOLD CROSS-VALIDATION TESTS).</figDesc><table><row><cell>Methods</cell><cell>Data</cell><cell>Feature</cell><cell>Classifier</cell><cell cols="2">Accuracy</cell></row><row><cell>Wang et al. [53]</cell><cell>3D</cell><cell>curvatures/hist.</cell><cell>LDA</cell><cell cols="2">61.79 83.60</cell></row><row><cell>Soyel et al. [44]</cell><cell>3D</cell><cell>points/distance</cell><cell>NN</cell><cell cols="2">67.52 91.30</cell></row><row><cell>Soyel et al. [45]</cell><cell>3D</cell><cell>points/distance</cell><cell>NN</cell><cell>-</cell><cell>93.72</cell></row><row><cell>Tang et al. [46]</cell><cell>3D</cell><cell>points/distance</cell><cell>LDA</cell><cell cols="2">74.51 95.10</cell></row><row><cell>Tang et al. [47]</cell><cell>3D</cell><cell>slopes, distance</cell><cell>SVM</cell><cell>-</cell><cell>87.10</cell></row><row><cell>Mpiperis [35]</cell><cell>3D</cell><cell>deformable model</cell><cell>ML</cell><cell>-</cell><cell>90.50</cell></row><row><cell>Gong et al. [15]</cell><cell>3D</cell><cell>depth/PAC</cell><cell>SVM</cell><cell>76.22</cell><cell>-</cell></row><row><cell>Berretti et al. [1]</cell><cell>3D</cell><cell>depth/SIFT</cell><cell>SVM</cell><cell>77.54</cell><cell>-</cell></row><row><cell>Maalej et al. [33]</cell><cell>3D</cell><cell>facial curves</cell><cell>muiti-boosting</cell><cell>-</cell><cell>98.81 92.75</cell></row><row><cell>Li et al. [26]</cell><cell>3D</cell><cell>normals, curv./hist.</cell><cell>SVM</cell><cell>82.01</cell><cell>-</cell></row><row><cell>Li et al. [23]</cell><cell>3D</cell><cell>normals/LBP</cell><cell>MKL</cell><cell>80.14</cell><cell>-</cell></row><row><cell>Lemaire [22]</cell><cell>3D</cell><cell>curvature/HOG</cell><cell>SVM</cell><cell>76.61</cell><cell>-</cell></row><row><cell>Ocegueda [36]</cell><cell>3D</cell><cell>coordinates, normals curvatures/DWT</cell><cell>Logistic Reg.</cell><cell>-</cell><cell>90.40</cell></row><row><cell>Zeng et al. [63]</cell><cell>3D</cell><cell>curvatures/LBP</cell><cell>SRC</cell><cell>70.93</cell><cell>-</cell></row><row><cell>Zhen et al. [67]</cell><cell>3D</cell><cell>coordinates, normals, shape index</cell><cell>SVM</cell><cell>84.50</cell><cell>-</cell></row><row><cell>Yang et al. [57]</cell><cell>3D</cell><cell>depth, normals, curv./scattering</cell><cell>SVM</cell><cell>84.80</cell><cell>-</cell></row><row><cell cols="2">Zhao et al. [66] 2D+3D</cell><cell>intensity,coordinates, shape index/LBP</cell><cell>BBN</cell><cell>-</cell><cell>82.30</cell></row><row><cell>Li et al. [24]</cell><cell>2D+3D</cell><cell>meshHOG/SIFT meshHOS/HSOG</cell><cell>SVM</cell><cell>86.32</cell><cell>-</cell></row><row><cell>DF-CNNsvm</cell><cell cols="2">2D+3D 32-D deep feature</cell><cell>SVM</cell><cell>86.86</cell><cell>-</cell></row><row><cell>DF-CNN softmax</cell><cell cols="4">2D+3D 6-D deep feature Softmax 86.20</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Results on BU-3DFE subset II: Table VIII reports the performance comparisons of the proposed DF-CNN with handcrafted features, pre-trained deep features, and fine-tuned deep features on BU-3DFE Subset II. From this table,</figDesc><table><row><cell></cell><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">COMPARISON OF THE AVERAGE ACCURACIES WITH HANDCRAFTED</cell></row><row><cell cols="8">FEATURES, PRE-TRAINED DEEP FEATURES, AND FINE-TUNED DEEP</cell></row><row><cell cols="5">FEATURES ON BU-3DFE SUBSET II.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Ig</cell><cell>I x n</cell><cell>I y n</cell><cell>I z n</cell><cell>Ic</cell><cell>It</cell><cell>All</cell></row><row><cell>MS-LBP</cell><cell cols="7">73.50 74.58 73.54 73.21 73.37 66.08 77.75</cell></row><row><cell>dense-SIFT</cell><cell cols="7">76.25 75.79 77.42 76.58 75.88 71.79 79.42</cell></row><row><cell>HOG</cell><cell cols="7">76.25 76.88 76.29 77.75 76.29 72.04 79.71</cell></row><row><cell>Gabor</cell><cell cols="7">73.04 75.00 78.29 76.42 76.33 75.86 80.00</cell></row><row><cell>vgg-net-m-conv5</cell><cell cols="7">76.17 75.04 76.92 76.54 75.54 76.42 79.75</cell></row><row><cell>vgg-net-m-full7</cell><cell cols="7">70.21 69.71 72.67 70.67 67.00 66.83 77.38</cell></row><row><cell cols="8">vgg-net-m-ft-full7svm 75.17 76.62 77.08 75.83 78.12 78.67 81.08</cell></row><row><cell>vgg-net-m-ft softmax</cell><cell cols="7">74.62 75.33 76.96 75.79 77.88 78.54 80.71</cell></row><row><cell>DF-CNNsvm</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.04</cell></row><row><cell>DF-CNN softmax</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.33</cell></row><row><cell cols="8">particular, for BU-3DFE subset II, 100 subjects are randomly</cell></row><row><cell cols="8">divided into 10 subsets, and for each session, 12,960 attribute</cell></row><row><cell cols="8">maps of 90 subjects are used for training and the remaining</cell></row><row><cell cols="8">1,440 attribute maps of 10 subjects are used for testing.</cell></row><row><cell cols="8">Similarly, for Bosphorus subset, 60 subjects are randomly</cell></row><row><cell cols="8">divided into 10 subsets, and for each session, 1,944 attribute</cell></row><row><cell cols="8">maps of 54 subjects are used for training and the remaining</cell></row><row><cell cols="7">216 attribute maps of 6 subjects are used for testing.</cell><cell></cell></row><row><cell>1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>OF THE AVERAGE ACCURACIES WITH HANDCRAFTED FEATURES PRE-TRAINED DEEP FEATURES, AND FINE-TUNED DEEP FEATURES ON BOSPHORUS SUBSET.</figDesc><table><row><cell>Method</cell><cell>Ig</cell><cell>I x n</cell><cell>I y n</cell><cell>I z n</cell><cell>Ic</cell><cell>It</cell><cell>All</cell></row><row><cell>MS-LBP</cell><cell cols="7">71.11 69.44 70.56 66.67 62.78 62.50 73.33</cell></row><row><cell>dense-SIFT</cell><cell cols="7">70.28 73.89 72.78 73.89 72.50 65.56 76.39</cell></row><row><cell>HOG</cell><cell cols="7">72.50 74.22 73.89 74.72 71.94 71.94 77.22</cell></row><row><cell>Gabor</cell><cell cols="7">67.78 73.61 75.83 71.61 75.56 70.56 77.50</cell></row><row><cell>vgg-net-m-conv5</cell><cell cols="7">71.94 72.50 73.61 71.67 72.78 73.06 79.72</cell></row><row><cell>vgg-net-m-full7</cell><cell cols="7">61.11 63.33 63.89 65.83 60.56 61.94 75.56</cell></row><row><cell cols="8">vgg-net-m-ft-full7svm 71.67 72.78 74.72 76.11 71.94 73.61 79.17</cell></row><row><cell>vgg-net-m-ft softmax</cell><cell cols="7">71.39 72.78 75.28 75.00 73.33 73.61 79.72</cell></row><row><cell>DF-CNNsvm</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.28</cell></row><row><cell>DF-CNN softmax</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table XI, we compared the parameter quantity, compute time, and accuracy between DF-CNN and DF-CNN a . We can see that, comparing with DF-CNN, DF-CNN a has much more parameters (50M vs. 300M) and thus runs more slowly (7.4 Hz vs. 4.1 Hz). However, DF-CNN still 1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE Transactions on Multimedia 14 achieves slightly better results than DF-CNN a . This might be due to that we used very limited number of training samples to train DF-CNN and DF-CNN a . Therefore, we guess that if one has sufficient training samples available, DF-CNN a still has a large potential to outperform DF-CNN in general but needs to learn more parameters, and to take more training time.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XI COMPARISON</head><label>XI</label><figDesc>OF DF-CNN AND DF-CNN a (WITHOUT PARAMETER SHARING, I.E., DIFFERENT ATTRIBUTE MAPS CORRESPONDING TO DIFFERENT FEATURE EXTRACTION SUBNETS) ON BU-3DFE SUBSET I.</figDesc><table><row><cell>Method</cell><cell>Parameter</cell><cell>Time/epoch</cell><cell>Accuracy</cell></row><row><cell>DF-CNNsvm</cell><cell>50 MB</cell><cell>7.4 Hz</cell><cell>86.86</cell></row><row><cell>DF-CNN softmax</cell><cell>50 MB</cell><cell>7.4 Hz</cell><cell>86.20</cell></row><row><cell>DF-CNN a svm</cell><cell>300 MB</cell><cell>4.1 Hz</cell><cell>86.48</cell></row><row><cell>DF-CNN a softmax</cell><cell>300 MB</cell><cell>4.1 Hz</cell><cell>85.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XII COMPARISON</head><label>XII</label><figDesc>OF THE LEARNING BASED FUSION STRATEGY (DF-CNN) WITH OTHERS ON BU-SUBSET I.</figDesc><table><row><cell>Feature and</cell><cell>linear SVM</cell><cell>MKL</cell><cell>DF-CNNsvm</cell><cell>DF-CNN softmax</cell></row><row><cell>fusion</cell><cell>(score-level)</cell><cell>(kernel)</cell><cell>(learning-based)</cell><cell>(learning-based)</cell></row><row><cell>DF-CNN-in-</cell><cell>84.22</cell><cell>85.07</cell><cell>84.79</cell><cell>83.90</cell></row><row><cell>conv5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DF-CNN-ft-</cell><cell>84.17</cell><cell>85.73</cell><cell>86.86</cell><cell>86.20</cell></row><row><cell>conv5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XIII COMPARISON</head><label>XIII</label><figDesc>OF DIFFERENT CLASSIFIERS OVER THE 32-DIMENSIONAL DEEP FEATURES EXTRACTED FROM DF-CNN ON BU-3DFE SUBSET I.</figDesc><table><row><cell>Classifier</cell><cell>Logistic</cell><cell>k-Nearest</cell><cell>Naive</cell><cell>Random</cell><cell>kernel</cell><cell>linear</cell></row><row><cell></cell><cell>Regres.</cell><cell>Neighbor</cell><cell>Bayes</cell><cell>Forests</cell><cell>SVM</cell><cell>SVM</cell></row><row><cell cols="2">Accuracy (%) 81.03</cell><cell>85.84</cell><cell>85.90</cell><cell>85.18</cell><cell>86.76</cell><cell>86.86</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.vlfeat.org/matconvnet/ 1520-9210 (c)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2016" xml:id="foot_1"><p>IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>http://www.csie.ntu.edu.tw/ cjlin/liblinear/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT Huibin Li was supported in part by the NSFC under grant 11401464, Chinese Postdoctoral Science Foundation under grant 2014M560785, and International Exchange Foundation of China NSFC and United Kingdom RS under grant 61711530242. Jian Sun was supported in part by the NSFC under grants 61472313 and 11622106. Liming Chen was supported in part by the French Research Agency, l'Agence Nationale de Recherche (ANR), through the Jemime project (N • contract ANR-13-CORD-0004-02) and the Biofence project (N • contract ANR-13-INSE-0004-02) and the PUF 4D Vision project funded by the Partner University Foundation. 1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE Transactions on Multimedia</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A set of selected sift features for 3d facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Int. Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4125" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dexpression: Deep convolutional neural network for expression recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Trier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<idno>CoRR, abs/1509.05371</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emotion recognition in text for 3-d facial expression rendering</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Calix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mallepudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Knapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="544" to="551" />
			<date type="published" when="2010-10">Oct 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial expression analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Corneanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emotion recognition using dynamic grid-based hog features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dahmane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011-03">March 2011</date>
			<biblScope unit="page" from="884" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prototype-based modeling for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dahmane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1574" to="1584" />
			<date type="published" when="2014-10">Oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011-11">Nov 2011</date>
			<biblScope unit="page" from="2106" to="2112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Member</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d facial expression recognition: A perspective on promises and challenges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ocegueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face Gesture Recognition and Workshops</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="603" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d/4d facial expression analysis: An advanced annotated face model approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ocegueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="738" to="749" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel cubic-order algorithm for approximating principal direction vectors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldfeather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Interrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="63" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition on a single 3d face by exploring shape deformation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM international conference on Multimedia</title>
		<meeting>the 17th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="569" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An automatic framework for textured 3d video-based facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="313" />
			<date type="published" when="2014-07">July 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segmentation and classification of range images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="608" to="620" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust symbolic dual-view facial expression recognition with skin wrinkles: Local versus global approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="536" to="543" />
			<date type="published" when="2010-10">Oct 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emonets: Multimodal deep learning approaches for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Do deep neural networks learn facial action units when doing expression recognition?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/1510.02969</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition</title>
		<author>
			<persName><forename type="first">B.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, ICMI &apos;15</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction, ICMI &apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully automatic 3d facial expression recognition using differential mean curvature maps and histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ardabilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop 3D Face Biometrics, IEEE Automatic Facial and Gesture Recognition, FG&apos;13</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d facial expression recognition via multiple kernel learning of multi-scale local normal patterns</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morvan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2577" to="2580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient multimodal 2d + 3d feature-based approach to automatic facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morvand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="83" to="92" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A group of facial normal descriptors for recognizing 3d identical twins</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Fifth International Conference on Biometrics: Theory, Applications and Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="271" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">3d facial expression recognition based on histograms of surface differential quantities. Advances Concepts for Intelligent Vision Systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="483" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A data-driven approach for facial expression retargeting in video</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="310" />
			<date type="published" when="2014-02">Feb 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Au-inspired deep networks for facial expression feature learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deeply Learning Deformable Facial Action Parts Model for Dynamic Expression Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="143" to="157" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1805" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<title level="m">Proceedings, Part IV, chapter Feature Disentangling Machine -A Novel Approach of Feature Selection and Disentangling in Facial Expression Analysis</title>
		<meeting>Part IV, chapter Feature Disentangling Machine -A Novel Approach of Feature Selection and Disentangling in Facial Expression Analysis<address><addrLine>Zurich, Switzerland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="151" to="166" />
		</imprint>
	</monogr>
	<note>ECCV 2014: 13th European Conference</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Local 3d shape analysis for facial expression recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010-08">Aug 2010</date>
			<biblScope unit="page" from="4129" to="4132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shape analysis of local facial patches for 3d facial expression recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1581" to="1589" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic 3d face detection, normalization and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DPVT</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="735" to="742" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bilinear models for 3-d face and facial expression recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mpiperis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>sept</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Expressive maps for 3d facial expression recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ocegueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops(ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1270" to="1275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial expressions: The state of the art</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1424" to="1445" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno>CoRR, abs/1403.6382</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation for facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision C ECCV 2012</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="808" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Static and dynamic 3d facial expression recognition: A comprehensive survey. Image and Vision Computing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sandbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Savran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alyüz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dibeklioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>¸eliktutan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gökberk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
		<title level="m">Bosphorus Database for 3D Face Analysis</title>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Facial action unit detection: 3d versus 2d modality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Savran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bilge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A generic framework for efficient 2-d and 3-d facial expression analogy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1384" to="1395" />
			<date type="published" when="2007-11">Nov 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial expression recognition using 3d facial feature distances</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Demirel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Recognition</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4633</biblScope>
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d facial expression recognition with geometrically localized facial features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Demirel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Symposium on Computer and Information Sciences (ISCIS)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d facial expression recognition based on automatically selected features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops(CVPRW)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3d facial expression recognition based on properties of line segments connecting facial feature points</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th IEEE International Conference on Automatic Face Gesture Recognition(FG)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep learning using support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<idno>CoRR, abs/1306.0239</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face expression recognition by cross modal data association</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tawari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1543" to="1552" />
			<date type="published" when="2013-11">Nov 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Real-time 2d+3d facial action and expression recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tsalakanidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1763" to="1775" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3d facial expression recognition based on primitive surface feature distribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern(CVPR)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1399" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A natural visible and infrared facial expression database for expression recognition and emotion inference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2010-11">Nov 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Analyses of a multimodal spontaneous facial expression database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2013-01">Jan 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Speaking effect removal on emotion recognition from facial expressions based on eigenface conversion</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1732" to="1744" />
			<date type="published" when="2013-12">Dec 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automatic 3d facial expression recognition using geometric scattering representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recognition of facial expressions and measurement of levels of interest from video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yeasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bullot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="508" />
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11">November 2015</date>
			<publisher>IEEE Institute of Electrical and Electronics Engineers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Discriminant graph structures for facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1528" to="1540" />
			<date type="published" when="2008-12">Dec 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning personalized models for facial expression analysis and gesture recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="775" to="788" />
			<date type="published" when="2016-04">April 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An automatic 3d expression recognition framework based on sparse representation of conformal images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Comparison between geometry-based and gabor-wavelets-based facial expression recognition using multi-layer perceptron</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="454" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Automatic 3d facial expression recognition based on a bayesian belief net and a statistical facial feature model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3724" to="3727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Muscular movement model based automatic 3d facial expression recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8935</biblScope>
			<biblScope unit="page" from="522" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Muscular movement model based automatic 3d/4d facial expression recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">He is currently an assistant professor in the school of mathematics and statistics, Xi&apos;an Jiaotong University. His research interests include 3D shape analysis, 3D face recognition, 3D facial expression analysis, discrete differential geometry, geometric data analysis, modeling and learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Jian Sun received Ph.D. in applied mathematics from Xian Jiaotong University. He worked as a visiting student in Microsoft Research Asia</title>
		<meeting><address><addrLine>Ecole Centrale de Lyon, LIRIS, Lyon, France; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">2015. 2006 and 2009. Nov. 2005 -March 2008. August 2009 -April 2010. Sept. 2012 -August 2014. 1987</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1499" to="1510" />
		</imprint>
		<respStmt>
			<orgName>University and Xian Jiaotong University ; Xian Jiaotong University</orgName>
		</respStmt>
	</monogr>
	<note>He now serves as a professor in the school of mathematics and statistics of Xian Jiaotong University. His research interests are in the mathematics &amp; machine learning-based approaches for image processing / recognition, and medical image analysis. Zongben Xu received his Ph.D. degree in mathemat</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
