<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-09-01">1 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Harrison</forename><surname>Lee</surname></persName>
							<email>harrisonlee@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Samrat</forename><surname>Phatale</surname></persName>
							<email>samratph@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Mansoor</surname></persName>
							<email>hassan@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Kellie</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Colton</forename><surname>Bishop</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><surname>Carbune</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cameron</forename><surname>Mckinnon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carol</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christo- Pher</forename><surname>Olah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dustin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eli</forename><surname>Tran-Johnson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jamie</forename><surname>Kerr</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Mueller</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><surname>Ladish</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Landau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kamile</forename><surname>Lukosuite</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Sellitto</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Schiefer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noemi</forename><surname>Mercado</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Lasenby</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robin</forename><surname>Larson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Ringer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><surname>John- Ston</surname></persName>
						</author>
						<author>
							<persName><roleName>Sheer</roleName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
						</author>
						<author>
							<persName><forename type="first">El</forename><surname>Showk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tamera</forename><surname>Lanham</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Telleen-Lawton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Con- Erly</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bow- Man</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><forename type="middle">2022b</forename><surname>Kaplan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
						</author>
						<title level="a" type="main">RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-01">1 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2309.00267v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high-quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) -a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ?70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement Learning from Human Feedback (RLHF) is an effective technique for aligning language models to human preferences <ref type="bibr" target="#b26">(Stiennon et al., 2020;</ref><ref type="bibr" target="#b21">Ouyang et al., 2022)</ref> and is cited as one of the key drivers of success in modern conversational language models like ChatGPT and Bard <ref type="bibr" target="#b14">(Liu et al., 2023;</ref><ref type="bibr" target="#b16">Manyika, 2023)</ref>. By training with reinforcement learning (RL), language models can be optimized on complex, sequence-level objectives that are not easily differentiable with traditional supervised fine-tuning.</p><p>The need for high-quality human labels is an obstacle for scaling up RLHF, and one natural question is whether artificially generated labels can achieve comparable results. Several works have shown that large language models (LLMs) exhibit a high degree of alignment with human judgmenteven outperforming humans on some tasks <ref type="bibr" target="#b7">(Gilardi et al., 2023;</ref><ref type="bibr">Ding et al., 2023)</ref>. <ref type="bibr">Bai et al. (2022b)</ref> was the first to explore using AI preferences to train a reward model used for RL fine-tuning -a Figure <ref type="figure">1</ref>: Human evaluators strongly prefer RLHF and RLAIF summaries over the supervised fine-tuned (SFT) baseline. The differences in win rates between RLAIF vs. SFT and RLHF vs. SFT are not statistically significant. Additionally, when compared head-to-head, RLAIF is equally preferred to RLHF by human evaluators. Error bars denote 95% confidence intervals.</p><p>technique called "Reinforcement Learning from AI Feedback" (RLAIF) 1 . While they showed that utilizing a hybrid of human and AI preferences in conjunction with the "Constitutional AI" selfrevision technique outperforms a supervised finetuned baseline, their work did not directly compare the efficacy of human vs. AI feedback, leaving the question unanswered whether RLAIF can be a suitable alternative to RLHF.</p><p>In this work, we directly compare RLAIF against RLHF on the task of summarization. Given a text and two candidate responses, we assign a preference label using an off-the-shelf LLM. We then train a reward model (RM) on the LLM preferences with a contrastive loss. Finally, we fine-tune a policy model with reinforcement learning, using 1 We use "RLAIF" to denote training a reward model on AIlabeled preferences followed by conducting RL fine-tuning. This is distinct from "Constitutional AI", which improves upon a supervised learning model through iteratively asking an LLM to generate better responses according to a constitution. Both were introduced in <ref type="bibr">Bai et al. (2022b)</ref> and are sometimes confused for one another. the RM to provide rewards.</p><p>Our results show that RLAIF achieves comparable performance to RLHF, measured in two ways. First, we observe that both RLAIF and RLHF policies are preferred by humans over a supervised fine-tuned (SFT) baseline 71% and 73% of the time, respectively, and the two win rates are not statistically significantly different. Second, when asked to directly compare generations from RLAIF vs. RLHF, humans prefer both at equal rates (i.e. 50% win rate). These results suggest that RLAIF is a viable alternative to RLHF that does not depend on human annotation and offers appealing scaling properties.</p><p>Additionally, we study techniques to maximize the alignment of AI-generated preferences with human preferences. We find that prompting our LLM with detailed instructions and soliciting chain-ofthought reasoning improve alignment. Surprisingly, we observe that both few-shot in-context learning and self-consistency -a process in which we sample multiple chain-of-thought rationales and average the final preferences -do not improve accuracy or even degrade it. Finally, we conduct scaling experiments to quantify the trade-offs between the size of the LLM labeler and the number of preference examples used in training vs. alignment with human preferences.</p><p>Our main contributions are the following:</p><p>? We demonstrate that RLAIF achieves comparable performance to RLHF on the task of summarization</p><p>? We compare various techniques for generating AI labels and identify optimal settings for RLAIF practitioners</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We first review the RLHF pipeline introduced in <ref type="bibr" target="#b26">Stiennon et al. (2020)</ref>; <ref type="bibr" target="#b21">Ouyang et al. (2022)</ref>, which consists of 3 phases: supervised fine-tuning, reward model training, and reinforcement learning-based fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supervised Fine-tuning</head><p>A pre-trained LLM is fine-tuned on a high quality labeled dataset for a downstream task using token-level supervision to produce a supervised fine-tuned (SFT) model ? SF T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reward Modeling</head><p>Given an input x, we sample a pair of responses from one or more models (y 1 , y 2 ) ? ?, where oftentimes the SFT model is used. The input and responses are sent to human annotators to rate which response is better according to some criteria. These annotations form a dataset of triplets D = {(x, y w , y l )}, where y w and y l are the preferred and non-preferred responses, respectively. A reward model r ? is trained by minimizing the following loss:</p><formula xml:id="formula_0">L r (?) = -E (x,yw,y l )?D [log ?(r ? (x, y w ) -r ? (x, y l ))]</formula><p>where ? is the sigmoid function.</p><formula xml:id="formula_1">E [r ? (y|x) -?D KL (? RL ? (y|x)||? SF T (y|x))]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RLAIF Methodology</head><p>In this section, we describe the techniques used to generate preference labels with an LLM, how we conduct RL, and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preference Labeling with LLMs</head><p>We annotate preferences among pairs of candidates with an "off-the-shelf" LLM, which is a model pre-trained or instruction-tuned for general usage but not fine-tuned for a specific downstream task. Given a piece of text and two candidate summaries, the LLM is asked to rate which summary is better. The input to the LLM is structured as follows (example in Table <ref type="table">1</ref>):</p><p>1. Preamble -Introduction and instructions describing the task at hand 2. Few-shot exemplars (optional) -An example of a text, a pair of summaries, a chain-ofthought rationale (if applicable), and a preference judgment 3. Sample to annotate -A text and a pair of summaries to be labeled 4. Ending -An ending string to prompt the LLM (e.g. "Preferred Summary=")</p><p>After the LLM is given the input, we obtain the log probabilities of generating the tokens "1" and "2" and compute the softmax to derive a preference distribution.</p><p>There are numerous alternatives to obtain preference labels from LLMs, such as decoding a freeform response from the model and extracting the preference heuristically (e.g. output = "The first summary is better"), or representing the preference distribution as a one-hot representation. However, we did not experiment with these alternatives because our approach already yielded high accuracy.</p><p>We experiment with two types of preambles: "Base", which briefly asks "which summary is better?", and "OpenAI", which closely mimics the rating instructions given to the human preference annotators that generated the OpenAI TL;DR preference dataset and contains detailed information about what constitutes a strong summary <ref type="bibr" target="#b26">(Stiennon et al., 2020)</ref>. See Appendix Table <ref type="table">6</ref> for full preambles. We also experiment with in-context learning by adding few-shot exemplars in the prompt, where exemplars were manually selected to cover different topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Addressing Position Bias</head><p>The order in which candidates are shown to the LLM can bias which candidate it prefers <ref type="bibr" target="#b22">(Pezeshkpour and Hruschka, 2023)</ref>. We find evidence that such a position bias exists, especially with smaller sizes of LLM labelers (see Appendix A).</p><p>To mitigate position bias in preference labeling, we make two inferences for every pair of candidates, where the order in which candidates are presented to the LLM is reversed for the second inference. The results from both inferences are then averaged to obtain the final preference distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Chain-of-thought Reasoning</head><p>We experiment with eliciting chain-of-thought (COT) reasoning from our AI labelers to improve alignment with human preferences <ref type="bibr" target="#b31">(Wei et al., 2022)</ref>. We replace the Ending of the standard prompt (i.e. "Preferred Summary=") with "Consider the coherence, accuracy, coverage, and overall quality of each summary and explain which one is better. Rationale:" and then decode a response from the LLM. Finally, we concatenate the original prompt, the response, and the original Ending string "Preferred Summary=" together, and follow the scoring procedure in Section 3.1 to obtain a preference distribution. See Figure <ref type="figure">3</ref> for an illustration.</p><p>In zero-shot prompts, the LLM is not given an example of what reasoning should look like, while in few-shot prompts, we provide examples of COT reasoning for the model to follow. See Tables <ref type="table">7</ref> and<ref type="table">8</ref> for examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Self-Consistency</head><p>For chain-of-thought prompts, we also experiment with self-consistency -a technique to improve upon chain-of-thought reasoning by sampling multiple reasoning paths and aggregating the final answer produced at the end of each path <ref type="bibr" target="#b29">(Wang et al., 2022)</ref>. Multiple chain-of-thought rationales are sampled with a non-zero decoding temperature, and then LLM preference distributions are obtained for each one -following the approach in Section 3.1.2. The results are then averaged to obtain the final preference distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reinforcement Learning from AI Feedback</head><p>After preferences are labeled by the LLM, a reward model (RM) is trained to predict preferences. Since our approach produces soft labels (e.g. pref erences i = [0.6, 0.4]), we apply a cross-entropy loss to the softmax of the reward scores generated by the RM instead of the loss mentioned in Section 2.2. The softmax converts the unbounded scores from the RM into a probability distribution.</p><p>Training a RM on a dataset of AI labels can be viewed as a form of model distillation, especially since our AI labeler is often larger and more powerful than our RM. An alternative approach is to bypass the RM and use AI feedback directly as a reward signal in RL, though this approach is more computationally expensive since the AI labeler is larger than the RM.</p><p>With the trained RM, we conduct reinforcement learning with a modified version of the Advantage Actor Critic (A2C) algorithm adapted to the language modeling domain <ref type="bibr" target="#b18">(Mnih et al., 2016</ref>) (details in Appendix B). While many recent works use Proximal Policy Optimization (PPO) <ref type="bibr" target="#b24">(Schulman et al., 2017</ref>) -a similar method that adds a few techniques to make training more conservative and stable (e.g. clipping the objective function), we utilize A2C given that it is simpler yet still effective for our problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>We evaluate our results with three metrics -AI Labeler Alignment, Pairwise Accuracy, and Win Rate.</p><p>AI Labeler Alignment measures the accuracy of AI-labeled preferences with respect to human preferences. For a single example, it is calculated by Figure <ref type="figure">3</ref>: To derive an AI preference label, the LLM is first prompted to verbally explain its thoughts on the quality of the two candidates (blue). The LLM response is then appended to the original prompt (orange) and fed to the LLM a second time to generate a preference distribution over "1" vs. "2" based on their log probabilities (green). converting a soft AI-labeled preference to a binary representation (e.g. pref erences i = [0.6, 0.4] ? [1, 0]), and then assigning a 1 if the label agrees with the target human preference and 0 otherwise. It can be expressed as follows:</p><formula xml:id="formula_2">Align = D i=1 1[arg max x pr ai i = arg max x pr h i ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|D|</head><p>where pr ai and p h are binary representations of AI and human preferences, respectively, x is an index, and D is a dataset.</p><p>Pairwise Accuracy measures how accurate a trained reward model is with respect to a held-out set of human preferences. Given a shared context and pair of candidate responses, the Pairwise Accuracy is 1 if the RM scores the preferred candidate higher than the non-preferred candidate, according to the human label. Otherwise the value is 0. This quantity is averaged over multiple examples to measure the total accuracy of the RM.</p><p>Win Rate evaluates the end-to-end quality of two policies by measuring how often one policy is preferred by humans over another one. Given an input and two generations, human annotators select which generation is preferred. The percentage of instances where policy A is preferred over policy B is referred to as the "Win Rate of A vs. B".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Following the work of <ref type="bibr" target="#b26">Stiennon et al. (2020)</ref>, we use the filtered Reddit TL;DR dataset curated by OpenAI. TL;DR contains ?3 million posts from Reddit<ref type="foot" target="#foot_0">2</ref> across a variety of topics (also known as "subreddits") alongside summaries of the posts written by the original authors. The data is additionally filtered by OpenAI to ensure high quality, which includes using a whitelist of subreddits that are understandable to the general population. Additionally, only posts where the summaries contain between 24 and 48 tokens are included. The filtered dataset contains 123,169 posts, where ?5% is held out as a validation set. More details on the dataset can be found in the original paper.</p><p>Additionally, OpenAI curated a human preference dataset from the filtered TL;DR dataset. For a given post, two candidate summaries were generated from different policies, and labelers were asked to rate which summary they preferred. The total dataset comprises ?92k pairwise comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LLM Labeling</head><p>For evaluating the efficacy of AI labeling techniques (e.g. prompting, self-consistency), we select examples from the TL;DR preference dataset where human annotators preferred one summary over the other with higher confidence 3 . We evaluate AI Labeler Alignment on a random 15% subset of the training split of the dataset to enable faster experiment iteration, yielding 2851 examples for evaluation. For reward model training, the full training split of the TL;DR preference dataset is labeled by the LLM and used for training -regardless of confidence scores.</p><p>We use PaLM 2 as our LLM for labeling preferences <ref type="bibr">(Google et al., 2023)</ref>. Unless otherwise specified, we use the Large model size with a maximum context length of 4096 tokens. For chain-ofthought generation, we set a maximum decoding length of 512 tokens and sample with temperature T = 0 (i.e. greedy decoding). For self-consistency experiments, we use temperature T = 1 with top-K sampling <ref type="bibr" target="#b4">(Fan et al., 2018)</ref>, where K = 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training</head><p>We train a SFT model on OpenAI's filtered TL;DR dataset, using PaLM 2 Extra-Small (XS) as our initial checkpoint.</p><p>We then initialize our RMs from the SFT model and train them on OpenAI's TL;DR human preference dataset. For the results in Section Table <ref type="table">1</ref> and 5.1, we generate AI-labeled preferences using PaLM 2 L, using the "OpenAI + COT 0-shot" prompt (see Section 5.2) without self consistency, and then train the RM on the full preference dataset.</p><p>For reinforcement learning, we train the policy with Advantage Actor Critic (A2C) as described in Appendix B. Both policy and value models are initialized from the SFT model. We rollout our policies using the filtered Reddit TL;DR dataset as our initial states.</p><p>For more training details, see Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Human Evaluation</head><p>We collected 1200 ratings from humans to evaluate RLHF and RLAIF policies. For each rating task, the evaluator was presented with a post and 4 summaries generated from different policies (one from each of RLAIF, RLHF, SFT, and Human Reference) and asked to rank them in order of quality without ties. Posts were drawn from the held-out set of the TL;DR supervised fine-tuning dataset, which was not used in any other evaluation. Once these rankings were collected, it was possible to calculate win rates with respect to any two policies.</p><p>3 This follows the evaluation procedure in <ref type="bibr" target="#b26">Stiennon et al. (2020)</ref> 5 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RLAIF vs. RLHF</head><p>Our results show that RLAIF achieves similar performance to RLHF (see Table <ref type="table">1</ref>). RLAIF is preferred by human evaluators over the baseline SFT policy 71% of the time. In comparison, RLHF is preferred over SFT 73% of the time. While RLHF slightly outperforms RLAIF, the difference is not statistically significant<ref type="foot" target="#foot_1">4</ref> . We also directly compare the win rate of RLAIF vs. RLHF and find that they are equally preferred -i.e. the win rate is 50%. To better understand how RLAIF compares to RLHF, we qualitatively compare summaries generated by both policies in Section 6.</p><p>We also compare RLAIF and RLHF summaries vs. human-written reference summaries. RLAIF summaries are preferred over the reference summaries 79% of the time, and RLHF are preferred over the reference summaries 80% of the time. The difference in win rates between RLAIF and RLHF vs. the reference summaries is also not statistically significant.</p><p>One confounding factor in our results is that our RLAIF and RLHF policies tend to generate longer summaries than the SFT policy, which can account for some of the quality improvements. Similar to <ref type="bibr" target="#b26">Stiennon et al. (2020)</ref>, we conduct post-hoc analysis that suggests that while both RLAIF and RLHF policies benefit from producing longer summaries, both still outperform the SFT policy by a similar margin after controlling for length. Full details in Appendix D.</p><p>These results suggest that RLAIF is a viable alternative to RLHF that does not depend on human annotation. To understand how well these findings generalize to other NLP tasks, experiments on a broader range of tasks are required, which we leave to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Prompting Techniques</head><p>We experiment with three types of prompting techniques -preamble specificity, chain-of-thought reasoning, and few-shot in-context learning -and report the results in Table <ref type="table" target="#tab_1">2</ref>.</p><p>Using the detailed OpenAI preamble improves alignment +1.3% (77.4% "OpenAI 0-shot" vs. 76.1% "Base 0-shot"), and chain-of-thought reasoning improves alignment +1.4% (77.5% "Base +   COT 0-shot" vs. 76.1% "Base 0-shot"). Though the improvement from combining the two techniques does not match the sum of their individual gains, the techniques are still complementary, together yielding +1.9% improvement. We observe that few-shot in-context learning does not improve alignment, even potentially degrading it. For "OpenAI + COT k-shot" prompts, we see accuracy monotonically decrease as k increases from 0 to 2. One hypothesis is that the LLM is able to generate more useful chain-of-thought rationales on its own than when it follows the rationales given in our 1-shot and 2-shot exemplars (see Table <ref type="table">9</ref> for examples).</p><p>To understand if adding more exemplars might yield improvements, we experimented with an 8shot prompt and found that accuracy decreased by -7.6% (69.8% "OpenAI 8-shot" vs. 77.4% "OpenAI 0-shot"). We verified that all examples used in this experiment fit within our AI labeler's context length.</p><p>Overall, we observe that the optimal configuration employs a detailed preamble, chain-of-thought reasoning, and no in-context learning ("OpenAI + COT 0-shot"). This combination achieves an AI Labeler Alignment of 78.0%, which is +1.9% higher than using our most basic prompt ("Base 0-shot"). As a point of comparison, <ref type="bibr" target="#b26">Stiennon et al. (2020)</ref> estimated that human inter-annotator agreement was 73-77% on the human preference dataset, suggesting that our LLM performs rather well. We use the "OpenAI + COT 0-shot" prompt for all other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Self-Consistency</head><p>Self-Consistency AI Labeler Alignment 1 sample, T=0 78.0% 4 samples, T=1 72.6% 16 samples, T=1 72.8% We experiment with self-consistency using 4 and 16 samples with decoding temperature of 1 as described in Section 3.1.3, and both settings show drops in alignment of greater than -5% versus not using self-consistency. Manually inspecting chainof-thought rationales did not reveal any common patterns for why self-consistency might result in lower accuracy (see examples in Table <ref type="table" target="#tab_4">10</ref>).</p><p>One hypothesis for the degradation in accuracy is that using a temperature of 1 leads the model to generate lower quality chain-of-thought rationales compared to greedy decoding, ultimately leading to worse accuracy overall. Using a temperature between 0 and 1 may yield better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Size of LLM Labeler</head><p>Model Size AI Labeler Alignment PaLM 2 XS 62.7% PaLM 2 S 73.8% PaLM 2 L 78.0% Large model sizes are not widely accessible and can be slow and expensive to run. We experiment with labeling preferences with different model sizes and observe a strong relationship between alignment and size. Alignment drops -4.2% when moving from PaLM 2 Large (L) down to PaLM 2 Small (S), and it drops another -11.1% when moving down to PaLM 2 XS. This trend is consistent with scaling laws observed in other work <ref type="bibr" target="#b11">(Kaplan et al., 2020)</ref>. One contributing factor to the decline in performance could be the increase in position bias in smaller LLMs (see Appendix A).</p><p>On the end of this trend, these results also suggest that scaling up AI labeler size may produce even higher quality preference labels. Since the AI labeler is only used to generate preference examples once and is not queried during RL training, using an even larger AI labeler is not necessarily prohibitively expensive. Furthermore, Section 5.5 suggests that a small number of examples may be sufficient to train a powerful RM (e.g. on the order of O(1k)), further reducing the costs of using a larger labeler model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Number of Preference Examples</head><p>To understand how RM accuracy changes with the number of training examples, we train a RM on varying amounts of AI-labeled preference examples and evaluate Pairwise Accuracy on a heldout set of human preferences. We obtain different </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Qualitative Analysis</head><p>To better understand how RLAIF compares to RLHF, we manually inspected summaries generated by both policies. In many cases, the two policies produced similar summaries, which is reflected in their similar win rates (see Section 5.1). However, we identified two patterns where they fre-quently diverged.</p><p>One pattern we observed is that RLAIF appears less likely to hallucinate than RLHF. The hallucinations in RLHF are often plausible but are inconsistent with the original text. For instance, in Example #1 of Table <ref type="table" target="#tab_5">11</ref>, the RLHF summary states that the author is 20 years old, but this is not mentioned or implied by the original text.</p><p>Another pattern we observed is that RLAIF sometimes produces less coherent or grammatical summaries than RLHF. For instance, in Example #1 of Table <ref type="table" target="#tab_1">12</ref>, the RLAIF summary produces run-on sentences.</p><p>Overall, while we observe certain tendencies for each policy, both produce high-quality summaries that are relatively similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>LLMs <ref type="bibr" target="#b11">(Brown et al., 2020;</ref><ref type="bibr" target="#b28">Thoppilan et al., 2022;</ref><ref type="bibr" target="#b1">Chowdhery et al., 2022;</ref><ref type="bibr">Google et al., 2023;</ref><ref type="bibr">Ope-nAI, 2023)</ref> have shown impressive performance over a wide range of NLP tasks. For several of these tasks, RL has emerged as an effective optimization technique. While initial applications of RL on tasks such as translation <ref type="bibr" target="#b34">(Wu et al., 2016</ref><ref type="bibr">(Wu et al., , 2018) )</ref> and summarization <ref type="bibr" target="#b6">(Gao et al., 2019;</ref><ref type="bibr" target="#b35">Wu and Hu, 2018)</ref> used automatic evaluation metrics as rewards, such simplified formulations of rewards did not fully align with human notions of quality.</p><p>Reinforcement learning from human feedback <ref type="bibr" target="#b2">Christiano et al. (2017)</ref> has been used as a technique to directly align LLMs with human preferences <ref type="bibr" target="#b37">(Ziegler et al., 2019)</ref> by training a reward model on pairwise comparisons of natural language responses, and has been successfully applied for summarization <ref type="bibr" target="#b26">(Stiennon et al., 2020)</ref>, generalized instruction following <ref type="bibr" target="#b21">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b13">Lai et al., 2023)</ref>, dialogue <ref type="bibr" target="#b7">(Gilardi et al., 2023;</ref><ref type="bibr" target="#b16">Manyika, 2023;</ref><ref type="bibr" target="#b8">Glaese et al., 2022;</ref><ref type="bibr">Bai et al., 2022a)</ref> and question answering <ref type="bibr" target="#b19">(Nakano et al., 2021)</ref>.</p><p>LLMs have also been extensively used for data generation <ref type="bibr" target="#b30">(Wang et al., 2021;</ref><ref type="bibr" target="#b17">Meng et al., 2023)</ref>, augmentation <ref type="bibr" target="#b5">(Feng et al., 2021)</ref> and in selftraining setups <ref type="bibr" target="#b29">(Wang et al., 2022;</ref><ref type="bibr" target="#b15">Madaan et al., 2023)</ref>. <ref type="bibr">Bai et al. (2022b)</ref> introduced the idea of RL from AI feedback (RLAIF), which used LLM labeled preferences in conjunction with human labeled preferences to jointly optimize for the two conflicting objectives of helpfulness and harmlessness. Recent works have also explored related tech-niques for generating rewards from LLMs <ref type="bibr" target="#b23">(Roit et al., 2023;</ref><ref type="bibr" target="#b12">Kwon et al., 2022;</ref><ref type="bibr" target="#b36">Yang et al., 2023)</ref>. These works demonstrate that LLMs can generate useful signals for RL fine-tuning, which inspired this work's investigation into whether LLMs can serve as a viable alternative to humans in collecting preference labels for reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we show that RLAIF can produce comparable improvements to RLHF without depending on human annotators. Our experiments show that RLAIF greatly improves upon a SFT baseline, and the margin of improvement is on par with that of RLHF. In head-to-head comparisons, RLAIF and RLHF are preferred at similar rates by humans. We also study various AI labeling techniques and conduct scaling studies to understand the optimal settings for generating aligned preferences.</p><p>While this work highlights the potential of RLAIF, we note some limitations of these findings. First, this work only explores the task of summarization, leaving an open question about generalizability to other tasks. Second, we did not estimate whether LLM inference is advantageous versus human labeling in terms of monetary costs. Additionally, there remain many interesting open questions, such as whether RLHF combined with RLAIF can outperform a single approach alone, how well using a LLM to directly assign rewards performs, whether improving AI Labeler Alignment translates to improved final policies, and whether using a LLM labeler the same size as the policy model can further improve the policy (i.e. whether a model can "self-improve"). We leave these questions for future work.</p><p>We hope that this paper motivates further research in the area of RLAIF. Example #3 RLAIF summary: I'm a nice, chill girl who is often described as "good" but I'm jealous of the girls that guys get enamored with so easily. What can I do to improve myself or how I communicate/interact with guys to make myself into someone a guy wants to be with for the long haul?</p><p>RLHF summary: I'm a nice, chill girl who is often described as "boring". I'm jealous of the girls that guys get enamored with so easily. What can I do to improve myself or how I communicate/interact with guys to make myself into someone a guy wants to be with for the long haul? Table <ref type="table" target="#tab_1">12</ref>: Another pattern identified through manually inspected summaries is that summaries from the RLAIF policy tend to be less coherent and grammatical than summaries from the RLHF policy. Less coherent phrases are highlighted in red.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A diagram depicting RLAIF (top) vs. RLHF (bottom)</figDesc><graphic url="image-2.png" coords="2,85.04,70.86,425.20,211.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example summaries generated by SFT, RLHF and RLAIF policies for a Reddit post. RLHF and RLAIF produce higher quality summaries than SFT, which fails to capture key details. Salient details are bolded.</figDesc><graphic url="image-4.png" coords="7,85.04,70.87,425.21,195.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: RM accuracy on a held-out set of human preferences increases rapidly as more preference pairs are used in training. After training on a few thousand examples, performance is close to training on the full dataset. The x-axis is in log-scale.</figDesc><graphic url="image-5.png" coords="8,306.14,71.86,238.10,147.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-3.png" coords="5,70.87,70.87,453.54,210.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>We observe that prompting with the detailed OpenAI preamble and eliciting chain-of-thought reasoning gives the highest AI Labeler Alignment. In-context learning does not improve accuracy, and possibly even makes it worse.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Sampling several chain-of-thought rationales</cell></row><row><cell>with T = 1 results in lower alignment with human pref-</cell></row><row><cell>erences. Note: 1, 4, and 16 samples represent 2, 8, and</cell></row><row><cell>32 inferences given our position de-biasing technique</cell></row><row><cell>(see Section 3.1.1).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>AI Labeler Alignment increases as the size of the LLM labeler increases.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 10 :</head><label>10</label><figDesc>An example comparing chain-of-thought rationales produced at different temperatures for self-consistency experiments. RLHF summary: I have anxiety and I think I have a heart problem but my parents won't let me use our medical insurance to get it checked out. What should I do? I'm 20 years old. ..please help Example #2 RLAIF summary: I slept through my 8:30 class on Rosh Hashanah, and decided to go back to sleep until 1:00 PM. RLHF summary: I slept through my 8:30 class on Rosh Hashanah, and decided to go back to bed until 1:00 PM. I'm a lazy f *** .</figDesc><table><row><cell>Example #1</cell><cell>RLAIF summary: I have anxiety and I think I have a heart</cell></row><row><cell></cell><cell>problem. My parents refuse to believe me and won't let me</cell></row><row><cell></cell><cell>use our medical insurance to get it checked out. What should</cell></row><row><cell></cell><cell>I do?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 11 :</head><label>11</label><figDesc>We observe that the RLHF policy tends to hallucinate more frequently than the RLAIF policy. Hallucinations are highlighted in red. RLAIF summary: Boyfriend is overly flirtatious with other girls, I've talked to him about it, he doesn't seem to care. It's causing trust issues. Am I overreacting? What else can I do? RLHF summary: Boyfriend is overly flirty with other girls and it makes me uncomfortable. I've talked to him about it multiple times, but he doesn't seem to care. What else can I do? Am I overreacting? RLAIF summary: Asked a girl to prom, things were going great until I asked her. Now our conversations are awkward and I'm not sure if I should ask her out. Should I just give up? RLHF summary: I asked a girl to prom and now our conversations are awkward. I like her and I'm wondering if I should ask her out or just give up. I'm not sure Example #3 RLAIF summary: Broke up with girl 1, she started dating my best friend. Dated girl 2, she became unstable and we broke up. I still feel attached to both. Why? How do I get over this? Any advice? RLHF summary: I broke up with a girl I was with for years, and she started dating my best friend. I started dating a new girl, and she became depressed and distant. I still feel attached to both. Why?</figDesc><table><row><cell>Example #1</cell></row><row><cell>Example #2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>www.reddit.com   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>When conducting a two-sample t-test, p-value = 0.25, which is greater than 0.05. Therefore, we cannot reject the null hypothesis.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank many people who have helped make this work complete. We thank <rs type="person">Chen Zhu</rs> for optimizing our LLM inference setup, <rs type="person">Le Hou</rs> for suggesting prompt improvements and experimenting with self-consistency, <rs type="person">Johan Ferret</rs> for tips on how to tune better RL policies, and <rs type="person">L?onard Hussenot</rs> for bringing the problem of position bias in LLMs to our attention.</p><p>We thank everyone who thoroughly reviewed our work and provided valuable feedback: <rs type="person">Hakim Sidahmed</rs>, <rs type="person">Michal Valko</rs>, <rs type="person">Nevan Wichers</rs>, <rs type="person">Sian Gooding</rs>, <rs type="person">Sushant Prakash</rs>, and <rs type="person">Yuan Cao</rs>.</p><p>Finally, we thank the individuals who designed and built the RL training infrastructure used in this paper: <rs type="person">L?onard Hussenot</rs>, <rs type="person">Johan Ferret</rs>, <rs type="person">Robert Dadashi</rs>, <rs type="person">Geoffrey Cideron</rs>, <rs type="person">Alexis Jacq</rs>, <rs type="person">Sabela Ramos</rs>, <rs type="person">Piotr Stanczyk</rs>, <rs type="person">Sertan Girgin</rs>, <rs type="person">Danila Sinopalnikov</rs>, <rs type="person">Am?lie H?liou</rs>, <rs type="person">Nikola Momchev</rs>, and <rs type="person">Olivier Bachem</rs>.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preamble</head><p>A good summary is a shorter piece of text that has the essence of the original. ... Given a piece of text and two of its possible summaries, output 1 or 2 to indicate which summary best adheres to coherence, accuracy, coverage, and overall quality as defined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-Shot Exemplar</head><p>???? Example ???? Text -We were best friends over 4 years ... Summary 1 -Broke up with best friend, should I wish her a happy birthday... And what do you think of no contact? Summary 2 -should I wish my ex happy birthday, I broke no contact, I'm trying to be more patient, I'm too needy, and I don't want her to think I'll keep being that guy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preferred Summary=1</head><p>???? Follow the instructions and the example(s) above ????</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample to Annotate</head><p>Text -{text} Summary 1 -{summary1} Summary 2 -{summary2}</p><p>Ending Preferred Summary= Table <ref type="table">1</ref>: An example of a prompt fed to an off-the-shelf LLM to generate AI preference labels. "{text}", "{sum-mary1}", and "{summary2}" are populated with unlabeled examples, and a preference distribution is obtained by computing the softmax of the log probabilities of generating the tokens "1" vs. "2".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Position Bias in LLM Labelers</head><p>Model Size % Same Position Preferred PaLM 2 L 18% PaLM 2 S 21% PaLM 2 XS 56%</p><p>Table <ref type="table">5</ref>: Position bias is more prevalent in smaller model sizes, as indicated by "% Same Position Preferred", which measures the percentage of examples where the LLM prefers the same position even after swapping the order of candidates. Analysis is conducted using the "OpenAI + COT 0-shot" prompt.</p><p>Our analysis suggests that the LLMs used for preference labeling are biased by the order in which candidates are shown. For each example in our AI labeling evaluation set, we query the LLM preferences for the pair of candidates, swap the order in which candidates are presented, and then query the LLM preferences again.</p><p>We consider an LLM to be more biased if it prefers the same position on both the original and reversed inferences. For example, let candidates A and B be in positions 1 and 2 for the first inference and then in positions 2 and 1 for the second, respectively. If the LLM prefers the same position on both inferences, we consider the LLM to be position-biased. We measure position bias by computing "% Same Position Preferred" -the percentage of inference pairs where this occurs, and a higher metric value indicates a more biased LLM.</p><p>We find that PaLM 2 L, S, and XS prefer the same position 18%, 21%, and 56% of the time, respectively (see Table <ref type="table">5</ref>), suggesting that position bias is inversely proportional to model size. One hypothesis is that larger models are more capable and therefore more faithfully judge preferences based on the content of the candidates rather than their positions, which are supposed to be immaterial.</p><p>We also observe that for PaLM 2 L, of the 18% of cases where it prefers the same position on both inferences, 94% of the time it prefers the first candidate shown. On the other hand, PaLM 2 S and XS show affinity for the second candidate shown, preferring it 91% and 99% of the time, respectively, when the same position is preferred on both inferences. These biases are statistically significant under a two-sided binomial test at ? = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B A2C for Language Models</head><p>Consider a generic MDP (X , A, R, P, ?). At each step t, given the current state X t ? X and the next action A t ? A, the model receives a reward R t = R(X t , A t ) and transitions to the next state X t+1 = (X t , A t ).</p><p>In the context of language models, X t is the concatenation of the input text and all text the policy has generated up to time t. Action A t is the token decoded at time t by the stochastic policy ? ? (?|X t ) from the considered vocabulary, where ? represents the policy parameters. Finally, the reward R t is given by the RM. The RM is only evaluated when the language model response has been fully generated, and therefore all rewards before the last token are 0 while the reward corresponding to the final token is R T last .</p><p>The cumulative sum of rewards received when following the policy ? from a state-action pair (X t = x, A t = a) is called the return. Generally, it is defined as Z ?</p><p>x,a = T last s=t ? s-t R s . However, since only the terminal reward is non-zero and we use ? = 1, the return can be simplified to Z ?</p><p>x,a = R T last . Given a trajectory (X t , A t , R t ) T last t=0 generated under ? ? , the Advantage Actor Critic estimator is defined as follows:</p><p>where the bar notation denotes that no gradient is passed through the advantage term during the policy training phase.</p><p>The baseline value function V ? ? (x) estimates the return-to-go R T last when following the policy ? ? and is parametrized by ? <ref type="bibr" target="#b32">(Williams, 1992;</ref><ref type="bibr" target="#b27">Sutton et al., 1999)</ref>. It is trained with the following loss:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Model Training Details</head><p>We train a SFT model with a batch size of 128 for a single epoch. We use the Adafactor <ref type="bibr" target="#b25">(Shazeer and Stern, 2018)</ref> optimizer with a learning rate of 10 -5 , and we set a maximum input and output length of 1024 and 128 tokens, respectively. Each RM is trained for 3 epochs, which is when loss and accuracy curves plateau. We use the Adafactor optimizer with a learning rate of 10 -5 and a batch size of 128.</p><p>For reinforcement learning, we sample from our language model policies with a temperature of T = 0.9 to encourage exploration. We train with a batch size of 128 and learning rate of 10 -5 for 8 epochs, resulting in ?1 million episodes. We set ? = 0.05 for the KL divergence loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Controlling for Summary Length</head><p>Our RLAIF and RLHF policies tend to generate longer summaries than the baseline SFT policy. For the summaries sent to human evaluation, the mean character-length of summaries produced by RLAIF, RLHF, and SFT policies were 164, 161, and 132, respectively. We conduct post-hoc analysis to estimate the win rates of RLAIF and RLHF vs. SFT after controlling for length. We take an approach similar to <ref type="bibr" target="#b26">Stiennon et al. (2020)</ref>. For each of our RL policies, we train a logistic regression model where the input is the ratio of the RL summary length to the SFT summary length (in characters) and the target is a binary label indicating whether RL was preferred to SFT. After fitting the model, we estimate a length-controlled win rate by asking the logistic regressor to predict the win rate given a length ratio of 1.0, which represents the scenario where both the RL and SFT summaries are of equal length.</p><p>After controlling for length, our estimated win rates for RLAIF and RLHF vs. SFT are 59% and 61%, respectively (see Figure <ref type="figure">6</ref>). Both RL policies continue to outperform the SFT policy by a similar margin, supporting our initial conclusion that RLAIF is comparable to RLHF.</p><p>We note that this post-hoc method of controlling for length is imperfect, as it assumes the logistic regression model can accurately learn the relationship between summary length and human preference. A more principled approach would be to have all policies generate summaries of similar length (e.g. by encouraging policies to generate summaries of a fixed length during optimization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Base" preamble</head><p>You are an expert summary rater. Given a piece of text and two of its possible summaries, output 1 or 2 to indicate which summary is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"OpenAI" preamble</head><p>A good summary is a shorter piece of text that has the essence of the original. It tries to accomplish the same purpose and conveys the key information from the original post. Below we define four evaluation axes for summary quality: coherence, accuracy, coverage, and overall quality.</p><p>Coherence: This axis answers the question "how coherent is the summary on its own?" A summary is coherent if it's easy to understand when read on its own and free of English errors. A summary is not coherent if it's difficult to understand what the summary is trying to say. Generally, it's more important that the summary is understandable than it being free of grammar errors.</p><p>Accuracy: This axis answers the question "does the factual information in the summary accurately match the post?" A summary is accurate if it doesn't say things that aren't in the article, it doesn't mix up people, and generally is not misleading.</p><p>Coverage: This axis answers the question "how well does the summary cover the important information in the post?" A summary has good coverage if it mentions the main information from the post that's important to understand the situation described in the post. A summary has poor coverage if someone reading only the summary would be missing several important pieces of information about the situation in the post. A summary with good coverage should also match the purpose of the original post (e.g. to ask for advice).</p><p>Overall quality: This axis answers the question "how good is the summary overall at representing the post?" This can encompass all of the above axes of quality, as well as others you feel are important. If it's hard to find ways to make the summary better, the overall quality is good. If there are lots of different ways the summary can be made better, the overall quality is bad.</p><p>You are an expert summary rater. Given a piece of text and two of its possible summaries, output 1 or 2 to indicate which summary best adheres to coherence, accuracy, coverage, and overall quality as defined above.</p><p>Table <ref type="table">6</ref>: The "Base" and "OpenAI" preambles given to the LLM labeler to obtain preference labels.</p><p>Preamble A good summary is a shorter piece of text that has the essence of the original. ... Given a piece of text and two of its possible summaries, explain which summary best adheres to coherence, accuracy, coverage, and overall quality as defined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample to Annotate</head><p>Text -{text} Summary 1 -{summary1} Summary 2 -{summary2}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ending</head><p>Consider the coherence, accuracy, coverage, and overall quality of each summary and explain which one is better.</p><p>Rationale:</p><p>Table <ref type="table">7</ref>: The template used for the "OpenAI + COT 0-shot" prompt, with some text removed for brevity. For COT prompts, we first decode a response from the LLM and then concatenate it with the original prompt and the ending "Preferred Summary=" before following the scoring procedure in Section 3.1 to obtain a preference distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preamble</head><p>A good summary is a shorter piece of text that has the essence of the original. ... Given a piece of text and two of its possible summaries, explain which summary best adheres to coherence, accuracy, coverage, and overall quality as defined above. Thoughts on Summary 1 -Coherence -7. Rationale: The summary is generally understandable, though it could be written with better grammar. Accuracy -9. Rationale: The summary doesn't say things that aren't in the original text, and isn't misleading. Coverage -6. Rationale: The summary covers most of the important information in the post and conveys the gist of the original text. However, it places more emphasis on "no contact" and could have mentioned the smothering/neediness to be more complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-shot Exemplar</head><p>Overall Quality -7. Rationale: The summary represents the post fairly well with only minor areas where it could be improved.</p><p>Thoughts on Summary 2 -Coherence -3. Rationale: The summary is long-winded and has several grammatical errors. Accuracy -4. Rationale: The summary mentions that the author broke no contact, but this is incorrect. Otherwise, it is accurate. Coverage -8. Rationale: The summary covers the key points in the original text. Overall Quality -4. Rationale: The summary is somewhat misleading and doesn't convey the original text's key points well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preferred Summary=1</head><p>???? Follow the instructions and the example(s) above ????</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample to Annotate</head><p>Text -{text} Summary 1 -{summary1} Summary 2 -{summary2}</p><p>Ending Thoughts on Summary 1 -Table <ref type="table">8</ref>: The template used for the "OpenAI + COT 1-shot" prompt, with some text removed for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample to Annotate</head><p>Text -I met my current girlfriend online around 6 months ago when another one of our online friends was going through some problems. ... However, I feel incredibly insecure as she has yet to begin college. I always hear stories about how people change and their minds change about relationships or their s/o especially in an international long distance (Canada/USA Thoughts on Summary 2 -Coherence -9. Rationale: The summary is concise and easy to understand. Accuracy -9. Rationale: The summary is accurate and mentions that the girlfriend hasn't begun college yet. Coverage -9. Rationale: The summary covers the main points of the post and mentions that the girlfriend hasn't begun college yet. Overall Quality -9. Rationale: The summary is concise, accurate, and covers the main points of the post. Table <ref type="table">9</ref>: An example of the different chain-of-thought rationales produced by the 0-shot ("OpenAI + COT 0-shot") vs. 1-shot ("OpenAI + COT 1-shot") prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample to Annotate</head><p>Text -I feel that out of principle I should be refunded the adoption fee since the agency's foster home infected the kittens with the parasite. Both cats were born in the foster home and there are 20 other cats. Do I have any legal right to ask for the fee back? Or help with the cost of treating? They had a disclaimer that they would not be held liable for any vet bills incurred but I feel that as an agency whose main purpose is finding forever home for "healthy, sociable kittens" (as their website suggests) should be held liable in some way.</p><p>Any thoughts or information would be greatly appreciated as I am not sure about the rules or laws dealing with adoption agencies.</p><p>Summary 1 -Adopted kittens from an agency who said it would not be held liable for any vet bills. Two days later they are diagnosed with Giardia. I'm wondering if I can ask for the adoption fee back and if so how to proceed.</p><p>Summary 2 -Adopted kittens have parasite. Want to know if I can get reimbursed from the agency.</p><p>Rationale @ T = 0 Rationale: Summary 1 is better. It is coherent, accurate, and has good coverage. It also has good overall quality. Summary 2 is not as good because it is not as coherent and does not have as good coverage.</p><p>Rationale @ T = 1 Rationale: Summary 2 is more coherent and accurate. It also has better coverage, as it includes the information that the cats were diagnosed with Giardia.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Man?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<title level="m">Concrete problems in ai safety</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Palm: Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miljan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.626</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Bosheng</forename><surname>Ding</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chengwei</forename><surname>Qin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Linlin</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ken</forename><surname>Yew</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Boyang</forename><surname>Chia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shafiq</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lidong</forename><surname>Joty</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bing</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017. 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11173" to="11195" />
		</imprint>
	</monogr>
	<note>Is GPT-3 a good data annotator?</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Avoiding wireheading with value reinforcement learning</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial General Intelligence: 9th International Conference, AGI 2016</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-07-16">2016. July 16-19, 2016</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of data augmentation approaches for NLP</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.84</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="968" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reward learning for efficient reinforcement learning in extractive document summarisation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Mesgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12894</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Gilardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meysam</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma?l</forename><surname>Kubli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.15056</idno>
		<title level="m">Chatgpt outperforms crowd-workers for textannotation tasks</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Tr?bacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phoebe</forename><surname>Thacker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14375</idno>
		<title level="m">Improving alignment of dialogue agents via targeted human judgements</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Rohan Anil Google</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Shafey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kefan</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Hernandez Abrego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cl?ment</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachi</forename><surname>Crepy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>D?az</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangxiaoyu</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Fienber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hurwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abe</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Kenealy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Music</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzhao</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongtao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aroma</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Mahendru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maysam</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Moussalem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><surname>Nham</surname></persName>
		</author>
		<editor>Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu</editor>
		<imprint/>
	</monogr>
	<note>Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><surname>Kakade</surname></persName>
		</author>
		<title level="m">A natural policy gradient. Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reward design with language models</title>
		<author>
			<persName><forename type="first">Minae</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalesha</forename><surname>Bullard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Dac</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nghia Trung</forename><surname>Van Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuat</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien Huu</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.16039</idno>
		<title level="m">Okapi: Instructiontuned large language models in multiple languages with reinforcement learning from human feedback</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengliang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01852</idno>
		<title level="m">Summary of chatgpt/gpt-4 research and perspective towards the future of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17651</idno>
		<title level="m">Self-refine: Iterative refinement with self-feedback</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An overview of bard: an early experiment with generative ai</title>
		<author>
			<persName><forename type="first">James</forename><surname>Manyika</surname></persName>
		</author>
		<ptr target="https://ai.google/static/documents/google-about-bard.pdf" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2023" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tuning language models as training data generators for augmentation-enhanced few-shot learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarek</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="24457" to="24477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri?</forename><surname>Puigdom?nech Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>CoRR, abs/1602.01783</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09332</idno>
		<title level="m">Webgpt: Browser-assisted questionanswering with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Large language models sensitivity to the order of options in multiple-choice questions</title>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Pezeshkpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estevam</forename><surname>Hruschka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.11483</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Factually consistent summarization via reinforcement learning with textual entailment feedback</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Roit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Cideron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sertan</forename><surname>Girgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?onard</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orgad</forename><surname>Keller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.00186</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno>CoRR, abs/1804.04235</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Lamda: Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.09193</idno>
		<title level="m">Towards zero-label language learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A study of reinforcement learning for neural machine translation</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3612" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to extract coherent summary via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">5602</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rlcd: Reinforcement learning from contrast distillation for language model alignment</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Daniel M Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593</idno>
		<title level="m">Fine-tuning language models from human preferences</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
