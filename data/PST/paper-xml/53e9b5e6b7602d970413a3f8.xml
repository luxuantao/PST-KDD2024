<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Motion Segmentation with Level Sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@mia.uni-saarland.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">CVPR Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<addrLine>Römerstr. 164</addrLine>
									<postCode>53113</postCode>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
							<email>bruhn@mia.uni-saarland.de</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="laboratory">Mathematical Image Analysis Group</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<addrLine>Building 27</addrLine>
									<postCode>66041</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
							<email>weickert@mia.uni-saarland.de</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="laboratory">Mathematical Image Analysis Group</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<addrLine>Building 27</addrLine>
									<postCode>66041</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Motion Segmentation with Level Sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1E532D1F85F3A29163FF553A8F45778</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We suggest a variational method for the joint estimation of optic flow and the segmentation of the image into regions of similar motion. It makes use of the level set framework following the idea of motion competition, which is extended to non-parametric motion. Moreover, we automatically determine an appropriate initialization and the number of regions by means of recursive two-phase splits with higher order region models. The method is further extended to the spatiotemporal setting and the use of additional cues like the gray value or color for the segmentation. It need not fear a quantitative comparison to pure optic flow estimation techniques: For the popular Yosemite sequence with clouds we obtain the currently most accurate result. We further uncover a mistake in the ground truth. Coarsely correcting this, we get an average angular error below 1 degree.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Motion estimation and segmentation are strongly related topics that can benefit from each other. While motion information gives important hints on how to partition an image, the separation of regions releases motion estimation from the problem of ambiguities near motion boundaries.</p><p>Both tasks have a long tradition in computer vision. In motion estimation, especially variational techniques based on modifications of the method of Horn and Schunck <ref type="bibr" target="#b14">[15]</ref> have yielded very convincing results. Important milestones on the way to today's stateof-the-art have been presented in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Also in the scope of segmentation, variational methods perform fairly well. Pioneering works in this field include <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>. In recent years, variational segmentation techniques have often been based on level sets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, which offer many advantages, among others the convenient implicit representation of regions and their separating contours. For the same reason, also the work presented in this paper will make use of the level set framework.</p><p>In most cases, segmentation relies on the image gray value or color, sometimes extended by texture representations. In case of image sequences, however, also motion information has been a popular cue for segmentation over the past decades, e.g. in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6]</ref>. Most motion segmentation techniques thereby handle the optic flow, or just the image difference, as a precomputed feature that is fed into a standard segmentation method.</p><p>In contrast to those methods, some more recent approaches embark on the strategy to solve the problems of optic flow estimation and segmentation simultaneously <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>. Cremers and Soatto introduced in <ref type="bibr" target="#b12">[13]</ref> the level set based motion competition technique. The optic flow is estimated separately for each region by a parametric model, and the region contour is evolved directly by means of the fitting error of the optic flow. This idea has been adopted in <ref type="bibr" target="#b1">[2]</ref>, where the parametric model has been replaced by the better performing non-parametric optic flow model from <ref type="bibr" target="#b6">[7]</ref>.</p><p>Also the method proposed in the present paper follows the concept of motion competition where the fitting error of the optic flow drives the contour represented by level sets. Further on, the underlying optic flow model is also based on the technique from <ref type="bibr" target="#b6">[7]</ref>. In this respect, our method is close to the approach in <ref type="bibr" target="#b1">[2]</ref>.</p><p>However, the method presented here is not restricted to two regions. The energy functional is inspired by the energy from <ref type="bibr" target="#b28">[29]</ref> where also the number of regions is an unknown variable. Optimization of this energy is performed by means of a methodology suggested for texture segmentation in <ref type="bibr" target="#b7">[8]</ref>: Starting with one region, regions are recursively split as long as this splitting decreases the total energy. For dealing with non-translational flow fields, we have to extend this idea by higher order region models. The recursive splitting yields the number of regions and appropriate initializations for the level set functions. These can then be evolved while the optic flow is simultaneously estimated within the regions. Moreover, our technique is not restricted to two frames but can also take more frames into account. In general, increasing the number of frames yields more accurate results.</p><p>The motion competition framework suffers from the fact that the optic flow is nonunique in those parts of the image with little or no structure. Although the smoothness term in variational techniques provides a dense flow field, it does not support the localization of the contour. Therefore, we also present a modification that allows the integration of additional cues like the gray value, color, or possibly even texture without the need to manually weight these different kinds of information.</p><p>Furthermore, we modify the underlying optic flow functional from <ref type="bibr" target="#b6">[7]</ref>. Instead of matching only the gray value and gradient of a single pixel, we match a small Gaussian neighborhood around this pixel. It turns out that this matching of neighborhoods provides the variational model for the nonlinear version of the so-called CLG method from <ref type="bibr" target="#b9">[10]</ref>.</p><p>Apart from all these modelling aspects our paper also offers an experimental evaluation with excellent results. Thanks to the level set framework the precision at motion boundaries is so high that one can even notice a mistake in the ground truth of the popular Yosemite sequence. In fact, it turns out that the horizon is shifted one pixel towards the bottom. Correcting this, we obtain a further improvement of the results. However, even with the original ground truth, our method provides the most accurate flow fields in the literature.</p><p>Paper organization. The next section introduces the variational energy model that integrates motion estimation and multi-region segmentation. Section 3 then deals with the minimization of this energy. This includes the iterative scheme and the way how the level sets can be initialized. It is further described how the motion segmentation model can be extended by additional cues. Section 4 presents experimental results and a comparison to methods from the literature. The paper is concluded by a brief summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>The variational model is based on the optic flow functional in <ref type="bibr" target="#b6">[7]</ref> and the segmentation model presented in <ref type="bibr" target="#b28">[29]</ref>. It further makes use of the level set framework <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref> in order to represent regions and their boundaries.</p><p>Given an image sequence I(x, y, t) : Ω → R, we seek at each (spatiotemporal) point x := (x, y, t) the optic flow vector w(x) := (u(x), v(x), 1) that describes the shift of the pixel at (x, y, t) to its new location (x + u, y + v, t + 1) in the next frame. Additionally, we seek the set of level set functions Φ i (x, y, t) : Ω → R, i = 1, ..., N , that represent the partitioning of the image domain Ω into disjoint regions Ω i . The regions are represented such that x ∈ Ω i if and only if Φ i (x) &gt; 0, and region contours are represented by the zero-level lines of Φ i . The number of regions N is also a free variable that is to be optimized.</p><p>In order to allow the motion estimation to benefit from the segmentation, we estimate a separate flow field w i for each region. The final flow field w can then be assembled from w i and the level set functions Φ i .</p><p>Our model can be described by the spatiotemporal energy functional</p><formula xml:id="formula_0">E(w, Φ, N) = N i=1 Ω H(Φi) Ψ |I(x + wi) -I(x)</formula><p>gray value constancy</p><formula xml:id="formula_1">| 2 + γ Ψ | ∇2I(x + wi) -∇2I(x)</formula><p>gradient constancy</p><formula xml:id="formula_2">| 2 dx + N i=1 Ω α Ψ |∇3ui| 2 + |∇3vi| 2 spatiotemporal smoothness + ν |∇3H(Φi)| contour length +λ dx<label>(1)</label></formula><p>that is sought to be minimized under the constraint of disjoint regions. Thereby, ∇ 3 denotes the spatiotemporal gradient (∂ x , ∂ y , ∂ t ) , while ∇ 2 stands for its spatial counterpart. Moreover, H(s) denotes a regularized Heaviside function, which is in our case the error function. Its derivative H (Φ) is a Gaussian with standard deviation 1.</p><p>The robust function Ψ (s 2 ) := √ s 2 + 0.001 2 is applied in order to deal with outliers. In contrast to <ref type="bibr" target="#b6">[7]</ref> we apply a separate robust function to both the gray value and the gradient constancy assumption, as suggested in <ref type="bibr" target="#b8">[9]</ref>. This has the advantage that the relative importance of both terms is locally adjusted to the reliability of each term. The robust function applied to the smoothness term yields a model that allows for discontinuities. This is important, since although the level set framework captures the main motion discontinuies, there may be further smaller discontinuities within the regions. The parameter γ ≥ 0 globally weights the influence of the gradient constancy assumption, whereas α ≥ 0 determines the penalty for non-smooth flow fields.</p><p>The energy in (1) follows the basic concept of motion competition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2]</ref>. In comparison to the classic Chan-Vese model <ref type="bibr" target="#b11">[12]</ref>, the distance between the local value and the mean of the region is replaced by the local energy evoked by the data term of the optic flow model, i.e., it is tested how well the estimated optic flow fits the constancy assumptions. This energy drives the contour. Simultaneously, the model separates the estimation of the optic flow within the different regions. The parameter ν ≥ 0 weights the penalty for the length of the region contours.</p><p>In order to allow for more than two regions, the classic two-phase model is replaced by a level set based version of the segmentation functional from <ref type="bibr" target="#b28">[29]</ref>. It handles not only more than two regions, but also optimizes the number of regions. The fixed penalty λ = 0.1 is added for each region in order to keep the number of regions small. The additional optimization variable increases the complexity of the segmentation task and, consequently, makes it more dependent on the initialization. How to find a good initialization will be an issue in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adding Color and Neighborhood Constraints</head><p>To further improve the quality of the estimated optic flow, the model can be extended by making use of color. For this purpose, the term from (1) that is responsible for the gray value constancy</p><formula xml:id="formula_3">E1(x) = Ψ |I(x + wi(x)) -I(x)| 2 (2)</formula><p>is replaced by a term that supposes a multi-channel image I = (I 1 , I 2 , I 3 ):</p><formula xml:id="formula_4">E2(x) = Ψ 3 k=1 |I k (x + wi(x)) -I k (x)| 2 . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>The gradient constancy assumption in ( <ref type="formula" target="#formula_2">1</ref>) is changed in the same way. This simple extension motivates another idea. Instead of assuming only the gray value and gradient of the point itself to stay constant, one can suppose also the neighborhood around this point not to change during motion. This is the standard assumption for block matching approaches. Since it is well known that block matching methods suffer seriously under affine transformations and run into problems at motion boundaries, we consider only a very small Gaussian neighborhood</p><formula xml:id="formula_6">K ρ (x, y) = 1 2πρ 2 exp -x 2 +y 2 2ρ 2</formula><p>with ρ = √ 2. Consequently, we obtain additional constraints in the new term</p><formula xml:id="formula_7">E3(x) = Ψ R 2 Kρ(ξ) |I(x -ξ + wi(x)) -I(x -ξ)| 2 dξ (4)</formula><p>that can improve the robustness in the case of corrupted data. The gradient constancy assumption is extended in the same way. Minimization of the resulting energy functional in the next section will show that the latter extension comes down to the so-called combined local-global (CLG) method suggested in <ref type="bibr" target="#b9">[10]</ref>, which has been demonstrated to yield good results also in the presence of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimization</head><p>In the optimization problem stated in (1), the optic flow field, the regions, and the number of regions are all unknown. Since variational approaches perform a local optimization, one has to take care of local optima. The initialization decides which optimum is hit by the method. For both optic flow estimation and segmentation techniques, coarseto-fine strategies have proven their value in this respect. Coarse-to-fine strategies shift the problem of initialization to successively coarser scales. Starting with a scale where multiple optima are rare, one can use the coarse result as initialization for the next finer scale. In the iteration scheme described in Section 3.3 we also make use of this concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initialization of the Regions at the Coarsest Scale</head><p>Before we minimize (1) by deriving the Euler-Lagrange equations, this section focuses on the initialization of the regions. Note that the number of regions N is an integer variable that cannot be optimized with a variational approach. For this purpose, we adopt a technique presented in the scope of texture segmentation in <ref type="bibr" target="#b7">[8]</ref> that recursively splits the image domain for determining both N and good initializations for Φ i . To this end, one needs a preliminary estimate of the optic flow, which is obtained by computing the optic flow without any partitioning.</p><p>The splitting works on the coarsest level, i.e., the flow is downsampled to this scale. The coarsest scale is chosen such that the image comprises at least 30 pixels in x and ydirection. At this scale, dominant regions are still visible and most disturbing structures have vanished. The scale of the temporal axis remains unchanged.</p><p>One starts with the whole image domain as one region, in which the level set function Φ is initialized by 8 horizontal stripes/boxes. The region is then split into two regions by minimizing the energy</p><formula xml:id="formula_8">E(Φ) = Ω -H(Φ) log p1 -(1 -H(Φ)) log p2 + ν |∇3H(Φ)| dx<label>(5)</label></formula><p>which leads to the gradient descent</p><formula xml:id="formula_9">∂τ Φ = H (Φ) log p1 p2 + ν div ∇3Φ |∇3Φ| . (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>At a first step, the two regions are modelled by the Gaussian probability densities</p><formula xml:id="formula_11">pj(u, v) ∝ 1 √ 2π(σu)j exp - (u -(μu)j) 2 2(σu) 2 j • 1 √ 2π(σv)j exp - (v -(μv)j) 2 2(σv) 2 j (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where (μ u/v ) j and (σ u/v ) j are the means and the variances of the precomputed optic flow components u and v in the two regions. They are updated iteratively with the evolving contour. Since this model assumes constant flow fields in the regions, which is often unrealistic, the model is, after 500 iterations, extended to a linear approximation model u(x, y, t) ≈ a j + b j x + c j y + d j t. Its parameters are estimated within the regions by least squares. With this model one can replace (u</p><formula xml:id="formula_13">-(μ u ) j ) 2 in (7) by (u -a j -b j x- c j y -d j t) 2 and (σ u ) 2 j by Ωj (u -a j -b j x -c j y -d j t) 2 dx/|Ω j |.</formula><p>The values for v can be handled the same way. After again 500 iterations, we finally switch to a quadratic model, which can coarsely capture most smooth motion fields. The model parameters are again obtained by least squares, and the counterparts to the expressions in <ref type="bibr" target="#b6">(7)</ref> are the deviation (ua jb j xc j yd j te j xxf j yyg j tth j xyr j xts j yt) 2 , the corresponding standard deviation, and the same for v. The successive increase of the model complexity avoids possible local optima that might disturb the partitioning when starting directly with the quadratic model.</p><p>The quadratic model is finally used to measure the energy according to <ref type="bibr" target="#b4">(5)</ref>, both in the original region and the separated regions. If the energy decrease is larger than λ|Ω|, the region is split and the same process is repeated for the two new regions. When all further splits do not decrease the energy anymore, one has determined N . Moreover, a good initialization for Φ i is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Euler-Lagrange Equations</head><p>With the abbreviations from <ref type="bibr" target="#b6">[7]</ref> (Ix)i := ∂xI(x + wi), (Ixy)i := ∂xyI(x + wi), (Iy)i := ∂yI(x + wi), (Iyy)i := ∂yyI(x + wi), (Iz)i := I(x + wi) -I(x), (Ixz)i := ∂xI(x + wi) -∂xI(x), (Ixx)i := ∂xxI(x + wi), (Iyz)i := ∂yI(x + wi) -∂yI(x) <ref type="bibr" target="#b7">(8)</ref> one can derive the following Euler-Lagrange equations from (1):</p><formula xml:id="formula_14">H(Φi) Ψ (Iz) 2 i ) (Ix)i(Iz)i + γ Ψ (Ixz) 2 i + (Iyz) 2 i ((Ixx)i(Ixz)i + (Ixy)i(Iyz)i) -α div Ψ |∇3ui| 2 + |∇3vi| 2 ∇3ui = 0, H(Φi) Ψ (Iz) 2 i ) (Iy)i(Iz)i + γ Ψ (Ixz) 2 i + (Iyz) 2 i ((Iyy)i(Iyz)i + (Ixy)i(Ixz)i) -α div Ψ |∇3ui| 2 + |∇3vi| 2 ∇3vi = 0, H (Φi) -Ψ ((Iz) 2 i ) -γ Ψ (Ixz) 2 i + (Iyz) 2 i + ν div ∇ 3 Φ i |∇ 3 Φ i | = 0.<label>(9)</label></formula><p>Obviously, the flow estimates w i are only influenced by the image data in areas where H(Φ i ) &gt; 0, i.e., Φ i &gt; 0. Thus they cannot be disturbed by data outside the region. The contour evolution is driven by the fitting energy of the optic flow. Note that the corresponding Euler-Lagrange equation does not respect the additional constraint of disjoint regions yet. This has to be ensured in the gradient descent equation by establishing a competition between neighboring regions <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_15">∂τ Φi = H (Φi) ei -max j =i H (Φ j )&gt;0.3 (ej, ei -1) , e k := -Ψ ((Iz) 2 k ) -γ Ψ (Ixz) 2 k + (Iyz) 2 k + ν div ∇ 3 Φ k |∇ 3 Φ k | . (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>Here, each region competes with the best performing neighboring region. This ensures that each pixel is part of exactly one region: the one where it fits best. The extensions to color images and the matching of neighborhoods lead to simple adaptations in <ref type="bibr" target="#b8">(9)</ref>. Using (4) instead of (2) leads to replacing</p><formula xml:id="formula_17">(I x ) 2 i by K ρ * (I x ) 2 i , (I y ) 2 i by K ρ * (I y ) 2 i , (I x ) i (I y ) i by K ρ * ((I x ) i (I y ) i )</formula><p>, and so on, where K ρ * (•) denotes a convolution with K ρ . One realizes that the resulting Euler-Lagrange equations coincide with those from the CLG method in <ref type="bibr" target="#b9">[10]</ref>. The same way, one obtains the color case by replacing (I x ) 2 i by 3 k=1 ((I k ) x ) i ) 2 and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Iteration Scheme</head><p>The iteration scheme for solving for w i and Φ i is similar to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2]</ref> and consists of three nested iteration loops. Starting with the level set functions and the preliminary optic flow from Section 3.1 at the coarsest scale, the most outer iteration loop transfers the current flow estimates w i and the level set functions Φ i to the next finer scale before warping the second frame according to w i towards the first one. Thus in each iteration, only an update (du, dv) i on w i has to be computed; see <ref type="bibr" target="#b6">[7]</ref>. The scaling factor between two successive levels is η = 0.95 like in <ref type="bibr" target="#b6">[7]</ref>. Depending on the size of the image, it determines the number of outer iterations. The parameter ν is scaled at each level by 0.0002 • A 0.7 where A denotes the size of the image at the respective level. This scaling has been determined empirically in many segmentation experiments not restricted to the motion segmentation examples in this paper.</p><p>The central iteration loop is a fixed point iteration loop that removes the remaining nonlinearity in the optic flow equations. Furthermore it alternates the solution of the resulting linear equations and the update on Φ i . We perform 10 of these central iterations, as suggested in <ref type="bibr" target="#b6">[7]</ref>.</p><p>There are two inner iteration loops, one for solving the linear equations on (du, dv) i via SOR with over-relaxation parameter ω = 1.95, and one for evolving the level sets via <ref type="bibr" target="#b9">(10)</ref>. We perform 15 SOR iterations and 50 iterations on the level sets. The curvature term in <ref type="bibr" target="#b9">(10)</ref> is implemented by mean curvature motion restricted to the narrow band given by H (Φ i ) &gt; 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Integrating Additional Cues for Segmentation</head><p>The location of the contour can only be determined reliably by the data fitting error of the optic flow, if there is distinctive data available. In areas with little structure, the optic flow is not uniquely determined and hence cannot drive the contour. For such cases it is helpful to integrate cues besides motion for the contour evolution. This can be achieved by adding the term <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8</ref>]</p><formula xml:id="formula_18">EImage = - N i=1 Ω H(Φi) log pi dx<label>(11)</label></formula><p>to <ref type="bibr" target="#b0">(1)</ref>. It includes a statistical region model by means of the probability densities p i . For making use of a color image I = (I 1 , I 2 , I 3 ), we model p i as</p><formula xml:id="formula_19">pi(I) ∝ 3 k=1 1 √ 2πσ ik exp - (I k -μ ik ) 2 2σ 2 ik (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>with the local means μ ik and standard deviations σ ik of channel k in region i.</p><p>Unfortunately, the contributions of the color channels and the optic flow can be different by some orders of magnitude and depend severely on the choice of the parameters in the optic flow model. A further weighting parameter seems necessary to balance the contributions. However, a manual choice of this parameter can be avoided.</p><p>One can verify that using a Gaussian model including the standard deviations like in <ref type="bibr" target="#b11">(12)</ref> makes the model independent from a different scaling of each feature channel. The same independence from scaling can also be achieved for the motion channel by normalizing the fitting error in <ref type="bibr" target="#b9">(10)</ref> by the average error in the whole image domain. Together with the contribution due to <ref type="bibr" target="#b10">(11)</ref> one obtains:</p><formula xml:id="formula_21">e k := Ψ ((Iz) 2 k ) + γ Ψ (Ixz) 2 k + (Iyz) 2 k 1 |Ω| Ω Ψ (I 2 z ) + γ Ψ I 2 xz + I 2 yz dx + log p k + ν div ∇3Φ k |∇3Φ k | . (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>The normalization factor, like the standard deviation in <ref type="bibr" target="#b11">(12)</ref>, can be regarded as an adaptive weight for the motion term that avoids a manual choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative Evaluation</head><p>The Yosemite sequence with clouds, created by Lynn Quam, is currently still the most interesting test sequence for comparing motion estimation techniques, as it contains many typical challenges: One large discontinuity at the horizon and some smaller ones in the canyon, divergent and translational motion, relatively large displacements in the lower left, brightness changes in the sky, and small occlusions at the image boundaries. Furthermore, the ground truth and many published results from other methods are available.</p><p>Table <ref type="table">1</ref> shows a comparison of our results to those from the literature using the angular error measure introduced in <ref type="bibr" target="#b3">[4]</ref>. Obviously, modeling the sky and the canyon by separate regions yields a significant improvement in comparison to the method in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>. Also the changes in comparison to the method in <ref type="bibr" target="#b2">[3]</ref> still have a large impact, Table <ref type="table">1</ref>. Comparison between our results and those from the literature with 100% density for the Yosemite sequence with cloudy sky. AAE = average angular error. STD = standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technique</head><p>AAE ± STD Alvarez et al. <ref type="bibr" target="#b0">[1]</ref> 5.53 • ± 7.40 • Mémin-Pérez <ref type="bibr" target="#b17">[18]</ref> 4.69 • ± 6.89 • Bruhn et al. <ref type="bibr" target="#b9">[10]</ref> 4.17 1.73 • ± 5.85 • Bruhn-Weickert <ref type="bibr" target="#b8">[9]</ref> 1.72 • ± 6.88 • Our method (frames 8,9)</p><p>1.67 • ± 6.30 • Our method (frames 7,8,9)</p><p>1.39 • ± 6.32 • Our method (all frames)</p><p>1.22 • ± 6.37 •    <ref type="bibr" target="#b6">[7]</ref>. Fig. <ref type="figure" target="#fig_1">1</ref> depicts the resulting contour and the remaining error between the estimated flow and the ground truth. Only few areas with larger errors persist. One of these is still the horizon, which is actually very well estimated by the partitioning. The width of the error is almost exactly one pixel along the horizon, which is a good motivation to take a closer look at the ground truth.</p><p>Indeed it turns out that the ground truth delivered with the sequence is erroneous, as demonstrated in Fig. <ref type="figure" target="#fig_2">2</ref>. Among other mistakes, the horizon is consistently one pixel too low. While this has not been decisive as long as the techniques had large errors at the horizon, it becomes quite important as soon as one is able to correctly estimate the flow there. We coarsely corrected the mistake by shifting the first 75 lines of the ground truth one pixel towards the top. We then obtained an average angular error of 1.40 • ± 3.69 • for the method with two frames and 0.92 • ± 3.35 • for the spatiotemporal version. In particular, the improvement of both statistical measures -the mean error and the standard deviation -confirms that our observation of an erroneous ground truth was right. This is also reflected in the corresponding error plots in Fig. <ref type="figure" target="#fig_3">3</ref>. They show no more than a few misclassified pixels at the horizon. In fact, our results for the Yosemite sequence with clouds are so accurate that they even outperform the results obtained by the method in <ref type="bibr" target="#b6">[7]</ref> for the much less challenging variant without cloudy sky.</p><p>The shifted horizon is the most significant mistake in the so-called ground truth, but unfortunately not the only one. The reader is invited to compare a zoomed version of frame 8 and the ground truth in order to realize further discrepancies in the canyon region that cannot be corrected so easily. In the near future, it may hence be necessary to have a good successor of this sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Motion Segmentation with Multiple Objects</head><p>While the Yosemite sequence allows for comparing the quality of the estimated optic flow to that of other methods, it cannot demonstrate one of the main novelties of this paper, namely the possibility to deal with more than two regions. Therefore, we show in Fig. <ref type="figure" target="#fig_4">4</ref> a test scenario with two objects moving in different directions and the camera moving towards them. This yields a divergent flow field for the background and two nearly translational motion fields for the two objects. In this experiment, we also integrated the CIELAB color channels into the segmentation. They can help to improve the result in areas with little structure where the optic flow is not well-determined. Due to the implicit weighting, it was not necessary to put explicit weights to the optic flow term and the color channels. The free parameters were set to α = 550, γ = 50, and ν = 0.5, and the images were presmoothed with σ = 1 as above.</p><p>Fig. <ref type="figure" target="#fig_4">4</ref> shows the segmentation result and the estimated flow field where the hue represents the direction and the intensity the length of the flow vectors. With the same parameter λ = 0.1 as for the Yosemite sequence, three regions have been detected by the initialization part. The final contours are very precise and the estimated optic flow is not disturbed by the occlusions that appear near the object boundaries. Note the hull of the boat having basically no gradients in motion direction. Here, integrating color information helps significantly to determine the object boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>We have presented an approach for joint motion estimation and segmentation with a non-parametric motion model. It is capable to automatically detect and deal with an arbitrary number of regions, and it can take more than two frames into account. Moreover, it has been shown that it is possible to make use of further cues besides the motion information. To the best of our knowledge, our experiments yielded the currently most accurate results in the literature. The accuracy of the motion boundaries is so high that it is even possible to spot a mistake of a one-pixel shift in the ground truth of the Yosemite sequence. Obviously, today's motion estimation techniques are partially more accurate than certain presumably correct flow fields. To support further research, our next effort hence will be to provide a new, possibly also more challenging, synthetic test sequence with ground truth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>• ± 7.72 • Papenberg et al. (frames 8,9) [23] 2.44 • ± 6.90 • Amiaz-Kiryati [2] 2.04 • ± 7.83 • Papenberg et al. (all frames) [23] 1.78 • ± 7.00 • Amiaz-Kiryati [3]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left: Frame 8 from the Yosemite sequence together with the estimated contour. Right: Angular error between the estimated flow field and the ground truth.</figDesc><graphic coords="8,63.59,444.81,145.25,115.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Discrepancy between frame 8 and the ground truth. In the ground truth, the horizon is consistently 1 pixel too low, which cannot be explained by interpolation artifacts.</figDesc><graphic coords="9,52.28,68.67,163.23,69.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Angular error with the corrected ground truth for the spatial (left) and the spatiotemporal method (right)</figDesc><graphic coords="9,63.03,203.23,145.25,115.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Top row: Two input images with two moving objects. Both objects move straight forward. The camera moves into the scene. Bottom left: Segmentation result. Bottom right: Estimated motion. The color distinguishes the direction of the flow vector, the intensity its magnitude.</figDesc><graphic coords="10,54.53,434.99,155.63,116.72" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>A. Leonardis, H. Bischof, and A. Pinz (Eds.): ECCV 2006, Part I, LNCS 3951, pp. 471-483, 2006. c Springer-Verlag Berlin Heidelberg 2006</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Luis Garrido (University Pompeu Fabral, Barcelona, Spain) for raising the question if there is an error in the ground truth of the Yosemite sequence.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reliable estimation of dense optical flow fields with large displacements</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="56" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dense discontinuous optical flow via contour-based segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiryati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Image Processing</title>
		<meeting>International Conference on Image essing<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">Sept. 2005</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1264" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Piecewise-smooth dense optical flow via level sets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiryati</surname></persName>
		</author>
		<idno>VIA-2005-6-2</idno>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<pubPlace>Israel</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Vision and Image Analysis Laboratory, School of Electrical Engineering, Tel Aviv University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Beauchemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="77" />
			<date type="published" when="1994-02">Feb. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The robust estimation of multiple motions: parametric and piecewise smooth flow fields</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="104" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Franc ¸ois. Motion segmentation and qualitative dynamic scene analysis from an image sequence</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="182" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -Proc. 8th European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="volume">3024</biblScope>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Level set based segmentation of multiple objects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bülthoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Giese</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004-08">Aug. 2004</date>
			<biblScope unit="volume">3175</biblScope>
			<biblScope unit="page" from="415" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards ultimate motion estimation: Combining highest accuracy with real-time performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th International Conference on Computer Vision</title>
		<meeting>10th International Conference on Computer Vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2005-10">Oct. 2005</date>
			<biblScope unit="page" from="749" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kanade meets Horn/Schunck: Combining local and global optic flow methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
		<author>
			<persName><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geodesic active contours</title>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="61" to="79" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Motion competition: A variational framework for piecewise parametric motion segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internatonal Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A finite element method for the simulation of Rayleigh-Taylor instability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dervieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thomasset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Mathematics</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Rautman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">771</biblScope>
			<biblScope unit="page" from="145" to="158" />
			<date type="published" when="1979">1979</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Approximation Methods for Navier-Stokes Problems</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conformal curvature flows: from phase transitions to active vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kichenassamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Olver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tannenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yezzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archive for Rational Mechanics and Analysis</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="275" to="301" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A multigrid approach for hierarchical motion estimation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mémin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th International Conference on Computer Vision</title>
		<meeting>6th International Conference on Computer Vision<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="933" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical estimation and segmentation of dense motion fields</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mémin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="155" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Boundary detection by minimizing functionals, I</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1985-06">June 1985</date>
			<biblScope unit="page" from="22" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An investigation of smoothness constraints for the estimation of displacement vector fields from image sequences</title>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Enkelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="565" to="593" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton-Jacobi formulations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sethian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="12" to="49" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Highly accurate optic flow computation with theoretically justified warping</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<idno>124</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2005-01">Jan. 2005</date>
			<pubPlace>Saarbrücken, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Mathematics, Saarland University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geodesic active regions: A new paradigm to deal with frame partition problems in computer vision</title>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="249" to="268" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geodesic active regions and level set methods for motion estimation and tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="282" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Velocity as a cue to segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="390" to="394" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Determining optical flow for irregular domains by minimizing quadratic functionals of a certain class</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="38" />
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining motion and contrast for segmentation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="543" to="549" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Region competition: unifying snakes, region growing, and Bayes/MDL for multiband image segmentation</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="884" to="900" />
			<date type="published" when="1996-09">Sept. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
