<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-27">27 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yan</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Yuan</surname></persName>
							<email>yifan.yuan@intel.com</email>
						</author>
						<author>
							<persName><forename type="first">Zeduo</forename><surname>Yu</surname></persName>
							<email>zeduoyu2@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Reese</forename><surname>Kuper</surname></persName>
							<email>rkuper2@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ipoom</forename><surname>Jeong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ren</forename><surname>Wang</surname></persName>
							<email>ren.wang@intel.com</email>
						</author>
						<author>
							<persName><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Kim</surname></persName>
							<email>nskim@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-27">27 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.15375v1[cs.PF]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The high demand for memory capacity in modern datacenters has led to multiple lines of innovation in memory expansion and disaggregation. One such effort is Compute eXpress Link (CXL)-based memory expansion, which has gained significant attention. To better leverage CXL memory, researchers have built several emulation and experimental platforms to study its behavior and characteristics. However, due to the lack of commercial hardware supporting CXL memory, the full picture of its capabilities may still be unclear to the community. In this work, we explore CXL memory's performance characterization on a state-of-the-art experimental platform. First, we study the basic performance characteristics of CXL memory using our proposed microbenchmark. Based on our observations and comparisons to standard DRAM connected to local and remote NUMA nodes, we also study the impact of CXL memory on end-to-end applications with different offloading and interleaving policies. Finally, we provide several guidelines for future programmers to realized the full potential of CXL memory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The explosive need of storing and processing data in the datacenters and the limited bandwidth and capacity scalability of the traditional DDR memory interface have demanded new memory interface technologies and system architectures. Among them, Compute eXpress Link (CXL) <ref type="bibr" target="#b25">[28]</ref> has emerged as one of the most promising technology not only for memory capacity/bandwidth expansion, but also for memory disaggregation in both the industry and academia.</p><p>CXL is an open standard made by a joint effort of major hardware vendors and cloud providers in 2019 and is still evolving rapidly. Specifically, compared to the conventional PCIe interconnect, it provides a set of new features that enable the CPU to communicate with peripheral devices (and their attached memory) in a cache-coherent way with Load/Store semantics. As such, memory-related device extension is one of CXL's major targeting scenarios <ref type="bibr" target="#b11">[14,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b23">26]</ref>. As the de-facto standard for future datacenter, major hardware vendors have announced CXL support in their product roadmaps [2, <ref type="bibr" target="#b11">14,</ref><ref type="bibr" target="#b16">19,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b23">26]</ref>.</p><p>Given its popularity and promising visions, CXL memory has attracted much attention in the community. However, due to the lack of commercially-available hardware (especially CPU) with CXL support, most recent studies on CXL memory have been based on emulation using multi-socket NUMA systems, as CXL memory is exposed as a NUMA node <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b21">24]</ref>. As such, these studies may not be able to accurately model and characterize the CXL memory in the real world.</p><p>With the arrival of Intel 4 th -generation Xeon scalable CPU (Sapphire Rapids or SPR) <ref type="bibr" target="#b16">[19]</ref> and commodity CXL devices <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b22">25]</ref>, we are able to start to understand the practical characteristics of CXL memory, as well as customizing software systems that make the most out of such characteristics. In this work, we conduct a comprehensive analysis on CXL memory with multiple microbenchmarks and end-to-end applications on our testbeds consisting of Intel SPR CPUs and Intel Agilex-I FPGA based CXL memory (CXL controller hardened in R-Tile) <ref type="bibr">[11]</ref>. From our microbenchmarks, we find that CXL memory behaves differently from memory in a remote NUMA node, which is often used for emulation. Compared to NUMAbased memory, real CXL memory has: (1) higher latency, (2) fewer memory channels (leads to lower throughput), and (3) different transfer efficiency under various operations.</p><p>Based on the aforementioned observations, we also apply CXL memory to three real applications exhibiting different memory access behaviors. We find that they have diverse sensitivities to the CXL memory offloading. Specifically, we found that (1) ?s-latency database is highly sensitive to the increase in memory latency, (2) ms-latency microservices, having layers of intermediate computations, are less affected when the databases run on CXL memory, (3) memory-intensive ML inference is sensitive to the random access throughput offered by CXL memory. In all cases, interleaving memory across the CPU-attached DRAMs and CXL memory reduces the performance penalty introduced by CXL memory.</p><p>Subsequently, we provide some practical guidelines for users to optimize their software stacks/library for the highest performance after analyzing the performance characteristics of the microbenchmarks and applications running on a system using CXL memory. For example, one should use evenly distribute the bandwidth across CXL memory and DRAM to maximize the performance; one should use cache-bypassing instructions for data movement from/to CXL memory; for a single CXL memory channel, since a few threads can easily saturate the load or store bandwidth, one should limit the number of write threads to CXL memory to reduce write interference; and one should target read-heavy application that run in ms-level latency, where the higher CXL memory latency can be amortized by intermediate computations.</p><p>The remainder of this paper is organized as follows. We give a brief introduction to CXL in ? 2 and describe our experimental setup in ? 3. We then present our findings by profiling CXL memory with our microbenchmark in ? 4, and with three representative applications in ? 5. Finally, we provide some guidelines for making efficient use of CXL memory in ? 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>2.1 Compute eXpress Link (CXL) PCI Express (PCIe) is a standard for high-speed serial computer expansion bus that replaces the older PCI buses. Since 2003, the bandwidth has doubled in each generation, and as of PCIe Gen 5, the bandwidth has reached 32 GT/s (i.e., 64 GB/s with 16 lanes). Its point-to-point topology aid by the growth of bandwidth enables low-latency, high-bandwidth communication with PCIe-attached devices such as Graphics Processing Units (GPUs), Network Interface Cards (NICs), and NVMe Solid-State Drives (SSDs).</p><p>CXL <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b25">28]</ref> builds a cache-coherent system over the PCIe physical layer. Similar to the standard PCIe communication, where data transfers with Transaction-Layer Packets (TLPs) headers and Data-Link-Layer Packets (DLLPs), subset of the CXL protocol uses predefined headers and 16 B blocks to transfer data. In CXL 1.1, depending on the protocol and the data transferred, the CXL hardware will pack the header and data into a 68 B flit (64 B CXL data + 2 B CRC + 2 B Protocol ID) based on a set of rules described in the CXL specification <ref type="bibr" target="#b3">[5]</ref>. Unless otherwise stated, CXL refers to CXL 1.1 for the remainder of this paper.</p><p>The CXL standard defines three separate protocols: CXL.io, CXL.cache, and CXL.mem. CXL.io uses features like TLP and DLLP from standard PCIe transactions <ref type="bibr" target="#b9">[12]</ref>, and it is mainly used for protocol negotiation and host-device initialization. CXL.cache and CXL.mem use the aforementioned protocol headers for the device to access the host's memory and for the host to access the device's memory, respectively.</p><p>By combining these three protocols, CXL identifies three types of devices for different use cases. Type-1 devices use CXL.io and CXL.cache, which usually refer to SmartNICs and accelerators where host-managed memory does not apply. Type-2 devices support all three protocols. These devices, like GP-GPUs and FPGAs, have attached memory (DRAM, HBM) that the host CPU can access and cache, and they also use CXL.cache for device-to-host memory accesses. Type-3 devices support CXL.io and CXL.mem, and such devices are usually treated as memory extensions to existing systems. In this paper, we will focus on Type-3 devices and present the lower-level details of CXL.mem.</p><p>Since the CXL.mem protocol only accounts for host-to-device memory accesses, the protocol consists of two simple memory accesses: read and write from the host to the device memory. Each access is accompanied by a completion reply from the device. The reply contains data when reading from the device memory and only contains the completion header in the case of write. Figure <ref type="figure">1</ref> illustrates the round trips of these accesses. The CXL.mem protocol is communicated between the CPU home agent and the CXL controller on the device. While the home agent handles the protocol, the CPU issues load and store instructions to access memory in the same way as it accesses DRAM. This has an advantage over other memory expansion solutions, such as Remote Direct Memory Access (RDMA), which involves DMA engine on the device and thus has different semantics. Integrating load/store instructions with CXL.mem also means the CPU will cache the PCIe attached memory in all level of its caches, which is impossible for any other memory extension solution, beside persistent memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CXL-enabled Commodity Hardware</head><p>CXL requires supports from both the host CPU side and the device side. As of today, in addition to some research prototypes, multiple CXL-enabled memory devices have been designed by major hardware vendors such as Samsung <ref type="bibr" target="#b11">[14]</ref>, SK Hynix <ref type="bibr" target="#b26">[29]</ref>, Micron <ref type="bibr" target="#b23">[26]</ref>, and Montage <ref type="bibr" target="#b22">[25]</ref>. To facilitate more flexible memory functionalities and near-memory computing research, Intel also enables CXL.mem on its latest Agilex-I series FPGA <ref type="bibr" target="#b8">[10]</ref>, where the CXL-and memory-related IP cores are hard coded on the chiplet <ref type="bibr" target="#b17">[20]</ref> to achieve high performance. On the host CPU side, Intel's latest 4 th Gen Xeon Scalable Processor (codename Sapphire Rapids, SPR) is the first highperformance commodity CPUs to support CXL 1.1 standard <ref type="bibr">[2,</ref><ref type="bibr" target="#b16">19]</ref>. We anticipate that more and more hardware vendors will have richer CXL supports in their products in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>In this work, we use two testbeds to evaluate the latest commodity CXL hardware, as summarized in or in the Sub-NUMA Clustering (SNC) mode, where each chiplet operates as a small NUMA node. Such flexibility allows users to fine-tune their system to fit their workload characteristics and apply fine-grain control over resource sharing and isolation. In our experiments, we will explore how memory interleaving across SNC and CXL.mem will affect the performance of applications. We also conduct some experiments on a dual-socket system, with two Intel Platinum 8460H and the same DDR5 DRAM, to draw some comparison between the regular NUMA-based memory and CXL.mem.</p><p>For a CXL memory device, the system has one Intel Agilex-I Development Kit <ref type="bibr" target="#b8">[10]</ref>. It has 16 GB 2666MT/s DDR4 DRAM as CXL memory, and it is connected to the CPU via an x16 PCIe Gen 5 interface. It is transparently exposed to the CPU and OS as a NUMA node having 16 GB memory without CPU cores, and the usage of the CXL memory is the same as regular NUMA-based memory management.</p><p>Note that, the CXL protocol itself does not define the configurations of the underlying memory. Such configurations include but are not limited to capacity, medium (DRAM, persistent memory, flash chips, etc.), and the number of memory channels. Hence, different devices may exhibit diverse performance characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Characteristic Study via Microbenchmark</head><p>In this section, we present our findings in evaluating CXL memory using our microbenchmark. We believe that this analysis provides insights into how CXL memory users can leverage CXL memory more efficiently according to their use cases. We will also compare these results with the assumptions and emulations from some recent work on CXL memory, where CXL memory is emulated using cross-NUMA data accesses with some additional latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Microbenchmark Description</head><p>To thoroughly examine the properties of CXL memory, we have developed an extensive microbenchmark called MEMO. This benchmark is designed to target various use cases of CXL memory and runs on the Linux user space. Users can provide command-line arguments to specify the workloads to be executed by <ref type="bibr">MEMO.</ref> We plan to open source MEMO.</p><p>In particular, MEMO provides the capability to (1) allocate memory from different sources, including the local DDR5 memory, the CXL memory CPU-less NUMA node, or the remote-socket DDR5, by using the numa_alloc_onnode function, (2) launch a specified number of testing threads, pin each thread to a core, and optionally enable or disable prefetching within the cores, and (3) perform memory accesses using inline assembly language. The benchmark reports the memory access latency or aggregated bandwidth from different instructions, such as Load, Store, and Non-temporal Store, with all memory accesses done using AVX-512 instructions. Additionally, MEMO can perform pointer chasing on a region of memory, and by varying the working set size (WSS), the benchmark can show how the average access latency changes as WSS crosses the different sizes of the cache hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Latency Analysis</head><p>In a latency test, MEMO starts by flushing the cacheline at the tested address and immediately issues a mfence. Then, MEMO issues a set of nop instructions to flush the CPU pipeline. When testing with load instructions, we record the time it takes to access the flushed-out cacheline; when testing with store instructions, we record the time it takes to do temporal store then a cacheline write back (clwb), or the execution time of non-temporal store, followed by a sfence. Additionally, we tested the average access latency through pointer chasing in a large memory space while all prefetching is disabled. Figure Figure <ref type="figure" target="#fig_0">2</ref> shows the latency across the four tested instructions, average pointer chasing latency with 1GB of memory space, and pointer chasing latency within different working set sizes.</p><p>Our results (Figure <ref type="figure" target="#fig_0">2</ref>) show CXL memory access latency is about 2.2? higher than the 8-channel local-socket-DDR5 Prefetching at all levels are disabled in both cases (DDR5-L8) accesses, while the single channel remote-socket DDR5 (DDR5-R1) is 1? ? 2.7? higher than that of DDR5-L8.</p><p>According to the prior study on persistent memory <ref type="bibr" target="#b28">[31]</ref>, accessing a recently flushed cacheline could incur a higher latency than normal cache-miss accesses due to extra cache coherence handshake for such flushed cachelines. Pointer chasing reflects a more realistic access latency experienced by applications. In MEMO, the working set is first brought into the cache hierarchy in a warm-up run. The right figure in Figure <ref type="figure" target="#fig_0">2</ref> shows the average memory access time with various working set size. Each jump in pointer chasing latency corresponds to crossing the boundary of L1, L2, and LLC size. The result in Figure <ref type="figure" target="#fig_0">2</ref> shows that pointer chasing in CXL memory has 4x higher latency than that of DDR5-L8 access. The pointer chasing latency on CXL memory is 2.2? higher than that of DDR5-R1 accesses. Interestingly, DDR5-R1 exhibits higher latency compared to CXL memory when the working set size falls between 4MB to 32MB. We believe this discrepancy can be attributed to the difference in LLC size, with Xeon 8460H having an LLC size almost twice as large as that of Xeon 6414U <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b7">9]</ref>. It is worth noting that CXL memory's longer access latency can be partly attributed to its FPGA implementation, despite the CXL controller and DDR4 memory controller being hardened on the FPGA chip. Although we anticipate that an ASIC implementation of the CXL memory device will result in improved latency, we maintain that it will still be higher than that of regular cross-NUMA access, primarily due to the overhead associated with the CXL protocol. Additionally, our real-application profiling in Sec. 5 revealed a decrease in latency penalty depending on the specific characteristics of the application. It should also be noted that the benefits of FPGA-based CXL devices lie in their ability to add (inline) accelerating logic on the CXL memory data path, as well as to offload memory-intensive tasks in a near-memory fashion.</p><p>On the other hand, non-temporal store instructions with sfence on CXL memory have notably lower latency than cacheline writeback after temporal store. Both operations transfer data from the CPU core to CXL memory, while the latency difference is about 2.6?. Such latency difference is due to the read-for-ownership (RFO) behavior in CXL's MESI cache coherence protocol, where cachelines are loaded into the cache for each store miss. This difference in access latency is later translated into bandwidth difference, which will be discussed in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bandwidth Analysis</head><p>In our bandwidth test, MEMO performs blocks of sequential or random access within each testing thread. The main program calculates the average bandwidth for a fixed interval by summing the number of bytes accessed. To facilitate a fair comparison of memory channel count, we tested remotesocket DDR5 with only 1 memory channel (DDR5-R1) alongside CXL memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Sequential Access Pattern</head><p>Sequential access reflects the maximum possible throughput of the memory scheme under certain operation, and the result is shown in Figure <ref type="figure" target="#fig_1">3</ref>. During testing with the DDR5-L8, we observed that the load bandwidth scaled linearly until it peaked at the maximum bandwidth of 221 GB/s with approximately 26 threads. In comparison, non-temporal store instructions reached their maximum bandwidth at 170 GB/s, which is lower than that of the load instructions but with a lower thread count of approximately <ref type="bibr" target="#b13">16</ref>.</p><p>In contrast, CXL memory exhibits a distinct bandwidth trend when compared to DDR5-L8. Specifically, with load instructions, CXL memory attains its maximum bandwidth with approximately 8 threads, but this value drops to 16.8 GB/s when we increase the thread count beyond 12 threads. Non-temporal stores, on the other hand, demonstrate an impressive maximum bandwidth of 22 GB/s with only 2 threads, which is close to the maximum theoretical bandwidth of the tested DRAM. However, this bandwidth drops immediately as we increase the thread count, indicating some interference on the FPGA memory controller.</p><p>The temporal-store bandwidth of CXL memory is significantly lower than that of non-temporal stores, which aligns with the high latency of temporal stores reported in Figure <ref type="figure" target="#fig_2">4</ref>.2. This discrepancy is due to the RFO behavior in temporal stores described in Sec. 4.2, which significantly reduces the transfer efficiency of CXL memory. This reduction is because RFO requires extra core resources and additional flit round trips for both loading and evicting a cache line compared to non-temporal stores.</p><p>Additionally, Figure <ref type="figure" target="#fig_1">3c</ref> show that DDR5-R1's sequentialaccess performance is similar to CXL memory. With the benefit of higher transfer rate and lower latency in both DDR5 and the UPI interconnect, DDR5-R1 shows a higher throughput in loads and non-temporal stores, but similar throughput in temporal stores.</p><p>In addition to the aforementioned instructions, we also tested a new x86 instruction, movdir64B <ref type="bibr" target="#b5">[7]</ref>, which is newly  available on SPR. This instruction moves a 64B data from the source memory address to a destination memory address and explicitly bypasses the cache for both loading the source and storing it to the destination. As shown in Figure <ref type="figure" target="#fig_2">4a</ref>, our results indicate that D2* operations exhibit similar behavior, while C2* operations show lower throughput in general. From these results, we can conclude that the slower load from CXL memory leads to the lower throughput in movdir64B, and this is even more true in the case of C2C.</p><p>New to SPR, the Intel Data Streaming Accelerator (Intel DSA) enables memory moving operations to be offloaded from the host processor. Intel DSA is comprised of work queues (WQs) to hold offloaded work descriptors, and processing engines (PEs) to pull descriptors from the WQs to operate on. Descriptors can be sent synchronously by waiting for each offloaded descriptor to complete before offloading another, or asynchronously by continuously sending descriptors resulting in WQs having many in-flight. With programs designed to best use Intel DSA asynchronously, higher throughput can be achieved. To further improve throughput, operations can be batched to amortize the offload latency. Figure <ref type="figure" target="#fig_2">4b</ref> shows maximum throughput observed performing memory copy operations on the host processor via memcpy() or movdir64B, and synchronously/asynchronously using Intel DSA with varying batch sizes (e.g. 1, 16, and 128). While a non-batched synchronous offload to Intel DSA matches the throughput of non-offloaded memory copying, any level of asynchronicity or batching brings improvements. Additionally, splitting the source and destination data locations yields higher throughput than exclusively using CXL attached memory, with the C2D case reporting higher throughput due to lower write latency on DRAM.</p><p>During our bandwidth analysis, we observed instances where increasing the thread count led to a decrease in bandwidth. While data accesses were sequential within each working thread, the memory controller between the CXL controller and the extended DRAM received requests with fewer patterns as the thread count increased. As a result, the performance of CXL memory was hindered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Random Access Pattern</head><p>To evaluate the performance of MEMO for random block access, we issue a block of AVX-512 access sequentially, but each time with a random offset. This approach enables us to measure the system's performance under realistic working conditions where data access patterns are unpredictable. As we increase the block size across the tested thread counts, the memory access pattern will converge to sequential access where both the CPU cache and the memory controller can enhance the overall bandwidth. To ensure write order in block level, we issue a sfence after each block of nt-store. The result for random block access is shown in Figure <ref type="figure" target="#fig_3">5</ref>. We observe similar patterns across the random block loads when the block size is small (1KB), with all three memory schemes suffering equally from random accesses. However, as we increase the block size to 16KB, a major difference emerges between DDR5-L8 and DDR5-R1/CXL memory. DDR5-L8's bandwidth scales sub-linearly with thread count, while DDR5-R1 and CXL memory benefit less from higher thread count (after 4 threads), and this is even more apparent in CXL memory. The memory channel count plays a crucial role, with DDR5-R1 and our CXL memory device having only one memory channel, while DDR5-L8 has eight channels in total. Random block stores exhibit a similar pattern to loads in terms of thread count, but with the added trend that bandwidth stops scaling with block size.</p><p>The behavior of random block non-temporal stores in CXL memory shows an interesting trend compared to all other tested workloads. Single-threaded nt-store scales nicely with block size, while higher thread counts experience a drop in throughput after reaching some sweet spots of block size and thread count. For instance, the 2-thread bandwidth reaches its peak when the block size is 32KB, and the 4-thread bandwidth peaks at a block size of 16KB.</p><p>We believe that this sweet spot is determined by the memory buffer inside the CXL memory device. Unlike regular Store instructions, nt-store does not occupy tracking resources in the CPU core. Therefore, it is easier to have more on-the-fly nt-store instructions at the same time, which may lead to buffer overflow in the CXL memory device.</p><p>Despite this, non-temporal instructions have the advantage of avoiding RFO (Sec. 4.2) and cache pollution, making them more attractive in the CXL memory setting. Programmers who wish to use nt-store should keep this behavior in mind to make the most out of nt-stores in CXL memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Emulation Using NUMA Systems</head><p>Recent work on CXL memory has usually been conducted by emulating the access latency of CXL memory through cross-NUMA memory access, i.e., by imposing additional latency to main memory accesses <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b21">24]</ref>. However, according to our observations, cross-NUMA emulation model cannot precisely mimic the following characteristics of CXL memory: (1) the impact of limited bandwidth in current CXL devices (unless remote-socket DIMMs are populated with the same number of channels as the CXL memory counterpart), (2) a variety of CXL memory implementations that have higher latency than cross-NUMA accesses (the impact of higher latency becomes more severe in the latency-bounded applications, ? 5.1), and (3) data transfer efficiency under different workloads (i.e., load and non-temporal store bandwidth, ? 4.3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Real-World Applications</head><p>To study the performance impact of CXL memory, we explore the CXL memory by binding the application's memory fully or partially to the CXL memory. Linux offers the numactl program which allows user to (1) bind a program to a specific memory node (membind mode), or (2) prioritize allocation to a memory node and only allocate memory to other nodes when the specified node runs out of memory (preferred mode), or (3) have the allocation spread evenly across a set of nodes (interleaved mode).</p><p>One of the recent patches in the Linux Kernel now allows for fine-grained control over the page interleaving ratio between memory nodes <ref type="bibr" target="#b27">[30]</ref>. This means that, for example, we can allocate 20% of memory to CXL memory if we set the DRAM:CXL ratio to 4:1. To investigate the impact of CXL memory on application performance, we tuned this interleaving ratio for some applications. In addition, we disabled NUMA balancing to prevent page migration to DRAM.</p><p>The performance of these applications using this heterogeneous memory scheme should serve as a baseline for most memory tiering policies. This is because the proposed optimization should, at the very least, perform equally well when compared against a weighted round-robin allocation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Redis-YCSB</head><p>Redis is a high-performance in-memory key-value store popular and widely used in the industry. We use YCSB <ref type="bibr" target="#b4">[6]</ref> to test Redis' performance with different memory allocation schemes by pinning its memory to CXL memory, DRAM or spread across the two. To evaluate system performance, we conducted multiple workloads while throttling query per second (QPS) in the YCSB clients. Specifically, we measured two metrics: (1) the 99 th percentile tail latency among queries, and (2) the maximum sustainable QPS. With the exception of workload D, all workloads used a uniform distribution for requests, ensuring maximal stress on the memory. We also fine-tuned the interleave ratio (DRAM:CXL) to offload a certain amount of memory to CXL, with ratios such as 30:1 (3.23%) and 9:1 (10%) utilized in different experiments.</p><p>Our result in Figure <ref type="figure">6</ref> shows that there exists a significant gap in p99 tail latency at low QPS (20k) when Redis runs purely on CXL memory. This gap remained relatively constant until 55k QPS, at which point the YCSB clients were not able to reach the targeted QPS, resulting in a sudden increase in tail latency. When 50% of Redis's memory was allocated to CXL memory, the p99 tail latency was between that of purely DRAM and purely CXL memory. Although the 50%-CXL memory Redis did not saturate its QPS until 65k, the tail latency spiked around 55k. Finally, the DRAM Redis exhibited a stable tail latency, and its QPS saturated at around 80k.</p><p>We believe the tail latency gap is attributed to the ultra-low response latency of Redis queries, making these ?s-level responses' latency highly sensitive to memory access latency. This correlates well with our latency measurements presented in Sec. 4.2, where CXL memory access latency ranges from hundreds to one thousand nano-second, and it is 2-4x higher than that of DRAM. However, intermediate computations and cache hits reduce the latency difference, in terms of application tail latency, to about 2x before the QPS saturation point.</p><p>On the other hand, the maximum sustainable QPS that CXL memory Redis can deliver (Figure <ref type="figure">7</ref>) is correlated to the random block access bandwidth observed in Sec. 4.3.2, where CXL memory has a much lower single thread load/store bandwidth when compared to local-DDR5 or remote-DDR5.</p><p>Single-thread random access bandwidth is bounded by the memory access latency, where data dependency within a single thread makes the load-store queues in the CPU hard to saturate. Moreover, Figure <ref type="figure">7</ref> shows the trend that having less memory allocated to CXL memory (lowered CXL memory percentage) delivers a higher max QPS across tested all workloads, but none of which can surpass the performance of running Redis purely on DRAM. In this case, memory interleaving is not able to improve a single application's performance as interleaving with CXL memory will always introduce higher access latency. Note that the current CXL memory setup is FPGA-based, where its true merit lies in its flexibility. We expect an ASIC-based CXL memory will provide a relatively lower access latency, improving the performance of latency bounded applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Embedding Reduction in DLRM</head><p>Deep learning recommendation models (DLRM) have been widely deployed in the industry. Embedding reduction, a step within the DLRM inference, is known to have a high memory footprint and occupies 50% to 70% of the inference Testing with 8-channel DRAM and CXL memory; throughput vs. thread count (left); throughput of different memory schemes normalized to DRAM at 32 threads (right) latency <ref type="bibr" target="#b19">[22]</ref>. We tested embedding reduction on DRAM, CXL memory, and interleaved-memory with the same setup as MERCI <ref type="bibr" target="#b19">[22]</ref>.</p><p>Result in Figure <ref type="figure" target="#fig_4">8</ref> shows that running DLRM inference on each scheme scales linearly and differ by the slope, as thread count increases. The overall trend of DDR5-R1 and CXL memory is similar, which aligns with the observations in Sec. 4.3.2, where DDR-R1 and CXL memory has a similar random load/store bandwidth when the access granularity is small. Two points of memory interleaving (3.23% and 50% on CXL memory) are shown in Figure <ref type="figure" target="#fig_4">8</ref>. As we reduce the amount of memory interleaved to CXL, inference throughput increases. However, we again observed that even when having 3.23% of memory allocated to CXL, the inference throughput can't match that of purely running on DRAM. Also note that the pure-DRAM inference throughput scales linearly, and its linear trend seems to extend beyond 32 threads. Combining these two observations, we can conclude that the 8-channel DDR5 memory can sustain DLRM inference beyond 32 threads.</p><p>To demonstrate a scenario where an application is bounded by the memory bandwidth, we tested the inference throughput in the SNC mode. Recall that Intel introduced the sub-NUMA clustering feature in SPR, where the chiplets are split into four individual NUMA nodes, and the memory controllers on each NUMA node work independently of the other nodes. By running inference on one SNC node, we are effective limiting the inference to run on two DDR5 channels, making it memory bounded.</p><p>Figure <ref type="figure">9</ref> illustrates the results of running inference on the SNC mode, with CXL memory interleaved in the same way as in all previous experiments. The green bar in the figure shows the inference throughput on SNC, which stops scaling linearly after 24 threads. At 28 threads, the inference is limited by the two memory channels, and interleaving memory to CXL yields a slightly higher inference throughput. This trend persists, and at 32 threads, putting 20% of memory on CXL increases the inference throughput by 11% compared to the SNC case. In the future, we anticipate that CXL devices will have a bandwidth that is comparable to native DRAM, which will further enhance the throughput of memory bandwidth-bound applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">DeathStarBench</head><p>DeathStarBench (DSB) <ref type="bibr" target="#b12">[15]</ref> is an open-source benchmark suite designed to evaluate the performance of microservices on a system. It uses Docker to launch components of a microservice, including machine learning inference logic, web backend, load balancer, caching, and storage. DSB provides three individual workloads and a mixed workload for a social network framework. Figure <ref type="figure">10</ref> shows the 99 th percentile tail latency for composing a post, reading user timeline, and the mixed workload. We omitted the results for reading home timeline as it does not operate on the databases, and hence, is indifferent to the memory type used for databases. In our experiment, we pinned the components with high working set size (i.e., the storage and caching applications) to either DDR5-L8 or CXL memory. We left the computation-intensive parts to run purely on DDR5-L8. The memory breakdown of these components is shown in Figure <ref type="figure">10</ref>.</p><p>The results in Figure <ref type="figure">10</ref> show that there is a tail latency difference in the case of composing posts, while there is little to no difference in the case of reading user timeline and the mixed workload. Note that the tail latency in DSB is at the millisecond level, which is much higher than that of YCSB-Redis.</p><p>As we analyzed the trace of composing posts and reading user timeline, we found that composing posts involve more database operations, which puts a heavier load on the CXL memory. Meanwhile, most of the response time in reading user timeline is spent on the nginx front end. This allows the longer CXL memory access latency to be amortized among the computation-intensive components, making tail latency much less dependent on the databases' access latency.</p><p>Finally, the mixed workload shows a simulated use case of a real social network, where overall, most users read the posts composed by some users. Although in the mixed workload, having the database pinned to CXL memory shows a slightly higher latency as the QPS increases, the overall saturation point is similar to running the database on DDR5-L8.</p><p>The result from DSB offers an interesting use case of CXL memory, that as long as computation-intensive components are kept in DRAM, caching and storage component that operates on a low demand rate may be assigned to the slower CXL memory, and the application's performance remains mostly the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Best Practices for CXL memory</head><p>Given the unique hardware characteristic of CXL memory, we offer the following insights in making the best use of CXL memory.</p><p>Use non-temporal store or movdir64B when moving data from/to CXL memory. As demonstrated in Sec. <ref type="bibr" target="#b2">4</ref>  behavior. Considering the use cases of CXL memory (e.g., memory tiering), where short-term data reuse is not highly likely, to achieve higher data movement throughput and avoid polluting the precious cache resources, we recommend that nt-store or movdir64B instructions should be prioritized in the corresponding software stacks. Note that, since both nt-store and movdir64B are weakly-ordered, a memory fence is necessary to make sure the data has been written.</p><p>Limit the number of threads writing to CXL memory concurrently. As analyzed before, CXL memory's performance depends on both CPU and the device controller. This is especially true for concurrent CXL memory accesses, as contention can happen in multiple places. Even though the current FPGA-based implementation of the CXL memory controller may limit the internal buffer size and thus the number of on-the-fly store instructions, we anticipate that the problem still exists on the pure ASIC-based CXL memory device. It is desirable to have a centralized communication stub on the CPU software side to conduct the data movement. We recommend that CXL memory should be managed by OS or a dedicated software daemon, instead of everything application.</p><p>Use Intel DSA for bulk memory movement from/to CXL memory. The first two insights may still fall short when transferring a large amount of data across regular DRAM and CXL memory as they consume significant CPU cycles and still have limited instruction/memory-level parallelism. We found that Intel DSA -with its high throughput, flexibility, and fewer restrictions compared to its predecessors -can be a good candidate to further improve the performance and efficiency of such data movement. This is especially useful in a tiered memory system, where data movement often happens in page granularity (i.e., 4KB or 2MB).</p><p>Interleave memory using NUMA polices and other tiering memory methods to evenly distribute the memory load across all DRAM and CXL channels. In addition to using CXL memory as a slower DRAM or faster SSD (e.g., memory tiering), CXL memory can also be interleaved with the regular memory channels to add to the total memory bandwidth, especially when the CXL memory device as more memory channels (and thus more comparable memory bandwidth). Carefully choosing the interleaving percentage/policies can largely mitigate the expected performance degradation.</p><p>Avoid running application with ?s-level latency entirely on the CXL memory. The relatively long access latency of CXL memory can become a major bottleneck for applications that require immediate data accesses in a fine time granularity (?s-level). Redis is such an example -the delayed data accesses due to CXL memory will accumulate to a significant value for end-to-end query processing. This type of application should still consider pinning the data on a faster medium.</p><p>Microservice can be a good candidate for CXL memory memory offloading. Microservice architecture has become a prevailing development methodology for today's internet and cloud services in datacenters because it is flexible, developerfriendly, scalable, and agile. However, its layered and modularized design does impose a higher runtime overhead compared to conventional monolithic applications. Such characterization makes it less sensitive to the underlying cache/memory configurations and parameters. Our study on DSB (see Sec. 5.3) also proves this point. We envision that a significant portion of the microservice data can be offloaded to CXL memory without affecting its latency or throughput performance.</p><p>Explore the potential of inline acceleration with programmable CXL memory devices. Given the insights above, those applications which are suitable for CXL memory offloading may not be highly sensitive to data access latency. This provides more design room for inline acceleration logic inside the CXL memory device -even though such acceleration may add extra latency to data access, such overhead will not be visible from an end-to-end point of view of the target applications. Hence, we still advocate for FPGA-based CXL memory devices for their flexibility and programmability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Application Categorization</head><p>From the profiled applications, we identify two types of applications based on their performance when running on CXL memory: bandwidth-bounded and latency-bounded. Memory-bandwidth-bounded applications typically experience a sublinear increase in throughput beyond a certain thread count. Although running Redis and DLRM inference on CXL memory both yield lower saturation points, one should make a clear distinction between the two, that only DLRM inference is the bandwidth-bounded application. The single-threaded Redis is bounded by the higher latency of CXL memory, which reduces the processing speed of Redis.</p><p>Memory-latency-bounded applications will perceive throughput degrade even when a small amount of their working set is allocated to a higher-latency memory. In the case of databases, they are also likely to show a tail latency gap when running on CXL memory, even when the QPS is far from the point the saturation. Databases such as Redis and memcached that operate on ?s-level latency have the highest penalty when they run purely on CXL memory. In contrast, microservices that operates on ms-level with layers of computation show a promising use case of offloading memory to CXL memory.</p><p>In both cases, however, having interleaved memory between conventional CPU attached DRAM and CXL memory reduces the penalty of the slower CXL memory memory, in both throughput (Sec. 5.2), and tail latency (Sec. 5.1). Such a round-robin strategy should serve as a baseline for tier memory policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>With the rapid development of memory technologies, many new types of memory other than the regular DDR-based DRAM have emerged in datacenters, each with distinct characteristics and trade-offs. These include but are not limited to persistent memory such as Intel Optane DIMM <ref type="bibr" target="#b24">[27,</ref><ref type="bibr" target="#b28">31,</ref><ref type="bibr" target="#b29">32]</ref>, RDMA-based remote/disaggregated memory <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b18">21]</ref>, and even byte-addressable SSD <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">3]</ref>. While they have been extensively studied and profiled, CXL memory, as a new member in the memory tier, still has unclear performance characteristics and indications.</p><p>Since the concept's inception in 2019, CXL has been discussed by a body of researchers. For instance, Meta envisions using CXL memory for memory tiering and swapping <ref type="bibr" target="#b21">[24]</ref>; Microsoft built a CXL memory prototype system for memory disaggregation exploration <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b20">23]</ref>. Most of them used NUMA machines to emulate the behavior of CXL memory. Gouk et al. built a CXL memory prototype on FPGA-based RISC-V CPU <ref type="bibr" target="#b13">[16]</ref>. Different from the prior studies, we are the first to conduct CXL memory research on the commodity CPU and CXL device with both micro-benchmarks and real applications. This makes our research more realistic and comprehensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>CXL has emerged as the future standard of device interconnect with a rich set of useful features, and CXL memory is an important one among them. In this work, based on the state-ofthe-art real hardware, we performed a detailed characterization analysis on CXL memory, using both micro-benchmarks and real applications. With the unique observations on CXL memory behavior, we also propose several usage guidelines for the programmers to best take advantage of CXL memory. We hope this work can facilitate the development and adoption of the CXL memory ecosystem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Access latency. Average latency for single AVX512 load (ld), store and write back (st+wb), non-temporal store (nt-st), and sequential pointer chasing in 1GB space(ptr-chase); Seqneutial pointer chasing latency vs. working set size (right).Prefetching at all levels are disabled in both cases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sequential access bandwidth. An experiment showing the maximum possible bandwidth on local-socket DDR5 with 8-channels (a), and CXL memory (b), and remote-socket DDR5 with 1 channel (c). The grey dash line in (b) shows the theoretical max speed of DDR4-2666MT/s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Data movement bandwidth. An experiment showing data movement efficiency under different workloads. D2C is short for local-DDR5 to CXL memory. All experiments in (b) is done with a single thread.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Random block access bandwidth. Row order (top to bottom): local-socket DDR5, CXL memory, remote-socket DDR5. Column order (left to right): load, store, nt-store. Thread count is denoted in the top legend</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: DLRM embedding-reduction throughput. Testing with 8-channel DRAM and CXL memory; throughput vs. thread count (left); throughput of different memory schemes normalized to DRAM at 32 threads (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: DLRM embedding-reduction throughput</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Testbed configurations</figDesc><table><row><cell>Single socket -Intel ? Xeon 6414U CPUs @2.0 GHz [8]</cell></row><row><cell>32 cores, hyperthreading enabled, running Ubuntu 22.04</cell></row><row><cell>60 MB shared LLC</cell></row><row><cell>Eight DDR5-4800 channels, 128 GB DRAM in total</cell></row><row><cell>CXL 1.1 with PCIe Gen 5 x16, 16 GB DRAM in total</cell></row><row><cell>Dual socket -2? Intel ? Xeon 8460H CPUs @2.0 GHz [9]</cell></row><row><cell>40 cores per socket, hyperthreading enabled, running Ubuntu 22.04</cell></row><row><cell>105 MB LLC per socket, 210 MB in total</cell></row><row><cell>Eight DDR5-4800 channels per socket, 256 GB DRAM in total</cell></row><row><cell>Intel ? Agliex I-series FPGA Dev Kit @400 MHz [10]</cell></row><row><cell>Hard CXL 1.1 IP, runs on PCIe Gen 5 x16</cell></row><row><cell>Single DIMM with 16 GB DDR4-2666</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The server is equipped with Intel Gold 6414U CPU and 128 GB</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CPU</cell><cell>CXL Ctrl</cell><cell>Mem Ctrl</cell></row><row><cell>DIMM Slots</cell><cell>CPU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DIMM Slots</cell><cell></cell><cell></cell><cell>CXL-2.0+</cell><cell></cell></row><row><cell></cell><cell cols="3">UPI / Acc / PCIe / CXL / IO</cell><cell cols="3">UPI / Acc / PCIe / CXL / IO</cell><cell></cell><cell></cell><cell></cell><cell>Type1 Device</cell><cell>MemRd</cell></row><row><cell>DDR5 DDR5</cell><cell>CH1 CH0</cell><cell>iMC</cell><cell>CHA Core / LLC /</cell><cell>Core / LLC / CHA</cell><cell>iMC</cell><cell>CH0 CH1</cell><cell>DDR5 DDR5</cell><cell>CXL Switch</cell><cell></cell><cell>Type2 Device Type3 Device</cell><cell>RdData</cell><cell>Data Read</cell></row><row><cell>DDR5 DDR5</cell><cell cols="3">CH0 CH1 UPI / Acc / PCIe / CXL / IO iMC Core / LLC / CHA</cell><cell cols="3">CH0 CH1 UPI / Acc / PCIe / CXL / IO Core / LLC / CHA iMC</cell><cell>DDR5 P-Mem</cell><cell>CXL Ctrl</cell><cell>Mem Ctrl Mem Ctrl Mem Ctrl</cell><cell>CXL-1.1 Type3 Board DRAM DRAM P-Mem DRAM DRAM DRAM</cell><cell>MemWr Cmp</cell><cell>Write Cmp</cell></row><row><cell cols="13">Figure 1: Typical CXL-enabled system architecture (left) and memory transaction flow of CXL.mem protocol (right)</cell></row><row><cell cols="9">4800MT/s DDR5 DRAM (spread across 8 memory channels).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">In the 4 th generation, an Intel Xeon CPU is implemented as</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">four individual chiplets. Users can decide to use the 4 chiplets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">as a unified processor (i.e., shared Last-Level Cache (LLC),</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Integrated Memory Controllers (iMCs), and root complexes),</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting the byte-accessibility of SSDs within a unified memory-storage hierarchy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abulila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Mailthody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><surname>Flatflash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;19)</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2B-SSD: The case for dual, byte-and block-addressable solid-state drives</title>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA&apos;18)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Design tradeoffs in CXL-based memory pools for public cloud platforms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zardoshti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajadnya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Compute Express Link Specification -Revision 3.0, Version 1.0</title>
		<imprint>
			<date type="published" when="2022-08">August 2022</date>
			<publisher>Compute Express Link Consortium</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with ycsb</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM Symposium on Cloud Computing, SoCC &apos;10</title>
		<meeting>the 1st ACM Symposium on Cloud Computing, SoCC &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Intel? 64 and IA-32 Architectures Optimization Reference Manual</title>
		<author>
			<persName><forename type="first">I</forename><surname>Corporation</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Intel</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Corporation</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/products/sku/231731/intel-xeon-gold-6414u-processor-60m-cache-2-00-ghz/specifications.html" />
		<title level="m">Intel Xeon Gold 6414U Processor</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Corporation</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/products/sku/231744/intel-xeon-platinum-8460h-processor-105m-cache-2-20-ghz/specifications.html" />
		<title level="m">Intel Xeon Platinum 8460H Processor</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Corporation</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/products/details/fpga/development-kits/agilex/i-series/dev-agi027.html" />
		<title level="m">Intel? Agilex? 7 FPGA I-Series Development Kit</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><surname>Cxl Consortium</surname></persName>
		</author>
		<ptr target="https://www.computeexpresslink.org" />
		<title level="m">Compute Express Link (CXL)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Farm: Fast remote memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dragojevi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;14</title>
		<meeting>the 11th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;14<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="401" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Eletronics</surname></persName>
		</author>
		<ptr target="https://github.com/OpenMPDK/SMDK" />
		<title level="m">Scalable Memory Development Kit v1.3</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An open-source benchmark suite for microservices and their hardware-software implications for cloud &amp; edge systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Katarki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ritchken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pancholi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Colen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zaruvinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;19</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Memory pooling with cxl</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient memory disaggregation with infiniswap</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;17</title>
		<meeting>the 14th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;17<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="649" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Incorporated</surname></persName>
		</author>
		<ptr target="https://www.rambus.com/memory-and-interfaces/cxl-memory-interconnect/" />
		<title level="m">Memory Interface Chips -CXL Memory Interconnect Initiative</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Intel launches 4th gen xeon scalable processors, max series cpus</title>
		<ptr target="https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Intel? fpga compute express link (cxl) ip</title>
		<ptr target="https://www.intel.com/content/www/us/en/products/details/fpga/intellectual-property/interface-protocols/cxl-ip.html" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Challenges and solutions for fast remote persistent memory access</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Symposium on Cloud Computing, SoCC &apos;20</title>
		<meeting>the 11th ACM Symposium on Cloud Computing, SoCC &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="105" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient embedding reduction on commodity hardware via sub-query memoization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">U</forename><surname>Sul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><surname>Merci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;21</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="302" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zardoshti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajadnya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><surname>Pond</surname></persName>
		</author>
		<title level="m">Cxl-based memory pooling systems for cloud platforms</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tpp: Transparent page placement for cxl-enabled tiered memory</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhanotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chauhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cxl memory expander controller (mxc)</title>
		<author>
			<persName><forename type="first">Montage</forename><surname>Technology</surname></persName>
		</author>
		<ptr target="https://www.montage-tech.com/MXC" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Patodia</surname></persName>
		</author>
		<ptr target="https://investors.micron.com/static-files/" />
		<title level="m">Micron -Investor Day</title>
		<imprint>
			<date type="published" when="2023">8d23a61f-0c3d-46. 2023</date>
			<biblScope unit="volume">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hemem: Scalable tiered memory management for big data applications and real nvm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raybuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stamler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles, SOSP &apos;21</title>
		<meeting>the ACM SIGOPS 28th Symposium on Operating Systems Principles, SOSP &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compute express linktm (cxltm): Enabling heterogeneous data-centric computing with heterogeneous memory hierarchy</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">SK hynix Introduces Industry&apos;s First CXL-based Computational Memory Solution (CMS) at the OCP Global Summit</title>
		<ptr target="https://news.skhynix.com/sk-hynix-introduces-industrys-first-cxl-based-cms-at-the-ocp-global-summit/" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>SK hynix Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Weiner</surname></persName>
		</author>
		<ptr target="https://lore.kernel.org/linux-mm/YqD0%" />
		<title level="m">mm: mempolicy: N:M interleave policy for tiered memory nodes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Characterizing the performance of intel optane persistent memory: A close look at its on-dimm buffering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth European Conference on Computer Systems, EuroSys &apos;22</title>
		<meeting>the Seventeenth European Conference on Computer Systems, EuroSys &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="488" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An empirical guide to the behavior and use of scalable persistent memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoseinzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th USENIX Conference on File and Storage Technologies, FAST&apos;20</title>
		<meeting>the 18th USENIX Conference on File and Storage Technologies, FAST&apos;20<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
