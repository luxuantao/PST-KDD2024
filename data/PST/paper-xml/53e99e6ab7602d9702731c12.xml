<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do User Preferences and Evaluation Measures Line Up?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<email>m.sanderson@shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>211 Portobello St</addrLine>
									<postCode>S1 4DP, +44 114, 22 22648</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Monica</forename><forename type="middle">Lestari</forename><surname>Paramita</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>211 Portobello St</addrLine>
									<postCode>S1 4DP, +44 114, 22 22648</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>211 Portobello St</addrLine>
									<postCode>S1 4DP, +44 114, 22 22648</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>211 Portobello St</addrLine>
									<postCode>S1 4DP, +44 114, 22 22648</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Information</forename><surname>Search</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Studies</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>211 Portobello St</addrLine>
									<postCode>S1 4DP, +44 114, 22 22648</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Do User Preferences and Evaluation Measures Line Up?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">25511F02669CB7104DF1B91A0C5ED44E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Measurement</term>
					<term>Experimentation Mechanical Turk</term>
					<term>User Experiment</term>
					<term>Evaluation Measures</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents results comparing user preference for search engine rankings with measures of effectiveness computed from a test collection. It establishes that preferences and evaluation measures correlate: systems measured as better on a test collection are preferred by users. This correlation is established for both "conventional web retrieval" and for retrieval that emphasizes diverse results. The nDCG and ERR measures were found to correlate best with user preferences compared to a selection of other well known measures. Unlike previous studies in this area, this examination involved a large population of users, gathered through crowd sourcing, exposed to a wide range of retrieval systems, test collections and search tasks. Reasons for user preferences were also gathered and analyzed. The work revealed a number of new results, but also showed that there is much scope for future work refining effectiveness measures to better capture user preferences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>There is a long tradition of encouraging conducting, and researching evaluation of search systems in the IR community. A test collection and an evaluation measure are together used as a tool to make a prediction about the behavior of users on the IR systems being measured. If measurement using the collection reveals that system A is more effective than system B, it is assumed that users will prefer A over B in an operational setting. One of the striking aspects of almost all the early work in test collections is that the predictions about users implied from such measurements were rarely, if ever, validated. Given that test collections are used to simulate users, that so little validation took place is perhaps surprising.</p><p>In the last ten years a series of papers employing a range of methods conducted such validation. The papers produced contradictory results, some failing to find any link between test collection measures and user preferences, performance, or satisfaction; others finding links, but only when differences between IR systems were large.</p><p>Much of the past work involved a small number of topics, systems, and users; and/or introduced some form of artificial manipulation of search results as part of their experimental method. There was also a strong focus on test collections and not on the relative merits of different evaluation measures.</p><p>Therefore, it was decided to examine, on a larger scale, if test collections and their associated evaluation measures do in fact predict user preferences across multiple IR systems, examining different measures and topic types. The study involved 296 users, working with 30 topics, comparing user preferences across <ref type="bibr" target="#b19">19</ref> runs submitted to a recent TREC evaluation. The research questions of the study were as follows 1. Does effectiveness measured on a test collection predict user preferences for one IR system over another? 2. If such a predictive power exists, does the strength of prediction vary across different search tasks and topic types? 3. If present, does the predictive power vary when different effectiveness measures are employed? 4. When choosing one system over another, what are the reasons given by users for their choice?</p><p>The rest of this paper starts with a literature review, followed by a description of the data sets and methods used in the study. Next, the results of experiments are described, the methods are reflected upon, conclusions are drawn, and future work is detailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PAST LITERATURE</head><p>The past work described here is grouped into two sections, based on the methods used to measure users. Contradictions between the results of the two groups are then discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Measures rarely predict users</head><p>The power to predict user preferences using a test collection and evaluation measure was first examined in the work of Hersh et al <ref type="bibr" target="#b16">[16]</ref> who used the 14 topics and qrels of TREC 6 and 7's interactive track to determine which of two retrieval systems was significantly better. They then conducted an experiment involving 24 searchers, retrieving over six topics of TREC-8: three topics on one system, three on the other. The researchers reported that there was no significant difference in the effectiveness of the searchers when using the different systems. This work was repeated on another test collection <ref type="bibr" target="#b26">[26]</ref> drawing the same conclusion.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19-23, 2010, Geneva, Switzerland.</p><p>Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.</p><p>Allan et al <ref type="bibr" target="#b4">[4]</ref> created artificial document rankings from TREC data each with controlled levels of effectiveness. Users were shown selections of the generated rankings and asked to identify relevant information. Unlike the work described above, a correlation between user behavior and test collection based evaluation measures was found, but mainly when measured differences were large. Turpin &amp; Scholer <ref type="bibr" target="#b27">[27]</ref> repeated the artificial document ranking method, getting thirty users to examine fifty topics. No significant difference in the time users took to find the first relevant document was found. A small significant difference in the number of relevant documents identified was observed for large differences in the MAP of the artificial ranks.</p><p>Inspired by Hersh and Turpin' s method Al-Maskari et al <ref type="bibr" target="#b3">[3]</ref> measured how well groups of users performed on two IR systems. Fifty six users searched from a selection of 56 topics. The researchers showed that test collection based measures were able to predict user behavior, and to some extent a user's level of satisfaction, however only when measured differences between the systems were large.</p><p>Although test collection based work is relatively recent, there is a longer tradition of correlating user outcomes with effectiveness measures calculated on actual searching systems. Tagliacozzo <ref type="bibr" target="#b23">[23]</ref> showed that 18% of ~900 surveyed MEDLINE users appeared unsatisfied with search results despite retrieving a large number of relevant documents. As part of a larger study, Su <ref type="bibr" target="#b22">[22]</ref> examined correlations between precision and user satisfaction; finding no significant link. Hersh et al <ref type="bibr" target="#b15">[15]</ref> examined medical students' ability to answer clinical questions after using a medical literature search engine. No correlation between search effectiveness measures and the quality of the student's answers was found. Huuskonen et al <ref type="bibr" target="#b17">[17]</ref> conducted a similar medical searching experiment reporting the same lack of correlation.</p><p>Smith and Kantor <ref type="bibr" target="#b21">[21]</ref> engaged 36 users to each search 12 information gathering topics on two versions of a web search engine: one, the normal searching system and the other, a degraded version which displayed results starting from rank 300. Users weren't aware they were being shown the different versions. Although no actual effectiveness measures were taken, it is reasonable to assume that there was a significant difference in precision between the versions. However, there was no significant difference in user success in finding relevant items. Smith and Kantor reported that users of the poorer system issued more queries, which appeared to mitigate the smaller number of relevant documents retrieved in each search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Measures predict user behavior</head><p>Measuring users through an analysis of query logs, Joachims <ref type="bibr" target="#b18">[18]</ref> described an experiment showing users different sets of search results; as with previous work although there were measurable differences between the quantity and rank of relevant documents, Joachims saw little difference in users' click behavior. Users given poorer search results still choose top ranked documents. He proposed an alternative approach, which was to interleave the retrieval outputs of the two systems into a single ranking and observe if users tended to click on documents from one ranking more often than the other. The results showed users consistently chose documents from the better part of the interleaved ranking. This method of giving users (unknowingly) a choice and observing their preference was repeated <ref type="bibr" target="#b19">[19]</ref> producing similar results. In this work, small, but measurable changes in document rankings were compared, and significant differences in user behavior were observed. Further analysis of query logs to model user click behavior was conducted by many researchers, e.g. <ref type="bibr" target="#b10">[10]</ref>.</p><p>Thomas et al <ref type="bibr" target="#b25">[25]</ref> described another preference methodology where two sets of search results were presented side-by-side to users who were then asked which of the two they preferred. The method was used to compare the top 10 results of Google and the (presumably worse) Google results in ranks 21-30. They reported a clear preference for the top ranked results over the lower ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lessons drawn from past work</head><p>After reading the first set of research results, one might question the value of all test collection based research, as the only time users show any difference in behavior, success in their work, or preference for searching systems is when large differences in effectiveness between IR systems are measured. In direct contradiction to this, is the smaller body of work in the following section measuring clear preferences by users even for subtle differences in retrieval results. What might be the cause of this apparent contradiction?</p><p>Smith and Kantor's work appears to be the clearest in demonstrating that if it is important for users to locate relevant documents they can cope with the burden of a poorer search engine by re-formulating their query. In addition, Joachims' work appears to show that users will often make do with poorer results. The work in Section 2.1 could be failing to observe differences across users because these two traits simply make human searchers hard to measure.</p><p>As can be seen, there is only limited work using the preference based approach and to the best of our knowledge there is no work using this method to test the correlations between users and evaluations based on test collections. Further, none of the past work has addressed the more nuanced questions of whether certain evaluation measures or search tasks show better prediction of user behavior over others. Although there are a plethora of papers comparing different evaluation measures, almost without exception they report cross-measure correlations or use some form of stability statistic to imply which might be better. The only exception is Al-Maskari et al who examined correlations between user satisfaction and evaluation measures <ref type="bibr" target="#b2">[2]</ref> finding that Cumulative Gain (CG) correlated better with user preferences than P(10), DCG and nDCG, but the experiment was based on a small sample of people.</p><p>Because examination of different measures is almost unexplored, we addressed it here. With a growth of interest in search systems supporting diversity, there is as yet little research examining the predictive power of test collections in relation to diverse queries. Therefore, this paper conducted such a broad investigation into the predictive power of test collections and evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>The experiment required six components: a test collection with diverse topics and QRELS; multiple IR systems; a population of users; a method of measuring them; the selection of effectiveness measures; and a method of selecting which systems to show to users. These components are now described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The test collection</head><p>The 50 million document Category B set of the ClueWeb09 collection was chosen as it was used in a diversity task for TREC's 2009 Web track. Given a short ill specified query, the goal of the diversity task was for participating groups to build IR systems that returned a ranked list of documents that collectively fulfilled the multiple information needs represented by the query.</p><p>For the diversity track, each topic was structured as a set of subtopics, each related to a different user need <ref type="bibr" target="#b14">[14]</ref>. The documents returned in the submitted runs were judged with respect to each subtopic. For each retrieved document, TREC assessors made a binary judgment as to whether or not the document satisfied the subtopic's information need.</p><p>Each one of the subtopics was categorized as being either navigational or informational (from Broder <ref type="bibr" target="#b9">[9]</ref>). The query was also classified as either ambiguous or faceted, with ambiguous queries having multiple distinct interpretations while faceted queries had a single interpretation but with many aspects.</p><p>The structuring of subtopics judged in their own right into aggregated diverse topics, allowed (in this paper) both an experiment on diverse search and on non-diverse search: the first using the aggregated topics, the second treating the subtopics as a large set of ordinary topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IR systems</head><p>A source of different outputs was needed against which user preferences could be measured. Al Maskari et al in their experiments drew from a pool of three live searching systems, however, the researchers often found that the systems performed very differently from each other, which unsurprisingly resulted in large differences in user preference. In the design of the experiments here, it was judged desirable to have more explicit control over the differences between the systems being compared.</p><p>Allan et al and others achieved this by artificially creating search results; we judged it preferable to use actual search output.</p><p>Arni et al <ref type="bibr" target="#b7">[7]</ref> used the runs of an evaluation exercise as a source of search outputs to draw from to show users. From that pool of runs the researchers were able to select those runs that had similar effectiveness scores. For the category B ClueWeb09 collection, 19 diversity runs were submitted from ten research groups, these were the pool of search outputs used. Their use is detailed in 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Measuring user preference</head><p>To measure user preferences between different search results, the side-by-side method from Thomas et al <ref type="bibr" target="#b25">[25]</ref> was chosen. For a particular topic, a pair of runs was selected from the pool and the top ten results (showing title, snippet and URL) were shown to users along with the topic title that generated the search and the subtopic description (referred to as an "aspect" in the interface) that expressed the information need behind the search request (example in Figure <ref type="figure" target="#fig_0">1</ref>). The snippets were generated using a web service from the Bing search engine. Not all ClueWeb09 collection URLs still exist, which meant that 35% of results did not have a snippet. A post hoc analysis of data showed that missing snippets did not appear to influence user preferences.</p><p>Users were asked to indicate which of the two results they preferred. Using QREL data from the web track, effectiveness was measured on the two rankings and the agreement between users and the measures was assessed.</p><p>The aim of the diversity track was to promote searching systems that retrieved documents covering multiple interpretations of the same query, thereby ensuring that the search output was of value to the widest possible range of users. In a pilot experiment, an attempt was made to elicit user preferences for one IR system over another by asking individual users to indicate their preference for a ranking based on the ambiguous topic title alone. The expectation was that users would judge the value of search results relative to the multiple interpretations of a topic. However, it was found that the users were not able to do this reliably.</p><p>Therefore, in the experiments reported here, users were asked to focus on a particular subtopic and judge pairs of rankings in that context. They were asked to imagine they were searching for the subtopic using the query title text. The instructions were worded avoiding terms such as "diversity", so as not to bias choices. No other information about the experiment was given to the users. Users could indicate that the left or right result was better, both were equally good, or none of them were relevant (the ordering of paired systems was randomized). They were also asked to write a reason for their choice.</p><p>Different users were given the different subtopics of a topic and their preferences were aggregated to form a judgment on the diverse topic as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Population of users</head><p>The goal of the research work was to examine the preferences of a large number of users across many IR systems searching on a wide range of topics. It was decided to use the crowd sourcing system Mechanical Turk <ref type="bibr" target="#b5">[5]</ref> to provide the large population. Mechanical Turk users (MTurkers) were asked to judge a set of paired rankings for a set of subtopics. As it was assumed that there could be some disagreement amongst MTurkers, each pairing was seen on average by eight. A "trap question" was shown in an attempt to identify those who were not conducting the experiment in good faith. For every five comparisons shown to an MTurker one was a trap, which was built by pairing a run relevant to the required subtopic with a run for an entirely different topic. MTurkers who did not answer such pairings correctly had all of their answers rejected from the study (example in Figure <ref type="figure" target="#fig_1">2</ref>). In total 342 MTurkers were used, 46 were rejected for failing a trap question (13%), which left 296 whose responses contributed to the results. We did not gather any demographic information from them. MTurkers were paid 8¢ for each block of five pairs they were shown. Many MTurkers worked on more than one block. The median time taken to complete the five pairs was just over 6 minutes. The total cost of the study including initial pilot studies was just under $60.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Selecting measures</head><p>The aim of the work was to examine how well evaluation measures predicted user preferences. Measures for both diversity and conventional IR were examined in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Diversity measures</head><p>With the growth of interest in diversity, a number of evaluation measures were proposed. These measures include Cluster Recall (CR) used in ImageCLEFPhoto 2008 <ref type="bibr" target="#b7">[7]</ref>, Clarke et al's -nDCG <ref type="bibr" target="#b12">[12]</ref>, Agrawal et al's intent aware Precision (IA-PC) <ref type="bibr" target="#b1">[1]</ref>, and Clarke et al.'s <ref type="bibr" target="#b13">[13]</ref> novelty-and rank-biased precision (NRBP).</p><p>Cluster Recall (CR) is based on the subtopic recall (or S-Recall) proposed by Zhai et al. <ref type="bibr" target="#b29">[29]</ref> to assess diversity. The CR at a cutoff rank k, CR(k), is defined as the percentage of subtopics covered by the first k documents in a ranked list. This is a pure diversity measure, i.e. it is not affected by the number of documents covering each cluster, or by their position in the ranked list. Further, it does not incorporate any notion of document novelty within a given subtopic.</p><p>Contrary to CR, both -nDCG and NRBP consider both the number of relevant documents and their rank position over the subtopics of a query. For both measures, each document is assigned a gain value that is a function of the number of subtopics the document covers, and for each subtopic, the number of documents ranked above the given document that cover the same subtopic. The variable  is used to control how important diversity is in the measure. The -nDCG metric is based on the traditional nDCG metric utilizing the aforementioned gain function, while the NRBP metric is based on Rank-Biased Precision (RBP). It is defined by replacing the traditional binary relevance of a document with the aforementioned gain. Thus, for a given subtopic, for =0 the two metrics do not assess the novelty of the subsequent documents that cover this subtopic, while for =1 they only consider relevant the first document that covers the given subtopic, ignoring all the subsequent ones.</p><p>Finally, intent aware Precision at rank k accounts for diversity and the number of relevant documents. Given a query, the precision value at cut-off k is computed for each subtopic separately (i.e. only the documents that cover the subtopic under consideration are considered relevantcalled aspect precision) and the weighted average of the precision values is computed, with weights being the popularity of each subtopic. In the Web Track data the subtopics of a query were assumed to be equally popular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Conventional evaluation measures</head><p>By considering each of the subtopics in the test collection as individual topics with their own QRELS, it was possible to examine differences across alternate conventional evaluation measures. Here nDCG, Mean Reciprocal Rank (MRR), Expected Reciprocal Rank (ERR) <ref type="bibr" target="#b11">[11]</ref> and Precision measured at rank 10, P(10), were the measures chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Selecting the pairs to show users</head><p>As seen in Section 2.1, existing research showed that differences in user performance could be measured on IR systems with large differences in search effectiveness. The challenge was in measuring user preference when searching on IR systems with far smaller differences. Therefore, the selection of run pairs to show the MTurkers focused on finding pairs that were similar to each other. There was a concern that using runs with low effectiveness could result in confusion when choosing between rankings. Therefore, topics where all runs had two or fewer relevant documents in the top ten were removed. This left thirty topics in the dataset.</p><p>A search was conducted across the remaining topics to locate pairs of runs that had the same number of relevant documents in the top ten, done to ensure that the rankings were similar. To enable diversity measures to be tested, runs were only paired when there was more than a minimum difference in subtopic coverage: ∆CR(10) and ∆α-nDCG(10) ≥0.1. Runs submitted by the same research group were not paired together.</p><p>In total, 79 system pairs matching the search criteria were found.</p><p>Each system pair was shown to, on average, eight MTurkers for each of a topic's subtopics. The MTurker judgments for one of the 79 pairs were gathered as follows. Each system pair displayed the two retrieval results for a search based on the query text of a particular topic. The MTurkers were asked to indicate their preference for one of the paired systems in relation to a particular subtopic. Multiple MTurkers were shown the same system/subtopic pair; although if an MTurker failed a trap question, their preference judgments were removed. MTurker preferences were treated as votes for one system or another normalized by the number of MTurkers who examined the system/subtopic pair.</p><p>This process was repeated for each of a topic's subtopics and the mean of the resulting normalized majority values was taken. The system that the majority of MTurkers preferred across the subtopics was selected as the best system for that topic. At the same time a diversity measure was calculated for the two system rankings, the best was the one with the highest effectiveness score. If there was a tie in scores, the pair was not considered. Across the 79 pairs, the number of times that MTurkers agreed/disagreed with the diversity measure was counted. If there was a tie, the MTurkers were judged to have said that the ranks from the systems were equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>The predictive power of test collections/measures was examined on both the diverse web search topics (section 4.1) and their component subtopics (section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">User preferences in diversity search</head><p>The results of the initial experiment are shown in Table <ref type="table">1</ref>. As can be seen, there was a preference amongst users for systems that were measured to be more diverse. Assuming a null hypothesis that MTurkers saw no difference between the paired systems, and the level of agreement was simply due to chance; using a t-test 1 it was found that p&lt;0.05; the null hypothesis was rejected and the level of agreement in Table <ref type="table">1</ref> was found to be significant.  <ref type="table">1</ref> -Agreement differences in small and large ∆α-nDCG Next, the 78 pairs, without a tie in α-nDCG, were sorted by their difference; those pairs greater than the mean of the differences were placed in a large Δ bin; the others in a small Δ bin. The figures for user agreement are also shown in Table <ref type="table">1</ref>. Although the agreement appeared to grow as the size of difference between the two rankings increased, a significance test between large and small Δ showed p&gt;0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Users α-nDCG</head><p>The range of different cluster evaluation measures described above were also examined, see Table <ref type="table" target="#tab_1">2</ref>. In percentage terms little difference was found between the measures, however, there were a large number of tied scores using IA-PC. It is notable that the measure CR provided as effective a prediction of user preference as the other measures. Cluster Recall is simply counting the percentage of topic interpretations that are covered in the ranking. Given that we have observed similar degrees of correlation between different diversity measures and user preferences, we next investigated how these different measures correlated with each other. Given that these measures assess somewhat different aspects of system effectiveness, a strong correlation would indicate that better systems are good in all aspects of effectiveness assessed by these measures. A weak correlation will indicate that different users prefer different qualities of the ranked lists and the sets of users whose preferences agree with each individual measure do not fully overlap even though it so happens to be of similar size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Users</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kendall's τ</head><p>CR@10 NRBP IA-PC@10 α-nDCG@10 0.7956 0.8523 0.8424 CR@10 0.7159 0.7219 NRBP 0.7010 AP-correl.</p><p>CR@10 NRBP IA-PC@10 α-nDCG@10 0.6719 0.8736 0.7867 CR@10 0.6282 0.5492 NRBP 0.6839 Table <ref type="table">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Correlations between diversity measures</head><p>For each measure, we considered the mean values for all systems/runs submitted to the TREC track and over all 50 queries were calculated. For each two measures, we calculated Kendall's , and the AP-correlation <ref type="bibr" target="#b28">[28]</ref>, see Table <ref type="table">3</ref>. Kendall's  is a function of the minimum number of pair wise adjacent interchanges needed to convert one ranking into the other. The AP-correlation is a similar metric, which however mostly accounts for the swaps towards the top of the system rankings, i.e. the disagreements over the top ranked systems. It can be seen that, there is a positive correlation among all measures, the strength of which however differs among different measures. In particular, the most correlated measures are -nDCG and NRBP. IA-PC and -nDCG are also well correlated, however, they mostly agree on the poorly performing systems as indicated by lower AP-correl. Further, there is a positive correlation between CR and -nDCG; however it also concerns the bottom performing systems. Finally, CR and IA-PC correlate well regarding the bottom performing systems but they rank the top performing systems differently.</p><p>Therefore, the weak correlation among several of these measures indicates that indeed they assess different aspects of system performance. However, given the results in Table <ref type="table" target="#tab_1">2</ref> it seems that all of these aspects are important for an average user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">User preferences in traditional search</head><p>If one treats each subtopic of the test collection as a distinct test collection topic, with its own QRELS, one can compare user preferences against traditional test collection measures. In total there were 250 subtopic/system pairs shown to MTurkers. Three standard evaluation measures -nDCG, MRR, and P <ref type="bibr" target="#b10">(10)</ref> and the newer measure ERRwere applied to the pairs and a prediction of which ranking users would prefer was made based on each measure. The standard measures were selected as exemplars of particular features in evaluation: P(10) is a simple count of the number of relevant documents in the top 10; MRR measures the rank of the highest relevant; nDCG combines both number of relevant documents and their rank. Both nDCG and ERR's ability to handle degrees of relevance was not exploited as the diversity track QRELS contained binary judgments only. For all measures, only the top 10 documents were examined.</p><p>If the effectiveness measure for the two rankings were the same, user preferences were not examined. Therefore the number of pairs considered differed across the measures. For example, three pairs were measured as the same by nDCG; therefore, only 247 pairs were evaluated. The results of this analysis are shown in Table <ref type="table" target="#tab_2">4</ref>. No significant difference in percentages between the measures was found. Focusing on nDCG, as in Section 4.1, the pairs were split into two bins: one for pairs with a large Δ and one for a small Δ. The split was defined by the mean difference between the 250 pairs. The figures for user agreement are shown in Table <ref type="table">5</ref>.  <ref type="table">5</ref> -Agreement differences in small and large ∆nDCG Users appeared to agree more when there was a large difference in the evaluation measure than if there was a small one. However, as with the comparison in Table <ref type="table">1</ref> no significant difference was found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Users</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Users</head><p>An alternate way to split the 250 pairs was on whether one of the two rankings contained no relevant documents in the top 10. Table <ref type="table" target="#tab_4">6</ref> and Table <ref type="table" target="#tab_5">7</ref> show the agreement figures based on this split. Contrasting the strength of user agreement between Table <ref type="table" target="#tab_4">6</ref> and Table <ref type="table" target="#tab_5">7</ref>, for all columns MTurkers agreed more strongly when one pair of runs had a score=0. This was confirmed with a statistically significant difference being found between the figures in the first columns (nDCG) of the two tables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Users</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Analysis of pairs with score&gt;0 in both results</head><p>An examination of mean Δ in nDCG between the pairs in Table <ref type="table" target="#tab_4">6</ref> and Table <ref type="table" target="#tab_5">7</ref> showed little difference, respectively 0.158 and 0.166. The best explanation for the difference was that it was due to the presence of a zero nDCG in one of the pairs. The results suggest a need for evaluation measures, e.g. GMAP <ref type="bibr" target="#b20">[20]</ref> which penalize systems that fail to return any relevant documents for particular topics.</p><p>As would be expected for Table <ref type="table" target="#tab_4">6</ref>, the agreements were identical for all the measures examined. However, Table <ref type="table" target="#tab_5">7</ref> showed key differences between the measures, particularly for P <ref type="bibr" target="#b10">(10)</ref>. Here the level of user agreement was almost random, this despite the measure scoring higher those ranking with more relevant documents. The highest levels of agreement were with MRR, but when the percentages were computed as a fraction of all pairs whose score &gt; 0 including ties (resulting in a total of 128 pairs) nDCG and ERR appeared better, see Table <ref type="table" target="#tab_6">8</ref>. What appeared to be in little doubt was that P <ref type="bibr" target="#b10">(10)</ref> was not well suited for assessing this sort of retrieval task. The final analysis of this data was to examine different types of topic. Within the TREC Web collection, a small number of the subtopics were navigational, most were informational <ref type="bibr" target="#b9">[9]</ref>. User agreement was measured split across these two topic types (see Table <ref type="table" target="#tab_7">9</ref>). For the small number of navigational topics, there was a strong agreement between users and the predictions made by the evaluation measures. No significance was found between the columns in this table. Given the small number of navigational topics, it would be valuable to repeat this experiment with an even balance in the number of navigational and informational topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Users</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Users</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MTurker comments on differences</head><p>In addition to indicating their preferences, MTurkers could also provide comments about their choices. In total, 96% of the judgments had associated comments that often indicated the reason(s) behind, or affecting, a decision. These often highlighted factors beyond the results simply having more relevant documents on a topic (informational) or a link to a required webpage (navigational). There were 11.6 words per comment, on average, and using an inductive approach to data analysis <ref type="bibr" target="#b24">[24]</ref> comments in which the users made a specific preference (54% of those submitted, 1,307) were categorized. Fifteen classes were derived and in 88 cases, comments were assigned multiple categories, e.g. "the left one has more useful results higher in the search" was assigned the classes 'position' and 'number' indicating that the number of results and their position in the ranking would have likely influenced their preference judgment, see Table <ref type="table" target="#tab_8">10</ref>.</p><p>Although these are factors which researchers often highlight as affecting relevance <ref type="bibr">[8]</ref>, we see these mentioned unprompted by the MTurkers in this study, again highlighting the benefit of using MTurk to gather data for this kind of study, beyond implicit feedback strategies such as query logs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">REFLECTIONS ON METHOD</head><p>Here we discuss using MTurk as a source of user preferences; and using preference as a means of determining the impact of search on users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quality of the MTurk data</head><p>With an anonymous monetized crowd sourcing system, there is always a concern that data gathered will be overwhelmed with noise from spammers. However, evidence in our analysis such as time taken to complete tasks (median ~6 min.) of this set indicated that the majority of data was created in good faith. Indeed this gathering from hundreds of users of not only quantitative data, but also qualitative data gave this set a value that query/click logs do not have.</p><p>Nevertheless there are collections of data points in the set which we do not fully understand. Unexpected user responses to search results is not uncommon: Tagliacozzo <ref type="bibr" target="#b23">[23]</ref>, surveying ~900 MEDLINE users, described how nearly 18% declared dissatisfaction with search results despite earlier indicating that a large number of relevant documents were returned in the search they were asked to judge. Alonzo described how the development of MTurk experiments required multiple iterations to ensure that MTurkers were given clear instructions on what to do and to be certain that the responses from them are given in good faith <ref type="bibr" target="#b6">[6]</ref>.</p><p>The data set yielded a set of significant results on evaluation measures, but we view the method used here as a first step towards developing a more refined approach. Improvements will come not only from avoiding erroneous output from MTurkers, but also from building a more refined model of how users determine their preference for one searching system over another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Does preference imply satisfaction?</head><p>The ultimate aim of this and past research was to understand if differences found between IR systems based on test collections were real: with a better system, would users be able to search more effectively, achieve their goals quicker, ultimately be more satisfied? Results from past work indicated that measuring such broader outcomes were challenging as users were adept at adapting either by searching more intensively or by managing to exploit relevant but poorer output.</p><p>The past work showed significant differences measured in a test collection did not necessarily show practical differences in user behavior. Was this simply because there was no practical difference to measure or was there simply a challenge in the way we measure users? This and previous work showed that a preference based methodology can measure significant differences in users for relatively small differences in retrieval effectiveness. However, it is worth remembering that the side-by-side method is a simulation of search and the measurement of preference says nothing about whether users are more satisfied with their search or will achieve their information seeking tasks more effectively. One might wish to hypothesize that such a link exists, but testing that hypothesis is for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS AND FUTURE WORK</head><p>The research questions posed at the start of the paper were answered through the gathering and examination of a large dataset of user preferences for a wide range of different IR systems tested over many queries and query types.</p><p>Clear evidence was found that effectiveness measured on a test collection predicted user preferences for one IR system over another. The strength of user prediction by test collection measures appeared to vary across different search tasks such as navigational or informational queries.</p><p>For diverse queries, little difference between diversity measures was found though the intent aware version of precision produced many ties between pairs. A conventional analysis of correlation between the measures was conducted confirming that they are similar. When comparing nDCG, MRR, ERR, and P(10) it was found that P(10) poorly modeled user preferences. However, user preferences between pairs of systems where one had failed to retrieve any relevant documents were notably stronger than when both rankings had at least one relevant document, which suggests a need for adjustments to these measures to allow for this user view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category Example #</head><p>On Finally, an examination and grouping of the written reasons that users provided for choosing one system ranking over another was outlined. Here it was shown that although relevance, rank, and information content of the documents was an important factor when users chose one system over another, a wide range of other reasons was also provided by users. Specific web sites were sought by some; avoidance of commercial sites or documents in other languages was desired by others. Such information from the MTurkers suggested that the test collections, relevance judgments and evaluation measures could be improved to provide a more effective model of user preferences than is currently available.</p><p>Such ideas are left to future work, where we will also continue to analyze our gathered data set as well as consider how to refine our collection methods to gather larger more informative sets in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 -</head><label>1</label><figDesc>Figure 1 -Screen shown to MTurkers: containing query, subtopic, instructions, paired rankings, input buttons, and text box</figDesc><graphic coords="3,56.47,72.96,499.45,115.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 -</head><label>2</label><figDesc>Figure 2 -Partial screen shot of a trap question shown to MTurkers</figDesc><graphic coords="4,56.57,598.62,499.05,102.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 -MTurkers' agreements to the diversity measures</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">α-nDCG</cell><cell>CR</cell><cell></cell><cell cols="2">NRBP</cell><cell cols="2">IA-PC</cell></row><row><cell>Agree</cell><cell cols="6">50 64% 54 69% 51 65%</cell><cell cols="2">28 60%</cell></row><row><cell>Rnk eq</cell><cell>4</cell><cell>5%</cell><cell>4</cell><cell>5%</cell><cell>4</cell><cell>5%</cell><cell>1</cell><cell>2%</cell></row><row><cell cols="7">Disgree 24 31% 20 26% 24 30%</cell><cell cols="2">18 38%</cell></row><row><cell></cell><cell>78</cell><cell></cell><cell>78</cell><cell></cell><cell>79</cell><cell></cell><cell>47</cell><cell></cell></row></table><note><p>1 A 2 tailed, 2 sample unequal variance test was used throughout.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 -MTurkers's agreement with traditional measures</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">nDCG</cell><cell cols="2">MRR</cell><cell cols="2">P(10)</cell><cell cols="2">ERR</cell></row><row><cell>Agree</cell><cell cols="8">160 65% 159 67% 131 62% 164 66%</cell></row><row><cell>Rnk eql</cell><cell>21</cell><cell>9%</cell><cell>21</cell><cell>9%</cell><cell>18</cell><cell>9%</cell><cell>21</cell><cell>9%</cell></row><row><cell>Disgree</cell><cell cols="2">66 27%</cell><cell cols="2">57 24%</cell><cell cols="2">61 29%</cell><cell cols="2">62 25%</cell></row><row><cell></cell><cell>247</cell><cell></cell><cell>237</cell><cell></cell><cell>210</cell><cell></cell><cell>247</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 -Analysis of pairs with score=0 in one result</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">nDCG</cell><cell cols="2">MRR</cell><cell cols="2">P(10)</cell><cell cols="2">ERR</cell></row><row><cell>Agree</cell><cell cols="2">88 72%</cell><cell cols="2">88 72%</cell><cell cols="2">88 72%</cell><cell cols="2">88 72%</cell></row><row><cell>Rnk eql</cell><cell>10</cell><cell>8%</cell><cell>10</cell><cell>8%</cell><cell>10</cell><cell>8%</cell><cell>10</cell><cell>8%</cell></row><row><cell>Disagree</cell><cell cols="2">24 20%</cell><cell cols="2">24 20%</cell><cell cols="2">24 20%</cell><cell cols="2">24 20%</cell></row><row><cell></cell><cell>122</cell><cell></cell><cell>122</cell><cell></cell><cell>122</cell><cell></cell><cell>122</cell><cell></cell></row><row><cell>Users</cell><cell cols="2">nDCG</cell><cell cols="2">MRR</cell><cell cols="2">P(10)</cell><cell>ERR</cell><cell></cell></row><row><cell>Agree</cell><cell cols="2">72 58%</cell><cell cols="4">71 62% 43 49%</cell><cell cols="2">76 61%</cell></row><row><cell>Rnk eql</cell><cell>11</cell><cell>9%</cell><cell cols="2">11 10%</cell><cell>8</cell><cell>9%</cell><cell>11</cell><cell>9%</cell></row><row><cell>Disagree</cell><cell cols="2">42 34%</cell><cell cols="4">33 29% 37 42%</cell><cell cols="2">38 30%</cell></row><row><cell></cell><cell>125</cell><cell></cell><cell>115</cell><cell></cell><cell>88</cell><cell></cell><cell>125</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 -figures from Table 7 with % based on all 128 pairs</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">nDCG</cell><cell cols="2">MRR</cell><cell cols="2">P(10)</cell><cell cols="2">ERR</cell></row><row><cell>Agree</cell><cell cols="2">72 56%</cell><cell cols="2">71 55%</cell><cell cols="2">43 34%</cell><cell cols="2">76 59%</cell></row><row><cell>Rnk eql</cell><cell>11</cell><cell>9%</cell><cell>11</cell><cell>9%</cell><cell>8</cell><cell>6%</cell><cell>11</cell><cell>9%</cell></row><row><cell>Disagree</cell><cell cols="2">42 33%</cell><cell cols="2">33 26%</cell><cell cols="2">37 29%</cell><cell cols="2">38 30%</cell></row><row><cell>Ties</cell><cell>3</cell><cell>2%</cell><cell cols="2">13 10%</cell><cell cols="2">40 31%</cell><cell>3</cell><cell>2%</cell></row><row><cell></cell><cell>128</cell><cell></cell><cell>128</cell><cell></cell><cell>128</cell><cell></cell><cell>128</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 -Analysis on different aspect types</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">nDCG</cell><cell cols="2">Informational</cell><cell cols="2">Navigational</cell></row><row><cell>Agree</cell><cell cols="2">160 65%</cell><cell>146</cell><cell>64%</cell><cell>14</cell><cell>78%</cell></row><row><cell>Rank equal</cell><cell>21</cell><cell>9%</cell><cell>21</cell><cell>9%</cell><cell>0</cell><cell>0%</cell></row><row><cell>Disagree</cell><cell cols="2">66 27%</cell><cell>62</cell><cell>27%</cell><cell>4</cell><cell>22%</cell></row><row><cell></cell><cell>247</cell><cell></cell><cell>229</cell><cell></cell><cell>18</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 -Analysis of 1,307 MTurk comments</head><label>10</label><figDesc>topic "All the results about secret garden" / "Contains work information in Michigan." Wikipedia link for Hoboken will have most of the information I would be looking for" / "Right side has a link to the ESPN home page as asked for." / "Right links to the desired information, the left does not." Each result would be helpful, but the left was easier." / "I thought the left side was better." 181 Irrelevant "More non-relevant results in right column." / "#5 on the left side is not for flame design at all" Both lists include a link to reviews, but it's higher on the list on the left than on the right." / "Top results more closely align to the request." Left seems to be legit. Right is more of junk and ads" / "Most of the right results are advertisements" / "less spammy"/ "Less commercial, more focused on the real subject results." / "The right column just lists pet adoption classified ads"</figDesc><table><row><cell>332</cell></row></table><note><p><p>6</p>Dead links "'The right had dead links" / "The left had a few links which did not work but the majority did and returned results on appraisals." 5</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>We are grateful to William Webber and the Melbourne IR reading group for spotting an error in the earlier version of this paper. This work was co-funded by the EU FP7 project ACCURAT contract FP7-ICT-248347 and Marie Curie Fellowship FP7-PEOPLE-2009-IIF-254562.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Diversifying search results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ieong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM WSDM</publisher>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The relationship between IR effectiveness measures and user satisfaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Maskari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="773" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Maskari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Airio</surname></persName>
		</author>
		<title level="m">The good and the bad system: does the test collection predict users&apos; effectiveness? ACM SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">When will information retrieval be &quot;good enough</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crowdsourcing for relevance evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can we get rid of TREC assessors? Using Mechanical Turk for relevance assessment</title>
		<author>
			<persName><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation</title>
		<meeting>the SIGIR 2009 Workshop on the Future of IR Evaluation</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="15" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Creating a test collection to evaluate diversity in image retrieval</title>
		<author>
			<persName><forename type="first">T</forename><surname>Arni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">User-defined relevance criteria: an exploratory study</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="149" to="159" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A taxonomy of web search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dynamic Bayesian network click model for web search ranking</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18 th WWW Conf</title>
		<meeting>18 th WWW Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM CIKM</publisher>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Novelty &amp; diversity in information retrieval evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGIR</title>
		<imprint>
			<biblScope unit="page" from="659" to="666" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An Effectiveness Measure for Ambiguous and Underspecified Queries</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval Theory</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="188" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<idno>TREC 2009 Web Track. TREC 2009 Notebook</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factors associated with success in searching MEDLINE and applying evidence to answer clinical questions</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am Med Inform Assoc</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sacherek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olson</surname></persName>
		</author>
		<title level="m">Do batch and user evaluations give the same results? ACM SIGIR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Students&apos; search process and outcome in Medline in writing an essay for a class on evidence-based medicine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huuskonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vakkari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="303" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluating retrieval performance using click through data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Workshop on Mathematical/Formal Methods in IR</title>
		<imprint>
			<biblScope unit="page" from="12" to="15" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How does click through data reflect retrieval quality?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM CIKM</publisher>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On GMAP: and other transformations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ACM CIKM</publisher>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">User adaptation: good results from poor systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluation measures for interactive information retrieval</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IP&amp;M</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="503" to="516" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating the satisfaction of information users</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tagliacozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="249" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A General Inductive Approach for Analyzing Qualitative Evaluation Data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Evaluation</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="246" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation by comparing result sets in context</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM CIKM</title>
		<imprint>
			<biblScope unit="page" from="94" to="101" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Why batch and user evaluations do not give the same results</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="225" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">User performance versus precision measures for simple search tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new rank correlation coefficient for information retrieval</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="587" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond independent relevance: methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGIR</title>
		<imprint>
			<biblScope unit="page" from="10" to="17" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
