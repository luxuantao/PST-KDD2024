<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning towards Minimum Hyperspherical Energy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
							<email>&lt;wyliu@gatech.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rongmei</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Emory University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lixin</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning towards Minimum Hyperspherical Energy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B5E459B374C82B927649F6D0FEF59D9D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent success of deep neural networks has led to its wide applications in a variety of tasks. With the over-parametrization nature and deep layered architecture, current deep networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b41">42]</ref> are able to achieve impressive performance on large-scale problems. Despite such success, having redundant and highly correlated neurons (e.g., weights of kernels/filters in convolutional neural networks (CNNs)) caused by over-parametrization presents an issue <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41]</ref>, which motivated a series of influential works in network compression <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref> and parameter-efficient network architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b61">62]</ref>. These works either compress the network by pruning redundant neurons or directly modify the network architecture, aiming to achieve comparable performance while using fewer parameters. Yet, it remains an open problem to find a unified and principled theory that guides the network compression in the context of optimal generalization ability.</p><p>Another stream of works seeks to further release the network generalization power by alleviating redundancy through diversification <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref> as rigorously analyzed by <ref type="bibr" target="#b58">[59]</ref>. Most of these works address the redundancy problem by enforcing relatively large diversity between pairwise projection bases via regularization. Our work broadly falls into this category by sharing similar high-level target, but the spirit and motivation behind our proposed models are distinct. In particular, there is a recent trend of studies that feature the significance of angular learning at both loss and convolution levels <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref>, based on the observation that the angles in deep embeddings learned by CNNs tend to encode semantic difference. The key intuition is that angles preserve the most abundant and discriminative information for visual recognition. As a result, hyperspherical geodesic distances between neurons naturally play a key role in this context, and thus, it is intuitively desired to impose discrimination by keeping their projections on the hypersphere as far away from each other as possible. While the concept of imposing large angular diversities was also considered in <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b35">36]</ref>, they do not consider diversity in terms of global equidistribution of embeddings on the hypersphere, which fails to achieve the state-of-the-art performances.</p><p>Given the above motivation, we draw inspiration from a well-known physics problem, called Thomson problem <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b42">43]</ref>. The goal of Thomson problem is to determine the minimum electrostatic potential energy configuration of N mutually-repelling electrons on the surface of a unit sphere. We identify the intrinsic resemblance between the Thomson problem and our target, in the sense that diversifying neurons can be seen as searching for an optimal configuration of electron locations. Similarly, we characterize the diversity for a group of neurons by defining a generic hyperspherical potential energy using their pairwise relationship. Higher energy implies higher redundancy, while lower energy indicates that these neurons are more diverse and more uniformly spaced. To reduce the redundancy of neurons and improve the neural networks, we propose a novel minimum hyperspherical energy (MHE) regularization framework, where the diversity of neurons is promoted by minimizing the hyperspherical energy in each layer. As verified by comprehensive experiments on multiple tasks, MHE is able to consistently improve the generalization power of neural networks. The red dots denote the neurons optimized by the gradient of the corresponding regularization. The rightmost pink dots denote the virtual negative neurons. We randomly initialize the weights of 10 neurons on a 3D Sphere and optimize them with SGD.</p><p>MHE faces different situations when it is applied to hidden layers and output layers. For hidden layers, applying MHE straightforwardly may still encourage some degree of redundancy since it will produce co-linear bases pointing to opposite directions (see Fig. <ref type="figure" target="#fig_0">1 middle</ref>). In order to avoid such redundancy, we propose the half-space MHE which constructs a group of virtual neurons and minimize the hyperspherical energy of both existing and virtual neurons. For output layers, MHE aims to distribute the classifier neurons<ref type="foot" target="#foot_0">1</ref> as uniformly as possible to improve the inter-class feature separability. Different from MHE in hidden layers, classifier neurons should be distributed in the full space for the best classification performance <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref>. An intuitive comparison among the widely used orthonormal regularization, the proposed MHE and half-space MHE is provided in Fig. <ref type="figure" target="#fig_0">1</ref>. One can observe that both MHE and half-space MHE are able to uniformly distribute the neurons over the hypersphere and half-space hypershpere, respectively. In contrast, conventional orthonormal regularization tends to group neurons closer, especially when the number of neurons is greater than the dimension. MHE is originally defined on Euclidean distance, as indicated in Thomson problem. However, we further consider minimizing hyperspherical energy defined with respect to angular distance, which we will refer to as angular-MHE (A-MHE) in the following paper. In addition, we give some theoretical insights of MHE regularization, by discussing the asymptotic behavior and generalization error. Last, we apply MHE regularization to multiple vision tasks, including generic object recognition, class-imbalance learning, and face recognition. In the experiments, we show that MHE is architectureagnostic and can considerably improve the generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Diversity regularization is shown useful in sparse coding <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>, ensemble learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24]</ref>, selfpaced learning <ref type="bibr" target="#b20">[21]</ref>, metric learning <ref type="bibr" target="#b57">[58]</ref>, etc. Early studies in sparse coding <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref> show that the generalization ability of codebook can be improved via diversity regularization, where the diversity is often modeled using the (empirical) covariance matrix. More recently, a series of studies have featured diversity regularization in neural networks <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">55]</ref>, where regularization is mostly achieved via promoting large angle/orthogonality, or reducing covariance between bases. Our work differs from these studies by formulating the diversity of neurons on the entire hypersphere, therefore promoting diversity from a more global, top-down perspective. Methods other than diversity-promoting regularization have been widely proposed to improve CNNs <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref> and generative adversarial nets (GANs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref>. MHE can be regarded as a complement that can be applied on top of these methods.</p><p>3 Learning Neurons towards Minimum Hyperspherical Energy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation of Minimum Hyperspherical Energy</head><p>Minimum hyperspherical energy defines an equilibrium state of the configuration of neuron's directions. We argue that the power of neural representation of each layer can be characterized by the hyperspherical energy of its neurons, and therefore a minimal energy configuration of neurons can induce better generalization. Before delving into details, we first define the hyperspherical energy functional for N neurons (i.e., kernels) with</p><formula xml:id="formula_0">(d + 1)-dimension W N = {w 1 , • • • , w N ∈ R d+1 } as E s,d ( ŵi| N i=1 ) = N i=1 N j=1,j =i fs ŵi -ŵj = i =j ŵi -ŵj -s , s &gt; 0 i =j log ŵi -ŵj -1 , s = 0 ,<label>(1)</label></formula><p>where • denotes Euclidean distance, f s (•) is a decreasing real-valued function, and ŵi = wi wi is the i-th neuron weight projected onto the unit hypersphere</p><formula xml:id="formula_1">S d = {w ∈ R d+1 | w = 1}. We also denote ŴN = { ŵ1 , • • • , ŵN ∈ S d }, and E s = E s,d ( ŵi | N i=1</formula><p>) for short. There are plenty of choices for f s (•), but in this paper we use f s (z) = z -s , s &gt; 0, known as Riesz s-kernels. Particularly, as s → 0, z -s → s log(z -1 ) + 1, which is an affine transformation of log(z -1 ). It follows that optimizing the logarithmic hyperspherical energy</p><formula xml:id="formula_2">E 0 = i =j log( ŵi -ŵj -1</formula><p>) is essentially the limiting case of optimizing the hyperspherical energy E s . We therefore define f 0 (z) = log(z -1 ) for convenience.</p><p>The goal of the MHE criterion is to minimize the energy in Eq. ( <ref type="formula" target="#formula_0">1</ref>) by varying the orientations of the neuron weights w 1 , • • • , w N . To be precise, we solve an optimization problem: min W N E s with s ≥ 0. In particular, when s = 0, we solve the logarithmic energy minimization problem:</p><formula xml:id="formula_3">arg min W N E0 = arg min W N exp(E0) = arg max W N i =j ŵi -ŵj ,<label>(2)</label></formula><p>in which we essentially maximize the product of Euclidean distances. E 0 , E 1 and E 2 have interesting yet profound connections. Note that Thomson problem corresponds to minimizing E 1 , which is a NP-hard problem. Therefore in practice we can only compute its approximate solution by heuristics.</p><p>In neural networks, such a differentiable objective can be directly optimized via gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Logarithmic Hyperspherical Energy E 0 as a Relaxation</head><p>Optimizing the original energy in Eq. ( <ref type="formula" target="#formula_0">1</ref>) is equivalent to optimizing its logarithmic form log E s .</p><p>To efficiently solve this difficult optimization problem, we can instead optimize the lower bound of log E s as a surrogate energy, by applying Jensen's inequality:</p><formula xml:id="formula_4">arg min W N E log := N i=1 N j=1,j =i log fs ŵi -ŵj<label>(3)</label></formula><p>With f s (z) = z -s , s &gt; 0, we observe that E log becomes sE 0 = s i =j log( ŵiŵj -1 ), which is identical to the logarithmic hyperspherical energy E 0 up to a multiplicative factor s. Therefore, minimizing E 0 can also be viewed as a relaxation of minimizing E s for s &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MHE as Regularization for Neural Networks</head><p>Now that we have introduced the formulation of MHE, we propose MHE regularization for neural networks. In supervised neural network learning, the entire objective function is shown as follows:</p><formula xml:id="formula_5">L = 1 m m j=1 ( w out i , xj c i=1 , yj)</formula><p>training data fitting</p><formula xml:id="formula_6">+ λh • L-1 j=1 1 Nj(Nj -1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) {Es}j</head><p>T h : hyperspherical energy for hidden layers</p><formula xml:id="formula_7">+ λo • 1 NL(NL -1) Es( ŵout i | c i=1 )</formula><p>To: hyperspherical energy for output layer <ref type="bibr" target="#b3">(4)</ref> where x i is the feature of the i-th training sample entering the output layer, w out i is the classifier neuron for the i-th class in the output fully-connected layer and ŵout i denotes its normalized version. {E s } i denotes the hyperspherical energy for the neurons in the i-th layer. c is the number of classes, m is the batch size, L is the number of layers of the neural network, and N i is the number of neurons in the i-th layer.</p><formula xml:id="formula_8">E s ( ŵout i | c i=1 ) denotes the hyperspherical energy of neurons { ŵout 1 , • • • , ŵout c }.</formula><p>The 2 weight decay is omitted here for simplicity, but we will use it in practice. An alternative interpretation of MHE regularization from a decoupled view is given in Section 3.7 and Appendix C. MHE has different effects and interpretations in regularizing hidden layers and output layers.</p><p>MHE for hidden layers. To make neurons in the hidden layers more discriminative and less redundant, we propose to use MHE as a form of regularization. MHE encourages the normalized neurons to be uniformly distributed on a unit hypersphere, which is partially inspired by the observation in <ref type="bibr" target="#b29">[30]</ref> that angular difference in neurons preserves semantic (label-related) information. To some extent, MHE maximizes the average angular difference between neurons (specifically, the hyperspherical energy of neurons in every hidden layer). For instance, in CNNs we minimize the hyperpsherical energy of kernels in convolutional and fully-connected layers except the output layer.</p><p>MHE for output layers. For the output layer, we propose to enhance the inter-class feature separability with MHE to learn discriminative and well-separated features. For classification tasks, MHE regularization is complementary to the softmax cross-entropy loss in CNNs. The softmax loss focuses more on the intra-class compactness, while MHE encourages the inter-class separability. Therefore, MHE on output layers can induce features with better generalization power. Directly applying the MHE formulation may still encouter some redundancy. An example in Fig. <ref type="figure" target="#fig_1">2</ref>, with two neurons in a 2dimensional space, illustrates this potential issue. Directly imposing the original MHE regularization leads to a solution that two neurons are colinear but with opposite directions. To avoid such redundancy, we propose the half-space MHE regularization which constructs some virtual neurons and minimizes the hyperspherical energy of both original and virtual neurons together. Specifically, half-space MHE constructs a colinear virtual neuron with opposite direction for every existing neuron. Therefore, we end up with minimizing the hyperspherical energy with 2N i neurons in the i-th layer (i.e., minimizing E s ({ ŵk , -ŵk }| 2Ni k=1 )). This half-space variant will encourage the neurons to be less correlated and less redundant, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. Note that, half-space MHE can only be used in hidden layers, because the colinear neurons do not constitute redundancy in output layers, as shown in <ref type="bibr" target="#b28">[29]</ref>. Nevertheless, colinearity is usually not likely to happen in high-dimensional spaces, especially when the neurons are optimized to fit training data. This may be the reason that the original MHE regularization still consistently improves the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MHE in Half Space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">MHE beyond Euclidean Distance</head><p>The hyperspherical energy is originally defined based on the Euclidean distance on a hypersphere, which can be viewed as an angular measure. In addition to Euclidean distance, we further consider the geodesic distance on a unit hypersphere as a distance measure for neurons, which is exactly the same as the angle between neurons. Specifically, we consider to use arccos( ŵ i ŵj ) to replace ŵiŵj in hyperspherical energies. Following this idea, we propose angular MHE (A-MHE) as a simple extension, where the hyperspherical energy is rewritten as:</p><formula xml:id="formula_9">E a s,d ( ŵi| N i=1 ) = N i=1 N j=1,j =i fs arccos( ŵ i ŵj) = i =j arccos( ŵ i ŵj) -s , s &gt; 0 i =j log arccos( ŵ i ŵj) -1 , s = 0<label>(5)</label></formula><p>which can be viewed as redefining MHE based on geodesic distance on hyperspheres (i.e., angle), and can be used as an alternative to the original hyperspherical energy E s in Eq. ( <ref type="formula">4</ref>). Note that, A-MHE can also be learned in full-space or half-space, leading to similar variants as original MHE. The key difference between MHE and A-MHE lies in the optimization dynamics, because their gradients w.r.t the neuron weights are quite different. A-MHE is also more computationally expensive than MHE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Mini-batch Approximation for MHE</head><p>With a large number of neurons in one layer, calculating MHE can be computationally expensive as it requires computing the pair-wise distances between neurons. To address this issue, we propose the mini-batch version of MHE to approximate the MHE (either original or half-space) objective. Mini-batch approximation for MHE on hidden layers. For hidden layers, mini-batch approximation iteratively takes a random batch of neurons as input and minimizes their hyperspherical energy as an approximation to the MHE. Note that the gradient of the mini-batch objective is an unbiased estimation of the original gradient of MHE. Data-dependent mini-batch approximation for output layers. For the output layer, the datadependent mini-batch approximation iteratively takes the classifier neurons corresponding to the classes that exist in mini-batches. It minimizes</p><formula xml:id="formula_10">1 m(N -1) m i=1</formula><p>N j=1,j =yi f s ( ŵyiŵj ) in each iteration, where y i denotes the class label of the i-th sample in each mini-batch, m is the mini-batch size, and N is the number of neurons (in one particular layer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Discussions</head><p>Connections to scientific problems. The hyperspherical energy minimization has close relationships with scientific problems. When s = 1, Eq. ( <ref type="formula" target="#formula_0">1</ref>) reduces to Thomson problem <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b42">43]</ref> (in physics) where one needs to determine the minimum electrostatic potential energy configuration of N mutuallyrepelling electrons on a unit sphere. When s = ∞, Eq. ( <ref type="formula" target="#formula_0">1</ref>) becomes Tammes problem <ref type="bibr" target="#b46">[47]</ref> (in geometry) where the goal is to pack a given number of circles on the surface of a sphere such that the minimum distance between circles is maximized. When s = 0, Eq. ( <ref type="formula" target="#formula_0">1</ref>) becomes Whyte's problem where the goal is to maximize product of Euclidean distances as shown in Eq. ( <ref type="formula" target="#formula_3">2</ref>). Our work aims to make use of important insights from these scientific problems to improve neural networks.</p><p>Understanding MHE from decoupled view. Inspired by decoupled networks <ref type="bibr" target="#b26">[27]</ref>, we can view the original convolution as the multiplication of the angular function g(θ) = cos(θ) and the magnitude function</p><formula xml:id="formula_11">h( w , x ) = w • x : f (w, x) = h( w , x ) • g(θ)</formula><p>where θ is the angle between the kernel w and the input x. From the equation above, we can see that the norm of the kernel and the direction (i.e., angle) of the kernel affect the inner product similarity differently. Typically, weight decay is to regularize the kernel by minimizing its 2 norm, while there is no regularization on the direction of the kernel. Therefore, MHE completes this missing piece by promoting angular diversity. By combining MHE to a standard neural networks, the entire regularization term becomes</p><formula xml:id="formula_12">Lreg = λw • 1 L j=1 Nj L j=1 N j i=1 wi</formula><p>Weight decay: regularizing the magnitude of kernels</p><formula xml:id="formula_13">+ λh • L-1 j=1 1 Nj(Nj -1) {Es}j + λo • 1 NL(NL -1) Es( ŵout i | c i=1 )</formula><p>MHE: regularizing the direction of kernels where λ w , λ h and λ o are weighting hyperparameters for these three regularization terms. From the decoupled view, MHE makes a lot of senses in regularizing the neural networks, since it serves as a complementary and orthogonal role to weight decay. More discussions are in Appendix C.</p><p>Comparison to orthogonality/angle-promoting regularizations. Promoting orthogonality or large angles between bases has been a popular choice for encouraging diversity. Probably the most related and widely used one is the orthonormal regularization <ref type="bibr" target="#b29">[30]</ref> which aims to minimize W W -I F , where W denotes the weights of a group of neurons with each column being one neuron and I is an identity matrix. One similar regularization is the orthogonality regularization <ref type="bibr" target="#b35">[36]</ref> which minimizes the sum of the cosine values between all the kernel weights. These methods encourage kernels to be orthogonal to each other, while MHE does not. Instead, MHE encourages the hyperspherical diversity among these kernels, and these kernels are not necessarily orthogonal to each other. <ref type="bibr" target="#b55">[56]</ref> proposes the angular constraint which aims to constrain the angles between different kernels of the neural network, but quite different from MHE, they use a hard constraint to impose this angular regularization. Moreover, these methods model diversity regularization at a more local level, while MHE regularization seeks to model the problem in a more top-down manner.</p><p>Normalized neurons in MHE. From Eq. 1, one can see that the normalized neurons are used to compute MHE, because we aim to encourage the diversity on a hypersphere. However, a natural question may arise: what if we use the original (i.e., unnormalized) neurons to compute MHE? First, combining the norm of kernels (i.e., neurons) into MHE may lead to a trivial gradient descent direction: simply increasing the norm of all kernels. Suppose all kernel directions stay unchanged, increasing the norm of all kernels by a factor can effectively decrease the objective value of MHE. Second, coupling the norm of kernels into MHE may contradict with weight decay which aims to decrease the norm of kernels. Moreover, normalized neurons imply that the importance of all neurons is the same, which matches the intuition in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref>. If we desire different importance for different neurons, we can also manually assign a fixed weight for each neuron. This may be useful when we have already known certain neurons are more important and we want them to be relatively fixed. The neuron with large weight tends to be updated less. We will discuss it more in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Insights</head><p>This section leverages a number of rigorous theoretical results from <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b53">54]</ref> and provides theoretical yet intuitive understandings about MHE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Asymptotic Behavior</head><p>This subsection shows how the hyperspherical energy behaves asymptotically. Specifically, as N → ∞, we can show that the solution ŴN tends to be uniformly distributed on hypersphere S d when the hyperspherical energy defined in Eq. ( <ref type="formula" target="#formula_0">1</ref>) achieves its minimum.</p><p>Definition 1 (minimal hyperspherical s-energy). We define the minimal s-energy for N points on the unit hypersphere</p><formula xml:id="formula_14">S d = {w ∈ R d+1 | w = 1} as ε s,d (N ) := inf ŴN ⊂S d E s,d ( ŵi| N i=1 ) (6)</formula><p>where the infimum is taken over all possible ŴN on S d . Any configuration of ŴN to attain the infimum is called an s-extremal configuration. Usually ε s,d (N ) = ∞ if N is greater than d and ε s,d (N ) = 0 if N = 0, 1.</p><p>We discuss the asymptotic behavior (N → ∞) in three cases: 0 &lt; s &lt; d, s = d, and s &gt; d. We first write the energy integral as I s (µ) = S d ×S d uv -s dµ(u)dµ(v), which is taken over all probability measure µ supported on S d . With 0 &lt; s &lt; d, I s (µ) is minimal when µ is the spherical measure</p><formula xml:id="formula_15">σ d = H d (•)| S d /H d (S d ) on S d</formula><p>, where H d (•) denotes the d-dimensional Hausdorff measure. When s ≥ d, I s (µ) becomes infinity, which therefore requires different analysis. In general, we can say all s-extremal configurations asymptotically converge to uniform distribution on a hypersphere, as stated in Theorem 1. This asymptotic behavior has been heavily studied in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Theorem 1 (asymptotic uniform distribution on hypersphere). Any sequence of optimal s-energy configurations ( Ŵ N )| ∞ 2 ⊂ S d is asymptotically uniformly distributed on S d in the sense of the weakstar topology of measures, namely</p><formula xml:id="formula_16">1 N v∈ Ŵ N δv → σ d , as N → ∞<label>(7)</label></formula><p>where δ v denotes the unit point mass at v, and σ d is the spherical measure on S d .</p><p>Theorem 2 (asymptotics of the minimal hyperspherical s-energy). We have that lim N →∞</p><formula xml:id="formula_17">ε s,d (N ) p(N )</formula><p>exists for the minimal s-energy.</p><formula xml:id="formula_18">For 0 &lt; s &lt; d, p(N ) = N 2 . For s = d, p(N ) = N 2 log N . For s &gt; d, p(N ) = N 1+s/d . Particularly if 0 &lt; s &lt; d, we have lim N →∞ ε s,d (N ) N 2 = I s (σ d ).</formula><p>Theorem 2 tells us the growth power of the minimal hyperspherical s-energy when N goes to infinity. Therefore, different potential power s leads to different optimization dynamics. In the light of the behavior of the energy integral, MHE regularization will focus more on local influence from neighborhood neurons instead of global influences from all the neurons as the power s increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generalization and Optimality</head><p>As proved in <ref type="bibr" target="#b53">[54]</ref>, in one-hidden-layer neural network, the diversity of neurons can effectively eliminate the spurious local minima despite the non-convexity in learning dynamics of neural networks. Following such an argument, our MHE regularization, which encourages the diversity of neurons, naturally matches the theoretical intuition in <ref type="bibr" target="#b53">[54]</ref>, and effectively promotes the generalization of neural networks. While hyperspherical energy is minimized such that neurons become diverse on hyperspheres, the hyperspherical diversity is closely related to the generalization error.</p><p>More specifically, in a one-hidden-layer neural network f</p><formula xml:id="formula_19">(x) = n k=1 v k σ(W k x) with least squares loss L(f ) = 1 2m m i=1 (y i -f (x i )) 2 , we can compute its gradient w.r.t W k as ∂L ∂W k = 1 m m i=1 (f (x i ) -y i )v k σ (W k x i )x i . (σ(•)</formula><p>is the nonlinear activation function and σ (•) is its subgradient. x ∈ is the training sample. W k denotes the weights of hidden layer and v k is the weights of output layer.) Subsequently, we can rewrite this gradient as a matrix form:</p><formula xml:id="formula_20">∂L ∂W = D • r where D ∈ R dn×m , D {di-d+1:di,j} = v i σ (W i x j )x j ∈ R d and r ∈ R m , r i = 1 m f (x i ) -y i .</formula><p>Further, we can obtain the inequality r ≤ 1 λmin(D) ∂L ∂W . r is actually the training error. To make the training error small, we need to lower bound λ min (D) away from zero. From <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b2">3]</ref>, one can know that the lower bound of λ min (D) is directly related to the hyperspherical diversity of neurons. After bounding the training error, it is easy to bound the generalization error using Rademachar complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Applications and Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Improving Network Generalization</head><p>First, we perform ablation study and some exploratory experiments on MHE. Then we apply MHE to large-scale object recognition and class-imbalance learning. For all the experiments on CIFAR-10 and CIFAR-100 in the paper, we use moderate data augmentation, following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>. For ImageNet-2012, we follow the same data augmentation in <ref type="bibr" target="#b29">[30]</ref>. We train all the networks using SGD with momentum 0.9, and the network initialization follows <ref type="bibr" target="#b12">[13]</ref>. All the networks use BN <ref type="bibr" target="#b19">[20]</ref> and ReLU if not otherwise specified. Experimental details are given in each subsection and Appendix A.   Ablation study. Since the current MHE regularizes the neurons in the hidden layers and the output layer simultaneously, we perform ablation study for MHE to further investigate where the gain comes from. This experiment uses the CNN-9. The results are given in Table <ref type="table" target="#tab_3">4</ref>. "H" means that we apply MHE to all the hidden layers, while "O" means that we apply MHE to the output layer. Because the half-space MHE can not be applied to the output layer, so there is "N/A" in the table. In general, we find that applying MHE to both the hidden layers and the output layer yields the best performance, and using MHE in the hidden layers usually produces better accuracy than using MHE in the output layer. Value of Hyperparameter Testing Error on CIFAR-100 (%) Hyperparameter experiment. We evaluate how the selection of hyperparameter affects the performance. We experiment with different hyperparameters from 10 -2 to 10 2 on CIFAR-100 with the CNN-9. HS-MHE denotes the half-space MHE. We evaluate MHE variants by separately applying MHE to the output layer ("O"), MHE to the hidden layers ("H"), and the half-space MHE to the hidden layers ("H"). The results in Fig. <ref type="figure" target="#fig_3">3</ref> show that our MHE is not very hyperparameter-sensitive and can consistently beat the baseline by a considerable margin. One can observe that MHE's hyperparameter works well from 10 -2 to 10 2 and therefore is easy to set. In contrast, the hyperparameter of weight decay could be more sensitive than MHE. Half-space MHE can consistently outperform the original MHE under all different hyperparameter settings. Interestingly, applying MHE only to hidden layers can achieve better accuracy than applying MHE only to output layers.  We evaluate MHE on large-scale ImageNet-2012 datasets. Specifically, we perform experiment using ResNets, and then report the top-1 validation error (center crop) in Table <ref type="table" target="#tab_5">6</ref>. From the results, we still observe that both MHE and half-space MHE yield consistently better recognition accuracy than the baseline and the orthonormal regularization (after tuning its hyperparameter). To better evaluate the consistency of MHE's performance gain, we use two ResNets with different depth: ResNet-18 and ResNet-34. On these two different networks, both MHE and half-space MHE outperform the baseline by a significant margin, showing consistently better generalization power. Moreover, half-space MHE performs slightly better than full-space MHE as expected.  The visualization for the full testing set is also given in Appendix H. From Fig. <ref type="figure" target="#fig_5">4</ref>, one can see that the CNN without MHE tends to ignore the imbalanced class (digit 0) and the learned classifier neuron is highly biased to another digit. In contrast, the CNN with MHE can learn reasonably separable distribution even if digit 0 only has 2% samples compared to the other classes. Using MHE in this toy setting can readily improve the accuracy on the full testing set from 88.5% to 98%. Most importantly, the classifier neuron for digit 0 is also properly learned, similar to the one learned on the balanced dataset. Note that, half-space MHE can not be applied to the classifier neurons, because the classifier neurons usually need to occupy the full feature space. We experiment MHE in two data imbalance settings on CIFAR-10: 1) single class imbalance (S) -All classes have the same number of images but one single class has significantly less number, and 2) multiple class imbalance (M) -The number of images decreases as the class index decreases from 9 to 0. We use CNN-9 for all the compared regularizations. Detailed setups are provided in Appendix A. In Table <ref type="table" target="#tab_6">7</ref>, we report the error rate on the whole testing set. In addition, we report the error rate (denoted by Err. (S)) on the imbalance class (single imbalance setting) in the full testing set. From the results, one can observe that CNN-9 with MHE is able to effectively perform recognition when classes are imbalanced. Even only given a small portion of training data in a few classes, CNN-9 with MHE can achieve very competitive accuracy on the full testing set, showing MHE's superior generalization power. Moreover, we also provide experimental results on imbalanced CIFAR-100 in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Ablation Study and Exploratory Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Class-imbalance Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SphereFace+: Improving Inter-class Feature Separability via MHE for Face Recognition</head><p>We have shown that full-space MHE for output layers can encourage classifier neurons to distribute more evenly on hypersphere and therefore improve inter-class feature separability. Intuitively, the classifier neurons serve as the approximate center for features from each class, and can therefore guide the feature learning. We also observe that open-set face recognition (e.g., face verification) requires the feature centers to be as separable as possible <ref type="bibr" target="#b27">[28]</ref>. This connection inspires us to apply MHE to face recognition. Specifically, we propose SphereFace+ by applying MHE to SphereFace <ref type="bibr" target="#b27">[28]</ref>. The objective of SphereFace, angular softmax loss ( SF ) that encourages intra-class feature compactness, is naturally complementary to that of MHE. The objective function of SphereFace+ is defined as</p><formula xml:id="formula_21">LSF+ = 1 m m j=1 SF( w out i , xj c i=1 , yj, mSF)</formula><p>Angular softmax loss: promoting intra-class compactness</p><formula xml:id="formula_22">+ λM • 1 m(N -1) m i=1 N j=1,j =y i fs( ŵout y i -ŵout j ) MHE: promoting inter-class separability<label>(8)</label></formula><p>where c is the number of classes, m is the mini-batch size, N is the number of classifier neurons, x i the deep feature of the i-th face (y i is its groundtruth label), w out i is the i-th classifier neuron. m SF is a hyperparameter for SphereFace, controlling the degree of intra-class feature compactness (i.e., the size of the angular margin). Because face datesets usually have thousands of identities, we will use the data-dependent mini-batch approximation MHE as shown in Eq. ( <ref type="formula" target="#formula_22">8</ref>) in the output layer to reduce computational cost. MHE completes a missing piece for SphereFace by promoting the interclass separability. SphereFace+ consistently outperforms SphereFace, and achieves state-of-the-art performance on both LFW <ref type="bibr" target="#b17">[18]</ref> and MegaFace <ref type="bibr" target="#b21">[22]</ref>   Performance under different m SF . We evaluate SphereFace+ with two different architectures (SphereFace-20 and SphereFace-64) proposed in <ref type="bibr" target="#b27">[28]</ref>. Specifically, SphereFace-20 and SphereFace-64 are 20-layer and 64-layer modified residual networks, respectively. We train our network with the publicly available CASIA-Webface dataset <ref type="bibr" target="#b59">[60]</ref>, and then test the learned model on LFW and MegaFace dataset. In MegaFace dataset, the reported accuracy indicates rank-1 identification accuracy with 1 million distractors. All the results in Table <ref type="table" target="#tab_8">8</ref>  Comparison to state-of-the-art methods. We also compare our methods with some widely used loss functions. All these compared methods use SphereFace-64 network that are trained with CASIA dataset. All the results are given in Table <ref type="table" target="#tab_10">10</ref> computed without model ensemble and PCA. Compared to the other state-of-the-art methods, SphereFace+ achieves the best accuracy on LFW dataset, while being comparable to the best accuracy on MegaFace dataset. Current state-of-the-art face recognition methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31]</ref> usually only focus on compressing the intra-class features, which makes MHE a potentially useful tool in order to further improve these face recognition methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks</head><p>We borrow some useful ideas and insights from physics and propose a novel regularization method for neural networks, called minimum hyperspherical energy (MHE), to encourage the angular diversity of neuron weights. MHE can be easily applied to every layer of a neural network as a plug-in regularization, without modifying the original network architecture. Different from existing methods, such diversity can be viewed as uniform distribution over a hypersphere. In this paper, MHE has been specifically used to improve network generalization for generic image classification, class-imbalance learning and large-scale face recognition, showing consistent improvements in all tasks. Moreover, MHE can significantly improve the image generation quality of GANs (see Appendix G). In summary, our paper casts a novel view on regularizing the neurons by introducing hyperspherical diversity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Orthonormal, MHE and half-space MHE regularization.The red dots denote the neurons optimized by the gradient of the corresponding regularization. The rightmost pink dots denote the virtual negative neurons. We randomly initialize the weights of 10 neurons on a 3D Sphere and optimize them with SGD.</figDesc><graphic coords="2,433.55,252.11,71.86,72.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Half-space MHE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hyperparameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Class-imbalance learning on MNIST.Because MHE aims to maximize the hyperspherical margin between different classifier neurons in the output layer, we can naturally apply MHE to class-imbalance learning where the number of training samples in different classes is imbalanced. We demonstrate the power of MHE in class-imbalance learning through a toy experiment. We first randomly throw away 98% training data for digit 0 in MNIST (only 100 samples are preserved for digit 0), and then train a 6-layer CNN on this imbalance MNIST. To visualize the learned features, we set the output feature dimension as 2. The features and classifier neurons on the full training set are visualized in Fig.4where each color denotes a digit and red arrows are the normalized classifier neurons. Although we train the network on the imbalanced training set, we visualize the features of the full training set for better demonstration. The visualization for the full testing set is also given in Appendix H. From Fig.4, one can see that the CNN without MHE tends to ignore the imbalanced class (digit 0) and the learned classifier neuron is highly biased to another digit. In contrast, the CNN with MHE can learn reasonably separable distribution even if digit 0 only has 2% samples compared to the other classes. Using MHE in this toy setting can readily improve the accuracy on the full testing set from 88.5% to 98%. Most importantly, the classifier neuron for digit 0 is also properly learned, similar to the one learned on the balanced dataset. Note that, half-space MHE can not be applied to the classifier neurons, because the classifier neurons usually need to occupy the full feature space.</figDesc><graphic coords="8,424.28,359.18,77.32,63.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Testing error (%) of different MHE on CIFAR-10/100. The results in Table1show that all the variants of MHE perform consistently better than the baseline. Specifically, the half-space MHE has more significant performance gain compared to the other MHE variants, and MHE with Euclidean and angular distance perform similarly. In general, MHE with s = 2 performs best among s = 0, 1, 2. In the following experiments, we use s = 2 and Euclidean distance for both MHE and half-space MHE by default if not otherwise specified.</figDesc><table><row><cell>Variants of MHE. We evaluate all dif-ferent variants of MHE on CIFAR-10</cell><cell>Method</cell><cell>s = 2</cell><cell>CIFAR-10 s = 1</cell><cell>s = 0</cell><cell>s = 2</cell><cell>CIFAR-100 s = 1</cell><cell>s = 0</cell></row><row><cell>and CIFAR-100, including original MHE (with the power s = 0, 1, 2) and half-space</cell><cell>MHE Half-space MHE A-MHE</cell><cell>6.22 6.28 6.21</cell><cell>6.74 6.54 6.77</cell><cell>6.44 6.30 6.45</cell><cell>27.15 25.61 26.17</cell><cell>27.09 26.30 27.31</cell><cell>26.16 26.18 27.90</cell></row><row><cell>MHE (with the power s = 0, 1, 2) with both Euclidean and angular distance. In</cell><cell>Half-space A-MHE Baseline</cell><cell>6.52</cell><cell>6.49 7.75</cell><cell>6.44</cell><cell>26.03</cell><cell>26.52 28.13</cell><cell>26.47</cell></row><row><cell>this experiment, all methods use CNN-9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(see Appendix A). Method</cell><cell cols="6">16/32/64 32/64/128 64/128/256 128/256/512 256/512/1024</cell></row><row><cell></cell><cell>Baseline</cell><cell>47.72</cell><cell>38.64</cell><cell>28.13</cell><cell cols="2">24.95</cell><cell>25.45</cell></row><row><cell></cell><cell>MHE</cell><cell>36.84</cell><cell>30.05</cell><cell>26.75</cell><cell cols="2">24.05</cell><cell>23.14</cell></row><row><cell></cell><cell>Half-space MHE</cell><cell>35.16</cell><cell>29.33</cell><cell>25.96</cell><cell cols="2">23.38</cell><cell>21.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Testing error (%) of different width on CIFAR-100. Results in Table2show that both MHE and half-space MHE consistently outperform the baseline, showing stronger generalization. Interestingly, both MHE and half-space MHE have more significant gain while the filter number is smaller in each layer, indicating that MHE can help the network to make better use of the neurons. In general, half-space MHE performs consistently better than MHE, showing the necessity of reducing colinearity redundancy among neurons. Both MHE and half-space MHE outperform the baseline with a huge margin while the network is either very wide or very narrow, showing the superiority in improving generalization.</figDesc><table><row><cell>Network width. We evaluate MHE with</cell><cell></cell><cell></cell><cell></cell></row><row><cell>different network width. We use CNN-9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>as our base network, and change its filter</cell><cell></cell><cell></cell><cell></cell></row><row><cell>number in Conv1.x, Conv2.x and Conv3.x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(see Appendix A) to 16/32/64, 32/64/128,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>64/128/256, 128/256/512 and 256/512/1024. Method</cell><cell>CNN-6</cell><cell>CNN-9</cell><cell>CNN-15</cell></row><row><cell>Baseline</cell><cell>32.08</cell><cell>28.13</cell><cell>N/C</cell></row><row><cell>MHE</cell><cell>28.16</cell><cell>26.75</cell><cell>26.9</cell></row><row><cell>Half-space MHE</cell><cell>27.56</cell><cell>25.96</cell><cell>25.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Testing error (%) of different depth on CIFAR-100. N/C: not converged.Network depth. We perform experiments with different network depth to better evaluate the performance of MHE. We fix the filter number in Conv1.x, Conv2.x and Conv3.x to 64, 128 and 256, respectively. We compare 6-layer CNN, 9-layer CNN and 15-layer CNN. The results are given in Table3. Both MHE and half-space MHE perform significantly better than the baseline. More interestingly, baseline CNN-15 can not converge, while CNN-15 is able to converge reasonably well if we use MHE to regularize the network. Moreover, we also see that half-space MHE can consistently show better generalization than MHE with different network depth.</figDesc><table><row><cell>Method</cell><cell>H O × √</cell><cell>H O √ ×</cell><cell>H O √ √</cell></row><row><cell>MHE</cell><cell>26.85</cell><cell>26.55</cell><cell>26.16</cell></row><row><cell>Half-space MHE</cell><cell>N/A</cell><cell>26.28</cell><cell>25.61</cell></row><row><cell>A-MHE</cell><cell>27.8</cell><cell>26.56</cell><cell>26.17</cell></row><row><cell>Half-space A-MHE</cell><cell>N/A</cell><cell>26.64</cell><cell>26.03</cell></row><row><cell>Baseline</cell><cell></cell><cell>28.13</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on CIFAR-100.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Error (%) of ResNet-32.MHE for ResNets.Besides the standard CNN, we also evaluate MHE on ResNet-32 to show that our MHE is architecture-agnostic and can improve accuracy on multiple types of architectures. Besides ResNets, MHE can also be applied to GoogleNet<ref type="bibr" target="#b45">[46]</ref>, SphereNets<ref type="bibr" target="#b29">[30]</ref> (the experimental results are given in Appendix E), DenseNet<ref type="bibr" target="#b16">[17]</ref>, etc. Detailed architecture settings are given in Appendix A. The results on CIFAR-10 and CIFAR-100 are given in Table5. One can observe that applying MHE to ResNet also achieves considerable improvements, showing that MHE is generally useful for different architectures. Most importantly, adding MHE regularization will not affect the original architecture settings, and it can readily improve the network generalization at a neglectable computational cost.</figDesc><table><row><cell>Method</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>ResNet-110-original [14]</cell><cell>6.61</cell><cell>25.16</cell></row><row><cell>ResNet-1001 [15]</cell><cell>4.92</cell><cell>22.71</cell></row><row><cell>ResNet-1001 (64 batch) [15]</cell><cell>4.64</cell><cell>-</cell></row><row><cell>baseline</cell><cell>5.19</cell><cell>22.87</cell></row><row><cell>MHE</cell><cell>4.72</cell><cell>22.19</cell></row><row><cell>Half-space MHE</cell><cell>4.66</cell><cell>22.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Top1 error (%) on ImageNet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Error on imbalanced CIFAR-10.</figDesc><table><row><cell>Method</cell><cell>Single</cell><cell>Err. (S)</cell><cell>Multiple</cell></row><row><cell>Baseline</cell><cell>9.80</cell><cell>30.40</cell><cell>12.00</cell></row><row><cell>Orthonormal</cell><cell>8.34</cell><cell>26.80</cell><cell>10.80</cell></row><row><cell>MHE</cell><cell>7.98</cell><cell>25.80</cell><cell>10.25</cell></row><row><cell>Half-space MHE</cell><cell>7.90</cell><cell>26.40</cell><cell>9.59</cell></row><row><cell>A-MHE</cell><cell>7.96</cell><cell>26.00</cell><cell>9.88</cell></row><row><cell>Half-space A-MHE</cell><cell>7.59</cell><cell>25.90</cell><cell>9.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>datasets. More results on MegaFace are put in Appendix I. MHE can also improve other face recognition methods, as shown in Appendix F.</figDesc><table><row><cell>m SF</cell><cell>SphereFace</cell><cell>LFW SphereFace+</cell><cell cols="2">MegaFace SphereFace SphereFace+</cell></row><row><cell>1</cell><cell>96.35</cell><cell>97.15</cell><cell>39.12</cell><cell>45.90</cell></row><row><cell>2</cell><cell>98.87</cell><cell>99.05</cell><cell>60.48</cell><cell>68.51</cell></row><row><cell>3</cell><cell>98.97</cell><cell>99.13</cell><cell>63.71</cell><cell>66.89</cell></row><row><cell>4</cell><cell>99.26</cell><cell>99.32</cell><cell>70.68</cell><cell>71.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Accuracy (%) on SphereFace-20 network.</figDesc><table><row><cell>m SF</cell><cell>SphereFace</cell><cell>LFW SphereFace+</cell><cell cols="2">MegaFace SphereFace SphereFace+</cell></row><row><cell>1</cell><cell>96.93</cell><cell>97.47</cell><cell>41.07</cell><cell>45.55</cell></row><row><cell>2</cell><cell>99.03</cell><cell>99.22</cell><cell>62.01</cell><cell>67.07</cell></row><row><cell>3</cell><cell>99.25</cell><cell>99.35</cell><cell>69.69</cell><cell>70.89</cell></row><row><cell>4</cell><cell>99.42</cell><cell>99.47</cell><cell>72.72</cell><cell>73.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Accuracy (%) on SphereFace-64 network.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>and Table 9 are computed without model ensemble and PCA. One can observe that SphereFace+ consistently outperforms SphereFace by a considerable margin on both LFW and MegaFace datasets under all different settings of m SF . Moreover, the performance gain generalizes across network architectures with different depth. Comparison to state-of-the-art.</figDesc><table><row><cell>Method</cell><cell>LFW</cell><cell>MegaFace</cell></row><row><cell>Softmax Loss</cell><cell>97.88</cell><cell>54.86</cell></row><row><cell>Softmax+Contrastive [45]</cell><cell>98.78</cell><cell>65.22</cell></row><row><cell>Triplet Loss [40]</cell><cell>98.70</cell><cell>64.80</cell></row><row><cell>L-Softmax Loss [29]</cell><cell>99.10</cell><cell>67.13</cell></row><row><cell>Softmax+Center Loss [53]</cell><cell>99.05</cell><cell>65.49</cell></row><row><cell>CosineFace [51, 49]</cell><cell>99.10</cell><cell>75.10</cell></row><row><cell>SphereFace</cell><cell>99.42</cell><cell>72.72</cell></row><row><cell>SphereFace+ (ours)</cell><cell>99.47</cell><cell>73.03</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Classifier neurons are the projection bases of the last layer (i.e., output layer) before input to softmax.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project was supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF IIS-1841351 EAGER, NSF CCF-1836822, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA, Amazon AWS and Siemens. We would like to thank NVIDIA corporation for donating Titan Xp GPUs to support our research. We also thank Tuo Zhao for the valuable discussions and suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Net-trim: A layer-wise convex pruning of deep neural networks</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Aghasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Romberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="1920">2016. 20</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One-bit sensing, discrepancy and stolarsky&apos;s principle. Sbornik: Mathematics</title>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Bilyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Lacey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page">744</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural photo editing with introspective adversarial networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="1920">2017. 2, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reducing overfitting in deep networks by decorrelating representations</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Note on d-extremal configurations for the sphere in r d+1</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Götz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">B</forename><surname>Saff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Progress in Multivariate Approximation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1920">2017. 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><surname>Saff</surname></persName>
		</author>
		<idno>math-ph/0311024</idno>
		<title level="m">Minimal riesz energy point configurations for rectifiable d-dimensional manifolds</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discretizing manifolds via minimum energy points</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><surname>Saff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Notices of the AMS</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016. 1, 6, 8, 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">Manu</forename><surname>Gary B Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Song</forename><surname>Forrest N Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1920">2015. 2, 6, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Asymptotics for minimal discrete energy on the sphere</title>
		<author>
			<persName><forename type="first">Arno</forename><surname>Kuijlaars</surname></persName>
		</author>
		<author>
			<persName><surname>Saff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ludmila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="207" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Foundations of modern potential theory</title>
		<author>
			<persName><forename type="first">Naum</forename><surname>Samouilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Landkof</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diversity regularized ensemble pruning</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongmei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2006">2018. 1, 5, 6</date>
		</imprint>
	</monogr>
	<note>Decoupled networks</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017. 1, 2, 5, 9, 14, 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016. 1, 2, 4, 9, 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep hyperspherical learning</title>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan-Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017. 1, 2, 4, 5, 6, 8, 16</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking feature discrimination and polymerization for large-scale recognition</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00870</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">All you need is a good init</title>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Classification and clustering via dictionary learning with structured incoherence and shared features</title>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularizing cnns with locally constrained decorrelations</title>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pau Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><forename type="middle">M</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName><surname>Roca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008">2017. 1, 2, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reducing duplicate filters in deep neural networks</title>
		<author>
			<persName><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aruni</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Deep Learning: Bridging Theory and Practice</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Distributing many points on a sphere. The mathematical intelligencer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amo Bj</forename><surname>Saff</surname></persName>
		</author>
		<author>
			<persName><surname>Kuijlaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mathematical problems for the next century</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Smale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The mathematical intelligencer</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">On the origin of number and arrangement of the places of exit on the surface of pollen-grains</title>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Merkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lambertus</forename><surname>Tammes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1930">1930</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="84" />
		</imprint>
	</monogr>
	<note>Recueil des travaux botaniques néerlandais</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">on the structure of the atom: an investigation of the stability and periods of oscillation of a number of corpuscles arranged at equal intervals around the circumference of a circle; with application of the results to the theory of atomic structure</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>John Thomson</surname></persName>
		</author>
		<author>
			<persName><surname>Xxiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1904">1904</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05599</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06369</idno>
		<imprint>
			<date type="published" when="2017">2017. 19</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09414</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving generative adversarial networks with denoising feature matching</title>
		<author>
			<persName><forename type="first">David</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Farley</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="1920">2017. 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03131</idno>
		<title level="m">Diverse neural network learns true target functions</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01827</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning latent space models with angular constraints</title>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimanu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005">2017. 1, 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Uncorrelation and evenness: a new diversity-promoting regularizer</title>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Orthogonality-promoting distance metric learning: convex relaxation and theoretical analysis</title>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Diversity-promoting bayesian learning of latent variable models</title>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
