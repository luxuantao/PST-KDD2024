<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Premier: A Concurrency-Aware Pseudo-Partitioning Framework for Shared Last-Level Cache</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoyang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Compute Science</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rujia</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Compute Science</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xian-He</forename><surname>Sun</surname></persName>
							<email>sun@iit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Compute Science</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Premier: A Concurrency-Aware Pseudo-Partitioning Framework for Shared Last-Level Cache</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICCD53106.2021.00068</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the number of on-chip cores and application demands increase, efficient management of shared cache resources becomes imperative. Cache partitioning techniques have been studied for decades to reduce interference between applications in a shared cache and provide performance and fairness guarantees. However, there are few studies on how concurrent memory accesses affect the effectiveness of partitioning. When concurrent memory requests exist, cache miss does not reflect concurrency overlapping well. In this work, we first introduce pure misses per kilo instructions (PMPKI), a metric that quantifies the cache efficiency considering concurrent access activities. Then we propose Premier, a dynamically adaptive concurrency-aware cache pseudo-partitioning framework. Premier provides insertion and promotion policies based on PMPKI curves to achieve the benefits of cache partitioning. Finally, our evaluation of various workloads shows that Premier outperforms state-of-the-art cache partitioning schemes in terms of performance and fairness. In an 8-core system, Premier achieves 15.45% higher system performance and 10.91% better fairness than the UCP scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In most multi-core systems, applications running on different cores share the last-level cache (LLC). As the number of cores on the chip increases, applications increasingly compete for the shared cache, which is detrimental to the overall system performance. As a result, it is critical to manage the shared cache to achieve high performance and fairness. Cache partitioning is an effective method to manage cache capacity per core and enforce access isolation between different workloads, thus mitigating contention and interference in shared LLCs.</p><p>Due to historical reasons, conventional cache partitioning schemes are designed to reduce cache misses, which may or may not be the best for concurrent cache memory accesses. Data access concurrency and overlapping are common in modern computing systems. In such cases, some cache misses occur concurrently with other hits (hit-miss overlapping), and the penalty of the misses could be reduced or hidden. As a result, classifying miss types may lead to a better understanding of miss penalty and a better optimized system performance.</p><p>In this work, we first introduce the concept and a formal definition of Pure Misses Per Kilo Instructions (PMPKI). Unlike the classical misses per kilo instructions metric (MPKI), which only focuses on data locality, PMPKI takes into account overlapping in concurrent memory systems and reflects the number of pure misses ( Â§II-A) that hurt the performance most. Next, we present Premier, a concurrency-aware shared cache management framework that takes both data locality and concurrency into account. Based on PMPKI curves, Premier provides insertion and promotion policies for each application to achieve efficient pseudo-partitioning. Our experimental results show that Premier outperforms state-of-the-art cache partitioning schemes in both performance and fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Concurrent Cache Accesses</head><p>Concurrent data accesses provide overlapping <ref type="bibr" target="#b9">[10]</ref>, which helps hide data access latency. At the same cache level, when the cache miss-access cycles overlap with the hit-access cycles, the cache miss penalty can be hidden because hit accesses continue to feed data to the processor <ref type="bibr" target="#b3">[4]</ref>. Note that since each core has its own workload, memory accesses from different cores are not related. Only the overlapping of accesses from the same core is considered meaningful.</p><p>The term Pure Miss was introduced to identify the misses that are more harmful to performance when considering data access concurrency. In multi-core systems, pure miss in the shared cache is the miss access that contains at least one miss-access cycle, which does not have any hit access activity from the same core to overlap. Reducing the number of pure misses has proven to be an effective way to improve the overall memory system performance <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cache Partitioning</head><p>Strict partitioning: Strict partitioning schemes in setassociative caches are typically implemented through waypartitioning, which provides each application with exclusive ownership of a specific partition. Qureshi and Patt proposed utility-based cache partitioning (UCP) <ref type="bibr" target="#b7">[8]</ref>. UCP uses miss curves to determine partitioning decisions, which capture the core's misses for each possible partition size. Subramanian et al. proposed ASM cache partitioning <ref type="bibr" target="#b8">[9]</ref>. ASM partitions the shared cache to achieve minimizing slowdown. However, to estimate the slowdown of the applications, the scheduler of the memory controller needs to be modified, which may negatively impact performance. Although strict partitioning schemes are straightforward, they may lead to low cache utilization <ref type="bibr" target="#b6">[7]</ref>. Pseudo-partitioning: Pseudo-partitioning techniques implicitly partition the cache by managing the cache insertion and promotion policies. Xie and Loh proposed PIPP <ref type="bibr" target="#b10">[11]</ref>, which uses UCP's monitoring circuit to determine the insertion points for all new incoming lines from each core. PIPP only promotes the cache hit line by a single position with a certain probability when a cache line is hit. <ref type="bibr">Kaseridis</ref>   an MLP-aware pseudo-partitioning scheme. However, MCFQ does not realize that the miss penalty can be hidden when miss accesses overlap with hit accesses. Therefore, MCFQ analyzes concurrent memory accesses in a coarse-grained manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Motivation</head><p>Current cache partitioning schemes mainly aim at reducing the absolute number of cache misses and assume there is a high correlation between miss reduction and performance improvement <ref type="bibr" target="#b7">[8]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> demonstrates when concurrent memory requests exist, the correlation between the saved misses by additional cache capacity and the overall system performance is weak. For 605.mcf, the number of misses tends to stabilize when the way is allocated exceeds 6. However, as the allocated cache increases, the CPI of the 605.mcf continues to decrease. For 649.fotonik3d, as the number of allocated ways increases from 1 to 5, the number of misses is significantly decreased. However, the CPI of 649.fotonik3d stays constant from 1 way to 5 ways. Therefore, by monitoring the missing curves, cache partitioning schemes with the goal of reducing the total number of misses cannot ensure the highest performance gains. We are motivated to design a new cache partitioning scheme with a different performance optimization goal by considering the cache misses that harm the performance the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PURE MISSES PER KILO INSTRUCTIONS (PMPKI) A. Definition and Measurement</head><p>We first introduce PMPKI to quantify the cache efficiency of a program in concurrent access activities. Different from the definition of MPKI, which relies on the ratio of miss accesses to evaluate the cache performance, PMPKI focuses on quantifying the ratio of pure misses ( Â§II-A). PMPKI is definded as the number of pure misses per kilo instructions over a given time interval:</p><p>Pure Misses Per Kilo Instructions = 1000 Ã Num. of Pure Misses Num. of Total Instructions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Accuracy of PMPKI Metric</head><p>To verify the correctness of the PMPKI metric, we first measure the L3 PMPKI, L3 MPKI, and CPI for 20 evaluated workloads from SPEC CPU 2017 benchmark suite <ref type="bibr" target="#b1">[2]</ref> in single-core configurations as the L3 cache size is varied. Then for each workload, we show the correlation (r) of PMPKI-CPI and MPKI-CPI. Figure <ref type="figure" target="#fig_1">2</ref> indicates that compare to MPKI, For all workloads, the majority of r(PMPKI, CPI) are more than 0.99. The geometric mean of r(PMPKI, CPI)s is around 0.99, which is much larger than the geometric mean of r(MPKI, CPI)s. The strong correlation between PMPKI and CPI shows that PMPKI has advantages in capturing the concurrency/locality combined characteristics of modern memory systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classify Workloads with PMPKI</head><p>The PMPKI curves also capture the sensitivity of the application performance with different cache sizes. We can classify workloads into different categories by directly monitoring the runtime PMPKI. Cache-insensitive applications are characterized by the fact that their PMPKI hardly changes as the cache size increases. The PMPKI of cache-sensitive applications continues to decrease as the cache size increases. Cache-fitting applications are also sensitive to allocated cache size. These applications benefit from the additional cache capacity until they are allocated enough cache space to fit their working sets. An increase in cache resources beyond their ideal capacity hardly further reduces pure misses. Figure <ref type="figure" target="#fig_2">3</ref> shows the overview of the Premier framework. The grey shaded modules are designs we added to a typical multicore architecture. First, each core has a PMPKI monitoring circuit (PMON) to estimate the number of pure misses for each core when allocated in all possible cache partition sizes (in terms of cache ways) without interfering other running applications ( Â§IV-B). Second, the applications are classified as cache-insensitive, cache-sensitive, or cache-fitting ( Â§IV-C). Then, the partitioning algorithm utilizes the PMPKI curves estimated by PMONs to determine the cache size allocated to each core to minimize the number of pure misses in the system ( Â§IV-D). Finally, Premier uses the pseudo-partitioning technique to sidestep the limitation of strict partitioning. At the end of each period (16K LLC misses), based on the category of each application provided by PMONs and the partitioning plan provided by the partitioning algorithm, Premier dynamically decides the insertion policy for cache misses and the promotion policy for hit accesses ( Â§IV-E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PREMIER: A CONCURRENCY-AWARE PSEUDO-PARTITION FRAMEWORK A. Design Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PMPKI Monitor (PMON)</head><p>To estimate the PMPKI curve when different numbers of ways are assigned to applications, an auxiliary tag directory (ATD) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref> is assigned to each core, tracking the state of the cache if the core has exclusive access to the shared cache. The ATD has the same associativity as the main tag directory of the shared cache. Based on the stack property, a hit access at the i-th most recent position in the LRU stack indicates that the hit will be converted to a miss in the cache with less than i ways (for the same count). Hit counters are assigned to each recency position ranging from MRU to LRU. By counting the number of hits corresponding to the LRU stack positions, a single ATD can provide the hit and miss information for all possible partition sizes at once. Each ATD is also attached with a miss holding buffer (MHB) which has the same entries of MSHR, to simulate the functions of MSHR. At the end of each period, PMON further estimates the number of pure miss accesses based on the hit/miss information provided by ATD, MHB, and the cycle information of each access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Application Classification</head><p>Assuming there is an N -way set-associative shared cache, to reduce the computational complexity, Premier classifies applications based on the PMPKI value when the application is assigned only 1 cache way, N â 1 ways, and N ways (noted as PMPKI 1 , PMPKI N-1 and PMPKI N respectively). If the ratio of PMPKI N and PMPKI 1 of an application is greater than a threshold T insen , we consider the application is cacheinsensitive. If an application is not cache-insensitive, and the difference between PMPKI N-1 and PMPKI N is greater than a threshold T sen , it is characterized as cache-sensitive. The remaining applications are classified as cache-fitting. Based on the analysis of the SPEC CPU 2017 benchmarks we evaluated, T insen is set to 0.95 and T sen is set to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Partitioning Algorithm</head><p>Once PMON has completed the computation of the PMPKI curve for each application, Premier uses the PMPKI curves to feed into the Lookahead algorithm <ref type="bibr" target="#b7">[8]</ref>. Due to the high correlation between PMPKI and performance, the lookahead algorithm is used to calculate the ideal partition cache sizes for each application online, intending to minimize the total number of pure misses incurred by all applications in the shared cache. The partitioning plan provided by the lookahead algorithm for k cores can be denoted as Î©={Ï 0 , Ï 1 , ... , Ï k-1 } and k-1 i=0 Ï i = N , where N is the associativity of the shared cache.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Pseudo-Partitioning Policies</head><p>Insertion policy: Premier first assigns priorities to applications based on how sensitive they are to cache size. Premier assigns the lowest priority to cache-insensitive applications and provides the highest priority to cache-sensitive applications.</p><p>If there are multiple applications in the same category, the priority between these applications is determined according to the PMPKI 1 of each application. Then, Premier combines the priority of the applications with the partition sizes calculated by the lookahead algorithm to determine the insertion point for each application. Figure <ref type="figure" target="#fig_3">4</ref> illustrates the insertion positions for a 16-way cache shared between four cores. Suppose the target partitioning plan is Î©={7, 5, 3, 1}; core 0 has the highest priority, followed by core 1 and core 2 , and core 3 has the lowest priority. Premier only inserts new cache blocks near MRU positions for higher priority applications to ensure that higher priority applications get the cache capacity they require and encourage them to steal cache capacity from other applications. New cache blocks from lower priority applications are inserted close to the LRU position.</p><p>Hit-promotion policy: In order to improve the data locality, for cache-sensitive and cache-fitting applications, if a cache block receives a hit, Premier moves the cache block to the MRU position in the LRU stack. Otherwise, Premier only promotes the cache block to its insertion position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL METHODOLOGY</head><p>We use the ChampSim <ref type="bibr" target="#b0">[1]</ref> simulator to evaluate Premier in multi-core systems. Table <ref type="table" target="#tab_1">I</ref> describes the configuration used in our study. We select benchmarks randomly from the SPEC CPU 2017 benchmarks <ref type="bibr" target="#b1">[2]</ref> to generate mixed-copy workloads as shown in Table <ref type="table" target="#tab_2">II</ref>. We warm the cache for 50M instructions and measure the behavior of the next 200M instructions.</p><p>For each workload we evaluate the throughput (sum of IPCs, IP C i ) and fairness (harmonic mean of normalized IPCs, N/ (IP C i,alone /IP C i ), where IP C i,alone is the IPC when the application executes in isolation under the ownership of all cache resources <ref type="bibr" target="#b5">[6]</ref>). We select UCP <ref type="bibr" target="#b7">[8]</ref> as the baseline for comparison. We further compare Premier against three stateof-the-art cache partitioning schemes: MCFQ <ref type="bibr" target="#b2">[3]</ref>, PIPP <ref type="bibr" target="#b10">[11]</ref>, and ASM <ref type="bibr" target="#b8">[9]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance Evaluation</head><p>Figure <ref type="figure" target="#fig_4">5</ref> shows that on the 2-core system, Premier outperforms existing schemes across the board, with a geometric mean speedup of 8.50% over UCP. For 4-core mixed workloads, Figure <ref type="figure">6</ref> shows Premier offers a speedup of 9.17% on average, an improvement of 7.62% over MCFQ, 4.95% over PIPP, 7.49% over ASM. Figure <ref type="figure" target="#fig_6">7</ref> shows that the Premier performance advantage comes from the fact that Premier significantly reduces LLC pure misses compared to the stateof-the-art schemes. Premier yields 4.13%, 3.32%, 2.09%, and 2.97% average pure miss reduction over UCP, MCFQ, PIPP, and ASM. Figure <ref type="figure" target="#fig_7">8</ref> shows that the advantage of Premier further increases on an 8-core system. Premier provides a 15.45% higher geometric mean throughput over the baseline UCP, 10.79% over MCFQ, 6.54% over PIPP, and 15.07% over ASM. When LLC cache size increases and the contention (core number) increases, we observe that Premier has a better opportunity to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fairness Evaluation</head><p>Figure <ref type="figure" target="#fig_8">9</ref> summarizes the fairness comparison as we increase the number of cores. Premier provides higher fairness than every state-of-the-art cache partitioning scheme in all configurations. Concurrency increases as the number of cores increases, and since Premier is concurrency-aware, the fairness advantage of Premier becomes greater. In the 8-core configuration, Premier achieves a fairness improvement over UCP by 10.91% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this paper, we propose pure miss per kilo instructions (PMPKI), a metric that considers both data locality and concurrency. We present Premier, a concurrency-aware pseudopartitioning framework based on monitoring the PMPKI of each application to provide the benefit of dynamic capacity allocation, adaptive insertion, and interference mitigation. Our evaluations across a wide variety of workloads and system    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: MPKI and CPI for SPEC CPU2017 benchmarks as the cache size is varied. (The x-axis is the number of ways allocated from a 16-way 2MB L3 cache to this workload.)</figDesc><graphic url="image-1.png" coords="2,84.71,72.79,93.78,68.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Correlation coefficient analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Block diagram of Premier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The insertion positions for four applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Throughput speedup over UCP for 2-core workloads.</figDesc><graphic url="image-7.png" coords="4,78.31,189.39,66.13,66.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.</head><label></label><figDesc>Fig. over UCP for 4-core workloads.</figDesc><graphic url="image-13.png" coords="4,336.47,151.14,66.13,64.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Pure miss workloads.</figDesc><graphic url="image-16.png" coords="4,346.38,231.35,59.45,62.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Throughput speedup over UCP for 8-core workloads.</figDesc><graphic url="image-19.png" coords="4,348.80,309.49,69.37,67.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Fairness over UCP for 2, 4, 8 cores. configurations show that Premier is superior to the state-ofthe-art cache partitioning schemes in terms of performance and fairness. ACKNOWLEDGMENT</figDesc><graphic url="image-20.png" coords="4,417.84,309.49,69.17,67.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al. proposed MCFQ [3],</figDesc><table><row><cell cols="2">2021 IEEE 39th International Conference on Computer Design (ICCD)</cell></row><row><cell>2021 IEEE 39th International Conference on Computer Design (ICCD) | 978-1-6654-3219-1/21/$31.00 Â©2021 IEEE | DOI: 10.1109/ICCD53106.2021.00068</cell><cell></cell></row><row><cell>978-1-6654-3219-1/21/$31.00 Â©2021 IEEE DOI 10.1109/ICCD53106.2021.00068</cell><cell>391</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Simulated system configurations</figDesc><table><row><cell>Processor</cell><cell>2 to 8 cores, 4GHz, 8-issue width, 256-entry ROB</cell></row><row><cell>L1 Cache</cell><cell>private, split 32KB I/D-cache/core, 64B line, 8-way, 4-cycle latency, 8-entry MSHR, LRU</cell></row><row><cell>L2 Cache</cell><cell>private, 256KB/core, 64B line. 8-way, 10-cycle latency, 32-entry MSHR, LRU</cell></row><row><cell>L3 Cache</cell><cell>shared, 2MB/core, 64B line, 16-way,</cell></row><row><cell>(LLC)</cell><cell>20-cycle latency, 64-entry MSHR</cell></row><row><cell>DRAM</cell><cell>8GB 2 channels, 64-bit channel, 2400MT/s, tRP=15ns, tRCD=15ns, tCAS=12.5ns</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Evaluated workloads</figDesc><table><row><cell>2-core</cell><cell>4-core</cell><cell>8-core</cell></row><row><cell>MIX 1: 603,623</cell><cell>MIX 1: 605,621,627,654</cell><cell>MIX 1: 607,619,620,</cell></row><row><cell>MIX 2: 603,654</cell><cell>MIX 2: 607,619,628,620</cell><cell>623,625,628,638,657</cell></row><row><cell>MIX 3: 605,607</cell><cell>MIX 3: 621,605,602,603</cell><cell>MIX 2: 605,621,627,</cell></row><row><cell>MIX 4: 605,627</cell><cell>MIX 4: 619,623,602,603</cell><cell>649,654,620,623,628</cell></row><row><cell>MIX 5: 607,619</cell><cell>MIX 5: 605,621,654,623</cell><cell>MIX 3: 605,621,654,</cell></row><row><cell>MIX 6: 619,623</cell><cell>MIX 6: 605,621,619,623</cell><cell>607,619,623,625,628</cell></row><row><cell>MIX 7: 621,627</cell><cell>MIX 7: 621,619,623,620</cell><cell>MIX 4: 605,627,649,</cell></row><row><cell>MIX 8: 623,649</cell><cell>MIX 8: 621,649,623,603</cell><cell>654,619,628,602,603</cell></row><row><cell>MIX 9: 627,654</cell><cell>MIX 9: 621,619,623,603</cell><cell>MIX 5: 605,654,607,</cell></row><row><cell cols="2">MIX 10: 649,654 MIX 10: 605,623,602,603</cell><cell>619,620,657,602,603</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:18:57 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:18:57 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research is supported in part by the National Science Foundation under Grants CCF-2029014, CCF-2008907, CNS-1730488, and by the NSF supported Chameleon testbed facility.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<title level="m">The champsim simulator</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.spec.org/cpu2017/" />
		<title level="m">Spec cpu2017 benchmark suite</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cache friendliness-aware management of shared last-level caches for high performance multi-core systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kaseridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on computers</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="874" to="887" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A study on modeling and optimization of memory systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Espina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="89" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Apac: An accurate and adaptive prefetch framework with concurrent memory access analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
		<idno>ICCD-38</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Balancing thoughput and fairness in smt processors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gummaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<idno>ISPASS&apos;01</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of techniques for cache partitioning in multicore processors</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Utility-based cache partitioning: A low-overhead, high-performance, runtime mechanism to partition shared caches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<idno>MICRO-39</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The application slowdown model: Quantifying and controlling the impact of inter-application interference at shared caches and main memory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<idno>MICRO-48</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Concurrent average memory access time</title>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pipp: Promotion/insertion pseudo-partitioning of multi-core shared caches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
