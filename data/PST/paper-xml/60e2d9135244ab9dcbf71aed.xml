<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">END-TO-END MULTILINGUAL AUTOMATIC SPEECH RECOGNITION FOR LESS-RESOURCED LANGUAGES: THE CASE OF FOUR ETHIOPIAN LANGUAGES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Solomon</forename><surname>Teferra Abate</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martha</forename><forename type="middle">Yifiru</forename><surname>Tachbelie</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
							<email>tanja.schultz@uni-bremen.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSL</orgName>
								<orgName type="institution" key="instit2">University of Bremen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SIS</orgName>
								<orgName type="institution" key="instit2">Addis Ababa University</orgName>
								<address>
									<country key="ET">Ethiopia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CSL</orgName>
								<orgName type="institution" key="instit2">University of Bremen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">END-TO-END MULTILINGUAL AUTOMATIC SPEECH RECOGNITION FOR LESS-RESOURCED LANGUAGES: THE CASE OF FOUR ETHIOPIAN LANGUAGES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICASSP39728.2021.9415020</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ethiopian Languages</term>
					<term>End-to-End ASR</term>
					<term>Deep Neural Networks</term>
					<term>Modeling Units</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The End-to-End (E2E) approach, which maps a sequence of input features into a sequence of graphemes or words, to Automatic Speech Recognition (ASR) is a hot research agenda. It is interesting for less-resourced languages since it avoids the use of pronunciation dictionary, which is one of the major components in the traditional ASR systems. However, like any deep neural network (DNN) approaches, E2E is data greedy. This makes the application of E2E to less-resourced languages questionable. However, using data from other languages in a multilingual (ML) setup is being applied to solve the problem of data scarcity. We have, therefore, conducted ML E2E ASR experiments for four less-resourced Ethiopian languages using different language and acoustic modelling units. The results of our experiments show that relative Word Error Rate (WER) reductions (over the monolingual E2E systems) of up to 29.83% can be achieved by just using data of two related languages in E2E ASR system training. Moreover, we have also noticed that the use of data from less related languages also leads to E2E ASR performance improvement over the use of monolingual data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Artificial Neural Networks (ANNs) have been used in Automatic Speech Recognition (ASR) for about five decades. However, their use resulted in dramatic improvement in ASR performance since 2009. Numerous studies showed that hybrid Hidden Markov Model-Deep Neural Network (HMM-DNN) systems outperform the dominant Hidden Markov Model-Gaussian Mixture Model (HMM-GMM) on the same data <ref type="bibr" target="#b0">[1]</ref>. Similarly we have also achieved improvements of ASR performance for four Ethiopian languages (Amharic, Oromo, Tigrigna and Wolaytta) as a result of using hybrid HMM-DNN <ref type="bibr" target="#b1">[2]</ref> over the HMM-GMM based ASR systems.</p><p>Recently, the End-to-End (E2E) framework, which directly maps a sequence of input acoustic features into a sequence of grapheme or words, is an active research area since it avoids the explicit use of linguistic knowledge (in the form of pronunciation dictionary and language model) that are required in the traditional ASR systems. Since the performance of E2E systems could not outperform the hybrid HMM-DNN approach, researchers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> try to improve the performance by integrating a language model. However, E2E ASR systems are still advantageous as they do not require pronunciation dictionary which requires linguistic expertise and is expensive to prepare. Thus, E2E framework is attractive for less-resourced languages as a pronunciation dictionary, which is one of the important resources/components in both HMM-DNN and HMM-GMM systems, is not required. However, like any other DNN approaches, E2E is training data greedy. This affects its application for less-resourced languages that do not have the required speech and text data for the development of speech processing applications. But the problem of training data scarcity has been addressed by the use of data from other languages in a ML approach.</p><p>ML ASR (MLASR) systems are useful in a number of ways, including the development of language agnostic speech technologies <ref type="bibr" target="#b4">[5]</ref>. They are particularly interesting for lessresourced languages where training data for the development of the ASR systems are sparse or not available at all <ref type="bibr" target="#b5">[6]</ref>. Consequently, various studies in MLASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> have been conducted for several language groups. The research trend shows that the use of DNNs results in better methods for the development of MLASR systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. ML systems in the E2E framework have also been and being investigated <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. The problem of using the E2E approach for the development of MLASR is that we should find common modeling units for the involved languages. Characters and words are not common across many languages. Researchers have, therefore, investigated the use of different units such as bytes <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this paper, we present ML E2E ASR experiments we have conducted for four less-resourced Ethiopian languages, namely, Amharic, Oromo, Tigrigna and Wollaytta. To our knowledge, such ML E2E investigation has not been conducted for these languages before and therefore the perfor-mance of ML E2E ASR systems of these languages is not known. In our experiments, we investigated the use of different modeling units (characters and phones) in ML E2E acoustic modeling and the use of speech data from closely related languages as well as GlobalPhone <ref type="bibr" target="#b22">[23]</ref> languages, considering the four Ethiopian languages as targets. Moreover, we have experimented by combining text data of related languages for ML language model (LM) training.</p><p>The next section provides a description of the four Ethiopian languages, which are target languages in our experiments. Section 3 describes the data used in our experiments. Section 4 presents the results of the baseline monolingual E2E systems using characters and phones as modeling units for the four target languages. In Section 5, the results of ML E2E systems are presented. Finally, in Section 6 conclusions and a way forward are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TARGET LANGUAGES</head><p>In our experiments, we have considered four Ethiopian languages (Amharic, Oromo, Tigrigna and Wolaytta) as targets. Amharic and Tigrigna from the Semitic language family while Oromo and Wolaytta are from Cushitic and Omotic language families, respectively. Amharic and Tigrigna are written in the Ethiopic script known as fid@l. It is syllabic since each symbol represents a consonant and a vowel. Each of the core consonants has seven shapes or orders according to the vowels combined with them. The writing system of Oromo and Wolaytta uses the Latin script. In these languages almost all the consonants could be in geminated and non-geminated forms. Current writers differentiate geminated and non-geminated consonants as well as long and short vowels. The numbers of graphemes covered in the corpora (without counting the geminated forms) are 233 (AMH2005) and 225 (AMH2020) for the Amharic corpora, while they are 247, 26 and 27 for Tigrigna, Oromo and Wolaytta, respectively. Amharic and Tigrigna have 28 and 31 consonants, respectively and both have the same 7 vowels. The Amharic phones are a subset of Tigrigna phones. Similarly, Oromo and Wolaytta use the same 5 vowels that come in long and short variants, while they have 28 and 26 consonants, respectively. Although they belong to different language families, Oromo and Wolaytta also share a number of consonants too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE CORPORA</head><p>We used five corpora (two corpora for Amharic) of the four target languages. In addition, GlobalPhone, a ML database of speech and text for 22 languages <ref type="bibr" target="#b22">[23]</ref>, has been used as source data. The following is a brief description of the corpora used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ethiopian languages corpora</head><p>Read speech corpora of four Ethiopian Languages (Amharic, Tigrigna, Oromo and Wolaytta) are used in our experiments.</p><p>For Amharic, we have used two corpora: AMH2005 and AMH2020. AMH2005 <ref type="bibr" target="#b23">[24]</ref> is a read speech corpus that contains 20 hours of training speech (11k utterances), development (dev) and test sets read by 20 other speakers <ref type="bibr">(10 each)</ref>. The domain of this corpus is broadcast news and the recording was done in a noise free environment. Moreover, the maximum length of the utterances is limited to 20 words. AMH2020 together with the corpora of other three Ethiopian languages have been collected in Ethiopia <ref type="bibr" target="#b24">[25]</ref>. The AMH2020, Tigrigna and Oromo speech corpora consist of speech from 98 speakers each while the Wolaytta corpus is read by 85 speakers. For each of these corpora development and evaluation sets (speech of 4 speakers per set, ranging from 1 hour to 1.7 hours) have been held out from the total recordings. The training speech sizes are 24, 22.1, 22.8 and 29.7 hours for AMH2020, Tigrigna, Oromo and Wollaytta, respectively. The domain of these corpora includes broadcast news, the bible, other religious books, etc. The recordings were done using smartphones in different environments and as a result, they are not as clean as the AMH2005 corpus. In addition, there is no limit with the maximum length of the utterances when sentences are selected. For details on Ethiopian languages corpora, we direct readers to <ref type="bibr" target="#b24">[25]</ref>.</p><p>We have used the dev and test sets of these speech corpora for tuning and testing, respectively. LM weighs are tuned using the transcriptions of the dev sets of each speech corpus, which consists of 760 sentences for Amharic AMH2005, 507 for AMH2020, 511, 505 and 553 sentences for Tigrigna, Oromo and Wolaytta corpora, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Globalphone</head><p>GlobalPhone (GP) is a ML corpus consisting of speech data, corresponding transcriptions and pronunciation dictionaries covering the vocabulary of the transcripts. Currently, the GP corpus covers 22 languages, i.e. Arabic (modern standard), Bulgarian, Chinese (Mandarin and Shanghai), Croatian, Czech, French, German, Hausa, Japanese, Korean, Polish, Portuguese, Russian, Spanish, Swahili, Swedish, Tamil, Thai, Turkish, Ukrainian, and Vietnamese. Detailed description of GP can be found <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BASELINE MONOLINGUAL E2E ASR SYSTEMS</head><p>As baselines in the E2E framework, we have developed monolingual E2E ASR systems for each of the target languages (Amharic, Oromo, Tigrigna and Wolaytta) using the training corpus of each language and the ESPRESSO E2E ASR toolkit. ESPRESSO is an open-source, modular, extensible E2E neural ASR toolkit developed based on PyTorch deep learning libraries and the FAIRSEQ toolkit <ref type="bibr" target="#b25">[26]</ref>.</p><p>Literature shows that the performance of a pure E2E ASR model, i.e. without an external LM component, is far from satisfactory <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Consequently, ESPRESSO provides the possibility of external LM integration employing shallow fusion. It supports integration of three types of neural LMs: character-based, look ahead word based and multi-level (a combination of character based and word based LMs) LMs <ref type="bibr" target="#b25">[26]</ref>. We used character-, phone-(when phones are used in acoustic modeling) and word based LMs.</p><p>For language modeling, we have used different sizes of text corpus obtained from the web, except for Wolaytta. Since we found no text resources on the web for Wolaytta, only the training transcription has been used to train LMs. For Amharic, Tigrigna, Oromo and Wolaytta we have used text data consisting of about 4M, 4M, 1.2M and 226K word tokens, respectively. These text data are used for language modeling in the ML experiments too.</p><p>For each of the languages, we have developed Long Short Term Memory (LSTM) based LMs following the wsj recipe provided in ESPRESSO. The default hyper-parameters of the recipe have been used. For the word based LMs, the number of decoder layers used is 3 and the vocabulary size of LMs is 32.5K for Amharic and Tigrigna while it is 21K and 25K for Oromo and Wolaytta, respectively. We have tried to use larger vocabularies but could not be successful due to memory problem 1 . The hidden and embedding dimensions are 1200 each. For character as well as phone based LMs, the number of decoder layers is 2. The embedding and hidden dimensions are 48 and 650, respectively. For phone-based LMs as well as E2E acoustic modeling, we have converted the training transcriptions as well as the LM text to phonebased representations as a pre-processing task. In this task we have used the pronunciation dictionary of the respective corpus (both target and source corpora) to convert the graphemebased speech transcriptions (of the training, dev and test sets) and LM training texts of the target languages to their phone representation.</p><p>WSJ recipe for the encoder-decoder model with the default hyper-parameters has been used to train the acoustic models. We have used 3 encoder and 3 decoder layers. The embedding dimension is 48 while the hidden dimension is 320. During decoding, we experimented on different beam sizes and found that the best beam size is 50. We have also experimented with different LM fusion weights and considered the weight that leads to low WER for the majority of the corpora. For the word-based LMs, the best LM fusion weight is 0.2 while for the character-and phone-based LMs, the best fusion weight is 0.5. Table <ref type="table" target="#tab_0">1</ref> presents the Character Error Rate (CER), Phone Error Rate (PER) and WER of the monolingual E2E ASR systems for each of the languages. For comparison purpose, we also presented the hybrid HMM-DNN WERs presented in <ref type="bibr" target="#b1">[2]</ref>, which used triphone-based acoustic models and word-based LMs. Although we have presented the CER and PER of all the E2E ASR systems, we have made all the comparisons and analysis on the basis of WER for it is the standard ASR evaluation metrics.</p><p>As can be seen from Table <ref type="table" target="#tab_0">1</ref>, E2E systems fused with word-based LMs did not perform better than the HMM-DNN 1 We used machines with a total RAM of 251 GBs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MULTILINGUAL E2E ASR SYSTEMS</head><p>The first ML E2E ASR experiment we have conducted is using data from two related languages. Since Amharic and Tigrigna are related and use the same writing system, we have developed ML E2E ASR by combining the data of these two languages. Similarly, since Oromo and Wolaytta are related and use a similar writing system, we have developed ML E2E ASR for these languages by combining their data. These systems are referred as ML2, where ML stands for multilingual and 2 stands for the number of languages whose data are used in E2E ASR system training. We did preliminary experiments by developing character-based ML2 E2E systems by combining LM training data only, speech data only and both LM and speech data. We have decoded speech data of Tigrigna and Wolaytta. However, ASR performance improvements have been obtained when we combine speech data only. Therefore, the remaining ML E2E experiments are conducted by combining only speech data of the involved languages.  As can be seen from Table <ref type="table" target="#tab_1">2</ref>, ML2 E2E systems led to lower WER compared to the monolingual E2E systems. Generally, relative WER improvements ranging from 2.96% to 29.83% have been obtained in the character-based ML2 E2E systems. The highest relative WER improvement being for AMH2005 corpus while the lowest relative improvemnt is for Wolaytta, which has relatively bigger speech corpus than the others. The relative WER improvements for phone-based ML2 E2E ASR ranges from 4.45% for Oromo to 28.45% for AMH2005.</p><p>Since using speech data of two languages in E2E ASR training brought improvement in ASR performance, we then experimented by using speech data of the four Ethiopian languages in E2E ASR system training. We call these systems ML4, following the naming convention described above. Since the writing systems used by the languages are different, we have converted the training transcriptions and LM training texts of the languages to phone-based representation so as to use data from all the languages. To enable the development of ML speech processing, the phone names are made consistent across languages using the ML phone representation we developed in <ref type="bibr" target="#b14">[15]</ref>. Table <ref type="table" target="#tab_3">3</ref> presents the PER and WER of ML4 E2E ASR systems. As can be seen from the Table, WER reductions, over the ML2 systems, have been obtained for three (AMH2020, Oromo and Wolaytta) of the five corpora, as a result of using all the corpora in E2E ASR system training. Though we did not get WER improvement for Tigrigna and AMH2005, the degradation in their performance (from the ML2 system) is statistically insignificant. However, compared to the monolingual E2E systems, the ML4 systems brought WER reduction for all the corpora. The relative improvement ranges from 4.69% for Tigrigna to 26.19% for AMH2005.</p><p>As the use of speech data from the four Ethiopia languages brought WER improvement, we have experimented on the use of more data from other languages. For this purpose, we have used GlobalPhone, which provides speech and text data for 22 languages as described in Section 3, together with the speech data of the four Ethiopian languages. As we did in ML4 E2E experiments, we have converted the transcriptions of the GlobalPhone languages to phone-based representation taking advantage of the ML phone representations we have developed in <ref type="bibr" target="#b14">[15]</ref> and the available pronunciation dictionar for all the languages. However, the conversion to phone transcriptions could not be successful for Arabic, Thai and Japanese due to the mixed encoding used in their transcriptions. Thus, the speech data of these languages could not be used in ML E2E training. We have, therefore, used data of the remaining 19 GlobalPhone languages together with the data of the 4 Ethiopian languages. The ML E2E system trained using data from these languages is referred as ML23, since data from 23 languages are used in training. However, the target languages are always the four Ethiopian languages. Table <ref type="table" target="#tab_3">3</ref> depicts the PER and WER of ML23 for the target languages. As shown in Table <ref type="table" target="#tab_3">3</ref>, ML23 system did not bring WER reduction, over the ML4 E2E systems, for majority of the target corpora except for AMH2005 corpus for which 4.41% relative WER reduction has been obtained. This might be due to the similarity in the domain of the source and this target corpora. The domain of AMH2005 and GlobalPhone is news, whereas the domain of the other Ethiopian languages is mixed (news, Bible, other religious books, etc.). Compared to the phone-based ML2 systems, ML23 resulted in WER reduction for AMH2005, Oromo and Wolaytta. However, ML23 systems outperformed all the monolingual systems except for Tigrigna for which insignificant performance degradation (0.25 absolute WER increase in ML23) is observed. The results of our experiments, generally, confirm that using data of related languages in ML ASR training leads to greater performance improvement. However, using data from any language in a ML setup is always more advantageous than the use of monolingual data only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we presented ML E2E ASR experiments for four Ethiopian target languages for which no E2E ASR experiments have been conducted previously. In our experiments, we used different language (word, character and phones) and acoustic (character and phone) modeling units. Character and phone units are generally better than small vocabulary words for language modeling. For acoustic modeling, phone-based models are better than character-based ones for most of the languages. Although combining only LM training text and combining both training speech and LM training text did not lead to performance improvement, combining training speech only from related and even less related languages resulted in performance improvement over monolingual E2E systems. We would propose extending this research for more target languages and conduct experiments using E2E approach at different levels of data scarcity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>CER, PER and WER of Monolingual E2E ASR DNN based systems, except for Wolaytta. This is due to the morphological complexity of the languages that increases the out-of-vocabulary (OOV) rate in the word based LMs. This can be avoided in character based LMs. For Wolaytta, since the training text is very small (we used only the training transcription consisting of 226K tokens), the character-based LM did not bring improvement over the HMM-DNN based as well as E2E systems that use wordbased LMs. Using phones in both acoustic and language modeling led to improvements in performance over the characterbased system for Amharic and Tigrigna. This is attributed to the fact that the Amharic and Tigrigna characters are CV syllables, which affects the number of examples used in the training. Since the word-based LMs have low performance compared to the character-and phone-based ones, we used character-and phone-based models in all other experiments.</figDesc><table><row><cell cols="2">Languages DNN</cell><cell>Word LM</cell><cell>Char LM</cell><cell>Phone LM</cell></row><row><cell></cell><cell cols="4">WER CER WER CER WER PER WER</cell></row><row><cell cols="5">AMH2005 23.05 9.29 26.28 7.63 19.81 5.16 19.05</cell></row><row><cell>AMH2020</cell><cell>-</cell><cell>-</cell><cell cols="2">-14.65 36.43 8.84 29.70</cell></row><row><cell>TIR</cell><cell cols="4">26.94 10.32 27.27 9.48 25.55 6.18 22.18</cell></row><row><cell>ORM</cell><cell cols="4">32.28 10.11 32.13 9.53 30.36 10.83 30.36</cell></row><row><cell>WAL</cell><cell cols="4">23.23 9.23 25.81 8.64 23.35 9.24 24.11</cell></row><row><cell cols="5">based systems for each of the languages. However, when</cell></row><row><cell cols="5">character based LMs are fused, the E2E ASR systems out-</cell></row><row><cell cols="2">perform the HMM-</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>presents results of both character-and phone-based ML2 systems. In these systems, for Amharic and Tigrigna, three speech corpora (two Amharic + one Tigrigna) are used in training the ML character-and phone-based E2E acoustic model. Test speech of each corpus is decoded. Oromo and Wolaytta speech data are used in training, and test speech of each language is decoded. The LMs fused during decoding are LMs developed using LM training text of each language.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>CER, PER and WER of ML2 E2E ASR</figDesc><table><row><cell>Language/Corpora</cell><cell cols="2">Character based</cell><cell cols="2">Phone based</cell></row><row><cell></cell><cell>CER</cell><cell>WER</cell><cell>PER</cell><cell>WER</cell></row><row><cell>AMH2005</cell><cell>4.23</cell><cell>13.90</cell><cell>3.28</cell><cell>13.63</cell></row><row><cell>AMH2020</cell><cell>10.81</cell><cell>29.58</cell><cell>7.98</cell><cell>27.81</cell></row><row><cell>TIR</cell><cell>8.21</cell><cell>23.00</cell><cell>5.30</cell><cell>20.91</cell></row><row><cell>ORM</cell><cell>9.41</cell><cell>29.12</cell><cell>10.13</cell><cell>29.01</cell></row><row><cell>WAL</cell><cell>8.3</cell><cell>22.66</cell><cell>8.7</cell><cell>21.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>PER and WER of ML4 and ML23 E2E ASR</figDesc><table><row><cell>Language/Corpora</cell><cell cols="2">ML4</cell><cell cols="2">ML23</cell></row><row><cell></cell><cell>PER</cell><cell>WER</cell><cell>PER</cell><cell>WER</cell></row><row><cell>AMH2005</cell><cell>3.26</cell><cell>14.06</cell><cell>3.12</cell><cell>13.44</cell></row><row><cell>AMH2020</cell><cell>7.26</cell><cell>26.16</cell><cell>7.67</cell><cell>29.34</cell></row><row><cell>TIR</cell><cell>5.12</cell><cell>21.14</cell><cell>5.49</cell><cell>22.43</cell></row><row><cell>ORM</cell><cell>8.7</cell><cell>27.37</cell><cell>9.17</cell><cell>28.86</cell></row><row><cell>WAL</cell><cell>6.5</cell><cell>18.51</cell><cell>6.63</cell><cell>19.91</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 03:36:42 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks based automatic speech recognition for four ethiopian languages</title>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Teferra Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Yifiru Tachbelie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">RWTH ASR Systems for LibriSpeech: Hybrid vs Attention</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfried</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of incorporating an external language model into a sequence-to-sequence model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICASSP</title>
		<imprint>
			<biblScope unit="page" from="1" to="5828" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language-agnostic multilingual modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8239" to="8243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language-independent and language-adaptive acoustic modeling for speech recognition</title>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="51" />
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A study of multilingual speech recognition</title>
		<author>
			<persName><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Bratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neumeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>EUROSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual and crosslingual speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Workshop on Broadcast News Transcription and Understanding</title>
				<meeting>DARPA Workshop on Broadcast News Transcription and Understanding</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="259" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Globalphone: a multilingual speech and text database developed at karlsruhe university</title>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in IN-TERSPEECH. 2002, ISCA</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multilingual acoustic modeling using graphemes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanthak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>in IN EUROSPEECH</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multilingual deep neural network based acoustic modeling for rapid language adaptation</title>
		<author>
			<persName><forename type="first">Ngoc Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Imseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlícek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Using language adaptive deep neural networks for improved multilingual speech recognition</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Waibel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multilingual techniques for low resource automatic speech recognition</title>
		<author>
			<persName><forename type="first">Ekapol</forename><surname>Chuangsuwanich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multilingual acoustic and language modeling for ethio-semitic languages</title>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Teferra Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Yifiru Tachbelie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Development of multilingual asr using globalphone for less-resourced languages: The case of ethiopian languages</title>
		<author>
			<persName><forename type="first">Martha</forename><surname>Yifiru Tachbelie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Teferra Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilingual acoustic models using distributed deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="8619" to="8623" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multilingual speech recognition with corpus relatedness sampling</title>
		<author>
			<persName><forename type="first">Xinjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Cernocký</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bytes are all you need: End-to-end multilingual speech recognition and synthesis with bytes</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5621" to="5625" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilingual speech recognition with a single end-to-end model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4904" to="4908" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilingual sequence-to-sequence speech recognition: Architecture, transfer learning, and language modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Sri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SLT</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilingual end-to-end speech recognition with a single transformer on lowresource languages</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1806.05059</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Globalphone: A multilingual text and speech database in 20 languages</title>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Schlippe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An amharic speech corpus for large vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Teferra Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Menzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bahiru</forename><surname>Tafila</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large vocabulary read speech corpora for four ethiopian languages: Amharic, tigrigna, oromo and wolaytta</title>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Teferra Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Yifiru Tachbelie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Melese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hafte</forename><surname>Abera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tewodros</forename><surname>Abebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wondwossen</forename><surname>Mulugeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaregal</forename><surname>Assabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Million</forename><surname>Meshesha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Atinafu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyam</forename><surname>Ephrem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Espresso: A fast end-to-end neural speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>in 2019 IEEE ASRU</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
