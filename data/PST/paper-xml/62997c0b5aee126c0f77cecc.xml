<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hard Negative Sampling Strategies for Contrastive Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Afrina</forename><surname>Tabassum</surname></persName>
							<email>afrina@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muntasir</forename><surname>Wahed</surname></persName>
							<email>mwahed@vt.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hoda</forename><surname>Eldardiry</surname></persName>
							<email>hdardiry@vt.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ismini</forename><surname>Lourentzou</surname></persName>
							<email>ilourentzou@vt.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hard Negative Sampling Strategies for Contrastive Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the challenges in contrastive learning is the selection of appropriate hard negative examples, in the absence of label information. Random sampling or importance sampling methods based on feature similarity often lead to sub-optimal performance. In this work, we introduce UnReMix, a hard negative sampling strategy that takes into account anchor similarity, model uncertainty and representativeness. Experimental results on several benchmarks show that UnReMix improves negative sample selection, and subsequently downstream performance when compared to state-of-the-art contrastive learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the potential of alleviating data annotation costs and the substantial effort requirements in encoding domain-specific knowledge, self-supervised representation learning methods have attracted research effort, with recent contrastive learning frameworks even surpassing the downstream performance of supervised learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. Typically, contrastive methods aim at minimizing distances in feature space for similar example pairs (positive examples) and maximizing distances of dissimilar example pairs (negative examples) <ref type="bibr" target="#b10">[11]</ref>. There has been a growing interest in contrastive learning research, in particular for obtaining better data representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Several recent studies investigate the impact of negative sampling strategies, with recent work showing that increasing the number of negative samples results in learning better representations. However, a few of the hardest negative samples tend to have the same label as the anchor, hampering the learning process <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>. Hence, selecting appropriate informative hard negative examples is a crucial step for the success of contrastive learning.</p><p>Various negative selection mechanisms have been proposed, that mainly aim to select discriminative negative examples based on the current learned feature representations, often used in conjunction with importance sampling or mixup interpolation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37]</ref>. Most contrastive methods either uniformly sample negatives, i.e., assuming that all negative examples are equally important <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>, compute importance scores based on feature similarity <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b21">22]</ref> or uncertainty <ref type="bibr" target="#b36">[37]</ref>, or employ mixing of features in the feature/image space <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b59">60]</ref>. As such, there is no clear notion of "informativeness" incorporated in the negative selection process. Particularly, prior works rarely consider the distance from the model decision boundary, e.g., by incorporating model uncertainty, or representativeness of the selected negative examples, e.g., whether selected negatives indeed represent the diverse distribution of negatives.</p><p>Hard negative example selection in contrastive learning poses three challenges: <ref type="bibr" target="#b0">(1)</ref> there is no label information available, hence there is a requirement for unsupervised strategies for instance selection, <ref type="bibr" target="#b1">(2)</ref> an efficient sampling method should avoid false "hardest" negative samples, i.e., samples that are most similar and originate from the same class as the anchor, and (3) an ideal set of negative examples should represent the whole population <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b3">4]</ref>. A selection mechanism should ideally capture all three properties: anchor similarity, model confidence and representativeness. Methods that only consider similarity with the anchor when sampling negative examples, i.e., assuming that higher similarity aligns with higher importance, tend to select same-class negatives (Figure <ref type="figure" target="#fig_0">1</ref> To address the aforementioned limitations, this paper introduces Uncertainty and Representativeness Mixing (UnReMix) for contrastive training, a method that combines importance scores that capture model uncertainty, representativeness, and anchor similarity. Specifically, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(c), UnReMix utilizes uncertainty to penalize false hard negatives and pairwise distance among negatives to select representative examples. To the best of our knowledge, we are the first to consider representativeness for hard negative sampling in contrastive learning in a computationally inexpensive way. We verify our method on several visual, text and graph benchmark datasets and perform comparisons over strong contrastive baselines. Experimental and qualitative results demonstrate the effectiveness of our proposed approach.</p><p>Contributions: The contributions of our work are summarized as follows:</p><p>(1) We delve into an empirical analysis of the efficacy of hard negative sampling strategies and featurebased importance sampling methods, observing that incorporating representativeness improves downstream performance.</p><p>(2) Based on our observations, we introduce an efficient method to calculate the representativeness of negative examples based on pairwise similarity and propose UnReMix, a flexible and efficient hard negative sampling method for contrastive learning that selects hard informative negatives based on anchor similarity, model confidence and representativeness.</p><p>(3) We verify the effectiveness of the proposed method and show that UnReMix improves downstream task performance and negative selection quality on several benchmarks in 3 domains (image, language and graph). Qualitative analysis shows that the proposed method encourages sampling a diverse set of negatives, resulting in better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Contrastive Learning: Recent work has largely contributed in developing contrastive selfsupervised learning methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b7">8]</ref>, that have produced promising results in a variety of domains, from learning unsupervised cross-modal representations and video representations <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref>, to natural language processing <ref type="bibr" target="#b0">[1]</ref> and graph learning <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b41">42]</ref>, and often achieving comparable results to supervised counterparts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. Chen et al. <ref type="bibr" target="#b6">[7]</ref> introduced SimCLR, a method that utilizes the examples of the current batch as negative samples. He et al. <ref type="bibr" target="#b18">[19]</ref> and Chen et al. <ref type="bibr" target="#b8">[9]</ref> introduced MoCo, a contrastive approach that incorporates a dynamic dictionary alongside with momentum update, and showed that larger dictionary sizes improve the accuracy on downstream tasks. He et al. <ref type="bibr" target="#b18">[19]</ref> and its later improvements <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> updated the dictionary by enqueueing the current batch and dequeuing the oldest batch. As a result, and according to Ma et al. <ref type="bibr" target="#b36">[37]</ref>, the dictionary will contain "biased keys", i.e., keys of the same class. Moreover, Saunshi et al. <ref type="bibr" target="#b43">[44]</ref> showed that increasing the dictionary size beyond a certain threshold may hamper the performance on downstream tasks. For audiovisual representations, Ma et al. <ref type="bibr" target="#b36">[37]</ref> proposed CM-ACC, where an actively sampled cross-modal dictionary look-up (to include keys of a wide range) is utilized. Cross-modal methods often cannot surpass strong baselines, e.g., MoCo <ref type="bibr" target="#b8">[9]</ref>, on unimodal datasets, because positive examples from a single modality (images) are less informative than cross-modal examples (video-audio pair), which can exchange auxiliary supervision signals between modalities.</p><p>In summary, research efforts can be largely divided into improvements over the contrastive loss calculation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27]</ref> and improvements over the hard positive/negative selection mechanisms <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7]</ref>. Our proposed UnReMix method falls under the latter category and improves over feature-based contrastive importance sampling methods. We compare UnReMix with state-of-the-art hard negative sampling techniques, briefly described below.</p><p>Hard Negative Sampling: Selection strategies for mining high-quality negative instances in contrastive learning have attracted substantial research interest, resulting in a wide range of contrastive methods proposed, e.g., i-Mix <ref type="bibr" target="#b31">[32]</ref>, HCL <ref type="bibr" target="#b42">[43]</ref>, Mochi <ref type="bibr" target="#b22">[23]</ref>, AdCo <ref type="bibr" target="#b20">[21]</ref>, etc. In particular, AdCo <ref type="bibr" target="#b20">[21]</ref> maintains a separate global set for negative examples that is updated actively using the contrastive loss gradients with respect to each negative example. However, the set of negative examples remains the same for all the anchors. Shah et al. <ref type="bibr" target="#b44">[45]</ref> formulated the contrastive loss function as an SVM objective and utilized the support vectors as hard negatives, resorting to approximations to solve a computationally expensive quadratic equation for each anchor. Cai et al. <ref type="bibr" target="#b3">[4]</ref> performed an empirical study on the importance of negative samples in MoCo-V2 and found that only the hardest 5% of the negatives are sufficient and necessary for achieving high accuracy and that same-class negatives can have detrimental effects on the representation learning process. Robinson et al. <ref type="bibr" target="#b42">[43]</ref> proposed HCL, a method that calculates the importance of each negative example by considering feature similarity w.r.t. the anchor, and achieved improvements over MoCo-V2 and SimCLR. Kalantidis et al. <ref type="bibr" target="#b22">[23]</ref> synthesized negatives samples by mixing hard negatives (most similar negatives to the query) and achieved approximately 1% improvement over MoCo-V2. Motivated by Mixup <ref type="bibr" target="#b56">[57]</ref>, a few methods create synthetic examples either by interpolating instances at an image/pixel or latent representation level <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">60]</ref>, or by interpolating virtual labels <ref type="bibr" target="#b31">[32]</ref>. Ge et al. <ref type="bibr" target="#b14">[15]</ref> design negative examples using texture-based and patch-based non-semantic augmentation techniques. Xiong et al. <ref type="bibr" target="#b51">[52]</ref> propose an asynchronously-updated approximate nearest neighbor index for selecting negatives in text domains.</p><p>HCL <ref type="bibr" target="#b42">[43]</ref> assigned more importance to negative examples that are similar to the query. Similarly, Huynh et al. <ref type="bibr" target="#b21">[22]</ref> relies on an anchor similarity threshold for filtering false negatives. However, considering only feature similarity results in assigning more importance to the same-class negatives, i.e., most likely false negatives which are detrimental to the representation learning process <ref type="bibr" target="#b3">[4]</ref>. In contrast, Ma et al. <ref type="bibr" target="#b36">[37]</ref> selects negative examples with high model uncertainty. In active learning, Ash et al. <ref type="bibr" target="#b1">[2]</ref> utilized the gradients of the loss function w.r.t. the model's most confident prediction as an approximation of uncertainty and theoretically showed that the gradient norm of the last layer of a neural network w.r.t. the predicted label provides a lower bound on gradient norms induced by any other label. Ma et al. <ref type="bibr" target="#b36">[37]</ref> utilized this measure to actively sample uncertain negatives when composing a memory bank. Our approach differs in that we leverage the gradients of the last layer as a model-based uncertainty measure so that our sampling method can assign more importance to the samples closer to the decision boundary, but also incorporate both anchor similarity and representativeness to capture other equally useful properties. Overall, and to the best of our knowledge, none of the prior contrastive learning works jointly considers model confidence, anchor similarity and representativeness, let alone analyze the importance of interpolation among such components. To this end, we propose UnReMix, a simple and efficient method that benefits from all aforementioned components when computing importance weights for negative examples. Our experimental analysis shows that UnReMix improves downstream performance and diversifies the negative example set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Problem Formulation: Given an unlabeled dataset X, we wish to learn an encoding function ? : X ? R ? that maps a data point ? ? ? X to a ?-dimensional embedding space, such that embeddings of similar instances (? ? , ? ? ) lie closer to each other, and vice versa. For a random subset (batch) of N positive pairs</p><formula xml:id="formula_0">X ? = {( x? , x? )} ? ?=1</formula><p>, where x? , x? are two augmented views of example ? ? , the contrastive loss for learning the encoder ? is defined as</p><formula xml:id="formula_1">L ?? = -log exp ?( x? , x? )/? exp ?( x? , x? )/? + x ??? ?X? exp ?( x? , x ? )/? ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">?(? ? , ? ? ) = ? (? ? ) ? (? ? )/ ? (? ? ) ? (? ? )</formula><p>is the inner product of the normalized latent representations, and ? is a temperature scaling hyperparameter. Here, x? is referred as the positive sample for x? and x ??? ? X ? are the remaining instances, that are considered negative samples.</p><p>The set of negative examples is typically selected by random sampling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. Recent works have individually proposed various "hard" negative mining or generation techniques, e.g., based on perturbations in the input space <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b46">47]</ref>, feature-based importance weights <ref type="bibr" target="#b42">[43]</ref> or uncertainty-based sampling <ref type="bibr" target="#b36">[37]</ref>. Yet, these methods consider only one selection indicator and hence achieve suboptimal performance in learning contrastive representations. In this work, we propose a sampling technique, termed UnReMix, that jointly considers both model-based uncertainty and representativeness to select negative examples. Below, we describe how UnReMix captures the necessary properties for mining informative negative samples.</p><p>UnReMix Description: We wish to select high-quality informative hard negative examples that exhibit the following properties:</p><p>P1: The ground-truth label of the selected negative example is different from the anchor label. We refer to P1 as the TRUENEGATIVE property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P2:</head><p>Hard negative examples resemble the anchor example, i.e., the feature representations of the hardest negative examples lie close to the anchor in the embedding space. We refer to P2 as ANCHOR VICINITY property. P3: Informative negative examples are representative of the sample population. In other words, semantically similar but not identical representative negative examples should be sufficient for contrastive training <ref type="bibr" target="#b3">[4]</ref>. We refer to P3 as the REPRESENTATIVENESS property.</p><p>In summary, UnReMix selects hard negative samples based on calculated importance scores. The higher the score is, the more informative the sample is assumed to be. The lack of access to ground-truth label information makes it impossible to maintain P1 (TRUENEGATIVE) completely. The challenge, therefore, lies in measuring the informativeness of negative samples without label information. Model uncertainty measures the degree of confidence of a model in its prediction i.e., high model uncertainty corresponds to lower model confidence, and neural models typically assign higher uncertainty to examples closer to the decision boundary <ref type="bibr" target="#b34">[35]</ref>.</p><p>We use this property to assign higher importance to negative examples, such that negative examples that are closer to the anchor but far from the decision boundary will have lower importance than negatives lying closer to the decision boundary.</p><p>Inspired by the use of similar information-theoretic metrics in metric learning <ref type="bibr" target="#b13">[14]</ref>, out-of-distribution detection <ref type="bibr" target="#b37">[38]</ref>, and reinforcement learning <ref type="bibr" target="#b57">[58]</ref>, we consider a gradient-based uncertainty metric.</p><p>In particular, pseudo-labeling (as an implicit method for entropy minimization) and gradient-based uncertainty (where a smaller gradient norm corresponds to higher model confidence) are established in semi-supervised and active learning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2]</ref>. In addition, gradient-based uncertainty comes with theoretical justifications for our chosen type of pseudo-labels <ref type="bibr" target="#b1">[2]</ref>.</p><p>More formally, we first define a pseudo-label space induced by the data distribution. We denote the most confident prediction for a negative example ? ??? ? X ? as its pseudo-label ? ? , and utilize the gradient of the last encoder layer w.r.t. this pseudo-label. Specifically, we calculate the pseudoposterior of the negative example ? ? via</p><formula xml:id="formula_3">? ? ? x ? , X ? = exp ?( x? , x ? ) x? ? ? ?X ? exp ?( x? , x ? ) ,<label>(2)</label></formula><p>where ?(? ? , ? ? ) = ? (? ? ) ? (? ? )/ ? (? ? ) ? (? ? ) is the inner product of the normalized representations of ? ? and ? ? , respectively. Equation (2) calculates the posterior as the similarity of a negative ? ? and all other examples ? ? ? X ? , considering them as individual anchors. We denote the most confident prediction as ? ? = arg max ? ? ? ? x ? , X ? ? , where [?] ? corresponds to the class index, and calculate the gradient of the cross-entropy loss via</p><formula xml:id="formula_4">? ? ? = ? ?? ???? ? ?? ? ? ? x ? , X ? , ? ? ?=? ? ,<label>(3)</label></formula><p>where ? ?? is the cross-entropy loss function and ? ???? is the parameter vector of the last layer of encoder ? . Intuitively, the gradient ? ? ? measures the model change caused by the negative example ? ? . The more uncertain the model is about its prediction for a particular sample, the higher the update of the model parameters. Similarly, we compute ? ? ? for a specific anchor ? ? . Finally, the uncertainty score of an example ? ? with respect to anchor ? ? is defined as</p><formula xml:id="formula_5">?( x ? , x? ) = ? ? ? ? ? ? .</formula><p>To incorporate P2 (ANCHOR VICINITY) in selecting hard negatives, we utilize instance similarity in the embedding space. Here, we use the inner product of the normalized vector representations as a similarity score for example ? ? with respect to anchor ? ? , i.e., ?(</p><formula xml:id="formula_6">x ? , x? ) = ? ( x? ) ? ( x ? )/ ? ( x? ) ? ( x ? ) .</formula><p>This means that the more similar ? ? is to anchor ? ? , the higher the importance of ? ? is <ref type="bibr" target="#b42">[43]</ref> # Calculate pseudo-labels using Eq. ( <ref type="formula" target="#formula_3">2</ref>)</p><formula xml:id="formula_7">? ? ? = ? ? ???? ? ? ? ? ? ? x ? , X ? , ? ? ?=? ? # Calculate gradients using Eq. (3) ?( x ? , x? ) = ? ? ? ? ? ? # Calculate uncertainty score ? ( x ? , x? ) = 1 ? -2 ? ? =1 ? ?{?, ? } 1 -?( x ? ,</formula><p>x ? ) # Calculate representativeness score using Eq. ( <ref type="formula" target="#formula_9">4</ref>) end for update the model to minimize the loss end for the gradient vectors along with the feature representations in the calculation of the importance score for each negative sample, this specific uncertainty metric lessens the importance of the "false" negative samples. Even so, incorporating only uncertainty and anchor similarity does not allow for a negative sample set that is representative of the data population. The addition of an appropriate representativeness score aids in selecting informative hard representative negatives, maintaining P3 (REPRESENTATIVENESS). Recent works utilize clustering of feature representations for selecting representative negative examples <ref type="bibr" target="#b33">[34]</ref>. However, clustering after each update is computationally challenging and requires hyperparameters (number of clusters). To simplify the calculation of the representativeness score for a negative example, we instead compute its average distance from all other negative examples in the embedding space space. The representativeness score of an example ? ? given anchor ? ? is</p><formula xml:id="formula_8">?( x ? , x? ) = ? 1 ?( x ? , x? ) +? 2 ?( x ? , x? ) +? 3 ? ( x ? , x? ) # Compute importance (? 1 , ?</formula><formula xml:id="formula_9">? ( x ? , x? ) = 1 ? -2 ? ?? ? =1 ? ?{?, ? } 1 -? x ? , x ? .<label>(4)</label></formula><p>In our experiments, we observe that the proposed representativeness performs well and encourages sampling a diverse set of negatives. Finally, we define the importance score of a negative example as follows:</p><formula xml:id="formula_10">?( x ? , x? ) = ? ?( x ? , x? ), ?( x ? , x? ), ? ( x ? , x? ) . (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>Here ? is an aggregation function, e.g., linear interpolation or attention weights. In our experiments, we use the latter and model the weight of each component as a learned hyper-parameter. Moreover, we present ablation studies for a variation with fixed equal weights. Figure <ref type="figure" target="#fig_2">2</ref> presents an overview of UnReMix and Algorithm 1 provides the pseudocode for the calculation of importance scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate UnReMix on several benchmarks from various domains (image, graph and text data) and compare against state-of-the-art contrastive learning methods. The baseline set is the most representative w.r.t. hard negative selection or generation with competitive results over related work:</p><formula xml:id="formula_12">MoCo [9]</formula><p>, more specifically MoCo-V2, a general dictionary-based contrastive learning method, for which negative examples are randomly sampled and stored in the dictionary.  Mochi <ref type="bibr" target="#b22">[23]</ref>, built on top of MoCo, this method generates synthetic hard negatives by mixing negatives stored in the dictionary.</p><p>SwAV <ref type="bibr" target="#b4">[5]</ref>, an online contrastive learning algorithm that uses a swapped prediction mechanism for clustering assignments. i-Mix <ref type="bibr" target="#b31">[32]</ref>, a regularization strategy that mixes data in both input and prediction level. <ref type="bibr" target="#b14">[15]</ref>, a negative sample generation technique built upon MoCo-V2 that generates negative examples from the anchor using patch-based techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch-based NS</head><p>HCL <ref type="bibr" target="#b42">[43]</ref>, a hard negative selection strategy that improves negative selection upon SimCLR <ref type="bibr" target="#b6">[7]</ref> by computing importance scores based on feature representations. <ref type="bibr" target="#b46">[47]</ref>, a self-mixture strategy in the image space using Mixup <ref type="bibr" target="#b56">[57]</ref> and Cutmix <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Un-Mix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Representations</head><p>Linear Evaluation: We follow prior contrastive learning works and train a linear classifier on frozen feature representations acquired from pre-trained contrastive models, and evaluate performance on the CIFAR-10, CIFAR-100, and TINY-IMAGENET datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Figure <ref type="figure" target="#fig_3">3</ref> depicts the consistent improvement of UnReMix over all other baseline methods. Table <ref type="table" target="#tab_3">1</ref> presents the top-1% accuracy after fine-tuning the linear classifier for 100 epochs. Results are averaged over multiple trials, i.e., we report the mean and standard deviation over 10 independent trials, and green arrows indicate relative gains over the next best method. UnReMix outperforms the best baseline (Un-Mix) by 0.76% on CIFAR-10, 1.97% on CIFAR-100 and 1.01% on TINY-IMAGENET. As far as the rest of the baselines, UnReMix obtains on average an improvement of 2% on CIFAR-10, 3.25% on CIFAR-100 and 3.83% on TINY-IMAGENET.</p><p>Qualitative Analysis: We compare the sampled negatives of both HCL <ref type="bibr" target="#b42">[43]</ref> and UnReMix.  Importance score components: We perform an ablation analysis for each of the score components, i.e., uncertainty (?), similarity (?), and representativeness (?). We train different variations of UnReMix, with one component at a time. Table <ref type="table" target="#tab_4">2</ref> presents the top-1% accuracy of linear evaluation on the CIFAR-10, CIFAR-100 and TINY-IMAGENET datasets, respectively. Note that including only feature similarity is essentially similar to HCL <ref type="bibr" target="#b42">[43]</ref>. We notice that including uncertainty and representativeness separately outperform HCL. Moreover, adding all components (UnReMix) results in performance improvements across all datasets.</p><p>Aggregation Function: UnReMix combines uncertainty, similarity and representativeness in a linearly interpolated importance score, computed for each example via attention weights that are learned during training. Table <ref type="table" target="#tab_5">3</ref> presents the accuracy of linear evaluation for UnReMix and a variation with fixed equal weights. We observe that fixed equal weights perform comparatively well. We also present the evolution of weights for each UnReMix component in the supplementary material (Appendix B.2). We leave further analysis with more variations to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence Representations</head><p>We evaluate our model on learning sentence representations using the Quick-Thought (QT) vectors <ref type="bibr" target="#b35">[36]</ref>, following the same experimental setting as Logeswaran and Lee <ref type="bibr" target="#b35">[36]</ref>. Specifically, we train sentence embeddings using the BookCorpus dataset <ref type="bibr" target="#b25">[26]</ref> and evaluate the learned embeddings on six downstream tasks: semantic relatedness (SICK), product reviews (CR), subjectivity classification (SUBJ), opinion polarity (MPQA), question type classification (TREC), and paraphrase identification (MSRP). Results are reported in Table <ref type="table" target="#tab_6">4</ref>, with UnReMix outperforming baselines in all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph Representations</head><p>We also evaluate UnReMix on a graph representation task. Using the experimental settings of HCL <ref type="bibr" target="#b42">[43]</ref>, we utilize the InfoGraph method <ref type="bibr" target="#b47">[48]</ref> as the baseline and fine-tune an SVM readout  function on the learned embeddings using ? = 1 (HCL hyperparameter) for six graph datasets. Overall InfoGraph performs better in 3 out of the 6 benchmarks, and HCL has the best performance 2 out of 6 times. Our method works better or is comparable to InfoGraph and HCL in 2 benchmark datasets. We hypothesize that there is less variability in some of the graph datasets and thus the representative component is not utilized completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present UnReMix, a hard negative selection strategy that samples informative negative examples for contrastive training. We define the notion of "informativeness" by utilizing feature representations, model uncertainty and representativeness. As a measure of uncertainty, we extract the gradients of the loss function w.r.t. a computed pseudo-posterior for the negative examples. In addition, we utilize the average distance of each negative example from all other examples as a measure of representativeness. Feature representations from the last encoder layer are utilized in computing anchor similarity. Our approach interpolates these three indicators to determine the importance of negative samples.</p><p>Through experimental analysis on a variety of visual, sentence and graph downstream benchmarks, we showcase that our proposed approach, UnReMix, outperforms previous state-of-the-art contrastive learning methods, that either rely on random sampling for selecting negative samples, or on importance sampling calculated solely via feature similarity. In the future, we hope to evaluate our method in multi-modal large-scale benchmark datasets and extend UnReMix to prototypical and graph contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training and hyper-parameter details</head><p>Image representation: For all models, we adopt the training setup of HCL <ref type="bibr" target="#b42">[43]</ref>. More specifically, we use Resnet-50 <ref type="bibr" target="#b17">[18]</ref> as the base encoder, followed by a projection head or reducing the dimensionality of the feature representations from 2048 to 128. We train with Adam <ref type="bibr" target="#b24">[25]</ref>, 10 -3 learning rate and 10 -6 weight decay. While our method is implemented on top of Un-Mix, the underlying importance score computations are fairly general and can be easily incorporated into any contrastive learning method. We pre-train all models for 400 epochs with a batch size of 256. All experiments are performed on an NVIDIA T4 GPU with 16GB memory.</p><p>Sentence representation: Similar to Robinson et al. <ref type="bibr" target="#b42">[43]</ref>, we built upon the official experimental settings of quick-thoughts vectors (https://github.com/lajanugen/S2V). For each anchor sentence, the previous and next ? sentences (hyper-parameter) are considered positive examples, and all other examples are considered negative examples. Subsequently, Equation 1 is used as loss function to learn the model with Adam optimizer <ref type="bibr" target="#b24">[25]</ref>, batch size of 400, learning rate of 5 ? 10 -4 , and sequence length of 30. We use the default values for all other hyper-parameters and implement our importance calculation in s2v-model.py. Since the official BookCorpus dataset <ref type="bibr" target="#b25">[26]</ref> is not available, we use an unofficial version obtained from https://github.com/soskek/bookcorpus, and following instructions from Robinson et al. <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph representation:</head><p>We adopt the code of Robinson et al. <ref type="bibr" target="#b42">[43]</ref> (https://github.com/ joshr17/HCL/tree/main/graph) and incorporate our importance score calculation mechanism in gan_losses.py. We utilize all datasets downloaded from www.graphlearning.io. For a fair comparison to the original InfoGraph method and HCL <ref type="bibr" target="#b42">[43]</ref>, we train all the models using the same hyper-parameters values. Following Robinson et al. <ref type="bibr" target="#b42">[43]</ref> we use the GIN architecture <ref type="bibr" target="#b52">[53]</ref> with ? = 3 layers and embedding dimension ? = 32, trained for 200 epochs with 128 batch size, Adam optimizer, 10 -3 learning rate, and 10 -6 weight decay. Experiments in Figure <ref type="figure" target="#fig_5">5</ref> are reported over 10 experimental trials. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ablation Studies of Image Representations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Weight variation for different datasets</head><p>UnReMix combines uncertainty, similarity and representativeness in one importance score for each example via attention weights that are learned during training. Figure <ref type="figure" target="#fig_7">7</ref> presents the weights for each component as learning progresses. For both CIFAR-100 and TINY-IMAGENET weights converge after 100 epochs, assigning most importance to representativeness. However, for CIFAR-10, weights for similarity are higher than the weights for representativeness. In addition, uncertainty weights have converged to lower values across all three datasets. We could infer that the weights for each   We experiment with two types of loss functions for calculating gradients w.r.t. each negative example in Eq. ( <ref type="formula" target="#formula_4">3</ref>): 1) CROSS-ENTROPY (CE) loss and 2) NT-XENT, i.e., the Normalized Temperature-scaled Cross-Entropy loss in Eq. ( <ref type="formula" target="#formula_1">1</ref>). Table <ref type="table" target="#tab_8">5</ref> presents top-1% linear evaluation accuracy for UnReMix trained with both variants and Figure <ref type="figure" target="#fig_8">8</ref> presents the distribution of gradient values for negative examples. We notice that CE gradients exhibit more variance than NT-XENT gradients (Figure <ref type="figure" target="#fig_8">8</ref>), and that the gradient component based on CE results in higher performance than the NT-XENT variation (Table <ref type="table" target="#tab_8">5</ref>). This might allude to the usefulness of pseudo-labeling strategies for tasks with limited labels <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b55">56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Evaluation on IMAGENET-1K</head><p>We additionally evaluate UnReMix on IMAGENET-1K <ref type="bibr" target="#b12">[13]</ref> using MoCo-V2 <ref type="bibr" target="#b8">[9]</ref> as backbone. For this experiment, we make use of the recently released FFCV data loader <ref type="bibr" target="#b29">[30]</ref>. We directly use the MoCo-V2 official implementation. Due to resource constraints, and only for this experiment, we run all compared models for fewer epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Qualitative Evaluation</head><p>We present additional qualitative results for UnReMix and HCL <ref type="bibr" target="#b42">[43]</ref> for CIFAR-10, CIFAR-100 and TINY-IMAGENET.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of negative samples (red shapes) for an anchor (yellow triangle) and its positive pair (green triangle), selected by three negative sampling techniques. Gray areas represent three clusters with different semantic labels. (a) Random sampling results in easy negatives being selected (1 triangle, 2 plus, 1 pentagon). (b) Methods that only consider anchor similarity may sample negatives that lie close to the anchor but also belong to the same semantic class (3 triangles, 1 pentagon). In contrast, (c) UnReMix samples negatives that lie close to the decision boundary and are far from each other, i.e., representing the whole data population (2 plus, 2 pentagons).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b)), which can be detrimental to the representation learning [4]. Besides, negative examples that lie closer to the decision boundary are naturally the hardest examples, encapsulating rich information about different class categories. On the other hand, representative negative examples help to learn global representations of the data distribution. Consequently, we argue that considering representativeness, along with uncertainty and anchor similarity, is helpful when selecting negative examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the proposed hard negative sampling technique, UnReMix. Given an anchor ? ? and a set of negative samples X ? \ ? ? , UnReMix computes an importance score for each negative sample, by linearly interpolating between gradient-based uncertainty, anchor similarity and representativeness indicators, capturing desirable negative sample properties, i.e., samples that are truly negative (P1), in close vicinity to the anchor (P2) and representative of the sample population (P3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Linear evaluation learning curves, UnReMix (orange lines) surpasses all baselines across all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Qualitative evaluation on CIFAR-100. Best viewed in color. HCL [43] (top row) has sampled the anchor as a negative example (red bounding box). UnReMix (bottom row) samples more diverse true negative examples (green bounding boxes) and avoids false negative examples. (b) Individual contribution of each of the three importance score components, for the top-5 important negative examples selected by our method (bottom row of left figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Classification Accuracy for six benchmark datasets. Results reported are averaged over 5 independent runs, each run with 10-fold cross-validation. Methods with the best performance are highlighted with bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Weight (y-axis) of each component over 400 epochs (x-axis) for UnReMix (red lines) and a variant with fixed equal weights, UnReMix ? ? ??? (blue lines). Results on CIFAR-10, CIFAR-100 and TINY-IMAGENET, best viewed in color.</figDesc><graphic url="image-67.png" coords="14,252.07,250.16,120.77,80.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Distribution of gradients of negative examples using (a) Cross-Entropy Loss and (b) NT-Xent Loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figures 9 and 10 CIFAR- 10 :</head><label>1010</label><figDesc>depict the five most important negative examples sampled by HCL (top row) and UnReMix (bottom row), for eight anchor examples from the CIFAR-10 and CIFAR-100 datasets, respectively. Similarly, Figures11 and 12depict the seven most important negative examples sampled by HCL (top row) and UnReMix (bottom row), for each of the six anchor examples from the TINY-IMAGENET dataset, observing in total consistent qualitative improvements for more than<ref type="bibr" target="#b21">22</ref> anchor examples across all datasets. In Figure9(a) we can observe that HCL assigns most importance to two pairs of negative examples from the same class "Airplane" and "Truck" (orange bounding boxes (bboxes)) whereas UnReMix selects one example from those classes and additionally selects examples from diverse classes including "Deer" and "Automobile" (green bboxes). Similar scenarios are visible in Figure9(b), (d), (e), (f), (g), (h). Moreover, Figure9(c), (e), (f), (g) depicts that HCL assigns more importance to the negative example with the same class as the anchor, i.e., "Bird" (Figure9(c), (e)), "Truck" (Figure9(f)), "Frog" (Figure9(g)) (red bboxes). On the other hand, UnReMix avoids negative examples of the same class as the anchor by including gradient component in the importance calculation of the negative examples. However, UnReMix sometimes redundant negative examples can also appear in the most five important negatives selected by UnReMix because of the fewer number of ground-truth classes in CIFAR-10 dataset (Figure 9 (c), (d), (h)).CIFAR-100:In Figure10(a), (f), (h), we notice that the top five most important negative examples for HCL contain examples of the same class as the anchor i.e., "Ray", "Girl", "Possum" (red bboxes) whereas UnReMix assigns most importance to representative negative examples (green). Besides, in Figure10(b), (c), (d), (e), (g), we can see that HCL assigns most importance to redundant examples of the same image with different augmentations ((b), (c), (e), (g)) or examples of the same class ((d)) (orange bboxes). On the other hand, UnReMix avoids assigning high importance to different augmentations of the same example or examples of the same class.TINY-IMAGENET:The qualitative results also follow similar trends as in CIFAR-10 and CIFAR-100. For example, Figure11depicts that HCL selects redundant negative examples in the top seven most important negatives, i.e., different augmentations of the same image (orange bboxes). On the other hand, UnReMix selects only one of those images (e.g., class 186) and additionally selects examples from representative classes (green bboxes). Similar observations can be made in Figure11(c), Figure12. Moreover, in Figure11(b), it is noticeable that HCL assigns most importance to a negative example of the same label as the anchor (red). On the other hand, UnReMix does not assign much importance to that negative example and instead selects a representative set of examples as top seven negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Qualitative evaluations on CIFAR-10, best viewed in color. Top Row: HCL [43] has sampled negative examples of the same class as the anchor as a negative example (red bbox) or selected examples of the same class multiple times (orange bboxes). Bottom Row: UnReMix samples more diverse true negative examples and avoids false negative examples (green bboxes).</figDesc><graphic url="image-136.png" coords="17,109.95,535.27,247.21,87.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Qualitative evaluations on TINY-IMAGENET, best viewed in color. Top Row: HCL [43] has sampled negative examples of the same class as the anchor as a negative example (red bbox) or selected examples of the same class multiple times (orange bboxes). Bottom Row: UnReMix samples more diverse true negative examples and avoids false negative examples (green bboxes).</figDesc><graphic url="image-179.png" coords="19,124.46,517.94,356.75,119.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The importance scores consist of three components: (1) a model-based component that utilizes the loss gradients w.r.t. each negative sample as a measure of uncertainty and approximates P1 by assigning more weight to the negative examples that lie closer to the decision boundary, (2) a feature-based component that leverages the feature space geometry via instance similarity to select informative negative samples, that satisfy P2, and (3) a density-based component that assigns more weight to negatives examples that are more distant on average from other negative examples in the batch, and satisfies P3. We describe each component separately.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Pseudocode for UnReMix Input: Dataset X, batch size ?, encoder ? for batch X ? = {( x? , x? )} ? ?=1 ? X do loss := 0 for ?, ? = 1, . . . , ? and ? ? ? do # Acquire embeddings for positives and negatives, and calculate pairwise similarity scores ?( x ? , x? ) = ? ( x? ) ? ( x ? )/ ? ( x? ) ? ( x ? ) ? ? ? x ? , X ? =</figDesc><table /><note><p>. Note that this contradicts P1 by assigning more importance to negative examples with the same label as the anchor. Incorporating uncertainty via Equation (3) alleviates this issue. Namely, by including Algorithm 1 exp ? ( x? , x ? ) x? ? ? ?X ? exp ? ( x? , x ? )</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2 , ? 3 learnable) # Compute contrastive loss with importance weights for negatives loss += -log exp ?( x? , x? )/? exp ?( x? , x? )/? + x ??? ?X? ?( x ? , x? ) exp ?( x? , x ? )/?</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Top 1% accuracy comparison over baselines. Mean and standard deviation reported over 10 trials. Green arrows indicate relative gains over the next best method.</figDesc><table><row><cell></cell><cell cols="3">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>TINY-IMAGENET</cell></row><row><cell></cell><cell cols="4">SwAV [5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.90?0.02</cell><cell>43.60?0.01</cell><cell>29.00?0.10</cell></row><row><cell></cell><cell cols="4">i-Mix [32]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>79.26?0.18</cell><cell>41.58 ?0.25</cell><cell>24.10?0.02</cell></row><row><cell></cell><cell cols="4">MoCo [9]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87.88?0.18</cell><cell>59.96?0.20</cell><cell>40.76?0.40</cell></row><row><cell></cell><cell cols="9">Patch-Based NS [15]</cell><cell></cell><cell>87.86?0.06</cell><cell>60.24?0.11</cell><cell>40.91?0.06</cell></row><row><cell></cell><cell cols="5">Mochi [23]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87.33?0.12</cell><cell>60.83?0.06</cell><cell>42.11?0.18</cell></row><row><cell></cell><cell cols="4">HCL [43]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91.19?0.03</cell><cell>67.87?0.09</cell><cell>45.62?0.07</cell></row><row><cell></cell><cell cols="5">Un-Mix [47]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92.42?0.06</cell><cell>69.15?0.06</cell><cell>48.44?0.06</cell></row><row><cell></cell><cell cols="4">UnReMix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93.18?0.06 ?0.76</cell><cell>71.12?0.09 ?1.97</cell><cell>49.45?0.10 ?1.01</cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top 1% accuracy</cell><cell>80 85 75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HCL Mochi</cell><cell></cell><cell></cell><cell>MoCo i-Mix</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Patch-Based NS</cell><cell></cell><cell>Un-Mix</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UnReMix</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epochs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Top-1% accuracy of different UnReMix variations.</figDesc><table><row><cell>Components of ?</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>TINY-IMAGENET</cell></row><row><cell>Feature similarity (HCL)</cell><cell>91.19</cell><cell>67.86</cell><cell>45.61</cell></row><row><cell>Uncertainty</cell><cell>91.88</cell><cell>68.77</cell><cell>47.44</cell></row><row><cell>Representativeness</cell><cell>92.02</cell><cell>68.60</cell><cell>48.03</cell></row><row><cell>All (UnReMix)</cell><cell>91.99</cell><cell>69.43</cell><cell>48.12</cell></row></table><note><p>as weights. Figure 4(b) depicts the contribution of each component in calculating the overall importance score of the top five important negative examples selected by UnReMix. We can observe that the representativeness component contributes the most. Additional qualitative examples can be found in the Appendix.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Top-1% accuracy comparison over UnReMix variations with learned or fixed equal weights for each component.</figDesc><table><row><cell>Method</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>TINY-IMAGENET</cell></row><row><cell>UnReMix ? ? ???</cell><cell>91.95?0.06</cell><cell>69.27?0.13</cell><cell>47.29?0.14</cell></row><row><cell>UnReMix ???? ???</cell><cell>92.00?0.06</cell><cell>68.78?0.15</cell><cell>47.85?0.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy on CR, SUBJ, MPQA, TREC, MSRP downstream tasks and test Pearson Correlation for Semantic-Relatedness (SICK) task. Sentence representations are learned using Quick-Thought (QT) vectors on the BookCorpus dataset and evaluated on six classification tasks. Evaluation with 10-fold cross-validation for binary classification tasks (CR, SUBJ, MPQA) and over multiple trials for the remaining tasks (TREC, MSRP). UnReMix 74.7 ?6.97 78.0 ?10.5 86.8 ?6.9 78.782.8 ?16.8 70.7 ?2.7 80.9?0.76   </figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell cols="2">SICK</cell><cell></cell><cell>CR</cell><cell></cell><cell cols="2">SUBJ</cell><cell cols="2">MPQA TREC</cell><cell></cell><cell cols="2">MSRP</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(Acc)</cell><cell cols="2">(F1)</cell></row><row><cell></cell><cell>QT</cell><cell></cell><cell cols="2">67.7</cell><cell></cell><cell>67.5</cell><cell></cell><cell cols="2">79.9</cell><cell>80.3</cell><cell>66.0</cell><cell cols="2">68.0</cell><cell cols="2">80.1</cell></row><row><cell></cell><cell>HCL</cell><cell></cell><cell cols="2">60.6</cell><cell></cell><cell>62.7</cell><cell></cell><cell cols="2">74.1</cell><cell>79.3</cell><cell>58.6</cell><cell cols="2">68.3</cell><cell cols="2">79.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">InfoGraph</cell><cell></cell><cell></cell><cell cols="2">HCL(?=1.0)</cell><cell cols="3">UnReMix(?=1.0)</cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell cols="3">PROTEINS</cell><cell></cell><cell cols="2">PTC</cell><cell cols="3">IMDB-M</cell><cell>IMDB-B</cell><cell cols="3">ENGYMES</cell><cell cols="3">MUTAGS</cell></row><row><cell></cell><cell>74.1</cell><cell>73.3</cell><cell>74.1</cell><cell>56.1</cell><cell>55.3</cell><cell>58.6</cell><cell>50.4</cell><cell>49.8</cell><cell>48.5</cell><cell>72.7 71.7 72.8</cell><cell>50.6</cell><cell>51.5</cell><cell>48.4</cell><cell>87.1</cell><cell>86.3</cell><cell>86.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Average Accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Top-1% accuracy of UnReMix variations with gradients computed with (a) Cross-Entropy loss (CE) and (b) NT-Xent loss.</figDesc><table><row><cell>Method</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>TINY-IMAGENET</cell></row><row><cell>UnReMix ? ?</cell><cell>91.99</cell><cell>69.43</cell><cell>48.12</cell></row><row><cell>UnReMix ? ? -? ???</cell><cell>91.95</cell><cell>69.27</cell><cell>47.29</cell></row><row><cell cols="2">B.3 Loss function for gradient calculation</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Table 6 presents the top-1% and top-5% accuracy after training with contrastive loss for 50 epochs, and then fine-tuning for 70 epochs, with UnReMix outperforming MoCo-V2. Comparison of performance on IMAGENET-1KEven though gradient-based uncertainty reduces the importance of most similar negative examples for which the model has low uncertainty, it fails to reduce the importance of most similar negative examples with high uncertainty (closer to the decision boundary). However, qualitative analysis shows that the occurrence of such a situation is so infrequent that it barely affects model training.</figDesc><table><row><cell>Accuracy</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Work in progress.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence contrastive learning for text recognition</title>
		<author>
			<persName><forename type="first">Aviad</forename><surname>Aberdam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Tsiper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Slossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shai Mazor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep batch active learning by diverse, uncertain gradient lower bounds</title>
		<author>
			<persName><forename type="first">Chicheng</forename><surname>Jordan T Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mine your own view: Self-supervised learning through across-sample prediction</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Azabou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Heng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">C</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Bhaskaran-Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Dabagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila-Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsey</forename><surname>Kitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">B</forename><surname>Hengen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gray-Roncal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><forename type="middle">L</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Tiffany Tianhui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<title level="m">Are all negatives created equal in contrastive instance discrimination? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Debiased contrastive learning</title>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised metric learning with synthetic examples</title>
		<author>
			<persName><forename type="first">Mehrtash</forename><surname>Ujjal Kr Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sekhar</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Robust contrastive learning using negative samples with diminished semantics</title>
		<author>
			<persName><forename type="first">Shlok</forename><surname>Songwei Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Liang</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Jacobs</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of visual features in the wild</title>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries</title>
		<author>
			<persName><forename type="first">Qianjiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Boosting contrastive self-supervised learning with false negative cancellation</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Khademi</surname></persName>
		</author>
		<editor>WACV</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><surname>Larlus</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Sungnyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gihun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangmin</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Se-Young</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><surname>Mixco</surname></persName>
		</author>
		<title level="m">Mix-up contrastive learning for visual representation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Skip-thought vectors</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Russ R Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Fidler</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Revisiting contrastive learning through the lens of neighborhood component analysis: an integrated framework</title>
		<author>
			<persName><forename type="first">Ching-Yun</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeet</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lily</forename><surname>Weng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>Cifar-10/cifar100</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName><forename type="first">Ya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">FFCV: an optimized data pipeline for accelerating ML training</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="https://github.com/libffcv/ffcv/,2022" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning at ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A domain-agnostic strategy for contrastive representation learning</title>
		<author>
			<persName><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee. I-Mix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient self-supervised vision transformers for representation learning</title>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Ch</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple and principled uncertainty estimation with deterministic deep learning via distance awareness</title>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreyas</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tania</forename><forename type="middle">Bedrax</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7498" to="7512" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active contrastive learning of audio-visual video representations</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Open set recognition through deep neural network uncertainty: Does out-of-distribution detection require generative classifiers?</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Mundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iuliia</forename><surname>Pliushch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagnik</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Visvanathan</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Videomoco: Contrastive video representation learning with temporally adversarial examples</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<title level="m">Max-margin contrastive learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Un-mix: Rethinking image mixtures for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Un-mix: Rethinking image mixtures for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Contrastive domain adaptation</title>
		<author>
			<persName><forename type="first">Mamatha</forename><surname>Thota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Leontidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Uncertainty-based decision making using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Xujiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Hee</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 22th International Conference on Information Fusion (FUSION)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Eqco: Equivalent rules for self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Improving contrastive learning by visualizing feature transformation</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
