<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Facebook</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption by the larger community.</p><p>In this work, with an adequate training scheme, we produce a competitive convolution-free transformer by training on Imagenet only. We train it on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. We share our code and models to accelerate community advances on this line of research.</p><p>Additionally, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this tokenbased distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 84.4% accuracy) and when transferring to other tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks have been the main design paradigm for image understanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set, namely Imagenet <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>. Motivated by the success of attention-based models in Natural Language Processing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref>, there has been increasing interest in architectures leveraging attention mechanisms within convnets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b57">58]</ref>. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref>. DeiT-B⚗ is identical to VIT-B, but trained with a procedure (initialization, optimization, data-augmentation, regularization and distillation) more adapted to a data-starving regime. It can be learned in a few days on one machine. The symbol ⚗ refers to models trained with our transformer-specific distillation method. See Table <ref type="table">6</ref> for details and more models.</p><p>The visual transformer (ViT) introduced by Dosovitskiy et al. <ref type="bibr" target="#b14">[15]</ref> is an architecture directly inherited from Natural Language Processing <ref type="bibr" target="#b48">[49]</ref>, but applied to image classification with raw image patches as input. Their paper presents promising results with transformers trained with a large strongly supervised image dataset (JFT-300M <ref type="bibr" target="#b42">[43]</ref>, 300 millions images) that is not publicly available. The authors concluded that transformers "do not generalize well when trained on insufficient amounts of data", and used extensive computing resources to train their models. Both aspects limit the adoption of ViT and more generally of transformers, for researchers without access to such computing resources, or without a privileged access to a large private dataset.</p><p>In this paper, we show that none of this is required: we actually train a transformer on a single 8-GPU node in two to three days (53 hours of pretraining, and optionally 20 hours of fine-tuning). This vanilla transformer is competitive with convnets of a similar number of parameters and efficiency, using Imagenet as the sole training set, and does not include a single convolution. We build upon the visual transformer architecture from Dosovitskiy et al. <ref type="bibr" target="#b14">[15]</ref>, which is very close to the original token-based transformer architecture <ref type="bibr" target="#b48">[49]</ref> where word embeddings are replaced with patch embeddings. With our Data-efficient image Transformers (DeiT), we report large improvements over previous results, see Figure <ref type="figure">1</ref>. They mainly come from DeiT's better training strategy for visual transformers, at both the initial training and the finetuning stage. Our ablation study details the key ingredients for a successful training, and hopefully will serve as guidelines for future works.</p><p>We address another question: how to distill these models? We introduce a token-based strategy, specific to transformers and denoted by DeiT⚗, and show that it advantageously replaces the usual distillation.</p><p>In summary, our work makes the following contributions:</p><p>• We show that our neural networks that contains no convolutional layer can achieve competitive results against the state of the art on ImageNet with no external data. They are learned on a single node with 4 GPUs in three days <ref type="foot" target="#foot_0">1</ref> . Our two new models DeiT-S and DeiT-Ti have less parameters and have similar memory usage as ResNet-50 and ResNet-18.</p><p>• We introduce a new distillation procedure based on a distillation token, which plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the transformer through attention. We show that this transformer-specific strategy outperforms vanilla distillation by a significant margin.</p><p>• Interestingly, with our distillation, image transformers learn more from a convnet than from a another transformer with comparable performance.</p><p>• Our models pre-learned on Imagenet are competitive when transferred to different downstream tasks such as fine-grained classification, on several popular public benchmarks: CIFAR-10, CIFAR-100, Flowers, Stanford Cars and iNaturalist-18/19.</p><p>• We present our training scheme and will release our training code and models, in the hope that it will facilitate the adoption of visual transformers by a larger audience.</p><p>This paper is organized as follows: we review related works in Section 2, and focus on transformers for image classification in Section 3. We introduce our distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent transformers, as well as a comparative evaluation of our transformer-specific distillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the key ingredients involved in DeiT. We conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Image Classification is so core to computer vision that it is often used as a benchmark to measure progress in image understanding. Any progress usually translates to improvement in other related tasks such as detection or segmentation. Since 2012's AlexNet <ref type="bibr" target="#b29">[30]</ref>, convnets have dominated this benchmark and have become the de facto standard. The evolution of the state of the art on the ImageNet dataset <ref type="bibr" target="#b38">[39]</ref> reflects the progress with convolutional neural network architectures and learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Despite several attempts to use transformers for image classification <ref type="bibr" target="#b6">[7]</ref>, until now their performance has been inferior to that of convnets. Nevertheless hybrid architectures that combine convnets and transformers, including the self-attention mechanism, have recently exhibited competitive results in image classification <ref type="bibr" target="#b52">[53]</ref>, detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>, video processing <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b49">50]</ref>, unsupervised object discovery <ref type="bibr" target="#b32">[33]</ref>, and unified text-vision tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Recently Visual transformers (ViT) <ref type="bibr" target="#b14">[15]</ref> closed the gap with the state of the art on ImageNet, without using any convolution. This performance is remarkable since convnet methods for image classification have benefited from years of tuning and optimization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">48]</ref>. Nevertheless, according to this study <ref type="bibr" target="#b14">[15]</ref>, a pre-training phase on a large volume of curated data is required for the learned transformer to be effective. In our paper we contradict this conclusion by achieving a strong performance without requiring a large training dataset, i.e., with training on Imagenet only. We also show that it is possible to train transformers in a few days on a single machine, using a smaller batch size. We hope that our findings will drive the adoption of image transformers by a larger part of the community.</p><p>The Transformer architecture, introduced by Vaswani et al. <ref type="bibr" target="#b48">[49]</ref> for machine translation are currently the reference model for all natural language processing (NLP) tasks. Many improvements of convnets for image classification are inspired by transformers. For example, Squeeze and Excitation <ref type="bibr" target="#b1">[2]</ref>, Selective Kernel <ref type="bibr" target="#b31">[32]</ref> and Split-Attention Networks <ref type="bibr" target="#b57">[58]</ref> exploit mechanism akin to transformers self-attention (SA) mechanism.</p><p>Knowledge Distillation (KD), introduced by Hinton et al. <ref type="bibr" target="#b21">[22]</ref>, refers to the training paradigm in which a student model leverages "soft" labels coming from a strong teacher network. This is the output vector of the teacher's softmax function rather than just the maximum of scores, wich gives a "hard" label. Such a training improves the performance of the student model (alternatively, it can be regarded as a form of compression of the teacher model into a smaller one -the student). On the one hand the teacher's soft labels will have a similar effect to labels smoothing <ref type="bibr" target="#b54">[55]</ref>. On the other hand as shown by Wei et al. <ref type="bibr" target="#b50">[51]</ref> the teacher's supervision takes into account the effects of the data augmentation, which sometimes causes a misalignment between the real label and the image. For example, let us consider image with a "cat" label that represents a large landscape and a small cat in a corner. If the cat is no longer on the crop of the data augmentation it implicitly changes the label of the image. KD can transfer inductive biases <ref type="bibr" target="#b0">[1]</ref> in a soft way in a student model using a teacher model where they would be incorporated in a hard way. For example, it may be useful to induce biases due to convolutions in a transformer model by using a convolutional model as teacher. In our paper we study the distillation of a transformer student by either a convnet or a transformer teacher. We introduce a new distillation procedure specific to transformers and show its superiority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual transformer: overview</head><p>In this section, we briefly recall preliminaries associated with the visual transformer <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b48">49]</ref>, and further discuss positional encoding and resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Self Attention layers (MSA).</head><p>The attention mechanism is based on a trainable associative memory with (key, value) vector pairs. A query vector q ∈ R d is matched against a set of k key vectors (packed together into a matrix K ∈ R k×d ) using inner products. These inner products are then scaled and normalized with a softmax function to obtain k weights. The output of the attention is the weighted sum of a set of k value vectors (packed into V ∈ R k×d ). For a sequence of N query vectors (packed into Q ∈ R N ×d ), it produces an output matrix (of size N × d):</p><formula xml:id="formula_0">Attention(Q, K, V ) = Softmax(QK / √ d)V,<label>(1)</label></formula><p>where the Softmax function is applied over each row of the input matrix and the √ d term provides appropriate normalization. In <ref type="bibr" target="#b48">[49]</ref>, a Self-attention layer is proposed. Query, key and values matrices are themselves computed from a sequence of N input vectors (packed into</p><formula xml:id="formula_1">X ∈ R N ×D ): Q = XW Q , K = XW K , V = XW V , using linear transformations W Q , W K , W V with the constraint k = N ,</formula><p>meaning that the attention is in between all the input vectors. Finally, Multi-head self-attention layer (MSA) is defined by considering h attention "heads", ie h self-attention functions applied to the input. Each head provides a sequence of size N × d. These h sequences are rearranged into a N × dh sequence that is reprojected by a linear layer into N × D.</p><p>Transformer block for images. To get a full transformer block as in <ref type="bibr" target="#b48">[49]</ref>, we add a Feed-Forward Network (FFN) on top of the MSA layer. This FFN is composed of two linear layers separated by a GeLu activation <ref type="bibr" target="#b20">[21]</ref>. The first linear layer expands the dimension from D to 4D, and the second layer reduces the dimension from 4D back to D. Both MSA and FFN are operating as residual operators thank to skip-connections, and with a layer normalization <ref type="bibr" target="#b2">[3]</ref>.</p><p>In order to get a transformer to process images, our work builds upon the ViT model <ref type="bibr" target="#b14">[15]</ref>. It is a simple and elegant architecture that processes input images as if they were a sequence of input tokens. The fixed-size input RGB image is decomposed into a batch of N patches of a fixed size of 16 × 16 pixels (N = 14 × 14). Each patch is projected with a linear layer that conserves its overall dimension 3 × 16 × 16 = 768. The transformer block described above is invariant to the order of the patch embeddings, and thus does not consider their relative position. The positional information is incorporated as fixed <ref type="bibr" target="#b48">[49]</ref> or trainable <ref type="bibr" target="#b15">[16]</ref> positional embeddings. They are added before the first transformer block to the patch tokens, which are then fed to the stack of transformer blocks.</p><p>The class token is a trainable vector, appended to the patch tokens before the first layer, that goes through the transformer layers, and is then projected with a linear layer to predict the class. This class token is inherited from NLP <ref type="bibr" target="#b13">[14]</ref>, and departs from the typical pooling layers used in computer vision to predict the class. The transformer thus process batches of (N + 1) tokens of dimension D, of which only the class vector is used to predict the output. This architecture forces the self-attention to spread information between the patch tokens and the class token: at training time the supervision signal comes only from the class embedding, while the patch tokens are the model's only variable input.</p><p>Fixing the positional encoding across resolutions. Touvron et al. <ref type="bibr" target="#b46">[47]</ref> show that it is desirable to use a lower training resolution and fine-tune the network at the larger resolution. This speeds up the full training and improves the accuracy under prevailing data augmentation schemes. When increasing the resolution of an input image, we keep the patch size the same, therefore the number N of input patches does change. Due to the architecture of transformer blocks and the class token, the model and classifier do not need to be modified to process more tokens. In contrast, one needs to adapt the positional embeddings, because there are N of them, one for each patch. Dosovitskiy et al. <ref type="bibr" target="#b14">[15]</ref> interpolate the positional encoding when changing the resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distillation through attention</head><p>In this section we assume we have access to a strong image classifier as a teacher model. It could be a convnet, or a mixture of classifiers. We address the question of how to learn a transformer by exploiting this teacher. As we will see in Section 5 by comparing the trade-off between accuracy and image throughput, it can be beneficial to replace a convolutional neural network by a transformer. This section covers two axes of distillation: hard distillation versus soft distillation, and classical distillation versus the distillation token.</p><p>Soft distillation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">51]</ref> minimizes the Kullback-Leibler divergence between the softmax of the teacher and the softmax of the student model. This distillation token is employed in a similar fashion as the class token, except that on output of the network its objective is to reproduce the (hard) label predicted by the teacher, instead of true label. Both the class and distillation tokens input to the transformers are learned by back-propagation.</p><p>Let Z t be the logits of the teacher model, Z s the logits of the student model. We denote by τ the temperature for the distillation, λ the coefficient balancing the Kullback-Leibler divergence loss (KL) and the cross-entropy (L CE ) on ground truth labels y, and ψ the softmax function. The distillation objective is</p><formula xml:id="formula_2">L global = (1 − λ)L CE (ψ(Z s ), y) + λτ 2 KL(ψ(Z s /τ ), ψ(Z t /τ )).<label>(2)</label></formula><p>Hard-label distillation. We consider a variant of distillation where we take the hard decision of the teacher as a true label. This method has the advantage of being parameter-free. Let y t = argmax c Z t (c) be the hard decision of the teacher, the objective associated with this hard-label distillation is:</p><formula xml:id="formula_3">L hardDistill global = 1 2 L CE (ψ(Z s ), y) + 1 2 L CE (ψ(Z s ), y t ).<label>(3)</label></formula><p>Note that, for a given image, the hard label associated with the teacher may change depending on the specific data augmentation. We will see that, by itself, this choice is competitive with the traditional one, while being conceptually simpler as the teacher label y t plays the same role as the true label y.</p><p>Note also that the hard labels can also be converted into soft labels with label smoothing <ref type="bibr" target="#b43">[44]</ref>, where the true label is considered to have a probability of 1 − ε, and the remaining ε is shared across the remaining classes. We fix this parameter to ε = 0.1 in our all experiments that use true labels.</p><p>Distillation token. We now focus on our proposal, which is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. We add a new token, the distillation token, to the initial embeddings (patches and class token). Our distillation token is used similarly as the class token: it interacts with other embeddings through self-attention, and is output by the network after the last layer. Its target objective is given by the distillation component of the loss. The distillation embedding allows our model to learn from the output of the teacher, as in a regular distillation, while remaining complementary to the class embedding.</p><p>Interestingly, we observe that the learned class and distillation tokens converge towards different vectors: the average cosine similarity between these tokens equal to 0.06. As the class and distillation embeddings are computed at each layer, they gradually become more similar through the network, all the way through the last layer at which their similarity is high (cos=0.93), but still lower than 1. This is expected since as they aim at producing targets that are similar but not identical.</p><p>We verified that our distillation token adds something to the model, compared to simply adding an additional class token associated with the same target label: instead of a teacher pseudo-label, we experimented with a transformer with two class tokens. Even if we initialize them randomly and independently, during training they converge towards the same vector (cos=0.999), and the output embedding are also quasi-identical. This additional class token does not bring anything to the classification performance. In contrast, our distillation strategy provides a significant improvement over a vanilla distillation baseline, as validated by our experiments in Section 5.2.</p><p>Classification with our approach: joint classifiers. At test time, both the class or the distillation embeddings produced by the transformer are associated with linear classifiers and able to infer the image label. It is also possible to add the softmax output by the two classifiers to estimate it in a late fusion fashion. We evaluate these three options in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section presents a few analytical experiments and results. We first discuss our distillation strategy. Then we comparatively analyze the efficiency and accuracy of convnets and visual transformers.</p><p>Table <ref type="table">1</ref>: Variants of our DeiT architecture. The larger model, DeiT-B, has the same architecture as the ViT-B <ref type="bibr" target="#b14">[15]</ref>. The only parameters that vary across models are the embedding dimension and the number of heads, and we keep the dimension per head constant (equal to 64). Smaller models have a lower parameter count, and a faster throughput. The throughput is measured for images at resolution 224×224. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Transformer models</head><p>As mentioned earlier, our architecture design is identical to the one proposed by Dosovitskiy et al. <ref type="bibr" target="#b14">[15]</ref> with no convolutions. Our only differences are the training strategies, and the distillation token. To avoid any confusion between models trained with our procedure, we refer to the results obtained in the prior work by ViT, and prefix ours by DeiT. If not specified, DeiT refers to our referent model DeiT-B, which has the same architecture as ViT-B. When we fine-tune DeiT at a larger resolution, we append the resulting operating resolution at the end, e.g, DeiT-B↑384. Last, when using our distillation procedure, we identify it with an alembic sign as DeiT⚗.</p><p>The parameters of ViT-B (and therefore of DeiT-B) are fixed as D = 768, h = 12 and d = D/h = 64. We introduce two smaller models, namely DeiT-S and DeiT-Ti, for which we change the number of heads, keeping d fixed. Table <ref type="table">1</ref> summarizes the models that we consider in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Distillation</head><p>Convnets teachers. We have observed that using a convnet teacher gives better performance than using a transformer. Table <ref type="table">2</ref> compares distillation results with different teacher architectures. The fact that the convnet is a better teacher is probably due to the inductive bias inherited by the transformers through distillation, as explained in Abnar et al. <ref type="bibr" target="#b0">[1]</ref>. In all of our subsequent distillation experiments the default teacher is a RegNetY-16GF <ref type="bibr" target="#b36">[37]</ref> (84M parameters) that we trained with the same data augmentation as DeiT. This teacher reaches 82.9% top-1 accuracy on ImageNet.</p><p>Comparison of distillation methods. We compare the performance of different distillation strategies in Table <ref type="table">3</ref>. Hard distillation significantly outperforms soft distillation for transformers, even when using only a class token: hard distillation reaches 83.0%, compared to the soft distillation accuracy of 81.8%. Our distillation strategy from Section 4 further improves the performance, showing Table <ref type="table">2</ref>: We compare on ImageNet <ref type="bibr" target="#b38">[39]</ref> the performance (top-1 acc., %) of the student as a function of the teacher model used for distillation. that the two tokens provide complementary information useful for classification: the classifier on the two tokens is significantly better than the independent class and distillation classifiers, which by themselves already outperform the distillation baseline. We observe that the distillation token gives slightly better results than the class token. It is also more correlated to the convnets prediction. This difference in performance is probably due to the fact that it benefits more from the inductive bias of convnets. We give more details and an analysis in the next paragraph. The distillation token has an undeniable advantage for the initial training. We observe that fine-tuning at a higher resolution tends to reduce the differences between the methods. This is probably due to the fact that, while fine-tuning, we do not use the teacher information. The correlation between the class token and the distillation tokens slightly increases with the fine-tuning, which may reflect a loss of the specificity of each token.</p><p>In summary, our method produces a visual transformer that becomes on par with the best convnets in terms of the trade-off between accuracy and throughput, see Table <ref type="table">6</ref>. Interestingly, the distilled model outperforms its teacher in terms of the trade-off between accuracy and throughput. This demonstrates the interest of our distillation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agreement with the teacher &amp; inductive bias?</head><p>As discussed above, the architecture of the teacher has an important impact. Does it inherit existing inductive bias that would facilitate the training? While we believe it difficult to formally answer this question, we analyze in Table <ref type="table" target="#tab_2">4</ref> the decision agreement between the convnet teacher, our image transformer DeiT learned from labels only, and our transformer DeiT⚗.</p><p>We observe that our distilled model is more correlated to the convnet than a transformer learned from scratch. As to be expected, the classifier associated with the distillation embedding is closer to the convnet that the one associated with the class embedding, and conversely the one associated with the class embedding is more similar to DeiT learned without distillation. Unsurprisingly, the joint classifier class+distil offers a middle ground.</p><p>Table <ref type="table">3</ref>: Distillation experiments on Imagenet with DeiT-B. We report the results for our new distillation method in the last three rows. We separately report the performance when classifying with only one of the class or distillation embeddings, and then with a classifier taking both of them as input. In the last row (class+distillation), the result "pretrain" correspond to the late fusion of the class and distillation classifiers, while "finetune" means that we fine-tune a classifier on the concatenation of the class and distillation embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of epochs.</head><p>We observe that increasing the number of epochs significantly improves the performance of training with distillation. With 300 epochs, our distilled network DeiT-B⚗ is already better than DeiT-B. But for the latter the performance saturates with longer schedules, while our distilled network continues to improve significantly and clearly benefit from a longer training time, see Table <ref type="table" target="#tab_3">5</ref> where we report our results with a varying number of epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Efficiency vs accuracy: a comparative study with convnets</head><p>In the literature, the image classificaton methods are often compared as a compromise between accuracy and another criterion, such as FLOPs, number of parameters, size of the network, etc. We focus in Figure <ref type="figure">1</ref> on the tradeoff between the throughput (images pro- cessed per second) and the top-1 classification accuracy on ImageNet. We focus on the popular state-of-the-art EfficientNet convnet, which has benefited from years of research on convnets and was optimized by architecture search on the ImageNet validation set. Our method DeiT is slightly below EfficientNet, which shows that we have almost closed the gap between visual transformers and convnets when training with Imagenet only. These results are a major improvement (+6.3% top-1 in a comparable setting) over previous ViT models trained on Imagenet1k only <ref type="bibr" target="#b14">[15]</ref>. Furthermore, when DeiT benefits from the distillation from a relatively weaker RegNetY to produce DeiT⚗, it outperforms EfficientNet. Table <ref type="table">6</ref> reports the numerical results in more details and additional evaluations on ImageNet V2 and ImageNet Real, that have a test set distinct from the ImageNet validation, which reduces overfitting on the validation set. Compared to EfficientNet, one can see that, for the same number of parameters, the convnet variants are much slower. This is because large matrix multiplications offer more opportunity for hardware optimization than small convolutions. On ImageNet Real and V2, EfficientNet-B4 has about the same speed as DeiT⚗, and their accuracies are on par.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Transfer learning: Performance on downstream tasks</head><p>Although DeiT perform very well on ImageNet it is important to evaluate them on other datasets with transfer learning in order to measure the power of generalization of DeiT. We evaluated this on transfer learning tasks by fine-tuning on the datasets in Table <ref type="table" target="#tab_5">7</ref>. Table <ref type="table" target="#tab_6">8</ref> compares DeiT transfer learning results to those of ViT <ref type="bibr" target="#b14">[15]</ref> and state of the art convolutional architectures <ref type="bibr" target="#b44">[45]</ref>. We observe that DeiT results are equivalent as those of the best convnets, which is in line with our previous conclusion on ImageNet. Table <ref type="table">6</ref>: Throughput on and accuracy on Imagenet <ref type="bibr" target="#b38">[39]</ref>, Imagenet Real <ref type="bibr" target="#b4">[5]</ref> and Imagenet V2 matched frequency <ref type="bibr" target="#b37">[38]</ref> of DeiT and of several state-of-the-art convnets, for models trained with no external data. The throughput is measured as the number of images that we can process per second on one 16GB V100 GPU. For each model we take the largest possible batch size for the usual resolution of the model and calculate the average time over 30 runs to process that batch. With that we calculate the number of images processed per second. Throughput can vary according to the implementation: for a direct comparison and in order to be as fair as possible, we use for each model the definition in the same GitHub <ref type="bibr" target="#b51">[52]</ref> repository. We do not include models for which the training procedure is not available or has not been reproduced. : Regnet optimized with a similar optimization procedure as ours, which boosts the results. These networks serve as teachers when we use our distillation strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Training details &amp; ablation</head><p>In this section we discuss the ingredients of the DeiT training strategy to learn visual transformers in a data-efficient manner. We provide hyper-parameters as well as an ablation study in which we analyze the impact of each choice. This study is intended to be transformer analogous of the "bag of tricks" for convnets proposed by He et al. <ref type="bibr" target="#b19">[20]</ref>.</p><p>Initialization and hyper-parameters. Transformers are relatively sensitive to initialization. After testing several options in preliminary experiments, some of them not converging, we follow the recommendation of Hanin and Rolnick <ref type="bibr" target="#b17">[18]</ref> to initialize the weights with a truncated normal distribution. Table <ref type="table" target="#tab_7">10</ref> indicates the hyper-parameters that we use by default at training time for all our experiments, unless stated otherwise. For distillation we follow the recommendations from Cho et al. <ref type="bibr" target="#b8">[9]</ref> to select the parameters τ and λ. Data-Augmentation. Compared to models that integrate more priors (such as convolutions), transformers require a larger amount of data. Thus, in order to train with datasets of the same size, we rely on extensive data augmentation. of transformers, especially deep ones. Regularization like Mixup <ref type="bibr" target="#b56">[57]</ref> and Cutmix <ref type="bibr" target="#b55">[56]</ref> improve performance. We also use repeated augmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>, which provides a significant boost in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exponential Moving Average (EMA).</head><p>We evaluate the EMA of our network obtained after training. There are small gains, which vanish after fine-tuning: the EMA model has an edge of is 0.1 accuracy points, but when fine-tuned the two models reach the same (improved) performance.</p><p>Fine-tuning at different resolution. We adopt the fine-tuning procedure from Touvron et al. <ref type="bibr" target="#b47">[48]</ref>: our schedule, regularization and optimization procedure are identical to that of FixEfficientNet but we keep the training time dataaugmentation (contrary to the dampened data augmentation of Touvron et al. <ref type="bibr" target="#b47">[48]</ref>). We also interpolate the positional embeddings: In principle any classical image scaling technique, like bilinear interpolation, could be used. However, a bilinear interpolation of a vector from its neighbors reduces its 2 -norm compared to its neighbors. These low-norm vectors are not adapted to the pre-trained transformers and we observe a significant drop in accuracy if we employ use directly without any form of fine-tuning. Therefore we adopt a bicubic interpolation that approximately preserves the norm of the vectors, before fine-tuning the network with either AdamW <ref type="bibr" target="#b33">[34]</ref> or SGD. These optimizers have a similar performance for the fine-tuning stage, see Table <ref type="table">9</ref>. By default and similar to ViT <ref type="bibr" target="#b14">[15]</ref> we train DeiT models at resolution 224 × 224 and we fine-tune at resolution 384×384. We detail how to do this effectively this interpolation in Section 3. However, in order to measure the influence of the resolution we have finetuned DeiT at different resolutions. We report these results in Table <ref type="table" target="#tab_8">11</ref>.</p><p>image throughput Imagenet <ref type="bibr" target="#b38">[39]</ref> Real <ref type="bibr" target="#b4">[5]</ref> V2 <ref type="bibr" target="#b37">[38]</ref>  Not having to rely on batch-norm allows us to reduce the batch size without impacting performance, which makes it easier to train larger models. Note that, since we use repeated augmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref> with 3 repetitions, we only see one third of the images during a single epoch<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have introduced DeiT, which are image transformers that do not require very large amount of data to be trained, thanks to improved training and distillation procedure. Convolutional neural networks have optimized, both in terms of architecture and optimization during almost a decade, including through extensive architecture search that is prone to overfiting, as it is the case for instance for EfficientNets <ref type="bibr" target="#b47">[48]</ref>. In contrast, for DeiT we have only optimized the existing data augmentation and regularization strategies pre-existing for convnets, not introducing any significant architectural beyond our novel distillation token. Therefore it is possible that research on dataaugmentation more adapted or learned for transformers will bring further gains.</p><p>Therefore, considering our results, where image transformers are on par with convnets already, we believe that they will rapidly become a method of choice considering their lower memory footprint for a given accuracy.</p><p>We provide an open-source implementation of our method. It is available at https://github.com/facebookresearch/deit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 arXivFigure 1 :</head><label>11</label><figDesc>Figure1: Throughput and accuracy on Imagenet of our methods compared to several state-of-the-art convnets, trained on Imagenet1k only. The throughput is measured as the number of images processed per second on a V100 GPU. DeiT-B⚗ is identical to VIT-B, but trained with a procedure (initialization, optimization, data-augmentation, regularization and distillation) more adapted to a data-starving regime. It can be learned in a few days on one machine. The symbol</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our distillation procedure: we simply include a new distillation token.It interacts with the class and patch tokens through the self-attention layers. This distillation token is employed in a similar fashion as the class token, except that on output of the network its objective is to reproduce the (hard) label predicted by the teacher, instead of true label. Both the class and distillation tokens input to the transformers are learned by back-propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Disagreement analysis between convnet, image transformers and distillated transformers: We report the fraction of sample classified differently for all classifier pairs, i.e., the rate of different decisions. We include two models without distillation (a RegNetY and DeiT), so that we can compare how our distilled models and classification heads are correlated to these teachers.</figDesc><table><row><cell></cell><cell cols="2">Supervision</cell><cell>Test tokens</cell><cell cols="2">ImageNet top-1 (%)</cell></row><row><cell>method ↓</cell><cell cols="5">label teacher class distil. pretrain finetune 384</cell></row><row><cell>DeiT-no distillation</cell><cell></cell><cell></cell><cell></cell><cell>81.8</cell><cell>83.1</cell></row><row><cell>DeiT-usual distillation</cell><cell></cell><cell>soft</cell><cell></cell><cell>81.8</cell><cell>83.1</cell></row><row><cell>DeiT-hard distillation</cell><cell></cell><cell>hard</cell><cell></cell><cell>83.0</cell><cell>84.0</cell></row><row><cell>DeiT⚗: class embedding</cell><cell></cell><cell>hard</cell><cell></cell><cell>83.0</cell><cell>84.1</cell></row><row><cell>DeiT⚗: distil. embedding</cell><cell></cell><cell>hard</cell><cell></cell><cell>83.1</cell><cell>84.2</cell></row><row><cell>DeiT⚗: class+distillation</cell><cell></cell><cell>hard</cell><cell></cell><cell>83.4</cell><cell>84.2</cell></row><row><cell cols="2">groundtruth</cell><cell cols="4">no distillation convnet DeiT class distillation DeiT⚗ student (of the convnet) DeiT⚗</cell></row><row><cell>groundtruth</cell><cell>0.000</cell><cell>0.171</cell><cell>0.182 0.170</cell><cell>0.169</cell><cell>0.166</cell></row><row><cell>convnet (RegNetY)</cell><cell>0.171</cell><cell>0.000</cell><cell>0.133 0.112</cell><cell>0.100</cell><cell>0.102</cell></row><row><cell>DeiT</cell><cell>0.182</cell><cell>0.133</cell><cell>0.000 0.109</cell><cell>0.110</cell><cell>0.107</cell></row><row><cell>DeiT⚗-class only</cell><cell>0.170</cell><cell>0.112</cell><cell>0.109 0.000</cell><cell>0.050</cell><cell>0.033</cell></row><row><cell>DeiT⚗-distil. only</cell><cell>0.169</cell><cell>0.100</cell><cell>0.110 0.050</cell><cell>0.000</cell><cell>0.019</cell></row><row><cell>DeiT⚗-class+distil.</cell><cell>0.166</cell><cell>0.102</cell><cell>0.107 0.033</cell><cell>0.019</cell><cell>0.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Distillation experiments on ImageNet<ref type="bibr" target="#b38">[39]</ref> with DeiT-B with distillation tokens and hard distillation with various number of epochs.</figDesc><table><row><cell>method ↓</cell><cell>Test tokens class distil.</cell><cell>Epochs</cell><cell cols="2">ImageNet top-1 (%) train @ 224 finetune↑384(%)</cell></row><row><cell>DeiT hard distillation</cell><cell></cell><cell>300</cell><cell>83.0</cell><cell>84.0</cell></row><row><cell>class embedding</cell><cell></cell><cell></cell><cell>83.0</cell><cell>84.1</cell></row><row><cell>distillation embedding</cell><cell></cell><cell>300</cell><cell>83.1</cell><cell>84.2</cell></row><row><cell>class+distillation</cell><cell></cell><cell></cell><cell>83.4</cell><cell>84.2</cell></row><row><cell>DeiT hard distillation</cell><cell></cell><cell>500</cell><cell>83.8</cell><cell>84.5</cell></row><row><cell>class embedding</cell><cell></cell><cell></cell><cell>83.6</cell><cell>84.3</cell></row><row><cell>distillation embedding</cell><cell></cell><cell>500</cell><cell>83.9</cell><cell>84.4</cell></row><row><cell>class+distillation</cell><cell></cell><cell></cell><cell>84.1</cell><cell>84.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Datasets used for our different tasks.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Train size Test size #classes</cell></row><row><cell>ImageNet [39]</cell><cell>1,281,167</cell><cell>50,000</cell><cell>1000</cell></row><row><cell>iNaturalist 2018 [24]</cell><cell>437,513</cell><cell>24,426</cell><cell>8,142</cell></row><row><cell>iNaturalist 2019 [25]</cell><cell>265,240</cell><cell>3,003</cell><cell>1,010</cell></row><row><cell>Flowers-102 [36]</cell><cell>2,040</cell><cell>6,149</cell><cell>102</cell></row><row><cell>Stanford Cars [28]</cell><cell>8,144</cell><cell>8,041</cell><cell>196</cell></row><row><cell>CIFAR-100 [29]</cell><cell>50,000</cell><cell>10,000</cell><cell>100</cell></row><row><cell>CIFAR-10 [29]</cell><cell>50,000</cell><cell>10,000</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>We compare Transformers based models on different transfer learning task with ImageNet pre-training. We also report results with convolutional architectures for reference.</figDesc><table><row><cell>Model</cell><cell cols="8">ImageNet CIFAR-10 CIFAR-100 Flowers Cars iNat-18 iNat-19 im/sec</cell></row><row><cell>Grafit ResNet-50 [46]</cell><cell>79.6</cell><cell></cell><cell></cell><cell>98.2</cell><cell>92.5</cell><cell>69.8</cell><cell>75.9</cell><cell>579.2</cell></row><row><cell>Grafit RegNetY-8GF [46]</cell><cell></cell><cell></cell><cell></cell><cell>99.0</cell><cell>94.0</cell><cell>76.8</cell><cell>80.0</cell><cell>334.8</cell></row><row><cell>ResNet-152 [10]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.1</cell><cell></cell><cell>263.8</cell></row><row><cell>EfficientNet-B7 [45]</cell><cell>84.3</cell><cell>98.9</cell><cell>91.7</cell><cell>98.8</cell><cell>94.7</cell><cell></cell><cell></cell><cell>27.5</cell></row><row><cell>ViT-B/32 [15]</cell><cell>73.4</cell><cell>97.8</cell><cell>86.3</cell><cell>85.4</cell><cell></cell><cell></cell><cell></cell><cell>209.5</cell></row><row><cell>ViT-B/16 [15]</cell><cell>77.9</cell><cell>98.1</cell><cell>87.1</cell><cell>89.5</cell><cell></cell><cell></cell><cell></cell><cell>53.8</cell></row><row><cell>ViT-L/32 [15]</cell><cell>71.2</cell><cell>97.9</cell><cell>87.1</cell><cell>86.4</cell><cell></cell><cell></cell><cell></cell><cell>75.6</cell></row><row><cell>ViT-L/16 [15]</cell><cell>76.5</cell><cell>97.9</cell><cell>86.4</cell><cell>89.7</cell><cell></cell><cell></cell><cell></cell><cell>17.3</cell></row><row><cell>DeiT-B</cell><cell>81.8</cell><cell>99.1</cell><cell>90.8</cell><cell>98.4</cell><cell>92.1</cell><cell>73.2</cell><cell>77.7</cell><cell>182.7</cell></row><row><cell>DeiT-B↑384</cell><cell>83.1</cell><cell>99.1</cell><cell>90.8</cell><cell>98.5</cell><cell>93.3</cell><cell>79.5</cell><cell>81.4</cell><cell>53.8</cell></row><row><cell>DeiT-B⚗</cell><cell>83.9</cell><cell>99.1</cell><cell>91.3</cell><cell>98.8</cell><cell>92.9</cell><cell>73.4</cell><cell>78.4</cell><cell>182.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Default training-time hyperparameters used for all the methods.</figDesc><table><row><cell>Batch size</cell><cell>1024</cell></row><row><cell>Base learning rate</cell><cell>0.0005</cell></row><row><cell>Learning rate decay</cell><cell>cosine</cell></row><row><cell>Weight decay</cell><cell>0.05</cell></row><row><cell>Label smoothing ε</cell><cell>0.1</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row><row><cell>Warmup epochs</cell><cell>5</cell></row><row><cell>Rand Augment</cell><cell>9/0.5</cell></row><row><cell>Mixup prob.</cell><cell>0.8</cell></row><row><cell>Cutmix prob.</cell><cell>1.0</cell></row><row><cell>Erasing prob.</cell><cell>0.25</cell></row><row><cell>Distillation soft weight λ</cell><cell>0.1</cell></row><row><cell>Distillation soft temperature τ</cell><cell>3.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 :</head><label>11</label><figDesc>We compare performance of DeiT train at resolution 224 2 with different finetuning resolutions on ImageNet<ref type="bibr" target="#b38">[39]</ref>, ImageNet-Real<ref type="bibr" target="#b4">[5]</ref> and ImageNet-v2 matched frequency<ref type="bibr" target="#b37">[38]</ref>.Training time. A typical training of 300 epochs takes 37 hours with 2 nodes or 53 hours on a single node for the DeiT-B.As a comparison point, a similar training with a RegNetY-16GF<ref type="bibr" target="#b36">[37]</ref> (84M parameters) is 20% slower. DeiT-S and DeiT-Ti are trained in less than 3 days on 4 GPU. Then, optionally we finetune the model at a larger resolution. This takes 20 hours on a single node (8 GPU) to produce a FixDeiT-B model at resolution 384×384, which corresponds to 25 epochs.</figDesc><table><row><cell cols="2">size (image/s)</cell><cell>acc. top-1</cell><cell cols="2">acc. top-1 acc. top-1</cell></row><row><cell>160 2</cell><cell>374.6</cell><cell>79.9</cell><cell>84.8</cell><cell>67.6</cell></row><row><cell>224 2</cell><cell>182.7</cell><cell>81.8</cell><cell>86.7</cell><cell>71.5</cell></row><row><cell>320 2</cell><cell>83.2</cell><cell>82.7</cell><cell>87.2</cell><cell>71.9</cell></row><row><cell>384 2</cell><cell>53.8</cell><cell>83.1</cell><cell>87.7</cell><cell>72.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We can accelerate the learning of the larger model DeiT-B by training it on 8 GPUs in two days.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Formally it means that we have 100 epochs, but each is</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">3x longer because of the repeated augmentations. We prefer to refer to this as 300 epochs in order to have a direct comparison on the effective training time with and without repeated augmentation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Vinicius Reis and Mannat Singh for exploring a first implementation of image transformers and the insights they gathered at this occasion. Thanks to Ari Morcos, Mark Tygert, Gabriel Synnaeve, and others colleagues at Facebook for brainstorming this axis. Thanks to Ross Girshick and Piotr Dollar for constructive comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">9</ref>: Ablation study on training methods on ImageNet <ref type="bibr" target="#b38">[39]</ref>. The top row ("none") corresponds to our default configuration employed for DeiT. The symbols and indicates that we use and do not use the corresponding method, respectively. We report the accuracy scores (%) after the initial training at resolution 224×224, and after fine-tuning at resolution 384×384. The hyper-parameters are fixed according to Table <ref type="table">10</ref>, and may be suboptimal.</p><p>* indicates that the model did not train well, possibly because hyper-parameters are not adapted.</p><p>We evaluate different types of strong data augmentation, with the objective to reach a data-efficient training regime. Auto-Augment <ref type="bibr" target="#b10">[11]</ref> and even more Rand-Augment <ref type="bibr" target="#b11">[12]</ref> improve the performance. We also used random erasing <ref type="bibr" target="#b58">[59]</ref> in the image domain. Overall our experiments confirm that transformers require a strong data augmentation: almost all the data-augmentation methods that we evaluate prove to be useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization &amp; Optimizers.</head><p>We have considered different optimizers and cross-validated different learning rates and weight decays. Transformers are sensitive to the setting of optimization hyper-parameters. Therefore, during cross-validation, we tried 4 different learning rates (5.10 −4 , 3.10 −4 , 5.10 −5 ) and 3 weight decay (0.03, 0.04, 0.05). We scale the learning rate according to the batch size with the formula: lr scaled = lr 512 × batchsize, similarly to Goyal et al. <ref type="bibr" target="#b16">[17]</ref> except that we use 512 instead of 256 as the base value.</p><p>The best results use the AdamW optimizer with the same learning rates as ViT <ref type="bibr" target="#b14">[15]</ref> but with a much smaller weight decay, as the weight decay reported in the paper hurts the convergence in our setting.</p><p>We have employed stochastic depth <ref type="bibr" target="#b26">[27]</ref>, which facilitates the convergence</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Transferring inductive biases through knowledge distillation</title>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00555</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andli</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet?</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<title level="m">On the efficacy of knowledge distillation. International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03673</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How to start training: The effect of initialization and architecture</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niv</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The inaturalist challenge 2018 dataset</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The inaturalist challenge 2019 dataset</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>CIFAR</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visu-alBERT: a simple and performant baseline for vision and language</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Selective kernel networks. Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Objectcentric learning with slot attention</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15055</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Fixing weight decay regularization in adam</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
				<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ternational journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Global self-attention networks for image recognition</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Hui</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03019</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Grafit: Learning fine-grained image representations with coarse labels</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237</idno>
		<title level="m">Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrepancy: Fixefficientnet</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Circumventing outliers of autoaugment with knowledge distillation</title>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>European Conference on Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<biblScope unit="page" from="2020" to="2032" />
		</imprint>
	</monogr>
	<note>Pytorch models</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Selftraining with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Revisit knowledge distillation: a teacher-free framework</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><surname>Cutmix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04899</idno>
		<title level="m">Regularization strategy to train strong classifiers with localizable features</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Resnest: Split-attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
