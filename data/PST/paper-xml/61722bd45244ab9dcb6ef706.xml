<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-cost Proxies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-20">20 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Shen</surname></persName>
							<email>shenyu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Zheng</surname></persName>
							<email>zhengjian2322@buaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
							<email>wentao.zhang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Yao</surname></persName>
							<email>yaopeng@kuaishou.com</email>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jixiang</forename><surname>Li</surname></persName>
							<email>lijixiang@kuaishou.com</email>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
							<email>senyang@kuaishou.com</email>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
							<email>jiliu@kwai.com</email>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
							<email>bin.cui@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-cost Proxies</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-20">20 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.10423v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing neural architectures requires immense manual efforts. This has promoted the development of neural architecture search (NAS) to automate this design. While previous NAS methods achieve promising results but run slowly and zero-cost proxies run extremely fast but are less promising, recent work considers utilizing zero-cost proxies via a simple warm-up. The existing method has two limitations, which are unforeseeable reliability and one-shot usage. To address the limitations, we present ProxyBO, an efficient Bayesian optimization framework that utilizes the zero-cost proxies to accelerate neural architecture search. We propose the generalization ability measurement to estimate the fitness of proxies on the task during each iteration and then combine BO with zero-cost proxies via dynamic influence combination. Extensive empirical studies show that ProxyBO consistently outperforms competitive baselines on five tasks from three public benchmarks. Concretely, ProxyBO achieves up to 5.41× and 3.83× speedups over the state-of-the-art approach REA and BRP-NAS, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Discovering state-of-the-art neural architectures <ref type="bibr" target="#b6">(He et al. 2016;</ref><ref type="bibr" target="#b7">Huang et al. 2017)</ref> requires substantial efforts of human experts. The manual design is often costly and becomes increasingly expensive when networks grow larger. Recently, the neural network community has witnessed the development of neural architecture search (NAS) <ref type="bibr" target="#b31">(Zoph et al. 2018;</ref><ref type="bibr" target="#b28">Real et al. 2019;</ref><ref type="bibr" target="#b14">Liu, Simonyan, and Yang 2018;</ref><ref type="bibr" target="#b2">Cai et al. 2018;</ref><ref type="bibr" target="#b5">Dudziak et al. 2020)</ref>, which turns the design of architectures into an optimization problem without human interaction and achieves promising results in a wide range of fields, such as image classification <ref type="bibr" target="#b31">(Zoph et al. 2018;</ref><ref type="bibr" target="#b28">Real et al. 2019)</ref>, sequence modeling <ref type="bibr" target="#b20">(Pham et al. 2018;</ref><ref type="bibr" target="#b23">So, Le, and Liang 2019)</ref>, etc.</p><p>Bayesian optimization (BO) <ref type="bibr" target="#b8">(Hutter, Hoos, and Leyton-Brown 2011;</ref><ref type="bibr" target="#b11">Li et al. 2021a</ref>) has emerged as a state-of-theart method for NAS <ref type="bibr" target="#b28">(Ying et al. 2019;</ref><ref type="bibr">White, Neiswanger, and Savani 2021;</ref><ref type="bibr" target="#b22">Siems et al. 2020)</ref>. It trains a predictor, namely surrogate, on observations and selects the next architecture to evaluate based on its predictions. Recent work differs in the choice of surrogate, including Bayesian neural networks <ref type="bibr" target="#b24">(Springenberg et al. 2016)</ref>, Graph neural networks <ref type="bibr" target="#b16">(Ma, Cui, and Yang 2019)</ref>, etc. Despite the promising converged results, they share the common drawback that training a well-performed surrogate requires a sufficient number of evaluations, which often take days to obtain. While several approaches attempt to reduce the evaluation cost via weight sharing <ref type="bibr" target="#b20">(Pham et al. 2018)</ref> or gradient decent <ref type="bibr" target="#b14">(Liu, Simonyan, and Yang 2018)</ref>, recent work <ref type="bibr" target="#b10">(Lee, Ajanthan, and Torr 2018;</ref><ref type="bibr" target="#b25">Tanaka et al. 2020;</ref><ref type="bibr" target="#b18">Mellor et al. 2021)</ref> proposes several zero-cost proxies to estimate the performance of architecture at initialization using only a few seconds instead of network training. Though they achieve less promising results than BO-based methods, the computation of zero-cost proxies is speedy. Then, there comes up a question: "Can we speed up neural architecture search by combining the advantages of both Bayesian optimization and zero-cost proxies, i.e., achieving promising results with fewer computationally expensive evaluations?" Opportunities. As shown in Figure <ref type="figure" target="#fig_0">1</ref>(a), the Spearman correlation coefficient between the proxy jacob cov and test accuracy on NAS-Bench-201 CIFAR-10 is 0.742. Since the coefficient is a relatively large positive value, the proxy can be applied to rank architectures and guide the selection of the next architecture to evaluate. Challenges. First, utilizing zero-cost proxies is nontrivial. Rather than applying Bayesian optimization, recent work <ref type="bibr" target="#b0">(Abdelfattah et al. 2020</ref>) attempts to perform a simple warm-up on binary relation predictors based on a specific proxy. However, the proxies are not fully utilized via warmup and may even lead to negative effects due to the two lim-itations: (a) Unforeseeable reliability. The warm-up method chooses to apply the best proxy found by exhaustively evaluating thousands of architectures over a specific search space. However, in practice, the correlation between proxy scores and objective values is unknown before searching. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, although the proxy jacob cov works well on NAS-Bench-201, it performs worse than ranking randomly on NAS-Bench-101. In this case, using this proxy may lead to negative effects; (b) One-shot usage. The warmup method applies the proxies only once before searching by pre-training the neural binary predictor based on proxy scores. As a result, the influence of zero-cost proxies in the warm-up method is irreversible, making it difficult to identify and get rid of those potentially bad proxies during searching. Due to the limitations, how to unleash the potential of zero-cost proxies is still an open question.</p><p>In addition, combining BO and zero-cost proxies is also non-trivial. As a learning model, the BO surrogate generalizes better with more evaluation results while the ranking ability of proxies is constant, which is only determined by the current task. In other words, zero-cost proxies bring benefits when few evaluations are given, but they become less helpful when the BO surrogate becomes accurate with sufficient evaluations. Therefore, the importance of the proxies should be decreased during optimization, and a dynamic design should be made to match this trend.</p><p>In this paper, we propose ProxyBO, an efficient Bayesian optimization (BO) framework that utilizes the zero-cost proxies to significantly accelerate neural architecture search. To address the aforementioned challenges, during each iteration in the search process, we first estimate the fitness of different proxies and then combine BO with the proxies based on their dynamic influence. Empirical studies on three public benchmarks showcase the superiority of ProxyBO compared with state-of-the-art evaluation-based methods. Concretely, ProxyBO achieves up to 5.41× and 3.83× speedups over the state-of-the-art approach REA and BRP-NAS, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Designing neural architectures manually is often a challenging and time-consuming task since it is quite difficult for human experts to choose the proper operations and place each connection. As a result, this arouses great interest from both academia and industry to design neural architectures in an automatic manner. Inspired by successful hand-crafted architectures, pioneering work <ref type="bibr" target="#b31">(Zoph et al. 2018</ref>) manually designs a fixed macro-structure, which is composed of stacks of cells (micro-structure). The search is then conducted over those cells instead of the whole architecture via different search strategies, including reinforcement learning <ref type="bibr" target="#b31">(Zoph et al. 2018;</ref><ref type="bibr" target="#b20">Pham et al. 2018</ref><ref type="bibr">), evolutionary algorithm (Real et al. 2019)</ref>, auto-encoders <ref type="bibr" target="#b15">(Luo et al. 2018)</ref>, gradient decent <ref type="bibr" target="#b14">(Liu, Simonyan, and Yang 2018)</ref>, binary relation predictor <ref type="bibr" target="#b5">(Dudziak et al. 2020)</ref>, etc.</p><p>Among various approaches proposed for neural architecture search, recent researches <ref type="bibr">(White, Neiswanger, and Savani 2021;</ref><ref type="bibr" target="#b28">Ying et al. 2019;</ref><ref type="bibr" target="#b22">Siems et al. 2020</ref>) have shown the competitive performance of Bayesian optimization (BO) with a performance predictor. The original BO <ref type="bibr" target="#b23">(Snoek, Larochelle, and Adams 2012)</ref> is proposed for solving blackbox optimization, in which the output can only be obtained by an objective function, and no extra information like derivatives is available. As evaluating the validation performance of a given neural architecture is also a black-box process, BO can be directly applied to search for neural architectures. Benchmark studies <ref type="bibr" target="#b28">(Ying et al. 2019;</ref><ref type="bibr" target="#b22">Siems et al. 2020</ref>) point out that SMAC <ref type="bibr" target="#b8">(Hutter, Hoos, and Leyton-Brown 2011)</ref>, a classical BO approach, achieves state-of-the-art performance given enough budgets. BANANAS (White, Neiswanger, and Savani 2021) digs deeper into the BO framework and compares each part of the framework via extensive experiments. Other work further improves BO by combining the characteristics of neural architectures, e.g., NASBOT <ref type="bibr" target="#b9">(Kandasamy et al. 2018</ref>) defines a pseudo-distance for kernel functions while GPWL <ref type="bibr" target="#b21">(Ru et al. 2020</ref>) adopts the Weisfeiler-Lehman kernel. However, the above methods share the same drawback that a sufficient number of evaluations are required to guide the BO framework. As the evaluation of neural architectures is timeconsuming, the cost of the search algorithm is exorbitant.</p><p>To reduce NAS search costs, several techniques have been applied in the literature. ENAS <ref type="bibr" target="#b20">(Pham et al. 2018)</ref> applies the weight sharing strategy by allowing multiple architectures to share weights in the same operation. PNAS <ref type="bibr">(Liu et al. 2018)</ref> proposes to search for the best architecture starting from a smaller subspace. DARTS <ref type="bibr" target="#b14">(Liu, Simonyan, and Yang 2018)</ref> models NAS as training an over-parameterized architecture including all candidate paths. EcoNAS <ref type="bibr" target="#b30">(Zhou et al. 2020)</ref> investigates proxies with reduced resources during evaluation, e.g., fewer epochs, training samples, etc.</p><p>Recent studies further accelerate NAS by estimating the performance of architectures at initialization, which we refer to as zero-cost proxies. For example, synaptic saliency metrics <ref type="bibr" target="#b10">(Lee, Ajanthan, and Torr 2018;</ref><ref type="bibr" target="#b26">Wang, Zhang, and Grosse 2019;</ref><ref type="bibr" target="#b25">Tanaka et al. 2020</ref>) measures the loss change when removing a certain parameter. Fisher <ref type="bibr" target="#b26">(Theis et al. 2018)</ref> estimates the loss change when removing activation channels. NASWOT <ref type="bibr" target="#b18">(Mellor et al. 2021</ref>) and TE-NAS (Chen, Gong, and Wang 2020) model the expressivity of architectures based on activation patterns. As proxy scores are somehow related to the ground-truth performance, a recent research <ref type="bibr" target="#b0">(Abdelfattah et al. 2020)</ref> warms up a neural binary relation predictor using a specific proxy before training and achieves acceleration on benchmarks. However, due to the aforementioned limitations, how to unleash the potential of zero-cost proxies is still an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary</head><p>As stated above, neural architecture search (NAS) can be modeled as a black-box optimization problem. The goal is to solve argmin x∈X f obj (x) over an architecture search space X , where f obj (x) is the objective performance metric (e.g., classification error on the validation set) corresponding to the architecture configuration x. In the following, we first introduce the framework of Bayesian optimization and then the zero-cost proxies used in our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Optimization</head><p>To solve black-box optimization problems with expensive evaluation costs, Bayesian optimization (BO) follows the framework of sequential model-based optimization. A typical BO iteration loops over the following three steps: 1) BO fits a probabilistic surrogate model M based on the observations D = {(x 1 , y 1 ), ..., (x n−1 , y n−1 )}, in which x i is the configuration evaluated in the i-th iteration and y i is its corresponding observed performance; 2) BO uses the surrogate M to select the most promising configuration x n by maximizing x n = arg max x∈X a(x; M ), where a(x; M ) is the acquisition function designed to balance the trade-off between exploration and exploitation; 3) BO evaluates the configuration x n to obtain y n (i.e., train the architecture and obtain its validation performance), and augment the observations</p><formula xml:id="formula_0">D = D ∪ (x n , y n ).</formula><p>We adopt the Probabilistic Random Forest <ref type="bibr" target="#b8">(Hutter, Hoos, and Leyton-Brown 2011)</ref> as the surrogate model and the Expected Improvement (EI) (Jones, Schonlau, and Welch 1998) as the acquisition function. Both components are widely used in the AutoML community for their satisfactory empirical performance <ref type="bibr" target="#b5">(Feurer et al. 2015;</ref><ref type="bibr" target="#b5">Eggensperger et al. 2013)</ref>. Concretely, the Expected Improvement is defined as follows:</p><formula xml:id="formula_1">a(x; M ) = ∞ −∞ max(y best − y, 0)p M (y|x)dy, (1)</formula><p>where p M (y|x) is the conditional probability of y given x under the surrogate model M , and y best is the best observed performance in observations D, i.e., y best = min{y 1 , ..., y n }. Note that, EI only takes the improvement over the best performance into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-cost Proxy</head><p>Different from the low-cost proxies (Zhou et al. 2020) which require less training resources, zero-cost proxies are a type of proxies that can be computed at initialization. The initial design goal of zero-cost proxies is to better direct exploration in existing NAS algorithms without the expensive training costs. In the following, we describe three metrics with their properties used in our proposed method.</p><p>snip (Lee, Ajanthan, and Torr 2018) is a saliency metric that is originally proposed to prune model parameters at initialization. The metric approximates the loss change when a certain parameter is removed. The formulation is as follows,</p><formula xml:id="formula_2">S(θ) = ∂L ∂θ θ , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where L is the loss function of a network and is the Hadamard product.</p><p>synflow <ref type="bibr" target="#b25">(Tanaka et al. 2020</ref>) optimizes snip to avoid layer collapse when performing parameter pruning. While the computation of snip requires a batch of data and the original loss function, synflow computes the product of all parameters as its loss function and thus requires no data. The formulation is as follows,</p><formula xml:id="formula_4">S(θ) = ∂L ∂θ θ.<label>(3)</label></formula><p>While both snip and synflow are per-parameter metrics, we extend them to score the entire architecture x following <ref type="bibr" target="#b0">(Abdelfattah et al. 2020)</ref> as P (x) = − θ∈Θ S(θ), where Θ refers to all the parameters of architecture x.</p><p>NASWOT <ref type="bibr" target="#b18">(Mellor et al. 2021</ref>) introduces a correlation metric jacob cov that captures the activation pattern of a given batch of data. We refer to the original paper for the detailed definition of the covariance matrix K H . The final score for the entire architecture to minimize is as follows,</p><formula xml:id="formula_5">P (x) = −log |K H | .</formula><p>(4)</p><p>The aforementioned three proxies are selected due to two considerations: 1) All of the three proxies can be computed in a relatively short time using at most a batch of data; 2) The proxies have their own properties, i.e., jacob cov highlights the activation pattern while snip and synflow are gradient-based proxies of different inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Proposed Method</head><p>In this section, we present ProxyBO -our proposed method for efficient Bayesian optimization with zero-cost proxies. To tackle the challenges in the introduction, we will answer the following two questions: 1) how to measure the generalization ability of zero-cost proxies as well as the BO surrogate without prior knowledge, and 2) how to effectively integrate BO with zero-cost proxies during optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Ability Measurement</head><p>The goal of Bayesian optimization (BO) is to iteratively find the best configuration with the optimal objective value. In other words, a proxy or a surrogate is helpful if it can order the performance of the given architecture configurations correctly. Therefore, in our framework, we design a measurement to dynamically estimate the usefulness of zero-cost proxies and BO surrogates during optimization. For simplicity, in the following discussion, we assume that the given objective function needs to be minimized. And remind that the smaller the proxy score is, the better the architecture is expected to be.</p><p>For zero-cost proxies, we wish to assess their ability to fit the ground-truth observations D, and thus we apply the ratio of order-preserving pairs to measure the correctness of their ranking results. The definition of the number of orderpreserving pairs is as follows,</p><formula xml:id="formula_6">F (P i ; D) = |D| j=1 |D| k=j+1 1 ((P i (x j ) &lt; P i (x k )) ⊗ (y j &lt; y k )) , (5)</formula><p>where |D| is the number of observations, P i (x j ) is the output of the zero-cost proxy i given the architecture configuration x j , and ⊗ is the exclusive-nor operation, in which the statement value is true only if the two sub-statements return the same value.</p><p>For BO surrogate, since the BO surrogate is directly trained on the observations D, the above definition only calculates the in-sample error of the BO surrogate and cannot correctly reflect the generalization ability of the surrogate on unseen data points. Therefore, we apply the k-fold crossvalidation strategy when measuring the BO surrogate. Denote f as the mapping function that maps an observed configuration x i to its corresponding fold index as f (x i ). The number of order-preserving pairs of the BO surrogate is then calculated as follows,</p><formula xml:id="formula_7">F (M ; D) = |D| j=1 |D| k=j+1 1((M −f (x j ) (xj) &lt; M −f (x k ) (x k )) ⊗ (yj &lt; y k )) ,<label>(6)</label></formula><p>where M −f (xj ) refers to the surrogate trained on observations D with the f (x j )-th fold left out. In this way, x j is not used when generating M −f (xj ) , thus the new definition is able to measure the generalization ability of BO surrogate only by using the observations D. The final measurement for each proxy and the BO surrogate is calculated by</p><formula xml:id="formula_8">G(•; D) = 2F (•; D)/(|D| * (|D| − 1)</formula><p>). Through this measurement, ProxyBO is able to judge the proxies and surrogate during the search process.</p><p>Note that, to measure the generalization ability, the ratio of order-preserving pairs is more appropriate than other intuitive alternatives such as mean squared error or loglikelihood because we do not care about the actual values of the predictions during the optimization -the framework only needs to identify the location of the optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Influence Combination</head><p>The original BO selects the next configuration to evaluate by maximizing its acquisition function. However, the BO surrogate is under-fitted when given few observations, i.e., it can not precisely predict the performance of unseen configurations to guide the selection of configurations. Rather than a machine learning model, the zero-cost proxies are formulated metrics, which may perform better than the underfitted surrogate at the beginning of the optimization. On the other hand, as the number of observations grows over time, the generalization ability of BO surrogate gradually outperforms the proxies. In this case, more attention should be paid to the surrogate when selecting the next configuration to evaluate. Therefore, the dynamic influence of the proxies and the surrogate on configuration selection should be considered, and the design is non-trivial.</p><p>Concretely in ProxyBO, we alter the acquisition function for selecting configurations by combining the influence of each component. Since the outputs of the proxies and the surrogate are of different scales, we propose to use the sum of ranking instead of directly adding the outputs. During each BO iteration, we sample Q configurations by random sampling and local sampling on well-performed observed configurations. Then we calculate the EI value and proxy values for each sampled configuration. Based on these values, we further rank the Q configurations and obtain the ranking value of x j as R M (x j ) and R Pi (x j ) for the BO surrogate and proxies, respectively. Finally, we define the combined ranking value of a configuration x j as: where K is the number of applied zero-cost proxies, and I(•; D) is the measured influence in the current iteration based on G(•; D). We use a softmax function with temperature to scale the sum of influence to 1 and use the temperature τ to control the softness of output distribution. The formulation is as follows,</p><formula xml:id="formula_9">CR(x j ) = I(M ; D)R M (x j ) + K i=1 I(P i ; D)R Pi (x j ), (7)</formula><formula xml:id="formula_10">I(•; D) = exp(G(•; D)/τ ) exp(G(•; D)/τ ) , τ = τ 0 1 + log T ,<label>(8)</label></formula><p>where τ is controlled by the only hyper-parameter τ 0 , and T is the current number of BO iteration. In each iteration, ProxyBO selects the configuration with the lowest combined ranking (CR) value to evaluate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Summary</head><p>Algorithm 1 illustrates the sampling procedure of Prox-yBO. It first calculates the generalization ability measurements (Line 2) and converts them to influence (Line 3). After that, it ranks the sampled configurations (Lines 4-6) and combines the rankings with the computed influence (Line 7). Algorithm 2 shows the pseudo-code of the ProxyBO framework. It follows a typical BO procedure while replacing the original EI-maximizing selection with the combined ranking-minimizing procedure (Line 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussions</head><p>In this subsection, we discuss the properties of our proposed ProxyBO as follows: Novelty Statement. ProxyBO is the first algorithm framework that effectively combines the advantages of both Bayesian optimization and zero-cost proxies. Prior knowledge about whether the proxies are suitable for the current task is not required in the proposed framework. Also, note that ProxyBO is independent of the choice of surrogate and proxies, i.e., users can replace those components with stateof-the-art ones in future researches. Time Complexity. The time complexity of each iteration in ProxyBO is O(|D|log|D|), which is dominated by the cost of fitting a probabilistic random forest surrogate. Note that since the computation cost of proxy scores is constant during each iteration, it is not taken into account.</p><p>Overhead Analysis. During each iteration, the computation cost of proxy scores is QT p in which Q is the number of computed configurations, and T p is the average cost of computation. In practice, T p can be calculated in seconds, and Q configurations can be parallelly computed even on a single GPU. We set Q to be 500, and the time cost for each iteration is less than a minute. Compared with the actual training cost (hours), this overhead can almost be ignored.</p><p>Convergence Discussion. As the number of observations grows, the proxies accumulate misrankings while the surrogate generalizes better. As a result, the generalization ability measurement of the BO surrogate will gradually outperform the proxies. Meanwhile, the temperature τ declines when T grows, which sharpens the distribution and leads to the domination of the BO surrogate on influence. Finally, ProxyBO puts almost all the weights on the BO surrogate, and the algorithm reverts to standard BO, which enjoys a convergence guarantee <ref type="bibr" target="#b8">(Hutter, Hoos, and Leyton-Brown 2011)</ref>. We will illustrate this trend of influence in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>To evaluate ProxyBO, we apply it on several public benchmarks for neural architecture search. Compared with stateof-the-art baselines, we list three main insights that we will investigate as follows,</p><p>• ProxyBO can dynamically measure the influence of zerocost proxies during the search process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>Baselines. We compare the proposed method Prox-yBO with the following eleven baselines -Five regular methods: (1) Random search (RS) (Bergstra and Bengio  <ref type="formula" target="#formula_2">2</ref>) REINFORCE (RL) <ref type="bibr" target="#b27">(Williams 1992)</ref>  <ref type="bibr" target="#b6">(Garofolo 1993)</ref>. The benchmark includes the statistics of 8242 unique models.</p><p>Since NAS-Bench-201 includes the statistics on three datasets, the experiments are conducted on five tasks over three different search spaces in total. We demonstrate the Spearman ρ of proxy scores related to the test results of all models and top-10% models on each task in Table <ref type="table" target="#tab_2">1</ref>, and note that ρ is the prior knowledge that can not be obtained before optimization. We observe that no proxy dominates the others on all the tasks, and some correlation coefficients are even negative, leading to a number of incorrect rankings. Basic Settings. In the following subsections, we search for the architecture with the best validation performance during the search process and report the average best test error for NAS-Bench-101 and NAS-Bench-201 and the average best test Phoneme error rate (PER) for NAS-Bench-ASR. The "average best" refers to the best-observed performance during optimization, and thus the curve is non-increasing.</p><p>For evaluation-based methods, we set the maximum evaluation iterations to be 200 on each task. For zero-cost proxies, we randomly sample 1000 architectures and report the test result of the model with the best proxy score. Each weight-sharing method is run until convergence. To avoid randomness, weight-sharing methods are repeated 5 times,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Analysis</head><p>Before stepping into the insights, we first analyze the parameter sensitivity on NAS-Bench-201 CIFAR-100. We choose this task because each proxy shows a positive correlation.</p><p>Figure <ref type="figure" target="#fig_2">2</ref>(a) demonstrates the effects of hyper-parameter τ 0 together with two extremes -only using the component with the largest influence and using the mean combination regardless of influence. We observe that the performance of mean combination is worse than standard BO. In addition, using the best component is not the best strategy, as the secondbest component may contain helpful ranking information.</p><p>In the following experiments, we set τ 0 = 0.05 by default, which is the best among different choices in early iterations.</p><p>ProxyBO can measure the influence of proxies during the search process. To show the influence of different proxies, we demonstrate the trend of generalization ability measurements and influence values on NAS-Bench-101 in <ref type="bibr">Figure 2(b)</ref>. As shown in Table <ref type="table" target="#tab_2">1</ref>, the correlation coefficients of snip and jacob cov are negative. Therefore, their measurements are much lower than synflow's, and their influence values are kept as zero after about 13 evaluations. synflow, the best among the three proxies, shows the largest influence in the beginning 36 evaluations. However, since zero-cost proxies are formulated metrics, their measurements are relatively stable. The measurement of BO surrogate exceeds synflow at the 39-th evaluation, and it takes the surrogate another 10 evaluations to enlarge the gap. After that, the generalization ability of the surrogate further increases with more evaluations, and its influence keeps closely to 1. In this case, ProxyBO turns back to standard BO with a convergence guarantee.</p><p>ProxyBO can effectively integrate BO procedure with zero-cost proxies. Figure <ref type="figure" target="#fig_2">2</ref>(c) further compares Prox-yBO using different zero-cost proxies. As the Spearman of jacob cov is negative, we observe that using only jacob cov leads to worse results before 56 evaluations but converges similar to standard BO. The reason is that Prox-yBO can quickly identify bad proxies after a few evaluations and then reduce their influences. In addition, using the only positive proxy synflow can further accelerate neural architecture search than using all proxies because it saves evaluation cost to identify bad proxies. Note that though the performance gain seems significant, the prior knowledge of which proxy shows positive correlation is unknown before searching. Overall, ProxyBO performs no worse than BO when the proxies are not helpful and leads to considerable gain if some proxies are useful, which indicates its effectiveness in integrating BO with zero-cost proxies. In the following, we compare ProxyBO with other competitive baselines.</p><p>ProxyBO achieves promising results. Table <ref type="table" target="#tab_4">2</ref> shows the test results on five tasks given the budget of 200 evaluations. We observe that zero-cost proxies require extremely short computation time, but the final results are not satisfactory.</p><p>The reason is that it can not correctly rank the most-accurate architectures (see top-10% architectures in Table <ref type="table" target="#tab_2">1</ref>). As also stated by previous work <ref type="bibr" target="#b30">(Zela et al. 2019;</ref><ref type="bibr" target="#b4">Dong and Yang 2019)</ref>, weight-sharing methods (DARTS and ENAS) may find sub-optimal architectures with many parameter-free operations and perform poorly. On NAS-Bench-201, their final performance is far worse than random search. Note that, though weight-sharing methods and zero-cost proxies require less budget than evaluation-based methods, their performance has converged. In addition, due to the limitations of the warm-up strategy, Warm-up BRP slightly outperforms BRP on NAS-Bench-101 but performs worse on the other benchmarks. Among the competitive baselines, ProxyBO achieves the best average results on the five tasks and reduces the test regret (i.e., the distance to the global optima) of the best baseline by 26-67%.</p><p>ProxyBO can significantly accelerate NAS. Figure <ref type="figure">3</ref> demonstrates the search results of regular and zero-cost proxy-based methods. Consistent with Table <ref type="table" target="#tab_4">2</ref>, we observe that Warm-up BRP only shows acceleration in early rounds on NB2 ImageNet16-120 and NAS-Bench-ASR. Compared with weight-sharing methods and zero-cost proxies, it takes ProxyBO less than 8 evaluations to surpass their converged results. In addition, the test results of ProxyBO decrease rapidly before 75 iterations, and it consistently outperforms the other baselines on the five tasks. To show the speedup of ProxyBO, we further compare the number of trained models required to achieve the same average results as REA using 200 evaluations in Table <ref type="table" target="#tab_5">3</ref>. Concretely, ProxyBO achieves 2.67 − 5.41× and 2.27 − 3.83× speedups relative to the state-of-the-art method REA and BRP, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we introduced ProxyBO, an efficient Bayesian optimization framework that leverages the auxiliary knowledge from zero-cost proxies. In ProxyBO, we proposed the generalization ability measurement and dynamic influence combination, which tackles the unforeseeable reliability and one-shot usage issues in the existing method, and developed</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Spearman ρ of jacob cov over NAS search spaces using 1000 randomly sampled architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, (3) Regularized evolutionary algorithm (REA)<ref type="bibr" target="#b28">(Real et al. 2019</ref>), (4) Bayesian optimization (BO)<ref type="bibr" target="#b8">(Hutter, Hoos, and Leyton- Brown 2011)</ref>, (5) Binary relation predictor (BRP)<ref type="bibr" target="#b5">(Dudziak et al. 2020)</ref>: the state-of-the-art evaluation-based algorithm BRP-NAS that uses a binary GCN predictor -Two weightsharing methods: (6) DARTS<ref type="bibr" target="#b14">(Liu, Simonyan, and Yang 2018)</ref>, (7) ENAS<ref type="bibr" target="#b20">(Pham et al. 2018)</ref>, -Three zero-cost proxies: (8) Snip (Lee, Ajanthan, and Torr 2018), (9) Synflow (Tanaka et al. 2020), (10) Jacob cov (Mellor et al. 2021) -One zero-cost proxy-based methods: (11) Warmup BRP (Abdelfattah et al. 2020): BRP-NAS with warmstart based on the relative rankings of proxy scores. NAS Benchmarks. To evaluate the performance of Prox-yBO and ensure reproducibility, we conduct the experiments on three public benchmarks for neural architecture search: • NAS-Bench-101 (Ying et al. 2019): The first NAS benchmark for image classification on CIFAR-10, which includes the statistics of 423k unique CNN models. • NAS-Bench-201 (Dong and Yang 2019): A more lightweight benchmark than NAS-Bench-101, which includes the statistics of 15,625 CNN models on three datasets -CIFAR-10, CIFAR-100, and ImageNet16-120. • NAS-Bench-ASR (Mehrotra et al. 2020): A benchmark for automatic speech recognition on TIMIT dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Algorithm analysis of ProxyBO. Figure (a) is conducted on NAS-Bench-201 CIFAR-100 while Figures (b) and (c) are conducted on NAS-Bench-101. The solid lines and dash lines in Figure (b) refer to the generalization ability measurements and influence values, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1: Pseudo code for Sample in ProxyBO Input: the observations D, the current number of iteration T , the number of sampled configurations Q, the Bayesian optimization surrogate M , the zero-cost proxies P1:K , and the temperature hyper-parameter τ0. Output: the next architecture configuration to evaluate.1: if |D| &lt; 5, then return a random configuration.</figDesc><table><row><cell>2: compute G(•; D) for each proxy and the surrogate.</cell></row><row><cell>3: compute I(•; D) according to Eq. 8.</cell></row><row><cell>4: draw Q configurations via random and local sampling.</cell></row><row><cell>5: compute the Expected Improvement (EI) based on surrogate</cell></row><row><cell>M according to Eq. 1, and the proxy values for P1:K according</cell></row><row><cell>to Eq. 2, 3, 4 for each sampled configuration.</cell></row><row><cell>6: rank the Q configurations and obtain the ranking value of con-</cell></row><row><cell>figuration xj as RM (xj) and RP i (xj) for the i-th proxy.</cell></row><row><cell>7: calculate the combined ranking CR(xj) for each configuration</cell></row><row><cell>xj according to Eq. 7.</cell></row><row><cell>8: return the configuration with the lowest combined ranking</cell></row><row><cell>value.</cell></row><row><cell>Algorithm 2: Pseudo code for the framework of ProxyBO</cell></row></table><note>Input: the search budget B, the architecture search space X . Output: the best observed architecture configuration.1: initialize observations D = ∅. 2: while budget B does not exhaust do 3: build surrogate M based on observations D.4:call Sample for the next configuration to evaluate. 5: evaluate the selected configuration xj and obtain its performance yj.6:augment D = D ∪ (xj, yj). 7: end while 8: return the configuration with the best observed performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Spearman ρ of proxies for all (and top-10%) architectures in NAS spaces. "NB2" refers to NAS-Bench-201.</figDesc><table><row><cell></cell><cell>snip</cell><cell cols="2">synflow jacob cov</cell></row><row><cell>NAS-Bench-101</cell><cell cols="3">-0.16 (-0.00) 0.37 (0.14) -0.38 (-0.08)</cell></row><row><cell>NB2 CIFAR-10</cell><cell>0.60 (-0.36)</cell><cell>0.72 (0.12)</cell><cell>0.74 (0.15)</cell></row><row><cell>NB2 CIFAR-100</cell><cell>0.64 (-0.09)</cell><cell>0.71 (0.42)</cell><cell>0.76 (0.06)</cell></row><row><cell>NB2 ImageNet16-120</cell><cell>0.58 (0.13)</cell><cell>0.70 (0.55)</cell><cell>0.75 (0.06)</cell></row><row><cell>NAS-Bench-ASR</cell><cell>0.03 (0.13)</cell><cell cols="2">0.41 (-0.01) -0.36 (0.06)</cell></row><row><cell>2012), (</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Mean ± std. test errors (%) on NAS-Bench-101 and NAS-Bench-201, and test PERs (%) on NAS-Bench-ASR. "NB2" refers to NAS-Bench-201, and "Optimal" refers to the best result in the entire benchmark. The evaluation number of weightsharing methods and zero-cost proxies is computed by their runtime divided by the average training time of architectures. The zero-cost proxy-based methods apply all three proxies. The results of weight-sharing methods on NAS-Bench-201 and NAS-Bench-101 follow<ref type="bibr" target="#b4">Dong and Yang (2019)</ref> and<ref type="bibr" target="#b29">Yu et al. (2019)</ref>, respectively.</figDesc><table><row><cell>Method</cell><cell cols="6">Runtime (#Eval) NB2-CIFAR-10 NB2-CIFAR-100 NB2-ImageNet16-120 NAS-Bench-101 NAS-Bench-ASR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Regular Methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RS RL REA BO BRP</cell><cell>200 200 200 200 200</cell><cell>9.11 ± 0.21 9.02 ± 0.24 8.62 ± 0.21 8.80 ± 0.22 8.58 ± 0.13</cell><cell cols="2">27.97 ± 0.66 27.66 ± 0.65 26.67 ± 0.35 27.03 ± 0.45 26.57 ± 0.12 Weight-sharing Methods 54.01 ± 0.55 53.58 ± 0.45 53.08 ± 0.36 53.12 ± 0.37 52.96 ± 0.29</cell><cell>6.29 ± 0.12 6.26 ± 0.14 6.14 ± 0.24 6.09 ± 0.23 6.00 ± 0.16</cell><cell>21.61 ± 0.10 21.62 ± 0.09 21.50 ± 0.07 21.47 ± 0.06 21.50 ± 0.08</cell></row><row><cell>DARTS ENAS</cell><cell>≈9 ≈7</cell><cell>45.70 ± 0.00 46.11 ± 0.58</cell><cell>85.38 ± 0.00 86.04 ± 2.33 Zero-cost Proxies</cell><cell>83.68 ± 0.00 85.19 ± 2.10</cell><cell>7.79 ± 0.61 8.17 ± 0.42</cell><cell>23.59 ± 0.43 24.45 ± 0.90</cell></row><row><cell>Snip Jacob cov Synflow</cell><cell>&lt;1 &lt;1 &lt;1</cell><cell>13.45 ± 1.80 12.19 ± 1.60 10.30 ± 0.94</cell><cell cols="2">36.41 ± 3.36 32.99 ± 2.84 29.55 ± 1.77 Zero-cost Proxy-based Methods 71.94 ± 9.09 60.43 ± 4.46 56.94 ± 3.57</cell><cell>10.68 ± 2.16 13.86 ± 1.86 8.32 ± 1.64</cell><cell>31.61 ± 18.17 69.95 ± 24.67 25.70 ± 12.91</cell></row><row><cell>Warm-up BRP ProxyBO</cell><cell>200 200</cell><cell>8.58 ± 0.21 8.56 ± 0.10</cell><cell>26.65 ± 0.31 26.52 ± 0.17</cell><cell>53.02 ± 0.35 52.82 ± 0.19</cell><cell>5.99 ± 0.15 5.91 ± 0.13</cell><cell>21.51 ± 0.10 21.43 ± 0.03</cell></row><row><cell>Optimal</cell><cell>/</cell><cell>8.55</cell><cell>26.49</cell><cell>52.69</cell><cell>5.68</cell><cell>21.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Number of evaluations required to achieve the same average results as REA with 200 evaluations.</figDesc><table><row><cell></cell><cell cols="3">BRP Warm-up BRP ProxyBO</cell></row><row><cell>NB2 CIFAR-10</cell><cell>170</cell><cell>178</cell><cell>75</cell></row><row><cell>NB2 CIFAR-100</cell><cell>142</cell><cell>184</cell><cell>37</cell></row><row><cell cols="2">NB2 ImageNet16-120 144</cell><cell>168</cell><cell>46</cell></row><row><cell>NAS-Bench-101</cell><cell>94</cell><cell>105</cell><cell>41</cell></row><row><cell>NAS-Bench-ASR</cell><cell>179</cell><cell>213</cell><cell>51</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a more principled way to utilize zero-cost proxies. We evaluated ProxyBO on three public benchmarks and demonstrated its superiority over competitive baselines.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zero-Cost Proxies for Lightweight NAS</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyperparameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02">2012. Feb</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards an empirical foundation for assessing bayesian optimization of hyperparameters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Bayesian Optimization in Theory and Practice</title>
				<imprint>
			<date type="published" when="2013">2020. 2013. 2015</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Timit acoustic phonetic continuous speech corpus</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="1993">1993. 1993. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Deep residual learning for image recognition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schonlau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning and Intelligent Optimization</title>
				<imprint>
			<date type="published" when="1998">2011. 1998</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="455" to="492" />
		</imprint>
	</monogr>
	<note>Efficient global optimization of expensive black-box functions</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural architecture search with Bayesian optimisation and optimal transport</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Snip: Single-shot Network Pruning Based on Connection Sensitivity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MFES-HB: Efficient Hyperband with Multi-Fidelity Quality Measurements</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8491" to="8500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">OpenBox: A Generalized Black-box Optimization Service</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7827" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep neural architecture search with deep graph bayesian optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/WIC/ACM International Conference on Web Intelligence (WI)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="500" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G C</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vipperla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishtiaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architecture search without training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7588" to="7598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
				<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">NAS-Bench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09777</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2012">2012. 2019</date>
			<biblScope unit="page" from="5877" to="5886" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust Bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4134" to="4142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BA-NANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Savani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05787</idno>
	</analytic>
	<monogr>
		<title level="m">Faster gaze prediction with dense networks and fisher pruning</title>
				<meeting><address><addrLine>White, C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2019. 2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10293" to="10301" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluating The Search Phase of Neural Architecture Search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09656</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2020</date>
			<biblScope unit="page" from="11396" to="11404" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Econas: Finding proxies for economical neural architecture search</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
