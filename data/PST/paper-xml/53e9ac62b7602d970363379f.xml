<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">R</forename><surname>Perfetti</surname></persName>
							<email>perfetti@diei.unipg.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Information Engineering</orgName>
								<orgName type="institution">University of Perugia</orgName>
								<address>
									<postCode>I-06125</postCode>
									<settlement>Perugia</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic and Information Engi-neering</orgName>
								<orgName type="institution">University of Perugia</orgName>
								<address>
									<postCode>I-06125</postCode>
									<settlement>Perugia</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">431D2B95341CDB03A91F1F518E46C3B1</idno>
					<idno type="DOI">10.1109/TMI.2007.898551</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retinal Blood Vessel Segmentation Using Line</head><p>Operators and Support Vector Classification </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D IGITAL fundus imaging in ophthalmology plays an im- portant role in medical diagnosis of several pathologies like hypertension, diabetes, and cardiovascular disease <ref type="bibr" target="#b0">[1]</ref>. Computer-aided image analysis of the eye fundus is highly desirable in many cases. For example, the diagnosis of diabetic retinopathy, the leading cause of blindness in the Western World, requires the screening of a large number of patients from specialized personnel and can be extremely facilitated with the adoption of automatic tools <ref type="bibr" target="#b1">[2]</ref>.</p><p>One task of the utmost importance is the segmentation of the vasculature in retinal fundus images. Several morphological features of retinal veins and arteries, like diameter, length, branching angle, and tortuosity, have diagnostic relevance and can be used to monitor the progression of diseases <ref type="bibr" target="#b2">[3]</ref>. Moreover blood vessels, being invariant features, may be very useful for registration of retinal images of the same patient gathered from different sources <ref type="bibr" target="#b3">[4]</ref>. Conversely, in some applications the vasculature must be excluded from the analysis to ease automatic detection of pathological elements like exudates or microaneurysms <ref type="bibr" target="#b4">[5]</ref>. Finally, its tree-like geometry makes it also useful to determine the position of other features as fovea or optic disk <ref type="bibr" target="#b5">[6]</ref>. Accurate vessel segmentation is a difficult task for several reasons: the presence of noise, the low contrast between vasculature and background, the variability of vessel width, brightness, and shape. Moreover, due to the presence of lesions, exudates, and other pathological effects, the image may have large abnormal regions.</p><p>Several methods have been proposed in the literature to address these problems including matched filtering <ref type="bibr" target="#b6">[7]</ref>, tracking methods <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, multithreshold probing <ref type="bibr" target="#b10">[11]</ref>, mathematical morphology <ref type="bibr" target="#b11">[12]</ref>, or a combination of nonlinear filtering and morphological operations <ref type="bibr" target="#b12">[13]</ref>.</p><p>Interesting results have been obtained by pixel classification based on supervised learning. The goal of classification approaches is to assign each pixel to one of two classes, i.e., vessel and non-vessel, based on some features extracted from the image in the neighborhood of the considered pixel. In <ref type="bibr" target="#b13">[14]</ref>, a neural network is trained by the backpropagation learning algorithm. The feature vectors are constructed using the first principal component values in a 10 10 window centered on the pixel being classified and the corresponding edge strength values in the same window (200 features). In <ref type="bibr" target="#b14">[15]</ref>, the outputs of multiscale Gaussian filters and the original grey level of the green channel image (31 features) are given as input to a k-nearest neighbor classifier. Better results have been obtained in <ref type="bibr" target="#b15">[16]</ref> where ridge profiles are extracted from which 27 features are computed. Then, a feature selection scheme is applied to select those corresponding to the better class separability. In <ref type="bibr" target="#b16">[17]</ref>, a multiscale analysis is performed on the image using the Gabor wavelet transform. Only five features are considered: the grey level of the inverted (green plane) image and the maximum Gabor transform response over angles at four different scales. For supervised classification a Gaussian mixture model (GMM) Bayesian classifier is adopted. Some of the above-mentioned methods are based on the detection of linear structures through angular scanning. In <ref type="bibr" target="#b6">[7]</ref>, the image is convolved with 12 rotated versions of a two-dimensional matched filter impulse response (15 of angular resolution), retaining the maximum response with respect to the angle. A similar approach is adopted in <ref type="bibr" target="#b16">[17]</ref> where the orientation spans from 0 to 170 at steps of 10 , and in <ref type="bibr" target="#b12">[13]</ref> where only four orientations are used. In <ref type="bibr" target="#b11">[12]</ref>, linear structures at 12 different orientations are detected by computing the supremum of openings using a linear structuring element. Suppression of undesirable patterns is then performed using the Laplacian and an original alternating filter. The final result is thresholded.</p><p>In this paper, we explore a computationally simpler but more effective realization of the same concept, inspired by a paper of Zwiggelaar et al. <ref type="bibr" target="#b17">[18]</ref> about the detection of linear structures in mammographic images. Among the techniques compared in <ref type="bibr" target="#b17">[18]</ref>, we select the line operator introduced in <ref type="bibr" target="#b18">[19]</ref>, which has been modified to take into account the peculiarities of retinal vessel structure. The resulting image is thresholded to obtain unsupervised binary pixel classification. Then, to further improve the performance, a supervised method is proposed where two line detectors are used to compute the feature vectors, while a linear support vector machine (SVM) <ref type="bibr" target="#b19">[20]</ref> is chosen as a classifier. With respect to previous supervised techniques, the proposed method exhibits the following desirable properties: it requires less features, feature extraction is computationally simpler, and fewer examples are needed for training.</p><p>The performance of both methods is evaluated on the publicly available STARE <ref type="bibr" target="#b20">[21]</ref> and DRIVE <ref type="bibr" target="#b14">[15]</ref> databases through receiver operating characteristic (ROC) analysis. From the area under the ROC curve and from the accuracy, we can see that our approach, despite its simplicity, matches or slightly outperforms the most recent methods.</p><p>The rest of the paper is organized as follows. In the next section we illustrate the proposed methods of vessel segmentation. In Section III, we review the main properties of SVMs. In Section IV, the experimental results are presented and compared to several existing methods. Some comments conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED METHODS</head><p>In the analysis of red-green-blue (RGB) non-mydriatic images, usually the color components are considered separately because the green channel exhibits the best vessel/background contrast while the red and blue ones tend to be very noisy. Therefore, we work on the inverted green channel images, where vessels appear brighter than the background. An example is shown in Fig. <ref type="figure" target="#fig_1">1</ref>. To preserve at most the vessel structure we avoid any preprocessing on the image. Indeed, low-pass filtering makes vessel boundaries less sharp and causes loss of thinnest vessels. The contrast enhancement suggested in <ref type="bibr" target="#b13">[14]</ref> has been tried but it was discarded due to the increased amount of noise. Also, experiments with lighting and contrast equalization have been performed but without any improvement. Details on these experiments will be given in Section IV.</p><p>The basic line detector is illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. The average grey level is evaluated along lines of fixed length passing through the target pixel at different orientations. We consider 12 angles (15 of angular resolution). The line with  the largest average grey level is found; its value is denoted with . The difference represents the line strength of the pixel <ref type="bibr" target="#b17">[18]</ref>, where is the average grey level in the square window, centered on the pixel, with edge length equal to . The line strength is large if the winning line is aligned within a vessel; otherwise, we have a partial overlap and the line strength is lower [Fig. <ref type="figure" target="#fig_3">3(a)</ref>]. This difference allows us to discriminate vessel pixels from non-vessel pixels. Lines of different lengths have been considered; the best performance was obtained with pixels (see Section IV for a comparison of different lengths). This result is in accordance with <ref type="bibr" target="#b11">[12]</ref> where a line operator of length 15 pixels is used as structuring element to perform morphological operations.</p><p>Conversely to <ref type="bibr" target="#b17">[18]</ref>, the square window is not oriented like the winning line but it is kept fixed. Moreover, the average grey level of lines is not obtained by interpolation, but the pixels to be averaged are found by rounding the coordinates of the points on the ideal line. The approximated lines for 15 , 30 , 60 , and 75 are shown in Fig. <ref type="figure" target="#fig_4">4</ref>. Lines with angles , , , and are turned right-left with respect to those shown in Fig. <ref type="figure" target="#fig_4">4</ref>. Lines with angles 0 , 90 , and are exact. Simulation results have demonstrated that both simplifications do not affect the segmentation performance while computational cost is significantly reduced.</p><p>There is a certain resemblance between the proposed line detector and the "tramline" filter proposed in <ref type="bibr" target="#b21">[22]</ref>, so we point out the differences between the two schemes. The tramline filter consists of three parallel lines of nine pixels, where the outer lines are displaced five pixels on each side of the inner line. The filter response is the difference between the minimum pixel intensity in the outer lines and the maximum intensity level in the inner line (with vessels darker than the background). The filter is rotated through 12 angles to select the maximum response. It  gives a strong response when the inner line is aligned within a blood vessel and the outer lines are on either side of the vessel. Thus, it is suited to detect vessel centerlines, but not for vessel segmentation. When it is applied to large vessels, only the innermost pixels are emphasized. Moreover, the proposed line detector has a lower computational cost and it is less sensitive to noise due to averaging.</p><p>As can be seen in Fig. <ref type="figure" target="#fig_1">1</ref>, the eye fundus images have a roughly circular field of view (FOV) surrounded by a bright frame. When the line detector is applied to the pixels inside the FOV near the border, the line strength is strongly biased by the external bright region. As a consequence, false vessel detection occurs along the borderline. To overcome this problem, applying the proposed method to border pixels, the out-of-the-FOV grey levels in the 15 15 square centered on the target pixel are replaced by the average grey level of the remaining pixels in the square. Identification of the FOV is not necessary for the database DRIVE where it is given. We identified the FOV only for STARE using routine morphological operations. In Fig. <ref type="figure" target="#fig_5">5</ref>, the line strength image corresponding to Fig. <ref type="figure" target="#fig_1">1</ref> is shown. For better visualization the image has been inverted. The basic line detector described above can be improved for retinal vessel segmentation, since the vessels can have different width and intensity. In proximity of a large and bright vessel, the line strength of a pixel can be comparable with that of the pixels inside a darker and thinner vessel. A possible way to overcome this problem is shown in Fig. <ref type="figure" target="#fig_3">3(b)</ref>. A line of three pixels is considered, centered on the midpoint of the main line and orthogonal to it. Its average grey level is denoted with . Its strength is obtained again subtracting the average intensity in the 15 15 square, i.e.,</p><p>. The angle of the orthogonal line is approximated with the closest principal angle (0 , 45 , 90 , 135 ). For example the "orthogonal" direction for the orientations 30 , 45 , and 60 is 135 .</p><p>The strength of the shorter line will be negligible for out-of-the-vessel pixels, while it is comparable to the main line strength for inside pixels. This additional information helps to discriminate between inside and outside pixels, decreasing the number of false positives. Moreover, we consider the grey level of the pixel as a third feature; we note that it helps to reduce false detection due to pathology or to the proximity of the optic disk. Then, we construct the feature vector , used to train a supervised classifier. A normal transformation is applied to the feature vector <ref type="bibr" target="#b22">[23]</ref> (1)</p><p>where is the th feature , while and are the corresponding mean value and standard deviation, respectively. Normalization is applied separately to each image in order to compensate for illumination variation between the images. For supervised binary classification we adopt an SVM because of its well-known good generalization <ref type="bibr" target="#b19">[20]</ref>. We carried out experiments with both linear and nonlinear SVMs. However, the results do not demonstrate any improvement of nonlinear SVM with respect to the linear SVM. Since the latter is much less demanding in terms of computation, it seems more suited to this application. Hence, in the paper we present only the linear case. A review on linear SVMs is given in the following section. where is the feature vector, is the weight vector, and is the bias. We may thus write <ref type="bibr" target="#b2">(3)</ref> The distance between the separating hyperplane and the closest points is called margin. The support vector machine identifies the optimal separating hyperplane (OSH) for which the margin of separation is maximized. We can rescale the weights and the bias so that the constraints (3) can be rewritten as <ref type="bibr" target="#b3">(4)</ref> As a consequence, the margin of separation is and the maximization of the margin is equivalent to the minimization of the Euclidean norm of the weight vector . The corresponding weights and bias represent the OSH.</p><p>Introducing the Lagrange multipliers , the minimization of under constraints (4) can be recast in the following dual form <ref type="bibr" target="#b19">[20]</ref>. Minimize <ref type="bibr" target="#b4">(5)</ref> subject to the linear constraints</p><formula xml:id="formula_0">(6a)<label>(6b)</label></formula><p>There is a Lagrange multiplier for every training point. Those points corresponding to are called support vectors and lie closest to the OSH.</p><p>Solving the quadratic programming (QP) problem ( <ref type="formula">5</ref>)-( <ref type="formula" target="#formula_0">6</ref>) we can compute the optimum weight vector as follows: <ref type="bibr" target="#b6">(7)</ref> For a support vector , it is , from which the optimum bias can be computed by substitution of (7) <ref type="bibr" target="#b7">(8)</ref> In practice, it is better to average the values obtained by considering the set of all support vectors. The class of a new point is given by the sign of the following decision function:</p><p>SVM formulation can be extended to the case of linearly nonseparable data, introducing the slack variables ; optimal class separation can be obtained by minimization of <ref type="bibr" target="#b9">(10)</ref> under constraints (11a) (11b)</p><p>In this case, the margin of separation is said to be soft. The constant is user-defined and controls the tradeoff between the maximization of the margin and the minimization of classification errors on the training set. The dual formulation of the QP is the same as ( <ref type="formula">5</ref>)-( <ref type="formula" target="#formula_0">6</ref>) with the only difference in the bound constraints <ref type="bibr" target="#b11">(12)</ref> In this case, for the Lagrange multipliers we have: for points correctly classified with and for points with (misclassified if ). In the soft margin case, ( <ref type="formula">7</ref>) and ( <ref type="formula">8</ref>) still hold.</p><p>Often, in many practical applications, it is important to distinguish between the two types of errors that can arise in a binary classification task, i.e., false positives and false negatives. This is particularly true for medical applications where performance is generally evaluated in terms of sensitivity (ratio of the number of true positives and the number of positive instances) and specificity (ratio of the number of true negatives and the number of negative instances).</p><p>One possible way to face this problem with SVMs is based on the idea of assigning different weights to the errors of the two classes. This leads to a slightly different formulation of the primal problem which becomes <ref type="bibr" target="#b23">[24]</ref> the following. Minimize <ref type="bibr" target="#b12">(13)</ref> under constraints (14a) (14b)</p><p>Therefore, also the dual formulation of the QP is the same of ( <ref type="formula">5</ref>)-( <ref type="formula" target="#formula_0">6</ref>) but with the Lagrange multipliers constrained as follows:</p><formula xml:id="formula_2">if (15a) if (15b)</formula><p>To solve the QP problem in its dual formulation, in our experiments we used SVMlight <ref type="bibr" target="#b24">[25]</ref> <ref type="foot" target="#foot_0">1</ref> which allows us to deal with unsymmetric cost factors. Here, varying the ratio , different possible tradeoffs between the cost of false positives and false negatives can be obtained. In practical applications <ref type="bibr" target="#b25">[26]</ref>, often the parameters and are chosen such that the cost of the false positives equals the cost of the false negatives, that is , where and are, respectively, the number of positive and negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Databases</head><p>We use two publicly available databases of non-mydriatic color images. The first one, known as STARE, was collected by Hoover et al. <ref type="bibr" target="#b20">[21]</ref>. <ref type="foot" target="#foot_1">2</ref> It consists of 20 images (ten with pathologies) captured by a TopCon TRV-50 fundus camera at 35 FOV. The slides were digitized to 700 605 pixels with 8 bits per color channel. The FOV in the images is approximately 650 550 pixels. All the images were manually segmented by two observers. The first observer marked 10.4% of pixels as vessel, the second one 14.9%. Performance of the proposed method is evaluated with the segmentation of the first observer as ground truth.</p><p>The second database was collected by Niemeijer et al. <ref type="bibr" target="#b14">[15]</ref> with the aim of comparing the performance of different segmentation methods. This database is known as DRIVE and it is publicly available. It consists of 40 images (seven with pathologies) captured by a Canon CR5 3CCD camera with a 45 FOV. The images are of size 768 584 pixels with 8 bits per color channel. The FOV is circular with approximately 540 in diameter. For each image, a mask image is provided that delineates the FOV. Hence, detection of the FOV border is not needed in this case. The images are compressed in JPEG format, which is a common practice in screening programs, and are divided into a training set and a test set, each containing 20 images. The test set has four images with pathology. The training set is useful to design supervised segmentation methods. All the images were manually segmented. Those of the test set were segmented twice, resulting in a set A and a set B. In set A, 12.7% of pixels were marked as vessel, against 12.3% for set B. Performance is evaluated on the test set using the segmentations of set A as ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Evaluation</head><p>It is common practice to evaluate the performance of retinal vessel segmentation algorithms using ROC curves <ref type="bibr" target="#b14">[15]</ref>. An ROC curve plots the fraction of vessel pixels correctly classified as vessel, namely the true positive rate (TPR), versus the fraction of nonvessel pixels wrongly classified as vessel, namely the false positive rate (FPR). The closer the curve approaches the top left corner, the better is the performance of the system. The most frequently used performance measure extracted from the ROC curve is the value of the area under the curve (AUC) which is 1 for an optimal system. For both databases, TPR and FPR are computed considering only pixels inside the FOV. The ROC curve of the basic line detector can be obtained thresholding the line strength at different levels.</p><p>As concerns the proposed supervised method, we first select a training set to build the SVM classifier and then evaluate the performance on a test set. For the database STARE, training is performed using 20 000 manually segmented pixels randomly extracted from the 20 images (1000 pixels per image). Due to the small size of the training set ( 0.5% of the entire database), classification performance is evaluated on the whole set of 20 images. For the database DRIVE 20 000 manually segmented pixels are randomly extracted from the available training set of 20 images (1000 pixels per image) and are used to train the classifier. Classification performance is then evaluated on the 20 images of the test set.  For linear SVMs the ROC curve can be obtained by varying the threshold as a free parameter <ref type="bibr" target="#b26">[27]</ref>. To this end, the value of obtained with ( <ref type="formula">8</ref>) is considered as a starting point; from there if diminishes, the TPR and FPR decrease, while increasing it the TPR and FPR approach to unity (vessel pixels correspond to class 1). It is worth noting that the ratio is unimportant in ROC analysis since varying is equivalent to moving on the ROC curve. However, also in this case, we set to 7 which is roughly equal to the ratio . The parameter is determined by cross-validation on the training set using the AUC as performance measure. For both databases the value corresponds to the best generalization performance. However, experiments have shown that the classification error is almost insensitive to the value of in the range 0.1, 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>The ROC curves for the two databases are depicted in Figs. <ref type="figure" target="#fig_7">6</ref> and<ref type="figure" target="#fig_8">7</ref>: the solid line represents the results of segmentation with the linear SVM, while the dashed line corresponds to the line detector with line length . The dot shows the performance of manual segmentation: for the STARE database, the second observer gives FPR and TPR , while for the DRIVE database results of set B are FPR and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TPR</head><p>. In Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_0">II</ref>, we show a comparison of performance for the basic line detector with varying line length , in terms of AUC and accuracy. The accuracy is the maximum average accuracy at a certain "optimum" threshold. For both databases seems the best choice; this value has been used in all the experiments presented in the following. However, we can see that the value of is not critical since the performance slowly varies with it. Moreover, the line length can be easily tuned to images with different resolution.</p><p>In Fig. <ref type="figure">8</ref>, the inverted green channel is shown for an image with nonuniform illumination from the STARE database. The segmentation obtained with the basic line detector is shown in Fig. <ref type="figure" target="#fig_9">9</ref>. This result corresponds to the best accuracy (0.9630). Even if the brightness of the image increases radially outwards, the performance is quite satisfactory both in the darker central region and in the brighter outermost one. This is due to the local differential computation of the line strength which makes the line detector somewhat robust with respect to nonuniform illumination and contrast.</p><p>We also tested two equalization techniques to ascertain their effect on the line detector behavior: the lighting compensation  proposed by Hoover et al. <ref type="bibr" target="#b5">[6]</ref> and the method of Foracchia et al. <ref type="bibr" target="#b27">[28]</ref>. Both methods rely on a window size, so different values were tried to get the best performance. However, when the line detector is combined with equalization the accuracy of the detection is generally worse. For the sake of comparison, we consider again the image in Fig. <ref type="figure">8</ref>. Using the lighting compensation in <ref type="bibr" target="#b5">[6]</ref> the maximum accuracy was 0.9419 (49 49 window), while using the method of <ref type="bibr" target="#b27">[28]</ref> the maximum accuracy was 0.9221 (80 80 window). The explanation for this reduced performance comes from the fact that these equalization techniques not only enhance vessels but also other structures in low contrast regions, increasing the effect of noise and the false detections of non-vessel pixels.</p><p>In Fig. <ref type="figure" target="#fig_10">10</ref>, we show an example of segmentation concerning a vessel with "central reflex" (a brighter strip along the center line present sometimes in wider vessels). In (a), a patch is shown from the STARE database where a central reflex is evident. In (b), the segmentation obtained by the line detector is shown (the accuracy on the patch is 0.9572). Generally, the line detector behavior in presence of a central reflex is quite satisfactory. The reason is that the winner line rotates with respect to the vessel center line, so the line strength is not much affected by the central strip intensity. When the central reflex is very large it may happen that the line detector splits the vessel into parallel lines, but this occurrence is very rare.</p><p>Fig. <ref type="figure" target="#fig_11">11</ref> shows two segmented images of the DRIVE database obtained using the linear SVM. In both cases the result corresponding to the best accuracy is considered: 0.9604 for the first image and 0.9703 for the second. The associated values of (TPR, FPR) are (0.7283, 0.0168) and (0.8346, 0.0175), respectively. The manual segmentations of the two observers A and B are also depicted.</p><p>Tables III and IV compare our approach with the most recent methods in terms of AUC and accuracy. The performance for the methods of Jiang et al. <ref type="bibr" target="#b10">[11]</ref> and Hoover et al. <ref type="bibr" target="#b20">[21]</ref> are taken from <ref type="bibr" target="#b15">[16]</ref>. The results for the matched filter method of Chaudhuri et al. <ref type="bibr" target="#b6">[7]</ref> and the matched filter based pixel classification are taken from <ref type="bibr" target="#b14">[15]</ref>. As concerns the methods of Mendonça et al. <ref type="bibr" target="#b12">[13]</ref> and Soares et al. <ref type="bibr" target="#b16">[17]</ref>, several variants have been proposed by the authors, so we report only the best results (for the method in <ref type="bibr" target="#b12">[13]</ref> the AUC is not available).</p><p>To summarize the results, we can say that the performance of the basic line detector, despite its simplicity, is comparable to or slightly better than some of the existing segmentation methods. The addition of the other two features and the use of the SVM classifier make our approach superior considering both AUC and accuracy. Even if the improvement is in the order of hundredths, this difference can be meaningful due to the huge number of pixels being classified. For both databases, the results obtained with a linear SVM have been also compared to those of a linear minimum squared error (LMSE) classifier. Even in this case the ROC curve is obtained varying the threshold as a free parameter. From Tables <ref type="table" target="#tab_1">III</ref> and<ref type="table" target="#tab_0">IV</ref>, it is easy to see that linear SVM clearly outperforms the LMSE, trained on the same training sets (20 000 data points), considering both AUC and accuracy. From our experiments we found that LMSE needs larger training sets to reach the same performance of SVM, as expected, due to the good generalization performance of the latter.</p><p>To ascertain the sensitivity of the SVM performance with respect to the training set choice, 20 different training sets of the same size (20 000 points) have been considered for the DRIVE database. The standard deviations for AUC and accuracy are 0.916 10 and 0.828 10 , respectively.</p><p>Figs. <ref type="figure" target="#fig_7">6</ref> and<ref type="figure" target="#fig_8">7</ref> show a small difference between ROC curves obtained with the line detector and with linear SVM, especially for the database DRIVE. A careful analysis reveals that the supervised method is useful in case of images containing artefacts due to pathologies. In the database STARE, in fact, there are ten images with pathologies, while for the test set of DRIVE we have only four. Moreover, in STARE the abnormal regions are wider. An example is given concerning an image from the STARE database: Fig. <ref type="figure" target="#fig_12">12</ref> shows the associated inverted green channel. The ROC curves for both methods are depicted in Fig. <ref type="figure" target="#fig_13">13</ref>. The AUC is 0.9491 for the linear SVM classifier while   it is only 0.9281 for the line detector. The difference is clearly visible from the segmented images shown in Fig. <ref type="figure" target="#fig_14">14</ref>. The first image is obtained with the linear SVM and corresponds to the value of giving the best accuracy (0.9395). The second image is obtained using the line detector: the associated accuracy is 0.9147. With the same TPR (0.63), we have far less false positives for the first method FPR with respect to the second FPR , making the linear SVM approach more robust to artefacts due to pathologies. The manual segmentation of the two observers is also depicted.</p><p>Finally, to investigate the dependence of the proposed pixel classification method on the dataset, we trained the classifier on each of the DRIVE and STARE databases and then we tested it on the other. The maximum accuracy is 0.9452 when training on DRIVE and 0.9266 in the second case. The performance is worse, as expected, due to the different image types (a similar behavior is observed in <ref type="bibr" target="#b16">[17]</ref>). Hence retraining of the classifier is needed when using a new dataset. However, this is not a problem with the proposed classification approach since the training set size is small, training is quite easy (no parametric model must be assumed, no local nonoptimal solutions can exist) and automatic ( should be tuned by the user but in practice it can be set to one with negligible effect on the performance).</p><p>V. DISCUSSION AND CONCLUSION In this paper, we have presented an effective retinal vessel segmentation technique based on simple line operators. In developing our method the usual pattern recognition paradigm has been followed: first feature extraction and then classification. The basic line detector is computationally simple and gives very good results with respect to existing unsupervised methods. The proposed operator retains some advantages of the matched filter <ref type="bibr" target="#b6">[7]</ref>, avoiding some of its drawbacks. The averaging implicit in the line strength computation reduces noise but it is simpler than a convolution with a two-dimensional kernel. Since no hypothesis is assumed on the cross-section profile, we obtain robust detection in presence of vessels with different size. Moreover, the one-pixel width of the rotating line gives good edge localization and increased selectivity between inside and outside pixels.</p><p>As concerns the proposed supervised approach, it requires fewer features with respect to existing methods in the literature <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. SVMs guarantee good generalization performance even with a small training set. Hence, the manual segmentation effort of the ophthalmologist is extremely reduced. Since a linear SVM has been chosen, the evaluation of the decision function in the test phase is very fast. Performance estimated both in terms of AUC and accuracy shows that our supervised method is slightly superior than the existing approaches. With respect to the line detector alone, in presence of abnormal regions, like hemorrhages, the supervised approach gives far less false positives producing cleaner segmented images.</p><p>From a visual inspection of the segmented images, vessels seem to have a few gaps. Therefore, our method is a good starting point to develop algorithms for accurate detection of the skeleton and the branching points <ref type="bibr" target="#b2">[3]</ref>. Concerning noise and other artefacts, our approach is quite robust but there is still room for improvement. In particular, some false positives are found around the border of the optic disk and in proximity of the pathological regions. Some problems occur also for the segmentation of the thinnest vessels or when the local contrast is quite low; in these cases, vessels can be captured only at the expense of an high FPR. In this condition, however, the segmented image tends to be quite noisy. Our ongoing work is devoted to facing these problems.</p><p>As a final remark, it should be pointed out that evaluating the classification performance pixelwise is not void of disadvantages. The importance of wide, well contrasted vessels is overestimated while false positive pixels are underestimated, due to the huge number of non-vessel pixels. So, a different widely accepted criterion which avoids the above-mentioned drawbacks would be desirable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Elisa</head><label></label><figDesc>Ricci and Renzo Perfetti* Abstract-In the framework of computer-aided diagnosis of eye diseases, retinal vessel segmentation based on line operators is proposed. A line detector, previously used in mammography, is applied to the green channel of the retinal image. It is based on the evaluation of the average grey level along lines of fixed length passing through the target pixel at different orientations. Two segmentation methods are considered. The first uses the basic line detector whose response is thresholded to obtain unsupervised pixel classification. As a further development, we employ two orthogonal line detectors along with the grey level of the target pixel to construct a feature vector for supervised classification using a support vector machine. The effectiveness of both methods is demonstrated through receiver operating characteristic analysis on two publicly available databases of color fundus images. Index Terms-Classification, line detector, retinal imaging, support vector machines (SVM), vessel segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Inverted green channel of non-mydriatic retinal image from DRIVE database.</figDesc><graphic coords="2,81.12,66.22,168.00,167.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Twelve orientations to evaluate line strength of shaded pixel.</figDesc><graphic coords="2,350.16,66.38,156.00,139.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Basic line detector. (b) Line detector with its orthogonal line.</figDesc><graphic coords="2,338.16,238.72,180.00,89.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Lines of 15 pixels with orientation: (a) 15 , (b) 30 , (c) 60 , and (d) 75 .</figDesc><graphic coords="3,73.62,66.18,180.00,168.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Inverted line strength of image in Fig. 1.</figDesc><graphic coords="3,73.62,275.38,180.00,185.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>examples of the classification problem, where and . In the case of linearly separable classes, there exists a separating hyperplane given by<ref type="bibr" target="#b1">(2)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. ROC curves for STARE database: line detector (dashed line) and linear SVM (solid line). Manual segmentation of second observer compared to first ().</figDesc><graphic coords="5,327.30,66.38,198.00,160.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. ROC curves for DRIVE database: line detector (dashed line) and linear SVM (solid line). Manual segmentation of set B compared to set A ().</figDesc><graphic coords="5,327.66,278.22,198.00,159.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Segmentation of image in Fig. 8 obtained using line detector.</figDesc><graphic coords="6,338.16,66.26,180.00,154.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Segmentation in presence of a central reflex: (a) patch from STARE database with a central reflex and (b) segmentation obtained using line detector (threshold is set to 15).</figDesc><graphic coords="6,344.16,257.78,168.00,250.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Example of segmentation results for two images of DRIVE database. (a) Vasculature reconstructed with linear SVM. (b) Manual segmentation of observer A. (c) Manual segmentation of observer B. Second image corresponds to Fig. 1.</figDesc><graphic coords="7,40.62,66.50,246.00,184.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Inverted green channel of non-mydriatic retinal image with pathology from STARE database.</figDesc><graphic coords="8,75.12,66.98,180.00,148.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. ROC curves for image in Fig. 12: line detector (dashed line) and linear SVM (solid line).</figDesc><graphic coords="8,66.12,261.38,198.00,157.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Vasculature segmentation of image in Fig. 12: (a) linear SVM, (b) line detector, (c) manual segmentation of the first observer, and (d) manual segmentation of second observer.</figDesc><graphic coords="8,39.60,464.96,252.00,67.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF LINE LENGTH-STARE TABLE II COMPARISON OF LINE LENGTH-DRIVE Fig. 8. Inverted green channel of retinal image from STARE database.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF SEGMENTATION METHODS-STARE TABLE IV COMPARISON OF SEGMENTATION METHODS-DRIVE</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Software, available at http://svmlight.joachims.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>It is available at http://www.parl.clemson.edu/stare/probing/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>It is available at http://www.isi.uu.nl/Research/Databases/DRIVE/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the owners of STARE and DRIVE databases for making their data publicly available, and Dr. F. Ricci from the Policlinico of Rome "Tor Vergata" for providing images used during this research. The authors would also like to thank O. Luzzi who carried out some computer simulations as part of his M.S. degree thesis.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Italian Ministry for Education and Scientific Research under PRIN Project 2004. Asterisk indicates corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Clinical Ophthalmology: A Systematic Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Kanski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Butterworth-Heinemann</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An economic analysis of interventions for diabetes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Klonoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes Care</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="390" to="404" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retinal vascular tree morphology: A semi-automatic quantification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Martinez-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Thom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="912" to="917" />
			<date type="published" when="2002-08">Aug. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multimodal registration algorithm of eye fundus images using vessels detection and Hough transform</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="419" to="428" />
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic detection of red lesions in digital color fundus photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S A</forename><surname>Suttorp-Schulten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="584" to="592" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="951" to="958" />
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection of blood vessels in retinal images using two-dimensional matched filters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chauduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="269" />
			<date type="published" when="1989-03">Mar. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recursive tracking of vascular networks in angiograms based on the detection-deletion scheme</title>
		<author>
			<persName><forename type="first">I</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="334" to="341" />
			<date type="published" when="1993-02">Feb. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fuzzy vessel tracking algorithm for retinal images based on fuzzy clustering</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Panas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="273" />
			<date type="published" when="1998-04">Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rapid automated tracing and feature extraction from retinal fundus images using direct exploratory algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Tanenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roysam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Technol. Biomed</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="138" />
			<date type="published" when="1999-06">Jun. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive local thresholding by verification based multithreshold probing with application to vessel detection in retinal images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mojon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="137" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Segmentation of vessel-like patterns using mathematical morphology and curvature evaluation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1010" to="1019" />
			<date type="published" when="2001-07">Jul. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmentation of retinal blood vessels by combining the detection of centerlines and morphological reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campilho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1213" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated localisation of the optic disc, fovea and retinal blood vessels from digital colour fundus images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sinthanayothin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Boyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Ophtalmol</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="902" to="910" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Comparative study of retinal vessel segmentation methods on a new publicly available database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<editor>SPIE Med. Imag., J. M. Fitzpatrick and M. Sonka</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5370</biblScope>
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ridge based vessel segmentation in color images of the retina</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2D Gabor wavelet and supervised classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V B</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J G</forename><surname>Leandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cesar</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linear structures in mammographic images: Detection and classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zwiggelaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Astley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R M</forename><surname>Boggis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1077" to="1086" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated asbestos fibre counting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inst. Phys. Conf. Ser</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="178" to="185" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Measurement of retinal vessel widths from fundus images based on 2-D modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1196" to="1204" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Controlling the sensitivity of support vector machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Veropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intelligence, T. Dean</title>
		<meeting>Int. Joint Conf. Artificial Intelligence, T. Dean<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making large-scale SVM learning practical</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods, Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining statistical learning with a knowledge-based approach-A case study in intensive care monitoring</title>
		<author>
			<persName><forename type="first">K</forename><surname>Morik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brockhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. Machine Learning (ICML-99)</title>
		<meeting>16th Int. Conf. Machine Learning (ICML-99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">RCV1: A new benchmark collection for text categorization research</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Luminosity and contrast normalization in retinal images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Foracchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruggeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
