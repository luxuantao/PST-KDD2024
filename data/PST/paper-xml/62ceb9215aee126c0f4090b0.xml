<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Risso</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Control and Computer Engineering</orgName>
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<postCode>10129</postCode>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Alessio</forename><surname>Burrello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Control and Computer Engineering</orgName>
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<postCode>10129</postCode>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical, Electronic, and Information Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<postCode>40136</postCode>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Francesco</forename><surname>Conti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Control and Computer Engineering</orgName>
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<postCode>10129</postCode>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical, Electronic, and Information Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<postCode>40136</postCode>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><forename type="middle">M</forename><surname>Risso</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Interuniversity Department of Regional and Urban Studies and Planning</orgName>
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<postCode>10129</postCode>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Interuniversity Department of Regional and Urban Studies and Planning</orgName>
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<postCode>10129</postCode>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Poncino</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Interuniversity Department of Regional and Urban Studies and Planning</orgName>
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<postCode>10129</postCode>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><forename type="middle">Jahier</forename><surname>Pagliari</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Interuniversity Department of Regional and Urban Studies and Planning</orgName>
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<postCode>10129</postCode>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Lamberti</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical, Electronic, and Information Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<postCode>40136</postCode>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">L</forename><surname>Benini</surname></persName>
							<email>lbenini@iis.ee.ethz.ch</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical, Electronic, and Information Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<postCode>40136</postCode>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lightweight Neural Architecture Search for Temporal Convolutional Networks at the Edge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TC.2022.3177955</idno>
					<note type="submission">received January XX, XXXX; revised January XX, XXXX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural Architecture Search</term>
					<term>Temporal Convolutional Networks</term>
					<term>Deep Learning</term>
					<term>Edge Computing</term>
					<term>Energy Efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) is quickly becoming the go-to approach to optimize the structure of Deep Learning (DL) models for complex tasks such as Image Classification or Object Detection. However, many other relevant applications of DL, especially at the edge, are based on time-series processing and require models with unique features, for which NAS is less explored. This work focuses in particular on Temporal Convolutional Networks (TCNs), a convolutional model for time-series processing that has recently emerged as a promising alternative to more complex recurrent architectures. We propose the first NAS tool that explicitly targets the optimization of the most peculiar architectural parameters of TCNs, namely dilation, receptive-field and number of features in each layer. The proposed approach searches for networks that offer good trade-offs between accuracy and number of parameters/operations, enabling an efficient deployment on embedded platforms. Moreover, its fundamental feature is that of being lightweight in terms of search complexity, making it usable even with limited hardware resources. We test the proposed NAS on four real-world, edge-relevant tasks, involving audio and bio-signals: (i) PPG-based Heart-Rate Monitoring, (ii) ECG-based Arrythmia Detection, (iii) sEMG-based Hand-Gesture Recognition, and (iv) Keyword Spotting. Results show that, starting from a single seed network, our method is capable of obtaining a rich collection of Pareto optimal architectures, among which we obtain models with the same accuracy as the seed, and 15.9-152? fewer parameters. Moreover, the NAS finds solutions that Pareto-dominate state-of-the-art hand-tuned models for 3 out of the 4 benchmarks, and are Pareto-optimal on the fourth (sEMG). Compared to three state-of-the-art NAS tools, ProxylessNAS, MorphNet and FBNetV2, our method explores a larger search space for TCNs (up to 10 12 ?) and obtains superior solutions, while requiring low GPU memory and search time. We deploy our NAS outputs on two distinct edge devices, the multicore GreenWaves Technology GAP8 IoT processor and the single-core STMicroelectronics STM32H7 microcontroller. With respect to the state-of-the-art hand-tuned models, we reduce latency and energy of up to 5.5? and 3.8? on the two targets respectively, without any accuracy loss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D EEP LEARNING (DL) models are at the core of many time-series processing applications. Notable examples are audio classification <ref type="bibr" target="#b0">[1]</ref>, bio-signals analysis <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and predictive maintenance <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. For many years, the stateof-the-art DL models for time-series-based tasks have been Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b5">[6]</ref>. Recently, however, new architectures have been proposed as viable alternatives to RNNs, such as Attention-based Transformers and Temporal Convolutional Networks (TCNs) <ref type="bibr" target="#b6">[7]</ref>. The latter, in particular, are uni-dimensional Convolutional Neural Networks (CNNs) specialized for time series, which have been shown to provide an accuracy comparable to RNNs, while providing computational advantages, namely higher arithmetic intensity, smaller memory footprint and more data reuse opportunities <ref type="bibr" target="#b6">[7]</ref>. Thanks to these features, TCNs are particularly interesting for edge computing applications, where inference is directly executed on Internet of Things (IoT) edge devices, rather than on centralized servers in the cloud. On-device inference requires small and efficient DL models, compatible with the limited memory spaces and tight energy budgets of edge nodes, while avoiding the transmission of raw data to the cloud provides many advantages, such as better privacy, higher energy efficiency and more predictable response latency <ref type="bibr" target="#b7">[8]</ref>.</p><p>To meet such tight constraints, however, selecting an efficient model such as a TCN is just the first step. Next, it is paramount to optimize its architectural hyperparameters based on the task at hand, so that the resulting network occupies as low memory and performs as few operations as possible to reach the desired accuracy level. Nowadays, rather than manually, such architectural optimization is increasingly performed with automatic Neural Architecture Search (NAS) tools. A plethora of different NAS approaches have been proposed in the last few years, and several of these works have targeted edge devices <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. However, to the best of our knowledge, none of them has focused specifically on models for time-series processing, nor specifically on TCNs, despite the unique features of these networks.</p><p>In fact, while TCNs share most of their key architectural features with standard CNNs, the peculiar 1D convolution operations at the heart of these networks increase the importance of some hyperparameters, such as the filters dilation and receptive field, resulting in a much larger variety in their values than what is common in 2D models for imageprocessing. Although there are NAS tools, originally designed for 2D CNNs, that can be easily extended to explore these parameters, they do so in a coarse-grain way, basically creating a different copy of all network layers for each architectural setting <ref type="bibr" target="#b12">[13]</ref>. This approach results in highly memory-and time-consuming searches, requiring 100s of GPU hours even for relatively simple tasks, which in turn translate into large energy wastes and CO2 emissions. In contrast, lightweight NAS approaches explore a finer-grain space with lower complexity, but they achieve this result at the cost of focusing only on the key characteristics of a specific model type, e.g., the number of channels in each layer of 2D CNNs for computer vision <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>In Risso et al. <ref type="bibr" target="#b13">[14]</ref>, we proposed the first lightweight NAS explicitly designed for optimizing TCNs by tuning the dilation hyperparameter. In this work, we extend and complete <ref type="bibr" target="#b13">[14]</ref> by including the optimization of the receptive field and of the number of channels of all convolutional layers in a TCN, as well as the number of neurons in Fully Connected layers, with a search time comparable to that of a single, standard training. Starting from a single seed model, our proposed tool, called Pruning In Time (PIT) can produce a rich set of Pareto optimal architectures in terms of number of operations/parameters versus accuracy. The following are the main contributions of our work:</p><p>? We frame the optimization of receptive field and dilation as a structured weight pruning, in which additional trainable masking parameters are added to different layer's weights so that their binarized values encode valid settings of the architectural hyperparameters. These masks are then trained with a regularizer to reduce the model complexity as much as possible while preserving accuracy. While similar masking approaches already exist for optimizing the number of channels in a 2D convolutional layer <ref type="bibr" target="#b10">[11]</ref>, our work is the first to extend this approach to filter size and dilation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We consider two different regularizers, targeting respectively the reduction of the number of parameters and of the number of inference operations. This allows us to enlarge and enrich the collection of Pareto architectures found by our NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We test and validate PIT on four benchmarks relative to real-world time-series processing tasks where TCNs are commonly employed and for which a deployment on edge devices is relevant: (i) PPG-Based Heart-Rate Monitoring; (ii) ECG-based Arrhythmia Detection; (iii) sEMG-based Hand-Gesture Recognition; (iv) Keyword Spotting. Results show that PIT can find multiple Pareto-optimal architectures starting from a single seed network, achieving 15.9-152? parameter reduction while maintaining the same accuracy of the seed. PIT is also capable of either matching or surpassing the accuracy and computa-tional cost of state-of-the-art hand-tuned networks. Furthermore, our approach Pareto-dominates three popular NAS tools developed for computer vision, thanks to the exploration of a larger search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We deploy some of the relevant Pareto-optimal solutions found for each task on two different edge devices, in order to measure their memory footprint, latency and energy consumption. The two considered platforms are the multicore GAP8 IoT processor <ref type="bibr" target="#b14">[15]</ref> and the single-core STM32H7 MCU <ref type="bibr" target="#b15">[16]</ref>. The deployment results show that, at iso-accuracy, solutions found by PIT reduce energy consumption and latency up to 5.45? on GAP8 and up to 3.83? on the STM32H7, compared to hand-tuned networks.</p><p>The code of PIT is released as open-source at: https://github.com/EmbeddedML-EDAGroup/PIT. The rest of the paper is structured as follows. Section 2 provides the required background and surveys some of the most relevant NAS methods proposed in the literature. Section 3 presents the proposed methodology. Section 4 details the target benchmarks while Section 5 discussed the obtained results, and Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporal Convolutional Networks</head><p>Temporal Convolutional Networks are 1-dimensional (1D) CNN variants that have recently gained significant traction for efficient time-series processing, obtaining state-of-theart results in several tasks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. With respect to RNNs and their successive evolutions, such as the Long-Short Term Memory (LSTM) and Gated Recurrent Unit (GRU), TCNs are less affected by training-time issues, such as vanishing/exploding gradients and the large amount of training memory required by RNNs for long input sequences. Moreover, they also have computational advantages at inference time, since they share the better data locality and arithmetic intensity of standard CNNs, which makes them latency-and energy-efficient <ref type="bibr" target="#b6">[7]</ref>.</p><p>The main building blocks of TCNs are the same ones found in standard CNNs, i.e. Convolutional, Pooling and Fully Connected (FC) layers. However, the convolutional layers of a TCN are characterized by causality and dilation, two properties that make them suited for temporal inputs.</p><p>Causality enforces that the outputs of convolutions do not violate the natural cause-effect ordering of events. In practice, the outputs y t of a TCN convolution only depend on a finite set of past inputs x [t-F ;t] , where t is a discrete index. Dilation is the mechanism used in TCNs for enlarging the receptive field of convolutions on the time axis, without requiring more trainable parameters and without increasing the number of operations required for inference. It is a fixed step d inserted between the input samples processed by each convolutional filter. Eq. 1 summarizes the 1D dilated convolution operation implemented by TCN layers:</p><formula xml:id="formula_0">y m t = K-1 i=0 Cin-1 l=0 x l ts-d i ?W l,m i , ?m ? [0, C out -1], ?t ? [0, T -1]</formula><p>(1) where x and y are the input/output activations, T is the output sequence length, W the array of filter weights, C in /C out the number of input/output channels, K the filter size and s the stride. We also define F = d ? (K -1) + 1 the receptive field of the layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Architecture Search</head><p>In recent years, several manually designed efficient and compact convolutional neural network architectures for edge devices have been proposed, including early MobileNets <ref type="bibr" target="#b19">[20]</ref>, ShuffleNets <ref type="bibr" target="#b20">[21]</ref>, EfficientNet <ref type="bibr" target="#b21">[22]</ref>, SqueezeNet <ref type="bibr" target="#b22">[23]</ref>, etc. While these models are very efficient, obtaining them required a long and time-consuming manual tuning of hyper-parameters, which has to be repeated from scratch when considering a different target task, or a different deployment target.</p><p>To solve this issue, many automated or semi-automated methods to optimize neural network architectures, easing the burden of designers, have been proposed. These approaches, generally denoted as Neural Architecture Search (NAS) algorithms, explore a large design space made of different combinations of layers and/or hyper-parameter values, selecting solutions that optimize a cost metric. The latter is often a function of both the accuracy of the network, and its computational cost (e.g., number of parameters or inference operations).</p><p>Table <ref type="table" target="#tab_0">1</ref> qualitatively compares some of the most relevant works in this field, in terms of search time, memory requirements during training (Mem.), search space size, and possibility to vary the topology (number and type of layers) of the resulting NNs. For Time and Mem., smaller is better, whereas for Search Space, larger is better. Early NAS tools were based on Reinforcement Learning (RL) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> or Evolutionary Algorithms (EA) <ref type="bibr" target="#b25">[26]</ref>. At each search iteration, these methods sample one or more architectures from the search space. Sampled networks are then trained to convergence to evaluate their accuracy (and possibly cost), which is then used to drive the next sampling. The repeated training in each iteration is the main drawback of these tools, for which a single search requires 1000s of GPU hours, even on relatively simple tasks. Accordingly, these methods are associated with large search time in Table <ref type="table" target="#tab_0">1</ref>. Memory occupation is low and comparable to a standard training, since each sampled architecture can be trained separately. The search space size is virtually unlimited, and these tools can easily support variable topologies. Notable exceptions are <ref type="bibr" target="#b8">[9]</ref>, which searches over a fixed convolutional topology of a variable number of layers without varying their type and the connections between them, and <ref type="bibr" target="#b23">[24]</ref>, that constrains its search space to a set of only 13 different layers per node.</p><p>To solve the search time issue of RL and EA methods, more recent Differentiable NAS (DNAS) approaches have proposed the so-called supernets <ref type="bibr" target="#b26">[27]</ref>. Supernets are DNNs that include all possible alternative layers to be considered during the optimization. For instance, a single supernet layer might include multiple Convolutional layers with different kernel sizes, operating in parallel. The problem of choosing a specific architecture is then translated into the problem of choosing a path in the supernet <ref type="bibr" target="#b26">[27]</ref>. The choice between the different paths is encoded with binary variables, jointly trained with the standard weights of the network using gradient-based learning. To search for accurate and efficient architectures, DNAS tools enhance the  <ref type="bibr" target="#b10">[11]</ref>. Mathematically, DNAS tools search for:</p><formula xml:id="formula_1">min W,? L(W ; ?) + ?R(?)<label>(2)</label></formula><p>where L is the standard loss function, W is the set of standard trainable weights (e.g., convolutional filters), ? is the set of additional NAS-specific trainable parameters that encode the different paths in the supernet, R is the regularization loss that measures the cost of the network and ? is a hand-tuned regularization strength, used to balance the two loss terms.</p><p>While DNAS algorithms are more efficient than early RL/EA-based solutions, training the entire supernet still requires huge computational resources both in terms of training time and memory occupation. This, in turn, translates in a reduction of the explored search space for practical DNASes such as <ref type="bibr" target="#b26">[27]</ref>, which have to limit the search to few alternatives per layer, in order to keep the memory occupation under reasonable bounds. The authors of <ref type="bibr" target="#b12">[13]</ref> have proposed ProxylessNAS, an advanced DNAS that reduces the memory requirements, keeping in memory at most two supernet paths for each batch of inputs. In ProxylessNAS, the normal weights and the additional parameters encoding supernet paths are trained and updated in an alternate manner. First, path parameters are frozen, and based on their current value, one sub-architecture of the supernet is stochastically sampled. Then, the weights of the sampled architecture are updated based on the training set. Second, the normal weights are frozen and the architectural parameters are trained on the validation set. This second phase updates two different paths at a time, sampling them from a multinomial distribution. In turn, this clever strategy allows ProxylessNAS to explore a significantly larger search space compared to other DNAS tools.</p><p>A further evolution in the direction of lightweight NAS is constituted by DMaskingNAS <ref type="bibr" target="#b9">[10]</ref>, fine-grain NAS <ref type="bibr" target="#b10">[11]</ref> and Single-Path NAS <ref type="bibr" target="#b27">[28]</ref> approaches. In these solutions, the supernet is replaced by a single, usually large, architecture with a unique path. Optimized architectures are found as modifications of this initial seed model, obtained tuning hyper-parameters, such as the number of channels in each layer <ref type="bibr" target="#b10">[11]</ref>. The key mechanism that enables this tuning within a normal training loop is the use of trainable masks, used to prune parts of the network. DMaskingNAS tools pursue the same DNAS objective of <ref type="bibr" target="#b1">(2)</ref>, where ? now represents the set of trainable masks. FBNet-V2 <ref type="bibr" target="#b9">[10]</ref>, for instance, uses a set of dedicated masks, each of which encodes a different number of output channels or a different spatial resolution, and is weighted with a trainable parameter. At the end of the search, the mask coupled with the largest parameter is used to determine the final architectural setting. Similarly, MorphNet <ref type="bibr" target="#b10">[11]</ref> exploit as masking parameters the pre-existing multiplicative terms of batch normalization layers <ref type="bibr" target="#b28">[29]</ref>. When these parameters assume a value lower than a threshold, the corresponding channels/feature maps from the preceding Convolutional layer are eliminated.</p><p>These approaches are more constrained than supernetbased ones in terms of NN topology. In fact, they do not allow to select between alternative layers (e.g., standard convolution versus depth-wise + point-wise convolution). On the other hand, they have two key advantages. First, they have much lower memory cost and search time, while still being able to find high-quality architectures. Crucially, the search time of a DMaskingNAS is comparable to a standard network training. Second, some DMaskingNASes (including our work) can explore the search space at a much finer grain. For example, MorphNet <ref type="bibr" target="#b10">[11]</ref> can easily select between 1 and 32 output channels in a Convolutional layer with a granularity of 1, by starting from a 32 channels seed layer, and eliminating those corresponding to the smallest batch normalization multiplicative parameters. Obtaining the same result with a standard DNAS would require a very large supernet, with 32 parallel convolutional layers. The masking and super-net approaches can also be combined, to bypass the limitations of DMaskingNAS <ref type="bibr" target="#b29">[30]</ref>.</p><p>The NAS literature referenced above focuses almost exclusively on 2D-CNNs for computer vision. None of the existing approaches has been applied to time-series processing tasks, despite the fact that a large amount of edge-relevant real-world tasks deal with uni-dimensional time-dependent signals (e.g., bio-signals, audio, energy-traces, sensor readings from industrial machines, etc). Our work tries to fill this gap, by proposing a novel DMaskingNAS that targets the optimization of 1D networks. Moreover, the working principles of our tool are general, and could form the basis for a more general NAS, able to explore temporal hyperparameters of arbitrary N-dimensional Convolutional layers (e.g., including also 3D-CNNs for spatio-temporal data processing), although this paper focuses exclusively on TCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>We name our proposed tool Pruning in Time (PIT), since it targets networks that process time-series, and the core mechanism of a DMaskingNAS is very similar to structured pruning <ref type="bibr" target="#b30">[31]</ref>. PIT explores the architectures of convolutional and fully-connected (FC) layers, the two most compute- and memory-expensive operations present in TCNs. For each convolutional layer, PIT jointly explores the number of channels (C out ), the receptive field (F ), and the dilation (d). Moreover, by tuning both F and d, it also indirectly affects the filter size K. To the best of our knowledge, no DMaskingNAS from literature has optimized the receptive field or the dilation, even for 2D-CNNs. Similarly, PIT can also optimize the number of output neurons of FC layers 1 .</p><p>We provide an overview of the search space explored by our tool and of its general working principle in Section 3.1. Then, we detail the mechanisms used to generate differentiable masks for each considered hyper-parameter in Sections 3.1.1-3.1.4. Finally, the two cost regularizers used to drive the search and the overall training procedure are described in Section 3.2 and 3.3 respectively. Table <ref type="table" target="#tab_1">2</ref> summarizes the main mathematical symbols used throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Search Space</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, PIT's search space encompasses all sub-architectures derived from a seed TCN by tweaking the three aforementioned hyperparameters. In particular, PIT can decide to reduce C out or F , and to increase d with respect to the seed, all of which have the effect of reducing the complexity and memory occupation of the layer.</p><p>To achieve this objective, each convolutional/FC layer of the seed is modified to become a function L n (W (n) ; ? (n) ) of its original weights tensor W (n) and of a new set of architectural parameters ? (n) . For a TCN with N layers, the search space of PIT is therefore defined by the set:</p><formula xml:id="formula_2">S = {L n (W (n) ; ? (n) )} N -1 n=0 (3)</formula><p>1. This can be seen as a corner case of the Cout optimization, since FC layers are just a special case of 1D convolutions with</p><formula xml:id="formula_3">F = K = d = 1</formula><p>and Cout equal to the number of output neurons. Accordingly, the rest of this section describes PIT's functionality for convolutions. During the search, the elements of ? (n) are properly combined to form a binary mask ? (n) , which is used to prune a portion of the layer weights. In practice, an architecture ? is sampled from S in each search iteration, by performing the Hadamard product between W (n) and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1D Convolu?onal Kernel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subsets of the kernel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smaller number of output channels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smaller recep?ve field</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Larger dila?on factor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed Network</head><formula xml:id="formula_4">? (n) , i.e., ? = {L n (W (n) ? (n) )} N -1</formula><p>n=0 . This eliminates the portions of W (n) that correspond to 0-valued mask elements, effectively letting the seed layer produce the same output that would be obtained with a smaller number of channels or receptive field, or with a larger dilation. The way in which ? (n) is generated from ? (n) to produce this effect is the topic of Sections 3.1.1-3.1.3.</p><p>Having binary masks is required to either completely eliminate slices of W (n) (with value 0) or keep them untouched (with value 1) when sampling an architecture with the Hadamard product. In practice, this corresponds to sampling only feasible architectures (with integer C out , F and d). To this end, we binarize ? (n) in the forward-pass of our search/training, applying an Heaviside step function with a fixed threshold th = 0.5.</p><p>At the same time, we also need to make the ? (n) ? ? (n) transformation differentiable, in order to embed the search into the standard gradient-based training of the network, learning contextually both the weights W (n) and the architectural parameters ? (n) . To cope with the Heaviside function derivation issues, i.e., derivative equal to 0 almost everywhere and not existent in ?, we follow the approach proposed in BinaryConnect <ref type="bibr" target="#b31">[32]</ref>, based on a Straight-Through Estimator (STE). Accordingly, during the backward-pass, the step function is simply replaced with an identity.</p><p>For notation simplicity, in the rest of the section we divide ? (n) parameters in three groups: ? (n) , used to tune the number of channels, ? (n) , which tune the receptive field, and ? (n) , which affect the dilation factor. We also drop the superscript (n) when not needed. In PIT, each of these three groups of parameters is used to generate an independent binary mask, which can be then combined with the other two. Having independent masks for C out , F and d, gives PIT the flexibility to optimize the three hyper-parameters either separately or jointly. At most, during a joint search, PIT explores: out,seed = 128 ?n, this corresponds to evaluating ? 10 32 architectures in a single training.</p><formula xml:id="formula_5">|S| ? N -1 n=0 (C (n) out,seed ? F (n) seed ? log 2 (F (n) seed ) )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Channels Search</head><p>To explore the number of channels in each convolutional layer, we take inspiration from <ref type="bibr" target="#b10">[11]</ref>. In that work, the parameters of batch normalization (BN) layers <ref type="bibr" target="#b28">[29]</ref> were transformed into binary masks to prune entire output channels and explore the space of all sub-layers with C out &lt; C out,seed . However, requiring the presence of a BN layer after each convolution, although common in modern 2D-CNNs, still limits the applicability of the approach of <ref type="bibr" target="#b10">[11]</ref>. Therefore, in PIT, we decouple the channel search from BN, and instead we exploit a dedicated trainable set of parameters ? to zero-out entire filters from the W tensor of convolutional layers. PIT treats each output channel independently. So, it uses an ? array of length C out,seed , and it generates binary masks simply as:</p><formula xml:id="formula_6">? A = H(|?|) (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where H is the Heaviside binarization. Then, the layer function defined in ( <ref type="formula">1</ref>) is modified to:</p><formula xml:id="formula_8">?m t = K-1 i=0 Cin-1 l=0 x l ts-di ? (? A,m ? W l,m i )<label>(6)</label></formula><p>In practice, each binarized mask element is multiplied with all the weights of the same convolutional filter, i.e., with an entire slice of the weights tensor over the output channels axis. Each filter multiplied with a 0-mask effectively removes the corresponding output channel from the layer. Figure <ref type="figure" target="#fig_1">2</ref> depicts the application of ? A parameters to a simple layer with C out,seed = 4. Noteworthy, besides reducing the number of channels, PIT can also eliminate entire layers from the network, if the latter includes skip-connections. In particular, if all the ? A,m of a convolutional layer are zeroed-out, then the inputs only flow through the skip connection, effectively reducing the number of the layers in the network by one. If skip connections are not present, instead, at least one output channel is always kept active to avoid breaking the network connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolu?onal Kernel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Channels Masks Output Examples</head><p>Our channels search scheme differs significantly from existing DMaskingNAS such as FBNetV2 <ref type="bibr" target="#b9">[10]</ref>, since we mask weights tensors rather than output activations. Fundamentally, as explained below, this makes our method easily extensible to the exploration of other hyper-parameters such as F and d, which would be much more difficult to optimize with an activations mask. Moreover, we use independent binarized parameters to mask each channel, rather than a set of predefined masks with an increasing number of trailing 0s, combined via Gumbel Softmax, as done in <ref type="bibr" target="#b9">[10]</ref>. This, in turn, means that we can eliminate any combination of channels, not just the trailing ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Receptive Field Search</head><p>The second critical hyperparameter that we explore is the receptive field F , i.e., the range of input time-steps involved in a convolution. In standard convolutions, the receptive field is equal to the filter size (F = K). However, as detailed in Section 2.1, this no longer holds for TCNs when the dilation factor d is &gt; 1, and the general relation becomes: F = (K -1)?d+1. As noted at the beginning of this section, by exploring both F and d, PIT also indirectly optimizes the filter dimension K.</p><p>The receptive field is explored using an array of additional trainable parameters ? of length F seed . Differently from the output channels, however, the ? need to be further combined to define the corresponding binary differentiable masks. The reason is that, to "simulate" the effect of a smaller receptive field through masking, it is not sufficient to mask any set of time-slices in the weights tensor: in a causal TCN convolution, the receptive field extends exclusively in the past. Thus, the slices that should be eliminated are always the "oldest" ones, i.e., those that are multiplied with input time-steps that are farthest in the past. To do so, we derive elements of the binary masks ? B from ? as:</p><formula xml:id="formula_9">? B,i = H ? ? F seed -i j=1 |? F seed -j | ? ?<label>(7)</label></formula><p>Each ? B,i is then multiplied with a time-slice of the W tensor during the forward-pass, as shown in Figure <ref type="figure">3</ref>. Therefore, when searching for the receptive field, (1) becomes:</p><formula xml:id="formula_10">y m t = K-1 i=0 Cin-1 l=0 x l ts-di ? (? B,di W l,m i )<label>(8)</label></formula><p>Thanks to the construction of ( <ref type="formula" target="#formula_9">7</ref>), we have that if i &gt; j, then ? B,i ? ? B,j . In turn, this ensures that the first weight slices to be pruned are always the leftmost ones, as shown in the example on the right of Figure <ref type="figure">3</ref>. Importantly, ? 0 is always kept constant and equal to 1. This ensures that, once binarized, ? B,0 is also always = 1, and consequently, that all convolutions take at least one time-step as input.</p><p>In practice, for efficiency reasons, we generate binary masks using the matrix transformation:</p><formula xml:id="formula_11">? B = H (C ? ? |?|)<label>(9)</label></formula><p>where C ? is a constant upper triangular matrix of 1s generated once at the beginning of a search, as shown on the left of Figure <ref type="figure">4</ref>.  </p><formula xml:id="formula_12">W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Dilation Search</head><p>Lastly, PIT also explores the dilation factor d. Similarly to the receptive field, also searching for dilation imposes some constraints on the portions of the weights tensor that should be pruned by our NAS. In particular, we need to ensure that only regular dilation factors are generated, i.e., that the time-steps gaps between consecutive convolution inputs are all equal for a given layer. For example, we do not want to obtain a layer that takes as input time-steps t, t -1, t -3, and t -10, corresponding to gaps of 0, 1, and 6 time-steps respectively. In fact, such a layer would not be supported by most inference libraries, in particular those for edge devices <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, which only implement regular dilation, as the latter enables more regular memory access patterns and better low-level optimizations.</p><p>Based on these observations, we follow an approach similar to the one described in Section 3.1.2. We start from an array of trainable parameters ?, which are then combined to compose differentiable binary masks 2 . Our method only supports power-of-2 dilation factors which, besides being the most commonly used values, also simplify the generation of the masks. Thus, we have: len(?) = log 2 (F seed ) .</p><p>In order to obtain the elements of ? ? , we pass through 2. In our preliminary work of <ref type="bibr" target="#b13">[14]</ref> we used a different mechanism to generate dilation masks as a product of ? elements instead of a sum. However, we found that this new approach is superior as it does not introduce nonlinear terms in the ? gradients. an intermediate array ?, generated similarly to <ref type="bibr" target="#b6">(7)</ref>:</p><formula xml:id="formula_13">W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 Convolu?onal Kernel W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 , W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0</formula><formula xml:id="formula_14">? i = H ? ? len(?)-i j=1 |? len(?)-j | ? ?<label>(10)</label></formula><p>Then, the mask is obtained by further reorganizing the ? i values into the vector ? ? , of length F seed , as follows:</p><formula xml:id="formula_15">? ?,i = ? k(i) , with k(i) = len(?) p=1 1 -?(i mod 2 p , 0)<label>(11)</label></formula><p>and where ?() is Kronecker's Delta function. This reorganization ensures that the ? element with the largest index (? len(?)-1 ) ends up in all positions corresponding to timesteps that would be skipped by a layer with d = 2. Similarly, the element with the second largest index ends-up in positions that are skipped when using d = 4, and so on. This, combined with the fact that, by construction of (10), it holds that ? i ? ? j for i &gt; j, ensures that the dilation is progressively increased. In other words, each new ? i binarized to 0 increases the dilation by a factor 2. The obtained ? ? vector is multiplied with the W tensor, exactly as in <ref type="bibr" target="#b8">(9)</ref>, after setting the seed layer dilation to 1. Again, we fix ? 0 = 1 to ensure that we never prune the entire convolution. An example of how the tensor is generated and of its effect on the dilation is shown in Figure <ref type="figure" target="#fig_3">5</ref>. In practice, similarly to the receptive field mask, also ? ? is obtained from ? with a simple matrix multiplication:</p><formula xml:id="formula_16">? ? = H(C ? ? |?|)<label>(12)</label></formula><p>where C ? is a constant matrix composed of 0s and 1s that can be generated procedurally based on the value of F seed . An example of C ? is shown on the right of Figure <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Joint Search</head><p>In order to jointly optimize all three aforementioned hyperparameters, we simply apply all three ? masks to the weight tensor of a layer. Therefore, the equivalent of equation ( <ref type="formula">1</ref>) for a seed convolutional layer during a joint search is:</p><formula xml:id="formula_17">y m t = K-1 i=0 Cin-1 l=0 x l ts-i ? (? B,i ? ?,i (? A,m ? W l,m i ))<label>(13)</label></formula><p>Note that, as anticipated in Section 3.1.3, we set the seed layer dilation to 1, since we want to let PIT explore all possible d values. In our experiments, we found that performing such a joint search yields superior results with respect to optimizing the three hyper-parameters sequentially, since PIT can take into account the complex interactions among them (especially among F and d), see Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Regularization</head><p>Following the same approach of state-of-the-art DNASes <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> PIT searches for accurate yet low-complexity architectures by combining the task-specific loss function L with a regularization term R as in <ref type="bibr" target="#b1">(2)</ref>. The additional differentiable term encodes a prior in the loss landscape that directs the optimization towards low-cost solutions. The two cost metrics considered in this work are the number of parameters (or size) of the model, and the number of operations (OPs) for an inference.</p><p>The corresponding two regularizers R size and R ops are differentiable functions of the pre-binarization masks ?A , ?B and ?? , i.e., the outputs of ( <ref type="formula" target="#formula_6">5</ref>), ( <ref type="formula" target="#formula_11">9</ref>) and ( <ref type="formula" target="#formula_16">12</ref>) but without the Heaviside binarization. The latter, in turn, depend on the trainable architectural parameters ?, ? and ?. We use pre-binarization masks as in <ref type="bibr" target="#b10">[11]</ref>, because this yields a smoother loss landscape, improving convergence. The details of the two regularizers are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Size Regularizer</head><p>The Size Regularizer R size estimates, during each forwardpass, the effective number of parameters of the network, based on the values of the differentiable binary masks.</p><p>The number of parameters of a convolutional layer, i.e., the size of weight tensor W , is equal to</p><formula xml:id="formula_18">C in ? C out ? K.</formula><p>Accordingly, we define the size regularizer for a TCN with N convolutional (or FC) layers as:</p><formula xml:id="formula_19">R size = N-1 n=0 (R (n) size ) = N-1 n=0 C (n-1) out,ef f ? C (n) out,ef f ? K (n) ef f<label>(14)</label></formula><p>where:</p><formula xml:id="formula_20">C (n) out,ef f = C (n) out,seed -1 i=0 ?(n) A,i<label>(15)</label></formula><p>is the effective number of channels in the n-th layer, and:</p><formula xml:id="formula_21">K (n) ef f = F (n) seed -1 i=0 ?(n) B,i F seed -i ? ?(n) ?,i len(?) -k(i)<label>(16)</label></formula><p>is the effective kernel size, which depends both on the total receptive field and on the dilation. For the 1st layer of the network, C</p><p>(n-1)</p><p>out,ef f is constant and equal to the number of channels of the input signal.</p><p>The definitions of ( <ref type="formula" target="#formula_20">15</ref>) and ( <ref type="formula" target="#formula_21">16</ref>) are continuous relaxations of the number of active (non-pruned) channels and time-slices of W (n) respectively. By minimizing R size , PIT is encouraged to reduce the ? values, bringing them below the binarization threshold. Depending on the regularization strength ? of (2) PIT balances the corresponding reduction in cost with the accuracy drop caused by eliminating W slices from the layer, reducing only the ? elements associated to unimportant slices.</p><p>The denominators in ( <ref type="formula" target="#formula_21">16</ref>) are needed to make sure that, when ? and ? are equal to 1 (i.e., the initialization value, ef f corresponds to the real filter size of the seed. In fact, each ?B/? is obtained as sum of a different number of ? (or ?) elements. As a result, without normalization, the estimated cost would be higher than the real filter size. For instance, in a layer with F seed = 5 and with all ?/? initialized at 1, without the denominators, we would have K ef f = 33, which is clearly incorrect. Conversely, with the denominators, we have K ef f = 5 = F seed , which is correct, since the initialization of ? = 1 implicitly imposes d = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">OPs Regularizer</head><p>The second proposed regularizer R ops estimates the number of operations required to perform an inference. Since the number of OPs of a 1D convolutional layer is T ? C in ? C out ? K, where T is the output sequence length defined in (1), the regularizer expression is simply:</p><formula xml:id="formula_22">R ops = N n=1 (R (n) size ? T (n) )<label>(17)</label></formula><p>In practice, when targeting the reduction of the total OPs for inference, the only difference in the regularizer is that the cost of each layer is weighted by the output sequence length. This is particularly important in presence of layers such as pooling, strided convolution, etc., which significantly reduce T , and consequently the number of OPs for the downstream part of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Procedure</head><p>Algorithm 1 summarizes the three main phases of a PIT architecture search. The first phase consists of Steps wu iterations of warmup. At this stage of the algorithm, all ? parameters (i.e., ?, ? and ?) are initialized to 1 and frozen. Accordingly, all elements of the binary masks ? are also binarized to 1. Therefore, warmup coincides with a normal training of the seed network, where the only objective is minimizing the task loss function L. The number of warmup iterations is a user-defined parameter. In practice, in all our experiments, we warm up to convergence. The second phase is where the actual NAS takes place. In the search loop, the model weights W and the architectural parameters ? are optimized simultaneously. Accordingly, the goal of this phase is to minimize the sum of the taskspecific loss L and of one of the two the regularization losses R discussed in Section 3.2, weighted by the regularization strength ?. The duration of the search phase is controlled by an early-stop mechanism which monitors the value of L on an unseen validation split of the target dataset, and stops the search when the latter does not improve for 20 epochs.</p><p>Finally, in the third and last phase the ? parameters, and corresponding ? binary masks, are frozen to their latest values. This corresponds to sampling from the search space the architecture that PIT determined as optimal during the previous phase. Then, the weights W of the selected network are fine-tuned or re-trained from scratch, considering only the L loss.</p><p>In order to obtain different Pareto points in the accuracy versus cost (size or OPs) space with PIT, it suffices to repeat Algorithm 1 changing the regularization strength ?. More precisely, the warmup phase can be performed just once, saving the final weights of the seed network. Overall, Algorithm 1 has a complexity that is comparable to a single TCN training. Moreover, the requirements in terms of GPU time and memory are greatly reduced with respect to a supernet-based DNAS. Therefore, obtaining 10s of Pareto points by changing ? still has a manageable cost, as shown for example by the results of Figure <ref type="figure" target="#fig_6">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BENCHMARKS</head><p>We test PIT on four edge-relevant real-world benchmarks. We select diverse benchmarks to comprehensively evaluate the effectiveness of the proposed NAS. Specifically, we consider regression as well as classification tasks; the inputs analyzed are both raw or extracted features, and the TCNs employed as seed are based on different architectural styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PPG-based Heart-Rate Monitoring</head><p>The first benchmark deals with Heart-Rate (HR) monitoring on wrist-worn devices, using Photoplethysmography (PPG) sensors coupled with tri-axial accelerometers to mitigate the effect of motion artifacts <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>. We target the PPG-Dalia <ref type="bibr" target="#b34">[35]</ref> dataset, and the task is formulated as a regression of the HR value, whose ground truth is derived with ECG measurements. All results refer to the same input windowing and cross-validation scheme proposed in <ref type="bibr" target="#b34">[35]</ref>.</p><p>The seed network for this task is TEMPONet, a TCN originally proposed in <ref type="bibr" target="#b1">[2]</ref> and later used for HR monitoring with state-of-the-art results in <ref type="bibr" target="#b16">[17]</ref>. The network is composed of three feature extraction blocks and a final regressor module with three FC layers. Each feature extraction block is made of three convolutional layers with BatchNorm and ReLU activation, followed by an average pooling. The FC layers are also followed by BatchNorm and ReLU, and by a dropout layer with 50 % rate. With respect to the original TEMPONet, our seed is obtained doubling the receptive field of all convolutions and setting the dilation to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ECG-based Arrhythmia Detection</head><p>Our second benchmark deals with Electrocardiogram (ECG)-based arrhythmia detection, for wearable medical devices. We target the ECG5000 dataset <ref type="bibr" target="#b35">[36]</ref>, and the task consists in classifying the ECG signals in 5 classes: Normal, R-on-T Premature Ventricular Contraction, Premature Ventricular Contraction, Supraventricular Premature or Ectopic beat, and Unclassified Beat.</p><p>The reference TCN is ECGTCN, originally proposed in <ref type="bibr" target="#b36">[37]</ref>. Differently from TEMPONet, ECGTCN is based on residual blocks. It has a first convolutional layer that enlarges the number of input channels, followed by three modular blocks, each including two dilated convolutions with ReLU activation, BatchNorm and 50% dropout. The input and output feature maps of each block are then summed together. When the number of input and output channels differs, the residual path also includes a point-wise convolution (i.e., K = 1) in order to adapt the tensor sizes. The PIT seed is obtained from ECGTCN, setting the dilation of all layers to 1, while keeping the original receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">sEMG-based Hand-Gesture Recognition</head><p>The third benchmark deals with hand-gesture recognition based on surface electromiography (sEMG) signals. For this task, we target the NinaPro DB1 dataset <ref type="bibr" target="#b37">[38]</ref>, which includes 52 heterogeneous gesture classes, using the same data pre-processing and augmentation described in <ref type="bibr" target="#b18">[19]</ref>.</p><p>The seed network is TCCNet, originally proposed in <ref type="bibr" target="#b18">[19]</ref>. The architecture includes three feature extraction blocks, each composed of two dilated convolutions with ReLU and dropout (5% rate) and a residual branch with a point-wise convolution. The classifier includes an attention layer of the type described in <ref type="bibr" target="#b38">[39]</ref> and a final FC layer with 53 output neurons (52 hand-gestures + 1 unknown class). The PIT seed is obtained simply setting the dilation to 1 in all layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Keyword Spotting</head><p>Our last benchmark is keyword spotting (KWS). We target the Speech Commands v2 dataset <ref type="bibr" target="#b39">[40]</ref>, following the preprocessing scheme proposed by the MLPerf Tiny benchmark suite <ref type="bibr" target="#b40">[41]</ref> which produces 12 possible labels, including 10 words and two special classes for "unknown" and "silence".</p><p>As seed, we use the TCN presented in <ref type="bibr" target="#b17">[18]</ref>, called TC-ResNet14. The main difference with the other reference TCNs is that the original TC-ResNet14 did not use dilation, and the modular convolutional blocks alternate plain convolutions with strided convolution with s = 2. PIT's seed is obtained doubling the receptive field in each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>This section discusses the results obtained by PIT on the four aforementioned benchmarks. In particular, in Section 5.1, we present the global results of our NAS search in the accuracy versus number of parameters and accuracy versus number of OPs planes. In Section 5.2 we conduct ablation studies on one of the benchmarks, and in Section 5.3 we compare our approach with a state-of-the-art DNAS, ProxylessNAS <ref type="bibr" target="#b12">[13]</ref>, and with two state-of-the-art DMaskingNAS approaches, namely, MorphNet <ref type="bibr" target="#b10">[11]</ref> and FBNetV2 <ref type="bibr" target="#b9">[10]</ref>. Since the code for <ref type="bibr" target="#b9">[10]</ref> is not publicly available, we re-implemented it based on the information provided in the paper. Finally, Section 5.4 presents the memory, latency and energy consumption results obtained deploying some of the networks found by PIT on two commercial edge devices.</p><p>PIT is written in Python (v3.6) and it is based on PyTorch (v1.7.1). All our training experiments and NAS searches are performed on a single NVIDIA TITAN Xp GPU with 12GB memory. The two deployment targets considered are: i) the multicore GAP-8 IoT processor by GreenWaves Technologies <ref type="bibr" target="#b14">[15]</ref> and ii) the single-core STM32H7 MCU by STMicroelectronics <ref type="bibr" target="#b15">[16]</ref>. As inference software backend, we use the open-source layers library of <ref type="bibr" target="#b41">[42]</ref> coupled with the tiling tool of <ref type="bibr" target="#b42">[43]</ref> for GAP-8, and the CMSIS-NN library [44] for the STM32H7. All deployed networks are quantized to 8-bit, using PyTorch's built-in quantization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Search Space Exploration</head><p>Figure <ref type="figure">6</ref> shows the results of applying PIT to the four benchmarks. The graphs report the TCNs accuracy (for classification tasks) or Mean Absolute Error (MAE, for regression tasks) on the x axis, and the number of parameters or OPs per inference on the y axis. The curves correspond to the outputs of PIT, where different points are obtained varying the regularization strength ? and considering both size and OPs regularizers. Moreover, each plot also reports the metrics of two additional TCNs. Black triangles correspond to the results obtained by the hand-tuned state-of-theart TCNs directly taken from <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref>, with the original number of channels, receptive fields, and dilation factors. Black squares, instead, indicate the metrics of the PIT seeds, i.e., the same networks modified as described in Section 4 (setting d = 1 everywhere, etc.) to enlarge the PIT search space.</p><p>The upper-left part of Figure <ref type="figure">6</ref> reports the results on the PPG-DaLia dataset for the PPG-based HR monitoring task. This is the only regression task considered, so the network performance is measured with the MAE, for which lower values are better. As shown by the graphs, starting from a single seed network, PIT is able to obtain a rich collection of Pareto-optimal architectures, spanning more than one order of magnitude both in terms of parameters (4.7k-78k) and OPs (0.27M-9.6M). Notably, PIT networks dominate in the Pareto sense both the seed architecture and the handtuned state-of-the-art TEMPONet. In particular, we obtain a similar MAE to the seed TCN (5.38 vs 5.40 BPM), with 120.0? less parameters and 96.0? less operations. Moreover, PIT also finds a new state-of-the-art deep learning model for this task, achieving a MAE of just 5.03 BPM while requiring only 53k parameters and 5.1M OPs, improving the best performing architecture proposed in <ref type="bibr" target="#b16">[17]</ref> 3 requiring 8.03? and 5.42? less parameters and OPs.</p><p>The Upper-right pair of charts shows the results obtained on the ECG5000 dataset for Arrhythmia Detection. PIT results span almost one order of magnitude in parameters (0.91k-5.36k) and OPs (50.3k-293.5k). Moreover, both the seed network and the hand-tuned one are Paretodominated. The best performing architecture found by our NAS improves the accuracy of the hand-tuned network (+1.03%) reducing both the number of parameters (-64.7%) and the FLOPs (-85.8%).</p><p>Lower-left part of Figure <ref type="figure">6</ref> shows the results obtained for the sEMG-based Hand-Gesture Recognition task on the NinaPro-DB1 dataset. The richness and diversity of the found architectures in terms of size and number of OPs are similar to the previous two benchmarks. However, while PIT results still dominate the seed, in this case the handtuned TCNNet sits on the Pareto front. Indeed, the PIT network that is nearest to the hand-tuned architecture on the 3. Note that this result is achieved without applying any additional post-processing as described in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed Network</head><p>Hand curve, achieves a slightly lower accuracy (-0.47%) traded-off with a reduction of size (-3.33%). This result demonstrates the goodness of the original TCNNet proposed in <ref type="bibr" target="#b18">[19]</ref> but, at the same time, it shows the good quality of the architectures found by our NAS, which despite starting from an oversized seed, is still able to produce optimized networks that closely resemble those tuned by experts. Lastly, the lower-right part of Figure <ref type="figure">6</ref> shows the two Pareto fronts obtained on the Google Speech Commands dataset for Keyword Spotting. Once again, we largely outperform both the seed and the hand-tuned TCNs. Specifically, the most accurate PIT architecture slightly improves the accuracy of the hand-tuned network (+0.36%) while greatly reducing both the number of parameters (-82.53%) and FLOPs (-44.53%). Moreover, we obtain Pareto points that span 10k-98k parameters and 0.87M-3.98M OPs. It is important to note that the bad performance obtained by the seeds (black squares) for all four benchmarks is due to over-fitting, which in turn is caused by the large number of channels and receptive fields, and the absence of dilation.</p><p>Table <ref type="table" target="#tab_5">3</ref> reports the range of regularizer strengths ? used on the four benchmarks to obtain these results. In general, ? should be set so that the two additive terms in the loss (L and ?R) assume comparable values at the beginning of a training. This ensures that PIT takes into account both accuracy and inference cost in its search, without degenerating to one of the two corner cases, i.e., accuracy-drivenonly and cost-driven-only optimization. The corresponding values of ? vary for different tasks, as shown in the table. However, we found that a good rule of thumb, which works for all benchmarks, to identify the order of magnitude of the regularization strength is to start from ? = 1/(Seed Model Size). Then, based on the results of a PIT search with this initial value, one can decide to increase/decrease ? to obtain smaller/more accurate TCNs respectively. By monitoring the loss in the initial epochs, it is also very easy to detect when the NAS is falling in one of the corner cases (one term much larger than the other) and stop the search immediately, without wasting training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies</head><p>This section analyzes the impact of some of the most important PIT parameters. Due to space limitations, we report the results of this study only for the PPG-based HR monitoring benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Hyper-parameters</head><p>Figure <ref type="figure">7</ref> analyzes the contribution of different hyperparameters to the quality of results found by PIT. For this experiment, we use the R size regularizer and consider solutions in the MAE versus number of parameters space. We then repeat the NAS search 3 times. In each run, we freeze two of the three sets of architectural parameters (?, ? and ?) to 1, letting PIT tune the third set. This gives us: i) the results of a search that only optimizes the number of channels in each layer (Ch-Only), performed on a TCN with maximal receptive field and d = 1, ii) the results of a receptive fieldonly search (Rf-Only), on a TCN with maximal C out and d = 1, and iii) the results of a dilation-only search (Dil-Only) on a network with maximal F and C out . The Pareto fronts obtained in each of these 3 conditions by varying the regularization strength ? are shown in the figure, together with the output of a complete search that optimizes all three hyper-parameters simultaneously (All-in-One).</p><p>The results clearly show that the main source of parameters reduction and performance improvement is the search along the channels dimension. This is probably due to the fact that the channels represent a large source of redundancy in hand-tuned TCNs, since their number is typically set using common heuristics, irrespective of the target task (e.g., C out multiple of 32, progressively increasing along the depth of the network). However, Figure <ref type="figure">7</ref> also shows that optimizing only the number of channels is not sufficient, and that a combined optimization that also consider receptive field and dilation can yield Pareto-optimal networks across the entire MAE/parameters range. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Regularizers</head><p>Figure <ref type="figure">8</ref> compares the Pareto fronts obtained using the R size regularizer (with orange stars) and R ops regularizer (with green diamonds). Note that the PPG-based HR monitoring benchmark is the one for which the distinction between model size and number of OPs is most relevant, due to the presence of several layers (average pooling and strided convolution) that modify the activation array length T .</p><p>The Figure shows that, as expected, the majority of the Pareto points in the MAE versus number of parameters plane are produced when using the R size regularizer, with the few exceptions being due to local minima. Vice versa, the R ops regularizer tends to generate superior solutions in terms of MAE versus number of OPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with state-of-the-art NAS tools</head><p>Figure <ref type="figure">9</ref> compares the Pareto fronts obtained with PIT and three state-of-the-art NAS tools, namely ProxylessNAS <ref type="bibr" target="#b12">[13]</ref> MorphNet <ref type="bibr" target="#b10">[11]</ref> and FBNetV2 <ref type="bibr" target="#b9">[10]</ref>, on the HR monitoring benchmark. Results show that PIT outperforms all three across the entire design space, except for one MorphNet and one FBNetV2 point, that achieve a very low number of operations, although at the cost of a quite large MAE. The main reason for the superior results of PIT is the fact that our NAS explores a larger and finer-grain search space with respect to the baselines. For what concerns MorphNet and FBNetV2, this is partly due to the intrinsic nature of those tools, which cannot explore receptive field, nor dilation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Accordingly, F and d in their respective seeds have been set to the hand-tuned values of the state-of-the-art network. This different search starting point compared to PIT is the reason why, in the low-size/high-MAE regime, these tools find a single Pareto-optimal point.</p><p>For FBNetV2, we considered a Coarse search space, including 4 C out alternatives per layer, uniformly spaced, i.e., 1 /4C out,seed , 1 /2C out,seed , 3 /4C out,seed and C out,seed . and a  Fine search space, which instead evaluates all C out values with a granularity of 1. The latter is more similar to PIT, but the former achieves superior results in most cases. This is because FBNetV2 uses a pre-defined binary mask for each layer variant, combining them through a Gumbel softmax, as explained in Sec. 3.1.1. Experimentally, we found that with a too large number of masks, the search becomes unstable and yields sub-optimal results. In contrast, PIT does not have this limitation since it uses independent trainable masks that keep or eliminate an individual channel. ProxylessNAS, being a super-net-based DNAS, would be virtually able to explore the entire PIT search space, as long as all the versions of layers to be explored are included in the super-net <ref type="bibr" target="#b12">[13]</ref>. However, doing so would result in a too huge network, impossible to train due to memory and time requirements. In fact, as detailed in Section 3, PIT explores C out and F with a granularity of 1, and for d, it considers all possible power-of-2 values. Therefore, each super-net node should include C out,seed ? F seed ? log 2 (F seed ) different layers, connected in parallel. With the same parameters used for the example at the end of Section 3.1, this would correspond to ?10000 different versions of each layer.</p><p>Therefore, we select a coarser-grain search space for ProxylessNAS, trying to make the comparison with PIT as fair as possible, while keeping the search space size similar to the one of the original paper <ref type="bibr" target="#b12">[13]</ref>. To do so, we use the following procedure. First, we perform multiple Prox-ylessNAS searches on C out , F and d separately, keeping the two not-optimized hyper-parameters at the seed values. In each of these searches, we consider 4 layers variants in each super-net node, uniformly sampling the PIT search space (in the same way described above for FBNetV2-Coarse). We then run ProxylessNAS multiple times with different regularization strengths. We identify, for every layer, the two values of each hyper-parameter that have been chosen more frequently. The 2 3 possible combinations of the latter are used to generate the combined search space for Proxyless-NAS, which, accordingly, includes 8 layer variants in each super-net node. The Pareto fronts of Figure <ref type="figure">9</ref> are obtained running ProxylessNAS multiple times on this combined search space, with different regularization strengths. We report both the results obtained with the training scheme proposed in the original paper (Original curve), which runs for a fixed number of epochs, and with the same early-stop mechanism employed for PIT (EarlyStop curve). As shown, the quality of results is similar in both cases. Figure <ref type="figure" target="#fig_6">10</ref> compares the search space dimension and average execution time of the different tools on the HR monitoring benchmark. For reference, the execution time of a standard training of the seed network is also reported. All time results refer to the search phase only (without warm up), and are obtained on a single NVIDIA Titan XP GPU with a batch-size of 128. For ProxylessNAS, we report the results of the the initial single-hyper-parameter searches (Proxyless-Single) and of the final combined search (Proxyless-Multiple), both with and without early-stopping.</p><p>Our algorithm explores a 10 26 ?/10 12 ? larger search space than Proxyless-Single/-Multiple. Further, it is only 1.13? slower than Proxyless-Single with early-stopping, and 3.55? faster than the variant without early-stopping, while it is 3.0?/14.22? faster compared to Proxyless-Multiple with/without the early-stopping training. With respect to MorphNet, we explore a 10 11 ? larger search space at the cost of a small 1.07? increase in runtime. FBNetV2-Coarse is the fastest tool, converging in few search epochs. Whereas offering a 2.5? speedup with respect to PIT, the explored search space is 10 26 smaller. Instead, FBNetV2-Fine explores a 10 11 smaller space while requiring the same search time of the proposed approach. Lastly, PIT's timeoverhead with respect to a normal training is only 34%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Embedded Deployment</head><p>This section analyzes the results obtained deploying two TCNs for each target benchmark on the GAP8 IoT processor (running at 100 MHz) and on the STM32H7 MCU (at 480 MHz). For each task, we deploy the best performing network in terms of MAE or accuracy (L). Moreover, we also select a small network that achieves a MAE drop &lt; 1 BPM, or an accuracy drop &lt; 5% with respect to the best performing one (S). For comparison, we also deploy the baseline hand-tuned architectures (HT). Table <ref type="table" target="#tab_8">4</ref> reports the results in terms of performance (MAE or accuracy, depending on the dataset), memory footprint, inference latency and energy consumption, while Figure <ref type="figure" target="#fig_7">11</ref> shows the hyper-parameters selected by our NAS. PIT finds competitive solutions for both hardware targets and for all 4 tasks, despite the large difference in complexity among them, testified by the more than two orders of magnitude span in memory, latency and energy consumption in the results of Table <ref type="table" target="#tab_8">4</ref>. For PPG-based HR monitoring, the L/S models achieve a 0/0.70 BPM MAE increase with respect to the hand-tuned TEMPONet respectively, while resulting in a 8.03/90.8? lower memory footprint, and a 5.45/19.6? lower latency and energy consumption on GAP8. On the STM32 MCU, the latency and energy reduction of the two PIT outputs is 3.83/18.2?. PIT's L/S models for ECG processing, instead, achieve +0.07%/-1.36% accuracy with respect to the hand-tuned ECGNET, with a 2.83/16.8? lower memory footprint, 2.13/3.44? latency and energy reduction on GAP8, and 2.34/3.7? on the STM32. For the sEMG gesture recognition task, the L/S models found by PIT obtain +2.31%/-1.92% accuracy compared to TCCNet. In this case, the higher accuracy of the large model is paid with a 3.57? larger memory footprint, and a 3.85? latency and energy increase on GAP8 (3.33? on the STM32H7), proving once again the goodness of the hand-tuned model for this task. The small TCN, instead, results in a 2.51? memory reduction, and 1.54? and 1.72? lower latency and energy on the two targets. Lastly, the L/S PIT outputs for KWS obtain +0.16%/-5% accuracy with respect to TCResNet-14, with a 5.72/33.1? lower memory footprint, 3.58/9.54? lower energy and latency on GAP8, and 2.9/11.54? lower energy and latency on STM32H7.</p><p>Figure <ref type="figure" target="#fig_7">11</ref> shows the high variability of hyper-parameters settings found by PIT, and the different optimization behaviours for different benchmarks. In general, comparing PIT outputs with the respective seeds, we can observe that the optimized networks found by our tool contradict several "rules of thumb" of manual DNN design, such as progressively increasing the number of channels and dilation for deeper layers. Accordingly, our NAS could also provide interesting insights for better TCN design. For instance, for the PPG benchmark, PIT finds solutions that are characterized by a large number of channels in the first and last layers, while keeping an overall high receptive field in the core of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have proposed PIT, a lightweight NAS tool for TCNs, able to explore a large, fine-grained search space of architectures with low GPU memory requirements. PIT is, to the best of our knowledge, the first DMaksingNAS tool explicitly designed for 1D convolutional networks, and the first to target the optimization of the receptive field and dilation of convolutional layers. With experiments of four real-world benchmarks, we have shown that PIT is able to find improved versions of state-of-the-art TCNs, with a memory compression of up to 8.03? (90.8?) and a latency and energy reduction of up to 5.45? (19.6?) without (with a reasonable) accuracy drop, when deployed on commercial edge devices. Our future work will focus on extending PIT principles to generic N-dimensional CNNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Search space of PIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Channels search example. Each ? A,m = 0 zeroes-out the m-th convolutional filter, i.e., a slice of size K ? C in of the weights tensor W .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Receptive field search example. Each ? B,i = 0 eliminates the contribution of 1 input time-step from the convolution output, by zeroing out a time-slice of size Cout ? C in of the weights tensor W .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Dilation search example. Each ? i = 0 increases d by a factor 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 1 :</head><label>1</label><figDesc>for i ? 1, . . . , Steps wu do #warmup loop 2: Update W based on ? W L(W ) 3: end for 4: while not converged do #search loop 5: Update W and ? based on ? W,? (L(W ; ?) + ?R(?)) 6: end while 7: for i ? 1, . . . , Steps ft do #fine-tuning loop 8: Update W based on ? W L(W ) 9: end for see Section 3.3), K (n)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Comparison between the results of PIT searches with different combinations of hyper-parameters for PPG-DaLia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Search space and time comparison between PIT and state-ofthe-art NAS tools on the PPG-DaLia dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Hyperparameters of the deployed PIT architectures and corresponding seed network for the four benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>State-of-the-art NAS (Values: ?= large, = medium, ?= small).</figDesc><table><row><cell></cell><cell cols="3">Time Mem. Search Space</cell><cell>Topology</cell></row><row><cell cols="2">Reinforcement Learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zoph et al. [9]</cell><cell>?</cell><cell>?</cell><cell></cell><cell>Variable  *</cell></row><row><cell>MNASNET [12]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>Variable</cell></row><row><cell>NASNET [24]</cell><cell>?</cell><cell>?</cell><cell></cell><cell>Variable</cell></row><row><cell>MetaQNN [25]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>Variable</cell></row><row><cell>Evolutionary</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Real et al. [26]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>Variable</cell></row><row><cell>DifferentiableNAS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DARTS [27]</cell><cell></cell><cell>?</cell><cell>?</cell><cell>Variable</cell></row><row><cell>ProxylessNAS [13]</cell><cell></cell><cell></cell><cell></cell><cell>Variable</cell></row><row><cell>DmaskingNAS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FBNetV2 [10]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>Fixed</cell></row><row><cell>MorphNet [11]</cell><cell>?</cell><cell>?</cell><cell></cell><cell>Fixed</cell></row><row><cell>S.-Path NAS [28]</cell><cell>?</cell><cell>?</cell><cell></cell><cell>Fixed</cell></row><row><cell>PIT (this work)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>Fixed</cell></row><row><cell>*  Depth only</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">normal training loss function with an additional differ-</cell></row><row><cell cols="5">entiable regularization term that encodes the cost of the</cell></row><row><cell cols="5">network. Typical cost metrics are the number of parameters</cell></row><row><cell cols="5">and the number of Floating Point Operations (FLOPs) per</cell></row><row><cell>inference</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>List of symbols used in the paper.</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>x</cell><cell>Input activations of a convolutional layer</cell></row><row><cell>y</cell><cell>Output activations of a convolutional layer</cell></row><row><cell>T</cell><cell>Output sequence length of a convolutional layer</cell></row><row><cell>C in , Cout</cell><cell>Number of input/output channels of a conv. layer</cell></row><row><cell>W</cell><cell>Convolutional filter weights</cell></row><row><cell>K</cell><cell>Convolution filter size</cell></row><row><cell>s</cell><cell>Convolution stride</cell></row><row><cell>d</cell><cell>Convolution dilation</cell></row><row><cell>F</cell><cell>Convolution receptive field</cell></row><row><cell>L</cell><cell>Task-specific loss function</cell></row><row><cell>R</cell><cell>Regularization loss function</cell></row><row><cell>?</cell><cell>Regularization strength</cell></row><row><cell>S, ?</cell><cell>Search space and sampled architecture</cell></row><row><cell>Ln</cell><cell>Generic convolutional/FC layer</cell></row><row><cell>N</cell><cell>Number of convolutional/FC layers</cell></row><row><cell>?, ?</cell><cell>Generic NAS architectural parameters and correspond-</cell></row><row><cell></cell><cell>ing binary mask</cell></row><row><cell>?, ? A</cell><cell>NAS architectural parameters to optimize Cout and cor-</cell></row><row><cell></cell><cell>responding binary mask</cell></row><row><cell>?, ? B</cell><cell>NAS architectural parameters for F , and corresponding</cell></row><row><cell></cell><cell>binary mask</cell></row><row><cell>?, ?, ?</cell><cell></cell></row></table><note><p>? NAS architectural parameters for d, intermediate binary mask elements and final binary mask C ? , C? Transformation matrices to generate ? B and ? ? from ? and ?. k(i) Index mapping function used to generate ? ? from ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 Convolu?onal Kernel W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 ... W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 , W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0 W 8 W 7 W 6 W 5 W 4 W 3 W 2 W 1 W 0</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>OPs 10 8 10 7 10 6 Num. of Parameters 10 6 10 5 10 4 OPs Num. of Parameters 10 4 10 3 10 6 10 5 10 4 .932</head><label></label><figDesc>Fig. 6. Overall PIT Pareto fronts for the four target benchmarks, and comparison with seed and hand-tuned TCNs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">-Tuned Network</cell><cell cols="2">Size Regularizer</cell><cell></cell><cell></cell><cell>OPs Regularizer</cell></row><row><cell></cell><cell>8.03x</cell><cell></cell><cell></cell><cell></cell><cell>PPG</cell><cell></cell><cell cols="2">5.42x</cell><cell></cell><cell></cell><cell>PPG</cell><cell></cell><cell>2.83x</cell><cell>ECG +0.36%</cell><cell></cell><cell>7.06x</cell><cell>ECG +0.36%</cell></row><row><cell></cell><cell cols="2">-0.11BPM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">-0.11BPM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">5.0</cell><cell>5.4</cell><cell>MAE</cell><cell>5.8</cell><cell>6.2</cell><cell cols="2">5.0</cell><cell>5.4</cell><cell>5.8</cell><cell>MAE</cell><cell>6.2</cell><cell></cell><cell>Accuracy .936 .940</cell><cell>.944</cell><cell></cell><cell>.938</cell><cell>.942</cell><cell>Accuracy</cell><cell>.946</cell></row><row><cell>Num. of Parameters 10 6 10 5 10 4</cell><cell>sEMG</cell><cell></cell><cell></cell><cell></cell><cell cols="2">10 8 10 7 10 6 OPs</cell><cell cols="2">sEMG</cell><cell></cell><cell></cell><cell>10 6 10 4 Num. of arameters 10 5</cell><cell>KWS</cell><cell></cell><cell>5.72x</cell><cell>10 8 10 6 OPs 10 7</cell><cell>KWS</cell><cell>1.8x</cell></row><row><cell></cell><cell>.78</cell><cell cols="3">.82 Accuracy .86</cell><cell>.90</cell><cell></cell><cell>.78</cell><cell cols="3">.82 Accuracy .86</cell><cell>.90</cell><cell>.80</cell><cell>.84 Accuracy .88</cell><cell>.92</cell><cell></cell><cell>.87</cell><cell>.88</cell><cell>.89 .90 .91 .92 Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Range of regularizer strength (?) values for the four benchmarks.</figDesc><table><row><cell>Regularizer</cell><cell>PPG</cell><cell>ECG</cell><cell>sEMG</cell><cell>KWS</cell></row><row><cell>R size</cell><cell cols="2">1e-7 : 5e-4 5e-7 : 7.5e-3</cell><cell>1e-7 : 5e-6</cell><cell>5e-10 : 1e-5</cell></row><row><cell>Rops</cell><cell>1e-8 : 5e-5</cell><cell>5e-8 : 5e-4</cell><cell cols="2">5e-10 : 5e-8 1e-10 : 1e-6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>10 6 10 5 Num. of Parameters 10 4 10 8 10 7 10 6 OPs 10 5</head><label></label><figDesc></figDesc><table><row><cell>Seed Network</cell><cell>Hand-Tuned Network</cell><cell cols="2">ProxylessNAS Original</cell><cell>ProxylessNAS EarlyStop</cell><cell cols="2">MorphNet</cell><cell>FBNetV2 Coarse</cell><cell>FBNetV2 Fine</cell><cell>PIT</cell></row><row><cell>5.00</cell><cell>5.50</cell><cell>MAE 6.00</cell><cell>6.50</cell><cell>7.00</cell><cell>5.00</cell><cell>5.50</cell><cell>MAE 6.00</cell><cell>6.50</cell><cell>7.00</cell></row><row><cell cols="10">Fig. 9. Quality of results comparison between PIT and state-of-the-art</cell></row><row><cell cols="5">NAS tools on the PPG-DaLia dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time [s]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>10 3 Search Space Dimension 10 32 10 28 10 16 10 12 10 8 10 24 10 20</head><label></label><figDesc></figDesc><table><row><cell>PIT</cell><cell cols="2">Proxyless Single</cell><cell cols="2">Proxyless Mul?ple</cell><cell>MorphNet</cell></row><row><cell cols="2">FbNetV2-Coarse</cell><cell cols="2">FbNetV2-Fine</cell><cell>Normal Training</cell></row><row><cell>Without</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Early-Stop</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4</head><label>4</label><figDesc>Detailed deployment results for the four benchmarks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GAP8</cell><cell cols="2">STM32</cell></row><row><cell></cell><cell></cell><cell>Perf.</cell><cell>Mem.</cell><cell>Lat.</cell><cell>En.</cell><cell>Lat.</cell><cell>En.</cell></row><row><cell>Task</cell><cell>TCN</cell><cell>int8 (float32)</cell><cell>[kB]</cell><cell>[ms]</cell><cell>[mJ]</cell><cell>[ms]</cell><cell>[mJ]</cell></row><row><cell></cell><cell>HT</cell><cell>5.01 (5.14) BPM</cell><cell>423</cell><cell>23.2</cell><cell>1.2</cell><cell>58.3</cell><cell>13.6</cell></row><row><cell>PPG</cell><cell>S</cell><cell>5.71 (6.17) BPM</cell><cell>4.7</cell><cell>1.18</cell><cell>0.06</cell><cell>3.2</cell><cell>0.75</cell></row><row><cell></cell><cell>L</cell><cell>5.01 (5.03) BPM</cell><cell>53.2</cell><cell>4.25</cell><cell>0.22</cell><cell>15.2</cell><cell>3.56</cell></row><row><cell></cell><cell>HT</cell><cell>94.2 (94.2) %</cell><cell>15.2</cell><cell>2.69</cell><cell>0.14</cell><cell>6.66</cell><cell>1.56</cell></row><row><cell>ECG</cell><cell>S</cell><cell>92.84 (93.16) %</cell><cell>0.9</cell><cell>0.78</cell><cell>0.04</cell><cell>1.8</cell><cell>0.42</cell></row><row><cell></cell><cell>L</cell><cell>94.13 (94.13) %</cell><cell>5.4</cell><cell>1.26</cell><cell>0.06</cell><cell>2.84</cell><cell>0.66</cell></row><row><cell></cell><cell>HT</cell><cell>88.89 (88.87) %</cell><cell>88.8</cell><cell>61.0</cell><cell>3.11</cell><cell>291</cell><cell>68.1</cell></row><row><cell>sEMG</cell><cell>S</cell><cell>86.97 (86.98) %</cell><cell>35.4</cell><cell>39.6</cell><cell>2.02</cell><cell>169</cell><cell>39.5</cell></row><row><cell></cell><cell>L</cell><cell>91.2 (90.99) %</cell><cell>317.8</cell><cell>238</cell><cell>12.1</cell><cell>960</cell><cell>225</cell></row><row><cell></cell><cell>HT</cell><cell>92 (92.31) %</cell><cell>323.4</cell><cell>13.4</cell><cell>0.68</cell><cell>30.7</cell><cell>7.17</cell></row><row><cell>KWS</cell><cell>S</cell><cell>87 (86.58) %</cell><cell>9.8</cell><cell>1.40</cell><cell>0.07</cell><cell>2.66</cell><cell>0.62</cell></row><row><cell></cell><cell>L</cell><cell>92.16 (92.64) %</cell><cell>56.5</cell><cell>3.74</cell><cell>0.19</cell><cell>10.6</cell><cell>2.48</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for multiple speaker detection and localization</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICRA</title>
		<imprint>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust real-time embedded emg recognition framework using temporal convolutional networks on a multicore iot processor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zanghieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Circuits Syst</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for seizure prediction using intracranial and scalp electroencephalogram</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="104" to="111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-driven structural health monitoring and damage detection through deep learning: State-of-the-art review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Azimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2778</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manufacturing as a data-driven practice: Methodologies, technologies, and tools</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cerquitelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="399" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A critical review of recurrent neural networks for sequence learning</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00019</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Edge computing: Vision and challenges</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="2016-10">Oct 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF CVPR</title>
		<meeting>IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Morphnet: Fast &amp; simple resource-constrained structure learning of deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE CVPR</title>
		<meeting>of the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pruning in time (pit): A light-weight network architecture optimizer for temporal convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Risso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 58th DAC</title>
		<meeting>58th DAC</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gap-8: A risc-v soc for ai at the edge of the iot</title>
		<author>
			<persName><forename type="first">E</forename><surname>Flamand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 29th ASAP</title>
		<meeting>IEEE 29th ASAP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><surname>St Microelectronics</surname></persName>
		</author>
		<ptr target="https://www.st.com/en/microcontrollers-microprocessors/stm32h7-series.html" />
		<title level="m">STM32H7</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust and energy-efficient ppg-based heart-rate monitoring</title>
		<author>
			<persName><forename type="first">M</forename><surname>Risso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ISCAS</title>
		<meeting>IEEE ISCAS</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03814</idno>
		<title level="m">Temporal convolution for real-time keyword spotting on mobile devices</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved gesture recognition based on semg signals and tcn</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tsinganos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1169" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1905">1905.11946, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt; 1mb model size</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<idno>abs/1602.07360</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF CVPR</title>
		<meeting>IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<idno>abs/1611.02167</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML. PMLR</title>
		<meeting>ICML. PMLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single-path mobile automl: Efficient convnet design and nas hyperparameter optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="609" to="622" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML. PMLR</title>
		<meeting>ICML. PMLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fine-Grained Stochastic Architecture Search</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Chaudhuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09581</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalpel: Customizing dnn pruning to the underlying hardware parallelism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="548" to="560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">X-cube-ai</title>
		<author>
			<persName><forename type="first">S</forename><surname>Microelectornics</surname></persName>
		</author>
		<ptr target="https://www.st.com/en/embedded-software/x-cube-ai.html" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gap8 nntool</title>
		<author>
			<persName><forename type="first">G</forename><surname>Technologies</surname></persName>
		</author>
		<ptr target="https://greenwaves-technologies.com/manuals/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep ppg: large-scale heart rate estimation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">3079</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A general framework for never-ending learning from time series streams</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1622" to="1664" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ecg-tcn: Wearable cardiac arrhythmia detection with a temporal convolutional network</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Ingolfsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE AICAS</title>
		<meeting>3rd IEEE AICAS</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Building the ninapro database: A resource for the biorobotics community</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atzori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IEEE RAS &amp; EMBS BioRob</title>
		<meeting>4th IEEE RAS &amp; EMBS BioRob</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1258" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2016 NAACL</title>
		<meeting>2016 NAACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Benchmarking tinyml systems: Challenges and direction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Banbury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04821</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tcn mapping optimization for ultra-low power time-series edge inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burrello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/ACM ISLPED</title>
		<meeting>IEEE/ACM ISLPED</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dory: Automatic end-to-end deployment of real-world dnns on low-cost iot mcus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burrello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
