<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deconfounded Training for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-30">30 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<email>xiangwang@u.nus.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<email>xiangnanhe@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deconfounded Training for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-30">30 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.15089v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Graph Representation Learning</term>
					<term>Causal Intervention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning powerful representations is one central theme of graph neural networks (GNNs). It requires refining the critical information from the input graph, instead of the trivial patterns, to enrich the representations. Towards this end, graph attention and pooling methods prevail. They mostly follow the paradigm of "learning to attend". It maximizes the mutual information between the attended subgraph and the ground-truth label. However, this training paradigm is prone to capture the spurious correlations between the trivial subgraph and the label. Such spurious correlations are beneficial to in-distribution (ID) test evaluations, but cause poor generalization in the out-of-distribution (OOD) test data. In this work, we revisit the GNN modeling from the causal perspective. On the top of our causal assumption, the trivial information serves as a confounder between the critical information and the label, which opens a backdoor path between them and makes them spuriously correlated. Hence, we present a new paradigm of deconfounded training (DTP) that better mitigates the confounding effect and latches on the critical information, to enhance the representation and generalization ability. Specifically, we adopt the attention modules to disentangle the critical subgraph and trivial subgraph. Then we make each critical subgraph fairly interact with diverse trivial subgraphs to achieve a stable prediction. It allows GNNs to capture a more reliable subgraph whose relation with the label is robust across different distributions. We conduct extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Mathematics of computing → Graph algorithms; • Computing methodologies → Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41]</ref> have exhibited impressive performance in various tasks, which involve graph-structured data such as chemical molecules, social networks, and transaction graphs. We attribute such a success to the powerful representation learning of GNNs, which distills useful information from the graph structure and encodes them into the representations in an end-to-end way. Hence, it is crucial to emphasize the critical part of the input graph, while filtering the trivial part out <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. For example, when classifying the mutagenic property of a molecular graph <ref type="bibr" target="#b15">[16]</ref>, the GNNs are expected to latch on the functional groups (i.e., nitrogen dioxide (NO 2 )), instead of the irrelevant patterns (i.e., carbon rings) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref>; when detecting fraud in a transaction network, malicious behaviors or coalitions of users are more informative than benign features.</p><p>However, the current training paradigms of GNNs struggle to distinguish the critical subgraph from the trivial subgraph of the input graph. Specifically, most of GNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> follow a conventional paradigm of supervised learning -minimizing the loss between the prediction on the full input graph and the groundtruth label. Despite great success, it makes the GNNs obscure as a black box, failing to exhibit what knowledge is exploited to make predictions. Towards specifying the critical subgraph, some followon works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> adopt another paradigm of "learning to attend" <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> -maximizing the mutual information between the attended subgraph and the ground-truth label. The attended subgraph is viewed as the estimation of critical information. Specifically, there are two research lines in this paradigm: <ref type="bibr" target="#b0">(1)</ref> For the input graph, attention-based GNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref> learn the attention scores for all edges or nodes, which identify their contributions to the representations and predictions. <ref type="bibr" target="#b1">(2)</ref> Pooling-based GNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> directly select some local parts of edges or nodes to perform the information propagation, while completely ignoring the rest. Clearly, the attentive or selective subgraphs are viewed as the knowledge memorized in GNNs to make predictions.</p><p>However, the current attention and pooling methods are prone to emphasize spurious correlations between the trivial patterns and the label, but ignore the critical subgraphs. For example, instead of probing different functional groups, the attended subgraphs prefer "carbon rings" as the cues of the "mutagenic" class, because most training "mutagenic" molecules are in the "carbon rings" context. While such correlations represent statistical relations inherent in the training data and are beneficial to the in-distribution (ID) test evaluations that follow the distribution of the training data, they inevitably cause a huge performance drop in the out-of-distribution (OOD) test data. Taking the molecule classification as an example again, when most test "non-mutagenic" molecules appear in the "carbon rings" context, the attention misleads the GNNs to still predict "mutagenic". As the assumption that the test data conforms to the training distribution is often infeasible in real-world scenarios, the poor generalization of these methods hinders their deployment on critical applications.</p><p>To resolve this issue, we first take a causal look at the decisionmaking process of GNNs, which delineates the relationships among the critical information, trivial information, and GNN's prediction. We argue that the trivial information is a confounder <ref type="bibr" target="#b18">[19]</ref> between the critical information and prediction. It opens a backdoor path from the critical information to the prediction, making them spuriously correlated. Hence, mitigating the confounding effect of the trivial information is promising to enhance the generalization.</p><p>Towards this end, we propose a new paradigm of deconfounded training (DTP) -maximizing the causal effect of the attended subgraph on predicting the label, while reducing the confounding effect of the complement subgraph. The attended subgraph aims to approach the critical subgraph, and its complement targets at the trivial patterns. Specifically, we first hire attention modules to disentangle the input graph into two complementary parts: the attended critical subgraph and the trivial subgraph. Then we obtain their representations by a GNN module. To keep the prediction invariant to the variant distribution caused by the trivial information, we make the critical subgraph fairly combined with diverse trivial subgraphs, and we hope the prediction of the combined representation always keeps consistent across different types of trivial subgraphs. DTP actually applies the backdoor adjustment <ref type="bibr" target="#b18">[19]</ref> to mitigate the confounder. It enables GNN models to identify better attended subgraphs, whose relationships with the labels are more stable across different distributions, thereby having better generalization. We apply DTP to train various GNN architectures, on numerous synthetic and real-world datasets. Experimental results validate better performance and generalization of DTP, as compared with the state-of-the-art attention and pooling methods.</p><p>Our technical contributions are summarized as:</p><p>• We emphasize the poor generalization of current attention-and pooling-based GNNs, and ascribe it to the confounding effect of the trivial subgraphs, from a causal perspective. • To mitigate the confounding effect, we present a deconfounded training paradigm, DTP, which better distinguishes the critical subgraphs from the trivial subgraphs. • Extensive experiments on synthetic and real-world datasets justify the effectiveness of DTP. More visualizations with in-depth analyses are provided to further demonstrate the rationality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 Notations</head><p>We denote a graph by G = {V, E} with the node set V and edge set E. Let X ∈ R | V |×F be the node feature matrix, where x 𝑖 = X[𝑖, :] is the F-dimensional attribute vector of node</p><formula xml:id="formula_0">𝑣 𝑖 ∈ V = {𝑣 1 , ..., 𝑣 |V | }.</formula><p>As for E = {𝑒 1 , ..., 𝑒 | E | }, 𝑒 𝑛 = (𝑣 𝑖 , 𝑣 𝑗 ) ∈ E indicates the existence of the edge from node 𝑣 𝑖 to node 𝑣 𝑗 . We use the adjacency matrix A ∈</p><formula xml:id="formula_1">R |V |× | V | to delineate the whole graph structure, where A[𝑖, 𝑗] = 1 if (𝑣 𝑖 , 𝑣 𝑗 ) ∈ E, otherwise A[𝑖, 𝑗] = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>Without loss of generality, we focus on graph classification task. GNNs aim to incorporate the graph structure into the node representation h 𝑣 and graph representation h G . Considering a 𝐾-layer GNN 𝑓 (•), the representation of node 𝑣 at the 𝑘-th layer is represented as:</p><formula xml:id="formula_2">h (𝑘) 𝑣 = 𝑓 combine (h (𝑘−1) 𝑣 , 𝑓 aggregate ({h (𝑘−1) 𝑢 |𝑢 ∈ N (𝑣)})),<label>(1)</label></formula><p>where N (𝑣) is the set of nodes connecting with 𝑣; 𝑓 aggregate (•) and 𝑓 combine (•) are message aggregation and combination functions, respectively. Hereafter, we summarize the representation for graph G via the readout function 𝑓 readout (•). Then a multi-layer perceptron (with a softmax function) MLP(•) is adopted as a classifier for the downstream task:</p><formula xml:id="formula_3">h G = 𝑓 readout ({h (𝐾) 𝑣 |𝑣 ∈ V}), 𝑧 G = MLP(h G ).<label>(2)</label></formula><p>To optimize the GNN parameters, we exploit the following objective function:</p><formula xml:id="formula_4">L sup = − 1 |D| ∑︁ G ∈D y ⊤ G log(z G ).<label>(3)</label></formula><p>where L sup is the cross-entropy loss over the training dataset D, and y G is the ground-truth label vector of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we analyze the graph representation learning from the perspective of causality. First, we give the definitions of critical and trivial subgraphs. Taking a causal look at the relationship between the critical information and prediction, we identity the trivial information as a confounder, which makes them spuriously correlated. Finally, we propose a deconfounded training paradigm to alleviate the confounding effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Critical Subgraph v.s. Trivial Subgraph</head><p>Given a graph G = {A, X}, we formulate the soft masks on the graph structure and node feature as M 𝑎 and M 𝑥 , which have the same size as the adjacency matrix A and feature matrix X, respectively. Wherein, each element of the masks indicates the importance relevant to the task of interest, which often falls into the range of (0, 1). Given an arbitrary soft mask M, we define its complementary mask as M = 1 − M, where 1 is the all-one matrix. Therefore, we can divide the full graph G into two masked graphs: G 1 and G 2 , where</p><formula xml:id="formula_5">G 1 = {A ⊙ M 𝑎 , X ⊙ M 𝑥 } and G 2 = {A ⊙ M 𝑎 , X ⊙ M 𝑥 }.</formula><p>With the inspection on the data-generating process of graphs, recent studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref> argue that the label of a graph is usually determined by its critical subgraph. Considering a molecular graph, its mutagenic property relies on the existence of relevant functional groups <ref type="bibr" target="#b34">[35]</ref>; Taking the digit image in the form of superpixel graph as another example, the coalition of digit-relevant nodes determines its label. Formally, given a graph G with the label, we define the subgraph collecting all critical information as the critical subgraph G 𝑐 , while the counterpart forms the trivial subgraph G 𝑡 . However, the ground-truth critical subgraph is usually unavailable in real-world applications. Hence, we aim to distinguish the critical and trivial subgraphs from the full graph by generating the soft masks:</p><formula xml:id="formula_6">G 𝑐 = {A ⊙ M 𝑎 , X ⊙ M 𝑥 } and G 𝑡 = {A ⊙ M 𝑎 , X ⊙ M 𝑥 }.</formula><p>Learning to identify critical subgraphs not only guides the representation learning of GNNs, but also answers "What knowledge does the GNN use to make predictions?", which is crucial to the applications on explainability, privacy, and fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Causal View on GNNs</head><p>Before learning critical graphs, we take a causal look at the GNN modeling and construct a Structural Causal Model (SCM) <ref type="bibr" target="#b18">[19]</ref> as Figure <ref type="figure" target="#fig_0">1</ref> shows. SCM presents the causalities among five variables: graph data 𝐺, trivial information 𝑇 , critical information 𝐶, graph representation 𝑅, and prediction 𝑌 , where the link from one variable to another variable indicates the cause-effect relationship: cause → effect. Now, we list the following explanations for the SCM. • 𝑻 ← 𝑮 → 𝑪. The variable 𝐶 denotes the information that truly reflects the intrinsic property of the observed graph data 𝐺, while 𝑇 represents the noisy or meaningless information which often misleads the GNN's decisions. Due to the existence of 𝑇 , the mutual information between 𝐶 and label is usually greater than that between 𝐺 and label. Since 𝑇 and 𝐶 naturally exist in the graph data 𝐺, the causal effects are established. • 𝑻 → 𝑹 ← 𝑪. The variable 𝑅 denotes the representation of the given graph 𝐺 (e.g., the embedding vector). Due to the messagepassing scheme of GNNs, trivial information 𝑇 and critical information 𝐶 will inevitably merge together. They both make contributions to generate the graph representation 𝑅. • 𝑹 → 𝒀 . The ultimate goal of graph representation learning is to predict the properties from the input graphs. The classifier will make prediction 𝑌 by observing the graph representation 𝑅. Therefore, the causal relationship 𝑅 → 𝑌 is obvious. Scrutinizing this SCM, we recognize a backdoor path between 𝐶 and 𝑌 , i.e., 𝐶 ← 𝐺 → 𝑇 → 𝑅 → 𝑌 , wherein the trivial information 𝑇 forms a confounder between 𝐶 and 𝑌 . Even if 𝐶 has no direct link to 𝑌 , the backdoor path will cause 𝐶 to establish a spurious correlation with 𝑌 . If 𝑇 contains distribution bias, the GNN model will inevitably learn the unreliable representation, which leads to a biased GNN model. Therefore, it is crucial to cut off the backdoor path and make the GNN focus on the reliable causal relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Backdoor Adjustment</head><p>We have realized that shielding the GNNs from the confounder 𝑇 is the key to stable causal relationships. Instead of modeling the confounded 𝑃 (𝑌 |𝐶) in Figure <ref type="figure" target="#fig_0">1</ref>, we should achieve the graph representation learning by eliminating the backdoor path. But how to achieve this? Fortunately, causal inference <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> provides us with a feasible solution: we can exploit the do-calculus on the variable 𝐶 to remove the backdoor path by estimating 𝑃 𝑚 (𝑌 |𝐶) = 𝑃 (𝑌 |𝑑𝑜 (𝐶)). To stratify the confounder 𝑇 between 𝐶 and 𝑌 , we can obtain the following three essential conclusions:</p><p>• The marginal probability 𝑃 (𝑇 = 𝑡) is invariant under the intervention, because the trivial information will not be affected by removing the causal relation from 𝐺 to 𝑇 . Thus, 𝑃 (𝑡) = 𝑃 𝑚 (𝑡).  Based on the above conclusions, we have:</p><formula xml:id="formula_7">𝑃 (𝑌 |𝑑𝑜 (𝐶)) = 𝑃 𝑚 (𝑌 |𝐶) = ∑︁ 𝑡 ∈T 𝑃 𝑚 (𝑌 |𝐶, 𝑡)𝑃 𝑚 (𝑡 |𝐶) (𝐵𝑎𝑦𝑒𝑠 𝑅𝑢𝑙𝑒) = ∑︁ 𝑡 ∈T 𝑃 𝑚 (𝑌 |𝐶, 𝑡)𝑃 𝑚 (𝑡) (𝐼𝑛𝑑𝑒𝑝𝑒𝑛𝑑𝑒𝑛𝑐𝑦) = ∑︁ 𝑡 ∈T 𝑃 (𝑌 |𝐶, 𝑡)𝑃 (𝑡),<label>(4)</label></formula><p>where T denotes the confounder set; 𝑃 (𝑌 |𝐶, 𝑡) represents the conditional probability given the critical information 𝐶 and confounder 𝑡; 𝑃 (𝑡) is a prior probability of the confounder. Equation ( <ref type="formula" target="#formula_7">4</ref>) is usually called backdoor adjustment <ref type="bibr" target="#b17">[18]</ref>, which is a powerful tool to eliminate the confounding effect. However, T is commonly unobservable and hard to obtain in graph representation learning, due to the discrete nature of graph data. Next, we will introduce a simple yet effective solution to establish the confounder set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Deconfounded Training Paradigm</head><p>To implement the aforementioned backdoor adjustment, we propose the Deconfounded Training Paradigm (DTP) as follows:</p><p>3.4.1 Estimating soft masks. Towards effective causal intervention, it is necessary to disentangle the trivial and critical subgraphs from the full graphs. To this end, we hire an attention module, which yields two branches for the critical and trivial proposals. Given a GNN encoder 𝑓 (•) and a graph G = {A, X} with |V | nodes, we can get the node representations:</p><formula xml:id="formula_8">H = 𝑓 (G).<label>(5)</label></formula><p>where </p><formula xml:id="formula_9">H ∈ R |V |×𝑑</formula><formula xml:id="formula_10">𝛼 𝑐 𝑖 , 𝛼 𝑡 𝑖 = 𝜎 (MLP node (h 𝑖 )),<label>(6)</label></formula><formula xml:id="formula_11">𝛽 𝑐 𝑖 𝑗 , 𝛽 𝑡 𝑖 𝑗 = 𝜎 (MLP edge (h 𝑖 ||h 𝑗 )),<label>(7)</label></formula><p>where </p><formula xml:id="formula_12">G 𝑐 = {A ⊙ M 𝑎 , X ⊙ M 𝑥 } and trivial subgraph G 𝑡 = {A ⊙ M 𝑎 , X ⊙ M 𝑥 }.</formula><p>3.4.2 Disentanglement. Until now, we have distributed the attentions at the granularity of nodes and edges to create the masks for the critical and trivial subgraphs. Now we need to distinguish the representations of the critical/trivial subgraph and push them to the opposite directions in latent space. Specifically, we adopt two graph convolutional layers <ref type="bibr" target="#b11">[12]</ref> GConv 𝑐 (•) and GConv 𝑡 (•) to obtain the subgraph representations and make predictions through readout function and MLP classifiers:</p><formula xml:id="formula_13">h G 𝑐 = 𝑓 readout (GConv 𝑐 (G 𝑐 )), z G 𝑐 = MLP 𝑐 (h G 𝑐 ).<label>(8)</label></formula><formula xml:id="formula_14">h G 𝑡 = 𝑓 readout (GConv 𝑡 (G 𝑡 )), z G 𝑡 = MLP 𝑡 (h G 𝑡 ).<label>(9)</label></formula><p>According to the definition, the critical subgraph is decisive for classification, so we classify the representation to the ground-truth label. Thus, we define the supervised classification loss as:</p><formula xml:id="formula_15">L sup = − 1 |D| ∑︁ G ∈ D y ⊤ G log(z G 𝑐 ). (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where the L sup is the cross-entropy loss over all the graphs in dataset D. The trivial subgraph contains redundant information and is unnecessary for classification. Hence, we push its representation evenly to all categories and define the uniform classification loss as:</p><formula xml:id="formula_17">L unif = 1 |D| ∑︁ G ∈ D KL(y unif , z G 𝑡 ). (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where KL denotes the KL-Divergence, y unif represents the uniform label distribution such that the representation of the trivial subgraph should be predicted to all categories with equal probability. By optimizing the above two objective functions, we can effectively disentangle critical and trivial subgraphs. Please note that existing works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref> have proved that the mutual information between critical subgraph and label is greater than that between full graph and label, due to the widespread noise. Hence, the proposed disentanglement will not make the captured critical subgraph converge to the full graph, which is not an optimal solution. The visualization results in section 4.5 further confirm this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Causal intervention.</head><p>As shown in Equation ( <ref type="formula" target="#formula_7">4</ref>), the key to eliminating the confounding effect of trivial subgraphs is to perform the backdoor adjustment -that is, stratifying the confounder and pairing the target critical subgraph with every stratification of trivial subgraph to compose the "intervened graphs". Furthermore, the predictions of such intervened graphs are expected to be invariant across different stratifications, due to the shared critical information. Towards this end, we make the implicit intervention on representation-level and propose the following loss guided by the backdoor adjustment:</p><formula xml:id="formula_19">L caus = − 1 |D| • |T | ∑︁ G ∈D ∑︁ 𝑡 ′ ∈T y ⊤ G log (z G ′ ), (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>where T is the stratification set of trivial subgraph, which collects the appearing trivial subgraphs of all full graphs during training; z G ′ denotes the prediction of a MLP classifier (with a softmax function) on the "implicit intervened graph" G ′ :</p><formula xml:id="formula_21">z G ′ = MLP(h G 𝑐 + h G 𝑡 ′ ),<label>(13)</label></formula><p>where h G 𝑐 is the representation of critical subgraph G 𝑐 derived from Equation ( <ref type="formula" target="#formula_13">8</ref>), while h G 𝑡 ′ is the representation of stratification G 𝑡 ′ obtained via Equation <ref type="bibr" target="#b8">(9)</ref>. Finally, the loss function of DTP can be defined as the sum of the above loss functions:</p><formula xml:id="formula_22">L = L sup + 𝜆 1 L unif + 𝜆 2 L caus (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>where 𝜆 1 and 𝜆 2 are hyper-parameters that determine the strength of disentanglement and causal intervention, respectively. The detailed algorithm of DTP is provided in Appendix A.1, Alg.1, and the overview of DTP is depicted as Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To verify the superiority and rationality of the proposed DTP, we conduct experiments to answer the following research questions:</p><p>• RQ1: How effective is the proposed DTP to alleviate the out-ofdistribution (OOD) issue? • RQ2: Can the proposed DTP achieve performance improvements on real-world datasets? • RQ3: For the different components in DTP, what are their roles and impacts on performance? • RQ4: Does DTP capture the critical subgraphs with significant patterns and insightful interpretations?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>4.1.1 Datasets. We conduct experiments on both synthetic datasets and real-world datasets.</p><p>• Synthetic graphs: To better demonstrate the advantages of DTP on biased datasets, we construct synthetic datasets for graph classification, which contain a total of 8,000 graphs with 4 classes, and keep balance (2,000 graphs) for each class. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, each graph consists of two subgraphs: trivial graph and critical graph, which remain the same as our definition in Section 3.1. More details about the trivial graph and critical graph are provided in Appendix A.2. The task is to predict the class of the critical graph in the whole graph. For simpleness, we choose "House" class to define the bias of trivial graphs as:</p><formula xml:id="formula_24">𝑏 = #Tree-House #House (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>where #Tree-House denotes the number of "House" critical graphs with the "Tree" trivial graphs, and #House presents the number of graphs in "House" class, which is 2,000. We set the proportion of "Tree" trivial graphs in the other three classes to 1 − 𝑏. Obviously, for the unbiased dataset, 𝑏 = 0.5. We abbreviate the synthetic  dataset with bias 𝑏 as SYN-𝑏. We keep the same bias level on the training/validation set and keep the testing set unbiased. Please refer to the Appendix A.2 for more details. • Real-world graphs: We conduct experiments on three biological graphs (MUTAG, NCI1, PROTEINS), three social graphs (COL-LAB, IMDB-B, IMDB-M) <ref type="bibr" target="#b15">[16]</ref>, and two superpixel graphs (MNIST, CIFAR-10) <ref type="bibr" target="#b12">[13]</ref>. More details, such as statistics and splitting of datasets, are provided in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines.</head><p>To verify the superiority of DTP, we adopt the following prevalent graph classification solutions as baselines:</p><p>• Kernel-based methods: Graphlet kernel (GK) <ref type="bibr" target="#b22">[23]</ref>, Weisfeiler Lehman Kernel (WL) <ref type="bibr" target="#b21">[22]</ref>, Deep Graph kernels (DGK) <ref type="bibr" target="#b30">[31]</ref>. • Pooling-based methods: SortPool <ref type="bibr" target="#b39">[40]</ref>, DiffPool <ref type="bibr" target="#b32">[33]</ref>, Top-𝑘 Pool <ref type="bibr" target="#b6">[7]</ref>, SAGPool <ref type="bibr" target="#b13">[14]</ref>. • GNN-based methods: GCN <ref type="bibr" target="#b11">[12]</ref>, GIN <ref type="bibr" target="#b28">[29]</ref>, GAT <ref type="bibr" target="#b26">[27]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance on Synthetic Graphs (RQ1)</head><p>To explore whether DTP can alleviate the OOD issue, we first conduct experiments on SYN-𝑏 with different biases: 𝑏 ∈ {0.1, 0.2, ..., 0.9}. The experimental results are summarized in Table <ref type="table" target="#tab_3">1</ref> and Figure <ref type="figure" target="#fig_3">4</ref>. We have the following Observations: Obs 1: Pursuing critical subgraphs without considering the causality is harmful. It can be seen from Table <ref type="table" target="#tab_3">1</ref> that most attention-based and pooling-based methods such as GAT, SAGPool, SortPool, Top-𝑘 Pool outperform GCN, which indicates the effectiveness of extracting critical information from original graphs. However, as the bias goes to extremes, the performance of all these baselines dramatically deteriorates. For instance, the performance drop of graph pooling-based methods ranges from 7.82% ∼ 14.24% on SYN-0.1, and ranges from 3.99% ∼ 12.10% on SYN-0.9. Attentionbased model GAT drops by 8.71% and 5.47% for SYN-0.1 and SYN-0.9, respectively. These results indicate that simply extracting critical subgraphs by attention or pooling is prone to overemphasize spurious correlations between the trivial patterns and the labels, which causes a large performance decline on the OOD test data. Taking SYN-0.9 as an example, most "House" critical graphs cooccur with "Tree" trivial graphs in the training set, so the models will mistakenly regard "Tree" as a critical part of "House" graph, which leads to wrong predictions in inference stage.</p><p>Obs 2: GNNs with better ID performance have worse OOD generalization. For the unbiased dataset, GIN achieves the best performance (96.74%), while GAT (92.69%) outperforms the GCN (90.84%). This indicates that the in-distribution (ID) performance of models exhibits such an order: GIN &gt; GAT &gt; GCN. However, when the bias is changed to 0.1 and 0.9, the performance of GIN drops by 9.55% and 7.36%, GAT drops by 8.71% and 5.47% and GCN drops by 6.60% and 5.43%, respectively. It shows that the rankings of models' robustness against OOD issues are in the opposite order: GCN &gt; GAT &gt; GIN. This indicates that GNNs with better ID performance are prone to learn more spurious correlations. After adopting the proposed DTP, this phenomenon is significantly alleviated, which verifies the effectiveness of DTP in overcoming the OOD issue.</p><p>Obs 3: Mitigating the confounder achieves more stable performance on OOD datasets. We first define the performance discount on SYN-𝑏 as the accuracy on SYN-𝑏 normalized by the accuracy on unbiased SYN-0.5. It indicates the degree of the performance degradation on biased synthetic datasets, without considering the model's ID generalization. To better compare DTP with graph pooling-based methods, we plot the performance discount curves on SYN-𝑏 with 𝑏 ∈ {0.1, 0.2, ..., 0.9}. As depicted in Figure <ref type="figure" target="#fig_3">4</ref>, we can find that SAGPool achieves better performance in a small range of biases (0.4 ∼ 0.6), while the performance drops sharply when 𝑏 goes to extremes. For example, the performance discount drops from 1.0 to 0.88 as the bias increases from 0.6 to 0.9. Sort-Pool and Top-𝑘 Pool achieve comparable performance discounts as GIN, while they do not perform well on SYN-0.1. DiffPool achieves slightly better performance than GIN in most cases (expect SYN-0.6). Finally, equipped with DTP, GIN (red star curve) consistently outperforms all the baselines on all ranges of bias levels and obviously keeps a large gap, which further demonstrates the significance of mitigating the confounding effect, and the effectiveness of DTP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance on Real-world Graphs (RQ2)</head><p>Unlike synthetic graphs, there may not exist visible or specific patterns of the critical/trivial subgraphs in real-world graphs. However, there still exist irregular core-subgraphs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35</ref>] that determine the predictions, which will inevitably involve different degrees of bias caused by the complementary subgraphs. Similar to SYN-𝑏, they mislead the GNNs to learn the spurious correlations. Hence, we verify the practicability of DTP on eight real-world datasets. We report the results of the baselines from the original papers by default and reproduce the missing results. The results are summarized in Table <ref type="table" target="#tab_4">2</ref> and we make the following Observations: Obs 4: The OOD issue is widespread in real-world datasets. Graph pooling-based methods are on a par with GNNs, and they both outperform graph kernel-based methods in most cases. It can be seen from the last six rows in Table <ref type="table" target="#tab_4">2</ref>, when DTP is applied to different GNN models, it consistently produces further performance improvements. It demonstrates that the distribution shifts also widely exist in real-world datasets. We can find that the performance of GCN on most datasets is worse than other GNNs or graph pooling-based methods, while the performance significantly improves after adopting DTP. For instance, on the MUTAG, IMDB-B and the MNIST, GCN+DTP achieve 3.37%, 1.92% and 4.52% improvements, respectively. This indicates that the GCN is vulnerable to the distribution shift in certain datasets. Thanks to the causality, DTP will push GCN to pursue reliable and critical subgraphs, which can establish robustness against the OOD issue and achieve better generalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYN-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study (RQ3)</head><p>In this section, we explore the role of different components in DTP. We investigate the impact of the node/edge attention, random combination and the loss coefficients 𝜆 1 and 𝜆 2 . 1) Node Attention v.s. Edge Attention. Node Attention (NA) and Edge Attention (EA) refine the information from two orthogonal views: node-level and edge-level. Here we want to examine the effect of adopting NA or EA alone. We adopt GCN as the encoder to conduct experiments on four biased synthetic datasets and two real-world datasets. GCN+DTP w/o NA or EA represents the node/edge attention scores in Equation ( <ref type="formula" target="#formula_10">6</ref>)/( <ref type="formula" target="#formula_11">7</ref>) is evenly set as 0.5. The experimental results are shown in Figure <ref type="figure">5</ref>. We can find that:</p><p>(1) Comparing NA with EA, the performance of DTP without NA is significantly worse than that without EA, which indicates that the node feature contains more significant information compared with graph structure. (2) Just adopting NA or EA alone still achieves better performance than baselines, which demonstrates that only applying NA or EA can also disentangle the trivial and critical subgraph to achieve deconfounded training to some extent.</p><p>2) Random Combination. We need to stratify the confounder distribution to achieve the causal intervention. With the random combination, each critical subgraph will interact with all types of trivial subgraphs with equal possibility. To verify its importance, we change the "Random Addition" module in Figure <ref type="figure" target="#fig_1">2</ref> to "Addition", which just adopts the addition operation orderly, and we rename it as GCN+DTP w/o RD. The experimental results are shown in Figure <ref type="figure">5</ref>. We can find that: (1) The performance drops severely compared with GCN+DTP, which demonstrates the importance of  (2) GCN+DTP w/o RD can also outperform the GCN baselines. We conjecture that just disentangling the critical and the trivial subgraphs makes GNN pay more attention to the critical subgraphs in the training stage, which will slightly ignore the distribution shift caused by the trivial subgraphs.</p><p>3) Loss coefficients 𝜆 1 and 𝜆 2 . According to Equation ( <ref type="formula" target="#formula_22">14</ref>), 𝜆 1 denotes the strength of the disentanglement for the critical/trivial subgraphs, while 𝜆 2 controls the strength of the causal intervention. To explore their impacts, we use GCN as an encoder and conduct experiments on two biased synthetic datasets and two real-world datasets. We fix one coefficient as 0.5 and change the other one in (0, 1) with a step size of 0.1. The experimental results are shown in Figure <ref type="figure">6</ref>. We can find that: (1) 𝜆 1 achieves better performance in a range of 0.3 ∼ 0.7. Too small or too large values will cause performance degradation. (2) 𝜆 2 is not as stable as 𝜆 1 . The optimal range is around 0.3 ∼ 0.5. It leads to a strong decline at 0.5 ∼ 0.8, which indicates that coefficient 𝜆 2 should be set prudently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization and Analysis (RQ4)</head><p>In this section, we first give visualizations to depict the captured critical subgraphs. Then we show the distribution of the misclassification to explain the performance improvements.</p><p>1) Critical subgraphs. We plot node/edge attention areas of the critical subgraphs based on the attention scores in DTP. We utilize the GCN-based encoder and apply DTP on SYN-0.7 and MNIST superpixel datasets. The visualizations are shown in Figure <ref type="figure" target="#fig_8">7</ref>. Nodes with darker colors and edges with wider lines indicate higher attention scores. We surprisingly find that almost all the darker   colors and the wider lines precisely distribute on the deterministic areas, such as the critical graphs we defined in synthetic graphs and the nodes located on digit pixel in MNIST superpixel graphs. It further demonstrates that the proposed DTP can effectively capture the reliable and critical subgraphs to make stable predictions.</p><p>2) The explanation for performance improvements. Figure <ref type="figure" target="#fig_10">8</ref> displays the distribution of misclassification on SYN-0.95. The abscissa represents the predicted labels, and the ordinate denotes ground-truths. The numbers in each row denote the proportion for each class. Figure <ref type="figure" target="#fig_10">8</ref> (Left) shows that the wrong predictions of graphs with "BA" trivial graphs mainly distribute in "Cycle", "Grid" and "Diamond" classes, while the wrong predictions of graphs with "Tree" mainly concentrate on "House" class (highlighted by the red circle). Because most of the "House" graphs co-occur with "Tree" in the training stage, the other three critical graphs with "Tree" trivial graphs will mainly be misclassified as "House" in the inference stage. On the other hand, only a few "House" graphs co-occur with "BA", so the other three critical graphs with "BA" will almost not be misclassified as "House". In contrast, Figure <ref type="figure" target="#fig_10">8</ref> (Right) shows that, by applying DTP on GCN, the concentration of misclassification is obviously alleviated. This further demonstrates that DTP improves performance by mitigating spurious correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK 5.1 Graph Representation Learning</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref> have become a powerful tool for processing graph structure data. Prevailing works use global readout functions such as mean or sum <ref type="bibr" target="#b28">[29]</ref> to summarize the graph-level representation, which can be used for the downstream tasks such as graph classification <ref type="bibr" target="#b28">[29]</ref> or graph regression <ref type="bibr" target="#b4">[5]</ref>. It can be seen in the works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref> that some key subgraphs determine the GNNs' prediction. Based on these observations, numerous efforts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> have been proposed to capture the distinguishable subgraph or information from graphs. Top-𝑘 Pool <ref type="bibr" target="#b6">[7]</ref> selects the significant nodes based on the importance score.</p><p>DiffPool <ref type="bibr" target="#b32">[33]</ref> utilizes a differentiable graph pooling module that can learn assignment matrices in an end-to-end fashion. SortPool <ref type="bibr" target="#b39">[40]</ref> sorts the embeddings for nodes based on the structural roles of a graph. SAGPool <ref type="bibr" target="#b13">[14]</ref> decides the preserving nodes according to the self-attention mask. Existing works make efforts to extract the seemingly distinguishable information from graphs, but they still stay at how to better fit the statistical correlations between graphs and labels. This causal relationship is unreliable when there exists distribution shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Out-of-distribution Generalization</head><p>Out-of-distribution (OOD) generalization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> has been extensively explored in recent years. IRM <ref type="bibr" target="#b1">[2]</ref> respectively minimize the empirical risk under different environments. Group-DRO <ref type="bibr" target="#b20">[21]</ref> adversarially explores the group with the worst risk, and achieves robust generalization by minimizing the empirical risk of the worst group. Existing works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> mainly focus on computer vision and natural language processing tasks, while the GNN field is of great need but largely unexplored, due to the challenging and irregular graph structure. Furthermore, they need the environment or group prior information for each sample, which is expensive to manually label in practice. Distinct from them, we exploit the environment or group information from the graph data, and adopt causal intervention to strengthen the reliable causal relationship between critical information and label, thereby achieving stable predictions in diverse distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Causal Inference</head><p>Causal inference <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> in machine learning is to endow the model with the ability to capture real causality. A growing number of works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> have shown that causal inference is beneficial to diverse computer vision tasks. <ref type="bibr" target="#b37">[38]</ref> uses backdoor adjustment to eliminate the confounder in weakly supervised semantic segmentation tasks. <ref type="bibr" target="#b8">[9]</ref> proposes to distill the colliding effect between the old and the new data to improve class-incremental learning.</p><p>Unlike computer vision, the application of causal intervention in the GNN community is still in its infancy. Recent work <ref type="bibr" target="#b5">[6]</ref> explores how to select trustworthy neighbors for GNN in the inference stage, and demonstrates its effectiveness in node classification tasks. <ref type="bibr" target="#b36">[37]</ref> studies the connection between GNNs and SCM from a theoretical perspective. Different from them, we introduce a causal intervention strategy to mitigate the confounding effect for GNNs in the training stage. It pushes GNNs to capture stable and reliable causal relationships, which will enhance the robustness against the distribution shift. To our best knowledge, we are the first to propose a practical end-to-end training framework to overcome the distribution shift issue for GNNs from a causal perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORKS</head><p>In this work, we revisit graph representation learning from a causal perspective and find that the trivial information is a confounder. It establishes a backdoor path between the critical information and label, which misleads GNNs to learn spurious correlations. To mitigate the confounding effect, we propose the Deconfounded Training Paradigm (DTP) to endow GNNs with the ability to capture critical subgraphs. Specifically, we disentangle the representations of the critical and trivial subgraph. Then we make the given critical subgraphs implicitly interact with diverse trivial subgraphs to achieve a stable prediction across different distributions. Extensive experimental results and insightful visualizations verify its effectiveness and rationality.</p><p>Future works include adopting powerful disentanglement methods and more advanced causal intervention strategies to improve DTP. We will also make efforts to apply DTP to other graph learning tasks, such as node classification or link prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed Structural Causal Model (SCM) for graph representation learning. 𝐶 → 𝑅 → 𝑌 forms a reliable causal path, while 𝐶 ← 𝐺 → 𝑇 → 𝑅 → 𝑌 opens a backdoor path that brings spurious correlations between 𝐶 and 𝑌 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overview of the proposed Deconfounded Training Paradigm (DTP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of synthetic graphs. (Left): The visualizations of the proposed four types of critical graphs and two types of trivial graphs. (Right): The visualizations of four selected synthetic graph examples.</figDesc><graphic url="image-5.png" coords="5,384.79,113.53,87.16,114.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance discount on synthetic graphs with different bias levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 1 . 3</head><label>13</label><figDesc>Hyper-parameters. All training hyper-parameters and model configurations are summarized in Appendix A.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: The comparison of different components in DTP.</figDesc><graphic url="image-9.png" coords="7,342.03,226.30,192.82,96.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualizations of node and edge attention of critical subgraphs from the DTP. Nodes with darker colors and edges with wider lines indicate higher attention scores. (Top): Synthetic graphs, (Bottom): MNIST superpixel graphs.the causal intervention. Furthermore, disentanglement and causal intervention can help each other to improve their own effectiveness.(2) GCN+DTP w/o RD can also outperform the GCN baselines. We conjecture that just disentangling the critical and the trivial subgraphs makes GNN pay more attention to the critical subgraphs in the training stage, which will slightly ignore the distribution shift caused by the trivial subgraphs.3) Loss coefficients 𝜆 1 and 𝜆 2 . According to Equation (14), 𝜆 1 denotes the strength of the disentanglement for the critical/trivial subgraphs, while 𝜆 2 controls the strength of the causal intervention. To explore their impacts, we use GCN as an encoder and conduct experiments on two biased synthetic datasets and two real-world datasets. We fix one coefficient as 0.5 and change the other one in (0, 1) with a step size of 0.1. The experimental results are shown in Figure6. We can find that: (1) 𝜆 1 achieves better performance in a range of 0.3 ∼ 0.7. Too small or too large values will cause performance degradation. (2) 𝜆 2 is not as stable as 𝜆 1 . The optimal range is around 0.3 ∼ 0.5. It leads to a strong decline at 0.5 ∼ 0.8, which indicates that coefficient 𝜆 2 should be set prudently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The misclassification distribution. (Left): Standard training on GCN, (Right): DTP on GCN, Red circle highlights the concentration degree of misclassification distribution.</figDesc><graphic url="image-18.png" coords="8,84.77,96.95,78.77,157.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>denotes the 𝑑-dimensional representations of all nodes, whose 𝑖-th row h 𝑖 denotes the representation of node 𝑣 𝑖 . Then we adopt two MLPs: MLP node (•) and MLP</figDesc><table /><note>edge (•) to estimate the attention scores from two orthogonal perspectives: node-level and edge-level. For node 𝑣 𝑖 and edge (𝑣 𝑖 , 𝑣 𝑗 ) we can obtain:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>𝜎 (•) is softmax function, || denotes concatenation operation; 𝛼 𝑐 𝑖 , 𝛽 𝑐 𝑖 𝑗 represent the node-level attention score for node 𝑣 𝑖 and edge-level attention score for edge (𝑣 𝑖 , 𝑣 𝑗 ) in critical subgraph; analogously, 𝛼 𝑡 𝑖 , 𝛽 𝑡 𝑖 𝑗 are for the trivial subgraph. Note that 𝛼 𝑐 𝑖 + 𝛼 𝑡 𝑖 = 1, and 𝛽 𝑐 𝑖 𝑗 + 𝛽 𝑡 𝑖 𝑗 = 1. The attention scores indicate how much the model pays attention to each node or edge in the corresponding subgraph. Now we can construct the masks M 𝑥 , M 𝑥 , M 𝑎 , and M 𝑎 based on the attention scores 𝛼 𝑐 𝑖 , 𝛼 𝑡 𝑖 , 𝛽 𝑐 𝑖 𝑗 , and 𝛽 𝑡 𝑖 𝑗 , respectively. Please note that the node-level attention score 𝛼 𝑐 𝑖 or 𝛼 𝑡 𝑖 is scalar designed for the whole feature vector, so mask M 𝑥 or M 𝑥 keeps the same value in each row. Finally, we decompose the original graph G into the critical subgraph</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Test Accuracy (%) of graph classification on synthetic datasets with diverse biases. The number in brackets represents the performance degradation compared with the unbiased dataset. Our methods are highlighted with a gray background.</figDesc><table><row><cell>Method</cell><cell>SYN-0.1</cell><cell>SYN-0.3</cell><cell>Unbiased</cell><cell>SYN-0.7</cell><cell>SYN-0.9</cell></row><row><cell>SortPool [40]</cell><cell>80.70 (↓ 14.24%)</cell><cell>92.33 (↓ 1.88%)</cell><cell>94.10</cell><cell>92.14 (↓ 2.08%)</cell><cell>90.35 (↓ 3.99%)</cell></row><row><cell>DiffPool [33]</cell><cell>82.28 (↓ 8.69%)</cell><cell>88.02 (↓ 2.32%)</cell><cell>90.11</cell><cell>88.83 (↓ 1.42%)</cell><cell>84.50 (↓ 6.23%)</cell></row><row><cell>Top-𝑘 Pool [7]</cell><cell>84.31 (↓ 11.81%)</cell><cell>93.53 (↓ 2.17%)</cell><cell>95.60</cell><cell>94.44 (↓ 1.21%)</cell><cell>88.02 (↓ 7.93%)</cell></row><row><cell>SAGPool [14]</cell><cell>88.08 (↓ 7.82%)</cell><cell>90.86 (↓ 4.91%)</cell><cell>95.55</cell><cell>92.22 (↓ 3.49%)</cell><cell>83.99 (↓ 12.10%)</cell></row><row><cell>GCN [12]</cell><cell>84.94 (↓ 6.60%)</cell><cell>89.38 (↓ 1.72%)</cell><cell>90.94</cell><cell>90.25 (↓ 0.76%)</cell><cell>86.00 (↓ 5.43%)</cell></row><row><cell>GCN + DTP</cell><cell>89.38 (↓ 6.03%)</cell><cell>93.50 (↓ 1.70%)</cell><cell>95.12</cell><cell>95.06 (↓ 0.06%)</cell><cell>93.31 (↓ 1.90%)</cell></row><row><cell>GIN [29]</cell><cell>87.50 (↓ 9.55%)</cell><cell>93.94 (↓ 2.89%)</cell><cell>96.74</cell><cell>94.88 (↓ 1.92%)</cell><cell>89.62 (↓ 7.36%)</cell></row><row><cell>GIN + DTP</cell><cell>93.19 (↓ 3.87%)</cell><cell>96.31 (↓ 0.65%)</cell><cell>96.94</cell><cell>96.56 (↓ 0.39%)</cell><cell>95.25 (↓ 1.74%)</cell></row><row><cell>GAT [27]</cell><cell>84.62 (↓ 8.71%)</cell><cell>89.50 (↓ 3.44%)</cell><cell>92.69</cell><cell>92.31 (↓ 0.41%)</cell><cell>87.62 (↓ 5.47%)</cell></row><row><cell>GAT + DTP</cell><cell>92.44 (↓ 4.37%)</cell><cell>96.25 (↓ 0.42%)</cell><cell>96.66</cell><cell>96.12 (↓ 0.56%)</cell><cell>92.56 (↓ 4.24%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Test Accuracy (%) of graph classification. For TUDataset, we perform 10-fold cross-validation to evaluate the performance, and report the mean and standard derivations. Our methods are highlighted with a gray background and if the performance improves, the number is bolded.</figDesc><table><row><cell>Dataset</cell><cell>MUTAG</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>COLLAB</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>MNIST</cell><cell>CIFAR-10</cell></row><row><cell>GK [23]</cell><cell>81.58±2.11</cell><cell>62.49±0.27</cell><cell>71.67±0.55</cell><cell>72.84±0.28</cell><cell>65.87±0.98</cell><cell>43.89±0.38</cell><cell>-</cell><cell>-</cell></row><row><cell>WL [22]</cell><cell>82.05±0.36</cell><cell>82.19±0.18</cell><cell>74.68±0.50</cell><cell>79.02±1.77</cell><cell>73.40±4.63</cell><cell>49.33±4.75</cell><cell>-</cell><cell>-</cell></row><row><cell>DGK [31]</cell><cell>87.44±2.72</cell><cell>80.31±0.46</cell><cell>75.68±0.54</cell><cell>73.09±0.25</cell><cell>66.96±0.56</cell><cell>44.55±0.52</cell><cell>-</cell><cell>-</cell></row><row><cell>SortPool [40]</cell><cell>86.17±7.53</cell><cell>79.00±1.68</cell><cell>75.48±1.62</cell><cell>77.84±1.22</cell><cell>73.00±3.50</cell><cell>49.53±2.29</cell><cell>-</cell><cell>-</cell></row><row><cell>DiffPool [33]</cell><cell>85.61±6.22</cell><cell>75.06±3.66</cell><cell>76.25±4.21</cell><cell>79.24±1.66</cell><cell>74.47±3.84</cell><cell>49.20±3.10</cell><cell>-</cell><cell>-</cell></row><row><cell>Top-𝑘 Pool [7]</cell><cell>89.36±7.22</cell><cell>79.54±1.71</cell><cell>72.16±4.81</cell><cell>81.44±1.20</cell><cell>74.30±3.23</cell><cell>51.33±3.62</cell><cell>-</cell><cell>-</cell></row><row><cell>SAGPool [14]</cell><cell>88.83±7.67</cell><cell>79.64±2.71</cell><cell>71.86±0.97</cell><cell>81.48±1.71</cell><cell>73.60±4.58</cell><cell>52.20±2.79</cell><cell>-</cell><cell>-</cell></row><row><cell>GCN [12]</cell><cell>87.20±4.63</cell><cell>82.97±1.36</cell><cell>75.65±3.24</cell><cell>81.72±1.64</cell><cell>73.00±5.85</cell><cell>51.53±3.28</cell><cell>90.49</cell><cell>54.68</cell></row><row><cell>GCN + DTP</cell><cell>90.14±4.27</cell><cell>83.48±1.94</cell><cell>76.28±3.65</cell><cell>82.08±2.40</cell><cell>74.40±4.55</cell><cell>52.13±2.96</cell><cell>94.58</cell><cell>56.21</cell></row><row><cell>GIN [29]</cell><cell>89.42±4.40</cell><cell>82.71±1.52</cell><cell>76.21±3.83</cell><cell>80.20±1.90</cell><cell>73.60±3.78</cell><cell>51.53±2.97</cell><cell>96.51</cell><cell>56.36</cell></row><row><cell>GIN + DTP</cell><cell>90.51±5.34</cell><cell>83.89±1.93</cell><cell>76.92±3.31</cell><cell>82.68±1.25</cell><cell>74.43±5.10</cell><cell>52.60±2.36</cell><cell>96.93</cell><cell>57.13</cell></row><row><cell>GAT [27]</cell><cell>88.58±3.52</cell><cell>82.11±1.43</cell><cell>75.96±3.26</cell><cell>81.42±1.41</cell><cell>72.70±4.37</cell><cell>50.60±3.75</cell><cell>95.53</cell><cell>64.22</cell></row><row><cell>GAT + DTP</cell><cell>89.94±3.87</cell><cell>83.55±1.42</cell><cell>76.39±3.65</cell><cell>82.12±1.95</cell><cell>73.30±4.16</cell><cell>50.93±3.84</cell><cell>95.91</cell><cell>66.16</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MORE IMPLEMENTATION DETAILS A.1 Algorithm</head><p>We provide the detailed implementation of the proposed Deconfounded Training Paradigm (DTP) in Algorithm 1. for 𝑘 ← 1 to 𝑀 do 3:</p><p>Compute 𝛼 𝑐 𝑖 , 𝛼 𝑡 𝑖 ← Equation ( <ref type="formula">6</ref>) for all nodes of G 𝑘 5:</p><p>Compute 𝛽 𝑐 𝑖 𝑗 , 𝛽 𝑡 𝑖 𝑗 ← Equation ( <ref type="formula">7</ref>) for all edges of G 𝑘 Causal loss: L caus ← Equation <ref type="bibr" target="#b11">(12)</ref> 22:</p><p>Total loss:</p><p>Update all the trainable parameters to minimize L 24: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Datasets Details</head><p>In this section, we give more details about the synthetic datasets and real-world datasets.</p><p>1) Synthetic graphs. For each synthetic graph instance, it consists of two subgraphs: trivial graph and critical graph. We introduce the proposed trivial graph and critical graph as follows:</p><p>• Trivial graph. There exist two types of trivial graphs: BA-SHAPES and Tree. The BA-SHAPES is a Barabási-Albert (BA) graph <ref type="bibr" target="#b2">[3]</ref>, and we abbreviate it as "BA" in this paper. The "Tree" graph is a base 12-level balanced binary tree <ref type="bibr" target="#b31">[32]</ref>. To reduce the influence, we control the number of nodes in the two kinds of trivial graphs to be similar. • Critical graph. There are four types of critical graph: "House", "Cycle", "Grid", "Diamond". The visualizations of these trivial graphs and critical graphs are depicted in Figure <ref type="figure">3</ref> (Left).</p><p>For each synthetic graph instance, a critical graph is randomly attached on one node of a trivial graph. Then the resulting graph is further perturbed by adding 10% random edges. We take the one-hot form of the node degree as the node feature and set the dimension of node feature to 20. The synthetic graph examples are displayed in Figure <ref type="figure">3</ref> (Right). The statistics of the synthetic datasets are summarized in Table <ref type="table">3</ref>. We split the dataset into training, validation and testing set with the ratio of 7: 1: 2. </p><p>2) Real-world graphs. To demonstrate the practicality of the proposed DTP, we conduct experiments on TUDataset <ref type="bibr" target="#b15">[16]</ref> and Superpixel graphs <ref type="bibr" target="#b12">[13]</ref>. For TUDataset, we gather three biological datasets (MUTAG, NCI1, PROTEINS) and three social networks datasets (COLLAB, IMDB-B, IMDB-M), which are commonly used in graph classification benchmarks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>. Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>, we use 10-fold cross-validation and report average accuracy and standard deviation. The superpixel graphs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> includes MNIST and CIFAR-10, which are classical image classification datasets converted into graphs using superpixels technology <ref type="bibr" target="#b0">[1]</ref> and assigning each node's features as the superpixel coordinates and intensity. Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>, we split the MNIST and CIFAR-10 to 55K training/5K validation/10K testing, and 45K training/5K validation/10K testing, respectively. All the detailed statistics about the real-world datasets are summarized in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyper-parameters</head><p>As for training parameters, we train the models for 100 epochs with batch size of 128. We optimize all models with the Adam <ref type="bibr" target="#b10">[11]</ref> optimizer and default choice of learning rate (i.e., 0.001) for all experiments. For SYN-𝑏 and TUDataset, we use GCN, GIN and GAT as GNN encoders with 3 layers and 128 hidden units. For Superpixel graphs MNIST and CIFAR-10, we use the GNN encoders with 4 layers and 146 hidden units as <ref type="bibr" target="#b4">[5]</ref>. For all the baselines, we follow the default settings from original papers and reproduce the missing results. For the proposed DTP, we search 𝜆 1 and 𝜆 2 in (0.1, 1.0) with a step size of 0.1 and report the results with the best settings. We adopt NVIDIA 2080 Ti (11GB GPU) to conduct all our experiments, the training time comparison is shown as Table <ref type="table">4</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réka</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName><forename type="first">Asim</forename><surname>Kumar Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corwin</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Should graph convolution trust neighbors? a simple causal inference method</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1208" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distilling Causal Effect of Data in Class-Incremental Learning</title>
		<author>
			<persName><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3957" to="3966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How to find your friendly neighborhood: Graph attention design with self-supervision</title>
		<author>
			<persName><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02850</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generative Causal Explanations for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06643</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Counterfactual vqa: A cause-effect look at language bias</title>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12700" to="12710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interpretation and identification of causal mediation</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">459</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Models, reasoning and inference</title>
				<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cam-bridgeUniversityPress</publisher>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05761</idno>
		<title level="m">The risks of invariant risk minimization</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization</title>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SUGAR: Subgraph neural network with reinforcement pooling and self-supervised mutual information mechanism</title>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanxing</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2081" to="2091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect</title>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Subgraph networks with application to structural feature space expansion</title>
		<author>
			<persName><forename type="first">Jinhuan</forename><surname>Qi Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenbo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanrong</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9240</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical graph representation learning with differentiable pooling</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">XGNN: Towards Model-Level Explanations of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Hao Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Hao Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15445</idno>
		<title level="m">Explainability in graph neural networks: A taxonomic survey</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Counterfactual zero-shot and open-set visual recognition</title>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15404" to="15414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Matej</forename><surname>Zečević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Dhami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:2109.04173</idno>
		<title level="m">Relating Graph Neural Networks to Structural Causal Models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Xiansheng Hua, and Qianru Sun</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An endto-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
