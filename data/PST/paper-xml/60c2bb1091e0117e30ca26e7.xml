<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-08">8 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Avisek</forename><surname>Lahiri</surname></persName>
							<email>avisek@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Kwatra</surname></persName>
							<email>kwatra@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Frueh</surname></persName>
							<email>frueh@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Lewis</surname></persName>
							<email>jplewis@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Bregler</surname></persName>
							<email>bregler@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-08">8 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.04185v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a video-based learning framework for animating personalized 3D talking faces from audio. We introduce two training-time data normalizations that significantly improve data sample efficiency. First, we isolate and represent faces in a normalized space that decouples 3D geometry, head pose, and texture. This decomposes the prediction problem into regressions over the 3D face shape and the corresponding 2D texture atlas. Second, we leverage facial symmetry and approximate albedo constancy of skin to isolate and remove spatio-temporal lighting variations. Together, these normalizations allow simple networks to generate high fidelity lip-sync videos under novel ambient illumination while training with just a single speaker-specific video. Further, to stabilize temporal dynamics, we introduce an auto-regressive approach that conditions the model on its previous visual state. Human ratings and objective metrics demonstrate that our method outperforms contemporary state-of-the-art audio-driven video reenactment benchmarks in terms of realism, lip-sync and visual quality scores. We illustrate several applications enabled by our framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>"Talking head" videos, consisting of closeups of a talking person, are widely used in newscasting, video blogs, online courses, etc. Other applications that feature talking faces prominently are face-to-face live chat, 3D avatars and animated characters in games and movies. We present a deep learning approach to synthesize 3D talking faces (both photorealistic and animated) driven by an audio speech signal. We use speaker-specific videos to train our model in a data-efficient manner by employing 3D facial tracking. The resulting system has multiple applications, including video editing, lip-sync for dubbing of videos in a new language, personalized 3D talking avatars in gaming, VR and CGI, as well as compression in multimedia communication. The importance of talking head synthesis has led to a variety of methods in the research literature. Many recent techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> use the approach of regressing facial motion from audio, employing it to deform one or more reference images of the subject. These approaches can inherit the realism of the reference photos, however, the results do not accurately reproduce 3D facial articulation and appearance under general viewpoint and lighting variations. Another body of research predicts 3D facial meshes from audio <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11]</ref>. These approaches are directly suitable for VR and gaming applications. However, visual realism is often restricted by the quality of texturing. Some recent approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref> attempt to bridge the gap by combining 3D prediction with high-quality rendering, but are only able to edit fixed target videos that they train on.</p><p>Our work encompasses several of the scenarios mentioned above. We can use 3D information to edit 2D video, including novel videos of the same speaker not seen during training. We can also drive a 3D mesh from audio or text-to-speech (TTS), and synthesize animated characters by predicting face blendshapes.</p><p>Next, we highlight some of our key design choices. Personalized models: We train personalized speakerspecific models, instead of building a single universal model to be applied across different people. While universal models like Wav2Lip <ref type="bibr" target="#b29">[30]</ref> are easier to reuse for novel speakers, they need large datasets for training and do not adequately capture person-specific idiosyncrasies <ref type="bibr" target="#b4">[5]</ref>. Personalized models like ours and NVP <ref type="bibr" target="#b32">[33]</ref> produce results with higher visual fidelity, more suitable for editing long speaker-specific videos. Additionally, our model can be trained entirely using a single video of the speaker.</p><p>3D pose normalization: We use a 3D face detector <ref type="bibr" target="#b19">[20]</ref> to obtain the pose and 3D landmarks of the speaker's face in the video. This information allows us to decompose the face into a normalized 3D mesh and texture atlas, thus decoupling head pose from speech-induced face deformations, e.g. lip motion and teeth/tongue appearance.</p><p>Lighting normalization: We design a novel algorithm for removing spatial and temporal lighting variations from the 3D decomposition of the face by exploiting traits such as facial symmetry and albedo constancy of the skin. This lighting normalization removes another confounding factor that can otherwise affect the speech-to-lips mapping.</p><p>Data-efficient learning: Our model employs an encoder-decoder architecture that computes embeddings from audio spectrograms, and decodes them to predict the decomposed 3D geometry and texture. Pose and lighting normalization allows us to train this model in a dataefficient manner. The model complexity is greatly reduced, since the network is not forced to disentangle unrelated head pose and lighting changes from speech, allowing it to synthesize high quality lip-sync results even from short training videos (2-5 minutes long). Lighting normalization allows training and inference illumination to be different, which obviates the need to train under multiple lighting scenarios. The model predicts 3D talking faces instead of just a 2D image, even though it learns just from video, broadening its applicability. Finally, pose and lighting normalization can be applied in a backward fashion to align and match the appearance of the synthesized face with novel target videos. See Figure <ref type="figure" target="#fig_0">1</ref> for an overview of our approach.</p><p>Our key technical contributions are: • A method to convert arbitrary talking head video footage into a normalized space that decouples 3D pose, geometry, texture, and lighting, thereby enabling data-efficient learning and versatile high-quality lip-sync synthesis for video and 3D applications.</p><p>• A novel algorithm for normalizing facial lighting in video that exploits 3D decomposition and face-specific traits such as symmetry and skin albedo constancy.</p><p>• To our best knowledge, this is the first attempt at disentangling pose and lighting from speech via data prenormalization for personalized models.</p><p>• An easy-to-train auto-regressive texture prediction model for temporally smooth video synthesis.</p><p>• Human ratings and objective metrics suggest that our method outperforms contemporary audio-driven video reenactment baselines in terms of realism, lip-sync and visual quality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Audio-driven 3D Mesh Animation: These methods generate 3D face models driven by input audio or text, but do not necessarily aim for photorealism. In <ref type="bibr" target="#b37">[38]</ref>, the authors learn a Hidden Markov Model (HMM) to map Mel-frequency Cepstral Coefficients (MFCC) to PCA model parameters.</p><p>Audio features are mapped to Jali <ref type="bibr" target="#b12">[13]</ref> coefficients in <ref type="bibr" target="#b43">[44]</ref>.</p><p>In <ref type="bibr" target="#b18">[19]</ref>, the authors learn to regress to 3D vertices of a face model conditioned on input audio spectrograms and simultaneously disambiguate variations in facial expressions unexplained by audio. In <ref type="bibr" target="#b17">[18]</ref>, the authors learn to regress blendshapes of a 3D face using the combined audio-visual embedding from a deep network. VOCA <ref type="bibr" target="#b11">[12]</ref> pre-registers subject-specific 3D mesh models using FLAME <ref type="bibr" target="#b25">[26]</ref> and then learns (using hours of high quality 4D scans) an offset to that template based on incoming speech, represented with DeepSpeech <ref type="bibr" target="#b14">[15]</ref> features.</p><p>Audio-driven Video Synthesis: These methods aim to generate visually plausible 2D talking head videos, conditioned on novel audio. In <ref type="bibr" target="#b5">[6]</ref>, an audio-visual correlation loss is used to match lip shapes to speech, while maintaining the identity of the target face. In <ref type="bibr" target="#b6">[7]</ref>, a two-stage cascaded network is used to first predict 2D facial landmarks from audio, followed by target frame editing conditioned upon these landmarks. In <ref type="bibr" target="#b35">[36]</ref>, the authors leverage a temporal GAN for synthesizing video conditioned on audio and a reference frame. They improve it further in <ref type="bibr" target="#b36">[37]</ref> via a specialized lip-sync discriminator. In contrast to our approach, the above methods fail to produce full-frame outputs; instead they generate normalized cropped faces, whose lips are animated based on input audio and a reference frame. Among efforts on full-frame synthesis, Video Rewrite <ref type="bibr" target="#b4">[5]</ref> was a pioneering work.</p><p>It represented speech with phonetic labels and used exemplar-based warping for mouth animation. Speech2Vid <ref type="bibr" target="#b7">[8]</ref> learns a joint embedding space for representing audio features and the target frame, and uses a shared decoder to transform the embedding into a synthesized frame. X2Face <ref type="bibr">[40]</ref> learns to drive a target frame with the head pose and expression of another source video, and it can optionally be also driven by an audio to animate a target frame. A framework to translate an input speech to another language and then modify the original video to match it is presented in <ref type="bibr" target="#b22">[23]</ref>. Recently, Wav2Lip <ref type="bibr" target="#b29">[30]</ref> reported appreciable lip-sync performance by using a powerful offline lip-sync discriminator <ref type="bibr" target="#b8">[9]</ref> as an expert to train their generator. While currently this is one of the best universal models, it lacks the visual fidelity of speaker-specific models.</p><p>Some recent works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref> have focused on 3D model guided video synthesis. In <ref type="bibr" target="#b31">[32]</ref> an RNN regresses audio to mouth shape, producing convincing results on President Obama. The approach required very extensive training data however (17 hours). In <ref type="bibr" target="#b32">[33]</ref>, the DeepSpeech RNN is used to map input speech to audio expression units which then drive a blendshapes-based 3D face model. Finally, a neural renderer <ref type="bibr" target="#b21">[22]</ref> is used to render the face model with the audio expressions. Since neural renderer training depends on target illumination, the methods leveraging such rendering <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref> suffer from the need for retraining if inference-time lighting conditions change. On the contrary, our method seamlessly adapts to novel lighting.</p><p>Text-based Video Editing: In <ref type="bibr" target="#b13">[14]</ref>, the authors present a framework for text based editing of videos (TBE). They first align written transcripts to audio and track each frame to create a face model. During edit operations, a (slow) viseme search is done to find best matching part of training video. This method needs a time-aligned transcript and around one hour of recorded data, and is mostly suitable for for small edits. Our method, on the other hand, relies on just the audio signal and can synthesize videos of unrestricted length.</p><p>Actor-driven Video Synthesis: <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22]</ref> present techniques for generating and dubbing talking head videos by transferring facial features, such as landmarks or blendshape parameters, from a different actor's video. These techniques generate impressive results, however they require a video of a surrogate actor to drive synthesis. We emphasize that our approach uses only audio or text-to-speech (TTS) as the driving input, and does not require any actors for dubbing. It is therefore fundamentally different from these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We now describe the various components of our approach including data extraction and normalization, neural network architecture and training, and finally, inference and synthesis. Figure <ref type="figure" target="#fig_1">2</ref> shows an overview of our model. We extract the audio channel from the training video and transform it into frequency-domain spectrograms. These spectrograms are computed using Short-time Fourier transforms (STFT) with a Hann window function <ref type="bibr" target="#b38">[39]</ref>, over 30ms wide sliding windows that are 10ms apart. We align these STFTs with video frames and stack them across time to create a 256 × 24 complex spectrogram image, spanning 240ms centered around each video frame. Our model predicts the face geometry, texture, and optionally, blendshape coefficients, for each frame based on the audio spectrogram.</p><p>The face in the video is tracked using a 3D face landmark detector <ref type="bibr" target="#b19">[20]</ref>, resulting in 468 facial features, with the depth (z-component) predicted using a deep neural network. We refer to these features as vertices, which are accompanied by a predefined triangulated face mesh with fixed topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Normalizing Training Data</head><p>We preprocess the training data to eliminate the effects of head movement and lighting variations, and work with normalized facial geometry and texture. Both training and inference take place in this normalized space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Pose normalization</head><p>For pose normalization, we first select one frame of the input video as a reference frame, and its respective 3D face feature points as reference vertices. The choice of frame is not critical; any frame where the face is sufficiently frontal is suitable. Using the reference vertices, we define a reference cylindrical coordinate system (similar to <ref type="bibr" target="#b3">[4]</ref>) with a vertical axis such that most face vertices are equidistant to the axis. We then scale the face size such that the eyes and nose project to fixed locations on this reference cylinder.</p><p>Next, for each frame of the training video, we stabilize the rigid head motion (see <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>) to provide a registered 3D mesh suitable for training our geometry model. Specifically, we approximately align the vertices of the upper, more rigid parts of the face with corresponding vertices in the normalized reference using Umeyama's algorithm <ref type="bibr" target="#b34">[35]</ref> and apply the estimated rotation R, translation t and scale c to all tracked vertices v as r = cRv + t.</p><p>We use these normalized vertices, along with the cylindrical mapping defined above, to create a pose-invariant, frontalized projection of the face texture for each video frame (including the reference frame). Mapping the face vertices to the reference cylinder creates a set of 2D texture coordinates for the face's surface, which are used to unroll its texture. We warp the triangles associated with these coordinates from the source frame onto the texture domain, resulting in a 256×256 texture atlas that resembles a frontal view of the face, but with the non-rigid features like the lips and mouth moving with the speech. Figure <ref type="figure" target="#fig_2">3</ref> demonstrates the effect of normalization; the head pose is removed, but the moving lip shapes and mouth interior are preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Lighting normalization</head><p>We normalize the frontalized texture atlas to remove lighting variations, which are mostly caused by head motion or changing illumination. Our lighting normalization algorithm works in two phases. It first exploits facial symmetry to normalize the reference atlas R spatially, removing specularities and lighting variations that run across the face. It then performs a temporal normalization across video frames that transforms each frame's atlas F to match the illumination of R. The resulting atlases have a more uniform albedo-like appearance, that stays consistent across frames.</p><p>We first describe the temporal normalization algorithm, as it is a core component also used during spatial normalization. This algorithm assumes that the two textures F and R are pre-aligned geometrically. However, any non-rigid facial movements, e.g. from speech, can result in different texture coordinates, and consequently, misalignments between R and F . Hence, we first warp R to align it with F 's texture coordinates, employing the same triangle-based warping algorithm used for frontalization.</p><p>Given the aligned R and F , we estimate a mapping that transforms F to match the illumination of R. This mapping is composed of a smooth multiplicative pixel-wise gain G in the luminance domain, followed by a global channel-wise gain and bias mapping {a, b} in the RGB domain. The resulting normalized texture F n is obtained via the following steps: <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_0">(F y , F u , F v ) = RGBtoYUV(F ); (2) F l y = G * F y ; (3) F l = YUVtoRGB(F l y , F u , F v ); (4) F n = aF l + b.</formula><p>Gain Estimation: To estimate the gain G, we observe that a pair of corresponding pixels at the same location k in F and R should have the same underlying appearance, modulo any change in illumination, since they are in geometric alignment (see Figure <ref type="figure" target="#fig_3">4(C)</ref>). This albedo constancy assumption, if perfectly satisfied, yields the gain at pixel k as G k = R k /F k . However, we note that (a) G is a smoothly varying illumination map, and (b) albedo constancy may be occasionally violated, e.g. in non-skin pixels like the mouth, eyes and nostrils, or where the skin deforms sharply, e.g. the nasolabial folds. We account for these factors by, firstly, estimating G k over a larger patch p k centered around k, and secondly, employing a robust estimator that weights pixels based on how well they satisfy albedo constancy. We formulate estimating G k as minimizing the error:</p><formula xml:id="formula_1">E k = j∈p k W j R j − G k * F j 2 , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where W is the per-pixel weights image, and solve it using iteratively reweighted least squares (IRLS). In particular, we initialize the weights uniformly, and then update them after each (i th ) iteration as:</p><formula xml:id="formula_3">W i+1 k = exp −E i k T ,<label>(2)</label></formula><p>where T is a temperature parameter. The weights and gain converge in 5-10 iterations; we use T = 0.1 and a patch size of 16 × 16 pixels for 256 × 256 atlases. Figure <ref type="figure" target="#fig_3">4(C)</ref> shows example weights and gain images. Pixels with large error E k get low weights, and implicitly interpolate their gain values from neighboring pixels with higher weights.</p><p>To estimate the global color transform {a, b} in closed form, we minimize k W k R k −aF k −b 2 over all pixels, with W k now fixed to the weights estimated above.</p><p>Reference Atlas Normalization using Facial Symmetry: We first estimate the gain G m between the reference R and its mirror image R , using the algorithm described above. This gain represents the illumination change between the left and right half of the face. To obtain a reference with uniform illumination, we compute the symmetrized gain G s = max(G m , G m ), where G m is the mirror image of G m , i.e. for every symmetric pair of pixels, we make the darker pixel match the brighter one. The normalized reference is then R n = G s * R, as shown in Figure 4(B). Note that our weighting scheme makes the method robust to inherent asymmetries on the face, since any inconsistent pixel pairs will be down-weighted during gain estimation, thereby preserving those asymmetries.</p><p>Specularity Removal: We remove specularities from the face before normalizing the reference and video frames, since they are not properly modeled as a multiplicative gain, and also lead to duplicate specularities on the reference due to symmetrization. We model specular image formation as:</p><formula xml:id="formula_4">I = α + (1 − α) * I c ,<label>(3)</label></formula><p>where I is the observed image, α is the specular alpha map and I c is the underlying clean image without specularities. We first compute a mask, where α &gt; 0, as pixels whose minimum value across RGB channels in a smoothed I exceeds the 90 th percentile intensity across all skin pixels in I. The face mesh topology is used to identify and restrict computation to skin pixels. We then estimate a pseudo clean image Ĩc by hole-filling the masked pixels from neighboring pixels, and use it to estimate α = (I − Ĩc )/(1 − Ĩc ).</p><p>The final clean image is then I c = (I − α)/(1 − α). Note that our soft alpha computation elegantly handles any erroneous over-estimation of the specularity mask (see Figure <ref type="figure" target="#fig_3">4(A)</ref>). The above method is specifically tailored for stabilized face textures and is simple and effective, thus we do not require more generalized specularity removal techniques <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint Prediction Model and Training Pipeline</head><p>In this section we describe the framework for learning a function F to jointly map from domain S of audio spectrograms to the domains V of vertices and A of texture atlases:</p><formula xml:id="formula_5">F : S → V ×A, with V ∈ R 468×3 and A ∈ R 128×128×3 ,</formula><p>where for the purpose of prediction, we crop the texture atlas to a 128 × 128 region around the lips, and only predict these cropped regions. The texture for the upper face is copied over from the reference, or target video frames, depending upon the application. We follow an encoderdecoder architecture for realizing F(•), as shown in Figure <ref type="figure" target="#fig_1">2</ref>. It consists of a shared encoder for audio, but separate dedicated decoders for geometry and texture. However, the entire model is trained jointly, end-to-end.</p><p>Audio encoder: The input at time instant t is a complex spectrogram, S t ∈ R 256×24×2 . Our audio encoder -and face geometry prediction model -is inspired by the one proposed in <ref type="bibr" target="#b18">[19]</ref>, in which the vertex positions of a fixedtopology face mesh are also modified according to an audio input. However, while <ref type="bibr" target="#b18">[19]</ref> used formant preprocessing and autocorrelation layers as input, we directly use complex spectrograms S t . Each S t tensor is passed through a 12 layer deep encoder network, where the first 6 layers apply 1D convolutions over frequencies (kernel 3×1, stride 2×1), and the subsequent 6 layers apply 1D convolution over time (kernel 1 × 3, stride 1 × 2), all with leaky ReLU activation, intuitively corresponding to phoneme detection and activation, respectively. This yields a latent code L s t ∈ R Ns . Geometry decoder: This decoder maps the latent audio code L s t to vertex deformations δ t , which are added to the reference vertices V r to obtain the predicted mesh Vt = V r + δ t . It consists of two fully connected layers with 150 and 1404 units, and linear activations, with a dropout layer in the middle. The resulting output is 468 vertices (1404 = 468 × 3 coordinates). As proposed in <ref type="bibr" target="#b18">[19]</ref>, we initialize the last layer using PCA over the vertex training data. Further, we impose 2 loss on the vertex positions: R geo = V t − Vt 2 , where V t are ground-truth vertices.</p><p>Texture decoder: This decoder maps the audio code L s t to a texture atlas update (difference map) ∆ t which is added to the reference atlas A r to obtain the predicted atlas, Ât = A r + ∆ t . It consists of a fully connected layer to distribute the latent code spatially, followed by progressive up-sampling using convolutional and interpolation layers to generate the 128 × 128 texture update image (see Appendix G.1). We impose an image similarity loss between the predicted and ground-truth atlas A t : R tex = d(A t , Ât ), where d is a visual distance measure. We tried different variants of d(•) including the 1 loss, Structural Similarity Loss (SSIM), and Gradient Difference Loss (GDL) <ref type="bibr" target="#b26">[27]</ref> and found SSIM to perform the best.</p><p>Blendshapes decoder: To animate CGI characters using audio, we optionally add another decoder to our network that predicts blendshape coefficients B t in addition to geometry and texture. For training, these blendshapes are derived from vertices V t by fitting them to an existing blendshapes basis either via optimization or using a pre-trained model <ref type="bibr" target="#b24">[25]</ref>. We use a single fully connected layer to predict coefficients Bt from audio code L s t , and train it using 1 loss R bs = B t − Bt 1 to encourage sparse coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Auto-regressive (AR) Texture Synthesis:</head><p>Ambiguities in facial expressions while speaking (or silent) can result in temporal jitters. We mitigate these by incorporating memory into the network. Rather than using RNNs, we condition the current output of the network (A t ) not only on S t but also on the previous predicted atlas Ât−1 , encoding it as a latent code vector L a t−1 ∈ R Na . L s t and L a t−1 are combined and passed to the texture decoder to generate the current texture Ât (Figure <ref type="figure" target="#fig_1">2</ref>). This appreciably improves the temporal consistency of synthesized results. We can train this AR network satisfactorily via Teacher Forcing <ref type="bibr" target="#b40">[41]</ref>, using previous ground truth atlases. The resulting network F is trained end-to-end, minimizing the combined loss R = R tex + α 1 R geo + α 2 R bs , where α 1 = 3.0 and α 2 = 0.3 (when enabled). We used hyperparameter search to determine the latent code lengths, N s = 32 and N a = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference and Synthesis</head><p>Textured 3D mesh: During inference, our model predicts geometry and texture from audio input. To convert it to a textured 3D mesh, we project the predicted vertices onto the reference cylinder, and use the resulting 2D locations as texture coordinates. Since our predicted texture atlas is defined on the same cylindrical domain, it is consistent with the computed texture coordinates. The result is a fully textured 3D face mesh, driven by audio input (Figure <ref type="figure" target="#fig_0">1a</ref>).  Talking head video synthesis: The pose and lighting normalization transforms (Section 3.1) are invertible, i.e. one can render the synthesized face mesh in a different pose under novel lighting, which allows us to procedurally blend it back into a different target video (Figure <ref type="figure" target="#fig_0">1b</ref>). Specifically, we warp the textured face mesh to align it with the target face, then apply our lighting normalization algorithm in reverse, i.e. on the warped texture, using the target face as reference. One caveat is that the target frame's area below the chin may not align with the warped synthesized face, due to inconsistent non-rigid deformations of the jaw. Hence, we pre-process each target frame by warping the area below the original chin to match the expected new chin position. To avoid seams at border areas, we gradually blend between the original and new face geometry, and warp the original face in the target frame according to the blended geometry.</p><p>Cartoon rendering: For stylized visualizations, we can create a cartoon rendering of the textured mesh (or video), by combining bilateral filtering with a line drawing of the facial features. In particular, we identify nose, lips, cheeks and chin contours in the synthesized face mesh, and draw them prominently over the filtered texture or video frame.</p><p>CGI Characters: Models trained with the blendshapes decoder also output blendshape coefficients that can drive a CGI character. We combine these predicted blendshapes (that generally affect the lips and mouth) with other blendshapes, such as those controlling head motion and eye gaze, to create lively real-time animations. Please refer to Appendix-J and Fig. <ref type="figure" target="#fig_15">14</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our training and inference pipelines were implemented in Tensorflow <ref type="bibr" target="#b0">[1]</ref>, Python and C++. We trained our models with batch sizes of 128 frames, for 500-1000 epochs, with each epoch spanning the entire training video. Sample training times were between 3-5 hours, depending on video length (usually 2-5min). Average inference times were 3.5ms for vertices, 31ms for texture and 2ms for blendshapes, as measured on a GeForce GTX 1080 GPU. Our research-quality code for blending into target videos takes 50-150ms per frame, depending on the output resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Studies</head><p>Benefit of Auto-Regressive Prediction: The autoregressive texture prediction algorithm stabilizes mouth dynamics considerably. In Figure <ref type="figure" target="#fig_4">5</ref>, we show that without auto-regression, the model can produce an unrealistic jittering effect, especially during silent periods.</p><p>Benefit of Lighting Normalization: We use a short training video (∼4 minutes) recorded in an outdoor setting but with varying illumination. However, during inference, we select two novel environments: a) indoor lighting with continuous change of lighting direction, and b) a dark room with a face illuminated by a moving flash light. Some representative frames of models trained with and without lighting normalization are shown in Figure <ref type="figure" target="#fig_5">6(a)</ref>. Without lighting normalization, the model produces disturbing artifacts around the lip region, exacerbated by the extreme changes in illumination. However, with normalized lighting, the model adapts to widely varying novel illumination conditions. This ability to edit novel videos of the same speaker on-the-fly without needing to retrain for new target illumination is a significant benefit. In contrast, neural rendering based approaches <ref type="bibr" target="#b32">[33]</ref> require retraining on each new video, because they map 3D face models directly to the facial texture in video without disentangling illumination.   We also visualize the loss curves on held out evaluation sets in Figure <ref type="figure" target="#fig_5">6(b)</ref>. With lighting normalization, the SSIM loss (used for texture generation) saturates at a much lower value than without normalization. This supports our hypothesis that lighting normalization results in more dataefficient learning, since it achieves a better loss with the same amount of training data. The vertex loss (responsible for lip dynamics) is similar for both models, because lighting normalization does not directly affect the geometry decoder, but overall lip-sync and visual quality are improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison: Self-reenactment</head><p>We objectively evaluate our model under the selfreenactment setting (audio same as target video), since it allows us to have access to ground truth facial information. We show experiments with three talking head datasets: GRID <ref type="bibr" target="#b9">[10]</ref>, TCD-TIMIT <ref type="bibr" target="#b15">[16]</ref> and CREMA-D <ref type="bibr" target="#b20">[21]</ref>.</p><p>Comparing Methods: We perform quantitative comparisons against state-of-the-art methods whose models/results are publicly available: CVPR'19 <ref type="bibr" target="#b6">[7]</ref>, IJCV'19 <ref type="bibr" target="#b36">[37]</ref>, CVPR-W <ref type="bibr" target="#b35">[36]</ref>. It is difficult to do an apples-to-apples comparison, since we use personalized models while other techniques use a universal model. However, we minimize this gap by testing on the same 10 subjects from each of the 3 datasets used in IJCV'19 and CVPR'19, and employing the official evaluation frameworks of these papers. We also compare against other prior methods, but reuse the results already reported by CVPR'19 or IJCV'19. Details of subject IDs are provided in Appendix-H Evaluation Metrics: We follow the trend in recent papers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36]</ref>, which use SSIM (Structural Similarity Index) as a reconstruction metric, LMD (Landmark Distance) on mouth features as a shape similarity metric, CPBD (Cumulative Probability Blur Detection) <ref type="bibr" target="#b28">[29]</ref> as a sharpness metric and WER (word error rate) as a content metric to evaluate the correctness of words from reconstructed videos. Following <ref type="bibr" target="#b36">[37]</ref>, we use a LipNet model <ref type="bibr" target="#b1">[2]</ref> pretrained for lip-reading on GRID dataset <ref type="bibr" target="#b9">[10]</ref>.</p><p>Observations: We report the metrics in Figure <ref type="figure" target="#fig_10">10</ref> (left). On LMD and WER, which capture lip-sync, our model is significantly better than any competing method. Also, in terms of reconstruction measures (SSIM, CPBD), our model almost always performs better. CVPR-W and IJCV'19 have a better (though comparable) CPBD on GRID, but it is a low-resolution dataset. On higher resolution TCD-TIMIT and CREMA-D, our CPBD is the best. We also show qualitative comparisons in Figure <ref type="figure" target="#fig_6">7</ref>. Note that we synthesize full frame videos, while CVPR'19 and IJCV'19 only generated normalized face crops at a resolution of 128 × 128, and 96 × 128 respectively. Thus our method is more suitable for practical video applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison: Audio-Driven Video Dubbing</head><p>In this section we focus on 'audio-driven' video dubbing where the driving audio is different from the target video.</p><p>User Study: We conducted a user study to quantitatively compare our lip-sync and perceptual quality against the state-of-the-art audio-driven frameworks of Wav2Lip, NVP, IJCV'19 and TBE. In the study, 35 raters were each shown 29 sample clips consisting of synthetic and real videos. For competing methods, we used their released videos or generated results with their pre-trained models. The raters were asked three questions: Q1) Is the video real or fake? Q2) Rate lip-sync quality on a 3-point discrete scale. Q3) Rate visual quality on a 5-point discrete scale. We report the Mean Opinion Scores (MOS) of the questions in Figure <ref type="figure" target="#fig_10">10</ref> (right). As is evident, among the competing methods our method receives the most favorable user ratings.  Comparison with Wav2Lip <ref type="bibr" target="#b29">[30]</ref>: Unlike other imagebased methods, Wav2Lip can paste back the generated face on background video. However, compared to our model, the outputs from Wav2Lip are of low resolution. Also, at high resolution, Wav2Lip produces significant visual artifacts (see Figure <ref type="figure" target="#fig_7">8</ref>) and lip-sync starts to degrade.</p><p>Comparison with NVP <ref type="bibr" target="#b32">[33]</ref>: The lip-sync and dynamics of our model are generally better than NVP. The lip movements of NVP are clearly muted compared to our model, as seen in representative frames in Figure <ref type="figure" target="#fig_8">9</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Applications</head><p>Speech/Text-to-Video: We can create or edit talking head videos for education, advertisement, and entertainment by simply providing new audio transcripts. "Actorfree" video translation: while 'actor-driven' video translation techniques <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref> generally require a professional actor to record the entire translated audio and video, our 'actor-free' approach does not need video, and can be driven by either recorded audio, TTS, or voice cloning <ref type="bibr" target="#b16">[17]</ref>. Voice controlled Avatars: Our model's blendshapes output can be used to animate CGI characters in real-time, allowing low-bandwidth voice-driven avatars for chat, VR, and games without the need for auxiliary cameras. Assistive technologies: Voice-driven 3D faces can support accessibility and educational applications, e.g. personified assistants and cartoon animations for visualizing pronunciation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations and Conclusion</head><p>Facial expressions: We do not explicitly handle facial expressions, though our model may implicitly capture correlations between expressions and emotion in the audio track. Strong movements in the target video: When synthesized faces are blended back into a target video, emphatic hand or head movement might seem out of place. This has not proved to be a problem in our experiments. Processing speed: Our research-quality code, running at highest quality, is slightly slower than real-time. We have presented a data efficient yet robust end-to-end system for synthesizing personalized 3D talking faces, with applications in video creation and editing, 3D gaming and CGI. Our proposed pose and lighting normalization decouples non-essential factors such as head pose and illumination from speech and enables training our model on a relatively short video of a single person while nevertheless generating high quality lip-sync videos under novel ambient lighting. We envision that our framework is a promising stepping stone towards personalized audio-visual avatars and AI-assisted video content creation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Ethical Considerations</head><p>Our technology focuses on world-positive use cases and applications. Video translation and dubbing have a variety of beneficial and impactful uses, including making educational lectures, video-blogs, public discourse, and entertainment media accessible to people speaking different languages, and creating personable virtual "assistants" that interact with humans more naturally.</p><p>However, we acknowledge the potential for misuse, especially since audiovisual media are often treated as veracious information. We strongly believe that the development of such generative models by good actors is crucial for enabling preemptive research on fake content detection and forensics, which would allow them to make early advances and stay ahead of actual malicious attacks. Approaches like ours can also be used to generate counterfactuals for training provenance and digital watermarking techniques. We also emphasize the importance of acting responsibly and taking ownership of synthesized content. To that end, we strive to take special care when sharing videos or other material that have been synthesized or modified using these techniques, by clearly indicating the nature and intent of the edits. Finally, we also believe it is imperative to obtain consent from all performers whose videos are being modified, and be thoughtful and ethical about the content being generated. We follow these guiding principles in our work.  lar the lack of significance for IJCV and TBE respectively) are plausible given cursory examination of the videos. The results of IJCV'19 show qood quality lip-sync but the overall image quality is limited, thus explaining its good performance on Q2 but poor performance on Q3. TBE operates in part by re-mixing input video frames so it results in high picture quality by definition (Q3), but its lip-sync quality is poorer than our method and that of IJCV'19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison Notes on Text Based Editing [14]</head><p>According to the user study, among the 3D model based methods, Text-based-Editing (TBE) is the secondbest method (following our method). However, our framework has some distinct training and inference time advantages over TBE:</p><p>• TBE was trained on a training corpus of more than 1 hour of video recording. Our model was trained on ∼7 minutes of data in this case.</p><p>• TBE assumes an accurate text transcript and uses phoneme based alignment tools to align the text with audio. In contrast, our model only requires a speech signal as input.</p><p>• The average training time of TBE is 42 hours. Our typical training time is somewhere in between 3-5 hours. Figure <ref type="figure" target="#fig_1">12</ref>: Raw user study results. Q1: Percentage of real/fake ratings for each of the six video categories. Viewers believe our synthetic videos (proposed) are real roughly two-thirds of the time. Q2: Ratings of lip-sync quality for the six video categories, expressed as percentages. Our synthetic videos (proposed) are perceived as at least comparable to those of IJCV while being clearly superior to other competing methods in lip-sync quality. Q3: Ratings of picture quality for the six video categories, expressed as percentages. Here our method greatly outperforms IJCV (and NVP and wav2lip) while being at least comparable to TBE. It can be seen that our method is the best performer across the three questions.</p><p>• The inference speed of TBE is significantly slower. This is mainly attributed to the costly viseme search (∼5 minutes for 3 words). Our method executes within a few tens of milliseconds.</p><p>• TBE also relies on neural rendering for learning to generate facial texture based on the illumination in the training sequence. It is thus not apt for operating under new ambient lighting without further retraining. Our framework is capable of seamlessly adapting to novel lighting conditions during inference.</p><p>C. Limited comparison with Suwajanakorn et al. <ref type="bibr" target="#b31">[32]</ref> The work by Suwajanakorn et al. also involves training a personalized talking face model. However, <ref type="bibr" target="#b31">[32]</ref> only synthesized results for a single person (former U.S. President Barack Obama), using hours of training video. While our model is perfectly capable of similar synthesis, we consciously refrain from training on living political personalities due to ethical considerations. Hence, we are unable to directly compare our work with that of Suwajanakorn et al.. Nevertheless, our framework offers the following advantages: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion on LSE metrics</head><p>We have used the official code release<ref type="foot" target="#foot_0">1</ref> by the authors of Wav2Lip for evaluating the automated LSE metrics, LSE-D (lower is better) and LSE-C (higher is better). We faced two issues while using this metric: (a:) Even though user study suggest that the lip-sync quality of Wav2Lip is usually inferior to our model, the LSE metrics are always better for Wav2Lip. We observed this pattern over a range of different videos. LSE metrics are computed from paired audio-visual representation coming from a SyncNet <ref type="bibr" target="#b8">[9]</ref> network. Wav2Lip also leverages a SyncNet architecture and audio-visual representations as a lip-sync loss during training. We hypothesize that since a similar architecture is used both as a training loss and for scoring, Wav2Lip may be biased to do particularly well on this metric. Thus, we refrained from reporting LSE metrics for Wav2Lip. However, for other methods, the metric yields numbers consistent with human evaluations of lip-sync. (b:) The code sometimes fails to detect faces even under normal illumination and thereby does not give LSE metrics. So, we could not report LSE metrics on all videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. out Wav2Lip for Self-reenactment</head><p>While comparing methods for self-reenactment tasks (Figure <ref type="figure" target="#fig_10">10</ref>), we do not include Wav2Lip among the competing methods. Along with the current audio, Wav2Lip also feeds in the sequence of target frames with the lip region unmasked. In a self-reenactment setting, the input target frames are same as the final expected output from the network. Hence, Wav2Lip would have an unfair advantage over our method, since our framework is entirely audio driven and only utilizes masked target frames (around the lip region) for pasting back the synthesized output. Therefore, we do not compare Wav2Lip for self-reenactment results. A similar advantage is also available to LipGAN <ref type="bibr" target="#b22">[23]</ref> which is a precursor to the framework of Wav2Lip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Selecting Code Length for Audio and Previous Atlas</head><p>In this section we report studies to determine the code length for encoding the current time step's spectrogram and previous time step's predicted atlas. Since we wish to automatically determine acceptable settings of these parameters, the study was conducted for a self-reenactment task in which we have access to ground truth facial information.</p><p>For this study, we curated a custom dataset of subjects selected from YouTube instructional videos, webcam recordings, and studio conversations. The custom dataset had around 10,000 audio-synchronized frames. Sample frames and corresponding reconstructions from two such subjects are shown in Figure <ref type="figure" target="#fig_14">13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Selection of Previous Atlas Code Length, N a</head><p>We conducted an ablation study to determine the length N a of the latent code L A t−1 . The code length governs the contribution of the previous visual state to the current frame. With increasing code length, the model starts to incorrectly neglect the current audio input, instead basing its output mostly on the previous state. In Table <ref type="table" target="#tab_1">2</ref> we report the average metrics over different N a . Note that N a = 0 signifies a model trained without auto-regression. Both SSIM and LMD improve when using auto-regression initially but deteriorate as we increase N a , N a = 2 giving the best results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Selection of Audio Code Length, N S</head><p>As mentioned earlier, we used a 32-dimensional vector for encoding the audio spectrogram. This number was chosen by performing a parameter sweep over N S ∈ {8, 16, 32, 64, 128} on our custom dataset. In Table <ref type="table" target="#tab_3">3</ref> we compare the SSIM and LMD metrics, averaged over the subjects in the dataset. We observe that N S = 32 yields the most favorable performance. Hence, we use it as our default choice when encoding the spectrogram. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Network Architectures</head><p>In the main paper, we describe the architecture of the audio encoder, which computes the latent code from audio spectrograms, and the geometry decoder, which computes the 3D vertices from the audio latent code. Here, we describe the additional network components of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Texture Decoder Architecture</head><p>In Table <ref type="table" target="#tab_4">4</ref> we present details of the texture decoder. The input to the decoder is either a 32D vector (conditioned on only audio latent code), or 34D (conditioned on audio latent code + previous time step atlas latent code). This is followed by a series of convolution and upsampling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Auto-regressive Encoder Architecture</head><p>In our auto-regressive architecture, the previous atlas at the last time step is encoded as an additional latent vector (along with the audio encoded vector). The output is a latent vector of length N a = 2. The encoder architecture for the auto-regressive model is shown in Table <ref type="table">5</ref>.</p><p>Training by "Teacher Forcing": As stated in the main paper, during training we do not provide the actual previous predicted atlas as input to the auto-regressive model, since that would entail a recursion in the network. Instead we follow the Teacher Forcing <ref type="bibr" target="#b40">[41]</ref> paradigm of training the network with the ground truth previous atlas.</p><p>In our initial experiments, we implemented a recursive network and fed in the actual predicted previous atlas to the model during training. The reconstruction quality of that approach was worse than using ground truth atlases during training (i.e. Teacher Forcing), however. Note that during inference, the predicted previous atlas is fed to the model, because the ground truth atlas is not known at that time. Also, for predicting the first frame, we provide an 'allzeros' image as a proxy for previous frame because there is no previous frame to start with. To handle this case, we train the model by feeding it with 'all-zeros' for the previous atlas with a probability of 20%. This trains the model to reconstruct the atlas both with and without the knowledge of previous time step's visual state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Subject Details: GRID, CREMA-D and TCD-TIMIT</head><p>For self-reenactment studies we performed experiments on GRID <ref type="bibr" target="#b9">[10]</ref>, TCD TMIT <ref type="bibr" target="#b15">[16]</ref> and CREMA-D <ref type="bibr" target="#b20">[21]</ref> datasets. Following the exact setting in <ref type="bibr" target="#b36">[37]</ref>, we select the same set of 10 subjects (see Table <ref type="table" target="#tab_5">6</ref>) from each of the datasets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Sharpness of Synthesized Lip Region</head><p>In main paper, we mentioned that our method is capable of generating high quality lip-sync, and we objectively established this with the commonly used CPBD metric. The metric was evaluated on the entire face. Table <ref type="table">5</ref>: Architecture of the encoder for the previous atlas in auto-regressive mode. The encoder input is an RGB image and output is a latent vector. Each convolution layer is followed by a ReLU non-linearity. The last fully-connected (FC) layer is followed by a tanh non-linearity.</p><p>that the sharpness of the final composite full face is a primary factor of photo-realism, we also acknowledge that our method benefits from copying the texture from upper part of face from target frames.</p><p>Here we focus on determining the sharpness of only the lower half of the face (below the nostrils). In Table <ref type="table">7</ref>, we compare against Wav2Lip, LipGAN, NVP and TBE on the user study videos. Even on the lower mouth region, our method attains better CPBD scores across all competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Applications</head><p>Our approach of generating textured 3D geometry enables us to address a broader variety of applications than purely image-based or 3D-only techniques, as discussed here. Sample screenshots from some of these applications are shown in Figure <ref type="figure" target="#fig_15">14</ref>.</p><p>3D Talking Avatars: 3D avatars can make multiplayer online games and Virtual Reality (VR) environments more social and engaging. They may also be employed for audio/video chat applications and virtual visual assistants. While such avatars can be driven by a video feed from a web-cam or head-mounted camera, the ability to generate a 3D talking face from just audio obviates the need for any auxiliary camera device, and also helps preserve privacy, while reducing bandwidth requirements at the same time. Our technique supports generating both 3D textured faces as well as CGI avatars for these applications.</p><p>Video creation and editing: Our approach can be used for editing videos, e.g. to insert new content in an online course, or to correct an error without the cumbersome  <ref type="table">7</ref>: Comparing CPBD metrics (on lower half of faces) of competing methods against our proposed method. For each method, we select common pairs of videos for the competing and our method from the pool of user study videos. and sometimes impossible procedure of re-shooting the whole video under original conditions. Instead, a new audio transcript may be recorded for the edited portion, followed by applying our synthesis technique to modify the corresponding video segment. Such a speech-to-video or text-to-video system may be useful in multiple domains such as education, advertising and entertainment. We can also generate cartoon renderings for these videos, which may be preferred in some applications, e.g. sketch videos, stylized animations, or assistive technologies such as pronunciation visualization.</p><p>Video translation and dubbing: Even though we train our models on videos in a single language, they are surprisingly robust to both different languages as well as text-tospeech (TTS) audio at inference time. Using available transcripts or a speech recognition system to obtain captions, and subsequently a text-to-speech system to generate audio, we can automatically translate and lip-sync existing videos into different languages. In conjunction with appropriate video re-timing and voice-cloning <ref type="bibr" target="#b16">[17]</ref>, the resulting videos look fairly convincing. We have employed our approach for translating and dubbing videos from English to Spanish or Mandarin, and vice-versa. Notably, in contrast to narratordriven techniques <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>, our approach for video dubbing does not require a human actor in the loop, and is therefore more scalable across languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flow diagram of our approach to (a) generate a dynamically textured 3D face mesh from audio, and (b) insert the generated face mesh into a target video to create a synthesized talking head video from new audio input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Joint prediction pipeline: geometry and texture models have dedicated decoders but share the audio encoder. The texture model also depends on the previously predicted atlas. Optionally, the audio embedding can drive a 3D CGI character via a blendshape coefficients decoder. Please enlarge to see details.</figDesc><graphic url="image-6.png" coords="3,50.11,72.00,236.23,86.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pose normalization of training data. For each subject-Left: input frames with detected features (see zoomed in); Middle: normalized vertices and triangle mesh; Right: texture atlas which acts as ground truth for texture prediction.</figDesc><graphic url="image-7.png" coords="3,308.86,72.00,236.24,74.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Steps of our proposed lighting normalization during training. (A:) First step is to specularity removal from an input frame. (B:) Second step is self normalization of the reference atlas. (C:) Finally, any given training frame is normalized with respect to the pre-normalized reference atlas of step B.</figDesc><graphic url="image-8.png" coords="4,59.18,72.00,476.84,66.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Benefits of proposed auto-regressive (AR) prediction. Left: Four consecutive frames when the subject was silent. Middle: Prediction without AR. Right: Prediction with AR. In absence of AR, the model fluctuates between different visual states, while the AR substantially improves temporal stability.</figDesc><graphic url="image-9.png" coords="6,69.91,72.00,455.39,55.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a:) Benefits of the proposed lighting normalization. Top row shows representative training frames in a sunny outdoor setting while we conduct inference under two novel lighting settings which have not been used in training. Note that the proposed lighting normalization enables realistic synthesis under new lighting while absence of lighting normalization yields degraded outputs. (b:) Plot of SSIM loss (texture prediction) and vertex loss (geometry prediction) on the evaluation set. Even though both models result in similar lip shapes, the lower SSIM loss of the lightingnormalized model boosts the visual realism and overall lip-sync quality.</figDesc><graphic url="image-10.png" coords="6,72.42,155.91,191.64,237.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Qualitative comparison on subjects from GRID, CREMA-D and TCD-TIMIT against IJCV'19 and CVPR'19 (latter only available on GRID). Our model is capable of seamlessly blending back into the video instead of animating a normalized cropped frame as in IJCV'19 and CVPR'19..</figDesc><graphic url="image-11.png" coords="7,69.91,72.00,455.38,78.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison with Wav2Lip [30]. Our model generates higher resolution outputs (evident by higher CPBD metric [29]) with fewer artifacts compared to Wav2Lip. Examples are provided in accompanying video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison with NVP<ref type="bibr" target="#b32">[33]</ref>. We show that a sequence generated by our method usually has better lip dynamics compared to NVP. The observation is also supported by LSE-D (lower is better) and LSE-C (higher is better) metrics<ref type="bibr" target="#b29">[30]</ref> for our model. Examples are provided in accompanying video. Best viewed zoomed in.</figDesc><graphic url="image-20.png" coords="7,55.82,295.25,224.85,72.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Left: Self-reenactment performance comparison against state-of-the-art benchmarks of CVPR-W'19<ref type="bibr" target="#b35">[36]</ref>, IJCV'19<ref type="bibr" target="#b36">[37]</ref>, CVPR'19<ref type="bibr" target="#b6">[7]</ref>, BMVC'17<ref type="bibr" target="#b7">[8]</ref>, Chen et al. [6] and Wiles et al. [40]. Pre-trained LipNet (for WER) is available only on GRID. Authors of [7] released checkpoint for GRID only. (↑):Higher is better. (↓):Lower is better. Best results are marked in bold. Right: Mean Opinion Scores of user study. The statistical significance of these differences in ratings is confirmed by ANOVA with Tukey post-hoc tests. Please see Appendix-A for details.</figDesc><graphic url="image-21.png" coords="8,50.28,79.94,329.28,63.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Screen shot from our user study.</figDesc><graphic url="image-23.png" coords="11,308.86,72.00,236.24,95.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>•</head><label></label><figDesc>Suwajanakorn et al. trained on 14 hours of weekly President addresses recorded between 2009-2016. In contrast, our framework just requires ∼5 minutes of training video. • Suwajanakorn et al. demonstrate their outputs only under the specific studio lighting setup of the President's office. Their texture generation network is not designed with the goal of handling diverse ambient lighting. In contrast, our network disentangles and normalizes the effects of illumination, thereby enabling inference under diverse lighting conditions. • Typical training data pre-processing time of Suwajanakorn et al. is around 2 weeks on 10 cluster nodes of Intel Xeon E5530. In contrast, our combined preprocessing and training takes only about 3-5 hours on a single system equipped with a NVIDIA P1000 GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Comparing our result against ground-truth. For each subject, top row is the original sequence of frames, while the bottom row is the resynthesized sequence.</figDesc><graphic url="image-24.png" coords="13,318.09,81.96,217.80,222.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Sample applications enabled by our 3D talking face generation pipeline.</figDesc><graphic url="image-25.png" coords="16,50.11,241.57,494.99,118.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>User study analysis. Column 1: percentage of "real" ratings by category. Column 2: percentage of "real" ratings with text-to-speech driven results removed. Column 3: mean opinion score of lip-sync quality. Column 4: mean opinion score of picture quality.</figDesc><table><row><cell>Method</cell><cell>Is Real ? (% Yes)</cell><cell>[no TTS] Is Real ? (% Yes)</cell><cell>Lip-Sync (1-3) )</cell><cell>Visual Quality (1-5)</cell></row><row><cell>Real</cell><cell>97.6</cell><cell>97.6</cell><cell>2.95±0.03</cell><cell>4.55±0.13</cell></row><row><cell>IJCV'19</cell><cell>60.7</cell><cell>60.7</cell><cell>2.45±0.11</cell><cell>2.49±0.17</cell></row><row><cell>Wav2Lip</cell><cell>34.4</cell><cell>37.6</cell><cell>1.72±0.12</cell><cell>3.35±0.20</cell></row><row><cell>NVP</cell><cell>44.4</cell><cell>50.0</cell><cell>1.80±0.13</cell><cell>3.75±0.19</cell></row><row><cell>TBE</cell><cell>50.7</cell><cell>n/a</cell><cell>2.17±0.17</cell><cell>4.07±0.26</cell></row><row><cell>Proposed</cell><cell>71.6</cell><cell>77.25</cell><cell>2.46±0.10</cell><cell>4.10±0.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Parameter sweep for selecting latent code lengths, N</figDesc><table><row><cell></cell><cell></cell><cell cols="3">← AR Code Length: N a →</cell></row><row><cell></cell><cell>0</cell><cell>2</cell><cell>8</cell><cell>16</cell></row><row><cell>SSIM</cell><cell>0.92</cell><cell>0.93</cell><cell>0.901</cell><cell>0.889</cell></row><row><cell>LMD</cell><cell>2.00</cell><cell>1.88</cell><cell>2.91</cell><cell>2.16</cell></row></table><note>a , for encoding previous time step atlas based on LMD (lower is better) and SSIM (higher is better) metrics. Best results are marked in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>←</head><label></label><figDesc>Audio Code Length: N S →</figDesc><table><row><cell cols="2">Metric 8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell></row><row><cell>LMD</cell><cell>1.83</cell><cell>1.87</cell><cell>1.77</cell><cell>1.81</cell><cell>1.82</cell></row><row><cell>SSIM</cell><cell>0.907</cell><cell>0.914</cell><cell>0.917</cell><cell>0.917</cell><cell>0.910</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Parameter sweep for selecting audio latent code length based on LMD and SSIM metrics. Best results are marked in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>While it is true Architecture of the texture decoder. Each fully connected (FC) and convolution layer is followed by a ReLU nonlinearity, while only the last convolution layer is followed by a tanh non-linearity. The length of the input latent vector depends on the mode of the experiment.</figDesc><table><row><cell>Input</cell><cell>Type</cell><cell>Kernel</cell><cell>Stride</cell><cell>Channels</cell><cell>Outputs</cell></row><row><cell cols="2">Input (Latent Vector):</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">= 32D (only audio)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">= 34D (Audio + AutoRegressive)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Latent Vector FC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>16384</cell></row><row><cell cols="2">Reshape: 4×4×1024</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">2× Bilinear Upsample</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8×8×1024</cell><cell>Conv</cell><cell>3×3</cell><cell>1×1</cell><cell>512</cell><cell>8×8×512</cell></row><row><cell cols="2">2× Bilinear Upsample</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>16×16×512</cell><cell>Conv</cell><cell>3×3</cell><cell>1×1</cell><cell>256</cell><cell>16×16×256</cell></row><row><cell cols="2">2× Bilinear Upsample</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>32×32×256</cell><cell>Conv</cell><cell>3×3</cell><cell>1×1</cell><cell>128</cell><cell>32×32×128</cell></row><row><cell cols="2">2× Bilinear Upsample</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>64×64×128</cell><cell>Conv</cell><cell>3×3</cell><cell>1×1</cell><cell>64</cell><cell>64×64×64</cell></row><row><cell cols="2">2× Bilinear Upsample</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">128×128×64 Conv</cell><cell>5×5</cell><cell>1×1</cell><cell>3</cell><cell>128×128×3</cell></row><row><cell>Input</cell><cell cols="4">Type Kernel Stride Channels</cell><cell>Outputs</cell></row><row><cell cols="2">Input (RGB):</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">=128×128×3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>Conv</cell><cell>5×5</cell><cell>2×2</cell><cell>128</cell><cell>64×64×128</cell></row><row><cell cols="2">64×64×128 Conv</cell><cell>5×5</cell><cell>2×2</cell><cell>256</cell><cell>32×32×256</cell></row><row><cell cols="2">32×32×256 Conv</cell><cell>5×5</cell><cell>2×2</cell><cell>512</cell><cell>16×16×512</cell></row><row><cell cols="2">16×16×512 Conv</cell><cell>5×5</cell><cell>2×2</cell><cell>1024</cell><cell>8×8×1024</cell></row><row><cell>8×8×1024</cell><cell>Conv</cell><cell>5×5</cell><cell>2×2</cell><cell>2048</cell><cell>4×4×2048</cell></row><row><cell>4×4×2048</cell><cell>FC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>, 11, 13, 15, 18, 19, 25, 31, 33 TCD-TMIT 8, 9, 15, 18, 25, 28, 33, 41, 55, 56 CREMA-D 15, 20, 21, 30, 33, 52, 62, 81, 82, 89 IDs of subjects used for self-reenactment experiment.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Test Subject ID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">GRID 2, 4LipGAN Proposed Wav2Lip Proposed TBE Proposed NVP Proposed</cell></row><row><cell>0.07</cell><cell>0.14</cell><cell>0.06</cell><cell>0.13</cell><cell>0.12</cell><cell>0.18</cell><cell>0.10</cell><cell>0.18</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/Rudrabha/Wav2Lip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgments</head><p>We would like to thank all the performers who graciously allowed us to use their videos for this work, including D. Sculley, Kate Lane, Ed Moreno, Glenn Davis, Martin Aguinis, Caile Collins, Laurence Moroney, Paige Bailey, Ayush Tewari and Yoshua Bengio. We also thank the performers and creators of external, public datasets, as well as all participants of our user study. We would also like to thank our collaborators at Google and Deep-Mind: Paul McCartney, Brian Colonna, Michael Nechyba, Avneesh Sud, Zachary Gleicher, Miaosen Wang, Yi Yang, Yannis Assael, Brendan Shillingford, and Yu Zhang.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. User Study Analysis</head><p>We conducted a user study to quantitatively compare our lip-sync and perceptual quality against the state-of-theart audio-driven frameworks of Wav2Lip <ref type="bibr" target="#b29">[30]</ref>, NVP <ref type="bibr" target="#b32">[33]</ref>, IJCV'19 <ref type="bibr" target="#b36">[37]</ref> and TBE <ref type="bibr" target="#b13">[14]</ref>. In the study, N=35 raters were each shown a total 29 sample clips consisting of synthetic and real videos. For competing methods, we used their released videos (NVP, TBE) or generated results with their pre-trained models (IJCV <ref type="bibr" target="#b18">'19,</ref><ref type="bibr">Wav2Lip)</ref>. The raters were primarily drawn from a pool of subjects without research expertise, supplemented with a minority (N=14) who were researchers. The subgroup of researchers included some having familiarity with computer vision topics but none were expert on speech-driven animation. The raters were asked three questions: Q1: Is the video real or fake? Q2: Rate the quality of the lip-sync, i.e. how well does the motion of the lips match the audio, on a 3-point (discrete) scale (poor, acceptable, great). Q3: Rate the picture quality, e.g. naturalness, resolution, and consistency of the video, on a discrete 5-point scale from 1-5 (poor-great).</p><p>Figure <ref type="figure">12</ref> shows, for each question, the percentage of raters who selected each rating. We report the Mean Opinion Scores (MOS) of the questions in Table <ref type="table">1</ref>. As is evident, among the competing methods, our method receives the most favorable user ratings.</p><p>We performed a statistical analysis to confirm the significance of these ratings. For Q1 (only) we excluded the textto-speech results from consideration, since it was straightforward to judge the videos as "fake" due to the computergenerated speech. However, questions Q2 and Q3 are still relevant in the text-to-speech case, since it is possible to rate the quality of lip-sync and overall image naturalness even when the voice is clearly synthetic.</p><p>The statistical analysis confirms that the differences in real-fake ratings on Q1 are significant (Kruskal-Wallis test, χ 2 = 158, p&lt;1e-04), and our method outperforms the other methods after adjusting for multiple comparisons (Tukey's Honest Significant Differences (HSD) IJCV p adj.= .003, Wav2Lip p adj.&lt;1e-04, NVP p adj.&lt;1e-04). For Q2 the differences in ratings are significant (Kruskal-Wallis χ 2 = 279, p&lt;1e-04), and our method outperforms most competing methods with statistical significance after adjusting for the multiple tests (Tukey's HSD TBE p adj.= .035, Wav2Lip p adj.&lt;1e-04, NVP p adj.&lt;1e-04), however the difference versus IJCV'19 is not significant. For Q3 (Kruskal-Wallis χ 2 = 248, p&lt;1e-04) our method outperforms most competing methods with statistical significance after adjustment for multiple comparison (HSD IJCV p adj.&lt;1e-04, Wav2lip p adj.&lt;1e-04, NVP p adj.= 0.04 however the comparison with TBE is not significant.</p><p>These significance results for Q2 and Q3 (and in particu-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fernanda Viégas</title>
				<meeting><address><addrLine>Pete Warden, Martin Wattenberg, Martin Wicke</addrLine></address></meeting>
		<imprint>
			<publisher>Yuan Yu, and Xiaoqiang Zheng</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 6</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Yannis M Assael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">Lipnet: Endto-end sentence-level lipreading</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rigid stabilization of facial expressions</title>
		<author>
			<persName><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2003">July 2014. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal UV spaces for facial morphable model construction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICIP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4672" to="4676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video Rewrite: driving visual speech with audio</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2019. 1, 2, 7, 8</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lipreading, ACCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2006-11">November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Capture, learning, and synthesis of 3D speaking styles</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cudeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10101" to="10111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Capture, learning, and synthesis of 3d speaking styles</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cudeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10101" to="10111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">JALI: an animator-centric viseme model for expressive lip synchronization</title>
		<author>
			<persName><forename type="first">Pif</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">127</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text-based editing of talking-head video</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TCD-TIMIT: An audio-visual corpus of continuous speech</title>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Harte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eoin</forename><surname>Gillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical generative modeling for controllable speech synthesis</title>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modality dropout for improved performance-driven talking faces</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelaziz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Barry-John</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Apostoloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kajareker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="378" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005">July 2017. 1, 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time facial surface geometry from monocular video on mobile GPUs</title>
		<author>
			<persName><forename type="first">Yury</forename><surname>Kartynnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artsiom</forename><surname>Ablavatski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Grishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Computer Vision for AR/VR</title>
				<meeting><address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating an item pool for translational social cognition research: methodology and initial validation</title>
		<author>
			<persName><forename type="first">Samantha</forename><forename type="middle">L</forename><surname>Michael K Keutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><forename type="middle">C</forename><surname>Savitt</surname></persName>
		</author>
		<author>
			<persName><surname>Gur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Video Portraits</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards automatic face-to-face translation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Prajwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudrabha</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerin</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face stabilization by mode pursuit for avatar construction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Mathieu Lamarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><surname>Danvoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Vision Computing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Practice and theory of blendshape facial models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Anjyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taehyun</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sylvain Lefebvre and Michela Spagnuolo</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Eurographics -State of the Art Reports</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning a model of facial shape and expression from 4D scans</title>
		<author>
			<persName><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">194</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Animating face using disentangled audio representations</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A noreference perceptual image sharpness metric based on a cumulative probability of blur detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><forename type="middle">J</forename><surname>Narvekar</surname></persName>
		</author>
		<author>
			<persName><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Quality of Multimedia Experience</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="87" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName><forename type="first">Rudrabha</forename><surname>Kr Prajwal</surname></persName>
		</author>
		<author>
			<persName><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><surname>Vinay P Namboodiri</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007">2020. 1, 2, 7</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Everybody&apos;s talkin&apos;: Let me talk as you want</title>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<idno>arXiv:, 2020. 3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Synthesizing Obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural voice puppetry: Audio-driven facial reenactment</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2020. 1, 2, 3, 6, 7</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="376" to="380" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end speech-driven realistic facial animation with temporal gans</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Realistic speech-driven facial animation with GANs. IJCV</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2019. 2, 7, 8, 11</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Text driven 3D photo-realistic talking head</title>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<author>
			<persName><surname>Short</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Short-time_Fourier_transform" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Sophia Koepke</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="670" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient and robust specular highlight removal</title>
		<author>
			<persName><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1304" to="1311" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visemenet: Audio-driven animator-centric speech animation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">161</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
