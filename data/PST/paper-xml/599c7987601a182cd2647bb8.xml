<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convergence Analysis of Two-layer Neural Networks with ReLU Activation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-01">November 1, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
							<email>yuanzhil@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yuan</surname></persName>
							<email>yangyuan@cs.cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convergence Analysis of Two-layer Neural Networks with ReLU Activation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-01">November 1, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">A10106387E6500724B27B7E31F6E1AC5</idno>
					<idno type="arXiv">arXiv:1705.09886v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing.</p><p>In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called "identity mapping". We prove that, if input follows from Gaussian distribution, with standard O(1/ √ d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the "identity mapping" makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks.</p><p>Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in "two phases": In phase I, the gradient points to the wrong direction, however, a potential function g gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, etc. <ref type="bibr" target="#b16">[17]</ref>. Despite its success, the theoretical understanding on how it works remains poor. It is well known that neural networks have great expressive power <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31]</ref>. That is, for every function there exists a set of weights on the neural network such that it approximates the function everywhere. However, it is unclear how to obtain the desired weights. In practice, the most commonly used method is stochastic gradient descent based methods (e.g., SGD, Momentum <ref type="bibr" target="#b39">[40]</ref>, Adagrad <ref type="bibr" target="#b9">[10]</ref>, Adam <ref type="bibr" target="#b24">[25]</ref>), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.</p><p>In this paper, we give the first convergence analysis of SGD for two-layer feedforward network with ReLU activations. For this basic network, it is known that even in the simplified setting where the weights are initialized symmetrically and the ground truth forms orthonormal basis, gradient descent might get stuck at saddle points <ref type="bibr" target="#b40">[41]</ref>.</p><p>Inspired by the structure of residual network (ResNet) <ref type="bibr" target="#b20">[21]</ref>, we add an extra identity mapping for the hidden layer (see Figure <ref type="figure">1</ref>). Surprisingly, we show that simply by adding this mapping, with the standard initialization scheme and small step size, SGD always converges to the ground truth. In other words, the optimization becomes significantly easier, after adding the identity mapping. See Figure <ref type="figure" target="#fig_9">2</ref>, based on our analysis, the region near the identity matrix I contains only one global minimum without any saddle points or local minima, thus is easy for SGD to optimize. The role of the identity mapping here, is to move the initial point to this easier region (better initialization). Other than being feedforward and shallow, our network is different from ResNet in the sense that our identity mapping skips one layer instead of two. However, as we will show in Section 5.1, the skip-one-layer identity mapping already brings significant improvement to vanilla networks.</p><p>Formally, we consider the following function.</p><formula xml:id="formula_0">f (x, W) = ReLU((I + W) x) 1<label>(1)</label></formula><p>where ReLU(v) = max(v, 0) is the ReLU activation function. x ∈ R d is the input vector sampled from a Gaussian distribution, and W ∈ R d×d is the weight matrix, where d is the number of input units. Notice that I adds e i to column i of W, which makes f asymmetric in the sense that by switching any two columns in W, we get different functions.</p><p>Following the standard setting <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>, we assume that there exists a two-layer teacher network with weight W * . We train the student network using 2 loss:</p><formula xml:id="formula_1">L(W) = E x [(f (x, W) -f (x, W * )) 2 ]<label>(2)</label></formula><p>We will define a potential function g, and show that if g is small, the gradient points to partially correct direction and we get closer to W * after every SGD step. However, g could be large and thus gradient might point to the reverse direction. Fortunately, we also show that if g is large, by doing SGD, it will keep decreasing until it is small enough while maintaining the weight W in a nice region. We call the process of decreasing g as Phase I, and the process of approaching W * as Phase II. See Figure <ref type="figure">3</ref> and simulations in Section 5.3.</p><p>Our two phases framework is fundamentally different from any type of local convergence, as in Phase I, the gradient is pointing to the wrong direction to W * , so the path from W to W * is non-convex, and SGD takes a long detour to arrive W * . This framework could be potentially useful for analyzing other non-convex problems.</p><p>To support our theory, we have done a few other experiments and got interesting observations. For example, as predicted by our theorem, we found that for multilayer feedforward network with identity mappings, zero initialization performs as good as random initialization. At the first glance, it contradicts the common belief "random initialization is necessary to break symmetry", but actually the identity mapping itself serves as the asymmetric component. See Section 5. <ref type="bibr" target="#b3">4</ref>.</p><p>Another common belief is that neural network has lots of local minima and saddle points <ref type="bibr" target="#b8">[9]</ref>, so even if there exists a global minimum, we may not be able to arrive there. As a result, even when the teacher network is shallow, the student network usually needs to be deeper, otherwise it will underfit. However, both our theorem and our experiment show that if the shallow teacher network is in a pretty large region near identity (Figure <ref type="figure" target="#fig_9">2</ref>), SGD always converges to the global minimum by initializing the weights I + W in this region, with equally shallow student network. By contrast, wrong initialization gets stuck at local minimum and underfit. See Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Expressivity. Even two-layer network has great expressive power. For example, two-layer network with sigmoid activations could approximate any continuous function <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref>. ReLU is the state-of-the-art activation function <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13]</ref>, and has great expressive power as well <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Learning. Most previous results on learning neural network are negative <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>, or positive but with algorithms other than SGD <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, or with strong assumptions on the model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. <ref type="bibr" target="#b34">[35]</ref> proved that with high probability, there exists a continuous decreasing path from random initial point to the global minimum, but SGD may not follow this path. Recently, Zhong et al. showed that with initialization point found using tensor decomposition, gradient descent could find the ground truth for one hidden layer network <ref type="bibr" target="#b43">[44]</ref>.</p><p>Linear network and independent activation. Some previous works simplified the model by ignoring the activation functions and considering deep linear networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref> or deep linear residual networks <ref type="bibr" target="#b18">[19]</ref>, which can only learn linear functions. Some previous results are based on independent activation assumption that the activations of ReLU and the input are independent <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Saddle points. It is observed that saddle point is not a big problem for neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. In general, if the objective is strict-saddle <ref type="bibr" target="#b10">[11]</ref>, SGD could escape all saddle points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Denote x as the input vector in R d . For now, we first consider x sampled from normal distribution N (0, I).</p><formula xml:id="formula_2">Denote W * = (w * 1 , • • • , w * n ) ∈ R d×d</formula><p>as the weights for the teacher network, W = (w 1 , • • • , w n ) ∈ R d×d as the weights for the student network, where w * i , w i ∈ R d are column vectors. f (x, W * ), f (x, W) are defined in (1), representing the teacher and student network.</p><p>We want to know whether a randomly initialized W will converge to W * , if we run SGD with l 2 loss defined in <ref type="bibr" target="#b1">(2)</ref>. Alternatively, we can write the loss L(W) as</p><formula xml:id="formula_3">E x [(Σ i ReLU( e i + w i , x ) -Σ i ReLU( e i + w * i , x )) 2 ]</formula><p>Taking derivative with respect to w j , we get</p><formula xml:id="formula_4">∇L(W) j = 2E x i ReLU( e i + w i , x ) - i ReLU( e i + w * i , x ) x1 ej +wj ,x ≥0</formula><p>where 1 e is the indicator function that equals 1 if the event e is true, and 0 otherwise. Here ∇L(W) ∈ R d×d , and ∇L(W) j is its j-th column. Denote θ i,j as the angle between e i + w i and e j + w j , θ i * ,j as the angle between e i + w * i and e j + w j . Denote v = v v 2 . Denote I + W * and I + W * as the column-normalized version of I + W * and I + W such that every column has unit norm. Since the input is from a normal distribution, one can compute the expectation inside the gradient as follows.</p><p>Lemma 2.1 (Eqn (13) from <ref type="bibr" target="#b40">[41]</ref>).</p><formula xml:id="formula_5">If x ∼ N (0, I), then -∇L(W) j = d i=1 π 2 (w * i -w i ) + π 2 -θ i * ,j (e i + w * i ) -π 2 -θ i,j (e i + w i ) + e i + w * i 2 sin θ i * ,j -e i + w i 2 sin θ i,j e j + w j</formula><p>Remark. Although the gradient of ReLU is not well defined at the point of zero, if we assume input x is from the Gaussian distribution, the loss function becomes smooth, and the gradient is well defined everywhere.</p><p>Denote u ∈ R d as the all one vector. Denote Diag(W) as the diagonal matrix of matrix W, Diag(v) as a diagonal matrix whose main diagonal equals to the vector v. Denote Off-Diag(W) W -Diag(W). Denote [d] as the set {1, • • • , d}. Throughout the paper, we abuse the notation of inner product between matrices W, W * , ∇L(W), such that ∇L(W), W means the summation of the entrywise products. W 2 is the spectral norm of W, and W F is the Frobenius norm of W. We define the potential function g and variables g j , A j , A below, which will be useful in the proof.  The function is one point strongly convex as every point's negative gradient points to the center, but not convex as any line between the center and the red region is below surface.</p><formula xml:id="formula_6">)e i + w * i - (e i + w i )e i + w i ) = (I + W * )I + W * -(I + W)I + W .</formula><p>In this paper, we consider the standard SGD with mini batch method for training the neural network. Assume W 0 is the initial point, and in step t &gt; 0, we have the following updating rule:</p><formula xml:id="formula_7">W t+1 = W t -η t G t</formula><p>where the stochastic gradient</p><formula xml:id="formula_8">G t = ∇L(W t ) + E t with E[E t ] = 0 and E t F ≤ ε. Let G 2 6dγ + ε, G F 6d 1.5 γ + ε,</formula><p>where γ is the upper bound of W * 2 and W 0 2 (defined later). As we will see in Lemma C.2, they are the upper bound of G t 2 and G t F respectively.</p><p>It's clear that L is not convex, In order to get convergence guarantees, we need a weaker condition called one point convexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.4 (One point strongly convexity</head><formula xml:id="formula_9">). A function f (x) is called δ-one point strongly convex in domain D with respect to point x * , if ∀x ∈ D, -∇f (x), x * -x &gt; δ x * -x 2 2</formula><p>. By definition, if a function f is strongly convex, it is also one point strongly convex in the entire space with respect to the global minimum. However, the reverse is not necessarily true, e.g., see Figure <ref type="figure">4</ref>. If a function is one point strongly convex, then in every step a positive fraction of the negative gradient is pointing to the optimal point. As long as the step size is small enough, we will finally arrive the optimal point, possibly by a winding path. See Figure <ref type="figure">3</ref> for illustration, where starting from W 6 (Phase II), we get closer to W * in every step. Formally, we have the following lemma.</p><p>Lemma 2.5. For function f (W), consider the SGD update</p><formula xml:id="formula_10">W t+1 = W t -ηG t , where E[G t ] = ∇f (W t ), E[ G t 2 F ] ≤ G 2 .</formula><p>Suppose for all t, W t is always inside the δ-one point strongly convex region with diameter D, i.e., W t -W * F ≤ D. Then for any α &gt; 0 and any T such that</p><formula xml:id="formula_11">T α log T ≥ D 2 δ 2 (1+α)G 2 , if η = (1+α) log T δT , we have E W T -W * 2 F ≤ (1+α) log T G 2 δ 2 T</formula><p>.</p><p>The proof can be found in Appendix J. Lemma 2.5 uses fixed step size, so it easily fits the standard practical scheme that shrinks η by a factor of 10 after every a few epochs. For example, we may apply Lemma 2.5 every time η gets changed. Notice that our lemma does not imply that W T will converge to W * . Instead, it only says W T will be sufficiently close to W * with small step size η.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Theorem Theorem 3.1 (Main Theorem). There exists constants</head><formula xml:id="formula_12">γ &gt; γ 0 &gt; 0 such that If x ∼ N (0, I), W 0 2 , W * 2 ≤ γ 0 , d ≥ 100, ε ≤ γ 2 ,</formula><p>then SGD for L(W) will find the ground truth W * by two phases. In Phase I, by setting</p><formula xml:id="formula_13">η ≤ γ 2 G 2 2</formula><p>, the potential function will keep decreasing until it is smaller than 197γ 2 , which takes at most 1 16η steps. In Phase II, for any α &gt; 0 and any T such that</p><formula xml:id="formula_14">T α log T ≥ 36d 100 4 (1+α)G 2 F , if we set η = (1+α) log T δT , we have E W T -W * 2 F ≤ 100 2 (1+α) log T G 2 F 9T .</formula><p>Remarks. Randomly initializing the weights with O(1/ √ d) is standard in deep learning, see <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>. It is also well known that if the entries are initialized with O(1/ √ d), the spectral norm of the random matrix is O(1) <ref type="bibr" target="#b32">[33]</ref>. So our result matches with the common practice. Moreover, as we will show in Section 5.5, networks with small average spectral norm already have good performance. Thus, our assumption W * 2 = O(1) is reasonable. Notice that here we assume the spectral norm of W * to be constant, which means the Frobenius norm W * F could be as big as O( √ d).</p><p>The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, and also considered plausible in <ref type="bibr" target="#b5">[6]</ref>). We could easily generalize the analysis to rotation invariant distributions, and potentially more general distributions (see <ref type="bibr">Section 6)</ref>. Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>, or directly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> or indirectly <ref type="bibr" target="#b40">[41]</ref> 1 assume that the activations are independent. By contrast, in our model the ReLU activations are highly correlated 2 as W 2 , W * 2 = Ω(1). As pointed out by <ref type="bibr" target="#b5">[6]</ref>, eliminating the unrealistic assumptions on activation independence is the central problem of analyzing the loss surface of neural network, which was not fully addressed by the previous analyses.</p><p>To prove the main theorem, we split the process and present the following two theorems, which will be proved in Appendix C and D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.2 (Phase I). There exists a constant</head><formula xml:id="formula_15">γ &gt; γ 0 &gt; 0 such that If W 0 2 , W * 2 ≤ γ 0 , d ≥ 100, η ≤ γ 2 G 2 2 , ε ≤ γ 2 ,</formula><p>then g t will keep decreasing by a factor of 1 -0.5ηd for every step, until g t1 ≤ 197γ 2 for step t 1 ≤ 1 16η . After that, Phase II starts. That is, for every T &gt; t 1 , we have W T 2 ≤ 1 100 and g T ≤ 0.1. Theorem 3.3 (Phase II). There exists a constant γ such that if W 2 , W * 2 ≤ γ, and g ≤ 0.1, then</p><formula xml:id="formula_16">-∇L(W), W * -W = d j=1 -∇L(W) j , w * j -w j &gt; 0.03 W * -W 2 F .</formula><p>With these two theorems, we get the main theorem immediately.</p><p>Proof for Theorem 3.1. By Theorem 3.2, we know the statement for Phase I is true, and we will enter phase II in 4 Overview of the Proofs General Picture. In many convergence analyses for non-convex functions, one would like to show that L is one point strongly convex, and directly apply Lemma 2.5 to get the convergence result. However, this is not true for 2-layer neural network, as the gradient may point to the wrong direction, see Section 5.3.</p><p>So when is our L one point convex? Consider the following thought experiment: First, suppose W 2 , W * 2 → 0, we know w i 2 , w * i 2 also go to 0. Thus, e i + w i and e i + w * i are close to e i . As a result, θ i,j , θ i * ,j ≈ π 2 , and θ i * ,i ≈ 0. Based on Lemma 2.1, this gives us a naïve approximation of the negative gradient, i.e.,</p><formula xml:id="formula_17">-∇L(W) j ≈ π 2 (w * j -w j ) + π 2 d i=1 (w * i -w i ) + e j + w j i =j ( e i + w * i 2 -e i + w i 2 )</formula><p>. While the first two terms π 2 (w * jw j ) and π 2 d i=1 (w * iw i ) have positive inner product with W * -W, the last term g j = e j + w j i =j ( e i + w * i 2e i + w i 2 ) can point to arbitrary direction. If the last term is small, it can be covered by the first two terms, and L becomes one point strongly convex. So we define a potential function closely related to the last term: g = d i=1 ( e i + w * i 2e i + w i 2 ). We show that if g is small enough, L is also one point strongly convex (Theorem 3.3).</p><p>However, from random initialization, g can be as large as of Ω( √ d), which is too big to be covered. Fortunately, we show that if g is big, it will gradually decrease simply by doing SGD on L. More specifically, we introduce a two phases convergence analysis framework:</p><p>1. In Phase I, the potential function g is decreasing to a small value. 2. In Phase II, g remains small, so L is one point convex and thus W starts to converge to W * .</p><p>Constant Part First Order Higher Order We believe that this framework could be helpful for other non-convex problems. Technical difficulty: Phase I. Our key technical challenge is to show that in Phase I, the potential function actually decreases to O(1) after polynomial number of iterations. However, we cannot show this by merely looking at g itself. Instead, we introduce an auxiliary variable s = (W * -W)u, where u is the all one vector. By doing a careful calculation, we get their joint update rules (Lemma C.3 and Lemma C.4):</p><formula xml:id="formula_18">, W * -W + + ≥ [ π 2 -O(g)] W * -W 2 F Lemma D.2 + Lemma D.3 -1.3 W * -W 2 F Lemma D.1 -0.085 W * -W 2 F Lemma B.2</formula><formula xml:id="formula_19">s t+1 ≈ s t -πηd 2 s t + ηO( √ dg t + √ dγ) g t+1 ≈ g t -ηdg t + ηO(γ √ d s t 2 + dγ 2 )</formula><p>Solving this dynamics, we can show that g t will approach to (and stay around) O(γ), thus we enter Phase II. Technical difficulty: Phase II. Although the overall approximation in the thought experiment looks simple, the argument is based on an over simplified assumption that θ i * ,j , θ i,j ≈ π 2 for i = j. However, when W * has constant spectral norm, even when W is very close to W * , θ i,j * could be constantly far away from π 2 , which prevents us from applying this approximation directly. To get a formal proof, we use the standard Taylor expansion and control the higher order terms. Specifically, we write θ i * ,j as θ i * ,j = arccos e i + w * i , e j + w j and expand arccos at point 0, thus,</p><formula xml:id="formula_20">θ i * ,j = π 2 -e i + w * i , e j + w j + O( e i + w * i , e j + w j 3 )</formula><p>However, even when W ≈ W * , the higher order term O( e i + w * i , e j + w j 3 ) still can be as large as a constant, which is too big for us. Our trick here is to consider the "joint Taylor expansion": θ i * ,jθ i,j = e i + w ie i + w * i , e j + w j + O(| e i + w * i , e j + w j 3e i + w i , e j + w j 3 |)</p><p>As W approaches W * , | e i + w * i , e j + w j 3e i + w i , e j + w j 3 | also tends to zero, therefore our approximation has bounded error.</p><p>In the thought experiment, we already know that the constant part in the Taylor expansion of ∇L(W) is π 2 -O(g)-one point convex. We show that after taking inner product with W * -W, the first order terms are lower bounded by (roughly) -1.3 W * -W<ref type="foot" target="#foot_1">2</ref> F and the higher order terms are lower bounded by -0.085 W * -W 2 F . Adding them together, we can see that L(W) is one point convex as long as g is small. See Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>Geometric Lemma. In order to get through the whole analysis, we need tight bounds on a few common terms that appear everywhere. Instead of using naïve algebraic techniques, we come up with a nice geometric proof to get nearly optimal bounds. Due to space limit, we defer it to Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present several simulation results to support our theory. Our code can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Importance of identity mapping</head><p>In this experiment, we compare the standard ResNet <ref type="bibr" target="#b20">[21]</ref> and single skip model where identity mapping skips only one layer. See Figure <ref type="figure" target="#fig_4">6</ref> for the single skip model. We also ran the vanilla network, where the identity mappings are completely removed.   In this experiment, we choose Cifar-10 as the dataset, and all the networks have 56-layers. Other than the identity mappings, all other settings are identical and default. We run the experiments for 5 times and report the average test error. As we can see in Table <ref type="table" target="#tab_1">1</ref>, compared with vanilla network, by simply using a single skip identity mapping, one can already improve the test error by 3.03%, and is 2.04% close to the ResNet. So single skip identity mapping brings significant improvement on test accuracy.</p><formula xml:id="formula_21">W (ResLink) W-W* (ResLink) W (Vanilla) W-W* (Vanilla) (b) W * -W F , W F</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Global minimum convergence</head><p>In this experiment, we verify our main theorem that for two-layer teacher network and student network with identity mappings, as long as W 0 2 , W * 2 is small, SGD always converges to the global minimum W * , thus gives almost 0 training error and test error. We consider three student networks. The first one (ResLink) is defined using (2), the second one (Vanilla) is the same model without the identity mapping. The last one (3-Block) is a three block network with each block containing a linear layer (500 hidden nodes), a batch normalization and a ReLU layer. The teacher network always shares the same structure as the student network.</p><p>The input dimension is 100. We generated a fixed W * for all the trials with W * 2 ≈ 0.6, W * F ≈ 5.7. We generated a training set of size 100, 000, and test set of size 10, 000, sampled from a Gaussian distribution. We use batch size 200, step size 0.001. We run ResLink for 5 times with random initialization ( W 2 ≈ 0.6 and W F ≈ 5), and plot the curves by taking the average.</p><p>Figure <ref type="figure" target="#fig_5">7</ref>(a) shows test error and training error of the three networks. Comparing Vanilla with 3-Block, we find that 3-Block is more expressive, so its training error is smaller compared with vanilla network; but it suffers from overfitting and has bigger test error. This is the standard overfitting vs underfitting tradeoff. Surprisingly, with only one hidden layer, ResLink has both zero test error and training error. If we look at Figure <ref type="figure" target="#fig_5">7</ref>(b), we know the distance between W and W * converges to 0, meaning ResLink indeed finds the global optimal in all 5 trials. By contrast, for vanilla network, which is essentially the same network with different initialization, W -W * 2 does not converge to zero <ref type="foot" target="#foot_2">3</ref> . This is exactly what our theory predicted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Verify the dynamics</head><p>In this experiment, we verify our claims on the dynamics. Based on the analysis, we construct a 1500 × 1500 matrix W s.t. W 2 ≈ 0.15, W F ≈ 5 , and set W * = 0. By plugging them into (2), one can see that even in this simple case that W * = 0, initially the gradient is pointing to the wrong direction, i.e., not one point convex. We then run SGD on W by using samples x from Gaussian distribution, with batch size 300, step size 0.0001.</p><p>Figure <ref type="figure" target="#fig_6">8</ref>(a) shows the first 100 iterations. We can see that initially the inner product defined in Definition 2.4 is negative, then after about 15 iterations, it turns positive, which means W is in the one point strongly convex region. At the same time, the potential g keeps decreasing to a small value, while the distance to optimal (which also equals to W F in this experiment) is not affected. They precisely match with our description of Phase I in Theorem 3.2.</p><p>After that, we enter Phase II and slowly approach to W * , see Figure <ref type="figure" target="#fig_6">8(b</ref>). Notice that the potential g is always very small, the inner product is always positive, and the distance to optimal is slowly decreasing. Again, they precisely match with our Theorem 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Zero initialization works</head><p>In this experiment, we used a simple 5-block neural network on MNIST, where every block contains a 784 * 784 feedforward layer, an identity mapping, and a ReLU layer. Cross entropy criterion is used. We compare zero initialization with standard O(1/ √ d) random initialization. We found that for zero initialization, we can get 1.28% test error, while for random initialization, we can get 1.27% test error. Both results were obtained by taking average among 5 runs and use step size 0.1, batch size 256. If the identity mapping is removed, zero initialization no longer works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Spectral norm of W *</head><p>We also applied the exact model f defined in (1) to distinguish two classes in MNIST. For any input image x, We say it's in class A if f (x, W) &lt; T A,B , and in class B otherwise. Here T A,B is the optimal threshold for the function f (x, 0) to distinguish A and B. If W = 0, we get 7% training error for distinguish class 0 and class 1. However, it can be improved to 1% with W 2 = 0.6. We tried this experiment for all possible 45 pairs of classes in MNIST, and improve the average training error from 34% (using W = 0) to 14% (using W 2 = 0.6). Therefore our model with W 2 = Ω(1) has reasonable expressive power, and is substantially different from just using the identity mapping alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions</head><p>The assumption that the input is Gaussian can be relaxed in several ways. For example, when the distribution is N (0, Σ) where Σ -I 2 is bounded by a small constant, the same result holds with slightly worse constants. Moreover, since the analysis relies Lemma 2.1, which is proved by converting the original input space into polar space, it is easy to generalize the calculation to rotation invariant distributions. Finally, for more general distributions, as long as we could explicitly compute the expectation, which is in the form of O(W * -W) plus certain potential function, our analysis framework may also be applied.</p><p>There are many exciting open problems. For example, Our paper is the first one that gives solid SGD analysis for neural network with nonlinear activations, without unrealistic assumptions like independent activation assumption. It would be great if one could further extend it to multiple layers, which would be a major breakthrough of understanding optimization for deep learning. Moreover, our two phase framework could be applied to other non-convex problems as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Flowchart of the proofs</head><p>Although the proofs of our theorems are intricate, many lemmas have clear intuition behind the statement. Therefore, we add "*" to these lemmas, so that time constrained readers could feel confident to skip the proofs. We also plot a flowchart of the proofs in Figure <ref type="figure" target="#fig_7">9</ref> to help the readers spend time wisely.</p><p>Since the proofs are long and complicated, we choose to present them in a top-down way. That is, we present the main theorems (Theorem 3.1, Theorem 3.2, and Theorem 3.3) in the main paper, and then present the necessary lemmas in order to prove those main theorems in Section B, Section C and Section D. Finally, we present the proofs for those lemma in Section G, Section H and Section I, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Compute Approximation Matrix</head><p>The exact form of -∇L(W) j in Lemma 2.1 contains variables like θ i * ,j , θ i,j , sin θ i * ,j , sin θ i,j , which are hard to deal with. In this section, we compute the approximation of these terms using Taylor series, and show that the approximation loss is minor. While the proofs are technically involved, the claims themselves are not surprising. Hence, we encourage the readers to skip the proofs (Appendix G) for the first reading.</p><p>Define the j-th column of the approximation matrix P as follows. See Definition 2.2 and Definition 2.3 for g j , A j . P j P 1,j + P 2,j + P 3,j , where</p><formula xml:id="formula_22">P 1,j d i=1 π 2 (w * i -w i ),</formula><p>P 2,j g j e j + w j + I -1 2 e j + w j • e j + w j A j e j + w j ,</p><formula xml:id="formula_23">P 3,j π 2 -θ j * ,j (e j + w * j ) - π<label>2</label></formula><p>(e j + w j ) + e j + w * j sin θ j * ,j e j + w j .</p><p>Treat P 1,j , P 2,j , P 3,j as j-th column of matrix P 1 , P 2 , P 3 respectively, we have P = P 1 + P 2 + P 3 . Although P depends on W, we abuse the notation and simply write P.</p><p>Claim B.1. P j approximates -∇L(W) j by setting ( π 2 -θ i,j ) ≈ e i + w i , e j + w j , ( π 2 -θ i * ,j ) ≈ e i + w * i , e j + w j , sin θ i,j ≈ 1 -1 2 e i + w i , e j + w j 2 and sin θ i * ,j ≈ 1 -1 2 e i + w * i , e j + w j 2 .</p><p>Below we show that the approximation loss is negligible in terms of one point convexity and spectral norm.</p><formula xml:id="formula_24">Lemma* B.2. If W 2 , W * 2 ≤ γ ≤ 1 100 , | P + ∇L(W), W * -W | &lt; 0.085 W * -W 2 F . Lemma* B.3. If W 2 , W * 2 ≤ γ ≤ 1 100 , P + ∇L(W) 2 ≤ 3.5γ 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Phase I: The Decreasing Potential Function</head><p>As we saw in Theorem 3.3, if W 2 , W * 2 is bounded by a constant γ = 1 100 , and the potential function g ≤ 0.1, L(W) is 0.03-one point convex, which will give us convergence guarantee according to Lemma 2.5. However, g could be larger than 0.1 initially, and as we run SGD, W 2 might be larger than 1 100 as well. In this section, we address both problems by analyzing the dynamics of SGD, thus prove Theorem 3.2. The proofs can be found in Appendix H. Before proceeding to the interesting stuff, we need a simpler form of ∇L(W) to work with, see below.</p><formula xml:id="formula_25">Lemma C.1. If W 2 , W * 2 ≤ γ ≤ 1 100 , the negative gradient of L(W) is approximately Q(W) π 2 (W * -W) I + uu + (W * -W) -2Diag(W * -W) + gI + W</formula><p>where u is the all 1 vector. The approximation error is</p><formula xml:id="formula_26">Q(W) -[-∇L(W)] 2 ≤ 61γ 2 .</formula><p>We immediately get the bound of the gradient norm.</p><formula xml:id="formula_27">Lemma* C.2. If W 2 , W * 2 ≤ γ ≤ 1 100 , ∇L(W) 2 ≤</formula><p>6dγ. Now we are ready to analyze the dynamics. We use subscript t under each variable to denote its value at the step t. For simplicity, let Q t Q(W t ). Define s t (W * -W t )u. We first compute the updating rule for g t .</p><formula xml:id="formula_28">Lemma C.3. If W t 2 , W * 2 ≤ γ ≤ 1 100 , d ≥ 100, η ≤ γ 2 G 2 2 , then |g t+1 | ≤ (1 -0.95ηd)|g t | + 86ηdγ 2 + 1.03η √ dε + 4.8η s t 2 γ √ d.</formula><p>The bound contains s t 2 which could be large, so we also need to compute its updating rule:</p><formula xml:id="formula_29">Lemma C.4. If W t 2 , W * 2 ≤ γ ≤ 1 100 , then s t+1 2 ≤ 1 -η (d+1)π 2 s t 2 +η(6.61γ +1.03|g t |+ε) √ d.</formula><p>Combining the two lemmas, we are ready to show that g t will shrink, conditioned on that W t 2 is bounded by γ.</p><formula xml:id="formula_30">Lemma C.5. If for every step t &gt; 0, W t 2 , W * 2 ≤ γ ≤ 1 100 ,d ≥ 100, η ≤ γ 2 G 2 2 , ε ≤ γ 2 , then |g t | will keep</formula><p>decreasing by a factor of 1 -0.5ηd for every step, until |g t1 | ≤ 197γ 2 for t 1 ≤ 1 16η . Fortunately, we also know that W t 2 is always bounded by γ during the process described in Lemma C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma C.6. There exists a constant</head><formula xml:id="formula_31">γ &gt; γ 0 &gt; 0 such that if W 0 2 , W * 2 ≤ γ 0 , d ≥ 100, η ≤ γ 2 G 2 2 , ε ≤ γ 2 ,</formula><p>then in the process of Phase I (Lemma C.5), we always have W T 2 ≤ γ ≤ 1 100 for any T &gt; 0. Now, we are at the state where |g t | is small, and W T 2 ≤ γ, which means we are in Phase II. The next lemma ensures that we will stay in Phase II forever.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma C.7. There exists a constant γ</head><formula xml:id="formula_32">0 &gt; γ &gt; 0 such that if W 0 2 , W * 2 ≤ γ 0 , d ≥ 100, η ≤ γ 2 G 2 2 , ε ≤ γ 2 ,</formula><p>then after |g t1 | ≤ 197γ 2 , Phase I ends and Phase II starts. That is, for every</p><formula xml:id="formula_33">T &gt; t 1 , W T 2 ≤ γ and |g T | ≤ 0.1.</formula><p>Proof for Theorem 3.2. We immediately get Theorem 3.2 by combining the above three lemmas. They show that g t will decrease to a small value in Phase I (Lemma C.5), W t 2 will keep small during this process (Lemma C.6), and they all keep small afterwards (Lemma C.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Phase II: One Point Convexity</head><p>In this section, we prove Theorem 3.3. See detailed proofs in Appendix I. Using Lemma B.2, it suffices to bound</p><formula xml:id="formula_34">P, W * -W = d j=1 P 1,j + P 2,j + P 3,j , w * j -w j</formula><p>Here the first term is easy to calculate.</p><formula xml:id="formula_35">d j=1 P 1,j , w * j -w j = π 2 d i=1 (w * i -w i ) 2 2 ≥ 0<label>(3)</label></formula><p>For notational simplicity, denote</p><p>x j e j + w j • e j + w j (w * jw j ),</p><formula xml:id="formula_36">X (x 1 , • • • , x d )<label>(4)</label></formula><formula xml:id="formula_37">z j I - 1 2 e j + w j • e j + w j (w * j -w j )<label>(5)</label></formula><p>By Definition of P 2,j and (5), we have</p><formula xml:id="formula_38">d j=1 P 2,j , w * j -w j = d j=1 g j e j + w j , w * j -w j + d j=1 z j A j e j + w j<label>(6)</label></formula><p>We bound the above two terms separately below.</p><formula xml:id="formula_39">Lemma D.1. If W 2 , W * 2 ≤ γ ≤ 1 100 , then d j=1 z j A j e j + w j ≥ -(1.3 + 8γ) W * -W 2 F + W * -W F X F . Lemma D.2. If W 2 , W * 2 ≤ γ ≤ 1 100 , then d j=1 g j e j + w j , w * j -w j ≥ -W * -W F X F - (1 + γ)g W * -W 2 F 2(1 -2γ)</formula><p>It remains to bound d j=1 P 3,j , w * jw j . We have the following lemma. <ref type="formula" target="#formula_38">6</ref>), Lemma D.1, Lemma D.2 and Lemma D.3, we know</p><formula xml:id="formula_40">Lemma D.3. If W 2 , W * 2 ≤ γ ≤ 1 100 , d j=1 P 3,j , w * j -w j ≥ π 2 -0.021 W * -W 2 F . Proof of Theorem 3.3. By (3), (</formula><formula xml:id="formula_41">P, W * -W ≥ π 2 -1.321 -8γ - (1 + γ)g 2(1 -2γ) W * -W 2 F &gt; 0.169 - (1 + γ)g 2(1 -2γ) W * -W 2 F Using Lemma B.2, we get -∇L(W), W * -W &gt; 0.084 - (1 + γ)g 2(1 -2γ) W * -W 2 F &gt; 0.03 W * -W 2 F</formula><p>The last inequality holds when g ≤ 0.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E A Geometric Lemma</head><p>In our proof, we need very tight bounds for a few terms. In order to get such bounds, we present a nice and intuitive geometric lemma as follows.</p><formula xml:id="formula_42">Lemma E.1. If W 2 , W * 2 ≤ γ, then ∀i ∈ [d],</formula><p>1.</p><formula xml:id="formula_43">e i + w * i -e i + w i 2 ≤ (I-ei+wi•ei+wi )(w * i -wi) 2 √ 1-2γ ≤ w * i -wi 2 √ 1-2γ 2.</formula><p>-</p><formula xml:id="formula_44">w * i -wi 2 2 2(1-2γ) ≤ e i + w * i -e i + w i , e i + w i ≤ 0 3. if γ ≤ 1 100 ,0 ≤ θ i,i * ≤ 1.001 w * i -w i 2 .</formula><p>Proof. See Figure <ref type="figure" target="#fig_8">10</ref>. Denote e i +w * i as --→ OC, e i +w i as</p><formula xml:id="formula_45">--→ OD, e i + w * i as -→ OA, e i + w i as --→ OB. Thus, w * i -w i 2 = --→ DC 2 . 1. Since --→ OD⊥ --→ CF , we know --→ CD 2 ≥ --→ CF 2 . Since CF O ∼ AEO, we know --→ CD 2 -→ AE 2 ≥ --→ CF 2 -→ AE 2 = --→ OC 2 -→ OA 2 = e i + w * i 2 ≥ 1 -γ<label>(7)</label></formula><p>The last inequality holds as</p><formula xml:id="formula_46">W * 2 ≤ γ. Notice that -→ OA 2 = --→ OB 2 = 1, we know ABO is a isosceles triangle. Thus, -→ AG 2 = --→ GB 2 . Notice that ABE ∼ BGO, we have -→ AE 2 --→ AB 2 = --→ OG 2 --→ OB 2 = 1 - --→ GB 2 2 1<label>(8)</label></formula><p>WLOG, assume --→ OC 2 ≥ --→ OD 2 , as shown in the figure. We draw --→ HB --→ CD, and we know</p><formula xml:id="formula_47">--→ OH 2 ≥ --→ OB 2 = -→ OA 2 . Since CDO ∼ HBO, we have --→ CD 2 --→ HB 2 = --→ OD 2 --→ OB 2 = --→ OD 2 ≥ 1 -γ So --→ CD 2 ≥ (1 -γ) --→ HB 2 .</formula><p>On the other hand, ∠BAO &lt; π 2 , and A is between H and O, so ∠BAH &gt; π 2 , which means</p><formula xml:id="formula_48">--→ HB 2 ≥ --→ AB 2 = 2 --→ GB 2 . Thus, --→ GB 2 ≤ --→ HB 2 2 ≤ --→ CD 2</formula><p>2(1-γ) . Substitute it into (8), we get</p><formula xml:id="formula_49">-→ AE 2 --→ AB 2 ≥ 1 - --→ CD 2 2 4(1 -γ) 2 ≥ 1 - γ 1 -γ 2</formula><p>The last inequality holds since</p><formula xml:id="formula_50">--→ CD 2 = w * i -w i 2 ≤ 2γ.</formula><p>Substitute this inequality into <ref type="bibr" target="#b6">(7)</ref>, we get</p><formula xml:id="formula_51">e i + w * i -e i + w i 2 = --→ AB 2 ≤ -→ AE 2 1 -γ 1-γ 2 ≤ --→ CF 2 (1 -γ) 1 -γ 1-γ 2 (9) ≤ --→ CD 2 (1 -γ) 1 -γ 1-γ 2 = w * i -w i 2 √ 1 -2γ<label>(10)</label></formula><p>Notice that</p><formula xml:id="formula_52">e i + w i (w * i -w i ) = - --→ DF 2 , so e i + w i • e i + w i (w * i -w i ) = --→ DF . That means, (I -e i + w i • e i + w i )(w * i -w i ) 2 = --→ DC - --→ DF 2 = --→ CF 2</formula><p>The lemma follows by ( <ref type="formula">9</ref>) and (10).</p><p>2. By Figure <ref type="figure" target="#fig_8">10</ref>, we know</p><formula xml:id="formula_53">| e i + w * i -e i + w i , e i + w i | = --→ BE 2 . Since ABE ∼ GBO, we have --→ BE 2 --→ AB 2 = --→ GB 2 --→ BO 2 = --→ AB 2<label>2</label></formula><p>Therefore, using <ref type="bibr" target="#b9">(10)</ref> we get</p><formula xml:id="formula_54">| e i + w * i -e i + w i , e i + w i | = --→ AB 2 2 2 ≤ w * i -w i 2 2 2(1 -2γ)</formula><p>Moreover, e i + w * ie i + w i , e i + w i = e i + w * i , e i + w i -1 ≤ 0. 3. We know that</p><formula xml:id="formula_55">θ i,i * = 2 arcsin -→ AG 2 = 2 arcsin e i + w * i -e i + w i 2 2 ≤ e i + w * i -e i + w i 2 + e i + w * i -e i + w i 3 2<label>8</label></formula><p>The last inequality holds by Taylor's Series for arcsin, and the fact</p><formula xml:id="formula_56">e i + w * i -e i + w i 2 = --→ AB 2 ≤ w * i - w i 2 ≤ 2γ ≤ 1 50 . Thus, we have θ i,i * ≤ 1.001 w * i -w i 2 .</formula><p>F More Handy Lemmas</p><formula xml:id="formula_57">Lemma* F.1. If W 2 , W * 2 ≤ γ, then • (1-γ) 2 (1+γ) 2 I I + W I + W (1+γ) 2 (1-γ) 2 I, (1-γ) 2 (1+γ) 2 I I + W * I + W * (1+γ) 2 (1-γ) 2 I, • (1 -γ) 2 I (I + W) (I + W) (1 + γ) 2 I, (1 -γ) 2 I (I + W * ) (I + W * ) (1 + γ) 2 I.</formula><p>Therefore, the singular value of I + W is at most 1+γ 1-γ and at least 1-γ 1+γ . The singular value of I + W is at most 1 + γ and at least 1γ. The same claims hold for I + W * , I + W * respectively.</p><p>Proof. Since W 2 ≤ γ, we have 1γ ≤ I + W 2 ≤ 1 + γ, and 1γ ≤ e i + w i 2 ≤ 1 + γ. Therefore, I + W = Σ(I + W) where Σ is a diagonal matrix whose entries are within</p><formula xml:id="formula_58">[ 1 1+γ , 1 1-γ ].</formula><p>Putting into I + W I + W, we have</p><formula xml:id="formula_59">I + W I + W = (I + W) Σ 2 (I + W) 1 (1 -γ) 2 (I + W) (I + W) (1 + γ) 2 (1 -γ) 2 I Similarly we can show I + W I + W (1-γ) 2</formula><p>(1+γ) 2 I. Thus we know the singular value of I + W is at most 1+γ 1-γ and at least </p><formula xml:id="formula_60">| e i + w * i , e j + w j | = | e i + w * i , e j + w j | e i + w * i 2 e j + w j 2 ≤ | e i + w * i , e j + w j | (1 -γ) 2 = |w * i,j | + |w i,j | + | w i , w j | (1 -γ) 2 ≤ (2 + γ)γ (1 -γ) 2 ≤ 2.1γ</formula><p>where the last inequality holds since γ ≤ 1 100 . The same analysis works for e i + w i , e j + w j . Lemma* F.3 (Triangle inequality between e i + w i , e i + w *</p><formula xml:id="formula_61">i , w * i -w i ). | e i + w i 2 -e i + w * i 2 | ≤ w * i -w i 2 . Lemma* F.4. If W 2 , W * 2 ≤ γ, |g| ≤ 2dγ.</formula><p>Proof. By definition and Lemma F.3, we know |g|</p><formula xml:id="formula_62">= d i=1 ( e i + w * i 2 -e i + w i 2 ) ≤ d i=1 w * i -w i 2 ≤ 2dγ. Lemma* F.5. If W 2 , W * 2 ≤ γ, | e i + w * i -e i + w i , e j + w j | ≤ w * i -wi 2 √</formula><p>1-2γ . Proof. By Cauchy Schwartz and Lemma E.1 term 1.</p><formula xml:id="formula_63">Lemma* F.6. |x k -y k | ≤ k 2 |x -y|(|x| k-1 + |y| k-1 ). Proof. |x k -y k | = (x -y) k-1 t=1 x t y k-t-1 +y t x k-t-1 2 ≤ k 2 |x -y|(|x| k-1 + |y| k-1 )</formula><p>, where the last inequality holds since  </p><formula xml:id="formula_64">|x t y k-t-1 + y t x k-t-1 | ≤ |x| t |y| k-t-1 + |y| t |x| k-t-1 ≤ |x| k-1 + |y| k-1 ,</formula><formula xml:id="formula_65">)(e i + w i ) 2 ≤ w * i -w i 2 | e i + w * i , e j + w j k | + (1 + γ)| e i + w * i , e j + w j k -e i + w i , e j + w j k | x ≤ w * i -w i 2 (2.1γ) k-2 e i + w * i , e j + w j 2 + (1 + γ)k 2 | e i + w * i -e i + w i , e j + w j |(| e i + w * i , e j + w j | k-1 + | e i + w i , e j + w j | k-1 ) ≤ e i + w * i , e j + w j 2 w * i -w i 2 (2.1γ) k-2 + (1 + γ)k(2.1γ) k-3 2 | e i + w * i -e i + w i , e j + w j | + e i + w i , e j + w j 2 (1 + γ)k(2.1γ) k-3 2 | e i + w * i -e i + w i , e j + w j | y ≤ w * i -w i 2 (2.1γ) k-2 + 0.52k(2.1γ) k-3 e i +</formula><formula xml:id="formula_66">| 2k-1 + | e i + w * i , e j + w j | 2k-1 + w * i -w i 2 (2.1γ) 2k-2 e i + w * i , e j + w j 2 z ≤ (1 + γ)k(2.1γ) 2k-3 √ 1 -2γ e i + w i , e j + w j 2 + (1 + γ)k(2.1γ) 2k-3 √ 1 -2γ + (2.1γ) 2k-2 e i + w * i , e j + w j 2 w * i -w i 2 { ≤1.05k(2.1γ) 2k-3 e i + w i , e j + w j 2 + e i + w * i , e j + w j 2 w * i -w i 2 | ≤8(2.2γ) 2k-3 e i + w i , e j + w j 2 + e i + w * i , e j + w j 2 w * i -w i 2</formula><p>where x uses Lemma F.2 and Lemma F.3, y uses Lemma F.6, z uses Lemma F.5, { holds as γ ≤ 1 100 , and } holds as</p><formula xml:id="formula_67">1.05k(2.1) 2k-3 ≤ 8(2.2) 2k-3 for k ≥ 2. Lemma* F.9. If W 2 , W * 2 ≤ γ, for fixed j ∈ [d], i =j e i + w i , e j + w j 2 ≤ 4γ (1 -γ) 2 , i =j e i + w * i , e j + w j 2 ≤ 4γ(1 + γ) 1 -2γ .</formula><p>Similarly, for fixed i ∈ [d],</p><formula xml:id="formula_68">j =i e i + w i , e j + w j 2 ≤ 4γ (1 -γ) 2 , j =i e i + w * i , e j + w j 2 ≤ 4γ(1 + γ) 1 -2γ<label>.</label></formula><p>Proof. By matrix multiplication,</p><formula xml:id="formula_69">d i=1 e i + w * i , e j + w j 2 = d i=1 e j + w j e i + w * i • e i + w * i e j + w j = e j + w j I + W * • I + W * e j + w j By Lemma F.1, we know I + W * • I + W * (1+γ) 2 (1-γ) 2 I. That means, d i=1 e i + w * i , e j + w j 2 ≤ (1+γ) 2 (1-γ) 2 .</formula><p>On the other hand, by Lemma E.1 term 2, e j + w * j , e j + w j 2 = (1e j + w * je j + w j , e j + w j ) 2 ≥ 1 -</p><formula xml:id="formula_70">w * i -wi 2 2 1-2γ</formula><p>. Therefore, we know</p><formula xml:id="formula_71">i =j e i + w * i , e j + w j 2 ≤ (1 + γ) 2 (1 -γ) 2 -1 + w * i -w i 2 2 1 -2γ = 4γ (1 -γ) 2 + w * i -w i 2 2 1 -2γ ≤ 4γ(1 + γ) 1 -2γ</formula><p>Using the same analysis, we get i =j e i + w i , e j + w j</p><formula xml:id="formula_72">2 ≤ (1+γ) 2 (1-γ) 2 -1 = 4γ (1-γ) 2 .</formula><p>The analysis for fixed i is similar.</p><p>Lemma* F.10. For any matrix A, we have Diag(A) 2 ≤ A 2 and Off-Diag(A) 2 ≤ 2 A 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. By definition, we know</head><formula xml:id="formula_73">Diag(A) 2 = max i∈[d] e i Ae i ≤ max v∈R d v Av = A 2 , and Off-Diag(A) 2 ≤ A 2 + Diag(A) 2 ≤ 2 A 2 . Lemma* F.11. If W 2 , W * 2 ≤ γ, A 2 ≤ 2γ(γ 2 +3)</formula><p>1-γ 2 . Proof. By Lemma F.1, we have</p><formula xml:id="formula_74">A 2 = (I + W * )I + W * -(I + W)I + W 2 ≤ (1 + γ) 2 1 -γ - (1 -γ) 2 1 + γ = 2γ(γ 2 + 3) 1 -γ 2 .</formula><p>Lemma* F.12. If W 2 , W * 2 ≤ γ ≤ 1 100 , |e j + w j Ae j + w je j Ae j | ≤ 5γ 2 . Proof.</p><formula xml:id="formula_75">|e j + w j Ae j + w j -e j Ae j | ≤ |e j + w j A(e j + w j -e j )| + |(e j + w j -e j ) Ae j | x ≤ 4γ 2 (γ 2 + 3) 1 -γ 2 y &lt; 5γ 2</formula><p>where x uses Cauchy Schwartz, Lemma F.11 and e j + w je j 2 ≤ γ, and y holds as γ ≤ 1 100 . Lemma* F.13. For any i ∈</p><formula xml:id="formula_76">[n], | [e i + w * i 2 -e i + w i 2 ] -[w * i,i -w i,i ]| ≤ 6.07γ 2 . Proof. e i + w i 2 -e i + w * i 2 = e i + w i , e i + w i -e i + w * i , e i + w * i = e i + w i , e i + w i -e i + w * i + w i -w * i , e i + w * i = w i -w * i , e i + e i + w i , e i + w i -e i + w * i + w i -w * i , e i + w * i -e i = w i,i -w * i,i + e i + w i , e i + w i -e i + w * i + w i -w * i , e i + w * i -e i As a result, |[ e i + w i 2 -e i + w * i 2 ] -[w i,i -w * i,i ]| ≤ || e i + w i , e i + w i -e i + w * i | + | w i -w * i , e i + w * i -e i | x ≤ (1 + γ)2γ 2 1 -2γ + 4γ 2 ≤ 6.07γ 2</formula><p>where x uses Lemma E.1 term 2 and e i + w * ie i 2 ≤ 2γ, and Cauchy Schwartz. So the claim follows. Corollary F.14. |g -Tr(W * -W)| ≤ 6.07dγ 2 . Lemma* F.15. I + W is close to I on its diagonals, and close to W on its off-diagonals. More specifically, if</p><formula xml:id="formula_77">W 2 , W * 2 ≤ γ ≤ 1 100 , Diag(I + W) -I 2 ≤ γ 2 2(1 -γ) 2 , Diag(I + W * ) -I 2 ≤ γ 2 2(1 -γ) 2 Off-Diag(I + W -W) 2 ≤ 4γ 2 1 -γ , Off-Diag(I + W * -W * ) 2 ≤ 4γ 2 1 -γ I + W -I 2 ≤ 2.05γ, I + W * -I 2 ≤ 2.05γ</formula><p>Proof. For the diagonal terms,</p><formula xml:id="formula_78">Diag(I + W) -I 2 = max j |I + W j,j -1| = max j 1 + w j,j -e j + w j 2 e j + w j 2 ≤ max j (1 + w j,j ) 2 -e j + w j 2 2</formula><p>e j + w j 2 1 1 + w j,j + e j + w j 2 ≤ max</p><formula xml:id="formula_79">j i =j w 2 j,i 2(1 -γ) 2 ≤ γ 2 2(1 -γ) 2</formula><p>For the off-diagonal terms, we know I + W = (I + W)Σ for some diagonal matrix Σ, so</p><formula xml:id="formula_80">Off-Diag(I + W -W) 2 = Off-Diag((I + W)Σ -W) 2 = Off-Diag((Σ -I)W) 2 x ≤ 2 (Σ -I)W 2 ≤ 4γ 2 1 -γ</formula><p>where x uses Lemma F.10. For the difference between I + W and I, we split I + W into diagonal and offdiagonal parts:</p><formula xml:id="formula_81">I + W -I 2 = Diag(I + W) + Off-Diag(I + W) -I 2 = Off-Diag(W) 2 + γ 2 2(1 -γ) 2 + 4γ 2 1 -γ x ≤ 2 W 2 + γ 2 (9 -8γ) 2(1 -γ) 2 ≤ 2.05γ</formula><p>where x uses Lemma F.10.</p><formula xml:id="formula_82">Lemma* F.16. If W 2 , W * 2 ≤ γ ≤ 1 100 , A -[W * -W + (W * -W) -Diag(W * -W)] 2 ≤ 9.2γ 2</formula><p>Proof. By definition,</p><formula xml:id="formula_83">(I + W * )I + W * -(I + W)I + W -(W * -W) + (I + W * -I + W ) 2 = W * (I + W * -I) -W(I + W -I) 2 ≤ W * (I + W * -I) 2 + W(I + W) -I) 2 ≤2.05γ 2 + 2.05γ 2 = 4.1γ 2</formula><p>where the last inequality uses Lemma F.15. Below we further approximate I + W * -I + W .</p><formula xml:id="formula_84">I + W * -I + W -(W * -W) -Diag(W * -W) 2 = Diag(I + W * -I + W ) + Off-Diag(I + W * -I + W ) -(W * -W) -Diag(W * -W) 2 x ≤ Off-Diag(I + W * -I + W ) -Off-Diag(W * -W) 2 + γ 2 (1 -γ) 2 y ≤ 4γ 2 1 -γ + γ 2 (1 -γ) 2 ≤ 5.1γ 2</formula><p>where x uses Lemma F.15, y uses Lemma F.15 Combining everything,</p><formula xml:id="formula_85">A -[W * -W + (W * -W) -Diag(W * -W)] 2 ≤ 9.2γ 2</formula><p>Using Lemma F.10, we immediately have the following corollary.</p><p>Corollary F.17.</p><formula xml:id="formula_86">Diag(A) -Diag(W * -W) 2 ≤ 9.2γ 2 . Lemma* F.18. For η ≤ 1 πd , I -η π 2 uu + π 2 + 1 I 2 ≤ 1 -η π 2 + 1</formula><p>Proof. Consider another basis (e 1 , • • • , e d ) where e 1 = u u 2 . For every unit vector</p><formula xml:id="formula_87">v = (v 1 , • • • , v d ) in this new space, we know v T I -η π 2 uu + π 2 + 1 I v = v 2 2 -η π 2 + 1 v 2 2 - πηd 2 v 2 1</formula><p>Hence we get</p><formula xml:id="formula_88">0 ≤ v T I -η π 2 uu + π 2 + 1 I v ≤ 1 -η π 2 + 1 v 2 2</formula><p>By definition of matrix norm, the lemma follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Proofs for Section B G.1 Proof for Claim B.1</head><p>Comparing with Lemma 2.1, we know that for fixed j, P 1,j is already contained in -∇L(W) j as the first term, while P 3,j is simply the summand when i = j, ignoring the first term. Below we show how to obtain P 2,j from i = j cases. We will bound the approximation error in Lemma B.2 and Lemma B.3. </p><formula xml:id="formula_89">i =j π 2 -θ i * ,j (e i + w * i ) - π 2 -θ i,</formula><formula xml:id="formula_90">( e i + w * i -e i + w i ) - i =j 1 2 e j + w j (e i + w * i )e i + w * i e j + w j + i =j</formula><p>1 2 e j + w j (e i + w i )e i + w i e j + w j   e j + w j</p><p>=A j e j + w j + g j -1 2 e j + w j A j e j + w j e j + w j = P 2,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Proof for Lemma B.2</head><p>In order to prove this lemma, we bound the approximation loss of θ i,j , θ i * ,j in Lemma G.1, and the approximation loss of sin θ i,j , sin θ i * ,j in Lemma G.2.</p><formula xml:id="formula_91">Lemma* G.1 (Approximation loss related to θ i,j , θ i * ,j ). If W 2 , W * 2 ≤ γ ≤ 1 100 , d j=1 i =j ( π 2 -θ i * ,j -e i + w * i , e j + w j )(e i + w * i ) -( π 2 -θ i,j -e i + w i , e j + w j )(e i + w i ), w * j -w j ≤0.083 W * -W 2 F Proof.</formula><p>By definition, π 2θ i * ,j = arcsin e i + w * i , e j + w j , and π 2θ i,j = arcsin e i + w i , e j + w j . The Taylor series of arcsin</p><formula xml:id="formula_92">x at x = 0 is ∞ k=0 (2k)! 4 k (k!) 2 (2k+1) x 2k+1 , where for k ≥ 1, (2k)! 4 k (k!) 2 (2k + 1) ≤ 1 6<label>(11)</label></formula><p>Thus, </p><formula xml:id="formula_93">d j=1 i =j ( π 2 -θ i * ,j -e i + w * i , e j + w j )(e i + w * i ) -( π 2 -θ i,j -e i + w i , e j + w j )(e i + w i ), w * j -w j x ≤ d j=1 i =j ∞ k=1<label>1</label></formula><formula xml:id="formula_94">w * i -w i 2 2   1 2   d j=1 i =j e i + w * i , e j + w j 2 + e i + w i , e j + w j 2 w * j -w j 2 2   1 2 ≤1.01   d i=1 w * i -w i 2 2   i =j e i + w * i , e j + w j 2 + e i + w i , e j + w j 2     1 2   d j=1 w * j -w j 2 2   i =j e i + w * i , e j + w j 2 + e i + w i , e j + w j 2     1 2 } ≤1.01 4γ (1 -γ) 2 + 4γ(1 + γ) 1 -2γ W * -W 2 F ≤ 0.083 W * -W 2 F</formula><p>where x is by Taylor series, y uses Cauchy Schwartz, z uses Lemma F.7, { holds as γ ≤ 1 100 , | uses Cauchy Schwartz, } uses Lemma F.9, ~holds as γ ≤ 1 100 .</p><p>Lemma* G.2 (Approximation loss related to sin θ i,j , sin</p><formula xml:id="formula_95">θ i * ,j ). If W 2 , W * 2 ≤ γ ≤ 1 100 , d j=1 i =j e i + w * i 2 sin θ i * ,j -1 + 1 2 e i + w * i , e j + w j 2 - e i + w i 2 sin θ i,j -1 + 1 2 e i + w i , e j + w j 2 e j + w j , w * j -w j ≤ 0.002 W * -W 2 F Proof.</formula><p>By definition, we know θ i * ,j = arccos e i + w * i , e j + w j , and θ i,j = arccos e i + w i , e j + w j . The Taylor series of sin(arccos</p><formula xml:id="formula_96">x) at x = 0 is 1 -x 2 2 -x 4 8 -x 6 16 -5x 8 128 -• • • = ∞ k=0 c k x 2k , where c k ≤ 1 8 for k ≥ 2.</formula><p>Thus,</p><formula xml:id="formula_97">d j=1 i =j e i + w * i 2 sin θ i * ,j -1 + 1 2 e i + w * i , e j + w j 2 - e i + w i 2 sin θ i,j -1 + 1 2 e i + w i , e j + w j 2 e j + w j , w * j -w j x ≤ d j=1 i =j ∞ k=2 1 8 e i + w i 2 e i + w i , e j + w j 2k -e i + w * i 2 e i + w * i , e j + w j 2k w * j -w j 2 y ≤ d j=1 i =j ∞ k=2 (2.2γ) 2k-3 e i + w i , e j + w j 2 + e i + w * i , e j + w j 2 w * i -w i 2 w * j -w j 2 z ≤2.3γ   d j=1 i =j e i + w i , e j + w j 2 + e i + w * i , e j + w j 2 w * i -w i 2 2   1 2   d j=1 i =j e i + w i , e j + w j 2 + e i + w * i , e j + w j 2 w * j -w j 2 2   1 2 ≤2.3γ   d i=1 w * i -w i 2 2   j =i e i + w i , e j + w j 2 + e i + w * i , e j + w j 2     1 2   d j=1 w * j -w j 2 2   i =j e i + w i , e j + w j 2 + e i + w * i , e j + w j 2     1 2 { ≤2.3γ 4γ (1 -γ) 2 + 4γ(1 + γ) 1 -2γ W * -W 2 F | &lt; 0.002 W * -W 2 F</formula><p>where x is by Taylor series, y uses Lemma F.8 and Cauchy Schwartz, z uses Cauchy Schwartz and γ ≤ 1 100 , { uses Lemma F.9, and | holds as γ ≤ 1 100 . Proof for Lemma B.2. Combining the results from Lemma G.1 and Lemma G.2, the lemma follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Proof for Lemma B.3</head><p>Denote ∆ P + ∇L(W). This lemma is harder to prove than the previous one since we need to bound the spectral norm of a matrix ∆. First of all, we need to represent ∆. Again, the difference has two parts: approximation for θ i,j , θ i * ,j , and sin θ i,j , sin θ i * ,j . Denote the two parts as ∆ 1 , ∆ 2 , where ∆ = ∆ 1 + ∆ 2 . From the proof of Lemma G.1, we know the j-th column of the first part is</p><formula xml:id="formula_98">∆ 1,j i =j ∞ k=1 (2k)! 4 k (k!) 2 (2k + 1) e i + w * i , e j + w j 2k+1 (e i + w * i ) -e i + w i , e j + w j 2k+1 (e i + w i )</formula><p>And the j-th column of the second part is</p><formula xml:id="formula_99">∆ 2,j i =j ∞ k=2</formula><p>c k e i + w i 2 e i + w i , e j + w j 2ke i + w * i 2 e i + w * i , e j + w j 2k e j + w j Below we bound ∆ 1 2 in Lemma G.3, and bounds ∆ 2 2 in Lemma G. <ref type="bibr" target="#b3">4</ref>.</p><formula xml:id="formula_100">Lemma* G.3. If W 2 , W * 2 ≤ γ ≤ 1 100 , ∆ 1 2 ≤ 3.4γ 2 .</formula><p>Proof. Define U, V such that for i = j, U i,j = V i,j = 0, and for i = j,</p><formula xml:id="formula_101">U i,j = ∞ k=1 (2k)! 4 k (k!) 2 (2k + 1) e i + w * i , e j + w j 2k+1 , V i,j = ∞ k=1 (2k)! 4 k (k!) 2 (2k + 1)</formula><p>e i + w i , e j + w j 2k+1</p><p>By matrix multiplication,</p><formula xml:id="formula_102">∆ 1 = d i=1 [(I + W * ) * ,i U i, * -(I + W) * ,i V i, * ] = (I + W * )U -(I + W)V<label>(12)</label></formula><p>So it suffices to bound U 2 , V 2 . For i = j,</p><formula xml:id="formula_103">|U i,j | = ∞ k=1 (2k)! 4 k (k!) 2 (2k + 1) e i + w * i , e j + w j 2k+1 x ≤ ∞ k=1 (2.1γ) 2k-1 6 e i + w * i , e j + w j 2 ≤ 0.4γ e i + w * i , e j + w j 2</formula><p>where x uses Lemma F.2 and ( <ref type="formula" target="#formula_92">11</ref>). Now, we know</p><formula xml:id="formula_104">U 1 x = max j d i=1 |U i,j | ≤ max j i =j 0.4γ e i + w * i , e j + w j 2 y ≤ 1.6(1 + γ)γ 2 1 -2γ ≤ 1.65γ 2</formula><p>where x is by definition, y uses Lemma F.9. Similarly,</p><formula xml:id="formula_105">U ∞ = max i d j=1 |U i,j | ≤ max i j =i 0.4γ e i + w * i , e j + w j 2 ≤ 1.65γ 2</formula><p>By Hölder's inequality, we have</p><formula xml:id="formula_106">U 2 ≤ U 1 U ∞ ≤ 1.65γ 2</formula><p>Now we do the same analysis for V.</p><formula xml:id="formula_107">|V i,j | = ∞ k=1 (2k)! 4 k (k!) 2 (2k + 1) e i + w i , e j + w j 2k+1 ≤ ∞ k=1 (2.1γ) 2k-1 6 e i + w i , e j + w j 2 ≤ 0.4γ e i + w i , e j + w j 2 Hence, V 1 = max j d i=1 |V i,j | ≤ max j i =j 0.4γ e i + w i , e j + w j 2 ≤ 1.65γ 2 . Similarly, V ∞ ≤</formula><p>1.65γ 2 , and by Hölder's inequality, <ref type="formula" target="#formula_102">12</ref>), we get</p><formula xml:id="formula_108">V 2 ≤ V 1 V ∞ ≤ 1.65γ 2 . Using (</formula><formula xml:id="formula_109">∆ 1 2 ≤ I + W * 2 U 2 + I + W 2 V 2 ≤ 2(1 + γ)1.65γ 2 &lt; 3.4γ 2 Lemma* G.4. If W 2 , W * 2 ≤ γ ≤ 1 100 , ∆ 2 2 ≤ 6γ 3 . Proof.</formula><p>By definition, we can write</p><formula xml:id="formula_110">∆ 2 = I + WDiag    i =j ∞ k=2 c k e i + w i 2 e i + w i , e j + w j 2k -e i + w * i 2 e i + w * i , e j + w j 2k    d j=1</formula><p>So it suffices to bound the norm of the diagonal matrix, which is the maximum of the diagonal entries. For any</p><formula xml:id="formula_111">j ∈ [d], we have i =j ∞ k=2 c k e i + w i 2 e i + w i , e j + w j 2k -e i + w * i 2 e i + w * i , e j + w j 2k ≤ i =j ∞ k=2 1 8 e i + w i 2 e i + w i , e j + w j 2k | + | e i + w * i 2 e i + w * i , e j + w j 2k x ≤ i =j ∞ k=2 1 4 (1 + γ)(2.1γ) 2k-2 e i + w i , e j + w j 2 + e i + w * i , e j + w j 2 y ≤0.6γ 2 i =j e i + w i , e j + w j 2 + e i + w * i , e j + w j 2 z ≤0.6γ 2 4γ (1 -γ) 2 + 4γ(1 + γ) 1 -2γ &lt; 5γ 3</formula><p>where x uses Lemma F.2, y uses γ ≤ 1 100 , z uses Lemma F.9. So we get ∆ 2 2 ≤ 1+γ 1-γ 5γ 3 ≤ 6γ 3 . Proof for Lemma B.3. Combining the results from Lemma G.3 and Lemma G.4, the lemma follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Proofs for Section C H.1 Proof for Lemma C.1</head><p>In Lemma B.3, we use P(W) to approximate -∇L(W) in terms of spectral norm, with approximation loss 3.5γ 2 . Below we will get Q(W) from P(W) by removing a few more lower order terms.</p><p>By definition 2.3, we have P 2,j =ge j + w j -( e j + w * j 2e j + w j 2 )e j + w j + I -1 2 e j + w j • e j + w j Ae j + w j + I -1 2 e j + w j • e j + w j (e j + w j ) -I -1 2 e j + w j • e j + w j (e j + w * j )e j + w * j e j + w j =ge j + w j -( e j + w * j 2e j + w j 2 )e j + w j + I -1 2 e j + w j • e j + w j Ae j + w j + 1 2 (e j + w j ) -(e j + w * j )e j + w * j e j + w j + 1 2 e j + w j e j + w * j 2 (e j + w * j e j + w j ) 2 =ge j + w j + I -1 2 e j + w j • e j + w j Ae j + w j + 3 2 (e j + w j )e j + w * j e j + w j (e j + w * j ) + 1 2 e j + w * j 2 (e j + w * j e j + w j ) 2e j + w * j 2 e j + w j =ge j + w j + I -1 2 e j + w j • e j + w j Ae j + w jw * j + w j + (1e j + w * j e j + w j )(e j + w * j ) + 1 2 e j + w j 2 + 1 2 e j + w * j 2 (e j + w * j e j + w j ) 2e j + w * j 2 e j + w j</p><p>Combining every column together, we get</p><formula xml:id="formula_112">P 2 = gI + W +AI + W - 1 2 I + WDiag({e j + w j Ae j + w j } d j=1 )-(W * -W)+I + W * Σ 1 +I + WΣ 2</formula><p>where Σ 1 = Diag({( e j + w * j 2e j + w * j 2 e j + w * j e j + w j )} d j=1 )</p><formula xml:id="formula_113">Σ 2 = Diag({ 1 2 e j + w j 2 + 1 2 e j + w * j 2 (e j + w * j e j + w j ) 2 -e j + w * j 2 } d j=1 )</formula><p>Using Lemma F.12, we replace e j + w j Ae j + w j with e j Ae j . By Lemma F.1,</p><formula xml:id="formula_114">P 2 -gI + W + AI + W - 1 2 I + WDiag(A) -(W * -W) + I + W * Σ 1 + I + WΣ 2 2 ≤ 5(1 + γ) 2(1 -γ) &lt; 2.6γ 2</formula><p>We then focus on the middle two summands in the sum.</p><formula xml:id="formula_115">AI + W - 1 2 I + WDiag(A) = (A - 1 2 Diag(A)) + A(I + W -I) - 1 2 (I + W -I)Diag(A)</formula><p>By Lemma F.10, Diag(A) 2 ≤ A 2 , so</p><formula xml:id="formula_116">AI + W - 1 2 I + WDiag(A) -A - 1 2 Diag(A) 2 = A(I + W -I) - 1 2 (I + W -I)Diag(A) 2 ≤ A 2 I + W -I 2 + 1 2 I + W -I 2 Diag(A) 2 x ≤ 3γ(γ 2 + 3) 1 -γ 2 2.05γ &lt; 18.5γ 2</formula><p>where x uses Lemma F.11 and Lemma F.15. Moreover, by Lemma E.1 term 2, we know</p><formula xml:id="formula_117">Σ 1 2 ≤ max i∈[d] (1 + γ) w * i -wi 2 2 2(1-2γ) ≤ 2.07γ 2 , and in Σ 2 , 1 2 e j + w * j 2 (e j + w * j e j + w j ) 2 - 1 2 e j + w * j 2 ≤<label>1 2 (1 + γ</label></formula><p>) e j + w * j e j + w j -1 e j + w * j e j + w j + 1 ≤ 2.07γ 2 so the following terms approximates P 2 with approximation loss (2.6 + 18.5 + 2.07 + 2.07)γ 2 &lt; 25.3γ 2 .</p><formula xml:id="formula_118">I + W(gI -Σ 3 ) + A - 1 2 Diag(A) -(W * -W)</formula><p>where Σ 3 = Diag({ 1 2 e j + w * j 2 -1 2 e j + w j 2 } d j=1 ). By Lemma F.16 and Corollary F.17, we know A -[W * -W + (W * -W) -Diag(W * -W)] 2 ≤ 9.2γ 2 and Diag(A) -Diag(W * -W) 2 ≤ 9.2γ 2 . Therefore, with approximation loss of 18.4γ 2 , we get</p><formula xml:id="formula_119">A - 1 2 Diag(A) -W * -W + (W * -W) - 3 2 Diag(W * -W) 2 ≤ 18.4γ 2</formula><p>We then approximate Σ 3 :</p><formula xml:id="formula_120">(I + W)Σ 3 -(I + W) 1 2 Diag(W * -W) 2 ≤ 1 + γ 1 -γ 1 2 max j | e j + w * j 2 -e j + w j 2 -w * j,j + w j,j | &lt; 3.1γ 2</formula><p>where the last inequality is by Lemma F.13. Moreover,</p><formula xml:id="formula_121">I + W 1 2 Diag(W * -W) - 1 2 Diag(W * -W) 2 ≤ I + W -I 2 1 2 Diag(W * -W) 2 &lt; 2.05γ 1 2 max i |w * i,i -w i,i | &lt; 2.05γ 2</formula><p>Putting everything together, with approximation loss of (25.3 + 18.4 + 3.1 + 2.05)γ 2 = 49γ 2 to P 2 , we get</p><formula xml:id="formula_122">(W * -W) -2Diag(W * -W) + gI + W e i + w i O e i ∆w i ∆ e i + w i 2</formula><p>Figure <ref type="figure">11</ref>: ∆g is approximately (the summation of) the projection of ∆w i onto e i + w i For P 3 , using the same idea in the proof of Lemma D.3, we have</p><formula xml:id="formula_123">P 3 = π 2 (W * -W) + I + W -I + W * Σ 4 + I + WΣ 5</formula><p>where Σ 4 = Diag({θ j,j * e j + w * j 2 } d j=1 ), Σ 5 = Diag({ e j + w * j 2 sin θ j,j *θ j,j * e j + w * j 2 } d j=1 ). By Taylor's Theorem, we know Σ 5 2 ≤ Diag({ e j + w * j 2 θ 3 j,j * /3} d j=1 ) 2 . Notice that θ j,j * ≤ 2.002γ by Lemma E.1 term 3, and</p><formula xml:id="formula_124">I + W -I + W * 2 ≤ 1+γ 1-γ -1-γ 1+γ ≤ 4.001γ. Consequently, P 3 - π 2 (W * -W) 2 ≤ I + W -I + W * Σ 4 2 + I + WΣ 5 2 &lt;4.001 * 2.002(1 + γ)γ 2 + (1 + γ) 2 3(1 -γ) (2.002γ) 3 &lt; 8.1γ 2 + 2.8γ 3 &lt; 8.2γ 2</formula><p>we only need to keep the term π 2 (W * -W) with approximation loss 8.2γ 2 to P 3 . Now, combining the approximations to P 2 and P 3 , and Lemma B.3, we have the following matrix with (49 + 8.2 + 3.5)γ 2 &lt; 61γ 2 approximation loss to -∇L(W): </p><formula xml:id="formula_125">∇L(W) 2 ≤ 61γ 2 + π 2 (W * -W) I + uu + (W * -W) -2Diag(W * -W) + gI + W 2 ≤61γ 2 + (d + 1)πγ + 2γ + 4γ + |g| 1 + γ 1 -γ &lt; 61γ 2 + (d + 3)πγ + 2.05dγ &lt; 6dγ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 Proof for Lemma C.3</head><p>In this proof, we use w j to represent the j-th column of W t , and denote w j as the j-th column of G t .</p><formula xml:id="formula_126">H.3.1 ∆g t ≈ η L(W t ), I + W t</formula><p>For the intuition of this section, see Figure <ref type="figure">11</ref>. The changes in potential function g is essentially the changes in e i + w i 2 (summing over i), which is approximately ∆w i projected onto e i + w i . If we write it in matrix form, we get ∆g t ≈ η L(W t ), I + W t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By definition we know</head><formula xml:id="formula_127">G t 2 = ∇L(W t ) + E t 2 x ≤ ∇L(W t ) 2 + E t 2 y ≤ 6dγ + ε = G 2 ,</formula><p>where x uses triangle inequality, y uses Lemma C.2. We have</p><formula xml:id="formula_128">η w j 2 ≤ η G t 2 ≤ γ 2 G 2 ≤ γ 6d , η 2 w j 2 ≤ η G t 2 2 ≤ γ 2<label>(13)</label></formula><p>By Definition 2.2, we know</p><formula xml:id="formula_129">g t g t+1 -g t = d j=1</formula><p>e j + w j , e j + w j e j + w j 2 -e j + w jη w j , e j + w jη w j e j + w jη w j 2 = d j=1 e j + w j , e j + w j e j + w jη w j 2e j + w jη w j , e j + w jη w j e j + w j 2 e j + w j 2 e j + w jη w j 2 = d j=1 e j + w j 2 ( e j + w jη w j 2e j + w j 2 ) + 2η w j , e j + w jη 2 w j 2 2</p><p>e j + w jη w j 2</p><p>If we project η w j onto the e j + w j direction, we get e j + w jη w j 2 = ( e j + w j 2e j + w j , η w j ) 2 + ( η j 2 2e j + w j , η w j 2 ) 2</p><p>≤ ( e j + w j 2e j + w j , η w j ) 2 + η w j 2 2</p><p>x ≤ e j + w j 2e j + w j , η w j + η w j 2 2</p><p>Using (13), we have e j + w j 2e j + w j , η w j ≥ 1 2 . By taking square on both sides, we know x holds. It is trivial to show that e j + w jη w j 2 ≥ e j + w j 2e j + w j , η w j , so we know e j + w j , η w j ≤ e j + w jη w j 2e j + w j 2 ≤e j + w j , η w j + η w j ( e j + w j 2e j + w jη w j 2 )η w j , e j + w j e j + w jη w j 2 + η G t , I + W t Thus we get the following approximation for g t .</p><formula xml:id="formula_130">| g t -η G t , I + W t | ≤ d j=1 -η 2 w j 2 2</formula><p>e j + w jη w j 2 + ( e j + w j 2e j + w jη w j 2 )η w j , e j + w j e j + w jη w j 2 + e j + w j 2 η w j 2 2</p><p>e j + w jη w j 2</p><p>x ≤ d j=1 η w j , e j + w j (η w j , e j + w j + η w j</p><p>2 ) e j + w jη w j 2 + 0.02η 2 w j e j + w jη w j 2 + 0.02ηγ 2 z ≤ 1.04ηdγ 2 where x uses ( <ref type="formula">14</ref>) again, and y z uses (13), γ ≤ 1 100 and e j + w jη w j 2 ≥ 0.98.</p><formula xml:id="formula_132">Thus | g t -η ∇L(W t ), I + W t | ≤ 1.04ηdγ 2 + |η E t , I + W t | &lt; 1.04ηdγ 2 + 1.03η √ dε H.3.2 ∆g t ≈ ηTr(∇L(W t ))</formula><p>We want to approximate I + W t with I. Below is the error bound. </p><formula xml:id="formula_133">| ∇L(W t ), I + W t -I | = | ∇L(W t ) + Q t -Q t , I + W t -I | x =d • 61γ 2 • 2.05γ + d i=1 2.05γ (Q t - π 2 (W * -W t )uu ) i 2 + π 2 (W * -W t )uu , I + W t -I y ≤1.251dγ 2 + 2.05dγ πγ + 2γ + 4γ + 1 + γ 1 -γ |g t | + Tr π 2 (W * -W t )u u I + W t -I z ≤20dγ 2 + 2.1dγ|g t | + π 2 (W * -W t )u 2 (I + W t -I)u 2 { ≤ 20dγ 2 +</formula><formula xml:id="formula_134">+ W t ) = π 2 -1 Tr(W * -W t ) + π 2 Tr (W * -W t ) uu + gTr(I + W t ) = π 2 -1 (Tr(W * -W t ) -g t ) + π 2 -1 g t + π 2 Tr (W * -W t ) uu + g t Tr(I + W t )</formula><p>Therefore, </p><formula xml:id="formula_135">Tr(Q t ) -g t Tr(I) - π 2 -1 g t = Tr(Q t ) -d + π 2 -1 g t ≤ π 2 -1 (Tr(W * -W t ) -g t ) + π<label>2</label></formula><formula xml:id="formula_136">-s t = (W t -W t+1 )u = η(∇L(W t ) + E t )u = -ηQ t u + η(Q t + ∇L(W t ) + E t )u By definition of Q t , Q t u = π 2 (W * -W t ) I + uu + (W * -W t ) -2Diag(W * -W t ) + g t I + W t u = (d + 1)π 2 s t + (W * -W t ) -2Diag(W * -W t ) + g t I + W t u</formula><p>Thus, we know</p><formula xml:id="formula_137">Q t u - (d + 1)π 2 s t 2 = (W * -W t ) -2Diag(W * -W t ) + g t I + W t u 2 ≤ √ d (W * -W t ) 2 + 2 Diag(W * -W t ) 2 + g t I + W t 2 x ≤ √ d 2γ + 4γ + |g t | 1 + γ 1 -γ &lt; (6γ + 1.03|g t |) √ d</formula><p>where x uses Lemma F. </p><formula xml:id="formula_138">H t+1 = H t -ηH t π 2 uu + π 2 -ηH t + 2ηDiag(H t ) + ηg t I + W -η(G t + Q t )</formula><p>That gives,</p><formula xml:id="formula_139">H t+1 + H t+1 2 ≤ (H t + H t ) I -η π 2 uu + π 2 + 1 2 + 2η Diag(H t + H t ) 2 + 2η|g t | I + W 2 + 2η E t + ∇L(W t ) + Q t 2 x ≤ I -η π 2 + 1 H t + H t 2 + 2η H t + H t 2 + 2(1 + γ)η|g t | 1 -γ + 2ηε + 122ηγ 2 y ≤ I -η π 2 -1 H t + H t 2 + 2.05η|g t | + 124ηγ 2<label>(15)</label></formula><p>where x uses Lemma F.18, Lemma F.10, E t 2 ≤ ε and Lemma C.1. y uses ε ≤ γ 2 and γ ≤ 1 100 . Similarly, we get</p><formula xml:id="formula_140">H t+1 -H t+1 2 x ≤ (H t -H t ) I -η π 2 uu + π 2 -1 2 + η|g t | I + W -I + I -I + W 2 + 2η E t + ∇L(W t ) + Q t 2 y ≤ I -η π 2 -1 H t -H t 2 + 4.10ηγ|g t | + 124ηγ 2<label>(16)</label></formula><p>where x holds as the diagonal terms cancel out, y uses Lemma F.18, Lemma F.15. Adding ( <ref type="formula" target="#formula_139">15</ref>) and ( <ref type="formula" target="#formula_140">16</ref>), we get</p><formula xml:id="formula_141">H t+1 + H t+1 2 + H t+1 -H t+1 2 ≤ I -η π 2 -1 H t + H t 2 + H t -H t 2 + 2.1η|g t | + 248ηγ 2<label>(17)</label></formula><p>For any T &gt; 0, by applying (17) recursively, we have </p><formula xml:id="formula_142">H T + H T 2 + H T -H T 2 ≤ H 0 + H 0 2 + H 0 -H 0 2 + 2.1η</formula><formula xml:id="formula_143">≤ W 0 2 + W * 2 ≤ 2γ 0 , so H 0 + H 0 2 + H 0 -H 0 2 ≤ 4 H 0 2 ≤ 8γ 0 .</formula><p>By triangle inequality again we get  <ref type="formula" target="#formula_139">15</ref>) and ( <ref type="formula" target="#formula_140">16</ref>), we get</p><formula xml:id="formula_144">H T 2 ≤ H T + H T 2 + H T -H T 2 ≤ H 0 + H 0 2 + H 0 -H 0 2 + 19γ</formula><formula xml:id="formula_145">H t+1 + H t+1 2 + H t+1 -H t+1 2 ≤ I -η π 2 -1 H t+1 + H t+1 2 + H t+1 -H t+1 2 + 2.1η|g t | + 248ηγ 2 x ≤ I -η π 2 -1 H t+1 + H t+1 2 + H t+1 -H t+1 2 + 661ηγ 2<label>(18)</label></formula><p>where x holds as</p><formula xml:id="formula_146">|g t | ≤ 197γ 2 . So either H t+1 + H t+1 2 + H t+1 -H t+1 2 keeps decreasing, or it increases, i.e., η π 2 -1 H t+1 + H t+1 2 + H t+1 -H t+1 2 ≤ 197ηγ 2</formula><p>That gives,</p><formula xml:id="formula_147">H t+1 + H t+1 2 + H t+1 -H t+1 2 ≤ 197γ 2 π 2 -1 ≤ 346γ 2</formula><p>Therefore, combined with the proof of Lemma C.6, we know H t+1 + H t+1 2 + H t+1 -H t+1 2 will keep decreasing until it is at most 346γ 2 . Now,</p><formula xml:id="formula_148">W t 2 ≤ H t 2 + W * 2 ≤ H t+1 + H t+1 2 + H t+1 -H t+1 2 + γ 0 x ≤ (346 + 20)γ 2 ≤ γ</formula><p>where x holds as γ 0 = 1 8000 . So W t 2 is always bounded by γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Proofs for Section D</head><p>For notational simplicity, denote</p><formula xml:id="formula_149">x j e j + w j • e j + w j (w * j -w j ), X (x 1 , • • • , x d )<label>(19)</label></formula><formula xml:id="formula_150">y j I -e j + w j • e j + w j (w * j -w j ), Y (y 1 , • • • , y d )<label>(20)</label></formula><formula xml:id="formula_151">z j I - 1 2 e j + w j • e j + w j (w * j -w j ), Z (z 1 , • • • , z d )</formula><p>We have the following relationship between x j , y j , z j .</p><p>Lemma I.1. The lemma follows.</p><formula xml:id="formula_152">z j 2 2 = 1 4 x j 2 2 + y j 2 2 , x j 2 2 + y j 2 2 = w * j -w j<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 Proof for Lemma D.1</head><p>In this proof, we heavily use the following trick between the summation of four vector products, and the trace of four matrix products. We give one example below, and other cases are similar. By definition, Z (I + W * ) j,i = z j (e i +w * i ), and (I + W * -I + W) I + W i,j = (e i + w * i -e i + w i ) e j + w j , so the lemma follows. Now we proceed to prove Lemma D.1. We first bound d j=1 z j A j e j + w j below by splitting A j into three parts, and then improve the lower bound in Lemma I.4.</p><formula xml:id="formula_153">Lemma I.3. If W 2 , W * 2 ≤ γ ≤ 1 100 , we have d j=1 z j A j e j + w j ≥ -8γ W * -W 2 F - W * -W 2 f - 3 4 X 2 F W * -W 2 F -X 2 F .</formula><p>Proof. We rewrite A j as</p><formula xml:id="formula_154">A j = B j + 1 2 C j + D j<label>(22)</label></formula><p>where </p><formula xml:id="formula_155">B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> </head><p>For notational simplicity, we also write B, C, D as the corresponding terms with sum d i=1 instead of i =j , so they do not depend on index j. We estimate B, C, D first, then estimate B j , C j , D j respectively by taking the differences.</p><p>1. From B to B j : </p><formula xml:id="formula_156">I + W * -I + W F ≤ d i=1 y i 2 2 1 -2γ = Y F √ 1 -2γ<label>(24)</label></formula><p>On the other hand, (w * jw j ) (I -1 2 e j + w j • e j + w j )(e j + w * j )(e j + w * je j + w j ) e j + w j For any vector x, e j + w j • e j + w j x is the projection of x onto the direction e j + w j , so 1 2 ≤ I -1 2 e j + w j • e j + w j 2 ≤ 1, and |(w * jw j ) (e j + w * j )(e j + w * je j + w j ) e j + w j |</p><p>x ≤ |(w * jw j ) (e j + w * j )| w * jw j 2 2</p><p>2(1 -2γ)</p><formula xml:id="formula_157">y ≤ w * j -w j 3 2 (1 + γ) 2(1 -2γ) ≤ w * j -w j 2 2 (1 + γ)γ 1 -2γ<label>(25)</label></formula><p>where x uses Lemma E.1 term 2, and y uses Cauchy-Schwartz. Combining ( <ref type="formula">23</ref>),( <ref type="formula" target="#formula_156">24</ref>),(25), we get d j=1 z j B j e j + w j ≥ - That gives, j z j D j e j + w j ≥ -4γ (1 + γ) 2 Z 2 F Now, combining B j , C j , D j together, using <ref type="bibr" target="#b21">(22)</ref>, we have d j=1 z j A j e j + w j ≥ -</p><formula xml:id="formula_158">(1 + γ) 2 (1 -γ) √ 1 -2γ Z F Y F - (1 + γ)γ 1 -2γ W * -W 2</formula><formula xml:id="formula_159">(1 + γ) 2 (1 -γ) √ 1 -2γ Z F Y F - (1 + γ)γ 1 -2γ W * -W 2 F - 2γ (1 -γ) 2 Z F X F - 4γ (1 + γ) 2 Z 2 F</formula><p>By definition, we know X F ≤ W * -W F , Y F ≤ W * -W F , Z F ≤ W * -W F , and γ ≤ 1 100 , so</p><formula xml:id="formula_160">- (1 + γ)γ 1 -2γ W * -W 2 F - 2γ (1 -γ) 2 Z F X F - 4γ (1 + γ) 2 Z 2 F ≥ -7γ W * -W 2 F<label>(26)</label></formula><p>Moreover,</p><formula xml:id="formula_161">- (1 + γ) 2 (1 -γ) √ 1 -2γ -1 Z F Y F ≥ -0.05γ W * -W 2 F (<label>27</label></formula><formula xml:id="formula_162">)</formula><p>Thus, those are small order terms. The only term left is Z F Y F . By <ref type="bibr" target="#b20">(21)</ref>, we know</p><formula xml:id="formula_163">Z F Y F ≤ W * -W 2 F - 3 4 X 2 F W * -W 2 F -X 2 F (<label>28</label></formula><formula xml:id="formula_164">)</formula><p>Combining ( <ref type="formula" target="#formula_160">26</ref>), ( <ref type="formula" target="#formula_161">27</ref>), <ref type="bibr" target="#b27">(28)</ref>, we get: where x uses Taylor's Theorem for sin θ j * ,j , so we know |α j * ,j | ≤ 1. y uses Lemma E.1 term 3 and Cauchy Schwartz, z uses Lemma E.1 term 1, { holds since γ ≤ 1 100 , and the two small order terms can be bounded by 0.021 W * -W 2 F .</p><formula xml:id="formula_165">d j=1 z j A j e j + w j ≥ -8γ W * -W 2 F - W * -W 2 f - 3 4 X 2 F W * -W 2 F -X 2</formula><p>J Proofs for Section 2 J.1 Proof for Lemma 2.5</p><p>By the updating rule, we have</p><formula xml:id="formula_166">E W t+1 -W * 2 F = E W t -W * -ηG t 2 F = E W t -W * 2 F -2 W t -W * , η∇f (W) + η 2 G t 2 F ≤E W t -W * 2 F -2 W t -W * , η∇f (W) + η 2 t G 2 ≤ (1 -2ηδ)E W t -W * 2 F + η 2 G 2</formula><p>Now if ηδE W t -W * 2 F ≥ η 2 G 2 , we know the E W t -W * 2 F will decrease by a factor of (1ηδ) for every step. Otherwise, although it could increase, we know</p><formula xml:id="formula_167">E W t -W * 2 F ≤ ηG 2 δ</formula><p>By setting η = (1+α) log T δT , we know after T steps, either E W T -W * 2 F is already smaller than</p><formula xml:id="formula_168">ηG 2 δ = (1+α) log T G 2 δ 2 T</formula><p>, or it is decreasing by factor of (1ηδ) for every step, which means</p><formula xml:id="formula_169">E W T -W * 2 F ≤ E W 0 -W * 2 F (1-ηδ) T ≤ D 2 e -ηδT = D 2 e -(1+α) log T = D 2 T -α T ≤ (1 + α) log T G 2 δ 2 T .</formula><p>The last inequality holds since</p><formula xml:id="formula_170">T α log T ≥ D 2 δ 2 (1 + α)G 2</formula><p>Thus, E W T -W * 2 F will be smaller than (1+α) log T G 2 δ 2 T .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Vanilla network (left), with identity mapping (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Phase I: W 1 → W 6 , W may go to the wrong direction but the potential is shrinking. Phase II: W 6 → W 10 , W gets closer to W * in every step by one point convexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1 16η steps. After entering Phase II, based on Theorem 3.3, we simply use Lemma 2.5 by setting δ = 0.03, D = √ d 50 , G = G F to get the convergence guarantee.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Lower bounds of inner product using Taylor expansion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustration of one block in single skip model in Sec 5.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Verifying the global convergence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Verifying the dynamics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Flowchart of the proofs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: For Lemma E.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>π 2 (</head><label>2</label><figDesc>W * -W) I + uu + (W * -W) -2Diag(W * -W) + gI + Wwhere u is the all 1 vector.H.2 Proof for Lemma C.2By Lemma F.4, we know |g| ≤ 2dγ. Using Lemma C.1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>+wj 2 η wj 2 2 2 e 2 e-η 2 w j 2 2 e</head><label>2222</label><figDesc>ej +wj -η wj 2 , we have :g t ≈ d j=1e j + w j 2 e j + w j , η w j + 2η w j , e j + w jη 2 w j2 j + w jη w j 2 = d j=1 η w j , e j + w jη 2 w j 2 j + w jη w j 2 = d j=1 j + w jη w j 2 + d j=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>η 2 w j 2 2 + η 3 w j 3 2</head><label>22</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2 ( 21 )-w j 2 2Ie j + w j • e j + w j , and similarly y j 2 2 = w * j -w j 2 2I 2 I 2 2 = w * j -w j 2 2 2 2</head><label>2212222222</label><figDesc>Proof for Lemma I.1. By definition, w j • e j + w j I -1 2 e j + w j • e j + w j = w * je j + w j • e j + w j + 1 4 e j + w j • e j + w j e j + w j • e j + w j e j + w j • e j + w j Ie j + w j • e j + w j = w * jw j2 e j + w j • e j + w j ,x j e j + w j • e j + w j e j + w j • e j + w j = w * jw j e j + w j • e j + w j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Lemma I. 2 .</head><label>2</label><figDesc>i,j z j (e i +w * i )(e i + w * i -e i + w i ) e j + w j = Tr Z (I + W * ) (I + W * -I + W) I + W . Proof. By definition, Tr(AB) = d j=1 (AB) j,j = i,j A j,i B i,j . Thus, Tr Z (I + W * ) (I + W * -I + W) I + W = i,j Z (I + W * ) j,i (I + W * -I + W) I + W i,j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Be j + w j = i,j z j (e i + w * i )(e i + w * ie i + w i ) e j + w j x =Tr Z (I + W) (I + W * -I + W) I + W y ≥ -(I + W) Z F I + W (I + W * -I + W) F z ≥ -I + W 2 I + W 2 Z F I + W * -I + W F { ≥ -(1 + γ) 2 1γ Z F I + W * -I + W F(23)where x uses Lemma I.2, y uses Tr(AB) ≥ -A F B F , z uses AB F ≤ A 2 B F , and { uses Lemma F.1. By Lemma E.1 term 1, we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>z</head><label></label><figDesc>j (B j -B)e j + w j = d j=1 z j (e j + w * j )(e j + w * je j + w j ) e j + w j = d j=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>F 2 . 2 z≥ 2 d 2 F</head><label>2222</label><figDesc>From C to C j :d j=1 z j Ce j + w j = i,j z j w * iw i , e i + w i e i + w i • e i + w i e j + w j x =Tr( Z X I + W I + W ) = Tr(Z X) + Tr(Z X(I + W I + W -I)) y ≥Tr(Z X) -Z F X F I + W I + W -I Tr(Z X) -4γ (1γ) 2 Z F X Fwhere x uses Lemma I.2 and x j = w * jw j , e j + w j e j + w j , y uses Tr(AB) ≥ -A F B F , and AB F ≤ A 2 B F , and z uses Lemma F.1. On the other hand, d j=1 z j (C -C j )e j + w j = d j=1 z j w * jw j , e j + w j e j + w j • e j + w j e j + w j = d j=1 z j w * jw j , e j + w j e j + w j = Tr(Z X)That implies, 1 j=1 z j C j e j + w j ≥ -2γ (1-γ) 2 Z F X F . 3. From D to D j : d j=1 z j De j + w j = i,j z j z i e i + w i e j + w j = Tr Z Z I + W I + W ≥ (1γ) 2 (1 + γ) 2 Zwhere the last inequality holds by Lemma F.1. On the other hand, z j (D -D j )e j + w j = z j 2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>F 2 FW * -W 2 F -X 2 F* j -w j 2 2 2 F</head><label>22222</label><figDesc>Now it remains to boundW * -W 2 f -3 4 X . jw j )θ j * ,j (e j + w * j ) + e j + w * j sin θ j * ,j e j + w j , w * jjw j )θ j * ,j e j + w * j 2 (e j + w * je j + w j ) + α j * ,j |θ j * ,j | 3 e j + w * j e j + w j 3 e j + w * je j + w j 2 -021 W * -W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Definition 2.2. We define the potential function g</figDesc><table /><note><p>d i=1 ( e i + w * i 2e i + w i 2 ), and variable g j i =j ( e i + w * i 2e i + w i 2 ). Definition 2.3. Denote A j i =j ((e i +w * i )e i + w * i -(e i +w i )e i + w i ), A d i=1 ((e i +w * i</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test error of three 56-layer networks on Cifar-10</figDesc><table><row><cell cols="3">ResNet Single skip Vanilla</cell><cell>input</cell><cell>Convolution</cell><cell>BatchNorm</cell><cell>⊕ Identity</cell><cell>ReLU</cell><cell>output</cell></row><row><cell>Test Err 6.97%</cell><cell>9.01%</cell><cell>12.04%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1-γ 1+γ . The same proof works for I + W, I + W * and I + W * . Lemma* F.2. If W 2 , W * 2 ≤ γ ≤ 1 100 , we have | e i + w * i , e j + w j | ≤ 2.1γ, | e i + w i , e j + w j | ≤ 2.1γ Proof. We know</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>by rearrangement inequality.</figDesc><table /><note><p>Lemma* F.7. If W 2 , W * 2 ≤ γ ≤ 1 100 , for k ≥ 3, we have e i + w * i , e j + w j k (e i + w * i )e i + w i , e j + w j k (e i + w i ) 2 ≤6(2.2γ) k-3 e i + w * i , e j + w j 2 + e i + w i , e j + w j 2 w * iw i 2 Proof. e i + w * i , e j + w j k (e i + w * i )e i + w i , e j + w j k (e i + w i ) 2 ≤ w * iw i 2 | e i + w * i , e j + w j k | + ( e i + w * i , e j + w j ke i + w i , e j + w j k</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>where x uses Lemma F.2 and Lemma F.6, y uses Lemma F.5, z holds as γ ≤ 1 100 , and { holds since 0.55k(2.1) k-3 ≤ 6(2.2) k-3 for k ≥ 3.Lemma* F.8. If W 2 , W * 2 ≤ γ ≤ 1 100 , for k ≥ 2, e i + w i 2 e i + w i , e j + w j 2ke i +w * i 2 e i + w * i , e j + w j i + w i 2 e i + w i , e j + w j 2ke i + w * i 2 e i + w * i , e j + w j 2k ≤ e i + w i 2 e i + w i , e j + w j 2ke i + w * i , e j + w j 2k + | e i + w i 2e i + w * i 2 | e i + w * i , e j + w j 2k x ≤ e i + w i 2 e i + w i , e j + w j 2ke i + w * i , e j + w j 2k + w * iw i 2 (2.1γ) 2k-2 e i + w * i , e j + w j + γ)k| e i + w ie i + w * i , e j + w j | | e i + w i , e j + w j</figDesc><table><row><cell>Proof. y ≤(1</cell><cell>≤8(2.2γ) 2k-3 e i + w i , e j + w j</cell><cell>w  *  i , e j + w j 2 + e i + w  *  i , e j + w j 2 + 0.52k(2.1γ) k-3 e 2k 2 w  *  i -w i 2</cell><cell>2</cell></row></table><note><p>i + w i , e j + w j 2 z ≤ w * iw i 2 0.55k(2.1γ) k-3 e i + w * i , e j + w j 2 + 0.52k(2.1γ) k-3 e i + w i , e j + w j 2 { ≤6(2.2γ) k-3 e i + w * i , e j + w j 2 + e i + w i , e j + w j 2 w * iw i 2 e</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>j (e i + w i ) + ( e i + w * i sin θ i * ,je i + w i sin θ i,j ) e j + w j ≈ i =j e i + w * i , e j + w j (e i + w * i )e i + w i , e j + w j (e i + w i )</figDesc><table><row><cell>+</cell><cell>i =j</cell><cell>e i + w  *  i</cell><cell>1 -</cell><cell>1 2</cell><cell>e i + w  *  i , e j + w j</cell><cell>2 -e i + w i 1 -</cell><cell>1 2</cell><cell>e i + w i , e j + w j</cell><cell>2</cell><cell>e j + w j</cell></row><row><cell>=</cell><cell cols="3">((e i + w  *  i )e i + w  *</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">i =j</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p>i</p>-(e i + w i )e i + w i )e j + w j</p>+ i =j e i + w * ie i + w i -</p>1</p>2 e j + w j e i + w * i e i + w * i e i + w * i e j + w j + 1 2 e j + w j e i + w i e i + w i e i + w i e j + w j e j + w j =A j e j + w j +   i =j</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>6 e i + w * i , e j + w j 2k+1 (e i + w * i )e i + w i , e j + w j 2k+1 (e i + w i ), w * jw j 2k-2 e i + w * i , e j + w j 2 + e i + w i , e j + w j</figDesc><table><row><cell>y ≤</cell><cell cols="3">d j=1 i =j</cell><cell>∞ k=1</cell><cell>1 6</cell><cell cols="2">e i + w  *  i , e j + w j</cell><cell>2k+1 (e i + w  *  i ) -e i + w i , e j + w j</cell><cell>2k+1 (e i + w i ) 2 w  *  j -w j 2</cell></row><row><cell>z ≤</cell><cell cols="3">d j=1 i =j</cell><cell cols="4">∞ k=1 (2.2γ) 2</cell><cell>w  *  i -w i 2 w  *  j -w j 2</cell></row><row><cell>{ ≤</cell><cell cols="3">d j=1 i =j</cell><cell cols="3">1.01 e i + w  *  i , e j + w j</cell><cell>2 + e i + w i , e j + w j</cell><cell>2</cell><cell>w  *  i -w i 2 w  *  j -w j 2</cell></row><row><cell cols="2">| ≤1.01</cell><cell> </cell><cell cols="3">d j=1 i =j</cell><cell></cell></row></table><note><p><p>e i + w *</p>i , e j + w j 2 + e i + w i , e j + w j 2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>where x uses Cauchy Schwartz and Lemma F.15, y uses the definition of Q and Lemma F.1, z holds as for any vector u, v, Tr(uv ) ≤ u 2 v 2 , { uses Lemma F.15. According to Lemma C.1, with approximation loss of 61γ 2 , we can use -Q t to approximate ∇L(W t ).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2.1dγ|g t | +</cell><cell>2.05π 2</cell><cell>s 2 γ</cell><cell>√</cell><cell>d</cell></row><row><cell>Hence,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">| g t -η ∇L(W t ), I | ≤1.04ηdγ 2 + 1.03η √ dε + |η ∇L(W t ), I + W t -I | &lt;1.04ηdγ 2 + 1.03η √ dε + 20ηdγ 2 + 2.1ηdγ|g t | + 2.05π 2 &lt;21.1ηdγ 2 + 1.03η √ dε + 2.1ηdγ|g t | + 2.05π 2 η s 2 γ √ d η s 2 γ</cell><cell>√</cell><cell>d</cell></row><row><cell cols="3">So with approximation loss of 21.1ηdγ 2 + 1.03η ηTr(∇L(W t )).</cell><cell>√</cell><cell>dε + 2.1ηdγ|g t | + 2.05π 2 η s 2 γ</cell><cell>√</cell><cell>d, it suffices to consider</cell></row><row><cell cols="3">H.3.3 ∆g t ≈ -η(d + π 2 -1)g t</cell><cell></cell><cell></cell></row><row><cell>Tr(Q t ) =</cell><cell>π 2</cell><cell>Tr (W</cell><cell></cell><cell></cell></row></table><note><p>* -W t ) I + uu + Tr(W * -W t ) -2Tr(Diag(W * -W t )) + gTr(I</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Tr (W * -W t ) uu + g t (Tr(I + W t -I))</figDesc><table><row><cell cols="6">Now we have |g t+1 | = |g t + g t | ≤ 1 -η d + ≤(1 -0.95ηd)|g t | + 86ηdγ 2 + 1.03η</cell><cell cols="3">π 2 √ dε + 4.8η s t 2 γ -1 -4.15dγ |g t | + 86ηdγ 2 + 1.03η √ d</cell><cell>√</cell><cell>dε + 4.8η s t 2 γ</cell><cell>√</cell><cell>d</cell></row><row><cell cols="4">H.4 Proof for Lemma C.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">By definition of s t ,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s t s t+1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x ≤6.07</cell><cell>π 2</cell><cell>-1 dγ 2 +</cell><cell>π 2</cell><cell>s t 2</cell><cell cols="2">√</cell><cell cols="2">d + 2.05|g t |dγ</cell></row><row><cell cols="8">where x uses Lemma F.14 and Lemma F.15. Thus,</cell></row><row><cell cols="8">g t --η d + ≤η 21.1dγ 2 + 1.03 √ π 2 dε + 2.1dγ|g t | + -1 g t ≤η 86dγ 2 + 1.03 √ dε + 4.15dγ|g t | + 4.8 s t 2 γ 2.05π 2 s 2 γ √ d √</cell><cell>d + 61dγ 2 + 2.05|g t |dγ + 6.07</cell><cell>π 2</cell><cell>-1 dγ 2 +</cell><cell>π 2</cell><cell>s t 2</cell><cell>√</cell><cell>d</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>So if the following inequality holds, |g t | + s t 2 will always decrease by factor at least 1 -0.5ηd. Hence, |g t | + s t 2 will keep decreasing by 1 -0.5ηd as long as it is larger than 4.5γ. So we have s t 2 ≤ 4.5γ. Now plug it back to the updating rule of |g t |: The last inequality uses d ≥ 100, ε ≤ γ 2 . So even after |g t | + s t 2 is below 4.5γ, |g t | will keep decreasing by factor 1 -0.5ηd until it is smaller than 197γ 2 . Finally we bound the number of steps to arrive 197γ 2 . Let γ = 1 400 , γ 0 = 1 8000 . Again, the constants here are pretty loose. Since |g t | ≤ (1 -0.5ηd) t |g 0 | ≤ (1 -0.5ηd) t 2dγ 0 , in order to let g t ≤ 197γ 2 , it suffices to have t ≥</figDesc><table><row><cell cols="5">In order to get factor 1 -0.5ηd, we have 0.45ηd|g t | ≥ 86ηdγ 2 + 1.03η</cell><cell>√</cell><cell cols="2">dε + 21.6ηγ 2 √</cell><cell>d</cell></row><row><cell cols="2">Solve this inequality, we get</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">86ηdγ 2 + 1.03η 0.45ηd √ dε + 21.6ηγ 2 √</cell><cell>d</cell><cell>=</cell><cell cols="3">86γ 2 0.45</cell><cell>+</cell><cell>1.03ε + 21.6γ 2 0.45 √ d</cell><cell>≤ 197γ 2</cell></row><row><cell cols="8">log 197γ 2 2dγ 0 log(1-ηd 2 ) . Since ηd is small, by Taylor expansion we know log(1 -ηd 2 ) ≈ -ηd 2 . Thus, it suffices to let t ≥ 2 log(0.203d) ηd . Notice that log(0.203d) d is decreasing for d ≥ 100, we know it suffices to let t ≥ 1 16η .</cell></row><row><cell cols="2">H.6 Proof for Lemma C.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>By Lemma C.1,</cell><cell>1 and Lemma F.10. s t -[-η (d+1)π</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">H.5 Proof for Lemma C.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Combining Lemma C.3 and Lemma C.4, we get</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">|g t+1 | + s t+1 2 ≤(1 -0.95ηd)(|g t | + s t 2 ) + η(6.6γ + 1.03|g t | + ε) x ≤(1 -0.95ηd)(|g t | + s t 2 ) + 6.6ηγ √ d + 86ηdγ 2 + η1.03|g t | √ d + 86ηdγ 2 + 1.03η √ d + 2.03η √ √ dε + (4.8ηγ dε</cell><cell>√</cell><cell>d -0.62ηd) s t 2</cell></row><row><cell>Which gives</cell><cell cols="6">0.34ηd(|g t | + s t 2 ) ≥ 6.6ηγ |g t | + s t 2 ≥ 6.6ηγ √ d + 87ηdγ 2 0.34ηd</cell><cell>√ = d + 87ηdγ 2 6.6γ 0.34 √ d +</cell><cell>87γ 2 0.34</cell></row><row><cell cols="8">where the last expression is smaller than 4.5γ. |g t+1 | ≤(1 -0.95ηd)|g t | + 86ηdγ 2 + 1.03η ≤(1 -0.95ηd)|g t | + 86ηdγ 2 + 1.03η</cell><cell>√ √</cell><cell>dε + 4.8η s t 2 γ dε + 21.6ηγ 2 √ d √</cell><cell>d</cell></row></table><note><p><p><p>y ≤(1 -0.84ηd)(|g t | + s t 2 ) + 6.6ηγ √ d + 87ηdγ 2</p>where x uses γ ≤ 1 100 , d ≥ 100, y uses ε ≤ γ 2 and d ≥ 100.</p>Let H = W -W * , by the updating rule of W t and the definition of Q t , we know</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>By Lemma F.4 we know |g 0 | ≤ 2dγ 0 , so 2.1η . By the proof of Lemma C.5, we know T ≤ 1 16η , so 248ηT γ 2 ≤ 15.5γ 2 . By triangle inequality, we know H 0 2</figDesc><table><row><cell>T -1 t=0 |g t | ≤ 2.1η|g0|(1-(1-0.5ηd) T ) (0.5ηd)</cell><cell>≤ 4.2|g0| d</cell><cell>≤ 8.4γ 0</cell></row></table><note><p><p>T -1</p>t=0 |g t | + 248ηT γ 2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>2 + 8.4γ 0 ≤ 16.4γ 0 + 15.5γ 2 Recall we set γ = 1 400 , γ 0 = 1 8000 in the proof of Lemma C.5, we know W T 2 ≤ W * 2 + H T 2 ≤ 17.4γ 0 + 15.5γ 2 ≤ 1 440 ≤ γ. First, by the proof of Lemma C.5, we know |g t | will keep small if W t 2 ≤ γ ≤ 1 100 . Adding (</figDesc><table><row><cell>H.7 Proof for Lemma C.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>They assume input is Gaussian and the W * is orthonormal, which means the activations are independent in teacher network.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Let σ i be the output of i-th ReLU unit, then in our setting, i,j Cov[σ i , σ j ] can be as large as Ω(d), which is far from being independent.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>To make comparison meaningful, we set W -I to be the actual weight for Vanilla as its identity mapping is missing, which is why it has a much bigger initial norm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>s t ] 2 &lt; η(6γ + 1.03|g t |) √ d + η (Q t + ∇L(W t ) + E t )u 2 ≤ η(6.61γ + 1.03|g t | + ε) √ d.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors want to thank Robert Kleinberg, Kilian Weinberger, Gao Huang, Adam Klivans and Surbhi Goel for helpful discussions, and the anonymous reviewers for their comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lemma I. <ref type="bibr" target="#b3">4</ref>.</p><p>Indeed, we know</p><p>When x = 0, f (x) = y &gt; 0, and when x → y, f (x) &lt; 0. We want to find the place where f (x) = 0, which gives the maximum value. Assume x = λy, this is equivalent to solve</p><p>Cancel all y, and we get the solution x ≈ 0.566y, where f (x) ≈ 1.2845y 2 &lt; 1.3y 2 .</p><p>Proof of Lemma D.1. Combining Lemma I.3 and Lemma I.4, we have proved Lemma D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Proof for Lemma D.2</head><p>Again, we first consider the full sum, g =</p><p>Thus by Cauchy Schwartz, |(gg j ) w * jw j , e j + w j | ≤ w * jw j 2 x j 2 Summing over j, we get</p><p>where the last inequality is by Cauchy Schwartz. Now</p><p>w * jw j , e j + w j = g d j=1 e j + w * je j + w j , e j + w j =g d j=1 ( e j + w * j 2e j + w j 2 + e j + w * j , e j + w je j + w * j ) = g 2 + gb ≥ gb <ref type="bibr" target="#b29">(30)</ref> where b is defined to be d j=1 e j + w * j , e j + w je j + w * j . By Lemma E.1 term 2 we know</p><p>Combining (29), <ref type="bibr" target="#b29">(30)</ref>, the lemma follows. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning polynomials with neural networks</title>
		<author>
			<persName><forename type="first">Alexandr</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rina</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1908" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Provable bounds for learning some deep representations</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014. 2014</date>
			<biblScope unit="page" from="584" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="930" to="945" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hinging hyperplanes for regression, classification, and function approximation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="999" to="1013" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open problem: The landscape of the loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arous</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th Conference on Learning Theory, COLT 2015</title>
		<meeting>The 28th Conference on Learning Theory, COLT 2015<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">July 3-6, 2015. 2015</date>
			<biblScope unit="page" from="1756" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MCSS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">455</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2253" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2933" to="2941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Escaping from saddle points -online stochastic gradient for tensor decomposition</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="797" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reliably learning the relu in polynomial time</title>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Klivans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Thaler</surname></persName>
		</author>
		<idno>CoRR, abs/1611.10258</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Eigenvalue decay implies polynomial-time learnability for neural networks</title>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Klivans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning Depth-Three Neural Networks in Polynomial Time</title>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Klivans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Qualitatively characterizing neural network optimization problems</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6544</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Identity matters in deep learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno>CoRR, abs/1611.04231</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><forename type="middle">B</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods</title>
		<author>
			<persName><forename type="first">Hanie</forename><surname>Majid Janzamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08473</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Risk Bounds for High-dimensional Ridge Function Combinations Including Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Klusowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Efficient BackProp</title>
		<imprint>
			<biblScope unit="page" from="9" to="50" />
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the computational efficiency of training neural networks</title>
		<author>
			<persName><forename type="first">Roi</forename><surname>Livni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="855" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName><forename type="first">Guido</forename><forename type="middle">F</forename><surname>Montúfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2924" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Expressiveness of rectifier networks</title>
		<author>
			<persName><forename type="first">Xingyuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2427" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On the number of inference regions of deep feed forward networks with piece-wise linear activations</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Montúfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6098</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Non-asymptotic theory of random matrices: extreme singular values</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rudelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vershynin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamics of on-line gradient descent learning for multilayer neural networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="302" to="308" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the quality of the initial basin in overspecified neural networks</title>
		<author>
			<persName><forename type="first">Itay</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="774" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6120</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Provable methods for training neural networks with sparse connectivity</title>
		<author>
			<persName><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Distribution-specific hardness of learning neural networks</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<idno>CoRR, abs/1609.01037</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training a single sigmoidal neuron is hard</title>
		<author>
			<persName><forename type="first">Jirí</forename><surname>Síma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2709" to="2728" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Symmetry-breaking convergence analysis of certain two-layered neural networks with relu nonlinearity</title>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Submitted to ICLR 2017</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Diversity leads to generalization in neural networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning halfspaces and neural networks with random initialization</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>CoRR, abs/1511.07948</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Recovery guarantees for one-hidden-layer neural networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
