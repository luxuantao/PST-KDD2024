<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University and Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University and Cornell Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing objects at vastly different scales is a fundamental challenge in computer vision. Feature pyramids built upon image pyramids (for short we call these featurized image pyramids) form the basis of a standard solution <ref type="bibr" target="#b0">[1]</ref> (Fig. <ref type="figure" target="#fig_0">1(a)</ref>). These pyramids are scale-invariant in the sense that an object's scale change is offset by shifting its level in the pyramid. Intuitively, this property enables a model to detect objects across a large range of scales by scanning the model over both positions and pyramid levels.</p><p>Featurized image pyramids were heavily used in the era of hand-engineered features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. They were so critical that object detectors like DPM <ref type="bibr" target="#b6">[7]</ref> required dense scale sampling to achieve good results (e.g., 10 scales per octave). For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (ConvNets) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Aside from being capable of representing higher-level semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref> (Fig. <ref type="figure" target="#fig_0">1(b)</ref>). But even with this robustness, pyramids are still needed to get the most accurate results. All recent top entries in the ImageNet <ref type="bibr" target="#b32">[33]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref> detection challenges use multi-scale testing on featurized image pyramids (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref>). The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.</p><p>Nevertheless, featurizing each level of an image pyramid has obvious limitations. Inference time increases considerably (e.g., by four times <ref type="bibr" target="#b10">[11]</ref>), making this approach impractical for real applications. Moreover, training deep networks end-to-end on an image pyramid is infeasible in terms of memory, and so, if exploited, image pyramids are used only at test time <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>, which creates an inconsistency between train/test-time inference. For these reasons, Fast and Faster R-CNN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref> opt to not use featurized image pyramids under default settings.</p><p>However, image pyramids are not the only way to compute a multi-scale feature representation. A deep ConvNet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multiscale, pyramidal shape. This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition.</p><p>The Single Shot Detector (SSD) <ref type="bibr" target="#b21">[22]</ref> is one of the first attempts at using a ConvNet's pyramidal feature hierarchy as if it were a featurized image pyramid (Fig. <ref type="figure" target="#fig_0">1(c)</ref>). Ideally, the SSD-style pyramid would reuse the multi-scale feature maps from different layers computed in the forward pass and thus come free of cost. But to avoid using low-level features SSD foregoes reusing already computed layers and instead builds the pyramid starting from high up in the network (e.g., conv4 3 of VGG nets <ref type="bibr" target="#b35">[36]</ref>) and then by adding several new layers. Thus it misses the opportunity to reuse the higher-resolution maps of the feature hierarchy. We show that these are important for detecting small objects.</p><p>The goal of this paper is to naturally leverage the pyramidal shape of a ConvNet's feature hierarchy while creating a feature pyramid that has strong semantics at all scales. To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections (Fig. <ref type="figure" target="#fig_0">1(d)</ref>). The result is a feature pyramid that has rich semantics at all levels and is built quickly from a single input image scale. In other words, we show how to create in-network feature pyramids that can be used to replace featurized image pyramids without sacrificing representational power, speed, or memory.</p><p>Similar architectures adopting top-down and skip connections are popular in recent research <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref>. Their goals are to produce a single high-level feature map of a fine resolution on which the predictions are to be made (Fig. <ref type="figure" target="#fig_1">2</ref> top). On the contrary, our method leverages the architecture as a feature pyramid where predictions (e.g., object detections) are independently made on each level (Fig. <ref type="figure" target="#fig_1">2</ref> bottom). Our model echoes a featurized image pyramid, which has not been explored in these works.</p><p>We evaluate our method, called a Feature Pyramid Network (FPN), in various systems for detection and segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27]</ref>. Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark <ref type="bibr" target="#b20">[21]</ref> simply based on FPN and where predictions are made on the finest level (e.g., <ref type="bibr" target="#b27">[28]</ref>). Bottom: our model that has a similar structure but leverages it as a feature pyramid, with predictions made independently at all levels.</p><p>a basic Faster R-CNN detector <ref type="bibr" target="#b28">[29]</ref>, surpassing all existing heavily-engineered single-model entries of competition winners. In ablation experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets <ref type="bibr" target="#b15">[16]</ref>. Our method is also easily extended to mask proposals and improves both instance segmentation AR and speed over state-of-the-art methods that heavily depend on image pyramids.</p><p>In addition, our pyramid structure can be trained end-toend with all scales and is used consistently at train/test time, which would be memory-infeasible using image pyramids. As a result, FPNs are able to achieve higher accuracy than all existing state-of-the-art methods. Moreover, this improvement is achieved without increasing testing time over the single-scale baseline. We believe these advances will facilitate future research and applications. Our code will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Hand-engineered features and early neural networks. SIFT features <ref type="bibr" target="#b24">[25]</ref> were originally extracted at scale-space extrema and used for feature point matching. HOG features <ref type="bibr" target="#b4">[5]</ref>, and later SIFT features as well, were computed densely over entire image pyramids. These HOG and SIFT pyramids have been used in numerous works for image classification, object detection, human pose estimation, and more. There has also been significant interest in computing featurized image pyramids quickly. Doll√°r et al. <ref type="bibr" target="#b5">[6]</ref> demonstrated fast pyramid computation by first computing a sparsely sampled (in scale) pyramid and then interpolating missing levels. Before HOG and SIFT, early work on face detection with ConvNets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b31">32]</ref> computed shallow networks over image pyramids to detect faces across scales.</p><p>Deep ConvNet object detectors. With the development of modern deep ConvNets <ref type="bibr" target="#b18">[19]</ref>, object detectors like Over-Feat <ref type="bibr" target="#b33">[34]</ref> and R-CNN <ref type="bibr" target="#b11">[12]</ref> showed dramatic improvements in accuracy. OverFeat adopted a strategy similar to early neural network face detectors by applying a ConvNet as a sliding window detector on an image pyramid. R-CNN adopted a region proposal-based strategy <ref type="bibr" target="#b36">[37]</ref> in which each proposal was scale-normalized before classifying with a ConvNet. SPPnet <ref type="bibr" target="#b14">[15]</ref> demonstrated that such region-based detectors could be applied much more efficiently on feature maps extracted on a single image scale. Recent and more accurate detection methods like Fast R-CNN <ref type="bibr" target="#b10">[11]</ref> and Faster R-CNN <ref type="bibr" target="#b28">[29]</ref> advocate using features computed from a single scale, because it offers a good trade-off between accuracy and speed. Multi-scale detection, however, still performs better, especially for small objects.</p><p>Methods using multiple layers. A number of recent approaches improve detection and segmentation by using different layers in a ConvNet. FCN <ref type="bibr" target="#b23">[24]</ref> sums partial scores for each category over multiple scales to compute semantic segmentations. Hypercolumns <ref type="bibr" target="#b12">[13]</ref> uses a similar method for object instance segmentation. Several other approaches (HyperNet <ref type="bibr" target="#b17">[18]</ref>, ParseNet <ref type="bibr" target="#b22">[23]</ref>, and ION <ref type="bibr" target="#b1">[2]</ref>) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features. SSD <ref type="bibr" target="#b21">[22]</ref> and MS-CNN <ref type="bibr" target="#b2">[3]</ref> predict objects at multiple layers of the feature hierarchy without combining features or scores.</p><p>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net <ref type="bibr" target="#b30">[31]</ref> and Sharp-Mask <ref type="bibr" target="#b27">[28]</ref> for segmentation, Recombinator networks <ref type="bibr" target="#b16">[17]</ref> for face detection, and Stacked Hourglass networks <ref type="bibr" target="#b25">[26]</ref> for keypoint estimation. Ghiasi et al. <ref type="bibr" target="#b7">[8]</ref> present a Laplacian pyramid presentation for FCNs to progressively refine segmentation. Although these methods adopt architectures with pyramidal shapes, they are unlike featurized image pyramids <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34]</ref> where predictions are made independently at all levels, see Fig. <ref type="figure" target="#fig_1">2</ref>. In fact, for the pyramidal architecture in Fig. <ref type="figure" target="#fig_1">2</ref> (top), image pyramids are still needed to recognize objects across multiple scales <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Pyramid Networks</head><p>Our goal is to leverage a ConvNet's pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout. The resulting Feature Pyramid Network is generalpurpose and in this paper we focus on sliding window proposers (Region Proposal Network, RPN for short) <ref type="bibr" target="#b28">[29]</ref> and region-based detectors (Fast R-CNN) <ref type="bibr" target="#b10">[11]</ref>. We also generalize FPNs to instance segmentation proposals in Sec. 6.</p><p>Our method takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref>), and in this paper we present results using ResNets <ref type="bibr" target="#b15">[16]</ref>. The construction of our pyramid involves a bottom-up pathway, a top-down pathway, and lateral connections, as introduced in the following.</p><p>Bottom-up pathway. The bottom-up pathway is the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. There are often many layers producing output maps of the same size and we say these layers are in the same network stage. For our feature pyramid, we define one pyramid level for each stage. We choose the output of the last layer of each stage as our reference set of feature maps, which we will enrich to create our pyramid. This choice is natural since the deepest layer of each stage should have the strongest features.</p><p>Specifically, for ResNets <ref type="bibr" target="#b15">[16]</ref> we use the feature activations output by each stage's last residual block. We denote the output of these last residual blocks as {C 2 , C 3 , C 4 , C 5 } for conv2, conv3, conv4, and conv5 outputs, and note that they have strides of {4, 8, 16, 32} pixels with respect to the input image. We do not include conv1 into the pyramid due to its large memory footprint.</p><p>Top-down pathway and lateral connections. The topdown pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows the building block that constructs our topdown feature maps. With a coarser-resolution feature map, we upsample the spatial resolution by a factor of 2 (using nearest neighbor upsampling for simplicity). The upsam-pled map is then merged with the corresponding bottom-up map (which undergoes a 1√ó1 convolutional layer to reduce channel dimensions) by element-wise addition. This process is iterated until the finest resolution map is generated. To start the iteration, we simply attach a 1√ó1 convolutional layer on C 5 to produce the coarsest resolution map. Finally, we append a 3√ó3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling. This final set of feature maps is called {P 2 , P 3 , P 4 , P 5 }, corresponding to {C 2 , C 3 , C 4 , C 5 } that are respectively of the same spatial sizes.</p><p>Because all levels of the pyramid use shared classifiers/regressors as in a traditional featurized image pyramid, we fix the feature dimension (numbers of channels, denoted as d) in all the feature maps. We set d = 256 in this paper and thus all extra convolutional layers have 256-channel outputs. There are no non-linearities in these extra layers, which we have empirically found to have minor impacts.</p><p>Simplicity is central to our design and we have found that our model is robust to many design choices. We have experimented with more sophisticated blocks (e.g., using multilayer residual blocks <ref type="bibr" target="#b15">[16]</ref> as the connections) and observed marginally better results. Designing better connection modules is not the focus of this paper, so we opt for the simple design described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>Our method is a generic solution for building feature pyramids inside deep ConvNets. In the following we adopt our method in RPN <ref type="bibr" target="#b28">[29]</ref> for bounding box proposal generation and in Fast R-CNN <ref type="bibr" target="#b10">[11]</ref> for object detection. To demonstrate the simplicity and effectiveness of our method, we make minimal modifications to the original systems of <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b10">11]</ref> when adapting them to our feature pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Pyramid Networks for RPN</head><p>RPN <ref type="bibr" target="#b28">[29]</ref> is a sliding-window class-agnostic object detector. In the original RPN design, a small subnetwork is evaluated on dense 3√ó3 sliding windows, on top of a singlescale convolutional feature map, performing object/nonobject binary classification and bounding box regression. This is realized by a 3√ó3 convolutional layer followed by two sibling 1√ó1 convolutions for classification and regression, which we refer to as a network head. The object/nonobject criterion and bounding box regression target are defined with respect to a set of reference boxes called anchors <ref type="bibr" target="#b28">[29]</ref>. The anchors are of multiple pre-defined scales and aspect ratios in order to cover objects of different shapes.</p><p>We adapt RPN by replacing the single-scale feature map with our FPN. We attach a head of the same design (3√ó3 conv and two sibling 1√ó1 convs) to each level on our feature pyramid. Because the head slides densely over all locations in all pyramid levels, it is not necessary to have multi-scale anchors on a specific level. Instead, we assign anchors of a single scale to each level. Formally, we define the anchors to have areas of {32<ref type="foot" target="#foot_1">2</ref> , 64 2 , 128 2 , 256 2 , 512 2 } pixels on {P 2 , P 3 , P 4 , P 5 , P 6 } respectively. <ref type="foot" target="#foot_0">1</ref> As in <ref type="bibr" target="#b28">[29]</ref> we also use anchors of multiple aspect ratios {1:2, 1:1, 2:1} at each level. So in total there are 15 anchors over the pyramid.</p><p>We assign training labels to the anchors based on their Intersection-over-Union (IoU) ratios with ground-truth bounding boxes as in <ref type="bibr" target="#b28">[29]</ref>. Formally, an anchor is assigned a positive label if it has the highest IoU for a given groundtruth box or an IoU over 0.7 with any ground-truth box, and a negative label if it has IoU lower than 0.3 for all ground-truth boxes. Note that scales of ground-truth boxes are not explicitly used to assign them to the levels of the pyramid; instead, ground-truth boxes are associated with anchors, which have been assigned to pyramid levels. As such, we introduce no extra rules in addition to those in <ref type="bibr" target="#b28">[29]</ref>.</p><p>We note that the parameters of the heads are shared across all feature pyramid levels; we have also evaluated the alternative without sharing parameters and observed similar accuracy. The good performance of sharing parameters indicates that all levels of our pyramid share similar semantic levels. This advantage is analogous to that of using a featurized image pyramid, where a common head classifier can be applied to features computed at any image scale.</p><p>With the above adaptations, RPN can be naturally trained and tested with our FPN, in the same fashion as in <ref type="bibr" target="#b28">[29]</ref>. We elaborate on the implementation details in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature Pyramid Networks for Fast R-CNN</head><p>Fast R-CNN <ref type="bibr" target="#b10">[11]</ref> is a region-based object detector in which Region-of-Interest (RoI) pooling is used to extract features. Fast R-CNN is most commonly performed on a single-scale feature map. To use it with our FPN, we need to assign RoIs of different scales to the pyramid levels.</p><p>We view our feature pyramid as if it were produced from an image pyramid. Thus we can adapt the assignment strategy of region-based detectors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref> in the case when they are run on image pyramids. Formally, we assign an RoI of width w and height h (on the input image to the network) to the level P k of our feature pyramid by:</p><formula xml:id="formula_0">k = ‚åäk 0 + log 2 ( ‚àö wh/224)‚åã. (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Here 224 is the canonical ImageNet pre-training size, and k 0 is the target level on which an RoI with w √ó h = 224 2 should be mapped into. Analogous to the ResNet-based Faster R-CNN system <ref type="bibr" target="#b15">[16]</ref> that uses C 4 as the single-scale feature map, we set k 0 to 4. Intuitively, Eqn. (1) means that if the RoI's scale becomes smaller (say, 1/2 of 224), it should be mapped into a finer-resolution level (say, k = 3).</p><p>We attach predictor heads (in Fast R-CNN the heads are class-specific classifiers and bounding box regressors) to all RoIs of all levels. Again, the heads all share parameters, regardless of their levels. In <ref type="bibr" target="#b15">[16]</ref>, a ResNet's conv5 layers (a 9-layer deep subnetwork) are adopted as the head on top of the conv4 features, but our method has already harnessed conv5 to construct the feature pyramid. So unlike <ref type="bibr" target="#b15">[16]</ref>, we simply adopt RoI pooling to extract 7√ó7 features, and attach two hidden 1,024-d fully-connected (fc) layers (each followed by ReLU) before the final classification and bounding box regression layers. These layers are randomly initialized, as there are no pre-trained fc layers available in ResNets. Note that compared to the standard conv5 head, our 2-fc MLP head is lighter weight and faster.</p><p>Based on these adaptations, we can train and test Fast R-CNN on top of the feature pyramid. Implementation details are given in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on Object Detection</head><p>We perform experiments on the 80 category COCO detection dataset <ref type="bibr" target="#b20">[21]</ref>. We train using the union of 80k train images and a 35k subset of val images (trainval35k <ref type="bibr" target="#b1">[2]</ref>), and report ablations on a 5k subset of val images (minival). We also report final results on the standard test set (test-std) <ref type="bibr" target="#b20">[21]</ref> which has no disclosed labels.</p><p>As is common practice <ref type="bibr" target="#b11">[12]</ref>, all network backbones are pre-trained on the ImageNet1k classification set <ref type="bibr" target="#b32">[33]</ref> and then fine-tuned on the detection dataset. We use the pre-trained ResNet-50 and ResNet-101 models that are publicly available. <ref type="foot" target="#foot_2">2</ref> Our code is a reimplementation of py-faster-rcnn<ref type="foot" target="#foot_3">3</ref> using Caffe2. <ref type="foot" target="#foot_4">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Region Proposal with RPN</head><p>We evaluate the COCO-style Average Recall (AR) and AR on small, medium, and large objects (AR s , AR m , and AR l ) following the definitions in <ref type="bibr" target="#b20">[21]</ref>. We report results for 100 and 1000 proposals per images (AR 100 and AR 1k ). <ref type="table" target="#tab_0">1</ref> are trained end-to-end. The input image is resized such that its shorter side has 800 pixels. We adopt synchronized SGD training on 8 GPUs. A mini-batch involves 2 images per GPU and 256 anchors per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the first 30k mini-batches and 0.002 for the next 10k. For all RPN experiments (including baselines), we include the anchor boxes that are outside the image for training, which is unlike <ref type="bibr" target="#b28">[29]</ref> where these anchor boxes are ignored. Other implementation details are as in <ref type="bibr" target="#b28">[29]</ref>. Training RPN with FPN on 8 GPUs takes about 8 hours on COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details. All architectures in Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Ablation Experiments</head><p>Comparisons with baselines. For fair comparisons with original RPNs <ref type="bibr" target="#b28">[29]</ref>, we run two baselines (Table <ref type="table" target="#tab_0">1</ref>(a, b)) using the single-scale map of C 4 (the same as <ref type="bibr" target="#b15">[16]</ref>) or C 5 , both using the same hyper-parameters as ours, including using 5 scale anchors of {32 2 , 64 2 , 128 2 , 256 2 , 512 2 }. Table <ref type="table" target="#tab_0">1</ref> (b) shows no advantage over (a), indicating that a single higherlevel feature map is not enough because there is a trade-off between coarser resolutions and stronger semantics.</p><p>Placing FPN in RPN improves AR 1k to 56.3 (Table 1 (c)), which is 8.0 points increase over the single-scale RPN baseline (Table <ref type="table" target="#tab_0">1</ref> (a)). In addition, the performance on small objects (AR 1k s ) is boosted by a large margin of 12.9 points. Our pyramid representation greatly improves RPN's robustness to object scale variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How important is top-down enrichment? Table 1(d)</head><p>shows the results of our feature pyramid without the topdown pathway. With this modification, the 1√ó1 lateral connections followed by 3√ó3 convolutions are attached to the bottom-up pyramid. This architecture simulates the effect of reusing the pyramidal feature hierarchy (Fig. <ref type="figure" target="#fig_0">1(b)</ref>).</p><p>The results in Table <ref type="table" target="#tab_0">1</ref>(d) are just on par with the RPN baseline and lag far behind ours. We conjecture that this is because there are large semantic gaps between different levels on the bottom-up pyramid (Fig. <ref type="figure" target="#fig_0">1(b</ref>)), especially for very deep ResNets. We have also evaluated a variant of Table 1(d) without sharing the parameters of the heads, but observed similarly degraded performance. This issue cannot be simply remedied by level-specific heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How important are lateral connections? Table 1(e)</head><p>shows the ablation results of a top-down feature pyramid without the 1√ó1 lateral connections. This top-down pyramid has strong semantic features and fine resolutions. But we argue that the locations of these features are not precise, because these maps have been downsampled and upsampled several times. More precise locations of features can be directly passed from the finer levels of the bottom-up maps via the lateral connections to the top-down maps. As a results, FPN has an AR 1k score 10 points higher than Table <ref type="table" target="#tab_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(e).</head><p>How important are pyramid representations? Instead of resorting to pyramid representations, one can attach the head to the highest-resolution, strongly semantic feature maps of P 2 (i.e., the finest level in our pyramids). Similar to the single-scale baselines, we assign all anchors to the P 2 feature map. This variant (Table <ref type="table" target="#tab_0">1</ref>(f)) is better than the baseline but inferior to our approach. RPN is a sliding window detector with a fixed window size, so scanning over pyramid levels can increase its robustness to scale variance.</p><p>In addition, we note that using P 2 alone leads to more anchors (750k, Table 1(f)) caused by its large spatial resolution. This result suggests that a larger number of anchors is not sufficient in itself to improve accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object Detection with Fast/Faster R-CNN</head><p>Next we investigate FPN for region-based (non-sliding window) detectors. We evaluate object detection by the COCO-style Average Precision (AP) and PASCAL-style AP (at a single IoU threshold of 0.5). We also report COCO AP on objects of small, medium, and large sizes (namely, AP s , AP m , and AP l ) following the definitions in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Implementation details. The input image is resized such that its shorter side has 800 pixels. Synchronized SGD is used to train the model on 8 GPUs. Each mini-batch involves 2 image per GPU and 512 RoIs per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the first 60k mini-batches and 0.002 for the next 20k. We use 2000 RoIs per image for training and 1000 for testing. Training Fast R-CNN with FPN takes about 10 hours on the COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Fast R-CNN (on fixed proposals)</head><p>To better investigate FPN's effects on the region-based detector alone, we conduct ablations of Fast R-CNN on a fixed set of proposals. We choose to freeze the proposals as com-puted by RPN on FPN (Table <ref type="table" target="#tab_0">1</ref>(c)), because it has good performance on small objects that are to be recognized by the detector. For simplicity we do not share features between Fast R-CNN and RPN, except when specified.</p><p>As a ResNet-based Fast R-CNN baseline, following <ref type="bibr" target="#b15">[16]</ref>, we adopt RoI pooling with an output size of 14√ó14 and attach all conv5 layers as the hidden layers of the head. This gives an AP of 31.9 in Table <ref type="table" target="#tab_1">2</ref>(a). Table <ref type="table" target="#tab_1">2</ref>(b) is a baseline exploiting an MLP head with 2 hidden fc layers, similar to the head in our architecture. It gets an AP of 28.8, indicating that the 2-fc head does not give us any orthogonal advantage over the baseline in Table <ref type="figure" target="#fig_1">2(a)</ref>.</p><p>Table <ref type="table" target="#tab_1">2</ref>(c) shows the results of our FPN in Fast R-CNN. Comparing with the baseline in Table <ref type="table" target="#tab_1">2</ref>(a), our method improves AP by 2.0 points and small object AP by 2.1 points. Comparing with the baseline that also adopts a 2fc head (Table 2(b)), our method improves AP by 5.1 points. <ref type="foot" target="#foot_5">5</ref> These comparisons indicate that our feature pyramid is superior to single-scale features for a region-based object detector.  <ref type="bibr">(33.4 AP)</ref> is marginally worse than that of using all pyramid levels (33.9 AP, Table 2(c)). We argue that this is because RoI pooling is a warping-like operation, which is less sensitive to the region's scales. Despite the good accuracy of this variant, it is based on the RPN proposals of {P k } and has thus already benefited from the pyramid representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Faster R-CNN (on consistent proposals)</head><p>In the above we used a fixed set of proposals to investigate the detectors. But in a Faster R-CNN system <ref type="bibr" target="#b28">[29]</ref>, the RPN and Fast R-CNN must use the same network backbone in order to make feature sharing possible. Table <ref type="table" target="#tab_2">3</ref> shows the comparisons between our method and two baselines, all using consistent backbone architectures for RPN and Fast R-CNN. Table <ref type="table" target="#tab_2">3</ref>(a) shows our reproduction of the baseline Faster R-CNN system as described in <ref type="bibr" target="#b15">[16]</ref>. Under controlled settings, our FPN (Table <ref type="table" target="#tab_2">3</ref>(c)) is better than this strong baseline by 2.3 points AP and 3.8 points AP@0.5.</p><p>Note that Table <ref type="table" target="#tab_2">3</ref>(a) and (b) are baselines that are much stronger than the baseline provided by He et al. <ref type="bibr" target="#b15">[16]</ref> in Table 3(*). We find the following implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (ii) We train with 512 RoIs per image which accelerate convergence, in contrast to 64 RoIs in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>; (iii) We use 5 scale anchors instead of 4 in <ref type="bibr" target="#b15">[16]</ref> (adding 32 2 ); (iv) At test time we use 1000 proposals per image instead of 300 in <ref type="bibr" target="#b15">[16]</ref>. So comparing with He et al.'s ResNet-50 Faster R-CNN baseline in Table <ref type="table" target="#tab_2">3</ref>(*), our method improves AP by 7.6 points and AP@0.5 by 9.6 points.</p><p>Sharing features. In the above, for simplicity we do not share the features between RPN and Fast R-CNN. In Ta- ble 5, we evaluate sharing features following the 4-step training described in <ref type="bibr" target="#b28">[29]</ref>. Similar to <ref type="bibr" target="#b28">[29]</ref>, we find that sharing features improves accuracy by a small margin. Feature sharing also reduces the testing time.</p><p>Running time. With feature sharing, our FPN-based Faster R-CNN system has inference time of 0.165 seconds per image on a single NVIDIA M40 GPU for ResNet-50, and 0.19 seconds for ResNet-101. <ref type="foot" target="#foot_6">6</ref> As a comparison, the single-scale ResNet-50 baseline in Table <ref type="table" target="#tab_2">3</ref>(a) runs at 0.32 seconds. Our method introduces small extra cost by the extra layers in the FPN, but has a lighter weight head. Overall our system is faster than the ResNet-based Faster R-CNN counterpart. We believe the efficiency and simplicity of our method will benefit future research and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Comparing with COCO Competition Winners</head><p>We find that our ResNet-101 model in  On the test-dev set, our method increases over the existing best results by 0.5 points of AP (36.2 vs. 35.7) and 3.4 points of AP@0.5 (59.1 vs. 55.7). It is worth noting that our method does not rely on image pyramids and only uses a single input image scale, but still has outstanding AP on small-scale objects. This could only be achieved by highresolution image inputs with previous methods. Moreover, our method does not exploit many popular improvements, such as iterative regression <ref type="bibr" target="#b8">[9]</ref>, hard negative mining <ref type="bibr" target="#b34">[35]</ref>, context modeling <ref type="bibr" target="#b15">[16]</ref>, stronger data augmentation <ref type="bibr" target="#b21">[22]</ref>, etc. These improvements are complementary to FPNs and should boost accuracy further.</p><p>Recently, FPN has enabled new top results in all tracks of the COCO competition, including detection, instance segmentation, and keypoint estimation. See <ref type="bibr" target="#b13">[14]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Extensions: Segmentation Proposals</head><p>Our method is a generic pyramid representation and can be used in applications other than object detection. In this section we use FPNs to generate segmentation proposals, following the DeepMask/SharpMask framework <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>DeepMask/SharpMask were trained on image crops for predicting instance segments and object/non-object scores. At inference time, these models are run convolutionally to generate dense proposals in an image. To generate segments at multiple scales, image pyramids are necessary <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>It is easy to adapt FPN to generate mask proposals. We use a fully convolutional setup for both training and inference. We construct our feature pyramid as in Sec. 5.1 and set d = 128. On top of each level of the feature pyramid, we apply a small 5√ó5 MLP to predict 14√ó14 masks and object scores in a fully convolutional fashion, see Fig. <ref type="figure" target="#fig_3">4</ref>. Additionally, motivated by the use of 2 scales per octave in the image pyramid of <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, we use a second MLP of input size 7√ó7 to handle half octaves. The two MLPs play a similar role as anchors in RPN. The architecture is trained end-to-end; full implementation details are given in the appendix. image pyramid AR ARs ARm AR l time (s) DeepMask <ref type="bibr" target="#b26">[27]</ref> 37.1 15.8 50.1 54.9 0.49 SharpMask <ref type="bibr" target="#b27">[28]</ref> 39 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Segmentation Proposal Results</head><p>Results are shown in Table <ref type="table" target="#tab_7">6</ref>. We report segment AR and segment AR on small, medium, and large objects, always for 1000 proposals. Our baseline FPN model with a single 5√ó5 MLP achieves an AR of 43.4. Switching to a slightly larger 7√ó7 MLP leaves accuracy largely unchanged. Using both MLPs together increases accuracy to 45.7 AR. Increasing mask output size from 14√ó14 to 28√ó28 increases AR another point (larger sizes begin to degrade accuracy). Finally, doubling the training iterations increases AR to 48.1.</p><p>We also report comparisons to DeepMask <ref type="bibr" target="#b26">[27]</ref>, Sharp-Mask <ref type="bibr" target="#b27">[28]</ref>, and InstanceFCN <ref type="bibr" target="#b3">[4]</ref>, the previous state of the art methods in mask proposal generation. We outperform the accuracy of these approaches by over 8.3 points AR. In particular, we nearly double the accuracy on small objects.</p><p>Existing mask proposal methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref> are based on densely sampled image pyramids (e.g., scaled by 2 {‚àí2:0.5:1} in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>), making them computationally expensive. Our approach, based on FPNs, is substantially faster (our models run at 4 to 6 fps). These results demonstrate that our model is a generic feature extractor and can replace image pyramids for other multi-scale detection problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented a clean and simple framework for building feature pyramids inside ConvNets. Our method shows significant improvements over several strong baselines and competition winners. Thus, it provides a practical solution for research and applications of feature pyramids, without the need of computing image pyramids. Finally, our study suggests that despite the strong representational power of deep ConvNets and their implicit robustness to scale variation, it is still critical to explicitly address multiscale problems using pyramid representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) Using an image pyramid to build a feature pyramid. Features are computed on each of the image scales independently, which is slow. (b) Recent detection systems have opted to use only single scale features for faster detection. (c) An alternative is to reuse the pyramidal feature hierarchy computed by a ConvNet as if it were a featurized image pyramid. (d) Our proposed Feature Pyramid Network (FPN) is fast like (b) and (c), but more accurate. In this figure, feature maps are indicate by blue outlines and thicker outlines denote semantically stronger features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Top: a top-down architecture with skip connections,where predictions are made on the finest level (e.g.,<ref type="bibr" target="#b27">[28]</ref>). Bottom: our model that has a similar structure but leverages it as a feature pyramid, with predictions made independently at all levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A building block illustrating the lateral connection and the top-down pathway, merged by addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. FPN for object segment proposals. The feature pyramid is constructed with identical structure as for object detection. We apply a small MLP on 5√ó5 windows to generate dense object segments with output dimension of 14√ó14. Shown in orange are the size of the image regions the mask corresponds to for each pyramid level (levels P3‚àí5 are shown here). Both the corresponding image region size (light orange) and canonical object size (dark orange) are shown. Half octaves are handled by an MLP on 7x7 windows (7 ‚âà 5 ‚àö 2), not shown here. Details are in the appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>RPNfeature # anchors lateral? top-down? AR 100 AR 1k AR 1k Bounding box proposal results using RPN<ref type="bibr" target="#b28">[29]</ref>, evaluated on the COCO minival set. All models are trained on trainval35k. The columns "lateral" and "top-down" denote the presence of lateral and top-down connections, respectively. The column "feature" denotes the feature maps on which the heads are attached. All results are based on ResNet-50 and share the same hyper-parameters.</figDesc><table><row><cell>s</cell><cell>AR 1k m</cell><cell>AR 1k l</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Object detection results using Fast R-CNN<ref type="bibr" target="#b10">[11]</ref> on a fixed set of proposals (RPN, {P k }, Table1(c)), evaluated on the COCO minival set. Models are trained on the trainval35k set. All results are based on ResNet-50 and share the same hyper-parameters.</figDesc><table><row><cell>Faster R-CNN</cell><cell>proposals</cell><cell cols="7">feature head lateral? top-down? AP@0.5 AP APs APm AP l</cell></row><row><cell>(*) baseline from He et al. [16]  ‚Ä†</cell><cell>RPN, C 4</cell><cell>C 4</cell><cell>conv5</cell><cell>47.3</cell><cell>26.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>(a) baseline on conv4</cell><cell>RPN, C 4</cell><cell>C 4</cell><cell>conv5</cell><cell>53.1</cell><cell cols="4">31.6 13.2 35.6 47.1</cell></row><row><cell>(b) baseline on conv5</cell><cell>RPN, C 5</cell><cell>C 5</cell><cell>2fc</cell><cell>51.7</cell><cell cols="4">28.0 9.6 31.9 43.1</cell></row><row><cell>(c) FPN</cell><cell cols="2">RPN, {P k } {P k }</cell><cell>2fc</cell><cell>56.9</cell><cell cols="4">33.9 17.8 37.7 45.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Object detection results using Faster R-CNN<ref type="bibr" target="#b28">[29]</ref> evaluated on the COCO minival set. The backbone network for RPN are consistent with Fast R-CNN. Models are trained on the trainval35k set and use ResNet-50.</figDesc><table /><note>‚Ä† Provided by authors of<ref type="bibr" target="#b15">[16]</ref>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 (</head><label>2</label><figDesc>AP APs APm AP l AP @.5 AP APs APm AP l ours, Faster R-CNN on FPN ResNet-101 -59.1 36.2 18.2 39.0 48.2 58.5 35.8 17.5 38.7 47.8 Competition-winning single-model results follow:</figDesc><table><row><cell>image</cell><cell>test-dev</cell><cell>test-std</cell></row></table><note>d) and (e) show that removing top-down con-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of single-model results on the COCO detection benchmark. Some results were not available on the test-std set, so we also include the test-dev results (and for Multipath<ref type="bibr" target="#b39">[40]</ref> on minival). ‚Ä† : http://image-net.org/challenges/ talks/2016/GRMI-COCO-slidedeck.pdf. ‚Ä° : http://mscoco.org/dataset/#detections-leaderboard. ¬ß : This entry of AttractioNet<ref type="bibr" target="#b9">[10]</ref> adopts VGG-16 for proposals and Wide ResNet<ref type="bibr" target="#b38">[39]</ref> for object detection, so is not strictly a single-model result.nections or removing lateral connections leads to inferior results, similar to what we have observed in the above subsection for RPN. It is noteworthy that removing top-down connections (Table2(d)) significantly degrades the accuracy, suggesting that Fast R-CNN suffers from using the low-level features at the high-resolution maps.In Table2(f), we adopt Fast R-CNN on the single finest scale feature map of P 2 . Its result</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>More object detection results using Faster R-CNN and our FPNs, evaluated on minival. Sharing features increases train time by 1.5√ó (using 4-step training<ref type="bibr" target="#b28">[29]</ref>), but reduces test time.</figDesc><table><row><cell></cell><cell cols="2">ResNet-50</cell><cell cols="2">ResNet-101</cell></row><row><cell>share features?</cell><cell>AP @0.5</cell><cell>AP</cell><cell>AP @0.5</cell><cell>AP</cell></row><row><cell>no</cell><cell>56.9</cell><cell>33.9</cell><cell>58.0</cell><cell>35.0</cell></row><row><cell>yes</cell><cell>57.2</cell><cell>34.3</cell><cell>58.2</cell><cell>35.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table 5 is not sufficiently trained with the default learning rate schedule. So we increase the number of mini-batches by 2√ó at each learning rate when training the Fast R-CNN step. This increases AP on minival to 35.6, without sharing features.This model is the one we submitted to the COCO detection leaderboard, shown in Table4. We have not evaluated its feature-sharing version due to limited time, which should be slightly better as implied by Table5.Table4compares our method with the single-model results of the COCO competition winners, including the 2016 winner G-RMI and the 2015 winner Faster R-CNN+++. Without adding bells and whistles, our single-model entry has surpassed these strong, heavily engineered competitors.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>.8 17.4 53.1 59.1 0.77 InstanceFCN [4] 39.2 ---1.50 ‚Ä† FPN Mask Results: single MLP [5√ó5] 43.4 32.5 49.2 53.7 0.15 single MLP [7√ó7] 43.5 30.0 49.6 57.8 0.19 dual MLP [5√ó5, 7√ó7] 45.7 31.9 51.5 60.8 0.24 + 2x mask resolution 46.7 31.7 53.1 63.2 0.25 + 2x train schedule 48.1 32.6 54.2 65.6 0.25 Instance segmentation proposals evaluated on the first 5k COCO val images. All models are trained on the train set. DeepMask, SharpMask, and FPN use ResNet-50 while Instance-FCN uses VGG-16. DeepMask and SharpMask performance is computed with models available from https://github. com/facebookresearch/deepmask (both are the 'zoom' variants). ‚Ä† Runtimes are measured on an NVIDIA M40 GPU, except the InstanceFCN timing which is based on the slower K40.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Here we introduce P 6 only for covering a larger anchor scale of 512</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">. P 6 is simply a stride two subsampling of P 5 . P 6 is not used by the Fast R-CNN detector in the next section.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://github.com/kaiminghe/deep-residual-networks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">https://github.com/rbgirshick/py-faster-rcnn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">https://github.com/caffe2/caffe2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">We expect a stronger architecture of the head<ref type="bibr" target="#b29">[30]</ref> will improve upon our results, which is beyond the focus of this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">These runtimes are updated from an earlier version of this paper.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pyramid methods in image processing</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ogden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RCA engineer</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion &amp; semantic segmentation-aware CNN model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attend refine repeat: Active box proposal generation via in-out localization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbel√°ez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<title level="m">Mask r-cnn</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ParseNet: Looking wider to see better</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Object detection networks on convolutional feature maps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Human face detection in visual scenes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-CS-95-158R</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Original approach for the localisation of objects in images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monrocq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proc. on Vision, Image, and Signal Processing</title>
				<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
